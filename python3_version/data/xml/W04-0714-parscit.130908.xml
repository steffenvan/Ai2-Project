<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997496">
Topic Identification in Chinese Based on Centering Model
</title>
<author confidence="0.995048">
Ching-Long Yeh and Yi-Chun Chen
</author>
<affiliation confidence="0.98355">
Department of Computer Science and Engineering
Tatung University
</affiliation>
<address confidence="0.491577">
40 Chungshan N. Rd. 3rd. Section
Taipei 104 Taiwan
R.O.C.
</address>
<email confidence="0.766104">
chingyeh@cse.ttu.edu.tw, d8806005@mail.ttu.edu.tw
</email>
<sectionHeader confidence="0.981448" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999672176470588">
In this paper we are concerned with
identifying the topics of sentences in Chinese
texts. The key elements of the centering model
of local discourse coherence are employed to
identify the topic which is the most salient
element in a Chinese sentence. Due to the
phenomenon of zero anaphora occurring in
Chinese texts frequently, in addition to the
centering model, we further employ the
constraint rules to identify the antecedents of
zero anaphors. Unlike most traditional
approaches to parsing sentences based on the
integration of complex linguistic information
and domain knowledge, we work on the
output of a part-of-speech tagger and use
shallow parsing instead of complex parsing to
identify the topics from sentences.
</bodyText>
<sectionHeader confidence="0.998694" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991940625">
One of the most striking characteristics in a
topic-prominent language like Chinese is the
important element, “topic,” in a sentence which
can represent what the sentence is about (Li and
Thompson, 1981). That is, if we can identify topics
from Chinese sentences, we can obtain the most
information embedded in the text. In this paper, we
tend to identify the topic of each utterance within a
discourse based on the centering model. However,
in many natural languages, elements that can be
easily deduced by the reader are frequently omitted
from expressions in texts. The elimination of
anaphoric expressions is termed zero anaphor (ZA)
which often occurs in topic position in a Chinese
sentence, due to their prominence in discourse.
Accordingly, to accomplish the task of topic
identification, we have to solve the problem of
zero anaphora resolution.
There are several methods of anaphora
resolution. One method is to integrate different
knowledge sources or factors (e.g. gender and
number agreement, c-command constraints,
semantic information) that discount unlikely
candidates until a minimal set of plausible
candidates is obtained (Grosz et al., 1995; Lappin
and Leass, 1994; Okumura and Tamura, 1996;
Walker et al., 1998; Yeh and Chen, 2001).
Anaphoric relations between anaphors and their
antecedents are identified based on the integration
of linguistic and domain knowledge. However, it is
very labor-intensive and time-consuming to
construct a domain knowledge base. Another
method employs statistical models or AI
techniques, such as machine learning, to compute
the most likely candidate (Aone and Bennett, 1995;
Connoly et al., 1994; Ge et al., 1998; Seki et al.,
2002). This method can sort out the above
problems. However, it heavily relies upon the
availability of sufficiently large text corpora that
are tagged, in particular, with referential
information (Stuckardt, 2002).
Our method is an inexpensive, fast and reliable
procedure for anaphora resolution, which relies on
cheaper and more reliable NLP tools such as part-
of-speech (POS) tagger and shallow parsers
(Baldwin, 1997; Ferrández et al., 1998; Kennedy
and Boguraev, 1996; Mitkov, 1998; Yeh and Chen,
2003). The resolution process works from the
output of a POS tagger enriched with annotations
of grammatical function of lexical items in the
input text stream. The shallow parsing technique is
used to detect zero anaphors and identifies the
noun phrases preceding the anaphors as
antecedents.
In the following sections we first describe the
centering model which including the key elements
of the centering model of local discourse
coherence. In Section 3 we describe the details of
shallow parsing we employed. In Section 4 we
explain our ZA resolution method based on the
centering model and the constraint rules. The
method of topic identification in Chinese sentences
is illustrated in Section 5. In the last section the
conclusions are made.
</bodyText>
<sectionHeader confidence="0.980363" genericHeader="introduction">
2 Centering Model
</sectionHeader>
<bodyText confidence="0.999868275862069">
In the centering theory (Grosz and Sidner, 1986;
Grosz et al, 1995; Walker et al., 1994; Strube and
Hahn, 1996), the &apos;attentional state&apos; was identified as
a basic component of discourse structure that
consisted of two levels of focusing: global and
local. For Grosz and Sidner, the centering theory
provided a model for monitoring local focus and
yielded the centering model which was designed to
account for the difference in the perceived
coherence of discourses. In the centering model,
each utterance U in a discourse segment has two
structures associated with it, called forward-
looking centers, Cf(U), and backward-looking
center, Cb(U). The forward-looking centers of Un,
Cf(Un), depend only on the expressions that
constitute that utterance. They are not constrained
by features of any previous utterance in the
discourse segment (DS), and the elements of Cf(Un)
are partially ordered to reflect relative prominence
in Un. Grosz et al., in their paper (Grosz et al,
1995), assume that grammatical roles are the major
determinant for ranking the forward-looking
centers, with the order “Subject &gt; Object(s) &gt;
Others”. The superlative element of Cf(Un) may
become the Cb of the following utterance, Cb(Un+1).
In addition to the structures for centers, Cb, and
Cf, the centering model specifies a set of
constraints and rules (Grosz et al, 1995; Walker et
al. 1994).
</bodyText>
<sectionHeader confidence="0.347074" genericHeader="method">
Constraints
</sectionHeader>
<bodyText confidence="0.965468">
For each utterance Ui in a discourse segment
consisting of utterances U1, ..., Um:
</bodyText>
<listItem confidence="0.996657">
1. Ui has exactly one Cb.
2. Every element of Cf(Ui) must be realized in Ui.
3. Ranking of elements in Cf(Ui) guides
determination of Cb(Ui+1).
4. The choice of Cb(Ui) is from Cf(Ui-1), and can
not be from Cf(Ui-2) or other prior sets of Cf.
</listItem>
<bodyText confidence="0.908676714285714">
Backward-looking centers, Cbs, are often
omitted or pronominalized and discourses that
continue centering the same entity are more
coherent than those that shift from one center to
another. This means that some transitions are
preferred over others. These observations are
encapsulated in two rules:
</bodyText>
<subsectionHeader confidence="0.902398">
Rules
</subsectionHeader>
<bodyText confidence="0.9768105">
For each utterance Ui in a discourse segment
consisting of utterances U1, ..., Um:
</bodyText>
<listItem confidence="0.8371026">
I. If any element of Cf(Ui) is realized by a
pronoun in Ui+1 then the Cb(Ui+1) must be
realized by a pronoun also.
II. Sequences of continuation are preferred over
sequence of retaining; and sequences of
</listItem>
<bodyText confidence="0.989661">
retaining are to be preferred over sequences of
shifting.
Rule I represents one function of pronominal
reference: the use of a pronoun to realize the Cb
signals the hearer that the speaker is continuing to
talk about the same thing. Psychological research
and cross-linguistic research have validated that
the Cb is preferentially realized by a pronoun in
English and by equivalent forms (i.e. zero
anaphora) in other languages (Grosz et al, 1995).
Rule II reflect the intuition that continuation of
the center and the use of retentions when possible
to produce smooth transitions to a new center
provide a basis for local coherence.
For example in (1), the subjects of the utterance
(1b) and (1d) are eliminated, and their antecedents
are identified as the subjects of the preceding
utterances (1a) and (1c) respectively1 according to
the centering model.
</bodyText>
<equation confidence="0.612322">
(1) a.
</equation>
<bodyText confidence="0.981918266666667">
Electronics stocki receive USA high-tech
stock heavy-fall affect
Electronics stocksi were affected by high-
tech stocks fallen heavily in America.
b. i
(Electronics stocks)i continue fall
(Electronics stocks)i continued falling down.
c. j
Securities stocksj also have relative
respondence
Securities stocksj also had respondence.
d. j
(Securities stocks) j continue fall by close.
(Securities stocks) j fell by close one after
another.
</bodyText>
<sectionHeader confidence="0.972179" genericHeader="method">
3 Shallow Parsing
</sectionHeader>
<bodyText confidence="0.982082111111111">
Shallow (or partial) parsing which is an
inexpensive, fast and reliable method does not
deliver full syntactic analysis but is limited to
parsing smaller constituents such as noun phrases
or verb phrases (Abney, 1996; Li and Roth, 2001;
Mitkov, 1999). For example, the sentence (2) can
be divided as follows:
(2)
Hualien became the popular tourist attraction.
</bodyText>
<equation confidence="0.594187">
Æ [NP ] [VP ] [NP ]
1 We use a φab to denote a zero anaphor, where the
</equation>
<bodyText confidence="0.823914181818182">
subscript a is the index of the zero anaphor itself and the
superscript b is the index of the referent. A single
without any script represents an intrasentential zero
anaphor. Also note that a superscript attached to an NP
is used to represent the index of the referent.
i
[NP Hualien] [VP became] [NP the popular
tourist attraction]
Given a Chinese sentence, our method of
shallow parsing is divided into the following steps:
First the sentence is divided into a sequence of
POS-tagged words by employing a segmentation
program, AUTOTAG, which is a POS tagger
developed by CKIP, Academia Sinica (CKIP,
1999). Second the sequence of words is parsed into
smaller constituents such as noun phrases and verb
phrases with phrase-level parsing. Each phrase is
represented as a word list. Then the sequence of
word lists is transformed into triples, [S,P,O]. For
example in (3), (3b) is the output of sentence (3a)
produced by AUTOTAG, and (3c) is the triple
representation.
</bodyText>
<listItem confidence="0.995048333333333">
(3) a. [ (Nc) (VG) (VH) (DE)
(VA) (Na)]
b. [NP,[ ]], [VP,[]], [NP,[
</listItem>
<bodyText confidence="0.9975515">
The definition of triple representation is
illustrated in Definition 1.The triple here is a
simple representation which consists of three
elements: S, P and O which correspond to the
Subject (noun phrase), Predicate (verb phrase) and
Object (noun phrase) respectively in a clause.
</bodyText>
<sectionHeader confidence="0.356686" genericHeader="method">
Definition 1:
</sectionHeader>
<bodyText confidence="0.878108">
A Triple T is characterized by a 3-tuple:
T = [S, P, O] where
</bodyText>
<listItem confidence="0.996083571428572">
• S is a list of nouns whose grammatical
role is the subject of a clause.
• P is a list of verbs or a preposition whose
grammatical role is the predicate of a
clause.
• O is a list of nouns whose grammatical
role is the object of a clause.
</listItem>
<bodyText confidence="0.985495961538462">
In the step of triple transformation, the sequence
of word lists as shown in (3b) is transformed into
triples by employing the Triple Rules. The Triple
Rules is built by referring to the Chinese syntax.
There are four kinds of Triples in the Triple Rules,
which corresponds to five basic clauses: subject +
transitive verb + object, subject + intransitive verb,
subject + preposition + object, and a noun phrase
only. The rules listed below are employed in order:
Triple Rules:
Triple1(S,P,O) 4 np(S), vtp(P), np(O).
Triple2(S,P,none) 4 np(S), vip(P).
Triple3(S,P,O) 4 np(S), prep(P), np(O).
Triple4(S,none,none) 4 np(S).
The vtp(P) denotes the predicate is a transitive
verb phrase, which contains a transitive verb in the
rightmost position in the phrase; likewise the vip(P)
denotes the predicate is an intransitive verb phrase,
which contains an intransitive verb in the rightmost
position in the phrase. In the rule Triple3, the
prep(P) denotes the predicate is a preposition. The
Triple4 is employed if only a sentence contains
only one noun phrase and no other constituent. If
all the rules in the Triple Rules failed, the ZA
Triple Rules are employed to detect zero anaphor
(ZA) candidates.
</bodyText>
<sectionHeader confidence="0.713009" genericHeader="method">
ZA Triple Rules:
</sectionHeader>
<equation confidence="0.9714805">
Triple1z1(zero,P,O)4 vtp(P), np(O).
Triple1z2(S,P,zero)4 np(S), vtp(P).
Triple1z3(zero,P,zero)4 vtp(P) .
Triple2z1 (zero,P,none)4 vip(P).
Triple3z1(zero,P,O) 4 prep(P), np(O).
Triple4z1(zero,P,O) 4 co-conj(P), np(O).
</equation>
<bodyText confidence="0.999923333333333">
The zero anaphora in Chinese generally occurs
in the topic, subject or object position. The rules
Triple1z1, Triple2z1, and Triple3z1 detect the zero
anaphora occurring in the topic or subject position.
The rule Triple1z2 detects the zero anaphora in the
object position and Triple1z3 detect the zero
anaphora occurring in both subject and object
positions. In the Triple4, the co-conj(P) denotes a
coordinating conjunction appearing in the initial
position of a clause. For example in (4), there are
two triples generated. In the second triple, zero
denotes a zero anaphor according to Triple1z1.
</bodyText>
<equation confidence="0.58722">
(4)
</equation>
<bodyText confidence="0.652634">
Zhangsan entered a competition and won the
champion.
4 [[[], [], []], [[zero], [], [
]]]
[[[Zhangsan], [enter], [competition]], [[zero],
[win], [champion]]]
</bodyText>
<sectionHeader confidence="0.8482725" genericHeader="method">
4 Zero Anaphora Resolution
4.1 ZA Resolution Method
</sectionHeader>
<bodyText confidence="0.999969777777778">
The process of analyzing Chinese zero anaphora
is different from general pronoun resolution in
English because zero anaphors are not expressed in
discourse. Therefore, the ZA resolution method we
develop is divided into three phases. First each
sentence of an input document is translated into
triples as described in Section 3. Second is ZA
identification that verifies each ZA candidates
annotated in triples by employing ZA
</bodyText>
<equation confidence="0.5187754">
,]]
f. [[ ], [], [
,,,
]]
,,
</equation>
<bodyText confidence="0.893560111111111">
identification constraints. Third is antecedent
identification that identifies the antecedent of each
detected ZA using rules based on the centering
model.
In the ZA detection phase, we use the ZA Triple
Rules described in the Section 3 to detect omitted
cases as ZA candidates denoted by zero in triples.
Table 1 shows some examples corresponding to
the ZA Triple Rules.
</bodyText>
<table confidence="0.999812515151515">
ZA Triple Rule Example
Triple1z1 (1b)
(zero,P,O) zhuangdao yi ge ren
(he) bump-to a person
(He) bumped into a person.
Triple1z2
(S,P,zero)
Zhangsan xihuan ma
Zhangsan like (somebody
or something) Q2
Does Zhangsan like
(somebody or something)?
Triple1z3
(zero,P,zero)
xihuan
(he) like (somebody or
something)
(He) likes (somebody or
something).
Triple2z1
(zero,P,none)
qu gouwu le
(he) go shopping ASPECT
(He) has gone shopping.
Triple3z1 zai nabian
(zero,P,O) (he) in there
(He) is there.
Triple4z1
(zero,P,O)
gen xiaopengyou wan
(he) with child play
(He) is playing with little
children.
</table>
<tableCaption confidence="0.999855">
Table 1: Examples of zero anaphora
</tableCaption>
<bodyText confidence="0.98911025">
After ZA candidates are detected by employing
the ZA Triple Rules, the ZA identification
constraints are utilized to filter out non-anaphoric
cases. In the ZA identification constraints, the
constraint 1 is employed to exclude the exophora3
or cataphora4 which is different from anaphora in
2 We use a Q to denote a question (ma); a
ASPECT to denote aspect marker.
</bodyText>
<footnote confidence="0.9958376">
3 Exophora is reference of an expression directly to
an extralinguistic referent and the referent does not
require another expression for its interpretation.
4 Cataphora arises when a reference is made to an
entity mentioned subsequently.
</footnote>
<bodyText confidence="0.999962333333333">
texts. The constraint 2 includes some cases might
be incorrectly detected as zero anaphors, such as
passive sentences or inverted sentences (Hu, 1995).
</bodyText>
<sectionHeader confidence="0.329679" genericHeader="method">
ZA identification constraints
</sectionHeader>
<bodyText confidence="0.670113">
For each ZA candidate c in a discourse:
</bodyText>
<listItem confidence="0.9935552">
1. c can not be in the first utterance in a
discourse segment
2. ZA does not occur in the following case:
NP + bei + NP + VP + c
NP (topic) + NP (subject) + VP + c
</listItem>
<bodyText confidence="0.999944">
In the antecedent identification phase, we
employ the concept, ‘backward-looking center’ of
centering model to identify the antecedent of each
ZA. First we use noun phrase rules to obtain noun
phrases in each utterance, and then the antecedent
is identified as the most prominent noun phrase of
the preceding utterance (Yeh and Chen, 2001):
</bodyText>
<subsectionHeader confidence="0.644314">
Antecedent identification rule:
</subsectionHeader>
<bodyText confidence="0.99467536">
For each zero anaphor z in a discourse segment
consisting of utterances U1, ... , Um:
If z occurs in Ui, and no zero anaphor occurs in
Ui-1
then choose the noun phrase with the
corresponding grammatical role in Ui-1 as
the antecedent
Else if only one zero anaphor occurs in Ui-1
then choose the antecedent of the zero
anaphor in Ui-1 as the antecedent of z
Else if more than one zero anaphor occurs in
Ui-1
then choose the antecedent of the zero
anaphor in Ui-1 as the antecedent of z
according to grammatical role criteria: Topic
&gt; Subject &gt; Object &gt; Others
End if
Due to topic-prominence in Chinese (Li and
Thompson, 1981), topic is the most salient
grammatical role. In general, if the topic is omitted,
the subject will be in the initial position of an
utterance. If the topic and subject are omitted
concurrently, the ZA occurs. Since the antecedent
identification rule is corresponding to the concept
of centering model.
</bodyText>
<subsectionHeader confidence="0.81578">
4.2 ZA Resolution Experiment
</subsectionHeader>
<bodyText confidence="0.996586916666667">
In the experiment of ZA resolution, we use a test
corpus which is a collection of 150 news articles
contained 998 paragraphs, 4631 utterances, and
40884 Chinese words. By employing the ZA Triple
Rules and ZA identification constraints mentioned
previously, zero anaphors occur in topic or subject,
and object positions can be detected. Because the
ZA Triple Rules cover each possible topic or
subject, and object omission cases, the result
shows that the zero anaphors are over detected and
the precision rate (PR) is 84% calculated using
equation 1.
</bodyText>
<equation confidence="0.804653333333333">
No. of ZA correctly detected ....(1)
PR of ZA detection =
No. of ZA candidates
</equation>
<bodyText confidence="0.96797322">
The main errors of ZA detection occur in the
experiment when parsing inverted sentences and
non-anaphoric cases (e.g. exophora or cataphora)
(Hu, 1995; Mitkov, 2002). Cataphora is similar to
anaphora, the difference being the direction of the
reference. In this paper, we do not deal with the
case that the referent of a zero anaphor is in the
following utterances, but we can detect about 60%
cataphora in the test corpus by employing ZA
identification constraint 1.
In the phase of antecedent identification, we
take the output of employing the ZA Triple Rules
and ZA identification constraints, and further to
identify the antecedents of zero anaphors by using
antecedent identification rule based on the
centering model. For example, in the discourse
segment (5), the zero anaphors are detected in the
utterances (5b) and (5c). According to the
antecedent identification rule, the noun phrase,
‘Kee-lung General Hospital,’ whose
grammatical role is corresponding to the zero
anaphor φ1i in (5b) is identified as the antecedent.
Subsequently, the antecedent of the zero anaphor
φ2 in (5c) is identified as the antecedent of i
i φ1 in
(5b),
(5) a.
Jilong yiyuan wei kuoda fuwu fanwei
Kee-lung hospital for expand service
coverage
Kee-lungi General Hospital aims to increase
service coverage.
c. φ2 i
huo weishengshu renke wei banli wailao
tijian yiyuan
(Kee-lung General Hospital)i obtain
Department-of-Health certify to-be handle
foreign-laborer physical-examination
hospital
(Kee-lung General Hospital)i is certified by
Department of Health as a hospital which
can handle physical examinations of foreign
laborers.
The recall rates (RR) and precision rates (PR) of
ZA resolution is 70% and 60.3% respectively
calculated using equation 2 and equation 3. Errors
occur in the phase when a zero anaphor refers to an
entity other than the corresponding grammatical
role or the antecedent of the zero anaphor in the
preceding utterance.
</bodyText>
<table confidence="0.96722">
No. of ant. correctly identified..(2)
RR of ZA resolution =
No. of ZA candidates
No. of ant. correctly identified...(3)
PR of ZA resolution =
No. of ZA occurred in text
</table>
<sectionHeader confidence="0.959195" genericHeader="method">
5 Topic Identification
</sectionHeader>
<bodyText confidence="0.999913611111111">
Topic identification is similar to theme
identification in (Rambow, 1993). The theme
clearly corresponds to the Cb: the theme, under a
general definition, is what the current utterance is
about; what utterances are about provides a link to
previous discourse, since otherwise the text would
be incoherent. The role of the Cb is precisely to
provide such a link.
In our approach, in addition to the centering
model, we further employ the antecedent
identification rule to identify the topic. When a ZA
occurs in the utterance Ui, the antecedent of the ZA
is identified as the topic of Ui. Otherwise, if the
transition relation, center shifting, occurs, topic
will not be identified as any of the element in the
preceding utterance but the element in the current
utterance according to grammatical role criteria.
The topic identification rule is described below:
</bodyText>
<subsectionHeader confidence="0.675644">
Topic identification rule:
</subsectionHeader>
<bodyText confidence="0.999469833333333">
For identifying each topic t in a discourse
segment consisting of utterances U1, ... , Um:
If at least one ZA occurs in Ui
then refer to grammatical role criteria to
choose the antecedent of the ZA as the t
Else if no ZA occurs in Ui
then refer to grammatical role criteria to
choose one element of Ui as the t
End if
We now take the discourse segment (1) as an
example to identify each topic of the utterances (1a)
to (1d) by employing the topic identification rule.
As illustrated in Table2, the topic of (1a) is
‘Electronics stocks,’ and the topic of (1b) is
identified as the antecedent of i,
‘Electronics stocks.’ Similarly, the topic of (1d) is
‘Securities stocks,’ which is the same as
the topic of (1c).
</bodyText>
<equation confidence="0.354379">
b. φ1i
</equation>
<bodyText confidence="0.752214571428571">
jiji tisheng yiliao fuwu pinzhi ji biaozhunhua
(Kee-lung General Hospital)i active improve
medical-treatment service quality and
standardization
(Kee-lung General Hospital)i actively
improves the service quality of medical
treatment and standardization.
</bodyText>
<table confidence="0.977005434782609">
.
i
Utterance Topic
(1a) i
Electronics
stocks
Electronics stocksi were
affected by high-tech
stocks fallen heavily in
America
(1b) i
(Electronics stocks)i
continued falling down.
Electronics
stocks
(1c) j
Securities stocksj also had Securities
respondence stocks
j
Securities
stocks
(Securities stocks)j fell by
close one after another
</table>
<tableCaption confidence="0.998575">
Table 2: Examples of zero anaphora
</tableCaption>
<sectionHeader confidence="0.997423" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999956444444444">
In this paper, we propose a method of topic
identification in Chinese based on the centering
model. Based on observations on real texts, we
found that to identify the topics in Chinese context
is much related to the issue of zero anaphora
resolution. We use a zero anaphora resolution
method to resolve the problem of ellipsis in
Chinese text. The zero anaphora resolution method
works on the output of a part-of-speech tagger and
employs a shallow parsing instead of a complex
parsing to resolve zero anaphors in Chinese text.
Due to time limit, we have not applied the result of
topic identification to applications for evaluation.
We will further continue improving the accuracy
of zero anaphora resolution and develop the
applications based on topic identification, such as
information extraction/retrieval and text
categorization.
</bodyText>
<sectionHeader confidence="0.998572" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998385">
We give our special thanks to CKIP, Academia
Sinica for making great efforts in computational
linguistics and sharing the Autotag program to
academic research.
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99518205785124">
Abney, Steven. 1996. Tagging and Partial Parsing.
In: Ken Church, Steve Young, and Gerrit
Bloothooft (eds.), Corpus-Based Methods in
Language and Speech. An ELSNET volume.
Kluwer Academic Publishers, Dordrecht.
Aone, Chinatsu and Bennett, Scott William. 1995.
Evaluating automated and manual acquisition of
anaphora resolution strategies. In Proceedings of
the 33rd Annual Meeting of the ACL, Santa Cruz,
CA, pages 122–129.
Baldwin B. 1997. CogNIAC: high precision
coreference with limited knowledge and
linguistic resources. ACL/EACL workshop on
Operational factors in practical, robust anaphor
resolution.
CKIP. 1999. Version 1.0
(Autotag), http://godel.iis.sinica.edu.tw/CKIP/,
Academia Sinica.
Connoly, Dennis, John D. Burger &amp; David S. Day.
1994. A Machine learning approach to
anaphoric reference. Proceedings of the
International Conference on New Methods in
Language Processing, 255-261, Manchester,
United Kingdom.
Eleni Miltsakaki. 1999. Locating Topics in Text
Processing. In Proceedings of CLIN &apos;99.
Ferrández, A., Palomar, Manuel and Moreno, Lidia.
1998. Anaphor Resolution in Unrestricted Texts
with Partial Parsing. Proceedings of the 18th
International Conference on Computational
Linguistics (COLING&apos;98)/ACL&apos;98 Conference,
pages 385-391. Montreal, Canada.
Ge, Niyu, Hale, John and Charniak, Eugene. 1998.
A statistical approach to anaphora resolution. In
Proceedings of the Sixth Workshop on Very
Large Corpora, pages 161 –170.
Grosz, B. J. and Sidner, C. L.. 1986. Attention,
intentions, and the structure of discourse.
Computational Linguistics, No 3 Vol 12, pp.
175-204.
Grosz, B. J., Joshi, A. K. and Weinstein, S. 1995.
Centering: A Framework for Modeling the
Local Coherence of Discourse. Computational
Linguistics, 21(2), pp. 203-225.
Hu, Wenze. 1995. Functional Perspectives and
Chinese Word Order. Ph. D. dissertation, The
Ohio State University.
Kennedy, Christopher and Branimir Boguraev.
1996. Anaphora for everyone: pronominal
anaphora resolution without a parser.
Proceedings of the 16th International
Conference on Computational Linguistics
(COLING&apos;96), 113-118. Copenhagen, Denmark.
Lappin, S. and Leass, H. 1994. An algorithm for
pronominal anaphor resolution. Computational
Linguistics, 20(4).
Li, Charles N. and Thompson, Sandra A. 1981.
Mandarin Chinese – A Functional Reference
Grammar, University of California Press.
Li, X.; Roth D. 2001. Exploring Evidence for
Shallow Parsing. Proceedings of Workshop on
Computational Natural Language Learning,
Toulouse, France.
Okumura, Manabu and Kouji Tamura. 1996. Zero
pronoun resolution in Japanese discourse based
on centering theory. In Proceedings of the 16th
International Conference on Computational
Linguistics (COLING-96), 871-876.
Mitkov, Ruslan. 1998. Robust pronoun resolution
with limited knowledge. Proceedings of the 18th
International Conference on Computational
Linguistics (COLING&apos;98)/ACL&apos;98 Conference.
Montreal, Canada.
Mitkov, Ruslan. 1999. Anaphora resolution: the
state of the art. Working paper (Based on the
COLING&apos;98/ACL&apos;98 tutorial on anaphora
resolution). University of Wolverhampton,
Wolverhampton.
Mitkov, Ruslan. 2002. Anaphora Resolution,
Longman.
Rambow, O. (1993). Pragmatic aspects of
scrambling and topicalization in German: A
Centering Approach. In IRCS Workshop on
Centering in Discourse. Univ. of Pennsylvania,
1993.
Seki, Kazuhiro, Fujii, Atsushi, and Ishikawa,
Tetsuya. 2002. A Probabilistic Method for
Analyzing Japanese Anaphora Integrating Zero
Pronoun Detection and Resolution. Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
pp.911-917.
Strube, M. and U. Hahn. 1996. Functional
Centering. Proc. Of ACL ’96, Santa Cruz, Ca.,
pp.270-277.
Stuckardt, Roland. 2002. Machine-Learning-Based
vs. Manually Designed Approaches to Anaphor
Resolution: the Best of Two Worlds. In
Proceedings of the 4th Discourse Anaphora and
Anaphor Resolution Colloquium (DAARC2002),
University of Lisbon, Portugal, pages 211-216.
Givón, T. 1983. Topic continuity in discourse: An
introduction. Topic continuity in discourse.
Amsterdam/Philadelphia [Pennsylvania]: John
Benjamins.
Walker, M. A., M. Iida and S. Cote. 1994. Japan
Discourse and the Process of Centering.
Computational Linguistics, 20(2): 193-233.
Walker, M. A. 1998. Centering, anaphora
resolution, and discourse structure. In Marilyn A.
Walker, Aravind K. Joshi, and Ellen F. Prince,
editors, Centering in Discourse. Oxford
University Press.
Yeh, Ching-Long and Chen, Yi-Chun. 2001. An
empirical study of zero anaphora resolution in
Chinese based on centering theory. In
Proceedings of ROCLING XIV, Tainan, Taiwan.
Yeh, Ching-Long and Chen, Yi-Chun. 2003. Zero
anaphora resolution in Chinese with partial
parsing based on centering theory. In
Proceedings of IEEE NLP-KE03, Beijing, China.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.298147">
<title confidence="0.998885">Topic Identification in Chinese Based on Centering Model</title>
<author confidence="0.998935">Ching-Long Yeh</author>
<author confidence="0.998935">Yi-Chun Chen</author>
<affiliation confidence="0.9993645">Department of Computer Science and Engineering Tatung University</affiliation>
<address confidence="0.942939">40 Chungshan N. Rd. 3rd. Section</address>
<note confidence="0.459603333333333">Taipei 104 Taiwan R.O.C. chingyeh@cse.ttu.edu.tw, d8806005@mail.ttu.edu.tw</note>
<abstract confidence="0.999788777777778">In this paper we are concerned with identifying the topics of sentences in Chinese texts. The key elements of the centering model of local discourse coherence are employed to identify the topic which is the most salient element in a Chinese sentence. Due to the phenomenon of zero anaphora occurring in Chinese texts frequently, in addition to the centering model, we further employ the constraint rules to identify the antecedents of zero anaphors. Unlike most traditional approaches to parsing sentences based on the integration of complex linguistic information and domain knowledge, we work on the output of a part-of-speech tagger and use shallow parsing instead of complex parsing to identify the topics from sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Tagging and Partial Parsing. In:</title>
<date>1996</date>
<booktitle>Corpus-Based Methods in Language and Speech. An ELSNET volume.</booktitle>
<editor>Ken Church, Steve Young, and Gerrit Bloothooft (eds.),</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="7795" citStr="Abney, 1996" startWordPosition="1220" endWordPosition="1221"> affect Electronics stocksi were affected by hightech stocks fallen heavily in America. b. i (Electronics stocks)i continue fall (Electronics stocks)i continued falling down. c. j Securities stocksj also have relative respondence Securities stocksj also had respondence. d. j (Securities stocks) j continue fall by close. (Securities stocks) j fell by close one after another. 3 Shallow Parsing Shallow (or partial) parsing which is an inexpensive, fast and reliable method does not deliver full syntactic analysis but is limited to parsing smaller constituents such as noun phrases or verb phrases (Abney, 1996; Li and Roth, 2001; Mitkov, 1999). For example, the sentence (2) can be divided as follows: (2) Hualien became the popular tourist attraction. Æ [NP ] [VP ] [NP ] 1 We use a φab to denote a zero anaphor, where the subscript a is the index of the zero anaphor itself and the superscript b is the index of the referent. A single without any script represents an intrasentential zero anaphor. Also note that a superscript attached to an NP is used to represent the index of the referent. i [NP Hualien] [VP became] [NP the popular tourist attraction] Given a Chinese sentence, our method of shallow par</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Abney, Steven. 1996. Tagging and Partial Parsing. In: Ken Church, Steve Young, and Gerrit Bloothooft (eds.), Corpus-Based Methods in Language and Speech. An ELSNET volume. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>Scott William Bennett</author>
</authors>
<title>Evaluating automated and manual acquisition of anaphora resolution strategies.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the ACL,</booktitle>
<pages>122--129</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="2640" citStr="Aone and Bennett, 1995" startWordPosition="395" endWordPosition="398">c-command constraints, semantic information) that discount unlikely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001). Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the</context>
</contexts>
<marker>Aone, Bennett, 1995</marker>
<rawString>Aone, Chinatsu and Bennett, Scott William. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. In Proceedings of the 33rd Annual Meeting of the ACL, Santa Cruz, CA, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Baldwin</author>
</authors>
<title>CogNIAC: high precision coreference with limited knowledge and linguistic resources. ACL/EACL workshop on Operational factors in practical, robust anaphor resolution.</title>
<date>1997</date>
<contexts>
<context position="3114" citStr="Baldwin, 1997" startWordPosition="471" endWordPosition="472"> method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream. The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Se</context>
</contexts>
<marker>Baldwin, 1997</marker>
<rawString>Baldwin B. 1997. CogNIAC: high precision coreference with limited knowledge and linguistic resources. ACL/EACL workshop on Operational factors in practical, robust anaphor resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CKIP</author>
</authors>
<date>1999</date>
<journal>Version</journal>
<volume>1</volume>
<note>(Autotag), http://godel.iis.sinica.edu.tw/CKIP/, Academia Sinica.</note>
<contexts>
<context position="8618" citStr="CKIP, 1999" startWordPosition="1364" endWordPosition="1365">he subscript a is the index of the zero anaphor itself and the superscript b is the index of the referent. A single without any script represents an intrasentential zero anaphor. Also note that a superscript attached to an NP is used to represent the index of the referent. i [NP Hualien] [VP became] [NP the popular tourist attraction] Given a Chinese sentence, our method of shallow parsing is divided into the following steps: First the sentence is divided into a sequence of POS-tagged words by employing a segmentation program, AUTOTAG, which is a POS tagger developed by CKIP, Academia Sinica (CKIP, 1999). Second the sequence of words is parsed into smaller constituents such as noun phrases and verb phrases with phrase-level parsing. Each phrase is represented as a word list. Then the sequence of word lists is transformed into triples, [S,P,O]. For example in (3), (3b) is the output of sentence (3a) produced by AUTOTAG, and (3c) is the triple representation. (3) a. [ (Nc) (VG) (VH) (DE) (VA) (Na)] b. [NP,[ ]], [VP,[]], [NP,[ The definition of triple representation is illustrated in Definition 1.The triple here is a simple representation which consists of three elements: S, P and O which corres</context>
</contexts>
<marker>CKIP, 1999</marker>
<rawString>CKIP. 1999. Version 1.0 (Autotag), http://godel.iis.sinica.edu.tw/CKIP/, Academia Sinica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Connoly</author>
<author>John D Burger</author>
<author>David S Day</author>
</authors>
<title>A Machine learning approach to anaphoric reference.</title>
<date>1994</date>
<booktitle>Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>255--261</pages>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="2662" citStr="Connoly et al., 1994" startWordPosition="399" endWordPosition="402">emantic information) that discount unlikely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001). Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagge</context>
</contexts>
<marker>Connoly, Burger, Day, 1994</marker>
<rawString>Connoly, Dennis, John D. Burger &amp; David S. Day. 1994. A Machine learning approach to anaphoric reference. Proceedings of the International Conference on New Methods in Language Processing, 255-261, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
</authors>
<title>Locating Topics in Text Processing.</title>
<date>1999</date>
<booktitle>In Proceedings of CLIN &apos;99.</booktitle>
<marker>Miltsakaki, 1999</marker>
<rawString>Eleni Miltsakaki. 1999. Locating Topics in Text Processing. In Proceedings of CLIN &apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ferrández</author>
<author>Manuel Palomar</author>
<author>Lidia Moreno</author>
</authors>
<title>Anaphor Resolution in Unrestricted Texts with Partial Parsing.</title>
<date>1998</date>
<booktitle>Proceedings of the 18th International Conference on Computational Linguistics (COLING&apos;98)/ACL&apos;98 Conference,</booktitle>
<pages>385--391</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3138" citStr="Ferrández et al., 1998" startWordPosition="473" endWordPosition="476"> statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream. The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Section 4 we explain our Z</context>
</contexts>
<marker>Ferrández, Palomar, Moreno, 1998</marker>
<rawString>Ferrández, A., Palomar, Manuel and Moreno, Lidia. 1998. Anaphor Resolution in Unrestricted Texts with Partial Parsing. Proceedings of the 18th International Conference on Computational Linguistics (COLING&apos;98)/ACL&apos;98 Conference, pages 385-391. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>161--170</pages>
<contexts>
<context position="2679" citStr="Ge et al., 1998" startWordPosition="403" endWordPosition="406">hat discount unlikely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001). Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagger enriched with a</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Ge, Niyu, Hale, John and Charniak, Eugene. 1998. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 161 –170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics, No</journal>
<volume>3</volume>
<pages>175--204</pages>
<contexts>
<context position="4009" citStr="Grosz and Sidner, 1986" startWordPosition="613" endWordPosition="616">is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Section 4 we explain our ZA resolution method based on the centering model and the constraint rules. The method of topic identification in Chinese sentences is illustrated in Section 5. In the last section the conclusions are made. 2 Centering Model In the centering theory (Grosz and Sidner, 1986; Grosz et al, 1995; Walker et al., 1994; Strube and Hahn, 1996), the &apos;attentional state&apos; was identified as a basic component of discourse structure that consisted of two levels of focusing: global and local. For Grosz and Sidner, the centering theory provided a model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses. In the centering model, each utterance U in a discourse segment has two structures associated with it, called forwardlooking centers, Cf(U), and backward-looking center, Cb(U). The </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B. J. and Sidner, C. L.. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, No 3 Vol 12, pp. 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A Framework for Modeling the Local Coherence of Discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>203--225</pages>
<contexts>
<context position="2172" citStr="Grosz et al., 1995" startWordPosition="326" endWordPosition="329">from expressions in texts. The elimination of anaphoric expressions is termed zero anaphor (ZA) which often occurs in topic position in a Chinese sentence, due to their prominence in discourse. Accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution. There are several methods of anaphora resolution. One method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001). Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies</context>
<context position="4028" citStr="Grosz et al, 1995" startWordPosition="617" endWordPosition="620">naphors and identifies the noun phrases preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Section 4 we explain our ZA resolution method based on the centering model and the constraint rules. The method of topic identification in Chinese sentences is illustrated in Section 5. In the last section the conclusions are made. 2 Centering Model In the centering theory (Grosz and Sidner, 1986; Grosz et al, 1995; Walker et al., 1994; Strube and Hahn, 1996), the &apos;attentional state&apos; was identified as a basic component of discourse structure that consisted of two levels of focusing: global and local. For Grosz and Sidner, the centering theory provided a model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses. In the centering model, each utterance U in a discourse segment has two structures associated with it, called forwardlooking centers, Cf(U), and backward-looking center, Cb(U). The forward-looking cen</context>
<context position="5310" citStr="Grosz et al, 1995" startWordPosition="819" endWordPosition="822">e that utterance. They are not constrained by features of any previous utterance in the discourse segment (DS), and the elements of Cf(Un) are partially ordered to reflect relative prominence in Un. Grosz et al., in their paper (Grosz et al, 1995), assume that grammatical roles are the major determinant for ranking the forward-looking centers, with the order “Subject &gt; Object(s) &gt; Others”. The superlative element of Cf(Un) may become the Cb of the following utterance, Cb(Un+1). In addition to the structures for centers, Cb, and Cf, the centering model specifies a set of constraints and rules (Grosz et al, 1995; Walker et al. 1994). Constraints For each utterance Ui in a discourse segment consisting of utterances U1, ..., Um: 1. Ui has exactly one Cb. 2. Every element of Cf(Ui) must be realized in Ui. 3. Ranking of elements in Cf(Ui) guides determination of Cb(Ui+1). 4. The choice of Cb(Ui) is from Cf(Ui-1), and can not be from Cf(Ui-2) or other prior sets of Cf. Backward-looking centers, Cbs, are often omitted or pronominalized and discourses that continue centering the same entity are more coherent than those that shift from one center to another. This means that some transitions are preferred ove</context>
<context position="6710" citStr="Grosz et al, 1995" startWordPosition="1051" endWordPosition="1054">alized by a pronoun in Ui+1 then the Cb(Ui+1) must be realized by a pronoun also. II. Sequences of continuation are preferred over sequence of retaining; and sequences of retaining are to be preferred over sequences of shifting. Rule I represents one function of pronominal reference: the use of a pronoun to realize the Cb signals the hearer that the speaker is continuing to talk about the same thing. Psychological research and cross-linguistic research have validated that the Cb is preferentially realized by a pronoun in English and by equivalent forms (i.e. zero anaphora) in other languages (Grosz et al, 1995). Rule II reflect the intuition that continuation of the center and the use of retentions when possible to produce smooth transitions to a new center provide a basis for local coherence. For example in (1), the subjects of the utterance (1b) and (1d) are eliminated, and their antecedents are identified as the subjects of the preceding utterances (1a) and (1c) respectively1 according to the centering model. (1) a. Electronics stocki receive USA high-tech stock heavy-fall affect Electronics stocksi were affected by hightech stocks fallen heavily in America. b. i (Electronics stocks)i continue fa</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Grosz, B. J., Joshi, A. K. and Weinstein, S. 1995. Centering: A Framework for Modeling the Local Coherence of Discourse. Computational Linguistics, 21(2), pp. 203-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenze Hu</author>
</authors>
<title>Functional Perspectives and Chinese Word Order.</title>
<date>1995</date>
<institution>The Ohio State University.</institution>
<note>Ph. D. dissertation,</note>
<contexts>
<context position="14094" citStr="Hu, 1995" startWordPosition="2235" endWordPosition="2236">es. In the ZA identification constraints, the constraint 1 is employed to exclude the exophora3 or cataphora4 which is different from anaphora in 2 We use a Q to denote a question (ma); a ASPECT to denote aspect marker. 3 Exophora is reference of an expression directly to an extralinguistic referent and the referent does not require another expression for its interpretation. 4 Cataphora arises when a reference is made to an entity mentioned subsequently. texts. The constraint 2 includes some cases might be incorrectly detected as zero anaphors, such as passive sentences or inverted sentences (Hu, 1995). ZA identification constraints For each ZA candidate c in a discourse: 1. c can not be in the first utterance in a discourse segment 2. ZA does not occur in the following case: NP + bei + NP + VP + c NP (topic) + NP (subject) + VP + c In the antecedent identification phase, we employ the concept, ‘backward-looking center’ of centering model to identify the antecedent of each ZA. First we use noun phrase rules to obtain noun phrases in each utterance, and then the antecedent is identified as the most prominent noun phrase of the preceding utterance (Yeh and Chen, 2001): Antecedent identificati</context>
<context position="16439" citStr="Hu, 1995" startWordPosition="2638" endWordPosition="2639">mploying the ZA Triple Rules and ZA identification constraints mentioned previously, zero anaphors occur in topic or subject, and object positions can be detected. Because the ZA Triple Rules cover each possible topic or subject, and object omission cases, the result shows that the zero anaphors are over detected and the precision rate (PR) is 84% calculated using equation 1. No. of ZA correctly detected ....(1) PR of ZA detection = No. of ZA candidates The main errors of ZA detection occur in the experiment when parsing inverted sentences and non-anaphoric cases (e.g. exophora or cataphora) (Hu, 1995; Mitkov, 2002). Cataphora is similar to anaphora, the difference being the direction of the reference. In this paper, we do not deal with the case that the referent of a zero anaphor is in the following utterances, but we can detect about 60% cataphora in the test corpus by employing ZA identification constraint 1. In the phase of antecedent identification, we take the output of employing the ZA Triple Rules and ZA identification constraints, and further to identify the antecedents of zero anaphors by using antecedent identification rule based on the centering model. For example, in the disco</context>
</contexts>
<marker>Hu, 1995</marker>
<rawString>Hu, Wenze. 1995. Functional Perspectives and Chinese Word Order. Ph. D. dissertation, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Kennedy</author>
<author>Branimir Boguraev</author>
</authors>
<title>Anaphora for everyone: pronominal anaphora resolution without a parser.</title>
<date>1996</date>
<booktitle>Proceedings of the 16th International Conference on Computational Linguistics (COLING&apos;96),</booktitle>
<pages>113--118</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="3166" citStr="Kennedy and Boguraev, 1996" startWordPosition="477" endWordPosition="480">I techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream. The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Section 4 we explain our ZA resolution method based on</context>
</contexts>
<marker>Kennedy, Boguraev, 1996</marker>
<rawString>Kennedy, Christopher and Branimir Boguraev. 1996. Anaphora for everyone: pronominal anaphora resolution without a parser. Proceedings of the 16th International Conference on Computational Linguistics (COLING&apos;96), 113-118. Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H Leass</author>
</authors>
<title>An algorithm for pronominal anaphor resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="2196" citStr="Lappin and Leass, 1994" startWordPosition="330" endWordPosition="333">texts. The elimination of anaphoric expressions is termed zero anaphor (ZA) which often occurs in topic position in a Chinese sentence, due to their prominence in discourse. Accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution. There are several methods of anaphora resolution. One method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001). Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability o</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Lappin, S. and Leass, H. 1994. An algorithm for pronominal anaphor resolution. Computational Linguistics, 20(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles N Li</author>
<author>Sandra A Thompson</author>
</authors>
<title>Mandarin Chinese – A Functional Reference Grammar,</title>
<date>1981</date>
<institution>University of California Press.</institution>
<contexts>
<context position="1215" citStr="Li and Thompson, 1981" startWordPosition="178" endWordPosition="181">ently, in addition to the centering model, we further employ the constraint rules to identify the antecedents of zero anaphors. Unlike most traditional approaches to parsing sentences based on the integration of complex linguistic information and domain knowledge, we work on the output of a part-of-speech tagger and use shallow parsing instead of complex parsing to identify the topics from sentences. 1 Introduction One of the most striking characteristics in a topic-prominent language like Chinese is the important element, “topic,” in a sentence which can represent what the sentence is about (Li and Thompson, 1981). That is, if we can identify topics from Chinese sentences, we can obtain the most information embedded in the text. In this paper, we tend to identify the topic of each utterance within a discourse based on the centering model. However, in many natural languages, elements that can be easily deduced by the reader are frequently omitted from expressions in texts. The elimination of anaphoric expressions is termed zero anaphor (ZA) which often occurs in topic position in a Chinese sentence, due to their prominence in discourse. Accordingly, to accomplish the task of topic identification, we hav</context>
<context position="15327" citStr="Li and Thompson, 1981" startWordPosition="2456" endWordPosition="2459">r each zero anaphor z in a discourse segment consisting of utterances U1, ... , Um: If z occurs in Ui, and no zero anaphor occurs in Ui-1 then choose the noun phrase with the corresponding grammatical role in Ui-1 as the antecedent Else if only one zero anaphor occurs in Ui-1 then choose the antecedent of the zero anaphor in Ui-1 as the antecedent of z Else if more than one zero anaphor occurs in Ui-1 then choose the antecedent of the zero anaphor in Ui-1 as the antecedent of z according to grammatical role criteria: Topic &gt; Subject &gt; Object &gt; Others End if Due to topic-prominence in Chinese (Li and Thompson, 1981), topic is the most salient grammatical role. In general, if the topic is omitted, the subject will be in the initial position of an utterance. If the topic and subject are omitted concurrently, the ZA occurs. Since the antecedent identification rule is corresponding to the concept of centering model. 4.2 ZA Resolution Experiment In the experiment of ZA resolution, we use a test corpus which is a collection of 150 news articles contained 998 paragraphs, 4631 utterances, and 40884 Chinese words. By employing the ZA Triple Rules and ZA identification constraints mentioned previously, zero anapho</context>
</contexts>
<marker>Li, Thompson, 1981</marker>
<rawString>Li, Charles N. and Thompson, Sandra A. 1981. Mandarin Chinese – A Functional Reference Grammar, University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Exploring Evidence for Shallow Parsing.</title>
<date>2001</date>
<booktitle>Proceedings of Workshop on Computational Natural Language Learning,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="7814" citStr="Li and Roth, 2001" startWordPosition="1222" endWordPosition="1225">ronics stocksi were affected by hightech stocks fallen heavily in America. b. i (Electronics stocks)i continue fall (Electronics stocks)i continued falling down. c. j Securities stocksj also have relative respondence Securities stocksj also had respondence. d. j (Securities stocks) j continue fall by close. (Securities stocks) j fell by close one after another. 3 Shallow Parsing Shallow (or partial) parsing which is an inexpensive, fast and reliable method does not deliver full syntactic analysis but is limited to parsing smaller constituents such as noun phrases or verb phrases (Abney, 1996; Li and Roth, 2001; Mitkov, 1999). For example, the sentence (2) can be divided as follows: (2) Hualien became the popular tourist attraction. Æ [NP ] [VP ] [NP ] 1 We use a φab to denote a zero anaphor, where the subscript a is the index of the zero anaphor itself and the superscript b is the index of the referent. A single without any script represents an intrasentential zero anaphor. Also note that a superscript attached to an NP is used to represent the index of the referent. i [NP Hualien] [VP became] [NP the popular tourist attraction] Given a Chinese sentence, our method of shallow parsing is divided int</context>
</contexts>
<marker>Li, Roth, 2001</marker>
<rawString>Li, X.; Roth D. 2001. Exploring Evidence for Shallow Parsing. Proceedings of Workshop on Computational Natural Language Learning, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Kouji Tamura</author>
</authors>
<title>Zero pronoun resolution in Japanese discourse based on centering theory.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>871--876</pages>
<contexts>
<context position="2222" citStr="Okumura and Tamura, 1996" startWordPosition="334" endWordPosition="337">f anaphoric expressions is termed zero anaphor (ZA) which often occurs in topic position in a Chinese sentence, due to their prominence in discourse. Accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution. There are several methods of anaphora resolution. One method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001). Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text </context>
</contexts>
<marker>Okumura, Tamura, 1996</marker>
<rawString>Okumura, Manabu and Kouji Tamura. 1996. Zero pronoun resolution in Japanese discourse based on centering theory. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), 871-876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Robust pronoun resolution with limited knowledge.</title>
<date>1998</date>
<booktitle>Proceedings of the 18th International Conference on Computational Linguistics (COLING&apos;98)/ACL&apos;98 Conference.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="3180" citStr="Mitkov, 1998" startWordPosition="481" endWordPosition="482">e learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream. The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Section 4 we explain our ZA resolution method based on the centering</context>
</contexts>
<marker>Mitkov, 1998</marker>
<rawString>Mitkov, Ruslan. 1998. Robust pronoun resolution with limited knowledge. Proceedings of the 18th International Conference on Computational Linguistics (COLING&apos;98)/ACL&apos;98 Conference. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora resolution: the state of the art. Working paper (Based on the COLING&apos;98/ACL&apos;98 tutorial on anaphora resolution).</title>
<date>1999</date>
<institution>University of Wolverhampton,</institution>
<location>Wolverhampton.</location>
<contexts>
<context position="7829" citStr="Mitkov, 1999" startWordPosition="1226" endWordPosition="1227"> affected by hightech stocks fallen heavily in America. b. i (Electronics stocks)i continue fall (Electronics stocks)i continued falling down. c. j Securities stocksj also have relative respondence Securities stocksj also had respondence. d. j (Securities stocks) j continue fall by close. (Securities stocks) j fell by close one after another. 3 Shallow Parsing Shallow (or partial) parsing which is an inexpensive, fast and reliable method does not deliver full syntactic analysis but is limited to parsing smaller constituents such as noun phrases or verb phrases (Abney, 1996; Li and Roth, 2001; Mitkov, 1999). For example, the sentence (2) can be divided as follows: (2) Hualien became the popular tourist attraction. Æ [NP ] [VP ] [NP ] 1 We use a φab to denote a zero anaphor, where the subscript a is the index of the zero anaphor itself and the superscript b is the index of the referent. A single without any script represents an intrasentential zero anaphor. Also note that a superscript attached to an NP is used to represent the index of the referent. i [NP Hualien] [VP became] [NP the popular tourist attraction] Given a Chinese sentence, our method of shallow parsing is divided into the following</context>
</contexts>
<marker>Mitkov, 1999</marker>
<rawString>Mitkov, Ruslan. 1999. Anaphora resolution: the state of the art. Working paper (Based on the COLING&apos;98/ACL&apos;98 tutorial on anaphora resolution). University of Wolverhampton, Wolverhampton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<date>2002</date>
<publisher>Anaphora Resolution, Longman.</publisher>
<contexts>
<context position="16454" citStr="Mitkov, 2002" startWordPosition="2640" endWordPosition="2641">he ZA Triple Rules and ZA identification constraints mentioned previously, zero anaphors occur in topic or subject, and object positions can be detected. Because the ZA Triple Rules cover each possible topic or subject, and object omission cases, the result shows that the zero anaphors are over detected and the precision rate (PR) is 84% calculated using equation 1. No. of ZA correctly detected ....(1) PR of ZA detection = No. of ZA candidates The main errors of ZA detection occur in the experiment when parsing inverted sentences and non-anaphoric cases (e.g. exophora or cataphora) (Hu, 1995; Mitkov, 2002). Cataphora is similar to anaphora, the difference being the direction of the reference. In this paper, we do not deal with the case that the referent of a zero anaphor is in the following utterances, but we can detect about 60% cataphora in the test corpus by employing ZA identification constraint 1. In the phase of antecedent identification, we take the output of employing the ZA Triple Rules and ZA identification constraints, and further to identify the antecedents of zero anaphors by using antecedent identification rule based on the centering model. For example, in the discourse segment (5</context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>Mitkov, Ruslan. 2002. Anaphora Resolution, Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
</authors>
<title>Pragmatic aspects of scrambling and topicalization in German: A Centering Approach.</title>
<date>1993</date>
<booktitle>In IRCS Workshop on Centering in Discourse. Univ. of Pennsylvania,</booktitle>
<contexts>
<context position="18489" citStr="Rambow, 1993" startWordPosition="2959" endWordPosition="2960">aminations of foreign laborers. The recall rates (RR) and precision rates (PR) of ZA resolution is 70% and 60.3% respectively calculated using equation 2 and equation 3. Errors occur in the phase when a zero anaphor refers to an entity other than the corresponding grammatical role or the antecedent of the zero anaphor in the preceding utterance. No. of ant. correctly identified..(2) RR of ZA resolution = No. of ZA candidates No. of ant. correctly identified...(3) PR of ZA resolution = No. of ZA occurred in text 5 Topic Identification Topic identification is similar to theme identification in (Rambow, 1993). The theme clearly corresponds to the Cb: the theme, under a general definition, is what the current utterance is about; what utterances are about provides a link to previous discourse, since otherwise the text would be incoherent. The role of the Cb is precisely to provide such a link. In our approach, in addition to the centering model, we further employ the antecedent identification rule to identify the topic. When a ZA occurs in the utterance Ui, the antecedent of the ZA is identified as the topic of Ui. Otherwise, if the transition relation, center shifting, occurs, topic will not be ide</context>
</contexts>
<marker>Rambow, 1993</marker>
<rawString>Rambow, O. (1993). Pragmatic aspects of scrambling and topicalization in German: A Centering Approach. In IRCS Workshop on Centering in Discourse. Univ. of Pennsylvania, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhiro Seki</author>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>A Probabilistic Method for Analyzing Japanese Anaphora Integrating Zero Pronoun Detection and Resolution.</title>
<date>2002</date>
<booktitle>Proceedings of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<pages>911--917</pages>
<contexts>
<context position="2699" citStr="Seki et al., 2002" startWordPosition="407" endWordPosition="410">kely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001). Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagger enriched with annotations of gramma</context>
</contexts>
<marker>Seki, Fujii, Ishikawa, 2002</marker>
<rawString>Seki, Kazuhiro, Fujii, Atsushi, and Ishikawa, Tetsuya. 2002. A Probabilistic Method for Analyzing Japanese Anaphora Integrating Zero Pronoun Detection and Resolution. Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002), pp.911-917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>U Hahn</author>
</authors>
<title>Functional Centering.</title>
<date>1996</date>
<booktitle>Proc. Of ACL ’96,</booktitle>
<pages>270--277</pages>
<location>Santa Cruz, Ca.,</location>
<contexts>
<context position="4073" citStr="Strube and Hahn, 1996" startWordPosition="625" endWordPosition="628">preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Section 4 we explain our ZA resolution method based on the centering model and the constraint rules. The method of topic identification in Chinese sentences is illustrated in Section 5. In the last section the conclusions are made. 2 Centering Model In the centering theory (Grosz and Sidner, 1986; Grosz et al, 1995; Walker et al., 1994; Strube and Hahn, 1996), the &apos;attentional state&apos; was identified as a basic component of discourse structure that consisted of two levels of focusing: global and local. For Grosz and Sidner, the centering theory provided a model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses. In the centering model, each utterance U in a discourse segment has two structures associated with it, called forwardlooking centers, Cf(U), and backward-looking center, Cb(U). The forward-looking centers of Un, Cf(Un), depend only on the expres</context>
</contexts>
<marker>Strube, Hahn, 1996</marker>
<rawString>Strube, M. and U. Hahn. 1996. Functional Centering. Proc. Of ACL ’96, Santa Cruz, Ca., pp.270-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Stuckardt</author>
</authors>
<title>Machine-Learning-Based vs. Manually Designed Approaches to Anaphor Resolution: the Best of Two Worlds.</title>
<date>2002</date>
<booktitle>In Proceedings of the 4th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC2002),</booktitle>
<pages>211--216</pages>
<institution>University of Lisbon,</institution>
<contexts>
<context position="2908" citStr="Stuckardt, 2002" startWordPosition="439" endWordPosition="440">n anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream. The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents. In the following s</context>
</contexts>
<marker>Stuckardt, 2002</marker>
<rawString>Stuckardt, Roland. 2002. Machine-Learning-Based vs. Manually Designed Approaches to Anaphor Resolution: the Best of Two Worlds. In Proceedings of the 4th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC2002), University of Lisbon, Portugal, pages 211-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Givón</author>
</authors>
<title>Topic continuity in discourse: An introduction. Topic continuity in discourse. Amsterdam/Philadelphia [Pennsylvania]: John Benjamins.</title>
<date>1983</date>
<marker>Givón, 1983</marker>
<rawString>Givón, T. 1983. Topic continuity in discourse: An introduction. Topic continuity in discourse. Amsterdam/Philadelphia [Pennsylvania]: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>M Iida</author>
<author>S Cote</author>
</authors>
<date>1994</date>
<booktitle>Japan Discourse and the Process of Centering. Computational Linguistics,</booktitle>
<volume>20</volume>
<issue>2</issue>
<pages>193--233</pages>
<contexts>
<context position="4049" citStr="Walker et al., 1994" startWordPosition="621" endWordPosition="624">ies the noun phrases preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Section 4 we explain our ZA resolution method based on the centering model and the constraint rules. The method of topic identification in Chinese sentences is illustrated in Section 5. In the last section the conclusions are made. 2 Centering Model In the centering theory (Grosz and Sidner, 1986; Grosz et al, 1995; Walker et al., 1994; Strube and Hahn, 1996), the &apos;attentional state&apos; was identified as a basic component of discourse structure that consisted of two levels of focusing: global and local. For Grosz and Sidner, the centering theory provided a model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses. In the centering model, each utterance U in a discourse segment has two structures associated with it, called forwardlooking centers, Cf(U), and backward-looking center, Cb(U). The forward-looking centers of Un, Cf(Un), d</context>
<context position="5331" citStr="Walker et al. 1994" startWordPosition="823" endWordPosition="826">hey are not constrained by features of any previous utterance in the discourse segment (DS), and the elements of Cf(Un) are partially ordered to reflect relative prominence in Un. Grosz et al., in their paper (Grosz et al, 1995), assume that grammatical roles are the major determinant for ranking the forward-looking centers, with the order “Subject &gt; Object(s) &gt; Others”. The superlative element of Cf(Un) may become the Cb of the following utterance, Cb(Un+1). In addition to the structures for centers, Cb, and Cf, the centering model specifies a set of constraints and rules (Grosz et al, 1995; Walker et al. 1994). Constraints For each utterance Ui in a discourse segment consisting of utterances U1, ..., Um: 1. Ui has exactly one Cb. 2. Every element of Cf(Ui) must be realized in Ui. 3. Ranking of elements in Cf(Ui) guides determination of Cb(Ui+1). 4. The choice of Cb(Ui) is from Cf(Ui-1), and can not be from Cf(Ui-2) or other prior sets of Cf. Backward-looking centers, Cbs, are often omitted or pronominalized and discourses that continue centering the same entity are more coherent than those that shift from one center to another. This means that some transitions are preferred over others. These obser</context>
</contexts>
<marker>Walker, Iida, Cote, 1994</marker>
<rawString>Walker, M. A., M. Iida and S. Cote. 1994. Japan Discourse and the Process of Centering. Computational Linguistics, 20(2): 193-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
</authors>
<title>Centering, anaphora resolution, and discourse structure.</title>
<date>1998</date>
<booktitle>Centering in Discourse.</booktitle>
<editor>In Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince, editors,</editor>
<publisher>Oxford University Press.</publisher>
<marker>Walker, 1998</marker>
<rawString>Walker, M. A. 1998. Centering, anaphora resolution, and discourse structure. In Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince, editors, Centering in Discourse. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Long Yeh</author>
<author>Yi-Chun Chen</author>
</authors>
<title>An empirical study of zero anaphora resolution in Chinese based on centering theory.</title>
<date>2001</date>
<booktitle>In Proceedings of ROCLING XIV,</booktitle>
<location>Tainan, Taiwan.</location>
<contexts>
<context position="2264" citStr="Yeh and Chen, 2001" startWordPosition="342" endWordPosition="345">(ZA) which often occurs in topic position in a Chinese sentence, due to their prominence in discourse. Accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution. There are several methods of anaphora resolution. One method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001). Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge. However, it is very labor-intensive and time-consuming to construct a domain knowledge base. Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, wi</context>
<context position="14669" citStr="Yeh and Chen, 2001" startWordPosition="2338" endWordPosition="2341">e sentences or inverted sentences (Hu, 1995). ZA identification constraints For each ZA candidate c in a discourse: 1. c can not be in the first utterance in a discourse segment 2. ZA does not occur in the following case: NP + bei + NP + VP + c NP (topic) + NP (subject) + VP + c In the antecedent identification phase, we employ the concept, ‘backward-looking center’ of centering model to identify the antecedent of each ZA. First we use noun phrase rules to obtain noun phrases in each utterance, and then the antecedent is identified as the most prominent noun phrase of the preceding utterance (Yeh and Chen, 2001): Antecedent identification rule: For each zero anaphor z in a discourse segment consisting of utterances U1, ... , Um: If z occurs in Ui, and no zero anaphor occurs in Ui-1 then choose the noun phrase with the corresponding grammatical role in Ui-1 as the antecedent Else if only one zero anaphor occurs in Ui-1 then choose the antecedent of the zero anaphor in Ui-1 as the antecedent of z Else if more than one zero anaphor occurs in Ui-1 then choose the antecedent of the zero anaphor in Ui-1 as the antecedent of z according to grammatical role criteria: Topic &gt; Subject &gt; Object &gt; Others End if </context>
</contexts>
<marker>Yeh, Chen, 2001</marker>
<rawString>Yeh, Ching-Long and Chen, Yi-Chun. 2001. An empirical study of zero anaphora resolution in Chinese based on centering theory. In Proceedings of ROCLING XIV, Tainan, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Long Yeh</author>
<author>Yi-Chun Chen</author>
</authors>
<title>Zero anaphora resolution in Chinese with partial parsing based on centering theory.</title>
<date>2003</date>
<booktitle>In Proceedings of IEEE NLP-KE03,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="3201" citStr="Yeh and Chen, 2003" startWordPosition="483" endWordPosition="486"> compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002). This method can sort out the above problems. However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002). Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003). The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream. The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents. In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence. In Section 3 we describe the details of shallow parsing we employed. In Section 4 we explain our ZA resolution method based on the centering model and the constr</context>
</contexts>
<marker>Yeh, Chen, 2003</marker>
<rawString>Yeh, Ching-Long and Chen, Yi-Chun. 2003. Zero anaphora resolution in Chinese with partial parsing based on centering theory. In Proceedings of IEEE NLP-KE03, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>