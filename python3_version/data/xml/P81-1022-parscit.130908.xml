<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.117265">
<note confidence="0.51445">
PARSING
</note>
<bodyText confidence="0.983426458015267">
Ralph Grishman
Dept. of Computer Science
New York University
New York, N. Y..
Onereason for the wide variety of views on many subjects
in computational linguistics (such as parsing) is the
diversity of objectives which lead people to do research
in this area. Some researchers are motivated primarily
by potential applications - the development of natural
language interfaces for computer systems. Others are
primarily concerned with the psychological processes
which underlie human language, and view the computer as
a tool for modeling and thus improving our understanding
of these processes. Since, as is often observed, man is
our best example of a natural language processor, these
two groups do have a strong commonality of research
interest. Nonetheless, their divergence of objective
must lead to differences in the way they regard the
component processes of natural language understanding.
(If - when human processing is better understood - it is
recognized that the simulation of human processes is not
the most effective way of constructing a natural language
interface, there may even be a deliberate divergence in
the processes themselves.) My work, and this position
paper, reflect an applications orientation; those with
different research objectives will come to quite
different conclusions.
WHY PARSE?
One of the tasks of computer science in general, and of
artificial intelligence in particular, is that of coping
in a systematic fashion with systems of high complexity.
Natural language interfaces certainly fit that
characterization.
A natural language interface must analyze input sequences
communicate with some underlying system (data base, robot,
etc.), and generate responses. In the transition from
the natural language input to the language of the under-
lying system there is in principle no need to make
explicit reference to any intermediate structures; we
could write our interface as a (huge) set of rules which
map directly from input sequences into our target
language. We know full well, however, that such a system
would be nearly impossible to write, and certainly
impossible to understand or modify. By introducing
intermediate structures, we are able to divide the task
into more manageable components.
Specific intermediate structures are of value insofar as
they facilitate the expression of relationships which
must be captured in the system - relationships which
would be more cumbersome to express using other repre-
sentations. For example, the representations at the
level of logical form (such as predicate calculus) are
chosen to facilitate the computation of logical inferences.
In the same way, a representation of constituent
structure (a parse tree), if properly chosen, will
facilitate the statement of many linguistic constraints
and relationships. Grammatical constraints will enable
the system to identify the pertinent syntactic category
for many multiply classified words. Some constraints
on anaphora (such as the notion of command) and on
quantifier structure are also best stated in terms of
surface structure.
Equally important, many sentence relationships which
must be captured at some point in the analysis (such as
the relation between active and passive sentences or
between reduced and expanded conjoinings) are most easily
stated as transformations between constituent structures.
By using syntactic transformations to regularize the
constituent structure, we can substantially simplify the
Specification of the subsequent stages of analysis.
SPECIFICATION VS. PROCEDURE
The arguments just given for parse trees (and other
intermediate structures) are arguments for how best to
specify the transformations which a natural language
input must undergo. They are not arguments for a
particular language analysis procedure. A direct imple-
mentation of the simplest specifications does not
necessarily yield the most efficient procedure; as our
systems become more sophisticated, the distance from
specification to implementation structure may increase.
We should therefore favor formalisms which (because of
their simple structure) can be automatically adapted to
a variety of procedures. Among these variations are:
PARALLEL PROCESSING. Phrase structure grammars and
augmented phrase structure grammars lend themselves
naturally to parallel parsing procedures - either top-
down (following alternative expansions in parallel),
bottom-up (trying alternative reductions in parallel),
or a combination of the two. In particular, some of the
parsing algorithms developed as part of the speech
recognition research of the past decade are readily
adaptable to parallel processing. TO minimize parallel-
ism, however, the grammatical constraints must be
organized to minimize or at least postpone the inter-
actions among the analyses of the various parts of a
sentence.
ANALYSIS AND GENERATION. In the sane way that sentence
analysis involves a translation to a &amp;quot;deep structure,&amp;quot;
an increasing number of systems now include a generation
component to translate from deep structure to sentences.
If the mapping from sentence to deep structure is direct
(without reference to a parse tree), the generation
component may require a separate design effort. On the
other hand, if the mapping is specified in terms of
incremental transformations of the constituent structure,
producing an inverse mapping may be relatively straight-
forward (and the greater the non-procedural content of
the transformations, the easier it should be to reverse
them).
AVOIDING THE PARSE TREE. To emphasize the distinction
between specification and procedure, let me mention a
possibility for an &amp;quot;optimizing&amp;quot; analyser of the future:
one whose specifications are given in terms of trans-
formations of the constituent Structure followed by
interpretation of the regularized (&amp;quot;deep&amp;quot;) structure,
but whose implementation avoids actually constructing
a parse tree. Instead, the transformations would be
applied to the deep structure interpretation rules,
producing a (much larger) set of rules for interpreting
the input sequences directly. Some small experiments
have been done in this direction (K. Konolige, &amp;quot;Capturing
Linguistic Generalizations with Grammar Metarules,&amp;quot;Proc.
18th Ann&apos;l Meeting ACL, 1979 ). By avoiding explicit
construction of a parse tree, we could accelerate the
analysis procedure while retaining the descriptive
advantages of independent, incremental transformations of
constituent structure. While development of any such
automatic grammar restructuring procedure would certainly
be a difficult task, it does indicate the possibilities
which open up when specification and implementation are
separated.
</bodyText>
<page confidence="0.998244">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.339090">
<title confidence="0.998567">PARSING</title>
<author confidence="0.99997">Ralph Grishman</author>
<affiliation confidence="0.988791">Dept. of Computer Science New York University</affiliation>
<address confidence="0.5622">New York, N. Y..</address>
<abstract confidence="0.998033952755905">Onereason for the wide variety of views on many subjects in computational linguistics (such as parsing) is the diversity of objectives which lead people to do research in this area. Some researchers are motivated primarily by potential applications the development of natural language interfaces for computer systems. Others are primarily concerned with the psychological processes which underlie human language, and view the computer as a tool for modeling and thus improving our understanding of these processes. Since, as is often observed, man is our best example of a natural language processor, these two groups do have a strong commonality of research interest. Nonetheless, their divergence of objective must lead to differences in the way they regard the component processes of natural language understanding. (If when human processing is better understood it is recognized that the simulation of human processes is not the most effective way of constructing a natural language interface, there may even be a deliberate divergence in the processes themselves.) My work, and this position paper, reflect an applications orientation; those with different research objectives will come to quite different conclusions. WHY PARSE? One of the tasks of computer science in general, and of artificial intelligence in particular, is that of coping in a systematic fashion with systems of high complexity. Natural language interfaces certainly fit that characterization. natural language interface must analyze input some underlying system (data base, robot, etc.), and generate responses. In the transition from the natural language input to the language of the underlying system there is in principle no need to make explicit reference to any intermediate structures; we could write our interface as a (huge) set of rules which map directly from input sequences into our target language. We know full well, however, that such a system would be nearly impossible to write, and certainly to understand or modify. intermediate structures, we are able to divide the task into more manageable components. Specific intermediate structures are of value insofar as they facilitate the expression of relationships which must be captured in the system relationships which would be more cumbersome to express using other representations. For example, the representations at the form (such as predicate calculus) are chosen to facilitate the computation of logical inferences. In the same way, a representation of constituent structure (a parse tree), if properly chosen, will facilitate the statement of many linguistic constraints and relationships. Grammatical constraints will enable the system to identify the pertinent syntactic category for many multiply classified words. Some constraints on anaphora (such as the notion of command) and on quantifier structure are also best stated in terms of surface structure. Equally important, many sentence relationships which must be captured at some point in the analysis (such as the relation between active and passive sentences or between reduced and expanded conjoinings) are most easily stated as transformations between constituent structures. By using syntactic transformations to regularize the constituent structure, we can substantially simplify the Specification of the subsequent stages of analysis. SPECIFICATION VS. PROCEDURE The arguments just given for parse trees (and other intermediate structures) are arguments for how best to specifythe transformations which a natural language input must undergo. They are not arguments for a language analysis procedure.A direct implementation of the simplest specifications does not necessarily yield the most efficient procedure; as our systems become more sophisticated, the distance from specification to implementation structure may increase. We should therefore favor formalisms which (because of their simple structure) can be automatically adapted to a variety of procedures. Among these variations are: PARALLEL PROCESSING. Phrase structure grammars and augmented phrase structure grammars lend themselves naturally to parallel parsing procedures either top- (following alternative expansions bottom-up (trying alternative reductions in parallel), or a combination of the two. In particular, some of the parsing algorithms developed as part of the speech recognition research of the past decade are readily adaptable to parallel processing. TO minimize parallelism, however, the grammatical constraints must be organized to minimize or at least postpone the interactions among the analyses of the various parts of a sentence. ANALYSIS AND GENERATION. In the sane way that sentence analysis involves a translation to a &amp;quot;deep structure,&amp;quot; an increasing number of systems now include a generation component to translate from deep structure to sentences. from sentence to deep structure is direct reference to a parse tree), generation may require a design effort. On the other hand, if the mapping is specified in terms of incremental transformations of the constituent structure, producing an inverse mapping may be relatively straightforward (and the greater the non-procedural content of the transformations, the easier it should be to reverse them). AVOIDING THE PARSE TREE. To emphasize the distinction between specification and procedure, let me mention a possibility for an &amp;quot;optimizing&amp;quot; analyser of the future: one whose specifications are given in terms of transconstituent Structure followed by interpretation of the regularized (&amp;quot;deep&amp;quot;) structure, but whose implementation avoids actually constructing a parse tree. Instead, the transformations would be applied to the deep structure interpretation rules, (much larger) set of rules for interpreting the input sequences directly. Some small experiments have been done in this direction (K. Konolige, &amp;quot;Capturing Linguistic Generalizations with Grammar Metarules,&amp;quot;Proc. Ann&apos;l MeetingACL, 1979 ). By avoiding explicit construction of a parse tree, we could accelerate the analysis procedure while retaining the descriptive of independent, of constituent structure. While development of any such automatic grammar restructuring procedure would certainly be a difficult task, it does indicate the possibilities which open up when specification and implementation are separated.</abstract>
<intro confidence="0.885968">101</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>