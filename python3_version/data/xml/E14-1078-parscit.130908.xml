<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000458">
<title confidence="0.989301">
Learning part-of-speech taggers with inter-annotator agreement loss
</title>
<author confidence="0.998299">
Barbara Plank, Dirk Hovy, Anders Søgaard
</author>
<affiliation confidence="0.998454">
Center for Language Technology
University of Copenhagen, Denmark
</affiliation>
<address confidence="0.797133">
Njalsgade 140, DK-2300 Copenhagen S
</address>
<email confidence="0.976852">
bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk
</email>
<bodyText confidence="0.995627236842105">
for languages where available resources are mu-
tually inconsistent (Johansson, 2013). Unfortu-
nately, there is no grand unifying linguistic the-
ory of how to analyze the structure of sentences.
While linguists agree on certain things, there is
still a wide range of unresolved questions. Con-
sider the following sentence:
(1) @GaryMurphyDCU of @DemMattersIRL
will take part in a panel discussion on Octo-
ber 10th re the aftermath of #seanref ...
While linguists will agree that in is a preposi-
tion, and panel discussion a compound noun, they
are likely to disagree whether will is heading the
main verb take or vice versa. Even at a more basic
level of analysis, it is not completely clear how to
assign POS tags to each word in this sentence: is
part a particle or a noun; is 10th a numeral or a
noun?
Some linguistic controversies may be resolved
by changing the vocabulary of linguistic theory,
e.g., by leaving out numerals or introducing ad
hoc parts of speech, e.g. for English to (Marcus
et al., 1993) or words ending in -ing (Manning,
2011). However, standardized label sets have
practical advantages in NLP (Zeman and Resnik,
2008; Zeman, 2010; Das and Petrov, 2011; Petrov
et al., 2012; McDonald et al., 2013).
For these and other reasons, our annotators
(even when they are trained linguists) often dis-
agree on how to analyze sentences. The strategy
in most previous work in NLP has been to monitor
and later resolve disagreements, so that the final
labels are assumed to be reliable when used as in-
put to machine learning models.
Our approach
Instead of glossing over those annotation disagree-
ments, we consider what happens if we embrace
the uncertainty exhibited by human annotators
</bodyText>
<sectionHeader confidence="0.648234" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930571428571">
In natural language processing (NLP) an-
notation projects, we use inter-annotator
agreement measures and annotation guide-
lines to ensure consistent annotations.
However, annotation guidelines often
make linguistically debatable and even
somewhat arbitrary decisions, and inter-
annotator agreement is often less than
perfect. While annotation projects usu-
ally specify how to deal with linguisti-
cally debatable phenomena, annotator dis-
agreements typically still stem from these
“hard” cases. This indicates that some er-
rors are more debatable than others. In this
paper, we use small samples of doubly-
annotated part-of-speech (POS) data for
Twitter to estimate annotation reliability
and show how those metrics of likely inter-
annotator agreement can be implemented
in the loss functions of POS taggers. We
find that these cost-sensitive algorithms
perform better across annotation projects
and, more surprisingly, even on data an-
notated according to the same guidelines.
Finally, we show that POS tagging mod-
els sensitive to inter-annotator agreement
perform better on the downstream task of
chunking.
</bodyText>
<sectionHeader confidence="0.982258" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999588555555556">
POS-annotated corpora and treebanks are collec-
tions of sentences analyzed by linguists accord-
ing to some linguistic theory. The specific choice
of linguistic theory has dramatic effects on down-
stream performance in NLP tasks that rely on syn-
tactic features (Elming et al., 2013). Variation
across annotated corpora in linguistic theory also
poses challenges to intrinsic evaluation (Schwartz
et al., 2011; Tsarfaty et al., 2012), as well as
</bodyText>
<page confidence="0.945105">
742
</page>
<note confidence="0.993049">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999928676470588">
when learning predictive models from the anno-
tated data.
To achieve this, we incorporate the uncertainty
exhibited by annotators in the training of our
model. We measure inter-annotator agreement on
small samples of data, then incorporate this in the
loss function of a structured learner to reflect the
confidence we can put in the annotations. This
provides us with cost-sensitive online learning al-
gorithms for inducing models from annotated data
that take inter-annotator agreement into consider-
ation.
Specifically, we use online structured percep-
tron with drop-out, which has previously been ap-
plied to POS tagging and is known to be robust
across samples and domains (Søgaard, 2013a). We
incorporate the inter-annotator agreement in the
loss function either as inter-annotator F1-scores
or as the confusion probability between annota-
tors (see Section 3 below for a more detailed de-
scription). We use a small amounts of doubly-
annotated Twitter data to estimate F1-scores and
confusion probabilities, and incorporate them dur-
ing training via a modified loss function. Specif-
ically, we use POS annotations made by two an-
notators on a set of 500 newly sampled tweets
to estimate our agreement scores, and train mod-
els on existing Twitter data sets (described be-
low). We evaluate the effect of our modified
training by measuring intrinsic as well as down-
stream performance of the resulting models on two
tasks, namely named entity recognition (NER) and
chunking, which both use POS tags as input fea-
tures.
</bodyText>
<sectionHeader confidence="0.824543" genericHeader="introduction">
2 POS-annotated Twitter data sets
</sectionHeader>
<bodyText confidence="0.999891893939394">
The vast majority of POS-annotated resources
across languages contain mostly newswire text.
Some annotated Twitter data sets do exist for En-
glish. Ritter et al. (2011) present a manually an-
notated data set of 16 thousand tokens. They
do not report inter-annotator agreement. Gimpel
et al. (2011) annotated about 26 thousand tokens
and report a raw agreement of 92%. Foster et
al. (2011) annotated smaller portions of data for
cross-domain evaluation purposes. We refer to the
data as RITTER, GIMPEL and FOSTER below.
In our experiments, we use the RITTER splits
provided by Derczynski et al. (2013), and the
October splits of the GIMPEL data set, version
0.3. We train our models on the concatenation of
RITTER-TRAIN and GIMPEL-TRAIN and evaluate
them on the remaining data, the dev and test set
provided by Foster et al. (2011) as well as an in-
house annotated data set of 3k tokens (see below).
The three annotation efforts (Ritter et al., 2011;
Gimpel et al., 2011; Foster et al., 2011) all used
different tagsets, however, and they also differ in
tokenization, as well as a wide range of linguistic
decisions. We mapped all the three corpora to the
universal tagset provided by Petrov et al. (2012)
and used the same dummy symbols for numbers,
URLs, etc., in all the data sets. Following (Fos-
ter et al., 2011), we consider URLs, usernames
and hashtags as NOUN. We did not change the tok-
enization.
The data sets differ in how they analyze many of
the linguistically hard cases. Consider, for exam-
ple, the analysis of will you come out to in GIM-
PEL and RITTER (Figure 1, top). While Gimpel
et al. (2011) tag out and to as adpositions, Ritter
et al. (2011) consider them particles. What is the
right analysis depends on the compositionality of
the construction and the linguistic theory one sub-
scribes to.
Other differences include the analysis of abbre-
viations (PRT in GIMPEL; X in RITTER and FOS-
TER), colon (X in GIMPEL; punctuation in RIT-
TER and FOSTER), and emoticons, which can take
multiple parts of speech in GIMPEL, but are al-
ways X in RITTER, while they are absent in FOS-
TER. GIMPEL-TRAIN and RITTER-TRAIN are
also internally inconsistent. See the bottom of Fig-
ure 1 for examples and Hovy et al. (2014) for a
more detailed discussion on differences between
the data sets.
Since the mapping to universal tags could
potentially introduce errors, we also annotated
a data set directly using universal tags. We
randomly selected 200 tweets collected over the
span of one day, and had three annotators tag
this set. We split the data in such a way that
each annotator had 100 tweets: two annotators
had disjoint sets, the third overlapped 50 items
with each of the two others. In this way, we
obtained an initial set of 100 doubly-annotated
tweets. The annotators were not provided with
annotation guidelines. After the first round of
annotations, we achieved a raw agreement of
0.9, a Cohen’s n of 0.87, and a Krippendorff’s
α of 0.87. We did one pass over the data to
adjudicate the cases where annotators disagreed,
</bodyText>
<page confidence="0.973858">
743
</page>
<equation confidence="0.735268888888889">
will you come out to the
GIMPEL ... VERB PRON VERB ADP ADP DET . . .
RITTER VERB PRON VERB PRT PRT DET
RITTER
you/PRON come/VERB out/PRT to/PRT
. . . it/PRON comes/VERB out/ADP nov/NOUN . . .
GIMPEL
Advances/NOUN and/CONJ Social/NOUN Media/NOUN .../X
. . . Journalists/NOUN and/CONJ Social/ADJ Media/NOUN experts/NOUN . . .
</equation>
<figureCaption confidence="0.999118">
Figure 1: Annotation differences between (top) and within (bottom) two available Twitter POS data sets.
</figureCaption>
<bodyText confidence="0.999204222222222">
or where they had flagged their choice as debat-
able. The final data set (lowlands.test),
referred below to as INHOUSE, contained 3,064
tokens (200 tweets) and is publicly available
at http://bitbucket.org/lowlands/
costsensitive-data/, along with the data
used to compute inter-annotator agreement scores
for learning cost-sensitive taggers, described in
the next section.
</bodyText>
<sectionHeader confidence="0.895697" genericHeader="method">
3 Computing agreement scores
</sectionHeader>
<bodyText confidence="0.999935291666667">
Gimpel et al. (2011) used 72 doubly-annotated
tweets to estimate inter-annotator agreement, and
we also use doubly-annotated data to compute
agreement scores. We randomly sampled 500
tweets for this purpose. Each tweet was anno-
tated by two annotators, again using the univer-
sal tag set (Petrov et al., 2012). All annotators
were encouraged to use their own best judgment
rather than following guidelines or discussing dif-
ficult cases with each other. This is in contrast to
Gimpel et al. (2011), who used annotation guide-
lines. The average inter-annotator agreement was
0.88 for raw agreement, and 0.84 for Cohen’s n.
Gimpel et al. (2011) report a raw agreement of
0.92.
We use two metrics to provide a more detailed
picture of inter-annotator agreement, namely
F1-scores between annotators on individual parts
of speech, and tag confusion probabilities, which
we derive from confusion matrices.
The F1-score relates to precision and recall
in the usual way, i.e, as the harmonic mean
between those two measure. In more detail, given
two annotators A1 and A2, we say the precision
</bodyText>
<figureCaption confidence="0.667052">
Figure 2: Inter-annotator F1-scores estimated
from 500 tweets.
</figureCaption>
<bodyText confidence="0.943835928571429">
of A1 relative to A2 with respect to POS tag T in
some data set X, denoted PrecT (A1(X), A2(X)),
is the number of tokens both A1 and A2 predict to
be T over the number of times A1 predicts a token
to be T. Similarly, we define the recall with re-
spect to some tag T, i.e., RecT (A1(X), A2(X)),
as the number of tokens both A1 and A2 predict
to be T over the number of times A2 predicts
a token to be T. The only difference with
respect to standard precision and recall is that
the gold standard is replaced by a second anno-
tator, A2. Note that PrecT (A1(X), A2(X)) =
RecT (A2(X), A1(X)). It follows from all of
the above that the F1-score is symmetrical, i.e.,
</bodyText>
<equation confidence="0.555027">
F 1T (A1(X), A2(X)) = F1T(A2(X),A1(X)).
</equation>
<bodyText confidence="0.540913">
The inter-annotator F1-scores over the 12
POS tags in the universal tagset are presented in
Figure 2. It shows that there is a high agreement
for nouns, verbs and punctuation, while the agree-
</bodyText>
<page confidence="0.989116">
744
</page>
<figureCaption confidence="0.994295">
Figure 3: Confusion matrix of POS tags obtained
from 500 doubly-annotated tweets.
</figureCaption>
<bodyText confidence="0.982598882352941">
ment is low, for instance, for particles, numerals
and the X tag.
We compute tag confusion probabilities
from a confusion matrix over POS tags like
the one in Figure 3. From such a matrix,
we compute the probability of confusing
two tags t1 and t2 for some data point x,
i.e. P({A1(x),A2(x)} = {t1,t2}) as the
mean of P(A1(x) = t1,A2(x) = t2) and
P(A1(x) = t2, A2(x) = t1), e.g., the confusion
probability of two tags is the mean of the prob-
ability that annotator A1 assigns one tag and A2
another, and vice versa.
We experiment with both agreement scores (F1
and confusion matrix probabilities) to augment the
loss function in our learner. The next section de-
scribes this modification in detail.
</bodyText>
<sectionHeader confidence="0.987451" genericHeader="method">
4 Inter-annotator agreement loss
</sectionHeader>
<bodyText confidence="0.970259">
We briefly introduce the cost-sensitive perceptron
classifier. Consider the weighted perceptron loss
on our ith example (xi, yi) (with learning rate α =
1), Lw((xi, yi)):
-y(sign(w · xi), yi) max(0, −yiw · xi)
In a non-cost-sensitive classifier, the weight
function -y(yj, yi) = 1 for 1 &lt; i &lt; N. The
</bodyText>
<listItem confidence="0.976915545454546">
1: X = {(xi,yi)}Ni=1 with xi = (x1i,...,xmi )
2: I iterations
3: w = (0)m
4: for iter E I do
5: for 1 &lt; i &lt; N do
6: yˆ = argmaxyEY w · Φ(xi, y)
7: w &lt;-- w + -y(ˆy, yi)[Φ(xi, yi) − Φ(xi, ˆy)]
8: w*+ = w
9: end for
10: end for
11: return w*/=(NxI)
</listItem>
<figureCaption confidence="0.927863">
Figure 4: Cost-sensitive structured perceptron (see
</figureCaption>
<bodyText confidence="0.9384812">
Section 3 for weight functions -y).
two cost-sensitive systems proposed only differ in
how we formulate -y(·, ·). In one model, the loss is
weighted by the inter-annotator F 1 of the gold tag
in question. This boils down to
</bodyText>
<equation confidence="0.981592">
-y(yj,yi) = F1yi(A1(X),A2(X))
</equation>
<bodyText confidence="0.998080260869565">
where X is the small sample of held-out data used
to estimate inter-annotator agreement. Note that
in this formulation, the predicted label is not taken
into consideration.
The second model is slightly more expressive
and takes both the gold and predicted tags into ac-
count. It basically weights the loss by how likely
the gold and predicted tag are to be mistaken for
each other, i.e., (the inverse of) their confusion
probability:
-y(yj, yi)) = 1− P({A1(X), A2(X)} = {yj, yi})
In both loss functions, a lower gamma value
means that the tags are more likely to be confused
by a pair of annotators. In this case, the update is
smaller. In contrast, the learner incurs greater loss
when easy tags are confused.
It is straight-forward to extend these cost-
sensitive loss functions to the structured percep-
tron (Collins, 2002). In Figure 4, we provide the
pseudocode for the cost-sensitive structured online
learning algorithm. We refer to the cost-sensitive
structured learners as F1- and CM-weighted be-
low.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998664">
In our main experiments, we use structured per-
ceptron (Collins, 2002) with random corruptions
</bodyText>
<page confidence="0.992744">
745
</page>
<bodyText confidence="0.998733333333333">
using a drop-out rate of 0.1 for regularization, fol-
lowing Søgaard (2013a). We use the LXMLS
toolkit implementation1 with default parameters.
We present learning curves across iterations, and
only set parameters using held-out data for our
downstream experiments.2
</bodyText>
<sectionHeader confidence="0.684201" genericHeader="method">
5.1 Results
</sectionHeader>
<bodyText confidence="0.999991">
Our results are presented in Figure 5. The top left
graph plots accuracy on the training data per iter-
ation. We see that CM-weighting does not hurt
training data accuracy. The reason may be that
the cost-sensitive learner does not try (as hard) to
optimize performance on inconsistent annotations.
The next two plots (upper mid and upper right)
show accuracy over epochs on in-sample evalua-
tion data, i.e., GIMPEL-DEV and RITTER-TEST.
Again, the CM-weighted learner performs better
than our baseline model, while the F1-weighted
learner performs much worse.
The interesting results are the evaluations on
out-of-sample evaluation data sets (FOSTER and
IN-HOUSE) - lower part of Figure 5. Here, both
our learners are competitive, but overall it is clear
that the CM-weighted learner performs best. It
consistently improves over the baseline and F1-
weighting. The former is much more expressive
as it takes confusion probabilities into account and
does not only update based on gold-label uncer-
tainty, as is the case with the F1-weighted learner.
</bodyText>
<subsectionHeader confidence="0.998269">
5.2 Robustness across regularizers
</subsectionHeader>
<bodyText confidence="0.999765625">
Discriminative learning typically benefits from
regularization to prevent overfitting. The simplest
is the averaged perceptron, but various other meth-
ods have been suggested in the literature.
We use structured perceptron with drop-out, but
results are relatively robust across other regular-
ization methods. Drop-out works by randomly
dropping a fraction of the active features in each
iteration, thus preventing overfitting. Table 1
shows the results for using different regularizers,
in particular, Zipfian corruptions (Søgaard, 2013b)
and averaging. While there are minor differences
across data sets and regularizers, we observe that
the corresponding cell using the loss function sug-
gested in this paper (CM) always performs better
than the baseline method.
</bodyText>
<footnote confidence="0.99671425">
1https://github.com/gracaninja/
lxmls-toolkit/
2In this case, we use FOSTER-DEV as our development
data to avoid in-sample bias.
</footnote>
<sectionHeader confidence="0.889707" genericHeader="method">
6 Downstream evaluation
</sectionHeader>
<bodyText confidence="0.99994065">
We have seen that our POS tagging model im-
proves over the baseline model on three out-of-
sample test sets. The question remains whether
training a POS tagger that takes inter-annotator
agreement scores into consideration is also effec-
tive on downstream tasks. Therefore, we eval-
uate our best model, the CM-weighted learner,
in two downstream tasks: shallow parsing—also
known as chunking—and named entity recogni-
tion (NER).
For the downstream evaluation, we used the
baseline and CM models trained over 13 epochs,
as they performed best on FOSTER-DEV (cf. Fig-
ure 5). Thus, parameters were optimized only on
POS tagging data, not on the downstream evalu-
ation tasks. We use a publicly available imple-
mentation of conditional random fields (Lafferty
et al., 2001)3 for the chunking and NER exper-
iments, and provide the POS tags from our CM
learner as features.
</bodyText>
<subsectionHeader confidence="0.99527">
6.1 Chunking
</subsectionHeader>
<bodyText confidence="0.99995312">
The set of features for chunking include informa-
tion from tokens and POS tags, following Sha and
Pereira (2003).
We train the chunker on Twitter data (Ritter et
al., 2011), more specifically, the 70/30 train/test
split provided by Derczynski et al. (2013) for POS
tagging, as the original authors performed cross
validation. We train on the 70% Twitter data (11k
tokens) and evaluate on the remaining 30%, as
well as on the test data from Foster et al. (2011).
The FOSTER data was originally annotated for
POS and constituency tree information. We con-
verted it to chunks using publicly available conver-
sion software.4 Part-of-speech tags are the ones
assigned by our cost-sensitive (CM) POS model
trained on Twitter data, the concatenation of Gim-
pel and 70% Ritter training data. We did not in-
clude the CoNLL 2000 training data (newswire
text), since adding it did not substantially improve
chunking performance on tweets, as also shown
in (Ritter et al., 2011).
The results for chunking are given in Ta-
ble 2. They show that using the POS tagging
model (CM) trained to be more sensitive to inter-
annotator agreement improves performance over
</bodyText>
<footnote confidence="0.991199666666667">
3http://crfpp.googlecode.com
4http://ilk.uvt.nl/team/sabine/
homepage/software.html
</footnote>
<page confidence="0.996166">
746
</page>
<figure confidence="0.999621164705882">
80.5
80.0
79.5
Accuracy (%)
79.0
78.5
78.0
77.5
85.0
84.5
84.0
Accuracy (%)
83.5
83.0
82.5
TRAINING
5 10 15 20
Epochs
FOSTER-DEV
5 10 15 20 25
Epochs
GIMPEL-DEV
5 10 15 20 25
Epochs
FOSTER-TEST
5 10 15 20 25
Epochs
RITTER-TEST
5 10 15 20 25
Epochs
IN-HOUSE
5 10 15 20 25
Epochs
BASELINE
F1
CM
BASELINE
F1
CM
BASELINE
F1
CM
BASELINE
F1
CM
BASELINE
F1
CM
Accuracy (%) 84.0
83.5
83.0
82.5
82.0
81.5
81.0
Accuracy (%) 82
81
80
79
78
77
76
75
74
Accuracy (%) 87.0
Accuracy (%) 86.5
86.0
85.5
85.0
84.5
84.0
83.5
84.0
83.8
83.6
83.4
83.2
83.0
82.8
82.6
82.4
82.2
BASELINE
F1
CM
</figure>
<figureCaption confidence="0.98567625">
Figure 5: POS accuracy for the three models: baseline, confusion matrix loss (CM) and F1-weighted
(F1) loss for increased number of training epochs. Top row: in-sample accuracy on training (left) and
in-sample evaluation datasets (center, right). Bottom row: out-of-sample accuracy on various data sets.
CM is robust on both in-sample and out-of-sample data.
</figureCaption>
<table confidence="0.999557875">
RITTER-TEST
F1: All NP VP PP
BL 76.20 78.61 74.25 86.79
CM 76.42 79.07 74.98 86.19
FOSTER-TEST
F1: All NP VP PP
BL 68.49 70.73 60.56 86.50
CM 68.97 71.25 61.97 87.24
</table>
<tableCaption confidence="0.969271">
Table 2: Downstream results on chunking. Overall
F1 score (All) as well as F1 for NP, VP and PP.
</tableCaption>
<bodyText confidence="0.9986539375">
the baseline (BL) for the downstream task of
chunking. Overall chunking F1 score improves.
More importantly, we report on individual scores
for NP, VP and PP chunks, where we see consis-
tent improvements for NPs and VPs (since both
nouns and verbs have high inter-annotator agree-
ment), while results on PP are mixed. This is to
be expected, since PP phrases involve adposition-
als (ADP) that are often confused with particles
(PRT), cf. Figure 3. Our tagger has been trained
to deliberately abstract away from such uncertain
cases. The results show that taking uncertainty in
POS annotations into consideration during train-
ing has a positive effect in downstream results. It
is thus better if we do not try to urge our models
to make a firm decision on phenomena that neither
</bodyText>
<page confidence="0.991935">
747
</page>
<table confidence="0.9993216">
BASELINE CM
Regularizer FOSTER-DEV FOSTER-TEST IN-HOUSE FOSTER-DEV FOSTER-TEST IN-HOUSE
Averaging 0.827 0.837 0.830 0.831 0.844 0.833
Drop-out 0.827 0.838 0.827 0.836 0.843 0.833
Zipfian 0.821 0.835 0.833 0.825 0.838 0.836
</table>
<tableCaption confidence="0.999889">
Table 1: Results across regularizers (after 13 epochs).
</tableCaption>
<bodyText confidence="0.643339">
linguistic theories nor annotators do agree upon.
</bodyText>
<subsectionHeader confidence="0.994552">
6.2 NER
</subsectionHeader>
<bodyText confidence="0.999012555555556">
In the previous section, we saw positive effects of
cost-sensitive POS tagging for chunking, and here
we evaluate it on another downstream task, NER.
For the named entity recognition setup, we use
commonly used features, in particular features
for word tokens, orthographic features like the
presence of hyphens, digits, single quotes, up-
per/lowercase, 3 character prefix and suffix infor-
mation. Moreover, we add Brown word cluster
features that use 2,4,6,8,..,16 bitstring prefixes es-
timated from a large Twitter corpus (Owoputi et
al., 2013).5
For NER, we do not have access to carefully
annotated Twitter data for training, but rely on
the crowdsourced annotations described in Finin
et al. (2010). We use the concatenation of the
CoNLL 2003 training split of annotated data from
the Reuters corpus and the Finin data for training,
as in this case training on the union resulted in a
model that is substantially better than training on
any of the individual data sets. For evaluation, we
have three Twitter data set. We use the recently
published data set from the MSM 2013 challenge
(29k tokens)6, the data set of Ritter et al. (2011)
used also by Fromheide et al. (2014) (46k tokens),
as well as an in-house annotated data set (20k to-
kens) (Fromheide et al., 2014).
</bodyText>
<table confidence="0.997115">
F1: RITTER MSM IN-HOUSE
BL 78.20 82.25 82.58
CM 78.30 82.00 82.77
</table>
<tableCaption confidence="0.806658">
Table 3: Downstream results for named entity
recognition (F1 scores).
Table 3 shows the result of using our POS mod-
els in downstream NER evaluation. Here we ob-
serve mixed results. The cost-sensitive model is
</tableCaption>
<footnote confidence="0.994978">
5http://www.ark.cs.cmu.edu/TweetNLP/
6http://oak.dcs.shef.ac.uk/msm2013/ie_
challenge/
</footnote>
<bodyText confidence="0.999685">
able to improve performance on two out of the
three test sets, while being slightly below baseline
performance on the MSM challenge data. Note
that in contrast to chunking, POS tags are just one
of the many features used for NER (albeit an im-
portant one), which might be part of the reason
why the picture looks slightly different from what
we observed above on chunking.
</bodyText>
<sectionHeader confidence="0.9998" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.999962696969697">
Cost-sensitive learning takes costs, such as mis-
classification cost, into consideration. That is,
each instance that is not classified correctly during
the learning process may contribute differently to
the overall error. Geibel and Wysotzki (2003) in-
troduce instance-dependent cost values for the per-
ceptron algorithm and apply it to a set of binary
classification problems. We focus here on struc-
tured problems and propose cost-sensitive learn-
ing for POS tagging using the structured percep-
tron algorithm. In a similar spirit, Higashiyama
et al. (2013) applied cost-sensitive learning to the
structured perceptron for an entity recognition task
in the medical domain. They consider the dis-
tance between the predicted and true label se-
quence smoothed by a parameter that they esti-
mate on a development set. This means that the
entire sequence is scored at once, while we update
on a per-label basis.
The work most related to ours is the recent study
of Song et al. (2012). They suggest that some er-
rors made by a POS tagger are more serious than
others, especially for downstream tasks. They de-
vise a hierarchy of POS tags for the Penn tree-
bank tag set (e.g. the class NOUN contains NN,
NNS, NNP, NNPS and CD) and use that in an
SVM learner. They modify the Hinge loss that
can take on three values: 0, Q,1. If an error oc-
curred and the predicted tag is in the same class as
the gold tag, a loss Q occurred, otherwise it counts
as full cost. In contrast to our approach, they let
the learner focus on the more difficult cases by oc-
curring a bigger loss when the predicted POS tag
</bodyText>
<page confidence="0.99423">
748
</page>
<bodyText confidence="0.999989450704226">
is in a different category. Their approach is thus
suitable for a fine-grained tagging scheme and re-
quires tuning of the cost parameter σ. We tackle
the problem from a different angle by letting the
learner abstract away from difficult, inconsistent
cases as estimated from inter-annotator scores.
Our approach is also related to the literature
on regularization, since our cost-sensitive loss
functions are aimed at preventing over-fitting to
low-confidence annotations. Søgaard (2013b;
2013a) presented two theories of linguistic varia-
tion and perceptron learning algorithms that reg-
ularize models to minimize loss under expected
variation. Our work is related, but models varia-
tions in annotation rather than variations in input.
There is a large literature related to the issue of
learning from annotator bias. Reidsma and op den
Akker (2008) show that differences between anno-
tators are not random slips of attention but rather
different biases annotators might have, i.e. differ-
ent mental conceptions. They show that a classi-
fier trained on data from one annotator performed
much better on in-sample (same annotator) data
than on data of any other annotator. They propose
two ways to address this problem: i) to identify
subsets of the data that show higher inter-annotator
agreement and use only that for training (e.g. for
speaker address identification they restrict the data
to instances where at least one person is in the
focus of attention); ii) if available, to train sepa-
rate models on data annotated by different anno-
tators and combine them through voting. The lat-
ter comes at the cost of recall, because they de-
liberately chose the classifier to abstain in non-
consensus cases.
In a similar vein, Klebanov and Beigman (2009)
divide the instance space into easy and hard cases,
i.e. easy cases are reliably annotated, whereas
items that are hard show confusion and disagree-
ment. Hard cases are assumed to be annotated
by individual annotator’s coin-flips, and thus can-
not be assumed to be uniformly distributed (Kle-
banov and Beigman, 2009). They show that learn-
ing with annotator noise can have deteriorating ef-
fect at test time, and thus propose to remove hard
cases, both at test time (Klebanov and Beigman,
2009) and training time (Beigman and Klebanov,
2009).
In general, it is important to analyze the data
and check for label biases, as a machine learner is
greatly affected by annotator noise that is not ran-
dom but systematic (Reidsma and Carletta, 2008).
However, rather than training on subsets of data or
training separate models – which all implicitly as-
sume that there is a large amount of training data
available – we propose to integrate inter-annotator
biases directly into the loss function.
Regarding measurements for agreements, sev-
eral scores have been suggested in the literature.
Apart from the simple agreement measure, which
records how often annotators choose the same
value for an item, there are several statistics that
qualify this measure by adjusting for other fac-
tors, such as Cohen’s κ (Cohen and others, 1960),
the G-index score (Holley and Guilford, 1964), or
Krippendorff’s α (Krippendorf, 2004). However,
most of these scores are sensitive to the label dis-
tribution, missing values, and other circumstances.
The measure used in this paper is less affected by
these factors, but manages to give us a good un-
derstanding of the agreement.
</bodyText>
<sectionHeader confidence="0.997389" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99999075">
In NLP, we use a variety of measures to assess
and control annotator disagreement to produce ho-
mogenous final annotations. This masks the fact
that some annotations are more reliable than oth-
ers, and which is thus not reflected in learned pre-
dictors. We incorporate the annotator uncertainty
on certain labels by measuring annotator agree-
ment and use it in the modified loss function of
a structured perceptron. We show that this ap-
proach works well independent of regularization,
both on in-sample and out-of-sample data. More-
over, when evaluating the models trained with our
loss function on downstream tasks, we observe im-
provements on two different tasks. Our results
suggest that we need to pay more attention to an-
notator confidence when training predictors.
</bodyText>
<sectionHeader confidence="0.994207" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998616">
We would like to thank the anonymous review-
ers and Nathan Schneider for valuable comments
and feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
</bodyText>
<sectionHeader confidence="0.994931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.676539">
Eyal Beigman and Beata Klebanov. 2009. Learning
with annotation noise. In ACL.
Jacob Cohen et al. 1960. A coefficient of agreement
</reference>
<page confidence="0.995367">
749
</page>
<reference confidence="0.993914456310679">
for nominal scales. Educational and psychological
measurement, 20(1):37–46.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In ACL.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez, and Anders
Søgaard. 2013. Down-stream effects of tree-to-
dependency conversions. In NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with
crowdsourcing. In NAACL-HLT 2010 Workshop on
Creating Speech and Language Data with Amazon’s
Mechanical Turk.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Hege Fromheide, Dirk Hovy, and Anders Søgaard.
2014. Crowdsourcing and annotating NER for Twit-
ter #drift. In Proceedings of LREC 2014.
Peter Geibel and Fritz Wysotzki. 2003. Perceptron
based learning with example dependent and noisy
costs. In ICML.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In ACL.
Shohei Higashiyama, Kazuhiro Seki, and Kuniaki Ue-
hara. 2013. Clinical entity recognition using
cost-sensitive structured perceptron for NTCIR-10
MedNLP. In NTCIR.
Jasper Wilson Holley and Joy Paul Guilford. 1964.
A Note on the G-Index of Agreement. Educational
and Psychological Measurement, 24(4):749.
Dirk Hovy, Barbara Plank, and Anders Søgaard. 2014.
When POS datasets don’t add up: Combatting sam-
ple bias. In Proceedings of LREC 2014.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In NAACL.
Beata Klebanov and Eyal Beigman. 2009. From an-
notator agreement to noise models. Computational
Linguistics, 35(4):495–503.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Christopher D Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Computational Linguistics and Intelligent
Text Processing, pages 171–189. Springer.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria
Bertomeu Castell´o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In ACL.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
NAACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Dennis Reidsma and Jean Carletta. 2008. Reliabil-
ity measurement without limits. Computational Lin-
guistics, 34(3):319–326.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ‘subjective’ annotations. In Workshop on
Human Judgements in Computational Linguistics,
COLING.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In EMNLP.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In ACL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL.
Anders Søgaard. 2013a. Part-of-speech tagging with
antagonistic adversaries. In ACL.
Anders Søgaard. 2013b. Zipfian corruptions for robust
pos tagging. In NAACL.
</reference>
<page confidence="0.968654">
750
</page>
<reference confidence="0.9983985">
Hyun-Je Song, Jeong-Woo Son, Tae-Gil Noh, Seong-
Bae Park, and Sang-Jo Lee. 2012. A cost sensitive
part-of-speech tagging: differentiating serious errors
from minor errors. In ACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources.
</reference>
<page confidence="0.998198">
751
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.628255">
<title confidence="0.998663">Learning part-of-speech taggers with inter-annotator agreement loss</title>
<author confidence="0.981224">Barbara Plank</author>
<author confidence="0.981224">Dirk Hovy</author>
<author confidence="0.981224">Anders</author>
<affiliation confidence="0.999514">Center for Language University of Copenhagen,</affiliation>
<address confidence="0.9906">Njalsgade 140, DK-2300 Copenhagen</address>
<email confidence="0.721233">bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk</email>
<abstract confidence="0.998036134328358">for languages where available resources are mutually inconsistent (Johansson, 2013). Unfortunately, there is no grand unifying linguistic theory of how to analyze the structure of sentences. While linguists agree on certain things, there is still a wide range of unresolved questions. Consider the following sentence: (1) @GaryMurphyDCU of @DemMattersIRL take part in a panel discussion on October 10th re the aftermath of #seanref ... linguists will agree that a preposiand discussion compound noun, they likely to disagree whether heading the verb vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is particle or a noun; is numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, by leaving out numerals or introducing of speech, e.g. for English al., 1993) or words ending in 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators Abstract In natural language processing (NLP) annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure consistent annotations. However, annotation guidelines often make linguistically debatable and even somewhat arbitrary decisions, and interannotator agreement is often less than perfect. While annotation projects usually specify how to deal with linguistically debatable phenomena, annotator disagreements typically still stem from these “hard” cases. This indicates that some errors are more debatable than others. In this paper, we use small samples of doublyannotated part-of-speech (POS) data for Twitter to estimate annotation reliability show how those metrics of interannotator agreement can be implemented in the loss functions of POS taggers. We find that these cost-sensitive algorithms perform better across annotation projects and, more surprisingly, even on data annotated according to the same guidelines. Finally, we show that POS tagging models sensitive to inter-annotator agreement perform better on the downstream task of chunking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eyal Beigman</author>
<author>Beata Klebanov</author>
</authors>
<title>Learning with annotation noise.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="26506" citStr="Beigman and Klebanov, 2009" startWordPosition="4353" endWordPosition="4356">lassifier to abstain in nonconsensus cases. In a similar vein, Klebanov and Beigman (2009) divide the instance space into easy and hard cases, i.e. easy cases are reliably annotated, whereas items that are hard show confusion and disagreement. Hard cases are assumed to be annotated by individual annotator’s coin-flips, and thus cannot be assumed to be uniformly distributed (Klebanov and Beigman, 2009). They show that learning with annotator noise can have deteriorating effect at test time, and thus propose to remove hard cases, both at test time (Klebanov and Beigman, 2009) and training time (Beigman and Klebanov, 2009). In general, it is important to analyze the data and check for label biases, as a machine learner is greatly affected by annotator noise that is not random but systematic (Reidsma and Carletta, 2008). However, rather than training on subsets of data or training separate models – which all implicitly assume that there is a large amount of training data available – we propose to integrate inter-annotator biases directly into the loss function. Regarding measurements for agreements, several scores have been suggested in the literature. Apart from the simple agreement measure, which records how o</context>
</contexts>
<marker>Beigman, Klebanov, 2009</marker>
<rawString>Eyal Beigman and Beata Klebanov. 2009. Learning with annotation noise. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and psychological measurement,</booktitle>
<pages>20--1</pages>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen et al. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="13648" citStr="Collins, 2002" startWordPosition="2261" endWordPosition="2262">xpressive and takes both the gold and predicted tags into account. It basically weights the loss by how likely the gold and predicted tag are to be mistaken for each other, i.e., (the inverse of) their confusion probability: -y(yj, yi)) = 1− P({A1(X), A2(X)} = {yj, yi}) In both loss functions, a lower gamma value means that the tags are more likely to be confused by a pair of annotators. In this case, the update is smaller. In contrast, the learner incurs greater loss when easy tags are confused. It is straight-forward to extend these costsensitive loss functions to the structured perceptron (Collins, 2002). In Figure 4, we provide the pseudocode for the cost-sensitive structured online learning algorithm. We refer to the cost-sensitive structured learners as F1- and CM-weighted below. 5 Experiments In our main experiments, we use structured perceptron (Collins, 2002) with random corruptions 745 using a drop-out rate of 0.1 for regularization, following Søgaard (2013a). We use the LXMLS toolkit implementation1 with default parameters. We present learning curves across iterations, and only set parameters using held-out data for our downstream experiments.2 5.1 Results Our results are presented in</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1424" citStr="Das and Petrov, 2011" startWordPosition="222" endWordPosition="225">o disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators Abstract In natural language processing (NLP) annotation projects, we use int</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Twitter part-of-speech tagging for all: overcoming sparse and noisy data. In RANLP.</title>
<date>2013</date>
<contexts>
<context position="5864" citStr="Derczynski et al. (2013)" startWordPosition="911" endWordPosition="914">notated Twitter data sets The vast majority of POS-annotated resources across languages contain mostly newswire text. Some annotated Twitter data sets do exist for English. Ritter et al. (2011) present a manually annotated data set of 16 thousand tokens. They do not report inter-annotator agreement. Gimpel et al. (2011) annotated about 26 thousand tokens and report a raw agreement of 92%. Foster et al. (2011) annotated smaller portions of data for cross-domain evaluation purposes. We refer to the data as RITTER, GIMPEL and FOSTER below. In our experiments, we use the RITTER splits provided by Derczynski et al. (2013), and the October splits of the GIMPEL data set, version 0.3. We train our models on the concatenation of RITTER-TRAIN and GIMPEL-TRAIN and evaluate them on the remaining data, the dev and test set provided by Foster et al. (2011) as well as an inhouse annotated data set of 3k tokens (see below). The three annotation efforts (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011) all used different tagsets, however, and they also differ in tokenization, as well as a wide range of linguistic decisions. We mapped all the three corpora to the universal tagset provided by Petrov et al. (20</context>
<context position="17342" citStr="Derczynski et al. (2013)" startWordPosition="2824" endWordPosition="2827"> 13 epochs, as they performed best on FOSTER-DEV (cf. Figure 5). Thus, parameters were optimized only on POS tagging data, not on the downstream evaluation tasks. We use a publicly available implementation of conditional random fields (Lafferty et al., 2001)3 for the chunking and NER experiments, and provide the POS tags from our CM learner as features. 6.1 Chunking The set of features for chunking include information from tokens and POS tags, following Sha and Pereira (2003). We train the chunker on Twitter data (Ritter et al., 2011), more specifically, the 70/30 train/test split provided by Derczynski et al. (2013) for POS tagging, as the original authors performed cross validation. We train on the 70% Twitter data (11k tokens) and evaluate on the remaining 30%, as well as on the test data from Foster et al. (2011). The FOSTER data was originally annotated for POS and constituency tree information. We converted it to chunks using publicly available conversion software.4 Part-of-speech tags are the ones assigned by our cost-sensitive (CM) POS model trained on Twitter data, the concatenation of Gimpel and 70% Ritter training data. We did not include the CoNLL 2000 training data (newswire text), since addi</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all: overcoming sparse and noisy data. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Anders Johannsen</author>
<author>Sigrid Klerke</author>
<author>Emanuele Lapponi</author>
<author>Hector Martinez</author>
<author>Anders Søgaard</author>
</authors>
<title>Down-stream effects of tree-todependency conversions.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3347" citStr="Elming et al., 2013" startWordPosition="516" endWordPosition="519">s functions of POS taggers. We find that these cost-sensitive algorithms perform better across annotation projects and, more surprisingly, even on data annotated according to the same guidelines. Finally, we show that POS tagging models sensitive to inter-annotator agreement perform better on the downstream task of chunking. 1 Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics when learning predictive models from the annotated data. To achieve this, we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, </context>
</contexts>
<marker>Elming, Johannsen, Klerke, Lapponi, Martinez, Søgaard, 2013</marker>
<rawString>Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi, Hector Martinez, and Anders Søgaard. 2013. Down-stream effects of tree-todependency conversions. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>Will Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in Twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="21325" citStr="Finin et al. (2010)" startWordPosition="3487" endWordPosition="3490">r chunking, and here we evaluate it on another downstream task, NER. For the named entity recognition setup, we use commonly used features, in particular features for word tokens, orthographic features like the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2,4,6,8,..,16 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013).5 For NER, we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6, the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: RITTER MSM IN-HOUSE BL 78</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in Twitter data with crowdsourcing. In NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Josef Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirde Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From news to comments: Resources and benchmarks for parsing the language of Web 2.0.</title>
<date>2011</date>
<booktitle>In IJCNLP.</booktitle>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hege Fromheide</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Crowdsourcing and annotating NER for Twitter #drift.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="21800" citStr="Fromheide et al. (2014)" startWordPosition="3572" endWordPosition="3575">R, we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6, the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: RITTER MSM IN-HOUSE BL 78.20 82.25 82.58 CM 78.30 82.00 82.77 Table 3: Downstream results for named entity recognition (F1 scores). Table 3 shows the result of using our POS models in downstream NER evaluation. Here we observe mixed results. The cost-sensitive model is 5http://www.ark.cs.cmu.edu/TweetNLP/ 6http://oak.dcs.shef.ac.uk/msm2013/ie_ challenge/ able to improve performance on two out of the three test sets, while being slightly below baseline performance on the MSM challenge data. Note </context>
</contexts>
<marker>Fromheide, Hovy, Søgaard, 2014</marker>
<rawString>Hege Fromheide, Dirk Hovy, and Anders Søgaard. 2014. Crowdsourcing and annotating NER for Twitter #drift. In Proceedings of LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Geibel</author>
<author>Fritz Wysotzki</author>
</authors>
<title>Perceptron based learning with example dependent and noisy costs.</title>
<date>2003</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="22892" citStr="Geibel and Wysotzki (2003)" startWordPosition="3743" endWordPosition="3746">mprove performance on two out of the three test sets, while being slightly below baseline performance on the MSM challenge data. Note that in contrast to chunking, POS tags are just one of the many features used for NER (albeit an important one), which might be part of the reason why the picture looks slightly different from what we observed above on chunking. 7 Related work Cost-sensitive learning takes costs, such as misclassification cost, into consideration. That is, each instance that is not classified correctly during the learning process may contribute differently to the overall error. Geibel and Wysotzki (2003) introduce instance-dependent cost values for the perceptron algorithm and apply it to a set of binary classification problems. We focus here on structured problems and propose cost-sensitive learning for POS tagging using the structured perceptron algorithm. In a similar spirit, Higashiyama et al. (2013) applied cost-sensitive learning to the structured perceptron for an entity recognition task in the medical domain. They consider the distance between the predicted and true label sequence smoothed by a parameter that they estimate on a development set. This means that the entire sequence is s</context>
</contexts>
<marker>Geibel, Wysotzki, 2003</marker>
<rawString>Peter Geibel and Fritz Wysotzki. 2003. Perceptron based learning with example dependent and noisy costs. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shohei Higashiyama</author>
<author>Kazuhiro Seki</author>
<author>Kuniaki Uehara</author>
</authors>
<title>Clinical entity recognition using cost-sensitive structured perceptron for NTCIR-10 MedNLP. In NTCIR.</title>
<date>2013</date>
<contexts>
<context position="23198" citStr="Higashiyama et al. (2013)" startWordPosition="3792" endWordPosition="3795">htly different from what we observed above on chunking. 7 Related work Cost-sensitive learning takes costs, such as misclassification cost, into consideration. That is, each instance that is not classified correctly during the learning process may contribute differently to the overall error. Geibel and Wysotzki (2003) introduce instance-dependent cost values for the perceptron algorithm and apply it to a set of binary classification problems. We focus here on structured problems and propose cost-sensitive learning for POS tagging using the structured perceptron algorithm. In a similar spirit, Higashiyama et al. (2013) applied cost-sensitive learning to the structured perceptron for an entity recognition task in the medical domain. They consider the distance between the predicted and true label sequence smoothed by a parameter that they estimate on a development set. This means that the entire sequence is scored at once, while we update on a per-label basis. The work most related to ours is the recent study of Song et al. (2012). They suggest that some errors made by a POS tagger are more serious than others, especially for downstream tasks. They devise a hierarchy of POS tags for the Penn treebank tag set </context>
</contexts>
<marker>Higashiyama, Seki, Uehara, 2013</marker>
<rawString>Shohei Higashiyama, Kazuhiro Seki, and Kuniaki Uehara. 2013. Clinical entity recognition using cost-sensitive structured perceptron for NTCIR-10 MedNLP. In NTCIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jasper Wilson Holley</author>
<author>Joy Paul Guilford</author>
</authors>
<date>1964</date>
<booktitle>A Note on the G-Index of Agreement. Educational and Psychological Measurement,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="27333" citStr="Holley and Guilford, 1964" startWordPosition="4488" endWordPosition="4491"> However, rather than training on subsets of data or training separate models – which all implicitly assume that there is a large amount of training data available – we propose to integrate inter-annotator biases directly into the loss function. Regarding measurements for agreements, several scores have been suggested in the literature. Apart from the simple agreement measure, which records how often annotators choose the same value for an item, there are several statistics that qualify this measure by adjusting for other factors, such as Cohen’s κ (Cohen and others, 1960), the G-index score (Holley and Guilford, 1964), or Krippendorff’s α (Krippendorf, 2004). However, most of these scores are sensitive to the label distribution, missing values, and other circumstances. The measure used in this paper is less affected by these factors, but manages to give us a good understanding of the agreement. 8 Conclusion In NLP, we use a variety of measures to assess and control annotator disagreement to produce homogenous final annotations. This masks the fact that some annotations are more reliable than others, and which is thus not reflected in learned predictors. We incorporate the annotator uncertainty on certain l</context>
</contexts>
<marker>Holley, Guilford, 1964</marker>
<rawString>Jasper Wilson Holley and Joy Paul Guilford. 1964. A Note on the G-Index of Agreement. Educational and Psychological Measurement, 24(4):749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Barbara Plank</author>
<author>Anders Søgaard</author>
</authors>
<title>When POS datasets don’t add up: Combatting sample bias.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="7477" citStr="Hovy et al. (2014)" startWordPosition="1198" endWordPosition="1201"> al. (2011) tag out and to as adpositions, Ritter et al. (2011) consider them particles. What is the right analysis depends on the compositionality of the construction and the linguistic theory one subscribes to. Other differences include the analysis of abbreviations (PRT in GIMPEL; X in RITTER and FOSTER), colon (X in GIMPEL; punctuation in RITTER and FOSTER), and emoticons, which can take multiple parts of speech in GIMPEL, but are always X in RITTER, while they are absent in FOSTER. GIMPEL-TRAIN and RITTER-TRAIN are also internally inconsistent. See the bottom of Figure 1 for examples and Hovy et al. (2014) for a more detailed discussion on differences between the data sets. Since the mapping to universal tags could potentially introduce errors, we also annotated a data set directly using universal tags. We randomly selected 200 tweets collected over the span of one day, and had three annotators tag this set. We split the data in such a way that each annotator had 100 tweets: two annotators had disjoint sets, the third overlapped 50 items with each of the two others. In this way, we obtained an initial set of 100 doubly-annotated tweets. The annotators were not provided with annotation guideline</context>
</contexts>
<marker>Hovy, Plank, Søgaard, 2014</marker>
<rawString>Dirk Hovy, Barbara Plank, and Anders Søgaard. 2014. When POS datasets don’t add up: Combatting sample bias. In Proceedings of LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Training parsers on incompatible treebanks.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<marker>Johansson, 2013</marker>
<rawString>Richard Johansson. 2013. Training parsers on incompatible treebanks. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>From annotator agreement to noise models.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="25969" citStr="Klebanov and Beigman (2009)" startWordPosition="4264" endWordPosition="4267">nnotator) data than on data of any other annotator. They propose two ways to address this problem: i) to identify subsets of the data that show higher inter-annotator agreement and use only that for training (e.g. for speaker address identification they restrict the data to instances where at least one person is in the focus of attention); ii) if available, to train separate models on data annotated by different annotators and combine them through voting. The latter comes at the cost of recall, because they deliberately chose the classifier to abstain in nonconsensus cases. In a similar vein, Klebanov and Beigman (2009) divide the instance space into easy and hard cases, i.e. easy cases are reliably annotated, whereas items that are hard show confusion and disagreement. Hard cases are assumed to be annotated by individual annotator’s coin-flips, and thus cannot be assumed to be uniformly distributed (Klebanov and Beigman, 2009). They show that learning with annotator noise can have deteriorating effect at test time, and thus propose to remove hard cases, both at test time (Klebanov and Beigman, 2009) and training time (Beigman and Klebanov, 2009). In general, it is important to analyze the data and check for</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>Beata Klebanov and Eyal Beigman. 2009. From annotator agreement to noise models. Computational Linguistics, 35(4):495–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorf</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology,</title>
<date>2004</date>
<location>Thousand Oaks, CA.</location>
<note>second edition, chapter 11. Sage,</note>
<contexts>
<context position="27374" citStr="Krippendorf, 2004" startWordPosition="4495" endWordPosition="4496"> or training separate models – which all implicitly assume that there is a large amount of training data available – we propose to integrate inter-annotator biases directly into the loss function. Regarding measurements for agreements, several scores have been suggested in the literature. Apart from the simple agreement measure, which records how often annotators choose the same value for an item, there are several statistics that qualify this measure by adjusting for other factors, such as Cohen’s κ (Cohen and others, 1960), the G-index score (Holley and Guilford, 1964), or Krippendorff’s α (Krippendorf, 2004). However, most of these scores are sensitive to the label distribution, missing values, and other circumstances. The measure used in this paper is less affected by these factors, but manages to give us a good understanding of the agreement. 8 Conclusion In NLP, we use a variety of measures to assess and control annotator disagreement to produce homogenous final annotations. This masks the fact that some annotations are more reliable than others, and which is thus not reflected in learned predictors. We incorporate the annotator uncertainty on certain labels by measuring annotator agreement an</context>
</contexts>
<marker>Krippendorf, 2004</marker>
<rawString>Klaus Krippendorf, 2004. Content Analysis: An Introduction to Its Methodology, second edition, chapter 11. Sage, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="16976" citStr="Lafferty et al., 2001" startWordPosition="2762" endWordPosition="2765">aining a POS tagger that takes inter-annotator agreement scores into consideration is also effective on downstream tasks. Therefore, we evaluate our best model, the CM-weighted learner, in two downstream tasks: shallow parsing—also known as chunking—and named entity recognition (NER). For the downstream evaluation, we used the baseline and CM models trained over 13 epochs, as they performed best on FOSTER-DEV (cf. Figure 5). Thus, parameters were optimized only on POS tagging data, not on the downstream evaluation tasks. We use a publicly available implementation of conditional random fields (Lafferty et al., 2001)3 for the chunking and NER experiments, and provide the POS tags from our CM learner as features. 6.1 Chunking The set of features for chunking include information from tokens and POS tags, following Sha and Pereira (2003). We train the chunker on Twitter data (Ritter et al., 2011), more specifically, the 70/30 train/test split provided by Derczynski et al. (2013) for POS tagging, as the original authors performed cross validation. We train on the 70% Twitter data (11k tokens) and evaluate on the remaining 30%, as well as on the test data from Foster et al. (2011). The FOSTER data was original</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: is it time for some linguistics?</title>
<date>2011</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>171--189</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1298" citStr="Manning, 2011" startWordPosition="205" endWordPosition="206">seanref ... While linguists will agree that in is a preposition, and panel discussion a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embra</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D Manning. 2011. Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In Computational Linguistics and Intelligent Text Processing, pages 171–189. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Mary Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1258" citStr="Marcus et al., 1993" startWordPosition="196" endWordPosition="199">scussion on October 10th re the aftermath of #seanref ... While linguists will agree that in is a preposition, and panel discussion a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Our approach Instead of glossing over those annotation disagreemen</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell Marcus, Mary Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang,</title>
<date>2013</date>
<journal>Oscar T¨ackstr¨om, Claudia Bedini, N´uria Bertomeu Castell´o, and</journal>
<booktitle>In ACL.</booktitle>
<marker>McDonald, Nivre, 2013</marker>
<rawString>Ryan McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria Bertomeu Castell´o, and Jungmee Lee. 2013. Universal dependency annotation for multilingual parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="1445" citStr="Petrov et al., 2012" startWordPosition="226" endWordPosition="229">l is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators Abstract In natural language processing (NLP) annotation projects, we use inter-annotator agreemen</context>
<context position="6467" citStr="Petrov et al. (2012)" startWordPosition="1017" endWordPosition="1020">ski et al. (2013), and the October splits of the GIMPEL data set, version 0.3. We train our models on the concatenation of RITTER-TRAIN and GIMPEL-TRAIN and evaluate them on the remaining data, the dev and test set provided by Foster et al. (2011) as well as an inhouse annotated data set of 3k tokens (see below). The three annotation efforts (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011) all used different tagsets, however, and they also differ in tokenization, as well as a wide range of linguistic decisions. We mapped all the three corpora to the universal tagset provided by Petrov et al. (2012) and used the same dummy symbols for numbers, URLs, etc., in all the data sets. Following (Foster et al., 2011), we consider URLs, usernames and hashtags as NOUN. We did not change the tokenization. The data sets differ in how they analyze many of the linguistically hard cases. Consider, for example, the analysis of will you come out to in GIMPEL and RITTER (Figure 1, top). While Gimpel et al. (2011) tag out and to as adpositions, Ritter et al. (2011) consider them particles. What is the right analysis depends on the compositionality of the construction and the linguistic theory one subscribes</context>
<context position="9430" citStr="Petrov et al., 2012" startWordPosition="1512" endWordPosition="1515">referred below to as INHOUSE, contained 3,064 tokens (200 tweets) and is publicly available at http://bitbucket.org/lowlands/ costsensitive-data/, along with the data used to compute inter-annotator agreement scores for learning cost-sensitive taggers, described in the next section. 3 Computing agreement scores Gimpel et al. (2011) used 72 doubly-annotated tweets to estimate inter-annotator agreement, and we also use doubly-annotated data to compute agreement scores. We randomly sampled 500 tweets for this purpose. Each tweet was annotated by two annotators, again using the universal tag set (Petrov et al., 2012). All annotators were encouraged to use their own best judgment rather than following guidelines or discussing difficult cases with each other. This is in contrast to Gimpel et al. (2011), who used annotation guidelines. The average inter-annotator agreement was 0.88 for raw agreement, and 0.84 for Cohen’s n. Gimpel et al. (2011) report a raw agreement of 0.92. We use two metrics to provide a more detailed picture of inter-annotator agreement, namely F1-scores between annotators on individual parts of speech, and tag confusion probabilities, which we derive from confusion matrices. The F1-scor</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability measurement without limits.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="26706" citStr="Reidsma and Carletta, 2008" startWordPosition="4388" endWordPosition="4391">t are hard show confusion and disagreement. Hard cases are assumed to be annotated by individual annotator’s coin-flips, and thus cannot be assumed to be uniformly distributed (Klebanov and Beigman, 2009). They show that learning with annotator noise can have deteriorating effect at test time, and thus propose to remove hard cases, both at test time (Klebanov and Beigman, 2009) and training time (Beigman and Klebanov, 2009). In general, it is important to analyze the data and check for label biases, as a machine learner is greatly affected by annotator noise that is not random but systematic (Reidsma and Carletta, 2008). However, rather than training on subsets of data or training separate models – which all implicitly assume that there is a large amount of training data available – we propose to integrate inter-annotator biases directly into the loss function. Regarding measurements for agreements, several scores have been suggested in the literature. Apart from the simple agreement measure, which records how often annotators choose the same value for an item, there are several statistics that qualify this measure by adjusting for other factors, such as Cohen’s κ (Cohen and others, 1960), the G-index score </context>
</contexts>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma and Jean Carletta. 2008. Reliability measurement without limits. Computational Linguistics, 34(3):319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Rieks op den Akker</author>
</authors>
<title>Exploiting ‘subjective’ annotations.</title>
<date>2008</date>
<booktitle>In Workshop on Human Judgements in Computational Linguistics, COLING.</booktitle>
<marker>Reidsma, den Akker, 2008</marker>
<rawString>Dennis Reidsma and Rieks op den Akker. 2008. Exploiting ‘subjective’ annotations. In Workshop on Human Judgements in Computational Linguistics, COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5433" citStr="Ritter et al. (2011)" startWordPosition="840" endWordPosition="843"> use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the effect of our modified training by measuring intrinsic as well as downstream performance of the resulting models on two tasks, namely named entity recognition (NER) and chunking, which both use POS tags as input features. 2 POS-annotated Twitter data sets The vast majority of POS-annotated resources across languages contain mostly newswire text. Some annotated Twitter data sets do exist for English. Ritter et al. (2011) present a manually annotated data set of 16 thousand tokens. They do not report inter-annotator agreement. Gimpel et al. (2011) annotated about 26 thousand tokens and report a raw agreement of 92%. Foster et al. (2011) annotated smaller portions of data for cross-domain evaluation purposes. We refer to the data as RITTER, GIMPEL and FOSTER below. In our experiments, we use the RITTER splits provided by Derczynski et al. (2013), and the October splits of the GIMPEL data set, version 0.3. We train our models on the concatenation of RITTER-TRAIN and GIMPEL-TRAIN and evaluate them on the remainin</context>
<context position="6922" citStr="Ritter et al. (2011)" startWordPosition="1102" endWordPosition="1105">lso differ in tokenization, as well as a wide range of linguistic decisions. We mapped all the three corpora to the universal tagset provided by Petrov et al. (2012) and used the same dummy symbols for numbers, URLs, etc., in all the data sets. Following (Foster et al., 2011), we consider URLs, usernames and hashtags as NOUN. We did not change the tokenization. The data sets differ in how they analyze many of the linguistically hard cases. Consider, for example, the analysis of will you come out to in GIMPEL and RITTER (Figure 1, top). While Gimpel et al. (2011) tag out and to as adpositions, Ritter et al. (2011) consider them particles. What is the right analysis depends on the compositionality of the construction and the linguistic theory one subscribes to. Other differences include the analysis of abbreviations (PRT in GIMPEL; X in RITTER and FOSTER), colon (X in GIMPEL; punctuation in RITTER and FOSTER), and emoticons, which can take multiple parts of speech in GIMPEL, but are always X in RITTER, while they are absent in FOSTER. GIMPEL-TRAIN and RITTER-TRAIN are also internally inconsistent. See the bottom of Figure 1 for examples and Hovy et al. (2014) for a more detailed discussion on difference</context>
<context position="17258" citStr="Ritter et al., 2011" startWordPosition="2812" endWordPosition="2815">. For the downstream evaluation, we used the baseline and CM models trained over 13 epochs, as they performed best on FOSTER-DEV (cf. Figure 5). Thus, parameters were optimized only on POS tagging data, not on the downstream evaluation tasks. We use a publicly available implementation of conditional random fields (Lafferty et al., 2001)3 for the chunking and NER experiments, and provide the POS tags from our CM learner as features. 6.1 Chunking The set of features for chunking include information from tokens and POS tags, following Sha and Pereira (2003). We train the chunker on Twitter data (Ritter et al., 2011), more specifically, the 70/30 train/test split provided by Derczynski et al. (2013) for POS tagging, as the original authors performed cross validation. We train on the 70% Twitter data (11k tokens) and evaluate on the remaining 30%, as well as on the test data from Foster et al. (2011). The FOSTER data was originally annotated for POS and constituency tree information. We converted it to chunks using publicly available conversion software.4 Part-of-speech tags are the ones assigned by our cost-sensitive (CM) POS model trained on Twitter data, the concatenation of Gimpel and 70% Ritter traini</context>
<context position="21763" citStr="Ritter et al. (2011)" startWordPosition="3565" endWordPosition="3568">us (Owoputi et al., 2013).5 For NER, we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6, the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: RITTER MSM IN-HOUSE BL 78.20 82.25 82.58 CM 78.30 82.00 82.77 Table 3: Downstream results for named entity recognition (F1 scores). Table 3 shows the result of using our POS models in downstream NER evaluation. Here we observe mixed results. The cost-sensitive model is 5http://www.ark.cs.cmu.edu/TweetNLP/ 6http://oak.dcs.shef.ac.uk/msm2013/ie_ challenge/ able to improve performance on two out of the three test sets, while being slightly below baseline perform</context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3473" citStr="Schwartz et al., 2011" startWordPosition="533" endWordPosition="536">surprisingly, even on data annotated according to the same guidelines. Finally, we show that POS tagging models sensitive to inter-annotator agreement perform better on the downstream task of chunking. 1 Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics when learning predictive models from the annotated data. To achieve this, we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. Th</context>
</contexts>
<marker>Schwartz, Abend, Reichart, Rappoport, 2011</marker>
<rawString>Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="17198" citStr="Sha and Pereira (2003)" startWordPosition="2801" endWordPosition="2804">sing—also known as chunking—and named entity recognition (NER). For the downstream evaluation, we used the baseline and CM models trained over 13 epochs, as they performed best on FOSTER-DEV (cf. Figure 5). Thus, parameters were optimized only on POS tagging data, not on the downstream evaluation tasks. We use a publicly available implementation of conditional random fields (Lafferty et al., 2001)3 for the chunking and NER experiments, and provide the POS tags from our CM learner as features. 6.1 Chunking The set of features for chunking include information from tokens and POS tags, following Sha and Pereira (2003). We train the chunker on Twitter data (Ritter et al., 2011), more specifically, the 70/30 train/test split provided by Derczynski et al. (2013) for POS tagging, as the original authors performed cross validation. We train on the 70% Twitter data (11k tokens) and evaluate on the remaining 30%, as well as on the test data from Foster et al. (2011). The FOSTER data was originally annotated for POS and constituency tree information. We converted it to chunks using publicly available conversion software.4 Part-of-speech tags are the ones assigned by our cost-sensitive (CM) POS model trained on Twi</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Part-of-speech tagging with antagonistic adversaries.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4412" citStr="Søgaard, 2013" startWordPosition="677" endWordPosition="678">we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost-sensitive online learning algorithms for inducing models from annotated data that take inter-annotator agreement into consideration. Specifically, we use online structured perceptron with drop-out, which has previously been applied to POS tagging and is known to be robust across samples and domains (Søgaard, 2013a). We incorporate the inter-annotator agreement in the loss function either as inter-annotator F1-scores or as the confusion probability between annotators (see Section 3 below for a more detailed description). We use a small amounts of doublyannotated Twitter data to estimate F1-scores and confusion probabilities, and incorporate them during training via a modified loss function. Specifically, we use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the ef</context>
<context position="14015" citStr="Søgaard (2013" startWordPosition="2317" endWordPosition="2318"> a pair of annotators. In this case, the update is smaller. In contrast, the learner incurs greater loss when easy tags are confused. It is straight-forward to extend these costsensitive loss functions to the structured perceptron (Collins, 2002). In Figure 4, we provide the pseudocode for the cost-sensitive structured online learning algorithm. We refer to the cost-sensitive structured learners as F1- and CM-weighted below. 5 Experiments In our main experiments, we use structured perceptron (Collins, 2002) with random corruptions 745 using a drop-out rate of 0.1 for regularization, following Søgaard (2013a). We use the LXMLS toolkit implementation1 with default parameters. We present learning curves across iterations, and only set parameters using held-out data for our downstream experiments.2 5.1 Results Our results are presented in Figure 5. The top left graph plots accuracy on the training data per iteration. We see that CM-weighting does not hurt training data accuracy. The reason may be that the cost-sensitive learner does not try (as hard) to optimize performance on inconsistent annotations. The next two plots (upper mid and upper right) show accuracy over epochs on in-sample evaluation </context>
<context position="15836" citStr="Søgaard, 2013" startWordPosition="2588" endWordPosition="2589"> is the case with the F1-weighted learner. 5.2 Robustness across regularizers Discriminative learning typically benefits from regularization to prevent overfitting. The simplest is the averaged perceptron, but various other methods have been suggested in the literature. We use structured perceptron with drop-out, but results are relatively robust across other regularization methods. Drop-out works by randomly dropping a fraction of the active features in each iteration, thus preventing overfitting. Table 1 shows the results for using different regularizers, in particular, Zipfian corruptions (Søgaard, 2013b) and averaging. While there are minor differences across data sets and regularizers, we observe that the corresponding cell using the loss function suggested in this paper (CM) always performs better than the baseline method. 1https://github.com/gracaninja/ lxmls-toolkit/ 2In this case, we use FOSTER-DEV as our development data to avoid in-sample bias. 6 Downstream evaluation We have seen that our POS tagging model improves over the baseline model on three out-ofsample test sets. The question remains whether training a POS tagger that takes inter-annotator agreement scores into consideration</context>
<context position="24714" citStr="Søgaard (2013" startWordPosition="4060" endWordPosition="4061"> our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicted POS tag 748 is in a different category. Their approach is thus suitable for a fine-grained tagging scheme and requires tuning of the cost parameter σ. We tackle the problem from a different angle by letting the learner abstract away from difficult, inconsistent cases as estimated from inter-annotator scores. Our approach is also related to the literature on regularization, since our cost-sensitive loss functions are aimed at preventing over-fitting to low-confidence annotations. Søgaard (2013b; 2013a) presented two theories of linguistic variation and perceptron learning algorithms that regularize models to minimize loss under expected variation. Our work is related, but models variations in annotation rather than variations in input. There is a large literature related to the issue of learning from annotator bias. Reidsma and op den Akker (2008) show that differences between annotators are not random slips of attention but rather different biases annotators might have, i.e. different mental conceptions. They show that a classifier trained on data from one annotator performed much</context>
</contexts>
<marker>Søgaard, 2013</marker>
<rawString>Anders Søgaard. 2013a. Part-of-speech tagging with antagonistic adversaries. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Zipfian corruptions for robust pos tagging.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="4412" citStr="Søgaard, 2013" startWordPosition="677" endWordPosition="678">we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost-sensitive online learning algorithms for inducing models from annotated data that take inter-annotator agreement into consideration. Specifically, we use online structured perceptron with drop-out, which has previously been applied to POS tagging and is known to be robust across samples and domains (Søgaard, 2013a). We incorporate the inter-annotator agreement in the loss function either as inter-annotator F1-scores or as the confusion probability between annotators (see Section 3 below for a more detailed description). We use a small amounts of doublyannotated Twitter data to estimate F1-scores and confusion probabilities, and incorporate them during training via a modified loss function. Specifically, we use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the ef</context>
<context position="14015" citStr="Søgaard (2013" startWordPosition="2317" endWordPosition="2318"> a pair of annotators. In this case, the update is smaller. In contrast, the learner incurs greater loss when easy tags are confused. It is straight-forward to extend these costsensitive loss functions to the structured perceptron (Collins, 2002). In Figure 4, we provide the pseudocode for the cost-sensitive structured online learning algorithm. We refer to the cost-sensitive structured learners as F1- and CM-weighted below. 5 Experiments In our main experiments, we use structured perceptron (Collins, 2002) with random corruptions 745 using a drop-out rate of 0.1 for regularization, following Søgaard (2013a). We use the LXMLS toolkit implementation1 with default parameters. We present learning curves across iterations, and only set parameters using held-out data for our downstream experiments.2 5.1 Results Our results are presented in Figure 5. The top left graph plots accuracy on the training data per iteration. We see that CM-weighting does not hurt training data accuracy. The reason may be that the cost-sensitive learner does not try (as hard) to optimize performance on inconsistent annotations. The next two plots (upper mid and upper right) show accuracy over epochs on in-sample evaluation </context>
<context position="15836" citStr="Søgaard, 2013" startWordPosition="2588" endWordPosition="2589"> is the case with the F1-weighted learner. 5.2 Robustness across regularizers Discriminative learning typically benefits from regularization to prevent overfitting. The simplest is the averaged perceptron, but various other methods have been suggested in the literature. We use structured perceptron with drop-out, but results are relatively robust across other regularization methods. Drop-out works by randomly dropping a fraction of the active features in each iteration, thus preventing overfitting. Table 1 shows the results for using different regularizers, in particular, Zipfian corruptions (Søgaard, 2013b) and averaging. While there are minor differences across data sets and regularizers, we observe that the corresponding cell using the loss function suggested in this paper (CM) always performs better than the baseline method. 1https://github.com/gracaninja/ lxmls-toolkit/ 2In this case, we use FOSTER-DEV as our development data to avoid in-sample bias. 6 Downstream evaluation We have seen that our POS tagging model improves over the baseline model on three out-ofsample test sets. The question remains whether training a POS tagger that takes inter-annotator agreement scores into consideration</context>
<context position="24714" citStr="Søgaard (2013" startWordPosition="4060" endWordPosition="4061"> our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicted POS tag 748 is in a different category. Their approach is thus suitable for a fine-grained tagging scheme and requires tuning of the cost parameter σ. We tackle the problem from a different angle by letting the learner abstract away from difficult, inconsistent cases as estimated from inter-annotator scores. Our approach is also related to the literature on regularization, since our cost-sensitive loss functions are aimed at preventing over-fitting to low-confidence annotations. Søgaard (2013b; 2013a) presented two theories of linguistic variation and perceptron learning algorithms that regularize models to minimize loss under expected variation. Our work is related, but models variations in annotation rather than variations in input. There is a large literature related to the issue of learning from annotator bias. Reidsma and op den Akker (2008) show that differences between annotators are not random slips of attention but rather different biases annotators might have, i.e. different mental conceptions. They show that a classifier trained on data from one annotator performed much</context>
</contexts>
<marker>Søgaard, 2013</marker>
<rawString>Anders Søgaard. 2013b. Zipfian corruptions for robust pos tagging. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyun-Je Song</author>
<author>Jeong-Woo Son</author>
<author>Tae-Gil Noh</author>
<author>SeongBae Park</author>
<author>Sang-Jo Lee</author>
</authors>
<title>A cost sensitive part-of-speech tagging: differentiating serious errors from minor errors.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23616" citStr="Song et al. (2012)" startWordPosition="3865" endWordPosition="3868">ssification problems. We focus here on structured problems and propose cost-sensitive learning for POS tagging using the structured perceptron algorithm. In a similar spirit, Higashiyama et al. (2013) applied cost-sensitive learning to the structured perceptron for an entity recognition task in the medical domain. They consider the distance between the predicted and true label sequence smoothed by a parameter that they estimate on a development set. This means that the entire sequence is scored at once, while we update on a per-label basis. The work most related to ours is the recent study of Song et al. (2012). They suggest that some errors made by a POS tagger are more serious than others, especially for downstream tasks. They devise a hierarchy of POS tags for the Penn treebank tag set (e.g. the class NOUN contains NN, NNS, NNP, NNPS and CD) and use that in an SVM learner. They modify the Hinge loss that can take on three values: 0, Q,1. If an error occurred and the predicted tag is in the same class as the gold tag, a loss Q occurred, otherwise it counts as full cost. In contrast to our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicted</context>
</contexts>
<marker>Song, Son, Noh, Park, Lee, 2012</marker>
<rawString>Hyun-Je Song, Jeong-Woo Son, Tae-Gil Noh, SeongBae Park, and Sang-Jo Lee. 2012. A cost sensitive part-of-speech tagging: differentiating serious errors from minor errors. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Joakim Nivre</author>
<author>Evelina Andersson</author>
</authors>
<title>Cross-framework evaluation for statistical parsing.</title>
<date>2012</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="3497" citStr="Tsarfaty et al., 2012" startWordPosition="537" endWordPosition="540">ata annotated according to the same guidelines. Finally, we show that POS tagging models sensitive to inter-annotator agreement perform better on the downstream task of chunking. 1 Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics when learning predictive models from the annotated data. To achieve this, we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2012</marker>
<rawString>Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012. Cross-framework evaluation for statistical parsing. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Crosslanguage parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="1389" citStr="Zeman and Resnik, 2008" startWordPosition="216" endWordPosition="219">on a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators Abstract In natural language processing (N</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Crosslanguage parser adaptation between related languages. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Hard problems of tagset conversion.</title>
<date>2010</date>
<booktitle>In Proceedings of the Second International Conference on Global Interoperability for Language Resources.</booktitle>
<contexts>
<context position="1402" citStr="Zeman, 2010" startWordPosition="220" endWordPosition="221"> are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators Abstract In natural language processing (NLP) annotatio</context>
</contexts>
<marker>Zeman, 2010</marker>
<rawString>Daniel Zeman. 2010. Hard problems of tagset conversion. In Proceedings of the Second International Conference on Global Interoperability for Language Resources.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>