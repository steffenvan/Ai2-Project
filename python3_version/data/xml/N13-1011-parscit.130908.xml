<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000074">
<title confidence="0.9988945">
Cross-Lingual Semantic Similarity of Words as the Similarity of Their
Semantic Word Responses
</title>
<author confidence="0.995905">
Ivan Vuli´c and Marie-Francine Moens
</author>
<affiliation confidence="0.999539">
Department of Computer Science
</affiliation>
<address confidence="0.647658333333333">
KU Leuven
Celestijnenlaan 200A
Leuven, Belgium
</address>
<email confidence="0.995239">
{ivan.vulic,marie-francine.moens}@cs.kuleuven.be
</email>
<sectionHeader confidence="0.998573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968884615385">
We propose a new approach to identifying
semantically similar words across languages.
The approach is based on an idea that two
words in different languages are similar if they
are likely to generate similar words (which in-
cludes both source and target language words)
as their top semantic word responses. Se-
mantic word responding is a concept from
cognitive science which addresses detecting
most likely words that humans output as free
word associations given some cue word. The
method consists of two main steps: (1) it uti-
lizes a probabilistic multilingual topic model
trained on comparable data to learn and quan-
tify the semantic word responses, (2) it pro-
vides ranked lists of similar words accord-
ing to the similarity of their semantic word
response vectors. We evaluate our approach
in the task of bilingual lexicon extraction
(BLE) for a variety of language pairs. We
show that in the cross-lingual settings without
any language pair dependent knowledge the
response-based method of similarity is more
robust and outperforms current state-of-the art
methods that directly operate in the semantic
space of latent cross-lingual concepts/topics.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871294117647">
Cross-lingual semantic word similarity addresses
the task of detecting words that refer to similar se-
mantic concepts and convey similar meanings across
languages. It ultimately boils down to the automatic
identification of translation pairs, that is, bilingual
lexicon extraction (BLE). Such lexicons and seman-
tically similar words serve as important resources
in cross-lingual knowledge induction (e.g., Zhao et
al. (2009)), statistical machine translation (Och and
Ney, 2003) and cross-lingual information retrieval
(Ballesteros and Croft, 1997; Levow et al., 2005).
From parallel corpora, semantically similar words
and bilingual lexicons are induced on the basis of
word alignment models (Brown et al., 1993; Och
and Ney, 2003). However, due to a relative scarce-
ness of parallel texts for many language pairs and
domains, there has been a recent growing interest in
mining semantically similar words across languages
on the basis of comparable data readily available on
the Web (e.g., Wikipedia, news stories) (Haghighi et
al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al.,
2011; Prochasson and Fung, 2011).
Approaches to detecting semantic word similarity
from comparable corpora are most commonly based
on an idea known as the distributional hypothesis
(Harris, 1954), which states that words with sim-
ilar meanings are likely to appear in similar con-
texts. Each word is typically represented by a high-
dimensional vector in a feature vector space or a so-
called semantic space, where the dimensions of the
vector are its context features. The semantic similar-
ity of two words, wS1 given in the source language
LS with vocabulary V S and wT2 in the target lan-
guage LT with vocabulary V T is then:
</bodyText>
<equation confidence="0.73176">
Sim(wS1 , wT2 ) = SF(cv(wS1 ), cv(wT2 )) (1)
</equation>
<bodyText confidence="0.9897294">
cv(wS1 ) = [scS1 (c1), ... , scS1 (cN)] denotes a context
vector for wS1 with N context features ck, where
scS1 (ck) denotes the score for wS1 associated with
context feature ck (similar for wT2 ). SF is a sim-
ilarity function (e.g., cosine, the Kullback-Leibler
</bodyText>
<page confidence="0.977205">
106
</page>
<note confidence="0.4724435">
Proceedings of NAACL-HLT 2013, pages 106–116,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999552208333333">
divergence, the Jaccard index) operating on the con-
text vectors (Lee, 1999; Cha, 2007).
In order to compute cross-lingual semantic word
similarity, one needs to design the context features
of words given in two different languages that span
a shared cross-lingual semantic space. Such cross-
lingual semantic spaces are typically spanned by:
(1) bilingual lexicon entries (Rapp, 1999; Gaussier
et al., 2004; Laroche and Langlais, 2010; Tamura
et al., 2012), or (2) latent language-independent se-
mantic concepts/axes (e.g., latent cross-lingual top-
ics) induced by an algebraic model (Dumais et al.,
1996), or more recently by a generative probabilis-
tic model (Haghighi et al., 2008; Daum´e III and Ja-
garlamudi, 2011; Vuli´c et al., 2011). Context vec-
tors cv(wi) and cv(w2) for both source and target
words are then compared in the semantic space in-
dependently of their respective languages.
In this work, we propose a new approach to con-
structing the shared cross-lingual semantic space
that relies on a paradigm of semantic word respond-
ing or free word association. We borrow that con-
cept from the psychology/cognitive science litera-
ture. Semantic word responding addresses a task
that requires participants to produce first words that
come to their mind that are related to a presented cue
word (Nelson et al., 2000; Steyvers et al., 2004).
The new cross-lingual semantic space is spanned
by all vocabulary words in the source and the target
language. Each axis in the space denotes a semantic
word response. The similarity between two words is
then computed as the similarity between the vectors
comprising their semantic word responses using any
of existing 5F-s. Two words are considered seman-
tically similar if they are likely to generate similar
semantic word responses and assign similar impor-
tance to them.
We utilize a shared semantic space of latent cross-
lingual topics learned by a multilingual probabilistic
topic model to obtain semantic word responses and
quantify the strength of association between any cue
word and its responses monolingually and across
languages, and, consequently, to build semantic re-
sponse vectors. That effectively translates the task
of word similarity from the semantic space spanned
by latent cross-lingual topics to the semantic space
spanned by all vocabulary words in both languages.
The main contributions of this article are:
</bodyText>
<listItem confidence="0.989818545454545">
• We propose a new approach to modeling cross-
lingual semantic similarity of words based on
the similarity of their semantic word responses.
• We present how to estimate and quantify se-
mantic word responses by means of a multilin-
gual probabilistic topic model.
• We demonstrate how to employ our novel
paradigm that relies on semantic word respond-
ing in the task of bilingual lexicon extraction
(BLE) from comparable data.
• We show that the response-based model of sim-
</listItem>
<bodyText confidence="0.905118181818182">
ilarity is more robust and obtains better results
for BLE than the models that operate in the se-
mantic space spanned by latent semantic con-
cepts, i.e., cross-lingual topics directly.
The following sections first review relevant prior
work and provide a very short introduction to multi-
lingual probabilistic topic modeling, then describe
our response-based approach to modeling cross-
lingual semantic word similarity, and finally present
our evaluation and results on the BLE task for a va-
riety of language pairs.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999108904761905">
When dealing with the cross-lingual semantic word
similarity, the focus of the researchers is typically
on BLE, since usually the most similar words across
languages are direct translations of each other. Nu-
merous approaches emerged over the years that try
to induce bilingual word lexicons on the basis of
distributional information. Especially challenging
is the task of mining semantically similar words
from comparable data without any external knowl-
edge source such as machine-readable seed bilin-
gual lexicons used in (Fung and Yee, 1998; Rapp,
1999; Fung and Cheung, 2004; Gaussier et al., 2004;
Morin et al., 2007; Andrade et al., 2010; Tamura
et al., 2012), predefined explicit ontology or cate-
gory knowledge used in (D´ejean et al., 2002; Hassan
and Mihalcea, 2009; Agirre et al., 2009), or ortho-
graphic clues as used in (Koehn and Knight, 2002;
Haghighi et al., 2008; Daum´e III and Jagarlamudi,
2011). This work addresses that particularly difficult
setting which does not assume any language pair de-
pendent background knowledge. It makes methods
</bodyText>
<page confidence="0.998166">
107
</page>
<bodyText confidence="0.99896235">
developed in such a setting applicable even on dis-
tant language pairs with scarce resources.
Recently, Griffiths et al. (2007), and Steyvers and
Griffiths (2007) proposed models of free word asso-
ciation and semantic word similarity in the monolin-
gual settings based on per-topic word distributions
from probabilistic topic models such as pLSA (Hof-
mann, 1999) and LDA (Blei et al., 2003). Addition-
ally, Vuli´c et al. (2011) constructed several models
that utilize a shared cross-lingual topical space ob-
tained by a multilingual topic model (Mimno et al.,
2009; De Smet and Moens, 2009; Boyd-Graber and
Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e
III, 2010; Zhang et al., 2010) to identify potential
translation candidates in the cross-lingual settings
without any background knowledge. In this paper,
we show that a transition from their semantic space
spanned by cross-lingual topics to a semantic space
spanned by all vocabulary words yields more robust
models of cross-lingual semantic word similarity.
</bodyText>
<sectionHeader confidence="0.779161" genericHeader="method">
3 Modeling Word Similarity as the
</sectionHeader>
<subsectionHeader confidence="0.964777">
Similarity of Semantic Word Responses
</subsectionHeader>
<bodyText confidence="0.999981857142857">
This section contains a detailed description of our
semantic word similarity method that relies on se-
mantic word responses. Since the method utilizes
the concept of multilingual probabilistic topic mod-
eling, we first provide a very short overview of that
concept, then present the intuition behind the ap-
proach, and finally describe our method in detail.
</bodyText>
<subsectionHeader confidence="0.997407">
3.1 Multilingual Probabilistic Topic Modeling
</subsectionHeader>
<bodyText confidence="0.991804522727273">
Assume that we are given a multilingual corpus
C of l languages, and C is a set of text collec-
tions {C1, ... , Cl} in those languages. A multi-
lingual probabilistic topic model (Mimno et al.,
2009; De Smet and Moens, 2009; Boyd-Graber
and Blei, 2009; Ni et al., 2009; Jagarlamudi and
Daum´e III, 2010; Zhang et al., 2010) of a mul-
tilingual corpus C is defined as a set of semanti-
cally coherent multinomial distributions of words
with values Pj(wj� |zk), j = 1, ... ,l, for each vo-
cabulary V 1, ... , Vj, ... , V l associated with text
collections C1, ... , Cj, ... , Cl ∈ C given in lan-
guages L1, ... , Lj, ... , Ll. Pj(wj� |zk) is calculated
for each wj� ∈ Vj. The probability scores Pj(wj� |zk)
build per-topic word distributions, and they consti-
tute a language-specific representation (e.g., a prob-
ability value is assigned only for words from Vj)
of a language-independent cross-lingual latent con-
cept, that is, latent cross-lingual topic zk ∈ Z.
Z = {z1, ... , zK} represents the set of all K la-
tent cross-lingual topics present in the multilingual
corpus. Each document in the multilingual corpus
is thus considered a mixture of K cross-lingual top-
ics from the set Z. That mixture for some docu-
ment dj� ∈ Cj is modeled by the probability scores
Pj(zk|dj� ) that altogether build per-document topic
distributions.
Each cross-lingual topic from the set Z can be
observed as a latent language-independent concept
present in the multilingual corpus, but each lan-
guage in the corpus uses only words from its own
vocabulary to describe the content of that concept.
For instance, having a multilingual collection in En-
glish, Spanish and Dutch and discovering a topic
on Soccer, that cross-lingual topic would be repre-
sented by words (actually probabilities over words)
{player, goal, coach, ... } in English, {bal´on (ball),
futbolista (soccer player), goleador (scorer), ... }
in Spanish, and {wedstrijd (match), elftal (soccer
team), doelpunt (goal), ... } in Dutch. We have
E&amp;quot;&amp;quot;j,Vj Pj (wz  |zk) = 1, for each vocabulary Vj
language Lj, and for each topic zk ∈
Z. Therefore, the latent cross-lingual topics also
span a shared cross-lingual semantic space.
</bodyText>
<subsectionHeader confidence="0.998504">
3.2 The Intuition Behind the Approach
</subsectionHeader>
<bodyText confidence="0.9999959375">
Imagine the following thought experiment. A group
of human subjects who have been raised bilingually
and thus are native speakers of two languages LS
and LT, is playing a game of word associations.
The game consists of possibly an infinite number of
iterations, and each iteration consists of 4 rounds.
In the first round (the S-S round), given a word in
the language LS, the subject has to generate a list
of words in the same language LS that first occur
to her/him as semantic word responses to the given
word. The list is in descending order, with more
prominent word responses occurring higher in the
list. In the second round (the S-T round), the sub-
ject repeats the procedure, and generates the list of
word responses to the same word from LS, but now
in the other language LT. The third (the T-T round)
</bodyText>
<page confidence="0.995946">
108
</page>
<bodyText confidence="0.999980740740741">
and the fourth round (the T-S round) are similar to
the first and the second round, but now a list of word
responses in both LS and LT has to be generated for
some cue word from LT. The process of generating
the lists of semantic responses then continues with
other cue words and other human subjects.
As the final result, for each word in the source
language LS, and each word in the target language
LT, we obtain a single list of semantic word re-
sponses comprising words in both languages. All
lists are sorted in descending order, based on some
association score that takes into account both the
number of times a word has occurred as an asso-
ciative response, as well as the position in the list
in each round. We can now measure the similarity
of any two words, regardless of their corresponding
languages, according to the similarity of their cor-
responding lists that contain their word responses.
Words that are equally likely to trigger the same as-
sociative responses in the human brain, and more-
over assign equal importance to those responses, as
provided in the lists of associative responses, are
very likely to be closely semantically similar. Addi-
tionally, for a given word wS1 in the source language
LS, some word wT2 in LT that has the highest simi-
larity score among all words in LT should be a direct
word-to-word translation of wS1 .
</bodyText>
<subsectionHeader confidence="0.9990155">
3.3 Modeling Semantic Word Responses via
Cross-Lingual Topics
</subsectionHeader>
<bodyText confidence="0.904238944444445">
Cross-lingual topics provide a sound framework to
construct a probabilistic model of the aforemen-
tioned experiment. To model semantic word re-
sponses via the shared space of cross-lingual top-
ics, we have to set a probabilistic mass that quan-
tifies the degree of association. Given two words
w1, w2 E V S U VT, a natural way of expressing the
asymmetric semantic association is by modeling the
probability P(w2|w1) (Griffiths et al., 2007), that is,
the probability to generate word w2 as a response
given word w1. After the training of a multilin-
gual topic model on a multilingual corpus, we obtain
per-topic word distributions with scores PS(wSi |zk)
and PT(wTi |zk) (see Sect. 3.1).1 The probability
1A remark on notation throughout the paper: Since the
shared space of cross-lingual topics allows us to construct a
uniform representation for all words regardless of a vocabulary
they belong to, due to simplicity and to stress the uniformity,
</bodyText>
<equation confidence="0.985925">
P(w2|w1) is then decomposed as follows:
Resp(w1, w2) = P(w2|w1) =
</equation>
<bodyText confidence="0.9999505">
The probability scores P(w2|zk) select words that
are highly descriptive for each particular topic. The
probability scores P(zk|w1) ensure that topics zk
that are semantically relevant to the given word
w1 dominate the sum, so the overall high score
Resp(w1, w2) of the semantic word response is as-
signed only to highly descriptive words of the se-
mantically related topics. Using the shared space
of cross-lingual topics, semantic response scores can
be derived for any two words w1, w2 E V S U VT .1
The generative model closely resembles the ac-
tual process in the human brain - when we gener-
ate semantic word responses, we first tend to as-
sociate that word with a related semantic/cognitive
concept, in this case a cross-lingual topic (the factor
P(zk|w1)), and then, after establishing the concept,
we output a list of words that we consider the most
prominent/descriptive for that concept (words with
high scores in the factor P(w2|zk)) (Nelson et al.,
2000; Steyvers et al., 2004). Due to such modeling
properties, this model of semantic word responding
tends to assign higher association scores for high
frequency words. It eventually leads to asymmet-
ric associations/responses. We have detected that
phenomenon both monolingually and across lan-
guages. For instance, the first response to Span-
ish word mutaci´on (mutation) is English word gene.
Other examples include caldera (boiler)-steam, de-
portista (sportsman)-sport, horario (schedule)-hour
or pescador (fisherman)-fish. In the other associa-
tion direction, we have detected top responses such
as merchant-comercio (trade) or neologism-palabra
(word). In the monolingual setting, we acquire
English pairs such as songwriter-music, discipline-
sport, or Spanish pairs gripe (flu)-enfermedad (dis-
ease), cuenca (basin)-r´ıo (river), etc.
</bodyText>
<sectionHeader confidence="0.755975" genericHeader="method">
3.4 Response-Based Model of Similarity
</sectionHeader>
<bodyText confidence="0.990657166666667">
Eq. (2) provides a way to measure the strength of
semantic word responses. In order to establish the
we sometimes use notation P(wi|zk) and P(zk|wi) instead of
PS(wi|zk) or PS(zk|wi) (similar for subscript T). However,
the reader must be aware that, for instance, P(wi|zk) actually
means PS(wi|zk) if wi ∈ V S, and PT(wi|zk) if wi ∈ V T.
</bodyText>
<equation confidence="0.99694175">
P(w2|zk)P(zk|w1) (2)
K
E
k=1
</equation>
<page confidence="0.987854">
109
</page>
<table confidence="0.999896583333333">
Semantic responses Response-based similarity
dramaturgo (playwright) play playwright dramaturgo
obra (play) .101 play .142 play .122 playwright
escritor (writer) .083 obra (play) .111 escritor (writer) .087 dramatist
play .066 player .033 obra (play) .073 tragedy
writer .050 escena (scene) .031 writer .060 play
poet .047 jugador (player) .026 poeta (poet) .055 essayist
autor (author) .041 adaptation .025 poet .053 novelist
poeta (poet) .039 stage .024 autor (author) .046 drama
teatro (theatre) .030 game .022 teatro (theatre) .043 tragedian
drama .026 juego (game) .021 tragedy .031 satirist
contribution .025 teatro (theatre) .019 drama .026 writer
</table>
<tableCaption confidence="0.99985">
Table 1: An example of top 10 semantic word responses and the final response-based similarity for some Spanish and
</tableCaption>
<bodyText confidence="0.956847133333333">
English words. The responses are estimated from Spanish-English Wikipedia data by bilingual LDA. We can observe
several interesting phenomena: (1) High-frequency words tend to appear higher in the lists of semantic responses
(e.g., play and obra for all 3 words), (2) Due to the modeling properties that give preference to high-frequency words
(Sect. 3.3), a word might not generate itself as the top semantic response (e.g., playwright-play), (3) Both source
and target language words occur as the top responses in the lists, (4) Although play is the top semantic response in
English for both dramaturgo and playwright, its list of top semantic responses is less similar to the lists of those two
words, (5) Although the English word playwright does not appear in the top 10 semantic responses to dramaturgo,
and dramaturgo does not appear in the top 10 responses to playwright, the more robust response-based similarity
method detects that the two words are actually very similar based on their lists of responses, (6) dramaturgo and
playwright have very similar lists of semantic responses which ultimately leads to detecting that playwright is the
most semantically similar word to dramaturgo across the two languages (the last column), i.e., they are direct one-to-
one translations of each other, (7) Another English word dramatist very similar to Spanish dramaturgo is also pushed
higher in the final list, although it is not found in the list of top semantic responses to dramaturgo.
final similarity between two words, we have to com-
pare their semantic response vectors, that is, their
semantic response scores over all words in both
vocabularies. The final model of word similarity
closely mimics our thought experiment. First, for
each word wSi E V S, we generate probability scores
P(wSj |wSi ) for all words wSj E V S (the S-S rounds).
Note that P(wSi |wSi ) is also defined by Eq. (2).
Following that, for each word wSi E V S, we gen-
erate probability scores P(wTj |wSi ), for all words
wTj E VT (the S-T rounds). Similarly, we calcu-
late probability scores P(wTj |wTi ) and P(wSj |wTi ),
for each wTi , wTj E VT , and for each wSj E V S (the
T-T and T-S rounds).
Now, each word wi E V S U VT may be repre-
sented by a (|V S |+ |V T |)-dimensional context vec-
tor cv(wi) as follows:2
</bodyText>
<equation confidence="0.965472">
[P(wS1 |wi), ... , P(wS S||wi), ... , P(wT T  ||wi)].
|V
</equation>
<bodyText confidence="0.974566">
We have created a language-independent cross-
</bodyText>
<footnote confidence="0.8928995">
2We assume that the two sets V S and V T are disjunct. It
means that, for instance, Spanish word pie (foot) from V S and
English word pie from V T are treated as two different word
types. In that case, it holds |V S U VT |= |V S |+ |V T |.
</footnote>
<bodyText confidence="0.999804157894737">
lingual semantic space spanned by all vocabulary
words in both languages. Each feature corresponds
to one word from vocabularies V S and V T, while
the exact score for each feature in the context
vector cv(wi) is precisely the probability that this
word/feature will be generated as a word response
given word wi. The degree of similarity between
two words is then computed on the basis of similar-
ity between their feature vectors using some of the
standard similarity functions (Cha, 2007).
The novel response-based approach of similarity
removes the effect of high-frequency words that tend
to appear higher in the lists of semantic word re-
sponses. Therefore, the real synonyms and trans-
lations should occur as top candidates in the lists
of similar words obtained by the response-based
method. That property may be exploited to identify
one-to-one translations across languages and build a
bilingual lexicon (see Table 1).
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.997485">
4.1 Data Collections
</subsectionHeader>
<bodyText confidence="0.998894">
We work with the following corpora:
</bodyText>
<page confidence="0.982259">
110
</page>
<listItem confidence="0.960165583333333">
• IT-EN-W: A collection of 18,898 Italian-
English Wikipedia article pairs previously used
by Vuli´c et al. (2011).
• ES-EN-W: A collection of 13,696 Spanish-
English Wikipedia article pairs.
• NL-EN-W: A collection of 7,612 Dutch-
English Wikipedia article pairs.
• NL-EN-W+EP: The NL-EN-W corpus aug-
mented with 6,206 Dutch-English document
pairs from Europarl (Koehn, 2005). Although
Europarl is a parallel corpus, no explicit use is
made of sentence-level alignments.
</listItem>
<bodyText confidence="0.998871333333333">
All corpora are theme-aligned, that is, the aligned
document pairs discuss similar subjects, but are
in general not direct translations (except the Eu-
roparl document pairs). NL-EN-W+EP serves to test
whether better semantic responses could be learned
from data of higher quality, and to measure how it
affects the response-based similarity method and the
quality of induced lexicons. Following (Koehn and
Knight, 2002; Haghighi et al., 2008; Prochasson and
Fung, 2011), we consider only noun word types. We
retain only nouns that occur at least 5 times in the
corpus. We record the lemmatized form when avail-
able, and the original form otherwise. Again follow-
ing their setup, we use TreeTagger (Schmid, 1994)
for POS tagging and lemmatization.
</bodyText>
<subsectionHeader confidence="0.965157">
4.2 Multilingual Topic Model
</subsectionHeader>
<bodyText confidence="0.999983607142857">
The multilingual probabilistic topic model we use
is a straightforward multilingual extension of the
standard Blei et al.’s LDA model (Blei et al., 2003)
called bilingual LDA (Mimno et al., 2009; Ni et
al., 2009; De Smet and Moens, 2009). For the de-
tails regarding the modeling assumptions, generative
story, training and inference procedure of the bilin-
gual LDA model, we refer the interested reader to
the aforementioned relevant literature. The poten-
tial of the model in the task of bilingual lexicon ex-
traction was investigated before (Mimno et al., 2009;
Vuli´c et al., 2011), and it was also utilized in other
cross-lingual tasks (e.g., Platt et al. (2010); Ni et
al. (2011)). We use Gibbs sampling for training.
In a typical setting for mining semantically similar
words using latent topic models in both monolingual
(Griffiths et al., 2007; Dinu and Lapata, 2010) and
cross-lingual setting (Vuli´c et al., 2011), the best re-
sults are obtained with the number of topics set to
a few thousands (Pt� 2000). Therefore, our bilingual
LDA model on all corpora is trained with the number
of topics K = 2000. Other parameters of the model
are set to the standard values according to Steyvers
and Griffiths (2007): α = 50/K and Q = 0.01.
We are aware that different hyper-parameter settings
(Asuncion et al., 2009; Lu et al., 2011), might have
influence on the quality of learned cross-lingual top-
ics, but that analysis is out of the scope of this paper.
</bodyText>
<subsectionHeader confidence="0.920769">
4.3 Compared Methods
</subsectionHeader>
<bodyText confidence="0.997236">
We evaluate and compare the following word simi-
larity approaches in all our experiments:
</bodyText>
<listItem confidence="0.897174807692308">
1) The method that regards the lists of semantic
word responses across languages obtained by Eq.
(2) directly as the lists of semantically similar words
(Direct-SWR).
2) The state-of-the-art method that employs a simi-
larity function (SF) on the K-dimensional word vec-
tors cv(wi) in the semantic space of latent cross-
lingual topics. The dimensions of the vectors are
conditional topic distribution scores P(zk|wi) that
are obtained by the multilingual topic model directly
(Steyvers and Griffiths, 2007; Vuli´c et al., 2011). We
have tested different SF-s (e.g., the Kullback-Leibler
and the Jensen-Shannon divergence, the cosine mea-
sure), and have detected that in general the best
scores are obtained when using the Bhattacharyya
coefficient (BC) (Bhattacharyya, 1943; Kazama et
al., 2010) (Topic-BC).
3) The best scoring similarity method from Vuli´c
et al. (2011) named TI+Cue. This state-of-the-art
method also operates in the semantic space of latent
cross-lingual concepts/topics.
4) The response-based similarity described in Sect.
3. As for Topic-BC, we again use BC as the simi-
larity function, but now on |V S U VT |-dimensional
context vectors in the semantic space spanned by
all words in both vocabularies that represent seman-
</listItem>
<bodyText confidence="0.820570333333333">
tic word responses (Response-BC). Given two N-
dimensional word vectors cv(wS1 ) and cv(wT2 ), the
BC or the fidelity measure (Cha, 2007) is defined as:
</bodyText>
<equation confidence="0.98796375">
V SCf (Cn) - SC2 (Cn) (3)
BC(Cv(ws ), Cv(w2 )) _
N
E
</equation>
<page confidence="0.8725595">
n=1
111
</page>
<table confidence="0.9985785">
Corpus: IT-EN-W ES-EN-W NL-EN-W NL-EN-W+EP
Method Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10
Direct-SWR .501 .576 .740 .332 .437 .675 .186 .254 .423 .344 .450 .652
Topic-BC .578 .667 .834 .433 .576 .843 .237 .314 .489 .534 .630 .836
TI+Cue .597 .702 .897 .429 .569 .828 .225 .296 .459 .446 .569 .808
Response-BC .622 .729 .882 .517 .635 .891 .236 .320 .511 .574 .653 .864
</table>
<tableCaption confidence="0.997111">
Table 2: BLE performance of all the methods for Italian-English, Spanish-English and Dutch-English (with 2 different
corpora utilized for the training of bilingual LDA and the estimation of semantic word responses for Dutch-English).
</tableCaption>
<bodyText confidence="0.9900166">
For the Topic-BC method N = K, while N =
JV S U VTJ for Response-BC. Additionally, since
P(zkJwi) &gt; 0 and P(wkJwi) &gt; 0 for each zk E i
and each wk E V S U VT , a lot of probability mass
is assigned to topics and semantic responses that
are completely irrelevant to the given word. Re-
ducing the dimensionality of the semantic repre-
sentation a posteriori to only a smaller number of
most important semantic axes in the semantic spaces
should decrease the effects of that statistical noise,
and even more firmly emphasize the latent corre-
lation among words. The utility of such semantic
space truncating or feature pruning in monolingual
settings (Reisinger and Mooney, 2010) was also de-
tected previously for LSA and LDA-based models
(Landauer and Dumais, 1997; Griffiths et al., 2007).
Therefore, unless noted otherwise, we perform all
our calculations over the best scoring 200 cross-
lingual topics and the best scoring 2000 semantic
word responses.3
</bodyText>
<subsectionHeader confidence="0.986875">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9998876">
Ground truth translation pairs.4 Since our task
is bilingual lexicon extraction, we designed a set
of ground truth one-to-one translation pairs for all
3 language pairs as follows. For Dutch-English
and Spanish-English, we randomly sampled a set
of Dutch (Spanish) nouns from our Wikipedia cor-
pora. Following that, we used the Google Trans-
late tool plus an additional annotator to translate
those words to English. The annotator manually
revised the lists and retained only words that have
</bodyText>
<footnote confidence="0.724024875">
3The values are set empirically. Calculating similarity
Sim(ws , w2 ) may be interpreted as: “Given word ws detect
how similar word w2 is to the word ws .” Therefore, when
calculating Sim(w1 ,w2), even when dealing with symmetric
similarity functions such as BC, we always consider only the
scores P(·|ws ) for truncating.
4Available online: http://people.cs.kuleuven.be
/-ivan.vulic/software/
</footnote>
<bodyText confidence="0.99881576">
their corresponding translation in the English vo-
cabulary. Additionally, only one possible translation
was annotated as correct. When more than 1 trans-
lation is possible, the annotator marked as correct
the translation that occurs more frequently in the En-
glish Wikipedia data. Finally, we built a set of 1000
one-to-one translation pairs for Dutch-English and
Spanish-English. The same procedure was followed
for Italian-English, but there we obtained the ground
truth one-to-one translation pairs for 1000 most fre-
quent Italian nouns in order to test the effect of word
frequency on the quality of semantic word responses
and the overall lexicon quality.
Evaluation metrics. All the methods under con-
sideration actually retrieve ranked lists of semanti-
cally similar words that could be observed as poten-
tial translation candidates. We measure the perfor-
mance on BLE as Top M accuracy (AccM). It de-
notes the number of source words from ground truth
translation pairs whose top M semantically simi-
lar words contain the correct translation according
to our ground truth over the total number of ground
truth translation pairs (=1000) (Tamura et al., 2012).
Additionally, we compute the mean reciprocal rank
(MRR) scores (Voorhees, 1999).
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.998814">
Table 2 displays the performance of each compared
method on the BLE task. It shows the difference in
results for different language pairs and different cor-
pora used to extract latent cross-lingual topics and
estimate the lists of semantic word responses. Ex-
ample lists of semantically similar words over all 3
language pairs are shown in Table 3. Based on these
results, we are able to derive several conclusions:
(i) Response-BC performs consistently better than
the other 3 methods over all corpora and all language
pairs. It is more robust and is able to find some
cross-lingual similarities omitted by the other meth-
</bodyText>
<page confidence="0.993646">
112
</page>
<table confidence="0.998969923076923">
Italian-English (IT-EN) Spanish-English (ES-EN) Dutch-English (NL-EN)
(1) affresco (2) spigolo (3) coppa (1) caza (2) discurso (3) comprador (1) behoud (2) schroef (3) spar
(fresco) (edge) (cup) (hunting) (speech) (buyer) (conservation) (screw) (fir)
fresco polyhedron club hunting rhetoric purchase conservation socket conifer
mural polygon competition hunt oration seller preservation wire pine
nave vertices final hunter speech tariff heritage wrap firewood
wall diagonal champion hound discourse market diversity wrench seedling
testimonial edge football safari dialectic bidding emphasis screw weevil
apse vertex trophy huntsman rhetorician auction consequence pin chestnut
rediscovery binomial team wildlife oratory bid danger fastener acorn
draughtsman solid relegation animal wisdom microeconomics contribution torque girth
ceiling graph tournament ungulate oration trade decline pipe lumber
palace modifier soccer chase persuasion listing framework routing bark
</table>
<tableCaption confidence="0.980479333333333">
Table 3: Example lists of top 10 semantically similar words across all 3 language pairs according to our Response-BC
similarity method, where the correct translation word is: (col. 1) found as the most similar word, (2) contained lower
in the list, and (3) not found in the top 10 words.
</tableCaption>
<table confidence="0.918237428571428">
IT-EN ES-EN NL-EN
direttore-director flauta-flute kustlijn-coastline
radice-root eficacia-efficacy begrafenis-funeral
sintomo-symptom empleo-employment mengsel-mixture
perdita-loss descubierta-discovery lijm-glue
danno-damage desalojo-eviction kijker-viewer
battaglione-battalion miedo-fear oppervlak-surface
</table>
<tableCaption confidence="0.997609">
Table 4: Example translations found by the Response-BC
method, but missed by the other 3 methods.
</tableCaption>
<bodyText confidence="0.987714175438597">
ods (see Table 4). The overall quality of the cross-
lingual word similarities and lexicons extracted by
the method is dependent on the quality of estimated
semantic response vectors. The quality of these
vectors is of course further dependent on the qual-
ity of multilingual training data. For instance, for
Dutch-English, we may observe a rather spectacular
increase in overall scores (the tests are performed
over the same set of 1000 words) when we aug-
ment Wikipedia data with Europarl data (compare
the scores for NL-EN-W and NL-EN-W+EP).
(ii) A transition from a semantic space spanned by
cross-lingual topics (Topic-BC) to a semantic space
spanned by vocabulary words (Response-BC) leads
to better results over all corpora and language pairs.
The difference is less visible when using training
data of lesser quality (the scores for NL-EN-W).
Moreover, since the shared space of cross-lingual
topics is used to obtain and quantify semantic word
responses, the quality of learned cross-lingual topics
influences the quality of semantic word responses.
If the semantic coherence of the cross-lingual top-
ical space is unsatisfying, the method is unable to
generate good semantic response vectors, and ul-
timately unable to correctly identify semantically
similar words across languages.
(iii) Due to its modeling properties that assign more
importance to high-frequency words, Direct-SWR
produces reasonable results in the BLE task only for
high-frequency words (see results for IT-EN-W). Al-
though Eq. (2) models the concept of semantic word
responding in a sound way (Griffiths et al., 2007),
using the semantic word responses directly is not
suitable for the actual BLE task.
(iv) The effect of word frequency is clearly visi-
ble when comparing the results obtained on IT-EN-
W with the results obtained on the other Wikipedia
corpora. High-frequency words produce more re-
dundancies in training data that are captured by sta-
tistical models such as latent topic models. High-
frequency words then obtain better estimates of their
semantic response vectors which consequently leads
to better overall scores. The effect of word fre-
quency on statistical methods in the BLE task was
investigated before (Pekar et al., 2006; Prochasson
and Fung, 2011; Tamura et al., 2012), and we also
confirm their findings.
(v) Unlike (Koehn and Knight, 2002; Haghighi et
al., 2008), our response-based method does not rely
on any orthographic features such as cognates or
words shared across languages. It is a pure statis-
tical method that only relies on word distributions
over a multilingual corpus. Based on these distribu-
tions, it performs the initial shallow semantic analy-
sis of the corpus by means of a multilingual prob-
abilistic model. The method then builds, via the
concept of semantic word responding, a language-
</bodyText>
<page confidence="0.997411">
113
</page>
<bodyText confidence="0.999927833333333">
independent semantic space spanned by all vocabu-
lary words/responses in both languages. That makes
the method portable to distant language pairs. How-
ever, for similar languages, including more evidence
such as orthographic clues might lead to further in-
crease in scores, but we leave that for future work.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999970133333333">
We have proposed a new statistical approach to iden-
tifying semantically similar words across languages
that relies on the paradigm of semantic word re-
sponding previously defined in cognitive science.
The proposed approach is robust and does not make
any additional language-pair dependent assumptions
(e.g., it does not rely on a seed lexicon, orthographic
clues or predefined concept categories). That effec-
tively makes it applicable to any language pair. Our
experiments on the task of bilingual lexicon extrac-
tion for a variety of language pairs have proved that
the response-based approach is more robust and out-
performs the methods that operate in the semantic
space of latent concepts (e.g., cross-lingual topics)
directly.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992225">
We would like to thank Steven Bethard and the
anonymous reviewers for their useful suggestions.
This research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research Fund,
KU Leuven, Belgium.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991115">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
NAACL-HLT, pages 19–27.
Daniel Andrade, Tetsuya Nasukawa, and Junichi Tsujii.
2010. Robust measurement and comparison of context
similarity for finding translation pairs. In Proceedings
of COLING, pages 19–27.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference for
topic models. In Proceedings of UAI, pages 27–34.
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In Proceedings of SI-
GIR, pages 84–91.
A. Bhattacharyya. 1943. On a measure of divergence be-
tween two statistical populations defined by their prob-
ability distributions. Bulletin of the Calcutta Mathe-
matical Society, 35:199–209.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of UAI, pages 75–82.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263–311.
Sung-Hyuk Cha. 2007. Comprehensive survey on
distance/similarity measures between probability den-
sity functions. International Journal of Mathematical
Models and Methods in Applied Sciences, 1(4):300–
307.
Hal Daum´e III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of ACL, pages 407–412.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the Web using in-
terlingual topic modeling. In CIKM Workshop on So-
cial Web Search and Mining (SWSM), pages 57–64.
Herv´e D´ejean, Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of COLING, pages 1–7.
Georgiana Dinu and Mirella Lapata. 2010. Topic models
for meaning similarity in context. In Proceedings of
COLING, pages 250–258.
Susan T. Dumais, Thomas K. Landauer, and Michael
Littman. 1996. Automatic cross-linguistic informa-
tion retrieval using Latent Semantic Indexing. In Pro-
ceedings of the SIGIR Workshop on Cross-Linguistic
Information Retrieval, pages 16–23.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
EMNLP, pages 57–63.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of COLING, pages 414–420.
Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herv´e D´ejean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In Proceedings of ACL, pages 526–533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211–244.
</reference>
<page confidence="0.994817">
114
</page>
<reference confidence="0.999866855769231">
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771–779.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
Samer Hassan and Rada Mihalcea. 2009. Cross-lingual
semantic relatedness using encyclopedic knowledge.
In Proceedings of EMNLP, pages 1192–1201.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proceedings of SIGIR, pages 50–57.
Jagadeesh Jagarlamudi and Hal Daum´e III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Proceedings of ECIR, pages 444–456.
Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki
Murata, and Kentaro Torisawa. 2010. A Bayesian
method for robust estimation of distributional similar-
ities. In Proceedings of ACL, pages 247–256.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition, pages
9–16.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit, pages 79–86.
Thomas K. Landauer and Susan T. Dumais. 1997. Solu-
tions to Plato’s problem: The Latent Semantic Analy-
sis theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211–
240.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In Proceedings of
COLING, pages 617–625.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of ACL, pages 25–32.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. Information Processing and
Management, 41:523–547.
Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011.
Investigating task performance of probabilistic topic
models: an empirical study of PLSA and LDA. In-
formation Retrieval, 14(2):178–203.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of EMNLP,
pages 880–889.
Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual terminology mining -
using brain, not brawn comparable corpora. In Pro-
ceedings of ACL, pages 664–671.
Douglas L. Nelson, Cathy L. McEvoy, and Simon Den-
nis. 2000. What is free association and what does it
measure? Memory and Cognition, 28:887–899.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
Proceedings of WWW, pages 1155–1156.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2011. Cross lingual text classification by mining mul-
tilingual topics from Wikipedia. In Proceedings of
WSDM, pages 375–384.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding translations for low-
frequency words in comparable corpora. Machine
Translation, 20(4):247–266.
John C. Platt, Kristina Toutanova, and Wen-Tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of EMNLP,
pages 251–261.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned comparable
documents. In Proceedings ofACL, pages 1327–1335.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL, pages 519–526.
Joseph Reisinger and Raymond J. Mooney. 2010. A
mixture model with sharing for lexical semantics. In
Proceedings of EMNLP, pages 1173–1182.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424–440.
Mark Steyvers, Richard M. Shiffrin, and Douglas L. Nel-
son. 2004. Word association spaces for predicting se-
mantic similarity effects in episodic memory. In Ex-
perimental Cognitive Psychology and Its Applications,
pages 237–249.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In Proceedings of
EMNLP, pages 24–36.
Ellen M. Voorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of TREC, pages 77–
82.
Ivan Vuli´c, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from comparable
corpora using latent topic models. In Proceedings of
ACL, pages 479–484.
</reference>
<page confidence="0.988724">
115
</page>
<reference confidence="0.998885857142857">
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proceedings
of ACL, pages 1128–1137.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of ACL, pages 55–
63.
</reference>
<page confidence="0.999015">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.540618">
<title confidence="0.9970625">Cross-Lingual Semantic Similarity of Words as the Similarity of Semantic Word Responses</title>
<author confidence="0.995565">Ivan Vuli´c</author>
<author confidence="0.995565">Marie-Francine</author>
<affiliation confidence="0.961869333333333">Department of Computer KU Celestijnenlaan</affiliation>
<address confidence="0.605748">Leuven,</address>
<abstract confidence="0.999474925925926">We propose a new approach to identifying semantically similar words across languages. The approach is based on an idea that two words in different languages are similar if they are likely to generate similar words (which includes both source and target language words) as their top semantic word responses. Semantic word responding is a concept from cognitive science which addresses detecting most likely words that humans output as free word associations given some cue word. The method consists of two main steps: (1) it utilizes a probabilistic multilingual topic model trained on comparable data to learn and quantify the semantic word responses, (2) it provides ranked lists of similar words according to the similarity of their semantic word response vectors. We evaluate our approach in the task of bilingual lexicon extraction (BLE) for a variety of language pairs. We show that in the cross-lingual settings without any language pair dependent knowledge the response-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>pages</pages>
<contexts>
<context position="7738" citStr="Agirre et al., 2009" startWordPosition="1215" endWordPosition="1218">t translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models suc</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of NAACL-HLT, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Andrade</author>
<author>Tetsuya Nasukawa</author>
<author>Junichi Tsujii</author>
</authors>
<title>Robust measurement and comparison of context similarity for finding translation pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>pages</pages>
<contexts>
<context position="7585" citStr="Andrade et al., 2010" startWordPosition="1190" endWordPosition="1193">cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of</context>
</contexts>
<marker>Andrade, Nasukawa, Tsujii, 2010</marker>
<rawString>Daniel Andrade, Tetsuya Nasukawa, and Junichi Tsujii. 2010. Robust measurement and comparison of context similarity for finding translation pairs. In Proceedings of COLING, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Asuncion</author>
<author>Max Welling</author>
<author>Padhraic Smyth</author>
<author>Yee Whye Teh</author>
</authors>
<title>On smoothing and inference for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of UAI,</booktitle>
<pages>27--34</pages>
<contexts>
<context position="24084" citStr="Asuncion et al., 2009" startWordPosition="3933" endWordPosition="3936">ibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (Pt� 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and Q = 0.01. We are aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned cross-lingual topics, but that analysis is out of the scope of this paper. 4.3 Compared Methods We evaluate and compare the following word similarity approaches in all our experiments: 1) The method that regards the lists of semantic word responses across languages obtained by Eq. (2) directly as the lists of semantically similar words (Direct-SWR). 2) The state-of-the-art method that employs a similarity function (SF) on the K-dimensional word vectors cv(wi) in the semantic space of latent crosslingual topics. The dimensions o</context>
</contexts>
<marker>Asuncion, Welling, Smyth, Teh, 2009</marker>
<rawString>Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. 2009. On smoothing and inference for topic models. In Proceedings of UAI, pages 27–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Ballesteros</author>
<author>W Bruce Croft</author>
</authors>
<title>Phrasal translation and query expansion techniques for crosslanguage information retrieval.</title>
<date>1997</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>84--91</pages>
<contexts>
<context position="1981" citStr="Ballesteros and Croft, 1997" startWordPosition="285" endWordPosition="288">te in the semantic space of latent cross-lingual concepts/topics. 1 Introduction Cross-lingual semantic word similarity addresses the task of detecting words that refer to similar semantic concepts and convey similar meanings across languages. It ultimately boils down to the automatic identification of translation pairs, that is, bilingual lexicon extraction (BLE). Such lexicons and semantically similar words serve as important resources in cross-lingual knowledge induction (e.g., Zhao et al. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic</context>
</contexts>
<marker>Ballesteros, Croft, 1997</marker>
<rawString>Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal translation and query expansion techniques for crosslanguage information retrieval. In Proceedings of SIGIR, pages 84–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bhattacharyya</author>
</authors>
<title>On a measure of divergence between two statistical populations defined by their probability distributions.</title>
<date>1943</date>
<journal>Bulletin of the Calcutta Mathematical Society,</journal>
<pages>35--199</pages>
<contexts>
<context position="25104" citStr="Bhattacharyya, 1943" startWordPosition="4094" endWordPosition="4095">rds (Direct-SWR). 2) The state-of-the-art method that employs a similarity function (SF) on the K-dimensional word vectors cv(wi) in the semantic space of latent crosslingual topics. The dimensions of the vectors are conditional topic distribution scores P(zk|wi) that are obtained by the multilingual topic model directly (Steyvers and Griffiths, 2007; Vuli´c et al., 2011). We have tested different SF-s (e.g., the Kullback-Leibler and the Jensen-Shannon divergence, the cosine measure), and have detected that in general the best scores are obtained when using the Bhattacharyya coefficient (BC) (Bhattacharyya, 1943; Kazama et al., 2010) (Topic-BC). 3) The best scoring similarity method from Vuli´c et al. (2011) named TI+Cue. This state-of-the-art method also operates in the semantic space of latent cross-lingual concepts/topics. 4) The response-based similarity described in Sect. 3. As for Topic-BC, we again use BC as the similarity function, but now on |V S U VT |-dimensional context vectors in the semantic space spanned by all words in both vocabularies that represent semantic word responses (Response-BC). Given two Ndimensional word vectors cv(wS1 ) and cv(wT2 ), the BC or the fidelity measure (Cha, </context>
</contexts>
<marker>Bhattacharyya, 1943</marker>
<rawString>A. Bhattacharyya. 1943. On a measure of divergence between two statistical populations defined by their probability distributions. Bulletin of the Calcutta Mathematical Society, 35:199–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8391" citStr="Blei et al., 2003" startWordPosition="1319" endWordPosition="1322">(Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more robust models of cross-lingual</context>
<context position="22925" citStr="Blei et al., 2003" startWordPosition="3736" endWordPosition="3739">s the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. We retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Again following their setup, we use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. 4.2 Multilingual Topic Model The multilingual probabilistic topic model we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining se</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Multilingual topic models for unaligned text.</title>
<date>2009</date>
<booktitle>In Proceedings of UAI,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="8616" citStr="Boyd-Graber and Blei, 2009" startWordPosition="1355" endWordPosition="1358">es methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more robust models of cross-lingual semantic word similarity. 3 Modeling Word Similarity as the Similarity of Semantic Word Responses This section contains a detailed description of our semantic word similarity method that relies on semantic word responses. Si</context>
</contexts>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber and David M. Blei. 2009. Multilingual topic models for unaligned text. In Proceedings of UAI, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2146" citStr="Brown et al., 1993" startWordPosition="311" endWordPosition="314">milar semantic concepts and convey similar meanings across languages. It ultimately boils down to the automatic identification of translation pairs, that is, bilingual lexicon extraction (BLE). Such lexicons and semantically similar words serve as important resources in cross-lingual knowledge induction (e.g., Zhao et al. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with simil</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung-Hyuk Cha</author>
</authors>
<title>Comprehensive survey on distance/similarity measures between probability density functions.</title>
<date>2007</date>
<journal>International Journal of Mathematical Models and Methods in Applied Sciences,</journal>
<volume>1</volume>
<issue>4</issue>
<pages>307</pages>
<contexts>
<context position="3663" citStr="Cha, 2007" startWordPosition="568" endWordPosition="569"> with vocabulary V S and wT2 in the target language LT with vocabulary V T is then: Sim(wS1 , wT2 ) = SF(cv(wS1 ), cv(wT2 )) (1) cv(wS1 ) = [scS1 (c1), ... , scS1 (cN)] denotes a context vector for wS1 with N context features ck, where scS1 (ck) denotes the score for wS1 associated with context feature ck (similar for wT2 ). SF is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e</context>
<context position="21016" citStr="Cha, 2007" startWordPosition="3442" endWordPosition="3443"> English word pie from V T are treated as two different word types. In that case, it holds |V S U VT |= |V S |+ |V T |. lingual semantic space spanned by all vocabulary words in both languages. Each feature corresponds to one word from vocabularies V S and V T, while the exact score for each feature in the context vector cv(wi) is precisely the probability that this word/feature will be generated as a word response given word wi. The degree of similarity between two words is then computed on the basis of similarity between their feature vectors using some of the standard similarity functions (Cha, 2007). The novel response-based approach of similarity removes the effect of high-frequency words that tend to appear higher in the lists of semantic word responses. Therefore, the real synonyms and translations should occur as top candidates in the lists of similar words obtained by the response-based method. That property may be exploited to identify one-to-one translations across languages and build a bilingual lexicon (see Table 1). 4 Experimental Setup 4.1 Data Collections We work with the following corpora: 110 • IT-EN-W: A collection of 18,898 ItalianEnglish Wikipedia article pairs previousl</context>
<context position="25709" citStr="Cha, 2007" startWordPosition="4192" endWordPosition="4193"> 1943; Kazama et al., 2010) (Topic-BC). 3) The best scoring similarity method from Vuli´c et al. (2011) named TI+Cue. This state-of-the-art method also operates in the semantic space of latent cross-lingual concepts/topics. 4) The response-based similarity described in Sect. 3. As for Topic-BC, we again use BC as the similarity function, but now on |V S U VT |-dimensional context vectors in the semantic space spanned by all words in both vocabularies that represent semantic word responses (Response-BC). Given two Ndimensional word vectors cv(wS1 ) and cv(wT2 ), the BC or the fidelity measure (Cha, 2007) is defined as: V SCf (Cn) - SC2 (Cn) (3) BC(Cv(ws ), Cv(w2 )) _ N E n=1 111 Corpus: IT-EN-W ES-EN-W NL-EN-W NL-EN-W+EP Method Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Acc1 MRR Acc10 Direct-SWR .501 .576 .740 .332 .437 .675 .186 .254 .423 .344 .450 .652 Topic-BC .578 .667 .834 .433 .576 .843 .237 .314 .489 .534 .630 .836 TI+Cue .597 .702 .897 .429 .569 .828 .225 .296 .459 .446 .569 .808 Response-BC .622 .729 .882 .517 .635 .891 .236 .320 .511 .574 .653 .864 Table 2: BLE performance of all the methods for Italian-English, Spanish-English and Dutch-English (with 2 different corpora utilized </context>
</contexts>
<marker>Cha, 2007</marker>
<rawString>Sung-Hyuk Cha. 2007. Comprehensive survey on distance/similarity measures between probability density functions. International Journal of Mathematical Models and Methods in Applied Sciences, 1(4):300– 307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>407--412</pages>
<marker>Daum´e, Jagarlamudi, 2011</marker>
<rawString>Hal Daum´e III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of ACL, pages 407–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wim De Smet</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Crosslanguage linking of news stories on the Web using interlingual topic modeling.</title>
<date>2009</date>
<booktitle>In CIKM Workshop on Social Web Search and Mining (SWSM),</booktitle>
<pages>57--64</pages>
<marker>De Smet, Moens, 2009</marker>
<rawString>Wim De Smet and Marie-Francine Moens. 2009. Crosslanguage linking of news stories on the Web using interlingual topic modeling. In CIKM Workshop on Social Web Search and Mining (SWSM), pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herv´e D´ejean</author>
<author>Eric Gaussier</author>
<author>Fatia Sadat</author>
</authors>
<title>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1--7</pages>
<marker>D´ejean, Gaussier, Sadat, 2002</marker>
<rawString>Herv´e D´ejean, Eric Gaussier, and Fatia Sadat. 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. In Proceedings of COLING, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for meaning similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>250--258</pages>
<contexts>
<context position="23643" citStr="Dinu and Lapata, 2010" startWordPosition="3855" endWordPosition="3858">tails regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (Pt� 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and Q = 0.01. We are aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned cross-lingual topics, but that analysis is out of the scope of this paper. 4.3 Compared Meth</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Topic models for meaning similarity in context. In Proceedings of COLING, pages 250–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>Michael Littman</author>
</authors>
<title>Automatic cross-linguistic information retrieval using Latent Semantic Indexing.</title>
<date>1996</date>
<booktitle>In Proceedings of the SIGIR Workshop on Cross-Linguistic Information Retrieval,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="4178" citStr="Dumais et al., 1996" startWordPosition="643" endWordPosition="646">tional Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(wi) and cv(w2) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word association. We borrow that concept from the psychology/cognitive science literature. Semantic word responding addresses a task that requires particip</context>
</contexts>
<marker>Dumais, Landauer, Littman, 1996</marker>
<rawString>Susan T. Dumais, Thomas K. Landauer, and Michael Littman. 1996. Automatic cross-linguistic information retrieval using Latent Semantic Indexing. In Proceedings of the SIGIR Workshop on Cross-Linguistic Information Retrieval, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Percy Cheung</author>
</authors>
<title>Mining verynon-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>57--63</pages>
<contexts>
<context position="7520" citStr="Fung and Cheung, 2004" startWordPosition="1178" endWordPosition="1181">a variety of language pairs. 2 Related Work When dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et</context>
</contexts>
<marker>Fung, Cheung, 2004</marker>
<rawString>Pascale Fung and Percy Cheung. 2004. Mining verynon-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM. In Proceedings of EMNLP, pages 57–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>414--420</pages>
<contexts>
<context position="7485" citStr="Fung and Yee, 1998" startWordPosition="1172" endWordPosition="1175">and results on the BLE task for a variety of language pairs. 2 Related Work When dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarc</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of COLING, pages 414–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gaussier</author>
<author>Jean-Michel Renders</author>
<author>Irina Matveeva</author>
<author>Cyril Goutte</author>
<author>Herv´e D´ejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>526--533</pages>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>Eric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Herv´e D´ejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In Proceedings of ACL, pages 526–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="8131" citStr="Griffiths et al. (2007)" startWordPosition="1277" endWordPosition="1280">Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidat</context>
<context position="14348" citStr="Griffiths et al., 2007" startWordPosition="2337" endWordPosition="2340"> wT2 in LT that has the highest similarity score among all words in LT should be a direct word-to-word translation of wS1 . 3.3 Modeling Semantic Word Responses via Cross-Lingual Topics Cross-lingual topics provide a sound framework to construct a probabilistic model of the aforementioned experiment. To model semantic word responses via the shared space of cross-lingual topics, we have to set a probabilistic mass that quantifies the degree of association. Given two words w1, w2 E V S U VT, a natural way of expressing the asymmetric semantic association is by modeling the probability P(w2|w1) (Griffiths et al., 2007), that is, the probability to generate word w2 as a response given word w1. After the training of a multilingual topic model on a multilingual corpus, we obtain per-topic word distributions with scores PS(wSi |zk) and PT(wTi |zk) (see Sect. 3.1).1 The probability 1A remark on notation throughout the paper: Since the shared space of cross-lingual topics allows us to construct a uniform representation for all words regardless of a vocabulary they belong to, due to simplicity and to stress the uniformity, P(w2|w1) is then decomposed as follows: Resp(w1, w2) = P(w2|w1) = The probability scores P(w</context>
<context position="23619" citStr="Griffiths et al., 2007" startWordPosition="3851" endWordPosition="3854">Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (Pt� 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and Q = 0.01. We are aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned cross-lingual topics, but that analysis is out of the scope of this </context>
<context position="27191" citStr="Griffiths et al., 2007" startWordPosition="4450" endWordPosition="4453"> a lot of probability mass is assigned to topics and semantic responses that are completely irrelevant to the given word. Reducing the dimensionality of the semantic representation a posteriori to only a smaller number of most important semantic axes in the semantic spaces should decrease the effects of that statistical noise, and even more firmly emphasize the latent correlation among words. The utility of such semantic space truncating or feature pruning in monolingual settings (Reisinger and Mooney, 2010) was also detected previously for LSA and LDA-based models (Landauer and Dumais, 1997; Griffiths et al., 2007). Therefore, unless noted otherwise, we perform all our calculations over the best scoring 200 crosslingual topics and the best scoring 2000 semantic word responses.3 4.4 Evaluation Ground truth translation pairs.4 Since our task is bilingual lexicon extraction, we designed a set of ground truth one-to-one translation pairs for all 3 language pairs as follows. For Dutch-English and Spanish-English, we randomly sampled a set of Dutch (Spanish) nouns from our Wikipedia corpora. Following that, we used the Google Translate tool plus an additional annotator to translate those words to English. The</context>
<context position="33401" citStr="Griffiths et al., 2007" startWordPosition="5370" endWordPosition="5373">ty of learned cross-lingual topics influences the quality of semantic word responses. If the semantic coherence of the cross-lingual topical space is unsatisfying, the method is unable to generate good semantic response vectors, and ultimately unable to correctly identify semantically similar words across languages. (iii) Due to its modeling properties that assign more importance to high-frequency words, Direct-SWR produces reasonable results in the BLE task only for high-frequency words (see results for IT-EN-W). Although Eq. (2) models the concept of semantic word responding in a sound way (Griffiths et al., 2007), using the semantic word responses directly is not suitable for the actual BLE task. (iv) The effect of word frequency is clearly visible when comparing the results obtained on IT-ENW with the results obtained on the other Wikipedia corpora. High-frequency words produce more redundancies in training data that are captured by statistical models such as latent topic models. Highfrequency words then obtain better estimates of their semantic response vectors which consequently leads to better overall scores. The effect of word frequency on statistical methods in the BLE task was investigated befo</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="2471" citStr="Haghighi et al., 2008" startWordPosition="364" endWordPosition="367">. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, wS1 given in the source language LS with vocabulary V</context>
<context position="4255" citStr="Haghighi et al., 2008" startWordPosition="656" endWordPosition="659">ectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(wi) and cv(w2) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word association. We borrow that concept from the psychology/cognitive science literature. Semantic word responding addresses a task that requires participants to produce first words that come to their mind that are related to a pre</context>
<context position="7819" citStr="Haghighi et al., 2008" startWordPosition="1230" endWordPosition="1233">y to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al</context>
<context position="22439" citStr="Haghighi et al., 2008" startWordPosition="3657" endWordPosition="3660"> corpus augmented with 6,206 Dutch-English document pairs from Europarl (Koehn, 2005). Although Europarl is a parallel corpus, no explicit use is made of sentence-level alignments. All corpora are theme-aligned, that is, the aligned document pairs discuss similar subjects, but are in general not direct translations (except the Europarl document pairs). NL-EN-W+EP serves to test whether better semantic responses could be learned from data of higher quality, and to measure how it affects the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. We retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Again following their setup, we use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. 4.2 Multilingual Topic Model The multilingual probabilistic topic model we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding th</context>
<context position="34168" citStr="Haghighi et al., 2008" startWordPosition="5495" endWordPosition="5498">omparing the results obtained on IT-ENW with the results obtained on the other Wikipedia corpora. High-frequency words produce more redundancies in training data that are captured by statistical models such as latent topic models. Highfrequency words then obtain better estimates of their semantic response vectors which consequently leads to better overall scores. The effect of word frequency on statistical methods in the BLE task was investigated before (Pekar et al., 2006; Prochasson and Fung, 2011; Tamura et al., 2012), and we also confirm their findings. (v) Unlike (Koehn and Knight, 2002; Haghighi et al., 2008), our response-based method does not rely on any orthographic features such as cognates or words shared across languages. It is a pure statistical method that only relies on word distributions over a multilingual corpus. Based on these distributions, it performs the initial shallow semantic analysis of the corpus by means of a multilingual probabilistic model. The method then builds, via the concept of semantic word responding, a language113 independent semantic space spanned by all vocabulary words/responses in both languages. That makes the method portable to distant language pairs. However,</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="2710" citStr="Harris, 1954" startWordPosition="401" endWordPosition="402">asis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, wS1 given in the source language LS with vocabulary V S and wT2 in the target language LT with vocabulary V T is then: Sim(wS1 , wT2 ) = SF(cv(wS1 ), cv(wT2 )) (1) cv(wS1 ) = [scS1 (c1), ... , scS1 (cN)] denotes a context vector for wS1 with N context features ck, where scS1 (ck) denotes the</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Rada Mihalcea</author>
</authors>
<title>Cross-lingual semantic relatedness using encyclopedic knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1192--1201</pages>
<contexts>
<context position="2498" citStr="Hassan and Mihalcea, 2009" startWordPosition="368" endWordPosition="371">machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, wS1 given in the source language LS with vocabulary V S and wT2 in the target la</context>
<context position="7716" citStr="Hassan and Mihalcea, 2009" startWordPosition="1211" endWordPosition="1214"> across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabil</context>
</contexts>
<marker>Hassan, Mihalcea, 2009</marker>
<rawString>Samer Hassan and Rada Mihalcea. 2009. Cross-lingual semantic relatedness using encyclopedic knowledge. In Proceedings of EMNLP, pages 1192–1201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="8363" citStr="Hofmann, 1999" startWordPosition="1314" endWordPosition="1316">raphic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more ro</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of SIGIR, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
</authors>
<title>Extracting multilingual topics from unaligned comparable corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of ECIR,</booktitle>
<pages>444--456</pages>
<marker>Jagarlamudi, Daum´e, 2010</marker>
<rawString>Jagadeesh Jagarlamudi and Hal Daum´e III. 2010. Extracting multilingual topics from unaligned comparable corpora. In Proceedings of ECIR, pages 444–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Stijn De Saeger</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Kentaro Torisawa</author>
</authors>
<title>A Bayesian method for robust estimation of distributional similarities.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>247--256</pages>
<marker>Kazama, De Saeger, Kuroda, Murata, Torisawa, 2010</marker>
<rawString>Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki Murata, and Kentaro Torisawa. 2010. A Bayesian method for robust estimation of distributional similarities. In Proceedings of ACL, pages 247–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="7796" citStr="Koehn and Knight, 2002" startWordPosition="1226" endWordPosition="1229">d over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Add</context>
<context position="22416" citStr="Koehn and Knight, 2002" startWordPosition="3653" endWordPosition="3656"> NL-EN-W+EP: The NL-EN-W corpus augmented with 6,206 Dutch-English document pairs from Europarl (Koehn, 2005). Although Europarl is a parallel corpus, no explicit use is made of sentence-level alignments. All corpora are theme-aligned, that is, the aligned document pairs discuss similar subjects, but are in general not direct translations (except the Europarl document pairs). NL-EN-W+EP serves to test whether better semantic responses could be learned from data of higher quality, and to measure how it affects the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. We retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Again following their setup, we use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. 4.2 Multilingual Topic Model The multilingual probabilistic topic model we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For t</context>
<context position="34144" citStr="Koehn and Knight, 2002" startWordPosition="5491" endWordPosition="5494">s clearly visible when comparing the results obtained on IT-ENW with the results obtained on the other Wikipedia corpora. High-frequency words produce more redundancies in training data that are captured by statistical models such as latent topic models. Highfrequency words then obtain better estimates of their semantic response vectors which consequently leads to better overall scores. The effect of word frequency on statistical methods in the BLE task was investigated before (Pekar et al., 2006; Prochasson and Fung, 2011; Tamura et al., 2012), and we also confirm their findings. (v) Unlike (Koehn and Knight, 2002; Haghighi et al., 2008), our response-based method does not rely on any orthographic features such as cognates or words shared across languages. It is a pure statistical method that only relies on word distributions over a multilingual corpus. Based on these distributions, it performs the initial shallow semantic analysis of the corpus by means of a multilingual probabilistic model. The method then builds, via the concept of semantic word responding, a language113 independent semantic space spanned by all vocabulary words/responses in both languages. That makes the method portable to distant </context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In ACL Workshop on Unsupervised Lexical Acquisition, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="21903" citStr="Koehn, 2005" startWordPosition="3578" endWordPosition="3579">d by the response-based method. That property may be exploited to identify one-to-one translations across languages and build a bilingual lexicon (see Table 1). 4 Experimental Setup 4.1 Data Collections We work with the following corpora: 110 • IT-EN-W: A collection of 18,898 ItalianEnglish Wikipedia article pairs previously used by Vuli´c et al. (2011). • ES-EN-W: A collection of 13,696 SpanishEnglish Wikipedia article pairs. • NL-EN-W: A collection of 7,612 DutchEnglish Wikipedia article pairs. • NL-EN-W+EP: The NL-EN-W corpus augmented with 6,206 Dutch-English document pairs from Europarl (Koehn, 2005). Although Europarl is a parallel corpus, no explicit use is made of sentence-level alignments. All corpora are theme-aligned, that is, the aligned document pairs discuss similar subjects, but are in general not direct translations (except the Europarl document pairs). NL-EN-W+EP serves to test whether better semantic responses could be learned from data of higher quality, and to measure how it affects the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>Solutions to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>240</pages>
<contexts>
<context position="27166" citStr="Landauer and Dumais, 1997" startWordPosition="4446" endWordPosition="4449"> i and each wk E V S U VT , a lot of probability mass is assigned to topics and semantic responses that are completely irrelevant to the given word. Reducing the dimensionality of the semantic representation a posteriori to only a smaller number of most important semantic axes in the semantic spaces should decrease the effects of that statistical noise, and even more firmly emphasize the latent correlation among words. The utility of such semantic space truncating or feature pruning in monolingual settings (Reisinger and Mooney, 2010) was also detected previously for LSA and LDA-based models (Landauer and Dumais, 1997; Griffiths et al., 2007). Therefore, unless noted otherwise, we perform all our calculations over the best scoring 200 crosslingual topics and the best scoring 2000 semantic word responses.3 4.4 Evaluation Ground truth translation pairs.4 Since our task is bilingual lexicon extraction, we designed a set of ground truth one-to-one translation pairs for all 3 language pairs as follows. For Dutch-English and Spanish-English, we randomly sampled a set of Dutch (Spanish) nouns from our Wikipedia corpora. Following that, we used the Google Translate tool plus an additional annotator to translate th</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. Solutions to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Laroche</author>
<author>Philippe Langlais</author>
</authors>
<title>Revisiting context-based projection methods for term-translation spotting in comparable corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>617--625</pages>
<contexts>
<context position="4009" citStr="Laroche and Langlais, 2010" startWordPosition="618" endWordPosition="621">is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(wi) and cv(w2) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word respo</context>
</contexts>
<marker>Laroche, Langlais, 2010</marker>
<rawString>Audrey Laroche and Philippe Langlais. 2010. Revisiting context-based projection methods for term-translation spotting in comparable corpora. In Proceedings of COLING, pages 617–625.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="3651" citStr="Lee, 1999" startWordPosition="566" endWordPosition="567">language LS with vocabulary V S and wT2 in the target language LT with vocabulary V T is then: Sim(wS1 , wT2 ) = SF(cv(wS1 ), cv(wT2 )) (1) cv(wS1 ) = [scS1 (c1), ... , scS1 (cN)] denotes a context vector for wS1 with N context features ck, where scS1 (ck) denotes the score for wS1 associated with context feature ck (similar for wT2 ). SF is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., </context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
<author>Douglas W Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Dictionary-based techniques for cross-language information retrieval.</title>
<date>2005</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>41--523</pages>
<contexts>
<context position="2002" citStr="Levow et al., 2005" startWordPosition="289" endWordPosition="292">atent cross-lingual concepts/topics. 1 Introduction Cross-lingual semantic word similarity addresses the task of detecting words that refer to similar semantic concepts and convey similar meanings across languages. It ultimately boils down to the automatic identification of translation pairs, that is, bilingual lexicon extraction (BLE). Such lexicons and semantically similar words serve as important resources in cross-lingual knowledge induction (e.g., Zhao et al. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from</context>
</contexts>
<marker>Levow, Oard, Resnik, 2005</marker>
<rawString>Gina-Anne Levow, Douglas W. Oard, and Philip Resnik. 2005. Dictionary-based techniques for cross-language information retrieval. Information Processing and Management, 41:523–547.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Lu</author>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA.</title>
<date>2011</date>
<journal>Information Retrieval,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="24102" citStr="Lu et al., 2011" startWordPosition="3937" endWordPosition="3940">ing. In a typical setting for mining semantically similar words using latent topic models in both monolingual (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (Pt� 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and Q = 0.01. We are aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned cross-lingual topics, but that analysis is out of the scope of this paper. 4.3 Compared Methods We evaluate and compare the following word similarity approaches in all our experiments: 1) The method that regards the lists of semantic word responses across languages obtained by Eq. (2) directly as the lists of semantically similar words (Direct-SWR). 2) The state-of-the-art method that employs a similarity function (SF) on the K-dimensional word vectors cv(wi) in the semantic space of latent crosslingual topics. The dimensions of the vectors are </context>
</contexts>
<marker>Lu, Mei, Zhai, 2011</marker>
<rawString>Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011. Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval, 14(2):178–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>880--889</pages>
<contexts>
<context position="8563" citStr="Mimno et al., 2009" startWordPosition="1346" endWordPosition="1349">e pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more robust models of cross-lingual semantic word similarity. 3 Modeling Word Similarity as the Similarity of Semantic Word Responses This section contains a detailed description of our semantic word similar</context>
<context position="22966" citStr="Mimno et al., 2009" startWordPosition="3743" endWordPosition="3746">d the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. We retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Again following their setup, we use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. 4.2 Multilingual Topic Model The multilingual probabilistic topic model we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent top</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of EMNLP, pages 880–889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>B´eatrice Daille</author>
<author>Koichi Takeuchi</author>
<author>Kyo Kageura</author>
</authors>
<title>Bilingual terminology mining -using brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>664--671</pages>
<contexts>
<context position="7563" citStr="Morin et al., 2007" startWordPosition="1186" endWordPosition="1189">en dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (20</context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and Kyo Kageura. 2007. Bilingual terminology mining -using brain, not brawn comparable corpora. In Proceedings of ACL, pages 664–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Simon Dennis</author>
</authors>
<title>What is free association and what does it measure? Memory and Cognition,</title>
<date>2000</date>
<pages>28--887</pages>
<contexts>
<context position="4891" citStr="Nelson et al., 2000" startWordPosition="762" endWordPosition="765">Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(wi) and cv(w2) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word association. We borrow that concept from the psychology/cognitive science literature. Semantic word responding addresses a task that requires participants to produce first words that come to their mind that are related to a presented cue word (Nelson et al., 2000; Steyvers et al., 2004). The new cross-lingual semantic space is spanned by all vocabulary words in the source and the target language. Each axis in the space denotes a semantic word response. The similarity between two words is then computed as the similarity between the vectors comprising their semantic word responses using any of existing 5F-s. Two words are considered semantically similar if they are likely to generate similar semantic word responses and assign similar importance to them. We utilize a shared semantic space of latent crosslingual topics learned by a multilingual probabilis</context>
<context position="15883" citStr="Nelson et al., 2000" startWordPosition="2592" endWordPosition="2595">he semantically related topics. Using the shared space of cross-lingual topics, semantic response scores can be derived for any two words w1, w2 E V S U VT .1 The generative model closely resembles the actual process in the human brain - when we generate semantic word responses, we first tend to associate that word with a related semantic/cognitive concept, in this case a cross-lingual topic (the factor P(zk|w1)), and then, after establishing the concept, we output a list of words that we consider the most prominent/descriptive for that concept (words with high scores in the factor P(w2|zk)) (Nelson et al., 2000; Steyvers et al., 2004). Due to such modeling properties, this model of semantic word responding tends to assign higher association scores for high frequency words. It eventually leads to asymmetric associations/responses. We have detected that phenomenon both monolingually and across languages. For instance, the first response to Spanish word mutaci´on (mutation) is English word gene. Other examples include caldera (boiler)-steam, deportista (sportsman)-sport, horario (schedule)-hour or pescador (fisherman)-fish. In the other association direction, we have detected top responses such as merc</context>
</contexts>
<marker>Nelson, McEvoy, Dennis, 2000</marker>
<rawString>Douglas L. Nelson, Cathy L. McEvoy, and Simon Dennis. 2000. What is free association and what does it measure? Memory and Cognition, 28:887–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Jian Hu</author>
<author>Zheng Chen</author>
</authors>
<title>Mining multilingual topics from Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>1155--1156</pages>
<contexts>
<context position="8633" citStr="Ni et al., 2009" startWordPosition="1359" endWordPosition="1362">such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more robust models of cross-lingual semantic word similarity. 3 Modeling Word Similarity as the Similarity of Semantic Word Responses This section contains a detailed description of our semantic word similarity method that relies on semantic word responses. Since the method ut</context>
<context position="22983" citStr="Ni et al., 2009" startWordPosition="3747" endWordPosition="3750">uced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. We retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Again following their setup, we use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. 4.2 Multilingual Topic Model The multilingual probabilistic topic model we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both</context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2009</marker>
<rawString>Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2009. Mining multilingual topics from Wikipedia. In Proceedings of WWW, pages 1155–1156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Jian Hu</author>
<author>Zheng Chen</author>
</authors>
<title>Cross lingual text classification by mining multilingual topics from Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of WSDM,</booktitle>
<pages>375--384</pages>
<contexts>
<context position="23452" citStr="Ni et al. (2011)" startWordPosition="3825" endWordPosition="3828">orward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (Pt� 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and Q = 0.01. We are aware that different hyper-parameter</context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2011</marker>
<rawString>Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2011. Cross lingual text classification by mining multilingual topics from Wikipedia. In Proceedings of WSDM, pages 375–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1912" citStr="Och and Ney, 2003" startWordPosition="277" endWordPosition="280">erforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics. 1 Introduction Cross-lingual semantic word similarity addresses the task of detecting words that refer to similar semantic concepts and convey similar meanings across languages. It ultimately boils down to the automatic identification of translation pairs, that is, bilingual lexicon extraction (BLE). Such lexicons and semantically similar words serve as important resources in cross-lingual knowledge induction (e.g., Zhao et al. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Pekar</author>
<author>Ruslan Mitkov</author>
<author>Dimitar Blagoev</author>
<author>Andrea Mulloni</author>
</authors>
<title>Finding translations for lowfrequency words in comparable corpora.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="34023" citStr="Pekar et al., 2006" startWordPosition="5471" endWordPosition="5474">ing the semantic word responses directly is not suitable for the actual BLE task. (iv) The effect of word frequency is clearly visible when comparing the results obtained on IT-ENW with the results obtained on the other Wikipedia corpora. High-frequency words produce more redundancies in training data that are captured by statistical models such as latent topic models. Highfrequency words then obtain better estimates of their semantic response vectors which consequently leads to better overall scores. The effect of word frequency on statistical methods in the BLE task was investigated before (Pekar et al., 2006; Prochasson and Fung, 2011; Tamura et al., 2012), and we also confirm their findings. (v) Unlike (Koehn and Knight, 2002; Haghighi et al., 2008), our response-based method does not rely on any orthographic features such as cognates or words shared across languages. It is a pure statistical method that only relies on word distributions over a multilingual corpus. Based on these distributions, it performs the initial shallow semantic analysis of the corpus by means of a multilingual probabilistic model. The method then builds, via the concept of semantic word responding, a language113 independe</context>
</contexts>
<marker>Pekar, Mitkov, Blagoev, Mulloni, 2006</marker>
<rawString>Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and Andrea Mulloni. 2006. Finding translations for lowfrequency words in comparable corpora. Machine Translation, 20(4):247–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
<author>Kristina Toutanova</author>
<author>Wen-Tau Yih</author>
</authors>
<title>Translingual document representations from discriminative projections.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>251--261</pages>
<contexts>
<context position="23434" citStr="Platt et al. (2010)" startWordPosition="3821" endWordPosition="3824">we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (Pt� 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and Q = 0.01. We are aware that differe</context>
</contexts>
<marker>Platt, Toutanova, Yih, 2010</marker>
<rawString>John C. Platt, Kristina Toutanova, and Wen-Tau Yih. 2010. Translingual document representations from discriminative projections. In Proceedings of EMNLP, pages 251–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Prochasson</author>
<author>Pascale Fung</author>
</authors>
<title>Rare word translation extraction from aligned comparable documents.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1327--1335</pages>
<contexts>
<context position="2547" citStr="Prochasson and Fung, 2011" startWordPosition="376" endWordPosition="379">s-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, wS1 given in the source language LS with vocabulary V S and wT2 in the target language LT with vocabulary V T is then: Sim(wS1 , </context>
<context position="22467" citStr="Prochasson and Fung, 2011" startWordPosition="3661" endWordPosition="3664">6,206 Dutch-English document pairs from Europarl (Koehn, 2005). Although Europarl is a parallel corpus, no explicit use is made of sentence-level alignments. All corpora are theme-aligned, that is, the aligned document pairs discuss similar subjects, but are in general not direct translations (except the Europarl document pairs). NL-EN-W+EP serves to test whether better semantic responses could be learned from data of higher quality, and to measure how it affects the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. We retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Again following their setup, we use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. 4.2 Multilingual Topic Model The multilingual probabilistic topic model we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, gene</context>
<context position="34050" citStr="Prochasson and Fung, 2011" startWordPosition="5475" endWordPosition="5478">d responses directly is not suitable for the actual BLE task. (iv) The effect of word frequency is clearly visible when comparing the results obtained on IT-ENW with the results obtained on the other Wikipedia corpora. High-frequency words produce more redundancies in training data that are captured by statistical models such as latent topic models. Highfrequency words then obtain better estimates of their semantic response vectors which consequently leads to better overall scores. The effect of word frequency on statistical methods in the BLE task was investigated before (Pekar et al., 2006; Prochasson and Fung, 2011; Tamura et al., 2012), and we also confirm their findings. (v) Unlike (Koehn and Knight, 2002; Haghighi et al., 2008), our response-based method does not rely on any orthographic features such as cognates or words shared across languages. It is a pure statistical method that only relies on word distributions over a multilingual corpus. Based on these distributions, it performs the initial shallow semantic analysis of the corpus by means of a multilingual probabilistic model. The method then builds, via the concept of semantic word responding, a language113 independent semantic space spanned b</context>
</contexts>
<marker>Prochasson, Fung, 2011</marker>
<rawString>Emmanuel Prochasson and Pascale Fung. 2011. Rare word translation extraction from aligned comparable documents. In Proceedings ofACL, pages 1327–1335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>519--526</pages>
<contexts>
<context position="3958" citStr="Rapp, 1999" startWordPosition="612" endWordPosition="613"> feature ck (similar for wT2 ). SF is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(wi) and cv(w2) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic spa</context>
<context position="7497" citStr="Rapp, 1999" startWordPosition="1176" endWordPosition="1177">LE task for a variety of language pairs. 2 Related Work When dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources.</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of ACL, pages 519–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>A mixture model with sharing for lexical semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1173--1182</pages>
<contexts>
<context position="27081" citStr="Reisinger and Mooney, 2010" startWordPosition="4432" endWordPosition="4435"> U VTJ for Response-BC. Additionally, since P(zkJwi) &gt; 0 and P(wkJwi) &gt; 0 for each zk E i and each wk E V S U VT , a lot of probability mass is assigned to topics and semantic responses that are completely irrelevant to the given word. Reducing the dimensionality of the semantic representation a posteriori to only a smaller number of most important semantic axes in the semantic spaces should decrease the effects of that statistical noise, and even more firmly emphasize the latent correlation among words. The utility of such semantic space truncating or feature pruning in monolingual settings (Reisinger and Mooney, 2010) was also detected previously for LSA and LDA-based models (Landauer and Dumais, 1997; Griffiths et al., 2007). Therefore, unless noted otherwise, we perform all our calculations over the best scoring 200 crosslingual topics and the best scoring 2000 semantic word responses.3 4.4 Evaluation Ground truth translation pairs.4 Since our task is bilingual lexicon extraction, we designed a set of ground truth one-to-one translation pairs for all 3 language pairs as follows. For Dutch-English and Spanish-English, we randomly sampled a set of Dutch (Spanish) nouns from our Wikipedia corpora. Following</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. A mixture model with sharing for lexical semantics. In Proceedings of EMNLP, pages 1173–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="22707" citStr="Schmid, 1994" startWordPosition="3705" endWordPosition="3706">t are in general not direct translations (except the Europarl document pairs). NL-EN-W+EP serves to test whether better semantic responses could be learned from data of higher quality, and to measure how it affects the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. We retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Again following their setup, we use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. 4.2 Multilingual Topic Model The multilingual probabilistic topic model we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated bef</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<date>2007</date>
<booktitle>Probabilistic topic models. Handbook of Latent Semantic Analysis,</booktitle>
<pages>427--7</pages>
<contexts>
<context position="8166" citStr="Steyvers and Griffiths (2007)" startWordPosition="1282" endWordPosition="1285">, 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings wi</context>
<context position="23984" citStr="Steyvers and Griffiths (2007)" startWordPosition="3915" endWordPosition="3918">nd it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (Pt� 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and Q = 0.01. We are aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned cross-lingual topics, but that analysis is out of the scope of this paper. 4.3 Compared Methods We evaluate and compare the following word similarity approaches in all our experiments: 1) The method that regards the lists of semantic word responses across languages obtained by Eq. (2) directly as the lists of semantically similar words (Direct-SWR). 2) The state-of-the-art method that employs a similarity function (SF) on the K-d</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2007. Probabilistic topic models. Handbook of Latent Semantic Analysis, 427(7):424–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Richard M Shiffrin</author>
<author>Douglas L Nelson</author>
</authors>
<title>Word association spaces for predicting semantic similarity effects in episodic memory.</title>
<date>2004</date>
<booktitle>In Experimental Cognitive Psychology and Its Applications,</booktitle>
<pages>237--249</pages>
<contexts>
<context position="4915" citStr="Steyvers et al., 2004" startWordPosition="766" endWordPosition="769">li´c et al., 2011). Context vectors cv(wi) and cv(w2) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word association. We borrow that concept from the psychology/cognitive science literature. Semantic word responding addresses a task that requires participants to produce first words that come to their mind that are related to a presented cue word (Nelson et al., 2000; Steyvers et al., 2004). The new cross-lingual semantic space is spanned by all vocabulary words in the source and the target language. Each axis in the space denotes a semantic word response. The similarity between two words is then computed as the similarity between the vectors comprising their semantic word responses using any of existing 5F-s. Two words are considered semantically similar if they are likely to generate similar semantic word responses and assign similar importance to them. We utilize a shared semantic space of latent crosslingual topics learned by a multilingual probabilistic topic model to obtai</context>
<context position="15907" citStr="Steyvers et al., 2004" startWordPosition="2596" endWordPosition="2599">ed topics. Using the shared space of cross-lingual topics, semantic response scores can be derived for any two words w1, w2 E V S U VT .1 The generative model closely resembles the actual process in the human brain - when we generate semantic word responses, we first tend to associate that word with a related semantic/cognitive concept, in this case a cross-lingual topic (the factor P(zk|w1)), and then, after establishing the concept, we output a list of words that we consider the most prominent/descriptive for that concept (words with high scores in the factor P(w2|zk)) (Nelson et al., 2000; Steyvers et al., 2004). Due to such modeling properties, this model of semantic word responding tends to assign higher association scores for high frequency words. It eventually leads to asymmetric associations/responses. We have detected that phenomenon both monolingually and across languages. For instance, the first response to Spanish word mutaci´on (mutation) is English word gene. Other examples include caldera (boiler)-steam, deportista (sportsman)-sport, horario (schedule)-hour or pescador (fisherman)-fish. In the other association direction, we have detected top responses such as merchant-comercio (trade) or</context>
</contexts>
<marker>Steyvers, Shiffrin, Nelson, 2004</marker>
<rawString>Mark Steyvers, Richard M. Shiffrin, and Douglas L. Nelson. 2004. Word association spaces for predicting semantic similarity effects in episodic memory. In Experimental Cognitive Psychology and Its Applications, pages 237–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using label propagation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>24--36</pages>
<contexts>
<context position="4031" citStr="Tamura et al., 2012" startWordPosition="622" endWordPosition="625">g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(wi) and cv(w2) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word ass</context>
<context position="7607" citStr="Tamura et al., 2012" startWordPosition="1194" endWordPosition="1197"> word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods 107 developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association</context>
<context position="29411" citStr="Tamura et al., 2012" startWordPosition="4794" endWordPosition="4797">s in order to test the effect of word frequency on the quality of semantic word responses and the overall lexicon quality. Evaluation metrics. All the methods under consideration actually retrieve ranked lists of semantically similar words that could be observed as potential translation candidates. We measure the performance on BLE as Top M accuracy (AccM). It denotes the number of source words from ground truth translation pairs whose top M semantically similar words contain the correct translation according to our ground truth over the total number of ground truth translation pairs (=1000) (Tamura et al., 2012). Additionally, we compute the mean reciprocal rank (MRR) scores (Voorhees, 1999). 5 Results and Discussion Table 2 displays the performance of each compared method on the BLE task. It shows the difference in results for different language pairs and different corpora used to extract latent cross-lingual topics and estimate the lists of semantic word responses. Example lists of semantically similar words over all 3 language pairs are shown in Table 3. Based on these results, we are able to derive several conclusions: (i) Response-BC performs consistently better than the other 3 methods over all</context>
<context position="34072" citStr="Tamura et al., 2012" startWordPosition="5479" endWordPosition="5482"> suitable for the actual BLE task. (iv) The effect of word frequency is clearly visible when comparing the results obtained on IT-ENW with the results obtained on the other Wikipedia corpora. High-frequency words produce more redundancies in training data that are captured by statistical models such as latent topic models. Highfrequency words then obtain better estimates of their semantic response vectors which consequently leads to better overall scores. The effect of word frequency on statistical methods in the BLE task was investigated before (Pekar et al., 2006; Prochasson and Fung, 2011; Tamura et al., 2012), and we also confirm their findings. (v) Unlike (Koehn and Knight, 2002; Haghighi et al., 2008), our response-based method does not rely on any orthographic features such as cognates or words shared across languages. It is a pure statistical method that only relies on word distributions over a multilingual corpus. Based on these distributions, it performs the initial shallow semantic analysis of the corpus by means of a multilingual probabilistic model. The method then builds, via the concept of semantic word responding, a language113 independent semantic space spanned by all vocabulary words</context>
</contexts>
<marker>Tamura, Watanabe, Sumita, 2012</marker>
<rawString>Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2012. Bilingual lexicon extraction from comparable corpora using label propagation. In Proceedings of EMNLP, pages 24–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>The TREC-8 question answering track report.</title>
<date>1999</date>
<booktitle>In Proceedings of TREC,</booktitle>
<pages>77--82</pages>
<contexts>
<context position="29492" citStr="Voorhees, 1999" startWordPosition="4807" endWordPosition="4808">ses and the overall lexicon quality. Evaluation metrics. All the methods under consideration actually retrieve ranked lists of semantically similar words that could be observed as potential translation candidates. We measure the performance on BLE as Top M accuracy (AccM). It denotes the number of source words from ground truth translation pairs whose top M semantically similar words contain the correct translation according to our ground truth over the total number of ground truth translation pairs (=1000) (Tamura et al., 2012). Additionally, we compute the mean reciprocal rank (MRR) scores (Voorhees, 1999). 5 Results and Discussion Table 2 displays the performance of each compared method on the BLE task. It shows the difference in results for different language pairs and different corpora used to extract latent cross-lingual topics and estimate the lists of semantic word responses. Example lists of semantically similar words over all 3 language pairs are shown in Table 3. Based on these results, we are able to derive several conclusions: (i) Response-BC performs consistently better than the other 3 methods over all corpora and all language pairs. It is more robust and is able to find some cross</context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>Ellen M. Voorhees. 1999. The TREC-8 question answering track report. In Proceedings of TREC, pages 77– 82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Wim De Smet</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Identifying word translations from comparable corpora using latent topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>479--484</pages>
<marker>Vuli´c, De Smet, Moens, 2011</marker>
<rawString>Ivan Vuli´c, Wim De Smet, and Marie-Francine Moens. 2011. Identifying word translations from comparable corpora using latent topic models. In Proceedings of ACL, pages 479–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duo Zhang</author>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Cross-lingual latent topic extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1128--1137</pages>
<contexts>
<context position="8688" citStr="Zhang et al., 2010" startWordPosition="1368" endWordPosition="1371">pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more robust models of cross-lingual semantic word similarity. 3 Modeling Word Similarity as the Similarity of Semantic Word Responses This section contains a detailed description of our semantic word similarity method that relies on semantic word responses. Since the method utilizes the concept of multilingual probabilistic topic </context>
</contexts>
<marker>Zhang, Mei, Zhai, 2010</marker>
<rawString>Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010. Cross-lingual latent topic extraction. In Proceedings of ACL, pages 1128–1137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Yan Song</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Cross language dependency parsing using a bilingual lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="1858" citStr="Zhao et al. (2009)" startWordPosition="270" endWordPosition="273">nse-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics. 1 Introduction Cross-lingual semantic word similarity addresses the task of detecting words that refer to similar semantic concepts and convey similar meanings across languages. It ultimately boils down to the automatic identification of translation pairs, that is, bilingual lexicon extraction (BLE). Such lexicons and semantically similar words serve as important resources in cross-lingual knowledge induction (e.g., Zhao et al. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi</context>
</contexts>
<marker>Zhao, Song, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 2009. Cross language dependency parsing using a bilingual lexicon. In Proceedings of ACL, pages 55– 63.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>