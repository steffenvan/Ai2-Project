<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.994798">
An effective Discourse Parser that uses Rich Linguistic Information
</title>
<author confidence="0.889498">
Rajen Subba ∗
</author>
<affiliation confidence="0.562791">
Display Advertising Sciences
Yahoo! Labs
</affiliation>
<address confidence="0.48145">
Sunnyvale, CA, USA
</address>
<email confidence="0.643463">
rajen@yahoo-inc.com
</email>
<author confidence="0.997027">
Barbara Di Eugenio
</author>
<affiliation confidence="0.999539">
Department of Computer Science
University of Illinois
</affiliation>
<address confidence="0.516265">
Chicago, IL, USA
</address>
<email confidence="0.997889">
bdieugen@cs.uic.edu
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995195">
This paper presents a first-order logic learn-
ing approach to determine rhetorical relations
between discourse segments. Beyond lin-
guistic cues and lexical information, our ap-
proach exploits compositional semantics and
segment discourse structure data. We report
a statistically significant improvement in clas-
sifying relations over attribute-value learn-
ing paradigms such as Decision Trees, RIP-
PER and Naive Bayes. For discourse pars-
ing, our modified shift-reduce parsing model
that uses our relation classifier significantly
outperforms a right-branching majority-class
baseline.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997968673913044">
Many theories postulate a hierarchical structure for
discourse (Mann and Thompson, 1988; Moser et.
al., 1996; Polanyi et. al., 2004). Discourse struc-
ture is most often based on semantic / pragmatic re-
lationships between spans of text and results in a tree
structure, as that shown in Figure 1. Discourse
parsing, namely, deriving such tree structures and
the rhetorical relations labeling their inner nodes is
still a challenging and mostly unsolved problem in
NLP. It is linguistically plausible that such structures
are determined at least in part on the basis of the
meaning of the related chunks of texts, and of the
rhetorical intentions of their authors. However, such
knowledge is extremely difficult to capture. Hence,
previous work on discourse parsing (Wellner et. al.,
2006; Sporleder and Lascarides, 2005; Marcu, 2000;
Polanyi et. al., 2004; Soricut and Marcu, 2003;
∗This work was done while the author was a student at the
University of Illinois at Chicago.
Baldridge and Lascarides, 2005) has relied only on
syntactic and lexical information, lexical chains and
shallow semantics.
We present an innovative discourse parser that
uses compositional semantics (when available) and
information on the structure of the segment being
built itself. Our discourse parser, based on a modi-
fied shift-reduce algorithm, crucially uses a rhetori-
cal relation classifier to determine the site of attach-
ment of a new incoming chunk together with the ap-
propriate relation label. Another novel aspect of our
work is the usage of Inductive Logic Programming
(ILP): ILP learns from first-order logic representa-
tions (FOL). The ILP-based relation classifier is
significantly more accurate than relation classifiers
that use competitive propositional ML algorithms
such as decision trees and Naive Bayes. In addi-
tion, it results in FOL rules that are linguistically
perspicuous. Our domain is that of instructional
how-to-do manuals, and we describe our corpus
in Section 2. In Section 3, we discuss the modified
shift-reduce parser we developed. The bulk of the
paper is devoted to the rhetorical relation classifier
in Section 4. Experimental results of both the rela-
tion classifier and the discourse parser in its entirety
are discussed in Section 5. Further details can be
found in (Subba, 2008).
</bodyText>
<sectionHeader confidence="0.993994" genericHeader="method">
2 Discourse Annotated Instructional
Corpus
</sectionHeader>
<bodyText confidence="0.9531035">
Existing corpora annotated with rhetorical relations
(Carlson et. al., 2003; Wolf and Gibson, 2005;
Prasad et. al., 2008) focus primarily on news arti-
cles. However, for us the development of the dis-
course parser is parasitic on our ultimate goal: de-
veloping resources and algorithms for language in-
</bodyText>
<page confidence="0.972321">
566
</page>
<note confidence="0.898992">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 566–574,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<equation confidence="0.505780166666667">
s1e1-s5e2
ddddddddddddddddddddddddddddddddddddd Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z
general:specific
s1e1 s2e1-s5e2
ggggggggggggggggggggggg WWWWWWWWWWWWWWWWWWWWWWW
preparation:act
</equation>
<table confidence="0.951970705882353">
Another way .. s2e1-s4e1 s5e1-s5e2
.. sheets. llllllllllllllll RRRRRRRR s3e2 x s5e2
s2e1 s2e1-s3e2 RR RRRRRR F panel.
Because RR preparation:act s4e1 s5e1 xxact:goal
sheets preparation:act RRRRRR Then lay Using x
llllllllllllllll RRRRRRRR s3e1-s3e2 the sheet .. pattern, the .. mark the
s2e1-s2e2 FF .. the panel.
x FFF xxxxxxxxxx FFFF it
x FFF disjunction FFFF
x the or cut
reason:act F .. .. blade.
x FF
x F
s2e2 s3e1
these you can Mark
.. tape one .. opening
.. panels, .. panel. .. sheet,
</table>
<figureCaption confidence="0.998936">
Figure 1: Discourse Parse Tree of the Text in Example (1)
</figureCaption>
<bodyText confidence="0.996714454545454">
terfaces to instructional applications. Hence, we are
interested in working with instructional texts. We
worked with a corpus on home-repair that is about
5MB in size and is made up entirely of written En-
glish instructions,1 such as that shown in Exam-
ple (1). The text has been manually segmented
into Elementary Discourse Units (EDUs), the small-
est units of discourse. In total, our corpus contains
176 documents with an average of 32.6 EDUs for a
total of 5744 EDUs and 53,250 words. The structure
for Example (1) is shown in Figure 1.
(1) [Another way to measure and mark panels for
cutting is to make a template from the protec-
tive sheets.(s1e1)] [Because these sheets are the
same size as the panels,(s2e1)] [you can tape
one to the wall as though it were a panel.(s2e2)]
[Mark the opening on the sheet(s3e1)] [or cut
it out with a razor blade.(s3e2)] [Then lay the
sheet on the panel.(s4e1)] [Using the template
as a pattern,(s5e1)] [mark the panel.(s5e2)]
To explore our hypothesis, that rich linguistic in-
formation helps discourse parsing, and that the state
</bodyText>
<footnote confidence="0.997619">
1The raw corpus was originally assembled at the Informa-
tion Technology Research Institute, University of Brighton.
</footnote>
<bodyText confidence="0.999843666666667">
of the art in machine learning supports such an
approach, we needed training data annotated with
both compositional semantics and rhetorical rela-
tions. We performed the first type of annotation al-
most completely automatically, and the second man-
ually, as we turn now to describing.
</bodyText>
<subsectionHeader confidence="0.988153">
2.1 Compositional Semantics Derivation
</subsectionHeader>
<bodyText confidence="0.999513722222222">
The type of compositional semantics we are inter-
ested in is heavily rooted in verb semantics, which
is particularly appropriate for the instructional text
we are working with. Therefore, we used VerbNet
(Kipper et. al., 2000) as our verb lexicon. VerbNet
groups together verbs that undergo the same syn-
tactic alternations and share similar semantics. It
accounts for 4962 distinct verbs classified into 237
main classes. Each verb class is described by the-
matic roles, selectional restrictions on the arguments
and frames consisting of a syntactic description and
semantic predicates. Such semantic classification of
verbs can be helpful in making generalizations, es-
pecially when data is not abundant. Generalization
can also be achieved by means of the semantic pred-
icates. Although the verb classes of two verb in-
stances may differ, semantic predicates are shared
across verbs. To compositionally build verb based
</bodyText>
<page confidence="0.994598">
567
</page>
<bodyText confidence="0.999928666666667">
semantic representations of our EDUs, we (Subba
et al., 2006) integrated a robust parser, LCFLEX
(Ros´e, 2000), with a lexicon and ontology based
both on VerbNet and, for nouns, on CoreLex (Buite-
laar, 1998). The augmented parser was able to de-
rive complete semantic representations for 3257 of
the 5744 EDUs (56.7%). The only manual step was
to pick the correct parse from a forest of parse trees,
since the output of the parser can be ambiguous.
</bodyText>
<subsectionHeader confidence="0.999521">
2.2 Rhetorical relation annotation
</subsectionHeader>
<bodyText confidence="0.9998995">
The discourse processing community has not yet
reached agreement on an inventory of rhetorical re-
lations. Among the many choices, our coding
scheme is a hybrid of (Moser et. al., 1996) and
(Marcu, 1999). We focused on what we call infor-
mational relations, namely, relations in the domain.
We used 26 relations, divided into 5 broad classes:
12 causal relations (e.g., preparation:act, goal:act,
cause:effect, step1:step2); 6 elaboration relations
(e.g., general:specific, set:member, object:attribute;
3 similarity relations (contrast1:contrast2, com-
parison, restatement); 2 temporal relations (co-
temp1:co-temp2, before:after); and 4 other rela-
tions, including joint and disjunction.
The annotation yielded 5172 relations, with rea-
sonable intercoder agreement. On 26% of the data,
we obtained n = 0.66; n rises to 0.78 when the two
most commonly confused relations, preparation:act
and step1:step2, are consolidated. We also anno-
tated the relata as nucleus (more important mem-
ber) and satellite (contributing member(s)) (Mann
and Thompson, 1988), with n = 0.67.2 The most
frequent relation is preparation:act (24.46%), and in
general, causal relations are more frequently used in
our instructional corpus than in news corpora (Carl-
son et. al., 2003; Wolf and Gibson, 2005).
</bodyText>
<sectionHeader confidence="0.958732" genericHeader="method">
3 Shift-Reduce Discourse Parsing
</sectionHeader>
<bodyText confidence="0.9698504">
Our discourse parser is a modified version of a shift-
reduce parser. The shift operation places the next
segment on top of the stack, TOP. The reduce oper-
ation will attach the text segment at TOP to the text
segment at TOP-1. (Marcu, 2000) also uses a shift-
reduce parser, though our parsing algorithm differs
2We don’t have space to explain why we annotate for nu-
cleus and satellite, even if (Moser et. al., 1996) argue that this
sort of distinction does not apply to informational relations.
in two respects: 1) we do not learn shift operations
and 2) in contrast to (Marcu, 2000), the attachment
of an incoming text segment to the emerging tree
may occur at any node on the right frontier. This al-
lows for the more sophisticated type of adjunction
operations required for discourse parsing as mod-
eled in D-LTAG (Webber, 2004). A reduce op-
eration is determined by the relation identification
component. We check if a relation exists between
the incoming text segment and the attachment points
on the right frontier. If more than one attachment
site exists, then the attachment site for which the rule
with the highest score fired (see below) is chosen for
the reduce operation. A reduce operation can fur-
ther trigger additional reduce operations if there is
more than one tree left in the stack after the first re-
duce operation. When no rules fire, a shift occurs.
In the event that all the segments in the input list
have been processed and a full DPT has not been
obtained, then we reduce TOP and TOP-1 using the
joint relation until a single DPT is built.
</bodyText>
<sectionHeader confidence="0.966436" genericHeader="method">
4 Classifying Rhetorical Relations
</sectionHeader>
<bodyText confidence="0.999889291666667">
Identifying the informational relations between text
segments is central to our approach for building the
informational tree structure of text. We believe that
the use of a limited knowledge representation for-
malism, essentially propositional logic, is not ad-
equate and that a relational model that can handle
compositional semantics is necessary. We cast the
problem of determining informational relations as a
classification task. We used the ILP system Aleph
that is based on (Muggleton, 1995). Formulation
of any problem within the ILP framework consists
of background knowledge B and the set of exam-
ples E (E+∪ E−). In our ILP framework, positive
examples are ground clauses describing a relation
and its relata, e.g. relation(s5e1,s5e2,act:goal), or
relation(s2e1-s3e2,s4e1,preparation:act) from Fig-
ure 1. If a is a positive example of a relation r, then
it is also a negative example for all the other rela-
tions.
Background Knowledge (B) can be thought of as
features used by ILP to learn rules, as in traditional
attribute-value learning algorithms. We use the fol-
lowing information to learn rules for classifying re-
lations. Figure 2 shows a sample of the background
</bodyText>
<page confidence="0.982087">
568
</page>
<figure confidence="0.5673735">
Verbs + Nouns: verb(’s5e2’,mark). noun(’s5e2’,panel).
Linguistic Cues: firstWordPOS(’s5e2’,’VB’). lastWordPOS(’s5e2’,’.’).
Similarity: segment sim score(’s5e1’,’s5e2’,0.0).
verbclass(’s5e2’,mark,’image impression-25.1’).
agent(’s5e2’,frame(mark),you).
Compositional Semantics: destination(’s5e2’,frame(mark),panel).
cause(’s5e2’,frame(mark),you,’s5e2-mark-e’).
prep(’s5e2’,frame(mark),end(’s5e2-mark-e’),mark,panel).
created image(’s5e2’,frame(mark),result(’s5e2-mark-e’),mark).
Structural Information: same sentence(’s5e1’,’s5e2’).
</figure>
<figureCaption confidence="0.998659">
Figure 2: Example Background Knowledge
</figureCaption>
<bodyText confidence="0.999864490196079">
knowledge provided for EDU s5e2.
Verbs + Nouns: These features were derived by
tagging all the sentences in the corpus with a POS
tagger (Brill, 1995).
WordNet: For each noun in our data, we also use
information on hypernymy and meronymy relations
using WordNet. In a sense, this captures the domain
relations between objects in our data.
Linguistic Cues: Various cues can facilitate the
inference of informational relations, even if it is well
known that they are based solely on the content of
the text segments, various cues can facilitate the in-
ference of such relations. At the same time, it is
well known that relations are often non signalled:
in our corpus, only 43% of relations are signalled,
consistently with figures from the literature (44%
in (Williams and Reiter, 2003) and 45% in (Prasad
et. al., 2008)). Besides lexical cues such as but,
and and if, we also include modals, tense, compara-
tives and superlatives, and negation. E.g., wrong-act
in relations like prescribe-act:wrong-act is often ex-
pressed using a negation.
Similarity: For the two segments in question, we
compute the cosine similarity of the segments using
only nouns and verbs.
Compositional semantics: the semantic infor-
mation derived by our parser, as described in Sec-
tion 2.1. The semantic representation of segment
s5e2 from Example (1) is shown in Figure 2. Each
semantic predicate is a feature for the classifier.
Structural Information: For relations between
two EDUs, we use knowledge of whether the two
EDUs are intra-sentential or inter-sentential, since
some relations, e.g. criterion:act, are more likely to
be realized intra-sententially than inter-sententially.
For larger segments, we also encode the hierarchi-
cal representation of text segments that contain more
than one nucleus, the distance between the nuclei
of the two segments and any relations that exist be-
tween the smaller inner segments.
At this point, the attentive reader will be wonder-
ing how we encode compositional semantics for re-
lations relating text segments larger than one EDU.
Clearly we cannot just list the semantics of each
EDU that is dominated by the larger segment. We
follow the intuition that nuclei represent the most
important portions of segments (Mann and Thomp-
son, 1988). For segments such as s5e1-s5e2 that
contains a single nucleus, we simply reduce the se-
mantic content of the larger segment to that of its
nucleus:
</bodyText>
<equation confidence="0.6074464">
s5e1-s5e2
verb(’s5e1-s5e2’,mark).
...
verbclass(’s5e1-s5e2’,..).
agent(’s5e1-s5e2’,..).
</equation>
<bodyText confidence="0.9999245">
In this case, the semantics of the complex text seg-
ment is represented by the compositional semantics
of the single most important EDU.
For segments that contain more than one nu-
cleus, such as s3e1-s3e2, the discourse struc-
ture information of the segment is represented with
the additional predicates internal relation and par-
ent segment. These predicates can be used recur-
sively at every level of the tree to specify the relation
between the most important segments. In addition,
they also provide a means to represent the compo-
sitional semantics of the most important EDUs and
</bodyText>
<page confidence="0.991498">
569
</page>
<bodyText confidence="0.839544">
make them available to the relational learning algo-
</bodyText>
<equation confidence="0.78109025">
rithm.
s3e1-s3e2
internal relation(s3e1,s3e2,’disjunction’).
parent segment(s3e1-s3e2,s3e1).
parent segment(s3e1-s3e2,s3e2).
�
��������������������� ��������������������
verb(’s3e1’,mark). verb(’s3e2’,cut).
noun(’s3e1’,opening). noun(’s3e2’,opening).
... ...
verbclass(’s3e1’,..). noun(’s3e1’,blade).
theme(’s3e1’,..).
</equation>
<subsectionHeader confidence="0.991614">
4.1 Learning FOL Rules for Discourse Parsing
</subsectionHeader>
<bodyText confidence="0.9999905">
In Aleph, the hypothesis space is restricted to a set of
rules that conform to a predefined language L. This
is done with the use of mode declarations which, in
other words, introduces a language bias in the learn-
ing process. modeh declarations inform the learning
algorithm about what predicates to use as the head
of the rule and modeb specifies what predicates to
use in the body of the rule. Not all the information
in B needs to be included in the body of the rule.
This makes sense since we often learn definitions of
concepts based on more abstract higher level infor-
mation that is inferred from some other information
that is not part of our final definition. Mode decla-
rations are used by Aleph to build the most specific
clause (+) that can be learned for each example. +
constrains the search for suitable hypotheses. +i is
built by taking an example ei E E+ and adding lit-
erals that are entailed by B and ei. We then have the
following property, where Hi is the hypothesis (rule)
we are trying to learn and � is a generality operator:
</bodyText>
<equation confidence="0.547228">
❑ � Hi � +i
</equation>
<bodyText confidence="0.99990632">
Finding the most specific clause (+) provides us
with a partially ordered set of clauses from which to
choose the best hypothesis based on some quantifi-
able qualitative criteria. This sub-lattice is bounded
by the most general clause (❑, the empty clause)
from the top and the most specific clause (+) at the
bottom. We use the heuristic search in Aleph that is
similar to the A*-like search strategy presented by
(Muggleton, 1995) to find the best hypothesis (rule).
A noise threshold on the number of negative exam-
ples that can be covered by a rule can be set. We
learn a model that learns perfect rules first and then
one that allows for at most 5 negative examples. A
backoff model that first uses the model trained with
noise = 0 and then noise = 5 if no classification
has been made is used. We use the evaluation func-
tion in Equation 1 to guide our search through the
tree of possible hypotheses. This evaluation func-
tion is also called the compression function since it
prefers simpler explanations to more complex ones
(Occam’s Razor). fs is the score for clause cs that
is being evaluated, ps is the number of positive ex-
amples, ns is the number of negative examples, ls is
the length of the clause (measured by the number of
clauses).
</bodyText>
<equation confidence="0.93801">
fs = ps − (ns + (0.1 x ls)) (1)
</equation>
<bodyText confidence="0.998533888888889">
Classification in most ILP systems, including
Aleph, is restricted to binary classification (positive
vs. negative). In many applications with just two
classes, this is sufficient. However, we are faced
with a multi-classification problem. In order to per-
form multi-class classification, we use a decision
list. First, we build m binary classifiers for each
relation r E R. Then, we form an ordered list of the
rules based on the following criterion:
</bodyText>
<listItem confidence="0.99361825">
1. Given two rules ri and rj, ri ,is ranked higher
than rj if (pi − ni) &gt; (pj − nj).
2. if (pi − ni) = (pj − nj), then ri is ranked higher
than rj if ( pi
</listItem>
<equation confidence="0.893202166666667">
pi+ni ) &gt; (pj
pj+nj ).
3. if (pi − ni) = (pj − nj) and (pi
pi+ni ) = (pj
pj+nj )
then ri is ranked higher than rj if (li) &gt; (lj).
</equation>
<listItem confidence="0.858039">
4. default: random order
</listItem>
<bodyText confidence="0.9545255">
Classifying an unseen example is done by using
the first rule in the ordered list that satisfies it.
</bodyText>
<sectionHeader confidence="0.995849" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9999405">
We report our results from experiments on both the
classification task and the discourse parsing task.
</bodyText>
<subsectionHeader confidence="0.87785">
5.1 Relation Classification Results
</subsectionHeader>
<bodyText confidence="0.999917">
For the classification task, we conducted exper-
iments using the stratified k-fold (k = 5) cross-
validation evaluation technique on our data. Unlike
</bodyText>
<page confidence="0.988211">
570
</page>
<bodyText confidence="0.99929865">
(Wellner et. al., 2006; Sporleder and Lascarides,
2005), we do not assume that we know the order
of the relation in question. Instead we treat reversals
of non-commutative relations (e.g. preparation:act
and act:goal) as separate relations as well. We
compare our ILP model to RIPPER, Naive Bayes
and the Decision Tree algorithm. We should point
out that since attribute-value learning models can-
not handle first-order logic data, they have been pre-
sented with features that lose at least some of this
information. While this may then seem to result in
an unfair comparison, to the contrary, this is pre-
cisely the point: can we do better than very effec-
tive attribute-value approaches that however inher-
ently cannot take richer information into account?
All the statistical significance tests were performed
using the value of F-Score obtained from each of the
folds. We report performance on two sets of data
since we were not able to obtain compositional se-
mantic data for all the EDUs in our corpus:
</bodyText>
<listItem confidence="0.99339475">
• Set A: Examples for which semantic data was
available for all the nuclei of the segments
(1789 total). This allows us to have a better
idea of how much impact semantic data has on
the performance, if any.
• Set B: All examples regardless of whether or
not semantic data was available for the nuclei
of the segments (5475 total).
</listItem>
<table confidence="0.999464166666667">
Model Semantics No Semantics
ILP 62.78 60.25
Decision Tree 56.29 55.45
RIPPER 58.02 56.96
Naive Bayes 35.83 34.66
Majority Class 31.63 31.63
</table>
<tableCaption confidence="0.999942">
Table 1: Classification Performance: Set A (F-Score)
</tableCaption>
<bodyText confidence="0.997534384615385">
Table 1 shows the results on Set A. ILP outper-
forms all the other models. Via ANOVA, we first
conclude that there is a statistically significant differ-
ence between the 8 models (p &lt; 2.2e−16). To then
pinpoint where the difference precisely lies, pair-
wise comparisons using Student’s t-test show that
the difference between ILP (using semantics) and all
of the other learning models is statistically signifi-
cant at p &lt; 0.05. Additionally, ILP with semantics
is significantly better than ILP without it (p &lt; 0.05).
For Decision Tree, Naive Bayes and RIPPER, the
improvement in using semantics is not statistically
significant.
</bodyText>
<table confidence="0.999424166666667">
Model Semantics No Semantics
ILP 59.43 59.22
Decision Tree 53.84 53.69
RIPPER 51.1 51.36
Naive Bayes 49.69 51.62
Majority Class 22.01 22.01
</table>
<tableCaption confidence="0.999765">
Table 2: Classification Performance: Set B (F-Score)
</tableCaption>
<bodyText confidence="0.999057375">
In Table 2, we list the results on Set B. Once
again, our ILP model outperforms the other three
learning models. Naive Bayes is much more com-
petitive when using all the examples compared to
using only examples with semantic data. In the case
of the attribute-value machine learning models, the
use of semantic data seems to marginally hurt the
performance of the classifiers. However, this is in
contrast to the relational ILP model which always
performs better when using semantics. This result
suggests that the use of semantic data with loss of in-
formation may not be helpful, and in fact, it may ac-
tually hurt performance. Based on ANOVA, the dif-
ferences in these 8 models is statistically significant
with p &lt; 6.95e−12. A pairwise t-test between ILP
(using semantics) and each of the other attribute-
value learning models shows that our results are sta-
tistically significant at p &lt; 0.05.
In Table 3, we report the performance of the two
ILP models on each relation.3 In general, the models
perform better on relations that have the most exam-
ples.
The evaluation of work in discourse parsing is
hindered by the lack of a standard corpus or task.
Hence, our results cannot be directly compared
to (Marcu, 2000; Sporleder and Lascarides, 2005;
Wellner et. al., 2006), but neither can those works
be compared among themselves, because of differ-
ences in underlying corpora, the type and number of
relations used, and various assumptions. However,
we can still draw some general comparisons. Our
ILP-based models provide as much or significantly
</bodyText>
<footnote confidence="0.966378">
3Due to space limitations, only relations with &gt; 10 examples
are shown.
</footnote>
<page confidence="0.984476">
571
</page>
<table confidence="0.99995875">
relation Semantics No Semantics
preparation:act 74.86 72.05
general:specific 31.74 28.24
joint 55.23 52
act:goal 86.12 83.85
criterion:act 77.37 75.32
goal:act 73.43 68.9
step1:step2 28.75 35.29
co-temp1:co-temp2 48.84 37.84
disjunction 83.33 80.81
act:criterion 54.29 54.79
contrast1:contrast2 22.22 5.0
act:preparation 65.31 70.59
act:reason 0 10.26
cause:effect 19.05 10.53
comparison 22.22 10.53
</table>
<tableCaption confidence="0.9830315">
Table 3: Classification Performance (F-Score) by
Relation: ILP on Set A
</tableCaption>
<bodyText confidence="0.99981925">
more improvement over a majority-class baseline
when compared to these other works. This is the
case even though our work is based on less training
data, relatively more relations, relations both be-
tween just two EDUs and those involving larger text
segments, and we make no assumptions about the
order of the relations. Our results are comparable to
(Marcu, 2000), which reports an accuracy of about
61% for his classifier. His majority class baseline
performs at about 50% accuracy. (Wellner et. al.,
2006) reports an accuracy of up to 81%, with a ma-
jority class baseline performance of 45.7%. How-
ever, our task is more challenging than (Wellner et.
al., 2006). They use only 11 relations compared to
the 26 we use. They also assume the order of the
relation in the examples (i.e. examples for goal:act
would be treated as examples for act:goal by revers-
ing the order of the arguments) whereas we do not
make such assumptions. In addition, their training
data is almost twice as large as ours, based on re-
lation instances. (Sporleder and Lascarides, 2005)
also makes the same assumption on the ordering of
the relations as (Wellner et. al., 2006). They re-
port an accuracy of 57.75%. Their work, though,
was based on only 5 relations. Importantly, neither
(Wellner et. al., 2006; Sporleder and Lascarides,
2005) model examples with complex text segments
with more than one EDU.
</bodyText>
<subsectionHeader confidence="0.991096">
5.2 How interesting are the rules?
</subsectionHeader>
<bodyText confidence="0.99950225">
Given that our ILP models learn first-order logic
rules, we can make some qualitative analysis of the
rules learned, such as those below, learnt by the ILP
model that uses semantics:
</bodyText>
<reference confidence="0.561527466666667">
(2a) relation(A,B,’act:goal’) :-
firstWordPOS(A,’VBG’),
verbclass(A,D,’use-1’),
firstWordPOS(B,’VB’).
[pos cover = 23 neg cover = 1]
(2b) relation(A,B,’preparation:act’) :-
discourse cue(B,front,and),
cause(A,frame(C),D,E),
theme(B,frame(F),G), theme(A,frame(C),G).
[pos cover = 12 neg cover = 0]
(2c) relation(A,B,’preparation:act’) :-
discourse cue(B,front,then),
parent segment(A,C), parent segment(A,D),
internal relation(C,D,’preparation:act’).
[pos cover = 17 neg cover = 0]
</reference>
<listItem confidence="0.739344">
(2a) is learned using examples such as
relation(s5e1,s5e2,’act:goal’) from Example (1).
(2b) uses relational semantic information. This rule
</listItem>
<bodyText confidence="0.955056214285714">
can be read as follows:
IF segment A contains a cause and a
theme, the same object that is the theme
in A is also the theme in segment B, and B
contains the discourse cue and at the front
THEN the relation between A and B is
preparation:act.
(2c) is a rule that makes use of the structural in-
formation about complex text segments. When us-
ing Set A, more than about 60% of the rules in-
duced include at least one semantic predicate in its
body. They occur more frequently in rules for re-
lations like preparation:act while less in rules for
general:speci�c and act:goal.
</bodyText>
<subsectionHeader confidence="0.97876">
5.3 Discourse Parsing Results
</subsectionHeader>
<bodyText confidence="0.9998942">
In order to test our discourse parser, we used 151
documents for training and 25 for testing. We eval-
uated the performance of our parser on both the
discourse parse trees it builds at the sentence level
and at the document level. The test set contained
</bodyText>
<page confidence="0.989347">
572
</page>
<table confidence="0.9988402">
Sentence Level Document Level
model Semantics span nuclearity relation span nuclearity relation
SR-ILP yes 92.91 71.83 63.06 70.35 49.47 35.44
SR-ILP no 91.98 69.59 58.58 68.95 48.16 33.33
Baseline - 93.66 74.44 34.32 70.26 47.98 22.46
</table>
<tableCaption confidence="0.999879">
Table 4: Parsing Performance (F-Score): (Baseline = right-branching majority)
</tableCaption>
<bodyText confidence="0.999919159090909">
341 sentences out of which 180 sentences were seg-
mented into more than one EDU. We ran experi-
ments using our two ILP models for the relation
identifier, namely ILP with semantics and without
semantics. Our ILP based discourse parsing models
are named SR-ILP. We compare the performance of
our models against a right branching majority class
baseline. We used the sign-test to determine statis-
tical significance of the results. Using the automatic
evaluation methodology in (Marcu, 2000), preci-
sion, recall and F-Score measures are computed for
determining the hierarchical spans, nucleus-satellite
assignments and rhetorical relations. The perfor-
mance on labeling relations is the most important
measure since the results on nuclearity and hierar-
chical spans are by-products of the decisions made
to attach segments based on relations.
On labeling relations, the parser that uses all the
features (including compositional semantics) for de-
termining relations performs the best with an F-
Score of 63.06%. The difference of about 4.5% (be-
tween ILP with semantics and without semantics)
in F-Score is statistically significant at p = 0.006.
Our best model, SR-ILP (using semantics) beats the
baseline by about 28% in F-Score. Since the task at
the document level is much more challenging than
building the discourse structure at the sentence level,
we were not surprised to see a considerable drop in
performance. For our best model, the performance
on labeling relations drops to 35.44%. Clearly, the
mistakes made when attaching segments at lower
levels have quite an adverse effect on the overall
performance. A less greedy approach to parsing dis-
course structure is warranted.
While we would have hoped for a better perfor-
mance than 35.44%, to start with, (Forbes et. al.,
2001), (Polanyi et. al., 2004), and (Cristea, 2000) do
not report the performance of their discourse parsers
at all. (Marcu, 2000) reports precision and recall of
up to 63.2% and 59.8% on labeling relations using
manually segmented EDUs on three WSJ articles.
(Baldridge and Lascarides, 2005) reports 43.2% F-
Score on parsing 10 dialogues using a probabilistic
head-driven parsing model.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99998947826087">
In conclusion, we have presented a relational ap-
proach for classifying informational relations and a
modified shift-reduce parsing algorithm for building
discourse parse trees based on informational rela-
tions. To our knowledge, this is the first attempt
at using a relational learning model for the task of
relation classification, or even discourse parsing in
general. Our approach is linguistically motivated.
Using ILP, we are able to account for rich composi-
tional semantic data of the EDUs based on VerbNet
as well as the structural relational properties of the
text segments. This is not possible using attribute-
value based models like Decision Trees and RIP-
PER and definitely not using probabilistic models
like Naive Bayes. Our experiments have shown that
semantics can be useful in classifying informational
relations. For parsing, our modified shift-reduce al-
gorithm using the ILP relation classifier outperforms
a right-branching baseline model significantly. Us-
ing semantics for parsing also yields a statistically
significant improvement. Our approach is also do-
main independent as the underlying model and data
are not domain specific.
</bodyText>
<sectionHeader confidence="0.99892" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.817957666666667">
This work is supported by the National Science Founda-
tion (IIS-0133123 and ALT-0536968) and the Office of
Naval Research (N000140010640).
</bodyText>
<page confidence="0.998144">
573
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918">
Asher, N., and Lascarides, A.: Logics of Conversation.
Cambridge University Press, 2003.
Baldridge, J. and Lascarides, A.: Probabilistic Head-
Driven Parsing for Discourse Structure In Proceed-
ings of the Ninth Conference on Computational Natu-
ral Language Learning (CoNNL), Ann Arbor, 2005.
Brill, E.: Transformation-based error-driven learning
and natural language processing: A case study in
part-of-speech tagging. Computational Linguistics,
21(4):543565, 1995.
Buitelaar, P.: CoreLex: Systematic Polysemy and Un-
derspecification. Ph.D. Thesis, Brandies University,
1998.
Carlson, L. D. M. and Okurowski., M. E.: Building a
discourse-tagged corpus in the framework of rhetorical
structure theory. In Current Directions in Discourse
and Dialogue pages 85–112, 2003.
Cristea, D.: An Incremental Discourse Parser Architec-
ture. In D. Christodoulakis (Ed.) Proceedings of the
Second International Conference - Natural Language
Processing - Patras, Greece, June 2000.
Forbes, K., Miltsakaki, E., R. P. A. S. A. J. and Web-
ber., B.: D-ltag system - discourse parsing with a lexi-
calized tree adjoining grammar. Information Stucture,
Discourse Structure and Discourse Semantics, ESS-
LLI 2001.
Grosz, B. J. and Sidner, C. L.: Attention, intention and
the structure of discourse. Computational Linguistics
12:175–204, 1988.
Hobbs, J. R.: On the coherence and structure of dis-
course. In Polyani, Livia editor, The Structure of Dis-
course, 1985.
Kipper, K., H. T. D. and Palmer., M.: Class-based con-
struction of a verb lexicon. AAAI-2000, Proceedings
of the Seventeenth National Conference on Artificial
Intelligence, 2000.
Mann, W. and Thompson, S.: Rhetorical structure the-
ory: Toward a functional theory of text organization.
Text, 8(3):243–281, 1988.
Marcu, D.: Instructions for Manually Annotating the
Discourse Structures of Texts. Technical Report, Uni-
versity of Southern California, 1999.
Marcu, D.: The theory and practice of discourse parsing
and summarization. Cambridge, Massachusetts, Lon-
don, England, MIT Press, 2000.
Moser, M. G., Moore, J. D., and Glendening, E.: In-
structions for Coding Explanations: Identifying Seg-
ments, Relations and Minimal Units. University of
Pittsburgh, Department of Computer Science, 1996.
Muggleton, S. H.: Inverse entailment and progol.
In New Generation Computing Journal 13:245–286,
1995.
Polanyi, L., Culy, C., van den Berg, M. H. and Thione,
G. L.: A Rule Based Approach to Discourse Pars-
ing. Proceedings of the 5th SIGdial Workshop in Dis-
course And Dialogue. Cambridge, MA USA pp. 108-
117., May 1, 2004.
Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo,
L., Joshi, A., and Webber, B.: The Penn Discourse
Treebank 2.0. LREC, 2008.
Ros´e, C. P.: A Syntactic Framework for Semantic In-
terpretation, Proceedings of the ESSLLI Workshop
on Linguistic Theory and Grammar Implementation,
2000.
Sporleder, C. and Lascarides., A.: Exploiting linguistic
cues to classify rhetorical relations. Recent Advances
in Natural Language Processing, 2005.
Soricut, R. and Marcu., D.: Sentence level discourse
parsing using syntactic and lexical information. Pro-
ceedings of the Human Language Technology and
North American Assiciation for Computational Lin-
guistics Conference, 2003.
Subba, R., Di Eugenio, B., E. T.: Building lexical re-
sources for princpar, a large coverage parser that gen-
erates principled semantic representations. LREC,
2006.
Subba, R.: Discourse Parsing: A Relational Learn-
ing Approach Ph.D. Thesis, University of Illinois
Chicago, December 2008.
Webber, B.: DLTAG: Extending Lexicalized TAG to Dis-
course. Cognitive Science 28:751-779, 2004.
Wellner, B., Pustejovsky, J., C. H. R. S. and Rumshisky.,
A.: Classification of discourse coherence rela-
tions: An exploratory study using multiple knowledge
sources. In Proceedings of the 7th SIGDIAL Work-
shop on Discourse and Dialogue, 2006.
Williams, S. and Reiter, E.: A corpus analysis of dis-
course relations for natural language generation. Pro-
ceedings of Corpus Linguistics, pages 899–908, 2003.
Wolf, F. and Gibson, E.: Representing discourse coher-
ence: A corpus-based analysis. Computational Lin-
guistics 31(2):249–287, 2005.
</reference>
<page confidence="0.998286">
574
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.094272">
<title confidence="0.792813">An effective Discourse Parser that uses Rich Linguistic Information Subba Display Advertising</title>
<author confidence="0.174659">Yahoo</author>
<affiliation confidence="0.299488">Sunnyvale, CA,</affiliation>
<email confidence="0.991045">rajen@yahoo-inc.com</email>
<author confidence="0.999851">Barbara Di</author>
<affiliation confidence="0.924777666666667">Department of Computer University of Chicago, IL,</affiliation>
<email confidence="0.999515">bdieugen@cs.uic.edu</email>
<abstract confidence="0.995264466666667">This paper presents a first-order logic learning approach to determine rhetorical relations between discourse segments. Beyond linguistic cues and lexical information, our approach exploits compositional semantics and segment discourse structure data. We report a statistically significant improvement in classifying relations over attribute-value learning paradigms such as Decision Trees, RIP- PER and Naive Bayes. For discourse parsing, our modified shift-reduce parsing model that uses our relation classifier significantly outperforms a right-branching majority-class baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>relation(A,B,’act:goal’) :-firstWordPOS(A,’VBG’), verbclass(A,D,’use-1’), firstWordPOS(B,’VB’). [pos cover</title>
<date></date>
<journal></journal>
<volume>23</volume>
<note>neg cover = 1</note>
<marker></marker>
<rawString>(2a) relation(A,B,’act:goal’) :-firstWordPOS(A,’VBG’), verbclass(A,D,’use-1’), firstWordPOS(B,’VB’). [pos cover = 23 neg cover = 1]</rawString>
</citation>
<citation valid="true">
<title>relation(A,B,’preparation:act’) :-discourse cue(B,front,and), cause(A,frame(C),D,E), theme(B,frame(F),G), theme(A,frame(C),G). [pos cover</title>
<date></date>
<journal></journal>
<volume>12</volume>
<note>neg cover = 0</note>
<marker></marker>
<rawString>(2b) relation(A,B,’preparation:act’) :-discourse cue(B,front,and), cause(A,frame(C),D,E), theme(B,frame(F),G), theme(A,frame(C),G). [pos cover = 12 neg cover = 0]</rawString>
</citation>
<citation valid="true">
<title>relation(A,B,’preparation:act’) :-discourse cue(B,front,then), parent segment(A,C), parent segment(A,D), internal relation(C,D,’preparation:act’).</title>
<date></date>
<note>[pos cover = 17 neg cover = 0]</note>
<marker></marker>
<rawString>(2c) relation(A,B,’preparation:act’) :-discourse cue(B,front,then), parent segment(A,C), parent segment(A,D), internal relation(C,D,’preparation:act’). [pos cover = 17 neg cover = 0]</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Asher, N., and Lascarides, A.: Logics of Conversation. Cambridge University Press, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>A Lascarides</author>
</authors>
<title>Probabilistic HeadDriven Parsing for Discourse Structure</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNNL),</booktitle>
<location>Ann Arbor,</location>
<contexts>
<context position="1880" citStr="Baldridge and Lascarides, 2005" startWordPosition="272" endWordPosition="275">lations labeling their inner nodes is still a challenging and mostly unsolved problem in NLP. It is linguistically plausible that such structures are determined at least in part on the basis of the meaning of the related chunks of texts, and of the rhetorical intentions of their authors. However, such knowledge is extremely difficult to capture. Hence, previous work on discourse parsing (Wellner et. al., 2006; Sporleder and Lascarides, 2005; Marcu, 2000; Polanyi et. al., 2004; Soricut and Marcu, 2003; ∗This work was done while the author was a student at the University of Illinois at Chicago. Baldridge and Lascarides, 2005) has relied only on syntactic and lexical information, lexical chains and shallow semantics. We present an innovative discourse parser that uses compositional semantics (when available) and information on the structure of the segment being built itself. Our discourse parser, based on a modified shift-reduce algorithm, crucially uses a rhetorical relation classifier to determine the site of attachment of a new incoming chunk together with the appropriate relation label. Another novel aspect of our work is the usage of Inductive Logic Programming (ILP): ILP learns from first-order logic represen</context>
</contexts>
<marker>Baldridge, Lascarides, 2005</marker>
<rawString>Baldridge, J. and Lascarides, A.: Probabilistic HeadDriven Parsing for Discourse Structure In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNNL), Ann Arbor, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="12149" citStr="Brill, 1995" startWordPosition="1864" endWordPosition="1865">’s5e2’,’.’). Similarity: segment sim score(’s5e1’,’s5e2’,0.0). verbclass(’s5e2’,mark,’image impression-25.1’). agent(’s5e2’,frame(mark),you). Compositional Semantics: destination(’s5e2’,frame(mark),panel). cause(’s5e2’,frame(mark),you,’s5e2-mark-e’). prep(’s5e2’,frame(mark),end(’s5e2-mark-e’),mark,panel). created image(’s5e2’,frame(mark),result(’s5e2-mark-e’),mark). Structural Information: same sentence(’s5e1’,’s5e2’). Figure 2: Example Background Knowledge knowledge provided for EDU s5e2. Verbs + Nouns: These features were derived by tagging all the sentences in the corpus with a POS tagger (Brill, 1995). WordNet: For each noun in our data, we also use information on hypernymy and meronymy relations using WordNet. In a sense, this captures the domain relations between objects in our data. Linguistic Cues: Various cues can facilitate the inference of informational relations, even if it is well known that they are based solely on the content of the text segments, various cues can facilitate the inference of such relations. At the same time, it is well known that relations are often non signalled: in our corpus, only 43% of relations are signalled, consistently with figures from the literature (</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, E.: Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543565, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
</authors>
<title>CoreLex: Systematic Polysemy and Underspecification.</title>
<date>1998</date>
<tech>Ph.D. Thesis,</tech>
<institution>Brandies University,</institution>
<contexts>
<context position="7071" citStr="Buitelaar, 1998" startWordPosition="1101" endWordPosition="1103">and frames consisting of a syntactic description and semantic predicates. Such semantic classification of verbs can be helpful in making generalizations, especially when data is not abundant. Generalization can also be achieved by means of the semantic predicates. Although the verb classes of two verb instances may differ, semantic predicates are shared across verbs. To compositionally build verb based 567 semantic representations of our EDUs, we (Subba et al., 2006) integrated a robust parser, LCFLEX (Ros´e, 2000), with a lexicon and ontology based both on VerbNet and, for nouns, on CoreLex (Buitelaar, 1998). The augmented parser was able to derive complete semantic representations for 3257 of the 5744 EDUs (56.7%). The only manual step was to pick the correct parse from a forest of parse trees, since the output of the parser can be ambiguous. 2.2 Rhetorical relation annotation The discourse processing community has not yet reached agreement on an inventory of rhetorical relations. Among the many choices, our coding scheme is a hybrid of (Moser et. al., 1996) and (Marcu, 1999). We focused on what we call informational relations, namely, relations in the domain. We used 26 relations, divided into </context>
</contexts>
<marker>Buitelaar, 1998</marker>
<rawString>Buitelaar, P.: CoreLex: Systematic Polysemy and Underspecification. Ph.D. Thesis, Brandies University, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L D M Carlson</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2003</date>
<booktitle>In Current Directions in Discourse and Dialogue</booktitle>
<pages>85--112</pages>
<marker>Carlson, Okurowski, 2003</marker>
<rawString>Carlson, L. D. M. and Okurowski., M. E.: Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Current Directions in Discourse and Dialogue pages 85–112, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cristea</author>
</authors>
<title>An Incremental Discourse Parser Architecture. In</title>
<date>2000</date>
<booktitle>Proceedings of the Second International Conference - Natural Language Processing - Patras,</booktitle>
<editor>D. Christodoulakis (Ed.)</editor>
<location>Greece,</location>
<marker>Cristea, 2000</marker>
<rawString>Cristea, D.: An Incremental Discourse Parser Architecture. In D. Christodoulakis (Ed.) Proceedings of the Second International Conference - Natural Language Processing - Patras, Greece, June 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes</author>
<author>E Miltsakaki</author>
<author>R P A S A J</author>
<author>B Webber</author>
</authors>
<title>D-ltag system - discourse parsing with a lexicalized tree adjoining grammar.</title>
<date>2001</date>
<booktitle>Information Stucture, Discourse Structure and Discourse Semantics, ESSLLI</booktitle>
<marker>Forbes, Miltsakaki, J, Webber, 2001</marker>
<rawString>Forbes, K., Miltsakaki, E., R. P. A. S. A. J. and Webber., B.: D-ltag system - discourse parsing with a lexicalized tree adjoining grammar. Information Stucture, Discourse Structure and Discourse Semantics, ESSLLI 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intention and the structure of discourse.</title>
<date>1988</date>
<journal>Computational Linguistics</journal>
<volume>12</volume>
<marker>Grosz, Sidner, 1988</marker>
<rawString>Grosz, B. J. and Sidner, C. L.: Attention, intention and the structure of discourse. Computational Linguistics 12:175–204, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>On the coherence and structure of discourse.</title>
<date>1985</date>
<booktitle>The Structure of Discourse,</booktitle>
<editor>In Polyani, Livia editor,</editor>
<marker>Hobbs, 1985</marker>
<rawString>Hobbs, J. R.: On the coherence and structure of discourse. In Polyani, Livia editor, The Structure of Discourse, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T D</author>
<author>M Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>AAAI-2000, Proceedings of the Seventeenth National Conference on Artificial Intelligence,</booktitle>
<marker>Kipper, D, Palmer, 2000</marker>
<rawString>Kipper, K., H. T. D. and Palmer., M.: Class-based construction of a verb lexicon. AAAI-2000, Proceedings of the Seventeenth National Conference on Artificial Intelligence, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="964" citStr="Mann and Thompson, 1988" startWordPosition="124" endWordPosition="127"> determine rhetorical relations between discourse segments. Beyond linguistic cues and lexical information, our approach exploits compositional semantics and segment discourse structure data. We report a statistically significant improvement in classifying relations over attribute-value learning paradigms such as Decision Trees, RIPPER and Naive Bayes. For discourse parsing, our modified shift-reduce parsing model that uses our relation classifier significantly outperforms a right-branching majority-class baseline. 1 Introduction Many theories postulate a hierarchical structure for discourse (Mann and Thompson, 1988; Moser et. al., 1996; Polanyi et. al., 2004). Discourse structure is most often based on semantic / pragmatic relationships between spans of text and results in a tree structure, as that shown in Figure 1. Discourse parsing, namely, deriving such tree structures and the rhetorical relations labeling their inner nodes is still a challenging and mostly unsolved problem in NLP. It is linguistically plausible that such structures are determined at least in part on the basis of the meaning of the related chunks of texts, and of the rhetorical intentions of their authors. However, such knowledge is</context>
<context position="8392" citStr="Mann and Thompson, 1988" startWordPosition="1297" endWordPosition="1300">; 6 elaboration relations (e.g., general:specific, set:member, object:attribute; 3 similarity relations (contrast1:contrast2, comparison, restatement); 2 temporal relations (cotemp1:co-temp2, before:after); and 4 other relations, including joint and disjunction. The annotation yielded 5172 relations, with reasonable intercoder agreement. On 26% of the data, we obtained n = 0.66; n rises to 0.78 when the two most commonly confused relations, preparation:act and step1:step2, are consolidated. We also annotated the relata as nucleus (more important member) and satellite (contributing member(s)) (Mann and Thompson, 1988), with n = 0.67.2 The most frequent relation is preparation:act (24.46%), and in general, causal relations are more frequently used in our instructional corpus than in news corpora (Carlson et. al., 2003; Wolf and Gibson, 2005). 3 Shift-Reduce Discourse Parsing Our discourse parser is a modified version of a shiftreduce parser. The shift operation places the next segment on top of the stack, TOP. The reduce operation will attach the text segment at TOP to the text segment at TOP-1. (Marcu, 2000) also uses a shiftreduce parser, though our parsing algorithm differs 2We don’t have space to explai</context>
<context position="14252" citStr="Mann and Thompson, 1988" startWordPosition="2197" endWordPosition="2201">ally than inter-sententially. For larger segments, we also encode the hierarchical representation of text segments that contain more than one nucleus, the distance between the nuclei of the two segments and any relations that exist between the smaller inner segments. At this point, the attentive reader will be wondering how we encode compositional semantics for relations relating text segments larger than one EDU. Clearly we cannot just list the semantics of each EDU that is dominated by the larger segment. We follow the intuition that nuclei represent the most important portions of segments (Mann and Thompson, 1988). For segments such as s5e1-s5e2 that contains a single nucleus, we simply reduce the semantic content of the larger segment to that of its nucleus: s5e1-s5e2 verb(’s5e1-s5e2’,mark). ... verbclass(’s5e1-s5e2’,..). agent(’s5e1-s5e2’,..). In this case, the semantics of the complex text segment is represented by the compositional semantics of the single most important EDU. For segments that contain more than one nucleus, such as s3e1-s3e2, the discourse structure information of the segment is represented with the additional predicates internal relation and parent segment. These predicates can be </context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>Mann, W. and Thompson, S.: Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Instructions for Manually Annotating the Discourse Structures of Texts.</title>
<date>1999</date>
<tech>Technical Report,</tech>
<institution>University of Southern California,</institution>
<contexts>
<context position="7549" citStr="Marcu, 1999" startWordPosition="1183" endWordPosition="1184"> a robust parser, LCFLEX (Ros´e, 2000), with a lexicon and ontology based both on VerbNet and, for nouns, on CoreLex (Buitelaar, 1998). The augmented parser was able to derive complete semantic representations for 3257 of the 5744 EDUs (56.7%). The only manual step was to pick the correct parse from a forest of parse trees, since the output of the parser can be ambiguous. 2.2 Rhetorical relation annotation The discourse processing community has not yet reached agreement on an inventory of rhetorical relations. Among the many choices, our coding scheme is a hybrid of (Moser et. al., 1996) and (Marcu, 1999). We focused on what we call informational relations, namely, relations in the domain. We used 26 relations, divided into 5 broad classes: 12 causal relations (e.g., preparation:act, goal:act, cause:effect, step1:step2); 6 elaboration relations (e.g., general:specific, set:member, object:attribute; 3 similarity relations (contrast1:contrast2, comparison, restatement); 2 temporal relations (cotemp1:co-temp2, before:after); and 4 other relations, including joint and disjunction. The annotation yielded 5172 relations, with reasonable intercoder agreement. On 26% of the data, we obtained n = 0.66;</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, D.: Instructions for Manually Annotating the Discourse Structures of Texts. Technical Report, University of Southern California, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The theory and practice of discourse parsing and summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, London, England,</location>
<contexts>
<context position="1706" citStr="Marcu, 2000" startWordPosition="245" endWordPosition="246">n spans of text and results in a tree structure, as that shown in Figure 1. Discourse parsing, namely, deriving such tree structures and the rhetorical relations labeling their inner nodes is still a challenging and mostly unsolved problem in NLP. It is linguistically plausible that such structures are determined at least in part on the basis of the meaning of the related chunks of texts, and of the rhetorical intentions of their authors. However, such knowledge is extremely difficult to capture. Hence, previous work on discourse parsing (Wellner et. al., 2006; Sporleder and Lascarides, 2005; Marcu, 2000; Polanyi et. al., 2004; Soricut and Marcu, 2003; ∗This work was done while the author was a student at the University of Illinois at Chicago. Baldridge and Lascarides, 2005) has relied only on syntactic and lexical information, lexical chains and shallow semantics. We present an innovative discourse parser that uses compositional semantics (when available) and information on the structure of the segment being built itself. Our discourse parser, based on a modified shift-reduce algorithm, crucially uses a rhetorical relation classifier to determine the site of attachment of a new incoming chun</context>
<context position="8892" citStr="Marcu, 2000" startWordPosition="1384" endWordPosition="1385">d the relata as nucleus (more important member) and satellite (contributing member(s)) (Mann and Thompson, 1988), with n = 0.67.2 The most frequent relation is preparation:act (24.46%), and in general, causal relations are more frequently used in our instructional corpus than in news corpora (Carlson et. al., 2003; Wolf and Gibson, 2005). 3 Shift-Reduce Discourse Parsing Our discourse parser is a modified version of a shiftreduce parser. The shift operation places the next segment on top of the stack, TOP. The reduce operation will attach the text segment at TOP to the text segment at TOP-1. (Marcu, 2000) also uses a shiftreduce parser, though our parsing algorithm differs 2We don’t have space to explain why we annotate for nucleus and satellite, even if (Moser et. al., 1996) argue that this sort of distinction does not apply to informational relations. in two respects: 1) we do not learn shift operations and 2) in contrast to (Marcu, 2000), the attachment of an incoming text segment to the emerging tree may occur at any node on the right frontier. This allows for the more sophisticated type of adjunction operations required for discourse parsing as modeled in D-LTAG (Webber, 2004). A reduce o</context>
<context position="22559" citStr="Marcu, 2000" startWordPosition="3606" endWordPosition="3607">actually hurt performance. Based on ANOVA, the differences in these 8 models is statistically significant with p &lt; 6.95e−12. A pairwise t-test between ILP (using semantics) and each of the other attributevalue learning models shows that our results are statistically significant at p &lt; 0.05. In Table 3, we report the performance of the two ILP models on each relation.3 In general, the models perform better on relations that have the most examples. The evaluation of work in discourse parsing is hindered by the lack of a standard corpus or task. Hence, our results cannot be directly compared to (Marcu, 2000; Sporleder and Lascarides, 2005; Wellner et. al., 2006), but neither can those works be compared among themselves, because of differences in underlying corpora, the type and number of relations used, and various assumptions. However, we can still draw some general comparisons. Our ILP-based models provide as much or significantly 3Due to space limitations, only relations with &gt; 10 examples are shown. 571 relation Semantics No Semantics preparation:act 74.86 72.05 general:specific 31.74 28.24 joint 55.23 52 act:goal 86.12 83.85 criterion:act 77.37 75.32 goal:act 73.43 68.9 step1:step2 28.75 35</context>
<context position="23804" citStr="Marcu, 2000" startWordPosition="3789" endWordPosition="3790"> disjunction 83.33 80.81 act:criterion 54.29 54.79 contrast1:contrast2 22.22 5.0 act:preparation 65.31 70.59 act:reason 0 10.26 cause:effect 19.05 10.53 comparison 22.22 10.53 Table 3: Classification Performance (F-Score) by Relation: ILP on Set A more improvement over a majority-class baseline when compared to these other works. This is the case even though our work is based on less training data, relatively more relations, relations both between just two EDUs and those involving larger text segments, and we make no assumptions about the order of the relations. Our results are comparable to (Marcu, 2000), which reports an accuracy of about 61% for his classifier. His majority class baseline performs at about 50% accuracy. (Wellner et. al., 2006) reports an accuracy of up to 81%, with a majority class baseline performance of 45.7%. However, our task is more challenging than (Wellner et. al., 2006). They use only 11 relations compared to the 26 we use. They also assume the order of the relation in the examples (i.e. examples for goal:act would be treated as examples for act:goal by reversing the order of the arguments) whereas we do not make such assumptions. In addition, their training data is</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Marcu, D.: The theory and practice of discourse parsing and summarization. Cambridge, Massachusetts, London, England, MIT Press, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Moser</author>
<author>J D Moore</author>
<author>E Glendening</author>
</authors>
<title>Instructions for Coding Explanations: Identifying Segments, Relations and Minimal Units.</title>
<date>1996</date>
<institution>University of Pittsburgh, Department of Computer Science,</institution>
<marker>Moser, Moore, Glendening, 1996</marker>
<rawString>Moser, M. G., Moore, J. D., and Glendening, E.: Instructions for Coding Explanations: Identifying Segments, Relations and Minimal Units. University of Pittsburgh, Department of Computer Science, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Muggleton</author>
</authors>
<title>Inverse entailment and progol.</title>
<date>1995</date>
<journal>In New Generation Computing Journal</journal>
<volume>13</volume>
<contexts>
<context position="10744" citStr="Muggleton, 1995" startWordPosition="1695" endWordPosition="1696">, then we reduce TOP and TOP-1 using the joint relation until a single DPT is built. 4 Classifying Rhetorical Relations Identifying the informational relations between text segments is central to our approach for building the informational tree structure of text. We believe that the use of a limited knowledge representation formalism, essentially propositional logic, is not adequate and that a relational model that can handle compositional semantics is necessary. We cast the problem of determining informational relations as a classification task. We used the ILP system Aleph that is based on (Muggleton, 1995). Formulation of any problem within the ILP framework consists of background knowledge B and the set of examples E (E+∪ E−). In our ILP framework, positive examples are ground clauses describing a relation and its relata, e.g. relation(s5e1,s5e2,act:goal), or relation(s2e1-s3e2,s4e1,preparation:act) from Figure 1. If a is a positive example of a relation r, then it is also a negative example for all the other relations. Background Knowledge (B) can be thought of as features used by ILP to learn rules, as in traditional attribute-value learning algorithms. We use the following information to le</context>
<context position="16977" citStr="Muggleton, 1995" startWordPosition="2633" endWordPosition="2634">adding literals that are entailed by B and ei. We then have the following property, where Hi is the hypothesis (rule) we are trying to learn and � is a generality operator: ❑ � Hi � +i Finding the most specific clause (+) provides us with a partially ordered set of clauses from which to choose the best hypothesis based on some quantifiable qualitative criteria. This sub-lattice is bounded by the most general clause (❑, the empty clause) from the top and the most specific clause (+) at the bottom. We use the heuristic search in Aleph that is similar to the A*-like search strategy presented by (Muggleton, 1995) to find the best hypothesis (rule). A noise threshold on the number of negative examples that can be covered by a rule can be set. We learn a model that learns perfect rules first and then one that allows for at most 5 negative examples. A backoff model that first uses the model trained with noise = 0 and then noise = 5 if no classification has been made is used. We use the evaluation function in Equation 1 to guide our search through the tree of possible hypotheses. This evaluation function is also called the compression function since it prefers simpler explanations to more complex ones (Oc</context>
</contexts>
<marker>Muggleton, 1995</marker>
<rawString>Muggleton, S. H.: Inverse entailment and progol. In New Generation Computing Journal 13:245–286, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>C Culy</author>
<author>M H van den Berg</author>
<author>G L Thione</author>
</authors>
<title>A Rule Based Approach to Discourse Parsing.</title>
<date>2004</date>
<booktitle>Proceedings of the 5th SIGdial Workshop in Discourse And Dialogue.</booktitle>
<pages>108--117</pages>
<location>Cambridge, MA USA</location>
<marker>Polanyi, Culy, van den Berg, Thione, 2004</marker>
<rawString>Polanyi, L., Culy, C., van den Berg, M. H. and Thione, G. L.: A Rule Based Approach to Discourse Parsing. Proceedings of the 5th SIGdial Workshop in Discourse And Dialogue. Cambridge, MA USA pp. 108-117., May 1, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0. LREC,</title>
<date>2008</date>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo, L., Joshi, A., and Webber, B.: The Penn Discourse Treebank 2.0. LREC, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
</authors>
<title>A Syntactic Framework for Semantic Interpretation,</title>
<date>2000</date>
<booktitle>Proceedings of the ESSLLI Workshop on Linguistic Theory and Grammar Implementation,</booktitle>
<marker>Ros´e, 2000</marker>
<rawString>Ros´e, C. P.: A Syntactic Framework for Semantic Interpretation, Proceedings of the ESSLLI Workshop on Linguistic Theory and Grammar Implementation, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>A Lascarides</author>
</authors>
<title>Exploiting linguistic cues to classify rhetorical relations.</title>
<date>2005</date>
<booktitle>Recent Advances in Natural Language Processing,</booktitle>
<contexts>
<context position="1693" citStr="Sporleder and Lascarides, 2005" startWordPosition="241" endWordPosition="244">/ pragmatic relationships between spans of text and results in a tree structure, as that shown in Figure 1. Discourse parsing, namely, deriving such tree structures and the rhetorical relations labeling their inner nodes is still a challenging and mostly unsolved problem in NLP. It is linguistically plausible that such structures are determined at least in part on the basis of the meaning of the related chunks of texts, and of the rhetorical intentions of their authors. However, such knowledge is extremely difficult to capture. Hence, previous work on discourse parsing (Wellner et. al., 2006; Sporleder and Lascarides, 2005; Marcu, 2000; Polanyi et. al., 2004; Soricut and Marcu, 2003; ∗This work was done while the author was a student at the University of Illinois at Chicago. Baldridge and Lascarides, 2005) has relied only on syntactic and lexical information, lexical chains and shallow semantics. We present an innovative discourse parser that uses compositional semantics (when available) and information on the structure of the segment being built itself. Our discourse parser, based on a modified shift-reduce algorithm, crucially uses a rhetorical relation classifier to determine the site of attachment of a new </context>
<context position="19057" citStr="Sporleder and Lascarides, 2005" startWordPosition="3015" endWordPosition="3018"> pi pi+ni ) &gt; (pj pj+nj ). 3. if (pi − ni) = (pj − nj) and (pi pi+ni ) = (pj pj+nj ) then ri is ranked higher than rj if (li) &gt; (lj). 4. default: random order Classifying an unseen example is done by using the first rule in the ordered list that satisfies it. 5 Experiments and Results We report our results from experiments on both the classification task and the discourse parsing task. 5.1 Relation Classification Results For the classification task, we conducted experiments using the stratified k-fold (k = 5) crossvalidation evaluation technique on our data. Unlike 570 (Wellner et. al., 2006; Sporleder and Lascarides, 2005), we do not assume that we know the order of the relation in question. Instead we treat reversals of non-commutative relations (e.g. preparation:act and act:goal) as separate relations as well. We compare our ILP model to RIPPER, Naive Bayes and the Decision Tree algorithm. We should point out that since attribute-value learning models cannot handle first-order logic data, they have been presented with features that lose at least some of this information. While this may then seem to result in an unfair comparison, to the contrary, this is precisely the point: can we do better than very effecti</context>
<context position="22591" citStr="Sporleder and Lascarides, 2005" startWordPosition="3608" endWordPosition="3611"> performance. Based on ANOVA, the differences in these 8 models is statistically significant with p &lt; 6.95e−12. A pairwise t-test between ILP (using semantics) and each of the other attributevalue learning models shows that our results are statistically significant at p &lt; 0.05. In Table 3, we report the performance of the two ILP models on each relation.3 In general, the models perform better on relations that have the most examples. The evaluation of work in discourse parsing is hindered by the lack of a standard corpus or task. Hence, our results cannot be directly compared to (Marcu, 2000; Sporleder and Lascarides, 2005; Wellner et. al., 2006), but neither can those works be compared among themselves, because of differences in underlying corpora, the type and number of relations used, and various assumptions. However, we can still draw some general comparisons. Our ILP-based models provide as much or significantly 3Due to space limitations, only relations with &gt; 10 examples are shown. 571 relation Semantics No Semantics preparation:act 74.86 72.05 general:specific 31.74 28.24 joint 55.23 52 act:goal 86.12 83.85 criterion:act 77.37 75.32 goal:act 73.43 68.9 step1:step2 28.75 35.29 co-temp1:co-temp2 48.84 37.8</context>
</contexts>
<marker>Sporleder, Lascarides, 2005</marker>
<rawString>Sporleder, C. and Lascarides., A.: Exploiting linguistic cues to classify rhetorical relations. Recent Advances in Natural Language Processing, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>Proceedings of the Human Language Technology and North American Assiciation for Computational Linguistics Conference,</booktitle>
<contexts>
<context position="1754" citStr="Soricut and Marcu, 2003" startWordPosition="251" endWordPosition="254">e structure, as that shown in Figure 1. Discourse parsing, namely, deriving such tree structures and the rhetorical relations labeling their inner nodes is still a challenging and mostly unsolved problem in NLP. It is linguistically plausible that such structures are determined at least in part on the basis of the meaning of the related chunks of texts, and of the rhetorical intentions of their authors. However, such knowledge is extremely difficult to capture. Hence, previous work on discourse parsing (Wellner et. al., 2006; Sporleder and Lascarides, 2005; Marcu, 2000; Polanyi et. al., 2004; Soricut and Marcu, 2003; ∗This work was done while the author was a student at the University of Illinois at Chicago. Baldridge and Lascarides, 2005) has relied only on syntactic and lexical information, lexical chains and shallow semantics. We present an innovative discourse parser that uses compositional semantics (when available) and information on the structure of the segment being built itself. Our discourse parser, based on a modified shift-reduce algorithm, crucially uses a rhetorical relation classifier to determine the site of attachment of a new incoming chunk together with the appropriate relation label. </context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Soricut, R. and Marcu., D.: Sentence level discourse parsing using syntactic and lexical information. Proceedings of the Human Language Technology and North American Assiciation for Computational Linguistics Conference, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Subba</author>
<author>B Di Eugenio</author>
<author>E T</author>
</authors>
<title>Building lexical resources for princpar, a large coverage parser that generates principled semantic representations. LREC,</title>
<date>2006</date>
<marker>Subba, Di Eugenio, T, 2006</marker>
<rawString>Subba, R., Di Eugenio, B., E. T.: Building lexical resources for princpar, a large coverage parser that generates principled semantic representations. LREC, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Subba</author>
</authors>
<title>Discourse Parsing: A Relational Learning Approach Ph.D. Thesis,</title>
<date>2008</date>
<institution>University of Illinois Chicago,</institution>
<contexts>
<context position="3169" citStr="Subba, 2008" startWordPosition="474" endWordPosition="475">te than relation classifiers that use competitive propositional ML algorithms such as decision trees and Naive Bayes. In addition, it results in FOL rules that are linguistically perspicuous. Our domain is that of instructional how-to-do manuals, and we describe our corpus in Section 2. In Section 3, we discuss the modified shift-reduce parser we developed. The bulk of the paper is devoted to the rhetorical relation classifier in Section 4. Experimental results of both the relation classifier and the discourse parser in its entirety are discussed in Section 5. Further details can be found in (Subba, 2008). 2 Discourse Annotated Instructional Corpus Existing corpora annotated with rhetorical relations (Carlson et. al., 2003; Wolf and Gibson, 2005; Prasad et. al., 2008) focus primarily on news articles. However, for us the development of the discourse parser is parasitic on our ultimate goal: developing resources and algorithms for language in566 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 566–574, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics s1e1-s5e2 ddddddddddddddddddddddddddddddddddddd Z Z Z Z Z Z Z</context>
</contexts>
<marker>Subba, 2008</marker>
<rawString>Subba, R.: Discourse Parsing: A Relational Learning Approach Ph.D. Thesis, University of Illinois Chicago, December 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
</authors>
<title>DLTAG: Extending Lexicalized TAG to Discourse. Cognitive Science 28:751-779,</title>
<date>2004</date>
<contexts>
<context position="9480" citStr="Webber, 2004" startWordPosition="1486" endWordPosition="1487">t at TOP-1. (Marcu, 2000) also uses a shiftreduce parser, though our parsing algorithm differs 2We don’t have space to explain why we annotate for nucleus and satellite, even if (Moser et. al., 1996) argue that this sort of distinction does not apply to informational relations. in two respects: 1) we do not learn shift operations and 2) in contrast to (Marcu, 2000), the attachment of an incoming text segment to the emerging tree may occur at any node on the right frontier. This allows for the more sophisticated type of adjunction operations required for discourse parsing as modeled in D-LTAG (Webber, 2004). A reduce operation is determined by the relation identification component. We check if a relation exists between the incoming text segment and the attachment points on the right frontier. If more than one attachment site exists, then the attachment site for which the rule with the highest score fired (see below) is chosen for the reduce operation. A reduce operation can further trigger additional reduce operations if there is more than one tree left in the stack after the first reduce operation. When no rules fire, a shift occurs. In the event that all the segments in the input list have bee</context>
</contexts>
<marker>Webber, 2004</marker>
<rawString>Webber, B.: DLTAG: Extending Lexicalized TAG to Discourse. Cognitive Science 28:751-779, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>J Pustejovsky</author>
<author>C H R S</author>
<author>A Rumshisky</author>
</authors>
<title>Classification of discourse coherence relations: An exploratory study using multiple knowledge sources.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th SIGDIAL Workshop on Discourse and Dialogue,</booktitle>
<marker>Wellner, Pustejovsky, S, Rumshisky, 2006</marker>
<rawString>Wellner, B., Pustejovsky, J., C. H. R. S. and Rumshisky., A.: Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. In Proceedings of the 7th SIGDIAL Workshop on Discourse and Dialogue, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>A corpus analysis of discourse relations for natural language generation.</title>
<date>2003</date>
<booktitle>Proceedings of Corpus Linguistics,</booktitle>
<pages>899--908</pages>
<contexts>
<context position="12783" citStr="Williams and Reiter, 2003" startWordPosition="1967" endWordPosition="1970">t: For each noun in our data, we also use information on hypernymy and meronymy relations using WordNet. In a sense, this captures the domain relations between objects in our data. Linguistic Cues: Various cues can facilitate the inference of informational relations, even if it is well known that they are based solely on the content of the text segments, various cues can facilitate the inference of such relations. At the same time, it is well known that relations are often non signalled: in our corpus, only 43% of relations are signalled, consistently with figures from the literature (44% in (Williams and Reiter, 2003) and 45% in (Prasad et. al., 2008)). Besides lexical cues such as but, and and if, we also include modals, tense, comparatives and superlatives, and negation. E.g., wrong-act in relations like prescribe-act:wrong-act is often expressed using a negation. Similarity: For the two segments in question, we compute the cosine similarity of the segments using only nouns and verbs. Compositional semantics: the semantic information derived by our parser, as described in Section 2.1. The semantic representation of segment s5e2 from Example (1) is shown in Figure 2. Each semantic predicate is a feature f</context>
</contexts>
<marker>Williams, Reiter, 2003</marker>
<rawString>Williams, S. and Reiter, E.: A corpus analysis of discourse relations for natural language generation. Proceedings of Corpus Linguistics, pages 899–908, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wolf</author>
<author>E Gibson</author>
</authors>
<title>Representing discourse coherence: A corpus-based analysis.</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="3312" citStr="Wolf and Gibson, 2005" startWordPosition="491" endWordPosition="494">results in FOL rules that are linguistically perspicuous. Our domain is that of instructional how-to-do manuals, and we describe our corpus in Section 2. In Section 3, we discuss the modified shift-reduce parser we developed. The bulk of the paper is devoted to the rhetorical relation classifier in Section 4. Experimental results of both the relation classifier and the discourse parser in its entirety are discussed in Section 5. Further details can be found in (Subba, 2008). 2 Discourse Annotated Instructional Corpus Existing corpora annotated with rhetorical relations (Carlson et. al., 2003; Wolf and Gibson, 2005; Prasad et. al., 2008) focus primarily on news articles. However, for us the development of the discourse parser is parasitic on our ultimate goal: developing resources and algorithms for language in566 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 566–574, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics s1e1-s5e2 ddddddddddddddddddddddddddddddddddddd Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z general:specific s1e1 s2e1-s5e2 ggggggggggggggggggggggg WWWWWWWWWWWWWWWWWWWWWWW preparation:act Another way .. s2e1-s4</context>
<context position="8619" citStr="Wolf and Gibson, 2005" startWordPosition="1334" endWordPosition="1337"> including joint and disjunction. The annotation yielded 5172 relations, with reasonable intercoder agreement. On 26% of the data, we obtained n = 0.66; n rises to 0.78 when the two most commonly confused relations, preparation:act and step1:step2, are consolidated. We also annotated the relata as nucleus (more important member) and satellite (contributing member(s)) (Mann and Thompson, 1988), with n = 0.67.2 The most frequent relation is preparation:act (24.46%), and in general, causal relations are more frequently used in our instructional corpus than in news corpora (Carlson et. al., 2003; Wolf and Gibson, 2005). 3 Shift-Reduce Discourse Parsing Our discourse parser is a modified version of a shiftreduce parser. The shift operation places the next segment on top of the stack, TOP. The reduce operation will attach the text segment at TOP to the text segment at TOP-1. (Marcu, 2000) also uses a shiftreduce parser, though our parsing algorithm differs 2We don’t have space to explain why we annotate for nucleus and satellite, even if (Moser et. al., 1996) argue that this sort of distinction does not apply to informational relations. in two respects: 1) we do not learn shift operations and 2) in contrast t</context>
</contexts>
<marker>Wolf, Gibson, 2005</marker>
<rawString>Wolf, F. and Gibson, E.: Representing discourse coherence: A corpus-based analysis. Computational Linguistics 31(2):249–287, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>