<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000247">
<title confidence="0.96398">
The Wild Thing!
</title>
<author confidence="0.985476">
Kenneth Church Bo Thiesson
</author>
<affiliation confidence="0.922609">
Microsoft Research
</affiliation>
<address confidence="0.916105">
Redmond, WA, 98052, USA
</address>
<email confidence="0.971862">
{church, thiesson}@microsoft.com
</email>
<sectionHeader confidence="0.99315" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999274375">
Suppose you are on a mobile device with
no keyboard (e.g., a cell or PDA). How
can you enter text quickly? T9? Graffiti?
This demo will show how language model-
ing can be used to speed up data entry, both
in the mobile context, as well as the desk-
top. The Wild Thing encourages users to
use wildcards (*). A language model finds
the k-best expansions. Users quickly figure
out when they can get away with wild-
cards. General purpose trigram language
models are effective for the general case
(unrestricted text), but there are important
special cases like searching over popular
web queries, where more restricted lan-
guage models are even more effective.
</bodyText>
<sectionHeader confidence="0.780101" genericHeader="method">
1 Motivation: Phone App
</sectionHeader>
<bodyText confidence="0.999901310344828">
Cell phones and PDAs are everywhere. Users love
mobility. What are people doing with their phone?
You’d think they would be talking on their phones,
but a lot of people are typing. It is considered rude
to talk on a cell in certain public places, especially
in Europe and Asia. SMS text messaging enables
people to communicate, even when they can’t talk.
It is bizarre that people are typing on their
phones given how painful it is. “Talking on the
phone” is a collocation, but “typing on the phone”
is not. Slate (slate.msn.com/id/2111773) recently
ran a story titled: “A Phone You Can Actually
Type On” with the lead:
“If you&apos;ve tried to zap someone a text mes-
sage recently, you&apos;ve probably discovered
the huge drawback of typing on your cell
phone. Unless you&apos;re one of those cyborg
Scandinavian teenagers who was born with
a Nokia in his hand, pecking out even a
simple message is a thumb-twisting chore.”
There are great hopes that speech recognition
will someday make it unnecessary to type on your
phone (for SMS or any other app), but speech rec-
ognition won’t help with the rudeness issue. If
people are typing because they can’t talk, then
speech recognition is not an option. Fortunately,
the speech community has developed powerful
language modeling techniques that can help even
when speech is not an option.
</bodyText>
<sectionHeader confidence="0.731784" genericHeader="method">
2 K-Best String Matching
</sectionHeader>
<bodyText confidence="0.997516">
Suppose we want to search for MSN using a cell
phone. A standard approach would be to type 6
&lt;pause&gt; 777 &lt;pause&gt; 66, where 6 4 M, 777 4 S
and 66 4 N. (The pauses are necessary for disam-
biguation.) Kids these days are pretty good at typ-
ing this way, but there has to be a better solution.
T9 (www.t9.com) is an interesting alternative.
The user types 676 (for MSN). The system uses a
(unigram) language model to find the k-best
matches. The user selects MSN from this list.
Some users love T9, and some don’t.
The input, 676, can be thought of as short hand
for the regular expression:
/^[6MNOmno][7PRSprs][6MNOmno]$/
using standard Unix notation. Regular expressions
become much more interesting when we consider
wildcards. So-called “word wheeling” can be
thought of as the special case where we add a
wildcard to the end of whatever the user types.
Thus, if the user types 676 (for MSN), we would
find the k-best matches for:
</bodyText>
<footnote confidence="0.415888">
/^[6MNOmno][7PRSprs][6MNOmno].*/
</footnote>
<page confidence="0.983098">
93
</page>
<note confidence="0.3447155">
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 93–96, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999862444444444">
See Google Suggests1 for a nice example of
word wheeling. Google Suggests makes it easy to
find popular web queries (in the standard non-
mobile desktop context). The user types a prefix.
After each character, the system produces a list of
the k most popular web queries that start with the
specified prefix.
Word wheeling not only helps when you know
what you want to say, but it also helps when you
don’t. Users can’t spell. And things get stuck on
the tip of their tongue. Some users are just brows-
ing. They aren’t looking for anything in particular,
but they’d like to know what others are looking at.
The popular query application is relatively easy
in terms of entropy. About 19 bits are needed to
specify one of the 7 million most popular web que-
ries. That is, if we assign each web query a prob-
ability based on query logs collected at msn.com,
then we can estimate entropy, H, and discover that
H≈19. (About 23 bits would be needed if these
pages were equally likely, but they aren’t.) It is
often said that the average query is between two
and three words long, but H is more meaningful
than query length.
General purpose trigram language models are
effective for the general case (unrestricted text),
but there are important special cases like popular
web queries, where more restricted language mod-
els are even more effective than trigram models.
Our language model for web queries is simply a
list of queries and their probabilities. We consider
queries to be a finite language, unlike unrestricted
text where the trigram language model allows sen-
tences to be arbitrarily long.
Let’s consider another example. The MSN
query was too easy. Suppose we want to find
Condoleezza Rice, but we can’t spell her name.
And even if we could, we wouldn’t want to. Typ-
ing on a phone isn’t fun.
We suggest spelling Condoleezza as 2*, where
2 4 [ABCabc2] and * is the wildcard. We then
type ‘#’ for space. Rice is easy to spell: 7423.
Thus, the user types, 2*#7423, and the system
searches over the MSN query log to produce a list
of k-best (most popular) matches (k defaults to 10):
</bodyText>
<listItem confidence="0.96332125">
1. Anne Rice
2. Book of Shadows
3. Chris Rice
4. Condoleezza Rice
</listItem>
<footnote confidence="0.597887">
1 http://www.google.com/webhp?complete=1
</footnote>
<sectionHeader confidence="0.404403" genericHeader="method">
5. Ann Rice
</sectionHeader>
<bodyText confidence="0.978376428571429">
...
8. Condoleeza Rice
The letters matching constants in the regular ex-
pression are underlined. The other letters match
wildcards. (An implicit wildcard is appended to
the end of the input string.)
Wildcards are very powerful. Strings with
wildcards are more expressive than prefix match-
ing (word wheeling). As mentioned above, it
should take just 19 bits on average to specify one
of the 7 million most popular queries. The query
2*#7423 contains 7 characters in an 12-character
alphabet (2-9 4 [A-Za-z2-9] in the obvious way,
except that 0 4 [QZqz0]; # 4 space; * is wild). 7
characters in a 12 character alphabet is 7 log212 =
25 bits. If the input notation were optimal (which
it isn’t), it shouldn’t be necessary to type much
more than this on average to specify one of the 7
million most popular queries.
Alphabetic ordering causes bizarre behavior.
Yellow Pages are full of company names starting
with A, AA, AAA, etc.. If prefix matching tools like
Google Suggests take off, then it is just a matter of
time before companies start to go after valuable
prefixes: mail, maps, etc. Wildcards can help soci-
ety avoid that non-sense. If you want to find a top
mail site, you can type, “*mail” and you’ll find:
Gmail, Hotmail, Yahoo mail, etc..
</bodyText>
<sectionHeader confidence="0.972317" genericHeader="method">
3 Collaboration &amp; Personalization
</sectionHeader>
<bodyText confidence="0.999645222222222">
Users quickly learn when they can get away with
wildcards. Typing therefore becomes a collabora-
tive exercise, much like Palm’s approach to hand-
writing recognition. Recognition is hard. Rather
than trying to solve the general case, Palm encour-
ages users to work with the system to write in a
way that is easier to recognize (Graffiti). The sys-
tem isn’t trying to solve the AI problem by itself,
but rather there is a man-machine collaboration
where both parties work together as a team.
Collaboration is even more powerful in the
web context. Users issue lots of queries, making it
clear what’s hot (and what’s not). The system con-
structs a language model based on these queries to
direct users toward good stuff. More and more
users will then go there, causing the hot query to
move up in the language model. In this way, col-
laboration can be viewed as a positive feedback
</bodyText>
<page confidence="0.997102">
94
</page>
<bodyText confidence="0.999839111111111">
loop. There is a strong herd instinct; all parties
benefit from the follow-the-pack collaboration.
In addition, users want personalization. When
typing names of our friends and family, technical
terms, etc., we should be able to get away with
more wildcards than other users would. There are
obvious opportunities for personalizing the lan-
guage model by integrating the language model
with a desktop search index (Dumais et al, 2003).
</bodyText>
<sectionHeader confidence="0.974235" genericHeader="method">
4 Modes, Language Models and Apps
</sectionHeader>
<bodyText confidence="0.986673692307692">
The Wild Thing demo has a switch for turning on
and off phone mode to determine whether input
comes from a phone keypad or a standard key-
board. Both with and without phone mode, the
system uses a language model to find the k-best
expansions of the wildcards.
The demo contains a number of different lan-
guage models, including a number of standard tri-
gram language models. Some of the language
models were trained on large quantities (6 Billion
words) of English. Others were trained on large
samples of Spanish and German. Still others were
trained on small sub-domains (such as ATIS,
available from www.ldc.upenn.edu). The demo
also contains two special purpose language models
for searching popular web queries, and popular
web domains.
Different language models are different. With
a trigram language model trained on general Eng-
lish (containing large amounts of newswire col-
lected over the last decade),
pres* rea* *d y* t* it is v*
imp* 4 President Reagan said
yesterday that it is very impor-
tant
With a Spanish Language Model,
</bodyText>
<figure confidence="0.430564">
pres* rea* 4 presidente Reagan
In the ATIS domain,
pres* rea* 4 &lt;UNK&gt; &lt;UNK&gt;
</figure>
<bodyText confidence="0.999845476190476">
The tool can also be used to debug language
models. It turns out that some French slipped into
the English training corpus. Consequently, the
English language model expanded the * in en * de
to some common French words that happen to be
English words as well: raison, circulation, oeuvre,
place, as well as &lt;OOV&gt;. After discovering this,
we discovered quite a few more anomalies in the
training corpus such as headers from the AP news.
There may also be ESL (English as a Second
Language) applications for the tool. Many users
have a stronger active vocabulary than passive vo-
cabulary. If the user has a word stuck on the tip of
their tongue, they can type a suggestive context
with appropriate wildcards and there is a good
chance the system will propose the word the user is
looking for.
Similar tricks are useful in monolingual con-
texts. Suppose you aren’t sure how to spell a ce-
lebrity’s name. If you provide a suggestive
context, the language model is likely to get it right:
</bodyText>
<figure confidence="0.384729333333333">
ron* r*g*n 4 Ronald Reagan
don* r*g*n 4 Donald Regan
c* rice 4 Condoleezza Rice
</figure>
<bodyText confidence="0.526414">
To summarize, wildcards are helpful in quite a
few apps:
</bodyText>
<listItem confidence="0.9998405">
• No keyboard: cell phone, PDA, Tablet PC.
• Speed matters: instant messaging, email.
• Spelling/ESL/tip of the tongue.
• Browsing: direct users toward hot stuff.
</listItem>
<sectionHeader confidence="0.873459" genericHeader="evaluation">
5 Indexing and Compression
</sectionHeader>
<bodyText confidence="0.999853730769231">
The k-best string matching problem raises a num-
ber of interesting technical challenges. We have
two types of language models: trigram language
models and long lists (for finite languages such as
the 7 million most popular web queries).
The long lists are indexed with a suffix array.
Suffix arrays2 generalize very nicely to phone
mode, as described below. We treat the list of web
queries as a text of N bytes. (Newlines are re-
placed with end-of-string delimiters.) The suffix
array, S, is a sequence of N ints. The array is ini-
tialized with the ints from 0 to N−1. Thus, S[i]=i,
for 0≤i&lt;N. Each of these ints represents a string,
starting at position i in the text and extending to the
end of the string. S is then sorted alphabetically.
Suffix arrays make it easy to find the frequency
and location of any substring. For example, given
the substring “mail,” we find the first and last suf-
fix in S that starts with “mail.” The gap between
these two is the frequency. Each suffix in the gap
points to a super-string of “mail.”
To generalize suffix arrays for phone mode we
replace alphabetical order (strcmp) with phone or-
der (phone-strcmp). Both strcmp and phone-
strcmp consider each character one at a time. In
standard alphabetic ordering, ‘a’&lt;‘b’&lt;‘c’, but in
</bodyText>
<footnote confidence="0.987013">
2 An excellent discussion of suffix arrays including source
code can be found at www.cs.dartmouth.edu/~doug.
</footnote>
<page confidence="0.998565">
95
</page>
<bodyText confidence="0.999865636363637">
phone-strcmp, the characters that map to the same
key on the phone keypad are treated as equivalent.
We generalize suffix arrays to take advantage
of popularity weights. We don’t want to find all
queries that contain the substring “mail,” but
rather, just the k-best (most popular). The standard
suffix array method will work, if we add a filter on
the output that searches over the results for the k-
best. However, that filter could take O(N) time if
there are lots of matches, as there typically are for
short queries.
An improvement is to sort the suffix array by
both popularity and alphabetic ordering, alternating
on even and odd depths in the tree. At the first
level, we sort by the first order and then we sort by
the second order and so on, using a construction,
vaguely analogous to KD-Trees (Bentley, 1975).
When searching a node ordered by alphabetical
order, we do what we would do for standard suffix
arrays. But when searching a node ordered by
popularity, we search the more popular half before
the second half. If there are lots of matches, as
there are for short strings, the index makes it very
easy to find the top-k quickly, and we won’t have
to search the second half very often. If the prefix
is rare, then we might have to search both halves,
and therefore, half the splits (those split by popu-
larity) are useless for the worst case, where the
input substring doesn’t match anything in the table.
Lookup is O(sqrt N).3
Wildcard matching is, of course, a different
task from substring matching. Finite State Ma-
chines (Mohri et al, 2002) are the right way to
think about the k-best string matching problem
with wildcards. In practice, the input strings often
contain long anchors of constants (wildcard free
substrings). Suffix arrays can use these anchors to
generate a list of candidates that are then filtered
by a regex package.
3 Let F(N) be the work to process N items on the
frequency splits and let A(N) be the work to proc-
ess N items on the alphabetical splits. In the worst
case, F(N) = 2A(N/2) + C1 and A(N) = F(N/2) + C2,
where C1 and C2 are two constants. In other
words, F(N) = 2F(N/4) + C, where C = C1 + 2C2.
We guess that F(N) = α sqrt(N) + β, where α and β
are constant. Substituting this guess into the recur-
rence, the dependencies on N cancel. Thus, we
conclude, F(N) = O(sqrt N).
Memory is limited in many practical applica-
tions, especially in the mobile context. Much has
been written about lossless compression of lan-
guage models. For trigram models, we use a lossy
method inspired by the Unix Spell program (McIl-
roy, 1982). We map each trigram &lt;x, y, z&gt; into a
hash code h = (V2 x + V y + z) % P, where V is the
size of the vocabulary and P is an appropriate
prime. P trades off memory for loss. The cost to
store N trigrams is: N [1/loge2 + log2(P/N)] bits.
The loss, the probability of a false hit, is 1/P.
The N trigrams are hashed into h hash codes.
The codes are sorted. The differences, x, are en-
coded with a Golomb code4 (Witten et al, 1999),
which is an optimal Huffman code, assuming that
the differences are exponentially distributed, which
they will be, if the hash is Poisson.
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999648285714286">
The Wild Thing encourages users to make use of
wildcards, speeding up typing, especially on cell
phones. Wildcards are useful when you want to
find something you can’t spell, or something stuck
on the tip of your tongue. Wildcards are more
expressive than standard prefix matching, great for
users, and technically challenging (and fun) for us.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.978093125">
J. L. Bentley (1975), Multidimensional binary search
trees used for associative searching, Commun. ACM,
18:9, pp 509-517.
S. T. Dumais, E. Cutrell, et al (2003). Stuff I&apos;ve Seen: A
system for personal information retrieval and re-use,
SIGIR.
M. D. McIlroy (1982), Development of a spelling list,
IEEE Trans. on Communications 30, 91-99.
M. Mohri, F. C. N. Pereira, and M. Riley. Weighted
Finite-State Transducers in Speech Recognition.
Computer Speech and Language, 16(1):69-88, 2002.
I. H. Witten, A. Moffat and T. C. Bell, (1999), Manag-
ing Gigabytes: Compressing and Indexing Docu-
ments and Images, by Morgan Kaufmann Publishing,
San Francisco, ISBN 1-55860-570-3.
4 In Golomb, x = xq m + xr, where xq = floor(x/m)
</reference>
<bodyText confidence="0.935523166666667">
and xr = x mod m. Choose m to be a power of two
near ceil(%2 E[x])=ceil(%2 P/N). Store quotients xq
in unary and remainders xr in binary. z in unary is
a sequence of z−1 zeros followed by a 1. Unary is
an optimal Huffman code when Pr(z)=(%2)z+1. Stor-
age costs are: xq bits for xq + log2m bits for xr.
</bodyText>
<page confidence="0.99316">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.005045">
<title confidence="0.999701">The Wild Thing!</title>
<author confidence="0.999181">Kenneth Church Bo Thiesson</author>
<affiliation confidence="0.999652">Microsoft Research</affiliation>
<address confidence="0.999921">Redmond, WA, 98052, USA</address>
<email confidence="0.999053">church@microsoft.com</email>
<email confidence="0.999053">thiesson@microsoft.com</email>
<abstract confidence="0.979455621082621">Suppose you are on a mobile device with no keyboard (e.g., a cell or PDA). How can you enter text quickly? T9? Graffiti? This demo will show how language modeling can be used to speed up data entry, both in the mobile context, as well as the desktop. The Wild Thing encourages users to use wildcards (*). A language model finds the k-best expansions. Users quickly figure out when they can get away with wildcards. General purpose trigram language models are effective for the general case (unrestricted text), but there are important special cases like searching over popular web queries, where more restricted language models are even more effective. 1 Motivation: Phone App Cell phones and PDAs are everywhere. Users love mobility. What are people doing with their phone? You’d think they would be talking on their phones, but a lot of people are typing. It is considered rude to talk on a cell in certain public places, especially in Europe and Asia. SMS text messaging enables people to communicate, even when they can’t talk. It is bizarre that people are typing on their phones given how painful it is. “Talking on the phone” is a collocation, but “typing on the phone” not. recently ran a story titled: “A Phone You Can Actually Type On” with the lead: “If you&apos;ve tried to zap someone a text message recently, you&apos;ve probably discovered the huge drawback of typing on your cell phone. Unless you&apos;re one of those cyborg Scandinavian teenagers who was born with a Nokia in his hand, pecking out even a simple message is a thumb-twisting chore.” There are great hopes that speech recognition will someday make it unnecessary to type on your phone (for SMS or any other app), but speech recognition won’t help with the rudeness issue. If people are typing because they can’t talk, then speech recognition is not an option. Fortunately, the speech community has developed powerful language modeling techniques that can help even when speech is not an option. 2 K-Best String Matching Suppose we want to search for MSN using a cell phone. A standard approach would be to type 6 777 &lt;pause&gt; 66, where 6 777 66 (The pauses are necessary for disambiguation.) Kids these days are pretty good at typing this way, but there has to be a better solution. T9 (www.t9.com) is an interesting alternative. user types 676 (for The system uses a (unigram) language model to find the k-best matches. The user selects MSN from this list. Some users love T9, and some don’t. The input, 676, can be thought of as short hand for the regular expression: /^[6MNOmno][7PRSprs][6MNOmno]$/ using standard Unix notation. Regular expressions become much more interesting when we consider wildcards. So-called “word wheeling” can be thought of as the special case where we add a wildcard to the end of whatever the user types. Thus, if the user types 676 (for MSN), we would find the k-best matches for: 93 Proceedings of the ACL Interactive Poster and Demonstration Sessions, 93–96, Ann Arbor, June 2005. Association for Computational Linguistics Google a nice example of word wheeling. Google Suggests makes it easy to find popular web queries (in the standard nonmobile desktop context). The user types a prefix. After each character, the system produces a list of popular web queries that start with the specified prefix. Word wheeling not only helps when you know what you want to say, but it also helps when you don’t. Users can’t spell. And things get stuck on the tip of their tongue. Some users are just browsing. They aren’t looking for anything in particular, but they’d like to know what others are looking at. The popular query application is relatively easy in terms of entropy. About 19 bits are needed to specify one of the 7 million most popular web queries. That is, if we assign each web query a probability based on query logs collected at msn.com, then we can estimate entropy, H, and discover that H≈19. (About 23 bits would be needed if these pages were equally likely, but they aren’t.) It is often said that the average query is between two and three words long, but H is more meaningful than query length. General purpose trigram language models are effective for the general case (unrestricted text), but there are important special cases like popular web queries, where more restricted language models are even more effective than trigram models. Our language model for web queries is simply a list of queries and their probabilities. We consider queries to be a finite language, unlike unrestricted text where the trigram language model allows sentences to be arbitrarily long. consider another example. The query was too easy. Suppose we want to find but we can’t spell her name. And even if we could, we wouldn’t want to. Typing on a phone isn’t fun. suggest spelling 2*, where and * is the wildcard. We then type ‘#’ for space. Rice is easy to spell: 7423. Thus, the user types, 2*#7423, and the system searches over the MSN query log to produce a list k-best (most popular) matches to 10): Anne Book of Chris Condoleezza Ann ... Condoleeza The letters matching constants in the regular expression are underlined. The other letters match wildcards. (An implicit wildcard is appended to the end of the input string.) Wildcards are very powerful. Strings with wildcards are more expressive than prefix matching (word wheeling). As mentioned above, it should take just 19 bits on average to specify one of the 7 million most popular queries. The query 2*#7423 contains 7 characters in an 12-character (2-9 in the obvious way, that 0 # * is wild). 7 in a 12 character alphabet is 7 = 25 bits. If the input notation were optimal (which it isn’t), it shouldn’t be necessary to type much more than this on average to specify one of the 7 million most popular queries. Alphabetic ordering causes bizarre behavior. Yellow Pages are full of company names starting AA, etc.. If prefix matching tools like Suggests off, then it is just a matter of time before companies start to go after valuable etc. Wildcards can help society avoid that non-sense. If you want to find a top mail site, you can type, “*mail” and you’ll find: etc.. 3 Collaboration &amp; Personalization Users quickly learn when they can get away with wildcards. Typing therefore becomes a collaborative exercise, much like Palm’s approach to handwriting recognition. Recognition is hard. Rather than trying to solve the general case, Palm encourages users to work with the system to write in a way that is easier to recognize (Graffiti). The system isn’t trying to solve the AI problem by itself, but rather there is a man-machine collaboration where both parties work together as a team. Collaboration is even more powerful in the web context. Users issue lots of queries, making it clear what’s hot (and what’s not). The system constructs a language model based on these queries to direct users toward good stuff. More and more users will then go there, causing the hot query to move up in the language model. In this way, collaboration can be viewed as a positive feedback 94 loop. There is a strong herd instinct; all parties benefit from the follow-the-pack collaboration. In addition, users want personalization. When typing names of our friends and family, technical terms, etc., we should be able to get away with more wildcards than other users would. There are obvious opportunities for personalizing the language model by integrating the language model a desktop search index (Dumais 2003). 4 Modes, Language Models and Apps The Wild Thing demo has a switch for turning on and off phone mode to determine whether input comes from a phone keypad or a standard keyboard. Both with and without phone mode, the system uses a language model to find the k-best expansions of the wildcards. The demo contains a number of different language models, including a number of standard trigram language models. Some of the language models were trained on large quantities (6 Billion words) of English. Others were trained on large samples of Spanish and German. Still others were trained on small sub-domains (such as ATIS, available from www.ldc.upenn.edu). The demo also contains two special purpose language models for searching popular web queries, and popular web domains. Different language models are different. With a trigram language model trained on general English (containing large amounts of newswire collected over the last decade), pres* rea* *d y* t* it is v* Reagan said that is important With a Spanish Language Model, rea* Reagan In the ATIS domain, rea* &lt;UNK&gt; The tool can also be used to debug language models. It turns out that some French slipped into the English training corpus. Consequently, the language model expanded the * in * de to some common French words that happen to be words as well: circulation, oeuvre, as well as &lt;OOV&gt;. After discovering this, we discovered quite a few more anomalies in the training corpus such as headers from the AP news. There may also be ESL (English as a Second Language) applications for the tool. Many users have a stronger active vocabulary than passive vocabulary. If the user has a word stuck on the tip of their tongue, they can type a suggestive context with appropriate wildcards and there is a good chance the system will propose the word the user is looking for. Similar tricks are useful in monolingual contexts. Suppose you aren’t sure how to spell a celebrity’s name. If you provide a suggestive context, the language model is likely to get it right: r*g*n Reagan r*g*n Regan rice Rice To summarize, wildcards are helpful in quite a few apps: • No keyboard: cell phone, PDA, Tablet PC. • Speed matters: instant messaging, email. • Spelling/ESL/tip of the tongue. • Browsing: direct users toward hot stuff. 5 Indexing and Compression The k-best string matching problem raises a number of interesting technical challenges. We have two types of language models: trigram language models and long lists (for finite languages such as the 7 million most popular web queries). The long lists are indexed with a suffix array. very nicely to phone mode, as described below. We treat the list of web as a text of (Newlines are replaced with end-of-string delimiters.) The suffix is a sequence of The array is iniwith the ints from 0 to Thus, Each of these ints represents a string, at position the text and extending to the of the string. then sorted alphabetically. Suffix arrays make it easy to find the frequency and location of any substring. For example, given the substring “mail,” we find the first and last sufin starts with “mail.” The gap between these two is the frequency. Each suffix in the gap points to a super-string of “mail.” To generalize suffix arrays for phone mode we replace alphabetical order (strcmp) with phone order (phone-strcmp). Both strcmp and phonestrcmp consider each character one at a time. In standard alphabetic ordering, ‘a’&lt;‘b’&lt;‘c’, but in excellent discussion of suffix arrays including source code can be found at www.cs.dartmouth.edu/~doug. 95 phone-strcmp, the characters that map to the same key on the phone keypad are treated as equivalent. We generalize suffix arrays to take advantage of popularity weights. We don’t want to find all queries that contain the substring “mail,” but rather, just the k-best (most popular). The standard suffix array method will work, if we add a filter on the output that searches over the results for the k- However, that filter could take time if there are lots of matches, as there typically are for short queries. An improvement is to sort the suffix array by both popularity and alphabetic ordering, alternating on even and odd depths in the tree. At the first level, we sort by the first order and then we sort by the second order and so on, using a construction, vaguely analogous to KD-Trees (Bentley, 1975). When searching a node ordered by alphabetical order, we do what we would do for standard suffix arrays. But when searching a node ordered by popularity, we search the more popular half before the second half. If there are lots of matches, as there are for short strings, the index makes it very easy to find the top-k quickly, and we won’t have to search the second half very often. If the prefix is rare, then we might have to search both halves, and therefore, half the splits (those split by popularity) are useless for the worst case, where the input substring doesn’t match anything in the table. is O(sqrt Wildcard matching is, of course, a different task from substring matching. Finite State Ma- (Mohri 2002) are the right way to think about the k-best string matching problem with wildcards. In practice, the input strings often contain long anchors of constants (wildcard free substrings). Suffix arrays can use these anchors to generate a list of candidates that are then filtered by a regex package. 3Let be the work to process on the splits and let be the work to procon the alphabetical splits. In the worst = + = + two constants. In other = + where = guess that = α + β, where α and β are constant. Substituting this guess into the recurthe dependencies on Thus, we = O(sqrt Memory is limited in many practical applications, especially in the mobile context. Much has been written about lossless compression of language models. For trigram models, we use a lossy method inspired by the Unix Spell program (McIl- 1982). We map each trigram into a code = x + V y + % where the of the vocabulary and an appropriate off memory for loss. The cost to is: + bits. loss, the probability of a false hit, is are hashed into codes. codes are sorted. The differences, are enwith a Golomb (Witten 1999), which is an optimal Huffman code, assuming that the differences are exponentially distributed, which they will be, if the hash is Poisson. 6 Conclusions The Wild Thing encourages users to make use of wildcards, speeding up typing, especially on cell phones. Wildcards are useful when you want to find something you can’t spell, or something stuck on the tip of your tongue. Wildcards are more expressive than standard prefix matching, great for users, and technically challenging (and fun) for us. References J. L. Bentley (1975), Multidimensional binary search used for associative searching, 18:9, pp 509-517. S. T. Dumais, E. Cutrell, et al (2003). Stuff I&apos;ve Seen: A system for personal information retrieval and re-use, SIGIR. M. D. McIlroy (1982), Development of a spelling list,</abstract>
<note confidence="0.9097854">Trans. on Communications 91-99. M. Mohri, F. C. N. Pereira, and M. Riley. Weighted Finite-State Transducers in Speech Recognition. Speech and 16(1):69-88, 2002. H. Witten, A. Moffat and T. C. Bell, (1999), Manag-</note>
<title confidence="0.375267">ing Gigabytes: Compressing and Indexing Docu-</title>
<affiliation confidence="0.957902">and Images, Morgan Kaufmann Publishing,</affiliation>
<address confidence="0.96486">San Francisco, ISBN 1-55860-570-3.</address>
<abstract confidence="0.938812428571429">4In Golomb, = m + where = = x Choose m to be a power of two ceil(%2 Store quotients unary and remainders in binary. unary is a sequence of z−1 zeros followed by a 1. Unary is optimal Huffman code when Storcosts are: bits for + for</abstract>
<intro confidence="0.445284">96</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J L Bentley</author>
</authors>
<title>Multidimensional binary search trees used for associative searching,</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<pages>509--517</pages>
<contexts>
<context position="12701" citStr="Bentley, 1975" startWordPosition="2181" endWordPosition="2182"> that contain the substring “mail,” but rather, just the k-best (most popular). The standard suffix array method will work, if we add a filter on the output that searches over the results for the kbest. However, that filter could take O(N) time if there are lots of matches, as there typically are for short queries. An improvement is to sort the suffix array by both popularity and alphabetic ordering, alternating on even and odd depths in the tree. At the first level, we sort by the first order and then we sort by the second order and so on, using a construction, vaguely analogous to KD-Trees (Bentley, 1975). When searching a node ordered by alphabetical order, we do what we would do for standard suffix arrays. But when searching a node ordered by popularity, we search the more popular half before the second half. If there are lots of matches, as there are for short strings, the index makes it very easy to find the top-k quickly, and we won’t have to search the second half very often. If the prefix is rare, then we might have to search both halves, and therefore, half the splits (those split by popularity) are useless for the worst case, where the input substring doesn’t match anything in the tab</context>
</contexts>
<marker>Bentley, 1975</marker>
<rawString>J. L. Bentley (1975), Multidimensional binary search trees used for associative searching, Commun. ACM, 18:9, pp 509-517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Dumais</author>
<author>E Cutrell</author>
</authors>
<title>Stuff I&apos;ve Seen: A system for personal information retrieval and re-use,</title>
<date>2003</date>
<publisher>SIGIR.</publisher>
<marker>Dumais, Cutrell, 2003</marker>
<rawString>S. T. Dumais, E. Cutrell, et al (2003). Stuff I&apos;ve Seen: A system for personal information retrieval and re-use, SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D McIlroy</author>
</authors>
<title>Development of a spelling list,</title>
<date>1982</date>
<journal>IEEE Trans. on Communications</journal>
<volume>30</volume>
<pages>91--99</pages>
<contexts>
<context position="14452" citStr="McIlroy, 1982" startWordPosition="2500" endWordPosition="2502">ss N items on the alphabetical splits. In the worst case, F(N) = 2A(N/2) + C1 and A(N) = F(N/2) + C2, where C1 and C2 are two constants. In other words, F(N) = 2F(N/4) + C, where C = C1 + 2C2. We guess that F(N) = α sqrt(N) + β, where α and β are constant. Substituting this guess into the recurrence, the dependencies on N cancel. Thus, we conclude, F(N) = O(sqrt N). Memory is limited in many practical applications, especially in the mobile context. Much has been written about lossless compression of language models. For trigram models, we use a lossy method inspired by the Unix Spell program (McIlroy, 1982). We map each trigram &lt;x, y, z&gt; into a hash code h = (V2 x + V y + z) % P, where V is the size of the vocabulary and P is an appropriate prime. P trades off memory for loss. The cost to store N trigrams is: N [1/loge2 + log2(P/N)] bits. The loss, the probability of a false hit, is 1/P. The N trigrams are hashed into h hash codes. The codes are sorted. The differences, x, are encoded with a Golomb code4 (Witten et al, 1999), which is an optimal Huffman code, assuming that the differences are exponentially distributed, which they will be, if the hash is Poisson. 6 Conclusions The Wild Thing enco</context>
</contexts>
<marker>McIlroy, 1982</marker>
<rawString>M. D. McIlroy (1982), Development of a spelling list, IEEE Trans. on Communications 30, 91-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F C N Pereira</author>
<author>M Riley</author>
</authors>
<title>Weighted Finite-State Transducers in Speech Recognition.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<pages>16--1</pages>
<contexts>
<context position="13443" citStr="Mohri et al, 2002" startWordPosition="2311" endWordPosition="2314"> a node ordered by popularity, we search the more popular half before the second half. If there are lots of matches, as there are for short strings, the index makes it very easy to find the top-k quickly, and we won’t have to search the second half very often. If the prefix is rare, then we might have to search both halves, and therefore, half the splits (those split by popularity) are useless for the worst case, where the input substring doesn’t match anything in the table. Lookup is O(sqrt N).3 Wildcard matching is, of course, a different task from substring matching. Finite State Machines (Mohri et al, 2002) are the right way to think about the k-best string matching problem with wildcards. In practice, the input strings often contain long anchors of constants (wildcard free substrings). Suffix arrays can use these anchors to generate a list of candidates that are then filtered by a regex package. 3 Let F(N) be the work to process N items on the frequency splits and let A(N) be the work to process N items on the alphabetical splits. In the worst case, F(N) = 2A(N/2) + C1 and A(N) = F(N/2) + C2, where C1 and C2 are two constants. In other words, F(N) = 2F(N/4) + C, where C = C1 + 2C2. We guess tha</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>M. Mohri, F. C. N. Pereira, and M. Riley. Weighted Finite-State Transducers in Speech Recognition. Computer Speech and Language, 16(1):69-88, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>A Moffat</author>
<author>T C Bell</author>
</authors>
<title>Managing Gigabytes: Compressing and Indexing Documents and Images, by</title>
<date>1999</date>
<pages>1--55860</pages>
<publisher>Morgan Kaufmann Publishing,</publisher>
<location>San Francisco, ISBN</location>
<marker>Witten, Moffat, Bell, 1999</marker>
<rawString>I. H. Witten, A. Moffat and T. C. Bell, (1999), Managing Gigabytes: Compressing and Indexing Documents and Images, by Morgan Kaufmann Publishing, San Francisco, ISBN 1-55860-570-3.</rawString>
</citation>
<citation valid="false">
<booktitle>4 In Golomb, x = xq m + xr, where xq = floor(x/m)</booktitle>
<marker></marker>
<rawString>4 In Golomb, x = xq m + xr, where xq = floor(x/m)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>