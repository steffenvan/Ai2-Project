<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002091">
<title confidence="0.864019">
Improving Question Recommendation by Exploiting Information Need
</title>
<author confidence="0.99851">
Shuguang Li
</author>
<affiliation confidence="0.9985385">
Department of Computer Science
University of York, YO10 5DD, UK
</affiliation>
<email confidence="0.984324">
sgli@cs.york.ac.uk
</email>
<author confidence="0.979415">
Suresh Manandhar
</author>
<affiliation confidence="0.998494">
Department of Computer Science
University of York, YO10 5DD, UK
</affiliation>
<email confidence="0.993161">
suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.998647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994607692307692">
In this paper we address the problem of ques-
tion recommendation from large archives of
community question answering data by ex-
ploiting the users’ information needs. Our
experimental results indicate that questions
based on the same or similar information need
can provide excellent question recommenda-
tion. We show that translation model can be
effectively utilized to predict the information
need given only the user’s query question. Ex-
periments show that the proposed information
need prediction approach can improve the per-
formance of question recommendation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972933333333">
There has recently been a rapid growth in the num-
ber of community question answering (CQA) ser-
vices such as Yahoo! Answers1, Askville2 and
WikiAnswer3 where people answer questions post-
ed by other users. These CQA services have built up
very large archives of questions and their answers.
They provide a valuable resource for question an-
swering research. Table 1 is an example from Ya-
hoo! Answers web site. In the CQA archives, the
title part is the user’s query question, and the user’s
information need is usually expressed as natural lan-
guage statements mixed with questions expressing
their interests in the question body part.
In order to avoid the lag time involved with wait-
ing for a personal response and to enable high quali-
</bodyText>
<footnote confidence="0.999985">
1http://answers.yahoo.com
2http://askville.amazon.com
3http://wiki.answers.com
</footnote>
<page confidence="0.970439">
1425
</page>
<bodyText confidence="0.999688941176471">
ty answers from the archives to be retrieved, we need
to search CQA archives of previous questions that
are closely associated with answers. If a question
is found to be interesting to the user, then a previ-
ous answer can be provided with very little delay.
Question search and question recommendation are
proposed to facilitate finding highly relevant or po-
tentially interesting questions. Given a user’s ques-
tion as the query, question search tries to return
the most semantically similar questions from the
question archives. As the complement of question
search, we define question recommendation as rec-
ommending questions whose information need is the
same or similar to the user’s original question. For
example, the question “What aspects of my com-
puter do I need to upgrade ...” with the informa-
tion need “... making a skate movie, my computer
freezes, ...” and the question “What is the most cost
effective way to expend memory space ...” with in-
formation need “... in need of more space for mu-
sic and pictures ...” are both good recommendation
questions for the user in Table 1. So the recommend-
ed questions are not necessarily identical or similar
to the query question.
In this paper, we discuss methods for question
recommendation based on using the similarity be-
tween information need in the archive. We also
propose two models to predict the information need
based on the query question even if there’s no infor-
mation need expressed in the body of the question.
We show that with the proposed models it is possi-
ble to recommend questions that have the same or
similar information need.
The remainder of the paper is structured as fol-
</bodyText>
<note confidence="0.997936416666667">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1425–1434,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
Q Title If I want a faster computer
should I buy more memory or s-
torage space? ...
Q Body I edit pictures and videos so I
need them to work quickly. Any
advice?
Answer ... If you are running out of s-
pace on your hard drive, then
... to boost your computer speed
usually requires more RAM ...
</note>
<tableCaption confidence="0.998352">
Table 1: Yahoo! Answers question example
</tableCaption>
<bodyText confidence="0.997895444444444">
lows. In section 2, we briefly describe the related
work on question search and recommendation. Sec-
tion 3 addresses in detail how we measure the sim-
ilarity between short texts. Section 4 describes two
models for information need prediction that we use
for the experiment. Section 5 tests the performance
of the proposed models for the task of question rec-
ommendation. Section 7 is the conclusion of this
paper.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.944544">
2.1 Question Search
</subsectionHeader>
<bodyText confidence="0.999988315789474">
Burke et al. (1997) combined a lexical metric and a
simple semantic knowledge-based (WordNet) simi-
larity method to retrieve semantically similar ques-
tions from frequently asked question (FAQ) data.
Jeon et al. (2005a) retrieved semantically similar
questions from Korean CQA data by calculating the
similarity between their answers. The assumption
behind their research is that questions with very sim-
ilar answers tend to be semantically similar. Jeon
et al. (2005b) also discussed methods for grouping
similar questions based on using the similarity be-
tween answers in the archive. These grouped ques-
tion pairs were further used as training data to es-
timate probabilities for a translation-based question
retrieval model. Wang et al. (2009) proposed a tree
kernel framework to find similar questions in the C-
QA archive based on syntactic tree structures. Wang
et al. (2010) mined lexical and syntactic features to
detect question sentences in CQA data.
</bodyText>
<subsectionHeader confidence="0.999757">
2.2 Question Recommendation
</subsectionHeader>
<bodyText confidence="0.9999493125">
Wu et al. (2008) presented an incremental auto-
matic question recommendation framework based
on probabilistic latent semantic analysis. Question
recommendation in their work considered both the
users’ interests and feedback. Duan et al. (2008)
made use of a tree-cut model to represent question-
s as graphs of topic terms. Questions were recom-
mended based on this topic graph. The recommend-
ed questions can provide different aspects around the
topic of the query question.
The above question search and recommendation
research provide different ways to retrieve question-
s from large archives of question answering data.
However, none of them considers the similarity or
diversity between questions by exploring their infor-
mation needs.
</bodyText>
<sectionHeader confidence="0.988291" genericHeader="method">
3 Short Text Similarity Measures
</sectionHeader>
<bodyText confidence="0.999950411764706">
In question retrieval systems accurate similarity
measures between documents are crucial. Most tra-
ditional techniques for measuring the similarity be-
tween two documents mainly focus on comparing
word co-occurrences. The methods employing this
strategy for documents can usually achieve good re-
sults, because they may share more common words
than short text snippets. However the state-of-the-
art techniques usually fail to achieve desired results
due to short questions and information need texts.
In order to measure the similarity between short
texts, we make use of three kinds of text similari-
ty measures: TFIDF based, Knowledge based and
Latent Dirichlet Allocation (LDA) based similarity
measures in this paper. We will compare their per-
formance for the task of question recommendation
in the experiment section.
</bodyText>
<subsectionHeader confidence="0.991183">
3.1 TFIDF
</subsectionHeader>
<bodyText confidence="0.9971705">
Baeza-Yates and Ribeiro-Neto (1999) provides a T-
FIDF method to calculate the similarity between two
texts. Each document is represented by a term vec-
tor using TFIDF score. The similarity between two
text Di and Dj is the cosine similarity in the vector
space model:
</bodyText>
<equation confidence="0.79391325">
DTi Dj
cos(Di, Dj) _
11D
i1111Dj11
</equation>
<page confidence="0.969991">
1426
</page>
<bodyText confidence="0.999818545454545">
This method is used in most information retrieval
systems as it is both efficient and effective. Howev-
er if the query text contains only one or two words
this method will be biased to shorter answer texts
(Jeon et al., 2005a). We also found that in CQA data
short contents in the question body cannot provide
any information about the users’ information needs.
Based on the above two reasons, in the test data sets
we do not include the questions whose information
need parts contain only a few noninformative words
.
</bodyText>
<subsectionHeader confidence="0.996476">
3.2 Knowledge-based Measure
</subsectionHeader>
<bodyText confidence="0.999413518518519">
Mihalcea et al. (2006) proposed several knowledge-
based methods for measuring the semantic level sim-
ilarity of texts to solve the lexical chasm problem be-
tween short texts. These knowledge-based similarity
measures were derived from word semantic similar-
ity by making use of WordNet. The evaluation on a
paraphrase recognition task showed that knowledge-
based measures outperform the simpler lexical level
approach.
We follow the definition in (Mihalcea et al., 2006)
to derive a text-to-text similarity metric mcs for two
given texts Di and Dj:
topics discovered by Latent Dirichlet Allocation (L-
DA) methods.
In contrast to the TFIDF method which measures
“common words”, short texts are not compared to
each other directly in probabilistic topic models. In-
stead, the texts are compared using some “third-
party” topics that relate to them. A passage D in the
retrieved documents (document collection) is repre-
sented as a mixture of fixed topics, with topic z get-
ting weight θ(D) zin passage D and each topic is a
distribution over a finite vocabulary of words, with
word w having a probability φ(z)
w in topic z. Gibbs
Sampling can be used to estimate the corresponding
expected posterior probabilities P(zID) = ˆθ(D)
</bodyText>
<equation confidence="0.9494195">
z and
P(wIz) = ˆφ(z) w(Griffiths and Steyvers, 2004).
</equation>
<bodyText confidence="0.995102333333333">
In this paper we use two LDA based similarity
measures in (Celikyilmaz et al., 2010) to measure
the similarity between short information need texts.
The first LDA similarity method uses KL divergence
to measure the similarity between two documents
under each given topic:
</bodyText>
<equation confidence="0.998084733333333">
�K
1
simLDA1(Di, Dj) = K
k=1
10W (D(z=k) ,D(z=k)
j )
i
EwEDi maxSim(w, Dj) * idf(w)
mcs(Di, Dj) = EwEDi idf(w)
W (D(z=k)
i , D(z=k)
j ) =
E
wEDj maxSim(w, Di) * idf(w)
EwEDj idf(w)
</equation>
<bodyText confidence="0.998836">
For each word w in Di, maxSim(w, Dj) com-
putes the maximum semantic similarity between w
and any word in Dj. In this paper we choose lin
(Lin, 1998) and jcn (Jiang and Conrath, 1997) to
compute the word-to-word semantic similarity.
We only choose nouns and verbs for calculating
mcs. Additionally, when w is a noun we restrict
the words in document Di (and Dj) to just nouns.
Similarly, when w is a verb, we restrict the words in
document Di (and Dj) to just verbs.
</bodyText>
<subsectionHeader confidence="0.997628">
3.3 Probabilistic Topic Model
</subsectionHeader>
<bodyText confidence="0.99996975">
Celikyilmaz et al. (2010) presented probabilistic
topic model based methods to measure the similar-
ity between question and candidate answers. The
candidate answers were ranked based on the hidden
</bodyText>
<equation confidence="0.99483475">
D(z=k) + D(z=k)
− KL(D�z=k) I i j )
2
D(z=k) + D(z=k)
− KL(D(z=k) I i j )
2
W(D(z=k)
i ,D(z=k)
</equation>
<bodyText confidence="0.975016875">
j ) calculates the similarity be-
tween two documents under topic z = k using KL
divergence measure. D(z=k) iis the probability distri-
bution of words in document Di given a fixed topic
z.
The second LDA similarity measure from (Grif-
fiths and Steyvers, 2004) treats each document as a
probability distribution of topics:
</bodyText>
<equation confidence="0.521948">
simLDA2(Di, Dj) = 10W (ˆθ(Di),ˆθ(Dj))
</equation>
<bodyText confidence="0.969040333333333">
where ˆθ(Di) is document Di’s probability distribu-
tion of topics as defined earlier.
+
</bodyText>
<page confidence="0.987974">
1427
</page>
<sectionHeader confidence="0.9132025" genericHeader="method">
4 Information Need Prediction using
Statistical Machine Translation Model
</sectionHeader>
<bodyText confidence="0.999969666666666">
There are two reasons that we need to predict in-
formation need. It is often the case that the query
question does not have a question body part. So we
need a model to predict the information need part
based on the query question in order to recommend
questions based on the similarity of their informa-
tion needs. Another reason is that information need
prediction plays a crucial part not only in Question
Answering but also in information retrieval (Liu et
al., 2008). In this paper we propose an information
need prediction method based on a statistical ma-
chine translation model.
</bodyText>
<subsectionHeader confidence="0.995014">
4.1 Statistical Machine Translation Model
</subsectionHeader>
<bodyText confidence="0.9997361">
(f(s), e(s)), s = 1,...,5 is a parallel corpus. In a
sentence pair (f, e), source language String, f =
f1f2...fJ has J words, and e = e1e2...eI has I word-
s. And alignment a = a1a2...aJ represents the map-
ping information from source language words to tar-
get words.
Statistical machine translation models estimate
Pr(f|e), the translation probability from source lan-
guage string e to target language string f (Och et al.,
2003):
</bodyText>
<equation confidence="0.7342335">
Pr(f|e) = � Pr(f, a|e)
a
</equation>
<bodyText confidence="0.9977025">
EM-algorithm is usually used to train the align-
ment models to estimate lexicon parameters p(f|e).
In E-step, the counts for one sentence pair (f ,e)
are:
</bodyText>
<equation confidence="0.967874166666667">
c(f|e; f, e) = � Pr(a|f, e) � W, fj)s(e, eaj)
a i,j
Pr(a|f, e) = Pr(f, a|e)/Pr(a|e)
In the M-step, lexicon parameters become:
p(f|e) a � c(f|e; f(s), e(s))
s
</equation>
<bodyText confidence="0.999503625">
Different alignment models such as IBM-1 to
IBM-5 (Brown et al., 1993) and HMM model (Och
and Ney, 2000) provide different decompositions of
Pr(f, a|e). For different alignment models differ-
ent approaches were proposed to estimate the cor-
responding alignments and parameters. The detail-
s can be found in (Och et al., 2003; Brown et al.,
1993).
</bodyText>
<subsectionHeader confidence="0.9559">
4.2 Information Need Prediction
</subsectionHeader>
<bodyText confidence="0.999976">
After estimating the statistical translation probabili-
ties, we treat the information need prediction as the
process of ranking words by p(w|Q), the probability
of generating word w from question Q:
</bodyText>
<equation confidence="0.996185">
P(w|Q) = A � Ptr(w|t)P(t|Q)+(1−A)P(w|C)
tEQ
</equation>
<bodyText confidence="0.999605083333333">
The word-to-word translation probability
Ptr(w|t) is the probability of word w is translated
from a word t in question Q using the translation
model. The above formula uses linear interpolation
smoothing of the document model with the back-
ground language model P(t|C). A is the smoothing
parameter. P(t|Q) and P(t|C) are estimated using
the maximum likelihood estimator.
One important consideration is that statistical ma-
chine translation models first estimate Pr(f|e) and
then calculate Pr(e|f) using Bayes’ theorem to min-
imize ordering errors (Brown et al., 1993):
</bodyText>
<equation confidence="0.997256">
Pr(f|e)Pr(e)
Pr(e|f) = Pr(f)
</equation>
<bodyText confidence="0.9999802">
But in this paper, we skip this step as we found out
the order of words in information need part is not
an important factor. In our collected CQA archive,
question title and information need pairs can be con-
sidered as a type of parallel corpus, which is used
for estimating word-to-word translation probabili-
ties. More specifically, we estimated the IBM-4
model by GIZA++4 with the question part as the
source language and information need part as the tar-
get language.
</bodyText>
<sectionHeader confidence="0.99883" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.961915">
5.1 Text Preprocessing
</subsectionHeader>
<bodyText confidence="0.9989835">
The questions posted on community QA sites often
contain spelling or grammar errors. These errors in-
</bodyText>
<footnote confidence="0.967678">
4http://fjoch.com/GIZA++.html
</footnote>
<page confidence="0.846532">
1428
</page>
<table confidence="0.999905428571429">
Test c Test t
Methods MRR Precision@5 Precision@10 MRR Precision@5 Precision@10
TFIDF 84.2% 67.1% 61.9% 92.8% 74.8% 63.3%
Knowledge1 82.2% 65.0% 65.6% 78.1% 67.0% 69.6%
Knowledge2 76.7% 54.9% 59.3% 61.6% 53.3% 58.2%
LDA1 92.5% 68.8% 64.7% 91.8% 75.4% 69.8%
LDA2 61.5% 55.3% 60.2% 52.1% 57.4% 54.5%
</table>
<tableCaption confidence="0.915347">
Table 2: Question recommendation results without information need prediction
</tableCaption>
<table confidence="0.999920142857143">
Test c Test t
Methods MRR Precision@5 Precision@10 MRR Precision@5 Precision@10
TFIDF 86.2% 70.8% 64.3% 95.1% 77.8% 69.3%
Knowledge1 82.2% 65.0% 66.6% 76.7% 68.0% 68.7%
Knowledge2 76.7% 54.9% 60.2% 61.6% 53.3% 58.2%
LDA1 95.8% 72.4% 68.2% 96.2% 79.5% 69.2%
LDA2 61.5% 55.3% 58.9% 68.1% 58.3% 53.9%
</table>
<tableCaption confidence="0.999752">
Table 3: Question recommendation results with information need predicted by translation model
</tableCaption>
<bodyText confidence="0.999928444444445">
fluence the calculation of similarity and the perfor-
mance of information retrieval (Zhao et al., 2007;
Bunescu and Huang, 2010). In this paper, we use
an open source software afterthedeadline5 to auto-
matically correct the spelling errors in the question
and information need texts first. We also made use
of Web 1T 5-gram6 to implement an N-Gram based
method (Cheng et al., 2008) to further filter out the
false positive corrections and re-rank correction sug-
gestions (Mudge, 2010). The texts are tagged by
Brill’s Part-of-Speech Tagger7 as the rule-based tag-
ger is more robust than the state-of-art statistical tag-
gers for raw web contents. This tagging informa-
tion is only used for WordNet similarity calculation.
Stop word removal and lemmatization are applied
to the all the raw texts before feeding into machine
translation model training, the LDA model estimat-
ing and similarity calculation.
</bodyText>
<subsectionHeader confidence="0.99975">
5.2 Construction of Training and Testing Sets
</subsectionHeader>
<bodyText confidence="0.999989">
We made use of the questions crawled from Yahoo!
Answers for the estimating models and evaluation.
More specifically, we obtained 2 million questions
under two categories at Yahoo! Answers: ‘travel’
</bodyText>
<footnote confidence="0.9998605">
5http://afterthedeadline.com
6http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?cata
logId=LDC2006T13
7http://www.umiacs.umd.edu/ jimmylin/resources.html
</footnote>
<bodyText confidence="0.9846406875">
(1 million), and ‘computers&amp;internet’ (1 million).
Depending on whether the best answers have been
chosen by the asker, questions from Yahoo! answers
can be divided into ‘resolved’ and ‘unresolved’ cat-
egories. From each of the above two categories, we
randomly selected 200 resolved questions to con-
struct two testing data sets: ‘Test t’ (‘travel’), and
‘Test c’ (‘computers&amp;internet’). In order to mea-
sure the information need similarity in our experi-
ment we selected only those questions whose infor-
mation needs part contained at least 3 informative
words after stop word removal. The rest of the ques-
tions ‘Train t’ and ‘Train c’ under the two categories
are left for estimating the LDA topic models and the
translation models. We will show how we obtain
these models later.
</bodyText>
<subsectionHeader confidence="0.987946">
5.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9998217">
For each question (query question) in ‘Test t’ or
‘Test c’, we used the words in the question title part
as the main search query and the other words in the
information need part as search query expansion to
retrieve candidate recommended questions from Ya-
hoo! Answers website. We obtained an average of
154 resolved questions under ‘travel’ or ‘computer-
s&amp;internet’ category, and three assessors were in-
volved in the manual judgments.
Given a question returned by a recommendation
</bodyText>
<page confidence="0.9913">
1429
</page>
<bodyText confidence="0.999962">
method, two assessors are asked to label it with
‘good’ or ‘bad’. The third assessor will judge the
conflicts. The assessors are also asked to read the in-
formation need and answer parts. If a recommended
question is considered to express the same or similar
information need, the assessor will label it ‘good’;
otherwise, the assessor will label it as ‘bad’.
Three measures for evaluating the recommenda-
tion performance are utilized. They are Mean Re-
ciprocal Rank (MRR), top five prediction accura-
cy (precision@5) and top ten prediction accuracies
(precision@10) (Voorhees and Tice, 2004; Cao et
al., 2008). In MRR the reciprocal rank of a query
question is the multiplicative inverse of the rank of
the first ‘good’ recommended question. The top five
prediction accuracy for a query question is the num-
ber of ‘good’ recommended questions out of the top
five ranked questions and the top ten accuracy is cal-
culated out of the top ten ranked questions.
</bodyText>
<subsectionHeader confidence="0.991615">
5.4 Similarity Measure
</subsectionHeader>
<bodyText confidence="0.999993416666667">
The first experiment conducted question recommen-
dation based on their information need parts. Dif-
ferent text similarity methods described in section
3 were used to measure the similarity between the
information need texts. In TFIDF similarity mea-
sure (TFIDF), the idf values for each word were
computed from frequency counts over the entire
Aquaint corpus8. For calculating the word-to-word
knowledge-based similarity, a WordNet::Similarity
Java implementation9 of the similarity measures lin
(Knowledge2) and jcn (Knowledge1) is used in this
paper. For calculating topic model based similarity,
we estimated two LDA models from ’Train t’ and
’Train c’ using GibbsLDA++10. We treated each
question including the question title and the infor-
mation need part as a single document of a sequence
of words. These documents were preprocessed be-
fore being fed into LDA model. 1800 iterations for
Gibbs sampling 200 topics parameters were set for
each LDA model estimation.
The results in table 2 show that TFIDF and LDA1
methods perform better for recommending questions
than the others. After further analysis of the ques-
tions recommended by both methods, we discov-
</bodyText>
<footnote confidence="0.999444666666667">
8http://ldc.upenn.edu/Catalog/docs/LDC2002T31
9http://cogs.susx.ac.uk/users/drh21/
10http://gibbslda.sourceforge.net
</footnote>
<figureCaption confidence="0.96423878125">
Q1: If I want a faster computer should I buy
more memory or storage space?
InfoN If I want a faster computer should I buy
more memory or storage space? What-
s the difference? I edit pictures and
videos so I need them to work quickly.
...
RQ1 Would buying 1gb memory upgrade
make my computer faster?
InfoN I have an inspiron B130. It has 512mb
memory now. I would add another 1gb
into 2nd slot ...
RQ2 whats the difference between memory
and hard drive space on a computer and
why is ?
InfoN see I am starting edit videos on my com-
puter but i am running out of space. why
is so expensive to buy memory but not
external drives? ...
Q2: Where should my family go for spring
break?
InfoN ... family wants to go somewhere for
a couple days during spring break ...
prefers a warmer climate and we live in
IL, so it shouldn’t be SUPER far away.
... a family road trip....
RQ1 Whats a cheap travel destination for
spring break?
InfoN I live in houston texas and i’m trying to
find i inexpensive place to go for spring
break with my family.My parents don’t
want to spend a lot of money due to the
</figureCaption>
<table confidence="0.9433584">
economy crisis, ... a fun road trip...
RQ2 Alright you creative deal-seekers, I need
some help in planning a spring break
trip for my family
InfoN Spring break starts March 13th and goes
until the 21st ... Someplace WARM!!!
Family-oriented hotel/resort ... North
American Continent (Mexico, America,
Jamaica, Bahamas, etc.) Cost= Around
$5,000
</table>
<tableCaption confidence="0.9938895">
Table 4: Question recommendation results by LDA mea-
suring the similarity between information needs
</tableCaption>
<page confidence="0.990802">
1430
</page>
<bodyText confidence="0.999940257142857">
ered that the ordering of the recommended questions
from TFIDF and LDA1 are quite different. TFIDF
similarity method prefers texts with more common
words, while the LDA1 method can find the rela-
tion between the non-common words between short
texts based on a series of third-party topics. The L-
DA1 method outperforms the TFIDF method in two
ways: (1) the top recommended questions’ informa-
tion needs share less common words with the query
question’s; (2) the top recommended questions span
wider topics. The questions highly recommended by
LDA1 can suggest more useful topics to the user.
Knowledge-based methods are also shown to per-
form worse than TFIDF and LDA1. We found that
some words were mis-tagged so that they were not
included in the word-to-word similarity calculation.
Another reason for the worse performance is that the
words out of the WordNet dictionary were also not
included in the similarity calculation.
The Mean Reciprocal Rank score for TFIDF and
LDA1 are more than 80%. That is to say, we are able
to recommend questions to the users by measuring
their information needs. The first two recommended
questions for Q1 and Q2 using LDA1 method are
shown in table 4. InfoN is the information need part
associated with each question.
In the preprocessing step, some words were suc-
cessfully corrected such as “What should I do this
saturday? ... and staying in a hotell ...” and “my
faimly is traveling to Borda ...”. However, there are
still a small number of texts such as “How come my
Gforce visualization doesn’t work?” and “Do i need
an Id to travel from new york to maimi?” failed to
be corrected. So in the future, a better method is
expected to correct these failure cases.
</bodyText>
<subsectionHeader confidence="0.984494">
5.5 Information Need Prediction
</subsectionHeader>
<bodyText confidence="0.998077728813559">
There are some retrieved questions whose informa-
tion need parts are empty or become empty or al-
most empty (one or two words left) after the prepro-
cessing step. The average number of such retrieved
questions for each query question is 10 in our exper-
iment. The similarity ranking scores of these ques-
tions are quite low or zero in the previous experi-
ment. In this experiment, we will apply information
need prediction to the questions whose information
needs are missing in order to find out whether we
improve the recommendation task.
The question and information need pairs in both
‘Train t’ and ‘Train c’ training sets were used to
train two IBM-4 translation models by GIZA++
toolkit. These pairs were also preprocessed before
training. And the pairs whose information need part
become empty after preprocessing were disregard-
ed.
During the experiment, we found that some of the
generated words in the information need parts are
themselves. This is caused by the self translation
problem in translation model: the highest transla-
tion score for a word is usually given to itself if
the target and source languages are the same (Xue
et al., 2008). This has always been a tough ques-
tion: not using self-translated words can reduce re-
trieval performance as the information need parts
need the terms to represent the semantic meanings;
using self-translated words does not take advantage
of the translation approach. To tackle this problem,
we control the number of the words predicted by the
translation model to be exactly twice the number of
words in the corresponding preprocessed question.
The predicted information need words for the re-
trieved questions are shown in Table 5. In Q1, the in-
formation need behind question “recommend web-
site for custom built computer parts” may imply
that the users need to know some information about
building computer parts such as “ram” and “moth-
erboard” for a different purpose such as “gaming”.
While in Q2, the user may want to compare comput-
ers in different brands such as “dell” and “mac” or
consider the “price” factor for “purchasing a laptop
for a college student”.
We also did a small scale comparison between the
generated information needs against the real ques-
tions whose information need parts are not empty.
Q3 and Q4 in Table 5 are two examples. The orig-
inal information need for Q3 is “looking for beauti-
ful beaches and other things to do such as museum-
s, zoos, shopping, and great seafood” in CQA. The
generated content for Q3 contains words in wider
topics such as ‘wedding’, ‘surf’ and the price infor-
mation (‘cheap’). This reflects that there are some
other users asking similar questions with the same
or other interests.
From the results in Table 3, we can see that the
performance of most similarity methods were im-
proved by making use of information need predic-
</bodyText>
<page confidence="0.982037">
1431
</page>
<bodyText confidence="0.999818424242424">
tion. Different similarity measures received differ-
ent degrees of improvement. LDA1 obtained the
highest improvement followed by the TFIDF based
method. These two approaches are more sensitive to
the contents generated by a translation model.
However we found out that in some cases the L-
DA1 model failed to give higher scores to good rec-
ommendation questions. For example, Q5, Q6, and
Q7 in table 5 were retrieved as recommendation can-
didates for the query question in Table 1. All of the
three questions were good recommendation candi-
dates, but only Q6 ranked fifth while Q5 and Q7
were out of the top 30 by LDA1 method. Moreover,
in a small number of cases bad recommendation
questions received higher scores and jeopardized the
performance. For example, for query question “How
can you add subtitles to videos?” with information
need “... add subtitles to a music video ... got off
youtube ...download for this ...”, a retrieved ques-
tion “How would i add a music file to a video clip.
...” was highly recommended by TFIDF approach
as predicted information need contained ‘youtube’,
‘video’, ‘music’, ‘download’, ... .
The MRR score received an improvement from
92.5% to 95.8% in the ‘Test c’ and from 91.8% to
96.2% in ‘Test t’. This means that the top one ques-
tion recommended by our methods can be quite well
catering to the users’ information needs. The top
five precision and the top ten precision scores us-
ing TFIDF and LDA1 methods also received dif-
ferent degrees of improvement. Thus, we can im-
prove the performance of question recommendation
by predicting information needs.
</bodyText>
<sectionHeader confidence="0.999556" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.985752886363636">
In this paper we addressed the problem of recom-
mending questions from large archives of commu-
nity question answering data based on users’ infor-
mation needs. We also utilized a translation mod-
el and a LDA topic model to predict the informa-
tion need only given the user’s query question. D-
ifferent information need similarity measures were
compared to prove that it is possible to satisfy user’s
information need by recommending questions from
large archives of community QA. The Latent Dirich-
let allocation based approach was proved to perfor-
m better on measuring the similarity between short
Please recommend A good website for
Custom Built Computer parts?
InfoN custom, site, ram, recommend, price,
motherboard, gaming,...
What is the best laptop for a college stu-
dent?
InfoN know, brand, laptop, college, buy, price,
dell, mac, ...
What is the best Florida beach for a honey-
moon?
InfoN Florida, beach, honeymoon, wedding, surf,
cheap, fun, ...
Are there any good clubs in Manchester
InfoN club, bar, Manchester, music, age, fun,
drink, dance, ...
If i buy a video card for my computer will
that make it faster?
InfoN nvidia, video, ati, youtube, card, buy, win-
dow, slow, computer, graphics, geforce,
faster, ...
If I buy a bigger hard drive for my laptop,
will it make my computer run faster or just
increase the memory?
InfoN laptop, ram, run, buy, bigger, memory,
computer, increase, gb, hard, drive, faster,
...
Is there a way I can make my computer
work faster rather than just increasing the
ram or harware space?
InfoN space, speed, ram, hardware, main, gig, s-
low, computer, increase, work, gb, faster,
...
</bodyText>
<tableCaption confidence="0.98604">
Table 5: Information need prediction examples using
IBM-4 translation model
</tableCaption>
<page confidence="0.993399">
1432
</page>
<bodyText confidence="0.999669">
texts in the semantic level than traditional method-
s. Experiments showed that the proposed transla-
tion based language model for question information
need prediction further enhanced the performance of
question recommendation methods.
</bodyText>
<sectionHeader confidence="0.998912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998766804123712">
Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval. Addison-Wesley
Longman Publishing Co., Inc., Boston, MA, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, Robert L. Mercer. 1993. The mathematics of
statistical machine translation: parameter estimation.
Computational Linguistics, v.19 n.2, June 1993.
Razvan Bunescu and Yunfeng Huang. 2010. Learning the
Relative Usefulness of Questions in Community QA.
Proceedings of the Conference on Empirical Method-
s in Natural Language Processing (EMNLP) , Cam-
bridge, MA.
Robin D. Burke and Kristian J. Hammond and Vladimir
A. Kulyukin and Steven L. Lytinen and Noriko To-
muro and Scott Schoenberg. 1997. Question answer-
ing from frequently-asked question files: Experiences
with the FAQ Finder system. AI Magazine, 18, 57C66.
Yunbo Cao, Huizhong Duan, Chin-Yew Lin, Yong Yu,
and Hsiao-Wuen Hon. 2008. Recommending Ques-
tions Using the MDL-based Tree Cut Model. In: Proc.
of the 17th Int. Conf. on World Wide Web, pp. 81-90.
Asli Celikyilmaz and Dilek Hakkani-Tur and Gokhan
Tur. 2010. LDA Based Similarity Modeling for Ques-
tion Answering. In NAACL 2010 C Workshop on Se-
mantic Search.
Charibeth Cheng, Cedric Paul Alberto, Ian Anthony
Chan, and Vazir Joshua Querol. 2008. SpellCheF:
Spelling Checker and Corrector for Filipino. Journal
of Research in Science, Computing and Engineering,
North America, 4, sep. 2008.
Lynn Silipigni Connaway and Chandra Prabha. 2005. An
overview of the IMLS Project “Sense-making the in-
formation confluence: The whys and hows of college
and university user satisficing of information needs”.
Presented at Library of Congress Forum, American
Library Association Midwinter Conference, Boston,
MA, Jan 16, 2005.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In HLT-ACL, pages
156C164.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Natl Acad Sci 101:5228C5235.
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005a.
Finding semantically similar questions based on their
answers. In Proc. of SIGIR05.
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005b.
Finding similar questions in large question and an-
swer archives. In CIKM, pages 84C90.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxono-
my. In Proceedings of International Conference on Re-
search in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning (ICML ’98),
Jude W. Shavlik (Ed.). Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA, 296-304.
Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008.
Predicting information seeker satisfaction in commu-
nity question answering. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval (SI-
GIR ’08). ACM, New York, NY, USA, 483-490.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
national conference on Artificial intelligence (AAAI
’06), pages 775C780. AAAI Press.
Raphael Mudge. 2010. The design of a proofreading soft-
ware service. In Proceedings of the NAACL HLT 2010
Workshop on Computational Linguistics and Writing:
Writing Processes and Authoring Aids (CL&amp;W ’10).
Association for Computational Linguistics, Morris-
town, NJ, USA, 24-32.
Franz Josef Och, Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation.
Proceedings of the 18th conference on Computational
linguistics, July 31-August 04, Saarbrucken, Germany.
Franz Josef Och, Hermann Ney. 2003.A Systematic Com-
parison of Various Statistical Alignment Models. Com-
putational Linguistics, volume 29, number 1, pp. 19-
51 March 2003.
Jahna Otterbacher, Gunes Erkan, Dragomir R. Radev.
2009. Biased LexRank: Passage retrieval using ran-
dom walks with question-based priors. Information
Processing and Management: an International Journal,
v.45 n.1, p.42-54, January, 2009.
Chandra Prabha, Lynn Silipigni Connaway, Lawrence
Olszewski, Lillie R. Jenkins. 2007. What is enough?
Satisficing information needs. Journal of Documenta-
tion (January, 63,1).
Ellen Voorhees and Dawn Tice. 2000. The TREC-8 ques-
tion answering track evaluation. In Text Retrieval
Conference TREC-8, Gaithersburg, MD.
Kai Wang, Yanming Zhao, and Tat-Seng Chua. 2009.
A syntactic tree matching approach to finding similar
</reference>
<page confidence="0.635578">
1433
</page>
<reference confidence="0.998287210526316">
questions in community-based qa services. In SIGIR,
pages 187C194.
Kai Wang and Tat-Seng Chua. 2010. Exploiting salient
patterns for question detection and question retrieval
in community-based question answering. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING ’10). Association for
Computational Linguistics, Stroudsburg, PA, USA,
1155-1163.
Hu Wu, Yongji Wang, and Xiang Cheng. 2008. Incremen-
tal probabilistic latent semantic analysis for automatic
question recommendation. In RecSys.
Xiaobing Xue, Jiwoon Jeon, W. Bruce Croft. 2008. Re-
trieval models for question and answer archives. In
SIGIR’08, pages 475C482. ACM.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning
Question Paraphrases for QA from Encarta Logs. In
Proceedings of International Joint Conferences on Ar-
tificial Intelligence (IJCAI), pages 1795-1800.
</reference>
<page confidence="0.995073">
1434
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.485335">
<title confidence="0.99897">Improving Question Recommendation by Exploiting Information Need</title>
<author confidence="0.828224">Shuguang</author>
<affiliation confidence="0.993401">Department of Computer University of York, YO10 5DD,</affiliation>
<email confidence="0.994307">sgli@cs.york.ac.uk</email>
<author confidence="0.613995">Suresh</author>
<affiliation confidence="0.992918">Department of Computer University of York, YO10 5DD,</affiliation>
<email confidence="0.998111">suresh@cs.york.ac.uk</email>
<abstract confidence="0.999120214285714">In this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users’ information needs. Our experimental results indicate that questions based on the same or similar information need can provide excellent question recommendation. We show that translation model can be effectively utilized to predict the information need given only the user’s query question. Experiments show that the proposed information need prediction approach can improve the performance of question recommendation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo A Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>Addison-Wesley Longman Publishing Co., Inc.,</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="6897" citStr="Baeza-Yates and Ribeiro-Neto (1999)" startWordPosition="1086" endWordPosition="1089">employing this strategy for documents can usually achieve good results, because they may share more common words than short text snippets. However the state-of-theart techniques usually fail to achieve desired results due to short questions and information need texts. In order to measure the similarity between short texts, we make use of three kinds of text similarity measures: TFIDF based, Knowledge based and Latent Dirichlet Allocation (LDA) based similarity measures in this paper. We will compare their performance for the task of question recommendation in the experiment section. 3.1 TFIDF Baeza-Yates and Ribeiro-Neto (1999) provides a TFIDF method to calculate the similarity between two texts. Each document is represented by a term vector using TFIDF score. The similarity between two text Di and Dj is the cosine similarity in the vector space model: DTi Dj cos(Di, Dj) _ 11D i1111Dj11 1426 This method is used in most information retrieval systems as it is both efficient and effective. However if the query text contains only one or two words this method will be biased to shorter answer texts (Jeon et al., 2005a). We also found that in CQA data short contents in the question body cannot provide any information abou</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<booktitle>Computational Linguistics, v.19 n.2,</booktitle>
<contexts>
<context position="12204" citStr="Brown et al., 1993" startWordPosition="1998" endWordPosition="2001">tion from source language words to target words. Statistical machine translation models estimate Pr(f|e), the translation probability from source language string e to target language string f (Och et al., 2003): Pr(f|e) = � Pr(f, a|e) a EM-algorithm is usually used to train the alignment models to estimate lexicon parameters p(f|e). In E-step, the counts for one sentence pair (f ,e) are: c(f|e; f, e) = � Pr(a|f, e) � W, fj)s(e, eaj) a i,j Pr(a|f, e) = Pr(f, a|e)/Pr(a|e) In the M-step, lexicon parameters become: p(f|e) a � c(f|e; f(s), e(s)) s Different alignment models such as IBM-1 to IBM-5 (Brown et al., 1993) and HMM model (Och and Ney, 2000) provide different decompositions of Pr(f, a|e). For different alignment models different approaches were proposed to estimate the corresponding alignments and parameters. The details can be found in (Och et al., 2003; Brown et al., 1993). 4.2 Information Need Prediction After estimating the statistical translation probabilities, we treat the information need prediction as the process of ranking words by p(w|Q), the probability of generating word w from question Q: P(w|Q) = A � Ptr(w|t)P(t|Q)+(1−A)P(w|C) tEQ The word-to-word translation probability Ptr(w|t) is</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, v.19 n.2, June 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Yunfeng Huang</author>
</authors>
<title>Learning the Relative Usefulness of Questions in Community QA.</title>
<date>2010</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) ,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="14895" citStr="Bunescu and Huang, 2010" startWordPosition="2412" endWordPosition="2415">.3% 60.2% 52.1% 57.4% 54.5% Table 2: Question recommendation results without information need prediction Test c Test t Methods MRR Precision@5 Precision@10 MRR Precision@5 Precision@10 TFIDF 86.2% 70.8% 64.3% 95.1% 77.8% 69.3% Knowledge1 82.2% 65.0% 66.6% 76.7% 68.0% 68.7% Knowledge2 76.7% 54.9% 60.2% 61.6% 53.3% 58.2% LDA1 95.8% 72.4% 68.2% 96.2% 79.5% 69.2% LDA2 61.5% 55.3% 58.9% 68.1% 58.3% 53.9% Table 3: Question recommendation results with information need predicted by translation model fluence the calculation of similarity and the performance of information retrieval (Zhao et al., 2007; Bunescu and Huang, 2010). In this paper, we use an open source software afterthedeadline5 to automatically correct the spelling errors in the question and information need texts first. We also made use of Web 1T 5-gram6 to implement an N-Gram based method (Cheng et al., 2008) to further filter out the false positive corrections and re-rank correction suggestions (Mudge, 2010). The texts are tagged by Brill’s Part-of-Speech Tagger7 as the rule-based tagger is more robust than the state-of-art statistical taggers for raw web contents. This tagging information is only used for WordNet similarity calculation. Stop word r</context>
</contexts>
<marker>Bunescu, Huang, 2010</marker>
<rawString>Razvan Bunescu and Yunfeng Huang. 2010. Learning the Relative Usefulness of Questions in Community QA. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) , Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin D Burke</author>
<author>Kristian J Hammond</author>
<author>Vladimir A Kulyukin</author>
<author>L Steven</author>
</authors>
<title>Lytinen and Noriko Tomuro and</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<pages>57--66</pages>
<contexts>
<context position="4304" citStr="Burke et al. (1997)" startWordPosition="694" endWordPosition="697">are running out of space on your hard drive, then ... to boost your computer speed usually requires more RAM ... Table 1: Yahoo! Answers question example lows. In section 2, we briefly describe the related work on question search and recommendation. Section 3 addresses in detail how we measure the similarity between short texts. Section 4 describes two models for information need prediction that we use for the experiment. Section 5 tests the performance of the proposed models for the task of question recommendation. Section 7 is the conclusion of this paper. 2 Related Work 2.1 Question Search Burke et al. (1997) combined a lexical metric and a simple semantic knowledge-based (WordNet) similarity method to retrieve semantically similar questions from frequently asked question (FAQ) data. Jeon et al. (2005a) retrieved semantically similar questions from Korean CQA data by calculating the similarity between their answers. The assumption behind their research is that questions with very similar answers tend to be semantically similar. Jeon et al. (2005b) also discussed methods for grouping similar questions based on using the similarity between answers in the archive. These grouped question pairs were fu</context>
</contexts>
<marker>Burke, Hammond, Kulyukin, Steven, 1997</marker>
<rawString>Robin D. Burke and Kristian J. Hammond and Vladimir A. Kulyukin and Steven L. Lytinen and Noriko Tomuro and Scott Schoenberg. 1997. Question answering from frequently-asked question files: Experiences with the FAQ Finder system. AI Magazine, 18, 57C66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunbo Cao</author>
<author>Huizhong Duan</author>
<author>Chin-Yew Lin</author>
<author>Yong Yu</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>Recommending Questions Using the MDL-based Tree Cut Model. In:</title>
<date>2008</date>
<booktitle>Proc. of the 17th Int. Conf. on World Wide Web,</booktitle>
<pages>81--90</pages>
<contexts>
<context position="17958" citStr="Cao et al., 2008" startWordPosition="2885" endWordPosition="2888">ation 1429 method, two assessors are asked to label it with ‘good’ or ‘bad’. The third assessor will judge the conflicts. The assessors are also asked to read the information need and answer parts. If a recommended question is considered to express the same or similar information need, the assessor will label it ‘good’; otherwise, the assessor will label it as ‘bad’. Three measures for evaluating the recommendation performance are utilized. They are Mean Reciprocal Rank (MRR), top five prediction accuracy (precision@5) and top ten prediction accuracies (precision@10) (Voorhees and Tice, 2004; Cao et al., 2008). In MRR the reciprocal rank of a query question is the multiplicative inverse of the rank of the first ‘good’ recommended question. The top five prediction accuracy for a query question is the number of ‘good’ recommended questions out of the top five ranked questions and the top ten accuracy is calculated out of the top ten ranked questions. 5.4 Similarity Measure The first experiment conducted question recommendation based on their information need parts. Different text similarity methods described in section 3 were used to measure the similarity between the information need texts. In TFIDF</context>
</contexts>
<marker>Cao, Duan, Lin, Yu, Hon, 2008</marker>
<rawString>Yunbo Cao, Huizhong Duan, Chin-Yew Lin, Yong Yu, and Hsiao-Wuen Hon. 2008. Recommending Questions Using the MDL-based Tree Cut Model. In: Proc. of the 17th Int. Conf. on World Wide Web, pp. 81-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
<author>Gokhan Tur</author>
</authors>
<title>LDA Based Similarity Modeling for Question Answering.</title>
<date>2010</date>
<booktitle>In NAACL 2010 C Workshop on Semantic Search.</booktitle>
<contexts>
<context position="9068" citStr="Celikyilmaz et al., 2010" startWordPosition="1451" endWordPosition="1454">babilistic topic models. Instead, the texts are compared using some “thirdparty” topics that relate to them. A passage D in the retrieved documents (document collection) is represented as a mixture of fixed topics, with topic z getting weight θ(D) zin passage D and each topic is a distribution over a finite vocabulary of words, with word w having a probability φ(z) w in topic z. Gibbs Sampling can be used to estimate the corresponding expected posterior probabilities P(zID) = ˆθ(D) z and P(wIz) = ˆφ(z) w(Griffiths and Steyvers, 2004). In this paper we use two LDA based similarity measures in (Celikyilmaz et al., 2010) to measure the similarity between short information need texts. The first LDA similarity method uses KL divergence to measure the similarity between two documents under each given topic: �K 1 simLDA1(Di, Dj) = K k=1 10W (D(z=k) ,D(z=k) j ) i EwEDi maxSim(w, Dj) * idf(w) mcs(Di, Dj) = EwEDi idf(w) W (D(z=k) i , D(z=k) j ) = E wEDj maxSim(w, Di) * idf(w) EwEDj idf(w) For each word w in Di, maxSim(w, Dj) computes the maximum semantic similarity between w and any word in Dj. In this paper we choose lin (Lin, 1998) and jcn (Jiang and Conrath, 1997) to compute the word-to-word semantic similarity. </context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, Tur, 2010</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-Tur and Gokhan Tur. 2010. LDA Based Similarity Modeling for Question Answering. In NAACL 2010 C Workshop on Semantic Search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charibeth Cheng</author>
<author>Cedric Paul Alberto</author>
<author>Ian Anthony Chan</author>
<author>Vazir Joshua Querol</author>
</authors>
<title>SpellCheF: Spelling Checker and Corrector for Filipino.</title>
<date>2008</date>
<journal>Journal of Research in Science, Computing and Engineering, North America,</journal>
<volume>4</volume>
<contexts>
<context position="15147" citStr="Cheng et al., 2008" startWordPosition="2455" endWordPosition="2458">6.7% 68.0% 68.7% Knowledge2 76.7% 54.9% 60.2% 61.6% 53.3% 58.2% LDA1 95.8% 72.4% 68.2% 96.2% 79.5% 69.2% LDA2 61.5% 55.3% 58.9% 68.1% 58.3% 53.9% Table 3: Question recommendation results with information need predicted by translation model fluence the calculation of similarity and the performance of information retrieval (Zhao et al., 2007; Bunescu and Huang, 2010). In this paper, we use an open source software afterthedeadline5 to automatically correct the spelling errors in the question and information need texts first. We also made use of Web 1T 5-gram6 to implement an N-Gram based method (Cheng et al., 2008) to further filter out the false positive corrections and re-rank correction suggestions (Mudge, 2010). The texts are tagged by Brill’s Part-of-Speech Tagger7 as the rule-based tagger is more robust than the state-of-art statistical taggers for raw web contents. This tagging information is only used for WordNet similarity calculation. Stop word removal and lemmatization are applied to the all the raw texts before feeding into machine translation model training, the LDA model estimating and similarity calculation. 5.2 Construction of Training and Testing Sets We made use of the questions crawle</context>
</contexts>
<marker>Cheng, Alberto, Chan, Querol, 2008</marker>
<rawString>Charibeth Cheng, Cedric Paul Alberto, Ian Anthony Chan, and Vazir Joshua Querol. 2008. SpellCheF: Spelling Checker and Corrector for Filipino. Journal of Research in Science, Computing and Engineering, North America, 4, sep. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Silipigni Connaway</author>
<author>Chandra Prabha</author>
</authors>
<title>An overview of the IMLS Project “Sense-making the information confluence: The whys and hows of college and university user satisficing of information needs”.</title>
<date>2005</date>
<booktitle>Presented at Library of Congress Forum, American Library Association Midwinter Conference,</booktitle>
<location>Boston, MA,</location>
<marker>Connaway, Prabha, 2005</marker>
<rawString>Lynn Silipigni Connaway and Chandra Prabha. 2005. An overview of the IMLS Project “Sense-making the information confluence: The whys and hows of college and university user satisficing of information needs”. Presented at Library of Congress Forum, American Library Association Midwinter Conference, Boston, MA, Jan 16, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huizhong Duan</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Yong Yu</author>
</authors>
<title>Searching questions by identifying question topic and question focus.</title>
<date>2008</date>
<booktitle>In HLT-ACL,</booktitle>
<pages>156--164</pages>
<contexts>
<context position="5509" citStr="Duan et al. (2008)" startWordPosition="875" endWordPosition="878">pairs were further used as training data to estimate probabilities for a translation-based question retrieval model. Wang et al. (2009) proposed a tree kernel framework to find similar questions in the CQA archive based on syntactic tree structures. Wang et al. (2010) mined lexical and syntactic features to detect question sentences in CQA data. 2.2 Question Recommendation Wu et al. (2008) presented an incremental automatic question recommendation framework based on probabilistic latent semantic analysis. Question recommendation in their work considered both the users’ interests and feedback. Duan et al. (2008) made use of a tree-cut model to represent questions as graphs of topic terms. Questions were recommended based on this topic graph. The recommended questions can provide different aspects around the topic of the query question. The above question search and recommendation research provide different ways to retrieve questions from large archives of question answering data. However, none of them considers the similarity or diversity between questions by exploring their information needs. 3 Short Text Similarity Measures In question retrieval systems accurate similarity measures between document</context>
</contexts>
<marker>Duan, Cao, Lin, Yu, 2008</marker>
<rawString>Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong Yu. 2008. Searching questions by identifying question topic and question focus. In HLT-ACL, pages 156C164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics. Natl Acad Sci 101:5228C5235.</title>
<date>2004</date>
<contexts>
<context position="8982" citStr="Griffiths and Steyvers, 2004" startWordPosition="1436" endWordPosition="1439"> which measures “common words”, short texts are not compared to each other directly in probabilistic topic models. Instead, the texts are compared using some “thirdparty” topics that relate to them. A passage D in the retrieved documents (document collection) is represented as a mixture of fixed topics, with topic z getting weight θ(D) zin passage D and each topic is a distribution over a finite vocabulary of words, with word w having a probability φ(z) w in topic z. Gibbs Sampling can be used to estimate the corresponding expected posterior probabilities P(zID) = ˆθ(D) z and P(wIz) = ˆφ(z) w(Griffiths and Steyvers, 2004). In this paper we use two LDA based similarity measures in (Celikyilmaz et al., 2010) to measure the similarity between short information need texts. The first LDA similarity method uses KL divergence to measure the similarity between two documents under each given topic: �K 1 simLDA1(Di, Dj) = K k=1 10W (D(z=k) ,D(z=k) j ) i EwEDi maxSim(w, Dj) * idf(w) mcs(Di, Dj) = EwEDi idf(w) W (D(z=k) i , D(z=k) j ) = E wEDj maxSim(w, Di) * idf(w) EwEDj idf(w) For each word w in Di, maxSim(w, Dj) computes the maximum semantic similarity between w and any word in Dj. In this paper we choose lin (Lin, 199</context>
<context position="10478" citStr="Griffiths and Steyvers, 2004" startWordPosition="1704" endWordPosition="1708">ict the words in document Di (and Dj) to just verbs. 3.3 Probabilistic Topic Model Celikyilmaz et al. (2010) presented probabilistic topic model based methods to measure the similarity between question and candidate answers. The candidate answers were ranked based on the hidden D(z=k) + D(z=k) − KL(D�z=k) I i j ) 2 D(z=k) + D(z=k) − KL(D(z=k) I i j ) 2 W(D(z=k) i ,D(z=k) j ) calculates the similarity between two documents under topic z = k using KL divergence measure. D(z=k) iis the probability distribution of words in document Di given a fixed topic z. The second LDA similarity measure from (Griffiths and Steyvers, 2004) treats each document as a probability distribution of topics: simLDA2(Di, Dj) = 10W (ˆθ(Di),ˆθ(Dj)) where ˆθ(Di) is document Di’s probability distribution of topics as defined earlier. + 1427 4 Information Need Prediction using Statistical Machine Translation Model There are two reasons that we need to predict information need. It is often the case that the query question does not have a question body part. So we need a model to predict the information need part based on the query question in order to recommend questions based on the similarity of their information needs. Another reason is th</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Natl Acad Sci 101:5228C5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding semantically similar questions based on their answers.</title>
<date>2005</date>
<booktitle>In Proc. of SIGIR05.</booktitle>
<contexts>
<context position="4500" citStr="Jeon et al. (2005" startWordPosition="723" endWordPosition="726">related work on question search and recommendation. Section 3 addresses in detail how we measure the similarity between short texts. Section 4 describes two models for information need prediction that we use for the experiment. Section 5 tests the performance of the proposed models for the task of question recommendation. Section 7 is the conclusion of this paper. 2 Related Work 2.1 Question Search Burke et al. (1997) combined a lexical metric and a simple semantic knowledge-based (WordNet) similarity method to retrieve semantically similar questions from frequently asked question (FAQ) data. Jeon et al. (2005a) retrieved semantically similar questions from Korean CQA data by calculating the similarity between their answers. The assumption behind their research is that questions with very similar answers tend to be semantically similar. Jeon et al. (2005b) also discussed methods for grouping similar questions based on using the similarity between answers in the archive. These grouped question pairs were further used as training data to estimate probabilities for a translation-based question retrieval model. Wang et al. (2009) proposed a tree kernel framework to find similar questions in the CQA arc</context>
<context position="7391" citStr="Jeon et al., 2005" startWordPosition="1176" endWordPosition="1179">erformance for the task of question recommendation in the experiment section. 3.1 TFIDF Baeza-Yates and Ribeiro-Neto (1999) provides a TFIDF method to calculate the similarity between two texts. Each document is represented by a term vector using TFIDF score. The similarity between two text Di and Dj is the cosine similarity in the vector space model: DTi Dj cos(Di, Dj) _ 11D i1111Dj11 1426 This method is used in most information retrieval systems as it is both efficient and effective. However if the query text contains only one or two words this method will be biased to shorter answer texts (Jeon et al., 2005a). We also found that in CQA data short contents in the question body cannot provide any information about the users’ information needs. Based on the above two reasons, in the test data sets we do not include the questions whose information need parts contain only a few noninformative words . 3.2 Knowledge-based Measure Mihalcea et al. (2006) proposed several knowledgebased methods for measuring the semantic level similarity of texts to solve the lexical chasm problem between short texts. These knowledge-based similarity measures were derived from word semantic similarity by making use of Wor</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005a. Finding semantically similar questions based on their answers. In Proc. of SIGIR05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In CIKM,</booktitle>
<pages>84--90</pages>
<contexts>
<context position="4500" citStr="Jeon et al. (2005" startWordPosition="723" endWordPosition="726">related work on question search and recommendation. Section 3 addresses in detail how we measure the similarity between short texts. Section 4 describes two models for information need prediction that we use for the experiment. Section 5 tests the performance of the proposed models for the task of question recommendation. Section 7 is the conclusion of this paper. 2 Related Work 2.1 Question Search Burke et al. (1997) combined a lexical metric and a simple semantic knowledge-based (WordNet) similarity method to retrieve semantically similar questions from frequently asked question (FAQ) data. Jeon et al. (2005a) retrieved semantically similar questions from Korean CQA data by calculating the similarity between their answers. The assumption behind their research is that questions with very similar answers tend to be semantically similar. Jeon et al. (2005b) also discussed methods for grouping similar questions based on using the similarity between answers in the archive. These grouped question pairs were further used as training data to estimate probabilities for a translation-based question retrieval model. Wang et al. (2009) proposed a tree kernel framework to find similar questions in the CQA arc</context>
<context position="7391" citStr="Jeon et al., 2005" startWordPosition="1176" endWordPosition="1179">erformance for the task of question recommendation in the experiment section. 3.1 TFIDF Baeza-Yates and Ribeiro-Neto (1999) provides a TFIDF method to calculate the similarity between two texts. Each document is represented by a term vector using TFIDF score. The similarity between two text Di and Dj is the cosine similarity in the vector space model: DTi Dj cos(Di, Dj) _ 11D i1111Dj11 1426 This method is used in most information retrieval systems as it is both efficient and effective. However if the query text contains only one or two words this method will be biased to shorter answer texts (Jeon et al., 2005a). We also found that in CQA data short contents in the question body cannot provide any information about the users’ information needs. Based on the above two reasons, in the test data sets we do not include the questions whose information need parts contain only a few noninformative words . 3.2 Knowledge-based Measure Mihalcea et al. (2006) proposed several knowledgebased methods for measuring the semantic level similarity of texts to solve the lexical chasm problem between short texts. These knowledge-based similarity measures were derived from word semantic similarity by making use of Wor</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005b. Finding similar questions in large question and answer archives. In CIKM, pages 84C90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="9618" citStr="Jiang and Conrath, 1997" startWordPosition="1553" endWordPosition="1556">per we use two LDA based similarity measures in (Celikyilmaz et al., 2010) to measure the similarity between short information need texts. The first LDA similarity method uses KL divergence to measure the similarity between two documents under each given topic: �K 1 simLDA1(Di, Dj) = K k=1 10W (D(z=k) ,D(z=k) j ) i EwEDi maxSim(w, Dj) * idf(w) mcs(Di, Dj) = EwEDi idf(w) W (D(z=k) i , D(z=k) j ) = E wEDj maxSim(w, Di) * idf(w) EwEDj idf(w) For each word w in Di, maxSim(w, Dj) computes the maximum semantic similarity between w and any word in Dj. In this paper we choose lin (Lin, 1998) and jcn (Jiang and Conrath, 1997) to compute the word-to-word semantic similarity. We only choose nouns and verbs for calculating mcs. Additionally, when w is a noun we restrict the words in document Di (and Dj) to just nouns. Similarly, when w is a verb, we restrict the words in document Di (and Dj) to just verbs. 3.3 Probabilistic Topic Model Celikyilmaz et al. (2010) presented probabilistic topic model based methods to measure the similarity between question and candidate answers. The candidate answers were ranked based on the hidden D(z=k) + D(z=k) − KL(D�z=k) I i j ) 2 D(z=k) + D(z=k) − KL(D(z=k) I i j ) 2 W(D(z=k) i ,D(</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning (ICML ’98), Jude W. Shavlik (Ed.).</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="9584" citStr="Lin, 1998" startWordPosition="1549" endWordPosition="1550">s, 2004). In this paper we use two LDA based similarity measures in (Celikyilmaz et al., 2010) to measure the similarity between short information need texts. The first LDA similarity method uses KL divergence to measure the similarity between two documents under each given topic: �K 1 simLDA1(Di, Dj) = K k=1 10W (D(z=k) ,D(z=k) j ) i EwEDi maxSim(w, Dj) * idf(w) mcs(Di, Dj) = EwEDi idf(w) W (D(z=k) i , D(z=k) j ) = E wEDj maxSim(w, Di) * idf(w) EwEDj idf(w) For each word w in Di, maxSim(w, Dj) computes the maximum semantic similarity between w and any word in Dj. In this paper we choose lin (Lin, 1998) and jcn (Jiang and Conrath, 1997) to compute the word-to-word semantic similarity. We only choose nouns and verbs for calculating mcs. Additionally, when w is a noun we restrict the words in document Di (and Dj) to just nouns. Similarly, when w is a verb, we restrict the words in document Di (and Dj) to just verbs. 3.3 Probabilistic Topic Model Celikyilmaz et al. (2010) presented probabilistic topic model based methods to measure the similarity between question and candidate answers. The candidate answers were ranked based on the hidden D(z=k) + D(z=k) − KL(D�z=k) I i j ) 2 D(z=k) + D(z=k) − </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In Proceedings of the Fifteenth International Conference on Machine Learning (ICML ’98), Jude W. Shavlik (Ed.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 296-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yandong Liu</author>
<author>Jiang Bian</author>
<author>Eugene Agichtein</author>
</authors>
<title>Predicting information seeker satisfaction in community question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR ’08).</booktitle>
<pages>483--490</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="11213" citStr="Liu et al., 2008" startWordPosition="1826" endWordPosition="1829">ocument Di’s probability distribution of topics as defined earlier. + 1427 4 Information Need Prediction using Statistical Machine Translation Model There are two reasons that we need to predict information need. It is often the case that the query question does not have a question body part. So we need a model to predict the information need part based on the query question in order to recommend questions based on the similarity of their information needs. Another reason is that information need prediction plays a crucial part not only in Question Answering but also in information retrieval (Liu et al., 2008). In this paper we propose an information need prediction method based on a statistical machine translation model. 4.1 Statistical Machine Translation Model (f(s), e(s)), s = 1,...,5 is a parallel corpus. In a sentence pair (f, e), source language String, f = f1f2...fJ has J words, and e = e1e2...eI has I words. And alignment a = a1a2...aJ represents the mapping information from source language words to target words. Statistical machine translation models estimate Pr(f|e), the translation probability from source language string e to target language string f (Och et al., 2003): Pr(f|e) = � Pr(f</context>
</contexts>
<marker>Liu, Bian, Agichtein, 2008</marker>
<rawString>Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008. Predicting information seeker satisfaction in community question answering. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR ’08). ACM, New York, NY, USA, 483-490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence (AAAI ’06),</booktitle>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="7736" citStr="Mihalcea et al. (2006)" startWordPosition="1233" endWordPosition="1236"> space model: DTi Dj cos(Di, Dj) _ 11D i1111Dj11 1426 This method is used in most information retrieval systems as it is both efficient and effective. However if the query text contains only one or two words this method will be biased to shorter answer texts (Jeon et al., 2005a). We also found that in CQA data short contents in the question body cannot provide any information about the users’ information needs. Based on the above two reasons, in the test data sets we do not include the questions whose information need parts contain only a few noninformative words . 3.2 Knowledge-based Measure Mihalcea et al. (2006) proposed several knowledgebased methods for measuring the semantic level similarity of texts to solve the lexical chasm problem between short texts. These knowledge-based similarity measures were derived from word semantic similarity by making use of WordNet. The evaluation on a paraphrase recognition task showed that knowledgebased measures outperform the simpler lexical level approach. We follow the definition in (Mihalcea et al., 2006) to derive a text-to-text similarity metric mcs for two given texts Di and Dj: topics discovered by Latent Dirichlet Allocation (LDA) methods. In contrast to</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st national conference on Artificial intelligence (AAAI ’06), pages 775C780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Mudge</author>
</authors>
<title>The design of a proofreading software service.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids (CL&amp;W ’10). Association for Computational Linguistics,</booktitle>
<pages>24--32</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="15249" citStr="Mudge, 2010" startWordPosition="2472" endWordPosition="2473"> 61.5% 55.3% 58.9% 68.1% 58.3% 53.9% Table 3: Question recommendation results with information need predicted by translation model fluence the calculation of similarity and the performance of information retrieval (Zhao et al., 2007; Bunescu and Huang, 2010). In this paper, we use an open source software afterthedeadline5 to automatically correct the spelling errors in the question and information need texts first. We also made use of Web 1T 5-gram6 to implement an N-Gram based method (Cheng et al., 2008) to further filter out the false positive corrections and re-rank correction suggestions (Mudge, 2010). The texts are tagged by Brill’s Part-of-Speech Tagger7 as the rule-based tagger is more robust than the state-of-art statistical taggers for raw web contents. This tagging information is only used for WordNet similarity calculation. Stop word removal and lemmatization are applied to the all the raw texts before feeding into machine translation model training, the LDA model estimating and similarity calculation. 5.2 Construction of Training and Testing Sets We made use of the questions crawled from Yahoo! Answers for the estimating models and evaluation. More specifically, we obtained 2 milli</context>
</contexts>
<marker>Mudge, 2010</marker>
<rawString>Raphael Mudge. 2010. The design of a proofreading software service. In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids (CL&amp;W ’10). Association for Computational Linguistics, Morristown, NJ, USA, 24-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>Proceedings of the 18th conference on Computational linguistics, July 31-August 04,</booktitle>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="12238" citStr="Och and Ney, 2000" startWordPosition="2005" endWordPosition="2008">target words. Statistical machine translation models estimate Pr(f|e), the translation probability from source language string e to target language string f (Och et al., 2003): Pr(f|e) = � Pr(f, a|e) a EM-algorithm is usually used to train the alignment models to estimate lexicon parameters p(f|e). In E-step, the counts for one sentence pair (f ,e) are: c(f|e; f, e) = � Pr(a|f, e) � W, fj)s(e, eaj) a i,j Pr(a|f, e) = Pr(f, a|e)/Pr(a|e) In the M-step, lexicon parameters become: p(f|e) a � c(f|e; f(s), e(s)) s Different alignment models such as IBM-1 to IBM-5 (Brown et al., 1993) and HMM model (Och and Ney, 2000) provide different decompositions of Pr(f, a|e). For different alignment models different approaches were proposed to estimate the corresponding alignments and parameters. The details can be found in (Och et al., 2003; Brown et al., 1993). 4.2 Information Need Prediction After estimating the statistical translation probabilities, we treat the information need prediction as the process of ranking words by p(w|Q), the probability of generating word w from question Q: P(w|Q) = A � Ptr(w|t)P(t|Q)+(1−A)P(w|C) tEQ The word-to-word translation probability Ptr(w|t) is the probability of word w is tran</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och, Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. Proceedings of the 18th conference on Computational linguistics, July 31-August 04, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Hermann Ney. 2003.A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>pp.</pages>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och, Hermann Ney. 2003.A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, volume 29, number 1, pp. 19-51 March 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Biased LexRank: Passage retrieval using random walks with question-based priors.</title>
<date>2009</date>
<booktitle>Information Processing and Management: an International Journal, v.45 n.1,</booktitle>
<pages>42--54</pages>
<marker>Otterbacher, Erkan, Radev, 2009</marker>
<rawString>Jahna Otterbacher, Gunes Erkan, Dragomir R. Radev. 2009. Biased LexRank: Passage retrieval using random walks with question-based priors. Information Processing and Management: an International Journal, v.45 n.1, p.42-54, January, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chandra Prabha</author>
<author>Lynn Silipigni Connaway</author>
<author>Lawrence Olszewski</author>
<author>Lillie R Jenkins</author>
</authors>
<title>What is enough? Satisficing information needs.</title>
<date>2007</date>
<journal>Journal of Documentation (January,</journal>
<volume>63</volume>
<marker>Prabha, Connaway, Olszewski, Jenkins, 2007</marker>
<rawString>Chandra Prabha, Lynn Silipigni Connaway, Lawrence Olszewski, Lillie R. Jenkins. 2007. What is enough? Satisficing information needs. Journal of Documentation (January, 63,1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
<author>Dawn Tice</author>
</authors>
<title>The TREC-8 question answering track evaluation.</title>
<date>2000</date>
<booktitle>In Text Retrieval Conference TREC-8,</booktitle>
<location>Gaithersburg, MD.</location>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Ellen Voorhees and Dawn Tice. 2000. The TREC-8 question answering track evaluation. In Text Retrieval Conference TREC-8, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Wang</author>
<author>Yanming Zhao</author>
<author>Tat-Seng Chua</author>
</authors>
<title>A syntactic tree matching approach to finding similar questions in community-based qa services.</title>
<date>2009</date>
<booktitle>In SIGIR,</booktitle>
<pages>187--194</pages>
<contexts>
<context position="5026" citStr="Wang et al. (2009)" startWordPosition="803" endWordPosition="806"> semantically similar questions from frequently asked question (FAQ) data. Jeon et al. (2005a) retrieved semantically similar questions from Korean CQA data by calculating the similarity between their answers. The assumption behind their research is that questions with very similar answers tend to be semantically similar. Jeon et al. (2005b) also discussed methods for grouping similar questions based on using the similarity between answers in the archive. These grouped question pairs were further used as training data to estimate probabilities for a translation-based question retrieval model. Wang et al. (2009) proposed a tree kernel framework to find similar questions in the CQA archive based on syntactic tree structures. Wang et al. (2010) mined lexical and syntactic features to detect question sentences in CQA data. 2.2 Question Recommendation Wu et al. (2008) presented an incremental automatic question recommendation framework based on probabilistic latent semantic analysis. Question recommendation in their work considered both the users’ interests and feedback. Duan et al. (2008) made use of a tree-cut model to represent questions as graphs of topic terms. Questions were recommended based on th</context>
</contexts>
<marker>Wang, Zhao, Chua, 2009</marker>
<rawString>Kai Wang, Yanming Zhao, and Tat-Seng Chua. 2009. A syntactic tree matching approach to finding similar questions in community-based qa services. In SIGIR, pages 187C194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Wang</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Exploiting salient patterns for question detection and question retrieval in community-based question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING ’10). Association for Computational Linguistics,</booktitle>
<pages>1155--1163</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>Wang, Chua, 2010</marker>
<rawString>Kai Wang and Tat-Seng Chua. 2010. Exploiting salient patterns for question detection and question retrieval in community-based question answering. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING ’10). Association for Computational Linguistics, Stroudsburg, PA, USA, 1155-1163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hu Wu</author>
<author>Yongji Wang</author>
<author>Xiang Cheng</author>
</authors>
<title>Incremental probabilistic latent semantic analysis for automatic question recommendation. In RecSys.</title>
<date>2008</date>
<contexts>
<context position="5283" citStr="Wu et al. (2008)" startWordPosition="845" endWordPosition="848">tions with very similar answers tend to be semantically similar. Jeon et al. (2005b) also discussed methods for grouping similar questions based on using the similarity between answers in the archive. These grouped question pairs were further used as training data to estimate probabilities for a translation-based question retrieval model. Wang et al. (2009) proposed a tree kernel framework to find similar questions in the CQA archive based on syntactic tree structures. Wang et al. (2010) mined lexical and syntactic features to detect question sentences in CQA data. 2.2 Question Recommendation Wu et al. (2008) presented an incremental automatic question recommendation framework based on probabilistic latent semantic analysis. Question recommendation in their work considered both the users’ interests and feedback. Duan et al. (2008) made use of a tree-cut model to represent questions as graphs of topic terms. Questions were recommended based on this topic graph. The recommended questions can provide different aspects around the topic of the query question. The above question search and recommendation research provide different ways to retrieve questions from large archives of question answering data</context>
</contexts>
<marker>Wu, Wang, Cheng, 2008</marker>
<rawString>Hu Wu, Yongji Wang, and Xiang Cheng. 2008. Incremental probabilistic latent semantic analysis for automatic question recommendation. In RecSys.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobing Xue</author>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
</authors>
<title>Retrieval models for question and answer archives.</title>
<date>2008</date>
<booktitle>In SIGIR’08,</booktitle>
<pages>475--482</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24016" citStr="Xue et al., 2008" startWordPosition="3899" endWordPosition="3902">ask. The question and information need pairs in both ‘Train t’ and ‘Train c’ training sets were used to train two IBM-4 translation models by GIZA++ toolkit. These pairs were also preprocessed before training. And the pairs whose information need part become empty after preprocessing were disregarded. During the experiment, we found that some of the generated words in the information need parts are themselves. This is caused by the self translation problem in translation model: the highest translation score for a word is usually given to itself if the target and source languages are the same (Xue et al., 2008). This has always been a tough question: not using self-translated words can reduce retrieval performance as the information need parts need the terms to represent the semantic meanings; using self-translated words does not take advantage of the translation approach. To tackle this problem, we control the number of the words predicted by the translation model to be exactly twice the number of words in the corresponding preprocessed question. The predicted information need words for the retrieved questions are shown in Table 5. In Q1, the information need behind question “recommend website for </context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>Xiaobing Xue, Jiwoon Jeon, W. Bruce Croft. 2008. Retrieval models for question and answer archives. In SIGIR’08, pages 475C482. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
</authors>
<title>Learning Question Paraphrases for QA from Encarta Logs.</title>
<date>2007</date>
<booktitle>In Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI),</booktitle>
<pages>1795--1800</pages>
<contexts>
<context position="14869" citStr="Zhao et al., 2007" startWordPosition="2408" endWordPosition="2411">69.8% LDA2 61.5% 55.3% 60.2% 52.1% 57.4% 54.5% Table 2: Question recommendation results without information need prediction Test c Test t Methods MRR Precision@5 Precision@10 MRR Precision@5 Precision@10 TFIDF 86.2% 70.8% 64.3% 95.1% 77.8% 69.3% Knowledge1 82.2% 65.0% 66.6% 76.7% 68.0% 68.7% Knowledge2 76.7% 54.9% 60.2% 61.6% 53.3% 58.2% LDA1 95.8% 72.4% 68.2% 96.2% 79.5% 69.2% LDA2 61.5% 55.3% 58.9% 68.1% 58.3% 53.9% Table 3: Question recommendation results with information need predicted by translation model fluence the calculation of similarity and the performance of information retrieval (Zhao et al., 2007; Bunescu and Huang, 2010). In this paper, we use an open source software afterthedeadline5 to automatically correct the spelling errors in the question and information need texts first. We also made use of Web 1T 5-gram6 to implement an N-Gram based method (Cheng et al., 2008) to further filter out the false positive corrections and re-rank correction suggestions (Mudge, 2010). The texts are tagged by Brill’s Part-of-Speech Tagger7 as the rule-based tagger is more robust than the state-of-art statistical taggers for raw web contents. This tagging information is only used for WordNet similarit</context>
</contexts>
<marker>Zhao, Zhou, Liu, 2007</marker>
<rawString>Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning Question Paraphrases for QA from Encarta Logs. In Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI), pages 1795-1800.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>