<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998478">
Stochastic Inversion Transduction
Grammars and Bilingual Parsing of
Parallel Corpora
</title>
<author confidence="0.999216">
Dekai Wu*
</author>
<affiliation confidence="0.749536">
Hong Kong University of Science and
Technology
</affiliation>
<bodyText confidence="0.998249083333333">
We introduce (1) a novel stochastic inversion transduction grammar formalism for bilingual
language modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety of
parallel corpus analysis applications. Aside from the bilingual orientation, three major features
distinguish the formalism from the finite-state transducers more traditionally found in compu-
tational linguistics: it skips directly to a context-free rather than finite-state base, it permits a
minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient
maximum-likelihood bilingual parsing algorithm. A convenient normal form is shown to exist.
Analysis of the formalism&apos;s expressiveness suggests that it is particularly well suited to modeling
ordering shifts between languages, balancing needed flexibility against complexity constraints.
We discuss a number of examples of how stochastic inversion transduction grammars bring bilin-
gual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing,
phrasal alignment, and parsing.
</bodyText>
<sectionHeader confidence="0.992106" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999918666666667">
We introduce a general formalism for modeling of bilingual sentence pairs, known as
an inversion transduction grammar, with potential application in a variety of corpus
analysis areas. Transduction grammar models, especially of the finite-state family, have
long been known. However, the imposition of identical ordering constraints upon both
streams severely restricts their applicability, and thus transduction grammars have re-
ceived relatively little attention in language-modeling research. The inversion trans-
duction grammar formalism skips directly to a context-free, rather than finite-state,
base and permits one extra degree of ordering flexibility, while retaining properties
necessary for efficient computation, thereby sidestepping the limitations of traditional
transduction grammars.
In tandem with the concept of bilingual language-modeling, we propose the con-
cept of bilingual parsing, where the input is a sentence-pair rather than a sentence.
Though inversion transduction grammars remain inadequate as full-fledged transla-
tion models, bilingual parsing with simple inversion transduction grammars turns
out to be very useful for parallel corpus analysis when the true grammar is not fully
known. Parallel bilingual corpora have been shown to provide a rich source of con-
straints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church,
and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993;
</bodyText>
<affiliation confidence="0.531666">
* Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong.
</affiliation>
<email confidence="0.956501">
E-mail: dekai@cs.ust.hk
</email>
<note confidence="0.9068525">
© 1997 Association for Computational Linguistics
Computational Linguistics Volume 23, Number 3
</note>
<bodyText confidence="0.999403222222222">
Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary
purpose of bilingual parsing with inversion transduction grammars is not to flag un-
grammatical inputs; rather, the aim is to extract structure from the input data, which
is assumed to be grammatical, in keeping with the spirit of robust parsing. The for-
malism&apos;s uniform integration of various types of bracketing and alignment constraints
is one of its chief strengths.
The paper is divided into two main parts. We begin in the first part below by
laying out the basic formalism, then show that reduction to a normal form is possible.
We then raise several desiderata for the expressiveness of any bilingual language-
modeling formalism in terms of its constituent-matching flexibility and discuss how
the characteristics of the inversion transduction formalism are particularly suited to
address these criteria. Afterwards we introduce a stochastic version and give an al-
gorithm for finding the optimal bilingual parse of a sentence-pair. The formalism is
independent of the languages; we give examples and applications using Chinese and
English because languages from different families provide a more rigorous testing
ground. In the second part, we survey a number of sample applications and exten-
sions of bilingual parsing for segmentation, bracketing, phrasal alignment, and other
parsing tasks.
</bodyText>
<sectionHeader confidence="0.981058" genericHeader="method">
2. Inversion Transduction Grammars
</sectionHeader>
<bodyText confidence="0.999887633333333">
A transduction grammar describes a structurally correlated pair of languages. For
our purposes, the generative view is most convenient: the grammar generates trans-
ductions, so that two output streams are simultaneously generated, one for each lan-
guage. This contrasts with the common input-output view popularized by both syntax-
directed transduction grammars and finite-state transducers. The generative view is
more appropriate for our applications because the roles of the two languages are sym-
metrical, in contrast to the usual applications of syntax-directed transduction gram-
mars. Moreover, the input-output view works better when a machine for accepting
one of the languages (the input language) has a high degree of determinism, which is
not the case here.
Our transduction model is context-free, rather than finite-state. Finite-state trans-
ducers, or FSTs, are well known to be useful for specific tasks such as analysis of
inflectional morphology (Koskenruiemi 1983), text-to-speech conversion (Kaplan and
Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and
Mellish 1989). FSTs may also be used to parse restricted classes of context-free gram-
mars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis
tasks we consider in this paper are quite different from the tasks for which FSTs are
apparently well suited. Our domain is broader, and the model possesses very little a
priori specific structural knowledge of the language.
As a stepping stone to inversion transduction grammars, we first consider what
a context-free model known as a simple transduction grammar (Lewis and Stearns
1968) would look like. Simple transduction grammars (as well as inversion transduc-
tion grammars) are restricted cases of ,the general class of context-free syntax-directed
transduction grammars (Aho and Ullman 1969a, 1969b, 1972); however, we will avoid
the term syntax-directed here, so as to de-emphasize the input-output connotation as
discussed above.
A simple transduction grammar can be written by marking every terminal symbol
for a particular output stream. Thus, each rewrite rule emits not one but two streams.
For example, a rewrite rule of the form A —&gt; Bxiy2Czi means that the terminal symbols
x and z are symbols of the language L1 emitted on stream 1, while y is a symbol of
</bodyText>
<page confidence="0.99184">
378
</page>
<figure confidence="0.9796705">
Wu Bilingual Parsing
(a) S —, [SP Stop]
SP --. [NP VP] I [NP VV] I [NP V]
PP -4 [Prep NP]
NP —) [Det NN] I [Det N] I [Pro] I [NP Conj NP]
NN —&gt; [A N] I [NN PP]
VP —› [Aux VP] I [Aux VV] I [VV PP]
VV —&gt; [V NP] I [Cop A]
Det —&gt; the/f
Prep -4- to/IJ
Pro --- I/ft I you/4
N -4- authority/Pelf N I secretary/if
A - accountable/i. * I financial/fit&amp;
Conj --4 and/fa
Aux —&gt; will/4411,77
Cop --4 be/c
Stop —&gt; ./.
(b) VP - (VV PP)
</figure>
<figureCaption confidence="0.99497">
Figure 1
</figureCaption>
<bodyText confidence="0.997526266666667">
A simple transduction grammar (a) and an inverted-orientation production (b).
the language L2 emitted on stream 2. It follows that every nonterminal stands for a
class of derivable substring pairs.
We can use a simple transduction grammar to model the generation of bilingual
sentence pairs. As a mnemonic convention, we usually use the alternative notation
A --÷ BxlyCzle to associate matching output tokens. Though this additional informa-
tion has no formal generative effect, it reminds us that xly must be a valid entry in the
translation lexicon. We call a matched terminal symbol pair such as xly a couple. The
null symbol e means that no output token is generated. We call x/€ an Li-singleton,
and cly an L2-singleton.
Consider the simple transduction grammar fragment shown in Figure 1(a). (It will
become apparent below why we explicitly include brackets around right-hand sides
containing nonterminals, which are usually omitted with standard CFGs.) The simple
transduction grammar can generate, for instance, the following pair of English and
Chinese sentences in translation:
</bodyText>
<listItem confidence="0.893056333333333">
(1) a. [[[[The [Financial SecretarY1NN NI, and [I[Np INP [will [be
accountable[vv lvp [sp
b. [[[[[I J1NN NP 11\IP PC* [Oulf]vv lvp lsp o Is
</listItem>
<bodyText confidence="0.592989166666667">
Notice that each nonterminal derives two substrings, one in each language. The two
substrings are counterparts of each other. In fact, it is natural to write the parse trees
together:
(2) E[E[The/e [Financial/14V Secretary/ni [NN Ni&apos; and /fl fi/RINP INP
[be/€ accountable/ftrtivv [vp [sr •/. Is
Of course, in general, simple transduction grammars are not very useful, precisely
</bodyText>
<page confidence="0.996085">
379
</page>
<note confidence="0.885326">
Computational Linguistics Volume 23, Number 3
</note>
<bodyText confidence="0.999568333333333">
because they require the two languages to share exactly the same grammatical structure
(modulo those distinctions that can be handled with lexical singletons). For example,
the following sentence pair from our corpus cannot be generated:
</bodyText>
<listItem confidence="0.733464">
(3) a. The Authority will be accountable to the Financial Secretary.
b. WIIINglitP111-6Iftific,
</listItem>
<subsubsectionHeader confidence="0.715867">
(Authority will to Financial Secretary accountable.)
</subsubsectionHeader>
<bodyText confidence="0.999930148148148">
To make transduction grammars truly useful for bilingual tasks, we must escape
the rigid parallel ordering constraint of simple transduction grammars. At the same
time, any relaxation of constraints must be traded off against increases in the com-
putational complexity of parsing, which may easily become exponential. The key is
to make the relaxation relatively modest but still handle a wide range of ordering
variations.
The inversion transduction grammar (ITG) formalism only minimally extends the
generative power of a simple transduction grammar, yet turns out to be surprisingly
effective.1 Like simple transduction grammars, ITGs remain a subset of context-free
(syntax-directed) transduction grammars (Lewis and Stearns 1968) but this view is too
general to be of much help.&apos; The productions of an inversion transduction grammar
are interpreted just as in a simple transduction grammar, except that two possible
orientations are allowed. Pure simple transduction grammars have the implicit char-
acteristic that for both output streams, the symbols generated by the right-hand-side
constituents of a production are concatenated in the same left-to-right order. Inversion
transduction grammars also allow such productions, which are said to have straight
orientation. In addition, however, inversion transduction grammars allow productions
with inverted orientation, which generate output for stream 2 by emitting the con-
stituents on a production&apos;s right-hand side in right-to-left order. We indicate a produc-
tion&apos;s orientation with explicit notation for the two varieties of concatenation operators
on string-pairs. The operator [1 performs the &amp;quot;usual&amp;quot; pairwise concatenation so that
[AB] yields the string-pair (C1, C2) where C1 =Bi and C2 = A2B2. But the operator ()
concatenates constituents on output stream 1 while reversing them on stream 2, so that
C1 = AiBi but C2 = B2A2. Since inversion is permitted at any level of rule expansion,
a derivation may intermix productions of either orientation within the parse tree. For
example, if the inverted-orientation production of Figure 1(b) is added to the earlier
simple transduction grammar, sentence-pair (3) can then be generated as follows:
</bodyText>
<listItem confidence="0.994623333333333">
(4) a. [[[The Authority]Nrp [will Ube accountablelvv [to [the [[Financial
Secretary1NN INNN INP lnr 1vr lvr lsr • Is
b. [MIT in NI, iNti IP [RIM 1711lNN INNN INP lrr IAA ivy lvp
</listItem>
<subsectionHeader confidence="0.324108">
lsr Is
</subsectionHeader>
<bodyText confidence="0.9910575">
We can show the common structure of the two sentences more clearly and com-
pactly with the aid of the () notation:
</bodyText>
<footnote confidence="0.9857238">
1 The expressiveness of simple transduction grammars is equivalent to nondeterministic pushdown
transducers (Savitch 1982).
2 Also keep in mind that ITGs turn out to be especially suited for bilingual parsing applications, whereas
pushdown transducers and syntax-directed transduction grammars are designed for monolingual
parsing (in tandem with generation).
</footnote>
<page confidence="0.993592">
380
</page>
<figure confidence="0.775281">
Wu Bilingual Parsing
</figure>
<figureCaption confidence="0.956144">
Figure 2
</figureCaption>
<bodyText confidence="0.784148">
Inversion transduction grammar parse tree.
</bodyText>
<listItem confidence="0.451010666666667">
(5) 111Theic Authority/Wit ffi, ]NP [Wi11/44ff ([be/c accountable/.]vv
[to/PI 1theie [[Financial/1ft&amp; Secretary/f4iNN 1NNN IN!&apos; ]p, )vp bp
./. Is
</listItem>
<bodyText confidence="0.998418411764706">
Alternatively, a graphical parse tree notation is shown in Figure 2, where the () level
of bracketing is indicated by a horizontal line. The English is read in the usual depth-
first left-to-right order, but for the Chinese, a horizontal line means the right subtree
is traversed before the left.
Parsing, in the case of an ITG, means building matched constituents for input
sentence-pairs rather than sentences. This means that the adjacency constraints given
by the nested levels must be obeyed in the bracketings of both languages. The result of
the parse yields labeled bracketings for both sentences, as well as a bracket alignment
indicating the parallel constituents between the sentences. The constituent alignment
includes a word alignment as a by-product.
The nonterminals may not always look like those of an ordinary CFG. Clearly, the
nonterminals of an ITG must be chosen in a somewhat different manner than for a
monolingual grammar, since they must simultaneously account for syntactic patterns
of both languages. One might even decide to choose nonterminals for an ITG that
do not match linguistic categories, sacrificing this to the goal of ensuring that all
corresponding substrings can be aligned.
An ITG can accommodate a wider range of ordering variation between the Ian-
</bodyText>
<equation confidence="0.8280066">
will/1
The/ e Authority/WA
be/e accountable/ ft* the/e
Financial/ft&amp;
Secretary/TA
</equation>
<page confidence="0.880679">
381
</page>
<figure confidence="0.910098">
Computational Linguistics Volume 23, Number 3
Where is the Secretary of Finance when needed
fitfl gIT1
</figure>
<figureCaption confidence="0.870105">
Figure 3
</figureCaption>
<bodyText confidence="0.9177624">
An extremely distorted alignment that can be accommodated by an ITG.
guages than might appear at first blush, through appropriate decomposition of pro-
ductions (and thus constituents), in conjuction with introduction of new auxiliary non-
terminals where needed. For instance, even messy alignments such as that in Figure 3
can be handled by interleaving orientations:
</bodyText>
<equation confidence="0.885578">
(6) [((Where/ g1W1 is/a) [[the/e (Secretary/ [of / f Finance/RA-0])]
(when/e needed/g)]) ?/?]
</equation>
<bodyText confidence="0.999545916666667">
This bracketing is of course linguistically implausible, so whether such parses are ac-
ceptable depends on one&apos;s objective. Moreover, it may even remain possible to align
constituents for phenomena whose underlying structure is not context-free—say, ellip-
sis or coordination—as long as the surface structures of the two languages fortuitously
parallel each other (though again the bracketing would be linguistically implausible).
We will return to the subject of ITGs&apos; ordering flexibility in Section 4.
We stress again that the primary purpose of ITGs is to maximize robustness for
parallel corpus analysis rather than to verify grammaticality, and therefore writing
grammars is made much easier since the grammars can be minimal and very leaky.
We consider elsewhere an extreme special case of leaky ITGs, inversion-invariant
transduction grammars, in which all productions occur with both orientations (Wu
1995). As the applications below demonstrate, the bilingual lexical constraints carry
greater importance than the tightness of the grammar.
Formally, an inversion transduction grammar, or ITG, is denoted by G =
(Al, Wi, W2, S), where .AT is a finite set of nonterminals, Wi is a finite set of words
(terminals) of language 1, W2 is a finite set of words (terminals) of language 2, R is
a finite set of rewrite rules (productions), and S E Ai is the start symbol. The space
of word-pairs (terminal-pairs) X = (WI U {E}) x (W2 U {e}) contains lexical transla-
tions denoted x/y and singletons denoted x/€ or c/y, where x E Wi and y E W2. Each
production is either of straight orientation written A [aia2 ad, or of inverted ori-
entation written A —) (aia2...a„), where ai E Alu X and r is the rank of the production.
The set of transductions generated by G is denoted T(G). The sets of (monolingual)
strings generated by G for the first and second output languages are denoted L1(G)
and L2(G), respectively.
</bodyText>
<sectionHeader confidence="0.651125" genericHeader="method">
3. A Normal Form for Inversion Transduction Grammars
</sectionHeader>
<bodyText confidence="0.999865">
We now show that every ITG can be expressed as an equivalent ITG in a 2-normal form
that simplifies algorithms and analyses on ITGs. In particular, the parsing algorithm
of the next section operates on ITGs in normal form. The availability of a 2-normal
</bodyText>
<page confidence="0.99332">
382
</page>
<subsectionHeader confidence="0.268818">
Wu Bilingual Parsing
</subsectionHeader>
<bodyText confidence="0.82235">
form is a noteworthy characteristic of ITGs; no such normal form is available for
unrestricted context-free (syntax-directed) transduction grammars (Aho and Ullman
1969b). The proof closely follows that for standard CFGs, and the proofs of the lemmas
are omitted.
Lemma 1
For any inversion transduction grammar G, there exists an equivalent inversion trans-
duction grammar G&apos; where T(G) = T(G&apos;), such that:
</bodyText>
<listItem confidence="0.995356">
1. If c E Li (G) and c E L2(G), then G&apos; contains a single production of the
form S&apos; c/e, where S&apos; is the start symbol of G&apos; and does not appear on
the right-hand side of any production of G&apos;;
2. otherwise G&apos; contains no productions of the form A ele.
</listItem>
<subsectionHeader confidence="0.846471">
Lemma 2
</subsectionHeader>
<bodyText confidence="0.919960428571429">
For any inversion transduction grammar G, there exists an equivalent inversion trans-
duction grammar G&apos; where T(G) = T(G&apos;), such that the right-hand side of any pro-
duction of G&apos; contains either a single terminal-pair or a list of nonterminals.
Lemma 3
For any inversion transduction grammar G, there exists an equivalent inversion trans-
duction grammar G&apos; where T(G) = T(G&apos;), such that G&apos; does not contain any produc-
tions of the form A B.
</bodyText>
<subsectionHeader confidence="0.763546">
Theorem 1
</subsectionHeader>
<bodyText confidence="0.9806655">
For any inversion transduction grammar G, there exists an equivalent inversion trans-
duction grammar G&apos; in which every production takes one of the following forms:
</bodyText>
<equation confidence="0.7441502">
S c/e A x/ e A [BC]
A —4 x/y A A —4 (BC)
Proof
By Lemmas 1, 2, and 3, we may assume G contains only productions of the form
S e/c, A x/y, A x/e, A -4 fly, A -4 [BiB2], A — (B1B2), A — [B1 Ba], and
</equation>
<bodyText confidence="0.785827">
A (Bi BO where n &gt; 3 and A 0 S. Include in G&apos; all productions of the first six
types. The remaining two types are transformed as follows:
</bodyText>
<equation confidence="0.841161666666667">
For each production of the form A [B1 • • • Bn] we introduce new nonterminals
... Xn_2 in order to replace the production with the set of rules A [Bi Xi], Xi
[B2X2],.,X_3 —4 [Bn-2Xn— 2], Xn — 2 [Bn-iBn]. Let (e,c) be any string-pair deriv-
</equation>
<bodyText confidence="0.8498955">
able from A [B1 • • • Bn], where e is output on stream 1 and c .on stream 2. Define
e` as the substring of e derived from B,, and similarly define c&apos;. Then Xi generates
</bodyText>
<equation confidence="0.88316525">
(el en, c1+ c&amp;quot;) for all 1 &lt; i &lt; n - 1, so the new production A -4 [Bi Xi] also
generates (e, c). No additional string-pairs are generated due to the new productions
(since each Xi is only reachable from Xi_i and X1 is only reachable from A).
For each production of the form A — (B1 • • • B„) we replace the production with
</equation>
<bodyText confidence="0.851288">
the set of rules A -4 (B2Y2) • • • , Yn-3 (Bn-2Yn-2), Yn-2 (Bn—lBn). Let
</bodyText>
<note confidence="0.519107">
(e, c) be any string-pair derivable from A • • • BO, where e is output on stream
1 and c on stream 2. Again define e&apos; and c&apos; as the substrings derived from B,, but
</note>
<footnote confidence="0.831753">
in this case (e, c) = (e1 • • • en, cn • • c1). Then Yi generates (e&apos;1 en, cn ci+
) for all
</footnote>
<page confidence="0.99687">
383
</page>
<figure confidence="0.8747205">
Computational Linguistics Volume 23, Number 3
1 &lt; i &lt; n – 1, so the new production A –+ (B1 Y1) also generates (e, c). Again, no
</figure>
<bodyText confidence="0.481428">
additional string-pairs are generated due to the new productions. 0
Henceforth all transduction grammars will be assumed to be in normal form.
</bodyText>
<sectionHeader confidence="0.781282" genericHeader="method">
4. Expressiveness Characteristics
</sectionHeader>
<bodyText confidence="0.999809813953489">
We now turn to the expressiveness desiderata for a matching formalism. It is of course
difficult to make precise claims as to what characteristics are necessary and/or suffi-
cient for such a model, since no cognitive studies that are directly pertinent to bilingual
constituent alignment are available. Nonetheless, most related previous parallel cor-
pus analysis models share certain conceptual approaches with ours, loosely based on
cross-linguistic theories related to constituency, case frames, or thematic roles, as well
as computational feasibility needs. Below we survey the most common constraints
and discuss their relation to ITGs.
Crossing Constraints. Arrangements where the matchings between subtrees cross
each another are prohibited by crossing constraints, unless the subtrees&apos; immediate
parent constituents are also matched to each other. For example, given the constituent
matchings depicted as solid lines in Figure 4, the dotted-line matchings corresponding
to potential lexical translations would be ruled illegal. Crossing constraints are im-
plicit in many phrasal matching approaches, both constituency-oriented (Kaji, Kida,
and Morimoto 1992; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994) and
dependency-oriented (Sadler and Vendelmans 1990; Matsumoto, Ishimoto, and Ut-
suro 1993). The theoretical cross-linguistic hypothesis here is that the core arguments
of frames tend to stay together over different languages. The constraint is also useful
for computational reasons, since it helps avoid exponential bilingual matching times.
ITGs inherently implement a crossing constraint; in fact, the version enforced by
ITGs is even stronger. This is because even within a single constituent, immediate
subtrees are only permitted to cross in exact inverted order. As we shall argue below,
this restriction reduces matching flexibility in a desirable fashion.
Rank Constraints. The second expressiveness desideratum for a matching formal-
ism is to somehow limit the rank of constituents (the number of children or right-
hand-side symbols), which dictates the span over which matchings may cross. As the
number of subtrees of an Li-constituent grows, the number of possible matchings to
subtrees of the corresponding L2-constituent grows combinatorially, with correspond-
ing time complexity growth on the matching process. Moreover, if constituents can
immediately dominate too many tokens of the sentences, the crossing constraint loses
effectiveness—in the extreme, if a single constituent immediately dominates the en-
tire sentence-pair, then any permutation is permissible without violating the crossing
constraint. Thus, we would like to constrain the rank as much as possible, while still
permitting some reasonable degree of permutation flexibility.
Recasting this issue in terms of the general class of context-free (syntax-directed)
transduction grammars, the number of possible subtree matchings for a single con-
stituent grows combinatorially with the number of symbols on a production&apos;s right-
hand side. However, it turns out that the JIG restriction of allowing only matchings
with straight or inverted orientation effectively cuts the combinatorial growth, while
still maintaining flexibility where needed.
To see how ITGs maintain needed flexibility, consider Figure 5, which shows all 24
possible complete matchings between two constituents of length four each. Nearly all
of these-22 out of 24—can be generated by an ITG, as shown by the parse trees (whose
</bodyText>
<page confidence="0.996118">
384
</page>
<figure confidence="0.760871">
Wu Bilingual Parsing
</figure>
<figureCaption confidence="0.976715">
Figure 4
</figureCaption>
<subsectionHeader confidence="0.781068">
The crossing constraint.
</subsectionHeader>
<bodyText confidence="0.862636230769231">
nonterminal labels are omitted).3 The 22 permitted matchings are representative of real
transpositions in word order between the English-Chinese sentences in our data. The
only two matchings that cannot be generated are very distorted transpositions that we
might call &amp;quot;inside-out&amp;quot; matchings. We have been unable to find real examples in our
data of constituent arguments undergoing &amp;quot;inside-out&amp;quot; transposition.
Note that this hypothesis is for fixed-word-order languages that are lightly in-
flected, such as English and Chinese. It would not be expected to hold for so-called
scrambling or free-word-order languages, or heavily inflected languages. However,
inflections provide alternative surface cues for determining constituent roles (and
3 As discussed later, in many cases more than one parse tree can generate the same subconstituent
matching. The trees shown are the canonical parses, as generated by the grammar of Figure 10.
the policy station .
The Security Bureau granted
</bodyText>
<page confidence="0.68623">
385
</page>
<figure confidence="0.998694678571429">
Computational Linguistics Volume 23, Number 3
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
1 2 3 4
4\ /
X X
1 2 3 4
2 3 4
1 2 3 4
1 2 3 4
51(
1 2 3 4
</figure>
<figureCaption confidence="0.990343">
Figure 5
</figureCaption>
<bodyText confidence="0.897371315789473">
The 24 complete matchings of length four, with ITG parses for 22.
thereby matchings), so it would not be necessary to apply the ITG model to such
languages.
On the other hand, to see how ITGs cut combinatorial growth, consider the table
in Figure 6, which compares growth in the number of legal complete matchings on a
pair of subconstituent sequences. The third column shows the number of all possible
complete matchings between two constituents with a rank of r subconstituents each
(therefore this is also the behavior for unconstrained context-free (syntax-directed)
transduction grammars). Compare this against the second column, which shows the
number of complete matchings that can be accepted by an ITG between a pair of
length-r sequences of subconstituents. The fourth column shows the proportion of
matchings that ITGs can accept. Flexibility is nearly total for sequences of up to r &lt;
4 subconstituents, with a rapid drop thereafter corresponding to the elimination of
undesirably tangled (i.e., noncompositional) matchings.
Figure 7 shows the same numbers over all possible matchings, both complete
and partial; in other words, for the more realistic case where some subconstituents
are permitted to remain unmatched as singletons. The same desirable behavior is
exhibited. The expressiveness of ITGs thus appears inherently suited to the degree of
flexibility versus constraints needed for constituent matching.
</bodyText>
<page confidence="0.961702">
386
</page>
<figure confidence="0.801947">
Wu Bilingual Parsing
</figure>
<figureCaption confidence="0.985835">
Figure 6
</figureCaption>
<bodyText confidence="0.981485">
Growth in number of legal complete subconstituent matchings for context-free (syntax-directed)
transduction grammars with rank r, versus ITGs on a pair of subconstituent sequences of
length r each.
</bodyText>
<sectionHeader confidence="0.831882" genericHeader="method">
5. Stochastic Inversion Transduction Grammars
</sectionHeader>
<bodyText confidence="0.999753777777778">
In a stochastic ITG (SITG), a probability is associated with each rewrite rule. Following
the standard convention, we use a and b to denote probabilities for syntactic and
lexical rules, respectively. For example, the probability of the rule NN [A N] is
aNN—[A = 0.4. The probability of a lexical rule A ox/y is bA(x,y) = 0.001. Let
W1, W2 be the vocabulary sizes of the two languages, and Al = {A1, ,AN} be the
set of nonterminals with indices 1,. . . , N. (For conciseness, we sometimes abuse the
notation by writing an index when we mean the corresponding nonterminal symbol,
as long as this introduces no confusion.) Then for every 1 &lt; i &lt; N, the production
probabilities are subject to the constraint that
</bodyText>
<equation confidence="0.9928535">
E (aq+ai (jk)) b (x,y) = 1
j,k&lt;N
</equation>
<bodyText confidence="0.9999547">
We now introduce an algorithm for parsing with stochastic ITGs that computes
an optimal parse given a sentence-pair using dynamic programming. In bilingual
parsing, just as with ordinary monolingual parsing, probabilizing the grammar permits
ambiguities to be resolved by choosing the maximum-likelihood parse. Our algorithm
is similar in spirit to the recognition algorithm for HMMs (Viterbi 1967) and to CYK
parsing (Kasami 1965; Younger 1967).
Let the input English sentence be e1,. , eT and the corresponding input Chinese
sentence be cl, , cv. As an abbreviation we write e, t for the sequence of words
es+i,es+2, • • • , et, and similarly for cu v; also, es s = c is the empty string. It is convenient
to use a 4-tuple of the form q = (s, t, u, v) to identify each node of the parse tree, where
</bodyText>
<figure confidence="0.997116148148148">
ITG all matchings ratio
1
1
2 2
3 6
4 22
5 90
6 394
7 1,806
8 8,558
9 41,586
10 206,098
11 1,037,718
12 5,293,446
13 27,297,738
14 142,078,746
15 745,387,038
16 3,937,603,038
1
1
2
6
24
120
720
5,040
40,320
362,880
3,628,800
39,916,800
479,001,600
6,227,020,800
87,178,291,200
1,307,674,368,000
20,922,789,888,000
1.000
1.000
1.000
1.000
0.917
0.750
0.547
0.358
0.212
0.115
0.057
0.026
0.011
0.004
0.002
0.001
0.000
387
Computational Linguistics Volume 23, Number 3
</figure>
<figureCaption confidence="0.999855">
Figure 7
</figureCaption>
<bodyText confidence="0.999226333333333">
Growth in number of all legal subconstituent matchings (complete or partial, meaning that
some subconstituents are permitted to remain unmatched as singletons) for context-free
(syntax-directed) transduction grammars with rank r, versus ITGs on a pair of subconstituent
sequences of length r each.
the substrings e, and cv v both derive from the node q. Denote the nonterminal label
on q by f(q). Then for any node q = (s, t, u, v), define
</bodyText>
<equation confidence="0.746624">
6q(i) = 6stu0(i) = max P[subtree of q, ?(q) es r/cu
subtrees of q
</equation>
<bodyText confidence="0.999905">
as the maximum probability of any derivation from i that successfully parses both es t
and cu v. Then the best parse of the sentence pair has probability 60,T,o,v(S).
The algorithm computes 60,T,o,v(S) using the following recurrences. Note that
we generalize argmax to the case where maximization ranges over multiple indices,
by making it vector-valued. Also note that [] and 0 are simply constants, written
mnemonically. The condition (S - s)(t - S) + (U - u)(v - U) 0 is a way to specify
that the substring in one, but not both, languages may be split into an empty string f
and the substring itself; this ensures tha t the recursion terminates, but permits words
that have no match in the other language to map to an € instead.
</bodyText>
<figure confidence="0.976327602040816">
1. Initialization
ITG all matchings ratio
1
2
2 7
3 34
4 207
5 1,466
6 11,471
7 96,034
8 843,527
9 7,678,546
10 71,852,559
11 687,310,394
12 6,693,544,171
13 66,167,433,658
14 662,393,189,919
15 6,703,261,197,506
16 68,474,445,473,303
1
2
7
34
209
1,546
13,327
130,922
1,441,729
17,572,114
234,662,231
3,405,357,682
53,334,454,417
896,324,308,634
16,083,557,845,279
306,827,170,866,106
6,199,668,952,527,617
1.000
1.000
1.000
1.000
0.990
0.948
0.861
0.734
0.585
0.437
0.306
0.202
0.126
0.074
0.041
0.022
0.011
br(er /cz)), 1 &lt; t &lt; T
Met/0, 1 &lt; v &lt; V
br(c/c,), 1 &lt; t &lt; T
0 &lt; v &lt; V
0 &lt; t &lt; T
1 &lt; v &lt; V
388
Wu Bilingual Parsing
2. Recursion
{
1.&lt;i &lt;IV
0&lt; s&lt;t &lt;T
For all i, s, t, u, v such that - -
t—si-v—u&gt;2
max[6L(i), 61(i)] (4)
[I if 6IL(i) 610(0 (5)
() otherwise
6 stuv(i)
stuv(i)
where
6L&apos;uvco =
- (i)
stuv \
ftuv(i)
o&apos;j
stuv(1)
_ v[stluv(i) _
61 uv(i) =
max a i_,uk] ssuu (j) 5s1u(k) (6)
1&lt;j&lt;1,1
1&lt;k&lt;N
s&lt;S&lt; t
u&lt;=t1&lt;tt
(S—s)(t— S)+(U—u)(v—L1)*0
argmax ai—uki 6ssuu(j) 6 stuv(k) (7)
1&lt;j&lt;N (1k) 6ssuv(1) 5su(k) (8)
ANt
u&lt;tl&lt;0
(S—s)(t—S)-1-(U-0)(v—U)$0
max
1&lt;j&lt;N
1&lt;k&lt;N
s&lt;SSt
u&lt;1.1S0
(S — s)(t — u)(v— U)$0
</figure>
<table confidence="0.9412946">
tPtuvco argmax a (jk) 6sSUv(I) 6S1uU(k)
Klu co 1&lt;j&lt;1,1
etuvco 1,7&lt;&lt;Jcs&lt;&lt;Ni
vPtuv co _ u&lt;li&lt;0
(S—s)(t— (LI= u)(1,—U)$0
</table>
<sectionHeader confidence="0.68989" genericHeader="method">
3. Reconstruction
</sectionHeader>
<bodyText confidence="0.986350333333333">
Initialize by setting the root of the parse tree to qi = (0, T, 0, V) and its
nonterminal label to t(qi) = S. The remaining descendants in the optimal
parse tree are then given recursively for any q = (s, t, u, v) by:
</bodyText>
<equation confidence="0.975076375">
NIL if t-s+v-u&lt;2
LEFT(q) = (S, (t(q)), u, v[q] ce(q))) if 0 q(e(q)) = [] and t—s+v—u&gt;3
{ (s, cr-) (e(q)), 4 (t(q)), v) if 8 q(f(q)) = 0 and
NIL if t-s+v-u&lt;2
RIGHT(q) = { (ail (f(q)), t, v[q] (t(q)), v) if Og(e(q)) = [] and t-s+v-u&gt;2
(4 (t(q)), t, u, 14) (t(q))) if et/((g)) = () and t-s+v-u&gt;2
r(LEFT(q)) = tgeg(t(q))(e(q))
e(RIGHT(q)) = K:q(i(q)) (t(q))
</equation>
<bodyText confidence="0.997236">
The time complexity of this algorithm in the general case is e(N3T3V3), where
N is the number of distinct nonterminals and T and V are the lengths of the two
sentences. This is a factor of V3 more than monolingual chart parsing, but has turned
out to remain quite practical for corpus analysis, where parsing need not be real-time.
</bodyText>
<equation confidence="0.357031">
(9)
</equation>
<page confidence="0.843713">
389
</page>
<note confidence="0.591481">
Computational Linguistics Volume 23, Number 3
</note>
<sectionHeader confidence="0.902205" genericHeader="method">
6. Translation-driven Segmentation
</sectionHeader>
<bodyText confidence="0.999962821428572">
Segmentation of the input sentences is an important step in preparing bilingual cor-
pora for various learning procedures. Different languages realize the same concept
using varying numbers of words; for example, a single English word may surface as
a compound in French. This complicates the problem of matching the words between
a sentence-pair, since it means that compounds or collocations must sometimes be
treated as lexical units. The translation lexicon is assumed to contain collocation trans-
lations to facilitate such multiword matchings. However, the input sentences do not
come broken into appropriately matching chunks, so it is up to the parser to decide
when to break up potential collocations into individual words.
The problem is particularly acute for English and Chinese because word bound-
aries are not orthographically marked in Chinese text, so not even a default chunking
exists upon which word matchings could be postulated. (Sentences (2) and (5) demon-
strate why the obvious trick of taking single characters as words is not a workable
strategy.) The usual Chinese NLP architecture first preprocesses input text through a
word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang
and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly,
bilingual parsing will be hampered by any errors arising from segmentation ambigui-
ties that could not be resolved in the isolated monolingual context because even if the
Chinese segmentation is acceptable monolingually, it may not agree with the words
present in the English sentence. Matters are made still worse by unpredictable omis-
sions in the translation lexicon, even for valid compounds.
We therefore extend the algorithm to optimize the Chinese sentence segmentation
in conjunction with the bracketing process. Note that the notion of a Chinese &amp;quot;word&amp;quot;
is a longstanding linguistic question, that our present notion of segmentation does
not address. We adhere here to a purely task-driven definition of what a correct &amp;quot;seg-
mentation&amp;quot; is, namely that longer segments are desirable only when no compositional
translation is possible. The algorithm is modified to include the following computa-
tions, and remains the same otherwise:
</bodyText>
<equation confidence="0.848555833333334">
1. Initialization 6(sItuv(i) = b (es t 0&lt;s&lt;t&lt;T (14)
2. Recursion 0&lt;u&lt;v&lt;V
6stuv(i) nclaxVistluv(i), 62uv(i), 6)tuv(i)} (15)
[] if ba„(i) &gt; etuv(i) and 64,(i) &gt; qtuv(i)
Ostuv(i)= () if d„(i) &gt; 6Llu0(i) and 151v( i) &gt; 63uv(i) (16)
0 otherwise
</equation>
<sectionHeader confidence="0.86402" genericHeader="method">
3. Reconstruction
</sectionHeader>
<table confidence="0.976338181818182">
{ NIL if t---s+v—u&lt;2 (17)
(S,01q}(e(11)),U, u[qi(e(q))) if 9q(P(q)) = [1 and t-s+v-u&gt;2
LEFT(q)
(s,4)(e(q)),4 (e(q)),v) if 0q(e(q)) = () and t-s+v-u&gt;2
NIL otherwise
390
Wu Bilingual Parsing
BIGHT(q) = NIL if t—s+v—u&lt;2 (18)
t, (f(q)), v) if Eig(t(q)) = [] and t-s+v--u&gt;2
(o-4) (f(q)), t, u, 4 (f(q))) if 04(q)) () and
NIL otherwise
</table>
<bodyText confidence="0.99636075">
In our experience, this method has proven extremely effective for avoiding misseg-
mentation pitfalls, essentially erring only in pathological cases involving coordination
constructions or lexicon coverage inadequacies. The method is also straightforward to
employ in tandem with other applications, such as those below.
</bodyText>
<sectionHeader confidence="0.551986" genericHeader="method">
7. Bracketing
</sectionHeader>
<bodyText confidence="0.9998712">
Bracketing is another intermediate corpus annotation, useful especially when a full-
coverage grammar with which to parse a corpus is unavailable (for Chinese, an even
more common situation than with English). Aside from purely linguistic interest,
bracket structure has been empirically shown to be highly effective at constraining sub-
sequent training of, for example, stochastic context-free grammars (Pereira and Schabes
1992; Black, Garside, and Leech 1993). Previous algorithms for automatic bracketing
operate on monolingual texts and hence require more grammatical constraints; for ex-
ample, tactics employing mutual information have been applied to tagged text (Mager-
man and Marcus 1990).
Our method based on SITGs operates on the novel principle that lexical correspon-
dences between parallel sentences yields information from which partial bracketings
for both sentences can be extracted. The assumption that no grammar is available
means that constituent categories are not differentiated. Instead, a generic bracket-
ing transduction grammar is employed, containing only one nonterminal symbol, A,
which rewrites either recursively as a pair of A&apos;s or as a single terminal-pair:
</bodyText>
<figure confidence="0.742472625">
a
A [AA]
a
A (AA)
A ui/vi for all i, j English-Chinese lexical translations
A bi,
for all i English vocabulary
A --+ f/vi for all j Chinese vocabulary
</figure>
<bodyText confidence="0.999656">
Longer productions with rank &gt; 2 are not needed; we show in the subsections below
that this minimal transduction grammar in normal form is generatively equivalent
to any reasonable bracketing transduction grammar. Moreover, we also show how
postprocessing using rotation and flattening operations restores the rank flexibility so
that an output bracketing can hold more than two immediate constituents, as shown
in Figure 11.
The bu distribution actually encodes the English-Chinese translation lexicon with
degrees of probability on each potential word translation. We have been using a lexicon
that was automatically learned from the HKUST English-Chinese Parallel Bilingual
Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and
collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM
word-translation-learning procedure (Wu and Xia 1994). The latter stage gives us the
probabilities directly. For the two singleton productions, which permit any word in
either sentence to be unmatched, a small &amp;constant can be chosen for the probabilities
b1, and b1, so that the optimal bracketing resorts to these productions only when it is
</bodyText>
<page confidence="0.995364">
391
</page>
<note confidence="0.651097">
Computational Linguistics Volume 23, Number 3
</note>
<bodyText confidence="0.982833388888889">
otherwise impossible to match the singletons. The parameter a here is of no practical
effect, and is chosen to be very small relative to the by probabilities of lexical translation
pairs. The result is that the maximum-likelihood parser selects the parse tree that best
meets the combined lexical translation preferences, as expressed by the by probabilities.
Pre-/post positional biases. Many bracketing errors are caused by singletons. With
singletons, there is no cross-lingual discrimination to increase the certainty between
alternative bracketings. A heuristic to deal with this is to specify for each of the two
languages whether prepositions or postpositions are more common, where &amp;quot;preposi-
tion&amp;quot; here is meant not in the usual part-of-speech sense, but rather in a broad sense
of the tendency of function words to attach left or right. This simple strategem is
effective because the majority of unmatched singletons are function words that lack
counterparts in the other language. This observation holds assuming that the transla-
tion lexicon&apos;s coverage is reasonably good. For both English and Chinese, we specify
a prepositional bias, which means that singletons are attached to the right whenever
possible.
A Singleton-Rebalancing Algorithm. We give here an algorithm for further improv-
ing the bracketing accuracy in cases of singletons. Consider the following bracketing
produced by the algorithm of the previous section:
</bodyText>
<listItem confidence="0.609505">
(7) [[The/e [Authority/Wit ( [be /e accountable/.] [to the/e
Prig [Financial/ft&amp; Secretary /1 UM I. I
</listItem>
<bodyText confidence="0.9999228125">
The prepositional bias has already correctly restricted the singleton The/e to attach to
the right, but of course The does not belong outside the rest of the sentence, but rather
with Authority. The problem is that singletons have no discriminative power between
alternative bracket matchings—they only contribute to the ambiguity. We can minimize
the impact by moving singletons as deep as possible, closer to the individual word
they precede or succeed; or in other words, we can widen the scope of the brackets
immediately following the singleton. In general this improves precision since wide-
scope brackets are less constraining.
The algorithm employs a rebalancing strategy reminiscent of balanced tree struc-
tures using left and right rotations. A left rotation changes a (A(BC)) structure to a
((AB)C) structure, and vice versa for a right rotation. The task is complicated by the
presence of both [] and () brackets with both L1- and L2-singletons, since each com-
bination presents different interactions. To be legal, a rotation must preserve symbol
order on both output streams. However, the following lemma shows that any subtree
can always be rebalanced at its root if either of its children is a singleton of either
language.
</bodyText>
<subsectionHeader confidence="0.665984">
Lemma 4
</subsectionHeader>
<bodyText confidence="0.99991525">
Let x be an Li-singleton, y be an L2-singleton, and A, B, C be arbitrary terminal or
nonterminal symbols. Then the following properties hold for the [ ] and () operators,
where the relation means that the same two output strings are generated, and the
matching of the symbols is preserved:
</bodyText>
<equation confidence="0.521790666666667">
(Associativity)
[A[BC]].=• [[AW]
(A(BC)) ((AB)C)
</equation>
<page confidence="0.658349">
392
</page>
<figure confidence="0.884932619047619">
Wu Bilingual Parsing
SINK-SINGLEToN(node)
1 if node is not a leaf
2 if a rotation property applies at node
3 apply the rotation to node
4 child the child into which the singleton was rotated
5 SINK-SINGLET0N(chi/d)
REBALANCE-TREE(node)
1 if node is not a leaf
2 REBALANCE-TREE(ieft-Chi/d[nOdel)
3 REBALANCE-TREE(right-Child[nOde])
4 SINK-SiNGLEToN(node)
Figure 8
The singleton rebalancing schema.
(Li-singleton bidirectionality)
[Ax] (Ax)
[xA] (xA)
(L2-singleton flipping commutativity)
[Ay] (yA)
[yA],== (Ay)
(Li-singleton rotation properties)
</figure>
<equation confidence="0.988315333333333">
[x(AB)] (x(AB)) ((xA)B) ([xA]B)
(x[AB]) [x[AB]] [[xA]B] [(xA)B]
[(AB)x] 7-= ((AB)x) (A(Bx)) (A[Bx])
([AB]x).=• [[AB]x] [A[Bx]] [A(Bx)]
(L2-singleton rotation properties)
[y(AB)] ((AB)y) (A(By)) .=s (A[yB])
(y[AB]) [[AB]y] [A[By]] [A(yB)]
[(AB)y] (y(AB)) ((yA)B) ([Ay]B)
([AB]y) [y[AB]] [[yA]B] [(Ay)B]
</equation>
<bodyText confidence="0.997990333333333">
The method of Figure 8 modifies the input tree to attach singletons as closely
as possible to couples, but remaining consistent with the input tree in the following
sense: singletons cannot &amp;quot;escape&amp;quot; their immediately surrounding brackets. The key is
that for any given subtree, if the outermost bracket involves a singleton that should
be rotated into a subtree, then exactly one of the singleton rotation properties will
apply. The method proceeds depth-first, sinking each singleton as deeply as possible.
</bodyText>
<page confidence="0.997411">
393
</page>
<figure confidence="0.997320666666667">
Computational Linguistics Volume 23, Number 3
1 2 3 4 1 2 3 4 1 2 3 4
(a) (b) (c)
</figure>
<figureCaption confidence="0.745291">
Figure 9
</figureCaption>
<bodyText confidence="0.9233775">
Alternative ITG parse trees for the same matching.
For example, after rebalancing, sentence (7) is bracketed as follows:
(8) [R[The/e Authority/t] [will/14f, ([be/€ accountable/RN] [to
the/e [e/lit] [Financial/k&amp; Secretary/151 111)11] ./. ]
Flattening the Bracketing. In the worst case, both sentences might have perfectly
aligned words, lending no discriminative leverage whatsoever to the bracketer. This
leaves a very large number of choices: if both sentences are of length 1, then there
are (2/1)/÷, possible bracketings with rank 2, none of which is better justified than any
other. Thus to improve accuracy, we should reduce the specificity of the bracketing&apos;s
commitment in such cases.
An inconvenient problem with ambiguity arises in the simple bracketing grammar
above, illustrated by Figure 9; there is no justification for preferring either (a) or (b) over
the other. In general the problem is that both the straight and inverted concatenation
operations are associative. That is, [A[AA]] and [[AA]A] generate the same two output
strings, which are also generated by [AAA]; and similarly with (A (AA)) and ((AA)A),
which can also be generated by (AAA). Thus the parse shown in (c) is preferable to
either (a) or (b) since it does not make an unjustifiable commitment either way.
Productions in the form of (c), however, are not permitted by the normal form we
use, in which each bracket can only hold two constituents. Parsing must overcommit,
since the algorithm is always forced to choose between (A(BC)) and ((AB)C) structures
even when no choice is clearly better. We could relax the normal form constraint,
but longer productions clutter the grammar unnecessarily and, in the case of generic
bracketing grammars, reduce parsing efficiency considerably.
Instead, we employ a more complicated but better-constrained grammar as shown
in Figure 10, designed to produce only canonical tail-recursive parses. We differenti-
ate type A and B constituents, representing subtrees whose roots have straight and
inverted orientation, respectively. Under this grammar, a series of nested constituents
with the same orientation will always have a left-heavy derivation. The guarantee
that parsing will produce a tail-recursive tree facilitates easily identification of those
nesting levels that are associative (and therefore arbitrary), so that those levels can
be &amp;quot;flattened&amp;quot; by a postprocessing stage after parsing into non-normal form trees like
the one in Figure 9(c). The algorithm proceeds bottom-up, eliminating as many brack-
ets as possible, by making use of the associativity equivalences [[AB]C] ,=s [ABC] and
((AB)C) (ABC). The singleton bidirectionality and flipping commutativity equiv-
alences (see Lemma 4) can also be applied whenever they render the associativity
equivalences applicable.
</bodyText>
<equation confidence="0.67655">
1 2 3 4 1 2 3 4 1 2 3 4
</equation>
<page confidence="0.813471">
394
</page>
<figure confidence="0.817623052631579">
Wu Bilingual Parsing
A 4 [AB]
A 4 [BB]
A [CB]
A 4 [AC]
A 4 [BC]
B 4 (AA)
B (BA)
B 4 (CA)
B `4 (AC)
B 4 (BC)
C udvi for all i,j English-Chinese lexical translations
C b&apos;&apos; Ude for all i English vocabulary
C €/v1 for all j Chinese vocabulary
Figure 10
A stochastic constituent-matching ITG.
The final result after flattening sentence (8) is as follows:
(9) [ The/ e Authority/ will/ ([ be/e accountable/ ] [ to the/c e/ Financial/
Secretary/ 1) .1
</figure>
<bodyText confidence="0.99802245">
Experiment. Approximately 2,000 sentence-pairs with both English and Chinese
lengths of 30 words or less were extracted from our corpus and bracketed using
the algorithm described. Several additional criteria were used to filter out unsuitable
sentence-pairs. If the lengths of the pair of sentences differed by more than a 2:1 ratio,
the pair was rejected; such a difference usually arises as the result of an earlier error
in automatic sentence alignment. Sentences containing more than one word absent
from the translation lexicon were also rejected; the bracketing method is not intended
to be robust against lexicon inadequacies. We also rejected sentence-pairs with fewer
than two matching words, since this gives the bracketing algorithm no discriminative
leverage; such pairs accounted for less than 2% of the input data. A random sample of
the bracketed sentence-pairs was then drawn, and the bracket precision was computed
under each criterion for correctness. Examples are shown in Figure 11.
The bracket precision was 80% for the English sentences, and 78% for the Chinese
sentences, as judged against manual bracketings. Inspection showed the errors to be
due largely to imperfections of our translation lexicon, which contains approximately
6,500 English words and 5,500 Chinese words with about 86% translation accuracy (Wu
and Xia 1994), so a better lexicon should yield substantial performance improvement.
Moreover, if the resources for a good monolingual part-of-speech or grammar-based
bracketer such as that of Magerman and Marcus (1990) are available, its output can
readily be incorporated in complementary fashion as discussed in Section 9.
</bodyText>
<page confidence="0.997628">
395
</page>
<table confidence="0.9914075">
Computational Linguistics Volume 23, Number 3
[These/lait. arrangements/ M4P will/E E/TIJ enhance/UA our/RITI ([cill&apos;) ability/NE
)3] [to/c c/19 lk maintain/a4 monetary/8 stability/ ff in the years to come/E])
./. 1
[The / E Authority/V 4 N will/14 &apos;ft ( [be / c accountable/] [to the /€ € /relt
Financial/ft&amp; Secretary/I-571) ./. ]
[They/MI ( are/e right/EA c/-I-3). tole do/( Eatiff so/c) ./o ]
[([ Even/e more/St important/IR ] [le however/ill ]) [If f /(fg, is/`1 to make the
very best of our/c e/§fg tig own/4 f c/Erg talent/AAs ] ./0 ]
PR hope/e c/&lt;&gt;0 employers/igI will/e make full/f c/5-j- A use/ fg [of/€
those/Ili -g.] (4e/Er31 who/A] [have acquired/e E/V-0.1 new/JI skills/RR ])
[through/&amp;114 this/201 programme/Pill]) ./o ]
[I/ft have/E &lt;&gt; at/e length/:: ( on/f how/ frAl we/fill E/MI.) [can/JJ
boost/e e/aie our/* 4 e/n prosperity/25k] ./ a ]
</table>
<figureCaption confidence="0.54519">
Figure 11
</figureCaption>
<bodyText confidence="0.359691">
Bracketing output examples. (&lt;&gt; = unrecognized input token.)
</bodyText>
<sectionHeader confidence="0.613125" genericHeader="method">
8. Alignment
</sectionHeader>
<subsectionHeader confidence="0.999193">
8.1 Phrasal Alignment
</subsectionHeader>
<bodyText confidence="0.999828">
Phrasal translation examples at the subsentential level are an essential resource for
many MT and MAT architectures. This requirement is becoming increasingly direct
for the example-based machine translation paradigm (Nagao 1984), whose translation
flexibility is strongly restricted if the examples are only at the sentential level. It can
now be assumed that a parallel bilingual corpus may be aligned to the sentence level
with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick
1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for lan-
guages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential
alignment have been developed as well as granularities of the character (Church 1993),
word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown
1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. How-
ever, the identification of subsentential, nested, phrasal translations within the parallel
texts remains a nontrivial problem, due to the added complexity of dealing with con-
stituent structure. Manual phrasal matching is feasible only for small corpora, either
for toy-prototype testing or for narrowly restricted applications.
Automatic approaches to identification of subsentential translation units have
largely followed what we might call a &amp;quot;parse-parse-match&amp;quot; procedure. Each half of
the parallel corpus is first parsed individually using a monolingual grammar. Subse-
quently, the constituents of each sentence-pair are matched according to some heuristic
procedure. A number of recent proposals can be cast in this framework (Sadler and
Vendelmans 1990; Kaji, Kida, and Morimoto 1992; Matsumoto, Ishimoto, and Utsuro
1993; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994).
The parse-parse-match procedure is susceptible to three weaknesses:
</bodyText>
<listItem confidence="0.9012576">
• Appropriate, robust, monolingual grammars may not be available. This
condition is particularly relevant for many non—Western European
languages such as Chinese. A grammar for this purpose must be robust
since it must still identify constituents for the subsequent matching
process even for unanticipated or ill-formed input sentences.
</listItem>
<page confidence="0.99365">
396
</page>
<note confidence="0.322854">
Wu Bilingual Parsing
</note>
<bodyText confidence="0.61521525">
• The grammars may be incompatible across languages. The best-matching
constituent types between the two languages may not include the same
core arguments. While grammatical differences can make this problem
unavoidable, there is often a degree of arbitrariness in a grammar&apos;s
chosen set of syntactic categories, particularly if the grammar is designed
to be robust. The mismatch can be exacerbated when the monolingual
grammars are designed independently, or under different theoretical
considerations.
</bodyText>
<listItem confidence="0.883852">
• Selection between multiple possible arrangements may be arbitrary. By an
&amp;quot;arrangement&amp;quot; between any given pair of sentences from the parallel
</listItem>
<bodyText confidence="0.948252378378378">
corpus, we mean a set of matchings between the constituents of the
sentences. The problem is that in some cases, a constituent in one
sentence may have several potential matches in the other, and the
matching heuristic may be unable to discriminate between the options.
In the sentence pair of Figure 4, for example, both Security Bureau and
police station are potential lexical matches to !/_`,-VN. To choose the best
set of matchings, an optimization over some measure of overlap between
the structural analysis of the two sentences is needed. Previous
approaches to phrasal matching employ arbitrary heuristic functions on,
say, the number of matched subconstituents.
Our method attacks the weaknesses of the parse-parse-match procedure by us-
ing (1) only a translation lexicon with no language-specific grammar, (2) a bilingual
rather than monolingual formalism, and (3) a probabilistic formulation for resolving
the choice between candidate arrangements. The approach differs in its single-stage
operation that simultaneously chooses the constituents of each sentence and the match-
ings between them.
The raw phrasal translations suggested by the parse output were then filtered to
remove those pairs containing more than 50% singletons, since such pairs are likely to
be poor translation examples. Examples that occurred more than once in the corpus
were also filtered out, since repetitive sequences in our corpus tend to be nongram-
matical markup. This yielded approximately 2,800 filtered phrasal translations, some
examples of which are shown in Figure 12. A random sample of the phrasal translation
pairs was then drawn, giving a precision estimate of 81.5%.
Although this already represents a useful level of accuracy, it does not in our opin-
ion reflect the full potential of the formalism. Inspection revealed that performance was
greatly hampered by our noisy translation lexicon, which was automatically learned;
it could be manually post-edited to reduce errors. Commercial on-line translation lex-
icons could also be employed if available. Higher precision could be also achieved
without great effort by engineering a small number of broad nonterminal categories.
This would reduce errors for known idiosyncratic patterns, at the cost of manual rule
building.
The automatically extracted phrasal translation examples are especially useful
where the phrases in the two languages are not compositionally derivable solely from
obvious word translations. An example is [have acquired/e oi.nij new/t skills/It
lig] in Figure 11. The same principle applies to nested structures also, such as ([ /Erg
I who/A. [ have acquired/€ c/VYIJ new /I skills/I ]), on up to the sentence
level.
</bodyText>
<page confidence="0.993811">
397
</page>
<table confidence="0.927583714285714">
Computational Linguistics Volume 23, Number 3
1 % in real ryonsA
Would you i&apos;VZ
an acceptable starting point for this new policy IITIJMAial_RM a M(3`g EM
are about 3. 5 million a 350M
born in Hong
for Hong Ati4
</table>
<bodyText confidence="0.897307">
have the right to decide our Vt&amp;tfil
in what way the Government would increase elf Mg tff &apos;A ft 111 Erg -ft g Nif, ;Rt.
their job opportunities; and
last month All A
never to say &amp;quot;never &amp;quot; TWV&amp;quot;*T&amp;quot;
reserves and surpluses ME RIMR
starting point for this new policy aitima&amp;Aurgcm
there will be many practical difficulties in terms NU- 14 P, --11/ X w fg INN
of implementation
year ended 3 1 March 1 9 9 1 *1 —AA —A-11--F—E1
</bodyText>
<figureCaption confidence="0.4958925">
Figure 12
Examples of extracted phrasal translations.
</figureCaption>
<subsectionHeader confidence="0.999487">
8.2 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999858458333333">
Under the ITG model, word alignment becomes simply the special case of phrasal
alignment at the parse tree leaves. This gives us an interesting alternative perspective,
from the standpoint of algorithms that match the words between parallel sentences. By
themselves, word alignments are of little use, but they provide potential anchor points
for other applications, or for subsequent learning stages to acquire more interesting
structures.
Word alignment is difficult because correct matchings are not usually linearly
ordered, i.e., there are crossings. Without some additional constraints, any word po-
sition in the source sentence can be matched to any position in the target sentence,
an assumption that leads to high error rates. More sophisticated word alignment al-
gorithms therefore attempt to model the intuition that proximate constituents in close
relationships in one language remain proximate in the other. The later IBM models are
formulated to prefer collocations (Brown et al. 1993). In the case of word_align (Dagan,
Church, and Gale 1993; Dagan and Church 1994), a penalty is imposed according to
the deviation from an ideal matching, as constructed by linear interpolation.&apos;
From this point of view, the proposed technique is a word alignment method that
imposes a more realistic distortion penalty. The tree structure reflects the assumption
that crossings should not be penalized as long as they are consistent with constituent
structure. Figure 7 gives theoretical upper bounds on the matching flexibility as the
lengths of the sequences increase, where the constituent structure constraints are re-
flected by high flexibility up to length-4 sequences and a rapid drop-off thereafter. In
other words, ITGs appeal to a language universals hypothesis, that the core arguments
of frames, which exhibit great ordering variation between languages, are relatively
few and surface in syntactic proximity. Of course, this assumption over-simplistically
</bodyText>
<footnote confidence="0.640555">
4 Direct comparison with word _align should be avoided, however, since it is intended to work on corpora
whose sentences are not aligned.
</footnote>
<page confidence="0.992495">
398
</page>
<bodyText confidence="0.9654174375">
Wu Bilingual Parsing
blends syntactic and semantic notions. That semantic frames for different languages
share common core arguments is more plausible than that syntactic frames do. In ef-
fect we are relying on the tendency of syntactic arguments to correlate closely with
semantics. If in particular cases this assumption does not hold, however, the damage
is not too great—the model will simply drop the offending word matchings (dropping
as few as possible).
In experiments with the minimal bracketing transduction grammar, the large ma-
jority of errors in word alignment were caused by two outside factors. First, word
matchings can be overlooked simply due to deficiencies in our translation lexicon. This
accounted for approximately 42% of the errors. Second, sentences containing nonliteral
translations obviously cannot be aligned down to the word level. This accounted for
another approximate 50% of the errors. Excluding these two types of errors, accuracy
on word alignment was 96.3%. In other words, the tree structure constraint is strong
enough to prevent most false matches, but almost never inhibits correct word matches
when they exist.
</bodyText>
<sectionHeader confidence="0.961459" genericHeader="method">
9. Bilingual Constraint Transfer
</sectionHeader>
<subsectionHeader confidence="0.999928">
9.1 Monolingual Parse Trees
</subsectionHeader>
<bodyText confidence="0.999748857142857">
A parse may be available for one of the languages, especially for well-studied lan-
guages such as English. Since this eliminates all degrees of freedom in the English
sentence structure, the parse of the Chinese sentence must conform with that given
for the English. Knowledge of English bracketing is thus used to help parse the Chi-
nese sentence; this method facilitates a kind of transfer of grammatical expertise in
one language toward bootstrapping grammar acquisition in another.
A parsing algorithm for this case can be implemented very efficiently. Note that
the English parse tree already determines the split point S for breaking eo T into two
constituent subtrees deriving eo s and es T respectively, as well as the nonterminal
labels j and k for each subtree. The same then applies recursively to each subtree.
We indicate this by turning S. j, and k into deterministic functions on the English
constituents, writing Sst, jst, and kt to denote the split point and the subtree labels for
any constituent e, t. The following simplifications can then be made to the parsing
algorithm:
</bodyText>
<sectionHeader confidence="0.989867" genericHeader="method">
2. Recursion
</sectionHeader>
<bodyText confidence="0.806765">
For all English constituents es..t and all i, u, v such that { o&lt;17&lt;&lt;11\1&lt;v
</bodyText>
<equation confidence="0.973144">
6Lluv(i) = max at—u,tksti 6s,s,t,u,u Ust) 6s„,t,u,v(kst)
u&lt;u&lt;v (19)
vav (i) = argmax 6s,sst,u,u (jst) 6s,,,t,u,0(kst) (20)
u&lt;u&lt;v
duo (i) = max ai— (j„k,t) (5s,Sst,U,v(ist) 6Sst,t,u,U(k5t)
U&lt;v (21)
u&lt;
vPtuv ( i) = argmax 6s,s,,,u,v(jst) 6s,t,u,u(kst) (22)
u&lt;u&lt;v
3. Reconstruction
LEFT(q) =
(s, Sst, u, v{q}(t(q))) if 0q(e(q)) = (s, Sst, 4 (t(q)), v) if 0q(e(q)) = (23)
()
</equation>
<page confidence="0.988834">
399
</page>
<note confidence="0.672768">
Computational Linguistics Volume 23, Number 3
</note>
<equation confidence="0.997315">
RIGHT(q) =
(S1, t, v[q1(f(q)),v) if 01((q)) = [1
(24)
{ (S5t, t, u, 4) (e(q))) if 0 q(f(q)) = ()
t(LEFT(q)) = jst (25)
e(RIGHT(q)) = icst (26)
</equation>
<bodyText confidence="0.9952865">
The time complexity for this constrained version of the algorithm drops from
e(N3T3V3) to e(TV3).
</bodyText>
<subsectionHeader confidence="0.99892">
9.2 Partial Parse Trees
</subsectionHeader>
<bodyText confidence="0.999984">
A more realistic in-between scenario occurs when partial parse information is available
for one or both of the languages. Special cases of particular interest include applications
where bracketing or word alignment constraints may be derived from external sources
beforehand. For example, a broad-coverage English bracketer may be available. If such
constraints are reliable, it would be wasteful to ignore them.
A straightforward extension to the original algorithm inhibits hypotheses that
are inconsistent with given constraints. Any entries in the dynamic programming ta-
ble corresponding to illegal subhypotheses—i.e., those that would violate the given
bracket-nesting or word alignment conditions—are preassigned negative infinity val-
ues during initialization indicating impossibility. During the recursion phase, computa-
tion of these entries is skipped. Since their probabilities remain impossible throughout,
the illegal subhypotheses will never participate in any ML bibracketing. The running
time reduction in this case depends heavily on the domain constraints.
We have found this strategy to be useful for incorporating punctuation constraints.
Certain punctuation characters give constituency indications with high reliability; &amp;quot;per-
fect separators&amp;quot; include colons and Chinese full stops, while &amp;quot;perfect delimiters&amp;quot; in-
clude parentheses and quotation marks.
</bodyText>
<sectionHeader confidence="0.549857" genericHeader="method">
10. Unrestricted-Form Grammars
</sectionHeader>
<bodyText confidence="0.999981777777778">
It is possible to construct a parser that accepts unrestricted-form, rather than normal-
form, grammars. In this case an Earley-style scheme (Earley 1970), employing an active
chart, can be used. The time complexity remains the same as the normal-form case.
We have found this to be useful in practice. For bracketing grammars of the type
considered in this paper, there is no advantage. However, for more complex, linguisti-
cally structured grammars, the more flexible parser does not require the unreasonable
numbers of productions that can easily arise from normal-form requirements. For most
grammars, we have found performance to be comparable or faster than the normal-
form parser.
</bodyText>
<sectionHeader confidence="0.688577" genericHeader="conclusions">
11. Conclusion
</sectionHeader>
<bodyText confidence="0.999987666666667">
The twin concepts of bilingual language modeling and bilingual parsing have been
proposed. We have introduced a new formalism, the inversion transduction grammar,
and surveyed a variety of its applications to extracting linguistic information from
parallel corpora. Its amenability to stochastic formulation, useful flexibility with leaky
and minimal grammars, and tractability for practical applications are desirable proper-
ties. Various tasks such as segmentation, word alignment, and bracket annotation are
naturally incorporated as subproblems, and a high degree of compatibility with con-
ventional monolingual methods is retained. In conjunction with automatic procedures
for learning word translation lexicons, SITGs bring relatively underexploited bilingual
</bodyText>
<page confidence="0.989416">
400
</page>
<bodyText confidence="0.936464888888889">
Wu Bilingual Parsing
correlations to bear on the task of extracting linguistic information for languages less
studied than English.
We are currently pursuing several directions. We are developing an iterative train-
ing method based on expectation-maximization for estimating the probabilities from
parallel training corpora. Also, in contrast to the applications discussed here, which
deal with analysis and annotation of parallel corpora, we are working on incorporating
the SITG model directly into our run-time translation architecture. The initial results
indicate excellent performance gains.
</bodyText>
<sectionHeader confidence="0.982865" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993355">
I would like to thank Xuanyin Xia, Eva
Wai-Man Fong, Pascale Fung, and Derick
Wood, as well as an anonymous reviewer
whose comments were of great value.
</bodyText>
<sectionHeader confidence="0.994117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999604717391305">
Aho, Alfred V. and Jeffrey D. Ullman. 1969a.
Properties of syntax directed translations.
Journal of Computer and System Sciences,
3(3):319-334.
Aho, Alfred V. and Jeffrey D. Ullman.
1969b. Syntax directed translations and
the pushdown assembler. Journal of
Computer and System Sciences, 3(1):37-56.
Aho, Alfred V. and Jeffrey D. Ullman. 1972.
The Theory of Parsing, Translation, and
Compiling. Prentice Hall, Englewood
Cliffs, NJ.
Black, Ezra, Roger Garside, and Geoffrey
Leech, editors. 1993. Statistically-Driven
Computer Grammars of English: The
IBM/Lancaster Approach. Editions Rodopi,
Amsterdam.
Brown, Peter F., John Cocke, Stephen
A. DellaPietra, Vincent J. DellaPietra,
Frederick jelinek, John D. Lafferty, Robert
L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine
translation. Computational Linguistics,
16(2):29-85.
Brown, Peter F., Stephen A. DellaPietra,
Vincent J. DellaPietra, and Robert
L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263-311.
Brown, Peter F., Jennifer C. Lai, and Robert
L. Mercer. 1991. Aligning sentences in
parallel corpora. In Proceedings of the 29th
Annual Meeting, pages 169-176, Berkeley,
CA. Association for Computational
Linguistics.
Catizone, Roberta, Graham Russell, and
Susan Warwick. 1989. Deriving
translation data from bilingual texts. In
Proceedings of the First Lexical Acquisition
Workshop, Detroit, MI.
Chang, Chao-Huang and Cheng-Der Chen.
1993. HMM-based part-or-speech tagging
for Chinese corpora. In Proceedings of the
Workshop on Very Large Corpora, pages
40-47, Columbus, OH, June.
Chen, Stanley F. 1993. Aligning sentences in
bilingual corpora using lexical
information. In Proceedings of the 31st
Annual Meeting, pages 9-16, Columbus,
OH. Association for Computational
Linguistics.
Chiang, Tung-Hui, Jing-Shin Chang,
Ming-Yu Lin, and Keh-Yih Su. 1992.
Statistical models for word segmentation
and unknown resolution. In Proceedings of
ROCLING-92, pages 121-146.
Church, Kenneth W. 1993. Char-align: A
program for aligning parallel texts at the
character level. In Proceedings of the 31st
Annual Meeting, pages 1-8, Columbus,
OH. Association for Computational
Linguistics.
Cranias, Lambros, Harris Papageorgiou, and
Stelios Peperidis. 1994. A matching
technique in example-based machine
translation. In Proceedings of the Fifteenth
International Conference on Computational
Linguistics, pages 100-104, Kyoto.
Dagan, Ido and Kenneth W. Church. 1994.
Termight: Identifying and translating
technical terminology. In Proceedings of the
Fourth Conference on Applied Natural
Language Processing, pages 34-40,
Stuttgart, October.
Dagan, Ido, Kenneth W. Church, and
William A. Gale. 1993. Robust bilingual
word alignment for machine aided
translation. In Proceedings of the Workshop
on Very Large Corpora, pages 1-8,
Columbus, OH, June.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. Communications of the
Association for Computing Machinery,
13(2):94-102.
Fung, Pascale and Kenneth W. Church.
1994. K-vec: A new approach for aligning
parallel texts. In Proceedings of the Fifteenth
International conference on Computational
Linguistics, pages 1096-1102, Kyoto.
Fung, Pascale and Kathleen McKeown.
1994. Aligning noisy parallel corpora
</reference>
<page confidence="0.994973">
401
</page>
<note confidence="0.69295">
Computational Linguistics Volume 23, Number 3
</note>
<reference confidence="0.9950515">
across language groups: Word pair
feature matching by dynamic time
warping. In AMTA-94, Association for
Machine Translation in the Americas, pages
81-88, Columbia, MD, October.
Fung, Pascale and Dekai Wu. 1994.
Statistical augmentation of a Chinese
machine-readable dictionary. In
Proceedings of the Second Annual Workshop
on Very Large Corpora, pages 69-85, Kyoto,
August.
Gale, William A. and Kenneth W. Church.
1991. A program for aligning sentences in
bilingual corpora. In Proceedings of the 29th
Annual Meeting, pages 177-184, Berkeley,
CA. Association for Computational
Linguistics.
Gale, William A., Kenneth W. Church, and
David Yarowsky. 1992. Using bilingual
materials to develop word sense
disambiguation methods. In TMI-92,
Proceedings of the Fourth International
Conference on Theoretical and Methodological
Issues in Machine Translation, pages
101-112, Montreal.
Gazdar, Gerald and Christopher S. Mellish.
1989. Natural Language Processing in LISP:
An Introduction to Computational Linguistics.
Addison-Wesley, Reading, MA.
Grishman, Ralph. 1994. Iterative alignment
of syntactic structures for a bilingual
corpus. In Proceedings of the Second Annual
Workshop on Very Large Corpora, pages
57-68, Kyoto, August.
Kaji, Hiroyuki, Yuuko Kida, and Yasutsugu
Morimoto. 1992. Learning translation
templates from bilingual text. In
Proceedings of the Fourteenth International
Conference on Computational Linguistics,
pages 672-678, Nantes.
Kaplan, Ronald M. and Martin Kay. 1994.
Regular models of phonological rule
systems. Computational Linguistics,
20(3):331-378.
Kasami, T. 1965. An efficient recognition
and syntax analysis algorithm for
context-free languages. Technical Report
AFCRL-65-758, Air Force Cambridge
Research Laboratory, Bedford, MA.
Kay, Martin and M. Roscheisen. 1988.
Text-translation alignment. Technical
Report P90-00143, Xerox Palo Alto
Research Center.
Koskenniemi, Kimmo. 1983. Two-level
morphology: A general computational
model for word-form recognition and
production. Technical Report 11,
Department of General Linguistics,
University of Helsinki.
Kupiec, Julian. 1993. An algorithm for
finding noun phrase correspondences in
bilingual corpora. In Proceedings of the 31st
Annual Meeting, pages 17-22, Columbus,
OH. Association for Computational
Linguistics.
Laporte, Eric. 1996. Context-free parsing
with finite-state transducers. In String
Processing Colloquium, Recife, Brazil.
Lewis, P. M. and R. E. Stearns. 1968.
Syntax-directed transduction. Journal of the
Association for Computing Machinery,
15:465-488.
Lin, Yi-Chung, Tung-Hui Chiang, and
Keh-Yih Su. 1992. discrimination oriented
probabilistic tagging. In Proceedings of
ROCLING-92, pages 85-96.
Lin, Ming-Yu, Tung-Hui Chiang, and
Keh-Yih Su. 1993. A preliminary study on
unknown word problem in chinese word
segmentation. In Proceedings of
ROCLING-93, pages 119-141.
Magerman, David M. and Mitchell
P. Marcus. 1990. Parsing a natural
language using mutual information
statistics. In Proceedings of AAAI-90, Eighth
National Conference on Artificial Intelligence,
pages 984-989.
Matsumoto, Yuji, Hiroyuki Ishimoto, and
Takehito Utsuro. 1993. Structural
matching of parallel texts. In Proceedings of
the 31st Annual Meeting, pages 23-30,
Columbus, OH. Association for
Computational Linguistics.
Nagao, Makoto. 1984. A framework of a
mechanical translation between Japanese
and English by analogy principle. In
Alick Elithorn and Ranan Banerji, editors,
Artifiical and Human Intelligence: Edited
Review Papers Presented at the International
NATO Symposium on Artificial and Human
Intelligence. North-Holland, Amsterdam,
pages 173-180.
Pereira, Fernando. 1991. Finite-state
approximation of phrase structure
grammars. In Proceedings of the 29th Annual
Meeting, Berkeley, CA. Association for
Computational Linguistics.
Pereira, Fernando and Yves Schabes. 1992.
Inside-outside reestimation from partially
bracketed corpora. In Proceedings of the
30th Annual Meeting, pages 128-135,
Newark, DE. Association for
Computational Linguistics.
Roche, Emmanuel. 1994. Two parsing
algorithms by means of finite-state
transducers. In Proceedings of the Fifteenth
International Conference on Computational
Linguistics, Kyoto.
Sadler, Victor and Ronald Vendelmans.
1990. Pilot implementation of a bilingual
knowledge bank. In Proceedings of the
Thirteenth International Conference on
</reference>
<page confidence="0.958149">
402
</page>
<reference confidence="0.989343963636363">
Wu Bilingual Parsing
Computational Linguistics, pages 449-451,
Helsinki.
Savitch, Walter J. 1982. Abstract Machines and
Grammars. Little, Brown, Boston, MA.
Smadja, Frank A. 1992. How to compile a
bilingual collocational lexicon
automatically. In AAAI-92 Workshop on
Statistically-Based NLP Techniques, pages
65-71, San Jose, CA, July.
Sproat, Richard, Chilin Shih, William Gale,
and Nancy Chang. 1994. A stochastic
word segmentation algorithm for a
Mandarin text-to-speech system. In
Proceedings of the 32nd Annual Meeting,
pages 66-72, Las Cruces, NM, June.
Association for Computational
Linguistics.
Viterbi, Andrew J. 1967. Error bounds for
convolutional codes and an
asymptotically optimal decoding
algorithm. IEEE Transactions on Information
Theory, 13:260-269.
Wu, Dekai. 1994. Aligning a parallel
English-Chinese corpus statistically with
lexical criteria. In Proceedings of the 32nd
Annual Meeting, pages 80-87, Las Cruces,
NM, June. Association for Computational
Linguistics.
Wu, Dekai. 1995. An algorithm for
simultaneously bracketing parallel texts
by aligning words. In Proceedings of the
33rd Annual Meeting, pages 244-251,
Cambridge, MA, June. Association for
Computational Linguistics.
Wu, Dekai and Pascale Fung. 1994.
Improving Chinese tokenization with
linguistic filters on statistical lexical
acquisition. In Proceedings of the Fourth
Conference on Applied Natural Language
Processing, pages 180-181, Stuttgart,
October.
Wu, Dekai and Xuanyin Xia. 1994. Learning
an English-Chinese lexicon from a
parallel corpus. In AMTA-94, Association
for Machine Translation in the Americas,
pages 206-213, Columbia, MD, October.
Wu, Zimin and Gwyneth Tseng. 1993.
Chinese text segmentation for text
retrieval: Achievements and problems.
Journal of The American Society for
Information Sciences, 44(9):532-542.
Younger, David H. 1967. Recognition and
parsing of context-free languages in time
n3. Information and Control, 10(2):189-208.
</reference>
<page confidence="0.999318">
403
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.275517">
<title confidence="0.998454666666667">Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora</title>
<author confidence="0.7816495">Dekai Wu Hong Kong University of Science</author>
<abstract confidence="0.947439461538462">Technology introduce (1) a novel inversion transduction formalism bilingual modeling of sentence-pairs, and (2) the concept of parsing a variety of parallel corpus analysis applications. Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm. A convenient normal form is shown to exist. Analysis of the formalism&apos;s expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints. We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Properties of syntax directed translations.</title>
<date>1969</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>3--3</pages>
<contexts>
<context position="6198" citStr="Aho and Ullman 1969" startWordPosition="893" endWordPosition="896">ngual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of ,the general class of context-free syntax-directed transduction grammars (Aho and Ullman 1969a, 1969b, 1972); however, we will avoid the term syntax-directed here, so as to de-emphasize the input-output connotation as discussed above. A simple transduction grammar can be written by marking every terminal symbol for a particular output stream. Thus, each rewrite rule emits not one but two streams. For example, a rewrite rule of the form A —&gt; Bxiy2Czi means that the terminal symbols x and z are symbols of the language L1 emitted on stream 1, while y is a symbol of 378 Wu Bilingual Parsing (a) S —, [SP Stop] SP --. [NP VP] I [NP VV] I [NP V] PP -4 [Prep NP] NP —) [Det NN] I [Det N] I [Pr</context>
<context position="16593" citStr="Aho and Ullman 1969" startWordPosition="2549" endWordPosition="2552">nolingual) strings generated by G for the first and second output languages are denoted L1(G) and L2(G), respectively. 3. A Normal Form for Inversion Transduction Grammars We now show that every ITG can be expressed as an equivalent ITG in a 2-normal form that simplifies algorithms and analyses on ITGs. In particular, the parsing algorithm of the next section operates on ITGs in normal form. The availability of a 2-normal 382 Wu Bilingual Parsing form is a noteworthy characteristic of ITGs; no such normal form is available for unrestricted context-free (syntax-directed) transduction grammars (Aho and Ullman 1969b). The proof closely follows that for standard CFGs, and the proofs of the lemmas are omitted. Lemma 1 For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G&apos; where T(G) = T(G&apos;), such that: 1. If c E Li (G) and c E L2(G), then G&apos; contains a single production of the form S&apos; c/e, where S&apos; is the start symbol of G&apos; and does not appear on the right-hand side of any production of G&apos;; 2. otherwise G&apos; contains no productions of the form A ele. Lemma 2 For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G&apos; </context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Aho, Alfred V. and Jeffrey D. Ullman. 1969a. Properties of syntax directed translations. Journal of Computer and System Sciences, 3(3):319-334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>3--1</pages>
<contexts>
<context position="6198" citStr="Aho and Ullman 1969" startWordPosition="893" endWordPosition="896">ngual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of ,the general class of context-free syntax-directed transduction grammars (Aho and Ullman 1969a, 1969b, 1972); however, we will avoid the term syntax-directed here, so as to de-emphasize the input-output connotation as discussed above. A simple transduction grammar can be written by marking every terminal symbol for a particular output stream. Thus, each rewrite rule emits not one but two streams. For example, a rewrite rule of the form A —&gt; Bxiy2Czi means that the terminal symbols x and z are symbols of the language L1 emitted on stream 1, while y is a symbol of 378 Wu Bilingual Parsing (a) S —, [SP Stop] SP --. [NP VP] I [NP VV] I [NP V] PP -4 [Prep NP] NP —) [Det NN] I [Det N] I [Pr</context>
<context position="16593" citStr="Aho and Ullman 1969" startWordPosition="2549" endWordPosition="2552">nolingual) strings generated by G for the first and second output languages are denoted L1(G) and L2(G), respectively. 3. A Normal Form for Inversion Transduction Grammars We now show that every ITG can be expressed as an equivalent ITG in a 2-normal form that simplifies algorithms and analyses on ITGs. In particular, the parsing algorithm of the next section operates on ITGs in normal form. The availability of a 2-normal 382 Wu Bilingual Parsing form is a noteworthy characteristic of ITGs; no such normal form is available for unrestricted context-free (syntax-directed) transduction grammars (Aho and Ullman 1969b). The proof closely follows that for standard CFGs, and the proofs of the lemmas are omitted. Lemma 1 For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G&apos; where T(G) = T(G&apos;), such that: 1. If c E Li (G) and c E L2(G), then G&apos; contains a single production of the form S&apos; c/e, where S&apos; is the start symbol of G&apos; and does not appear on the right-hand side of any production of G&apos;; 2. otherwise G&apos; contains no productions of the form A ele. Lemma 2 For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G&apos; </context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Aho, Alfred V. and Jeffrey D. Ullman. 1969b. Syntax directed translations and the pushdown assembler. Journal of Computer and System Sciences, 3(1):37-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, Alfred V. and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach. Editions Rodopi,</booktitle>
<editor>Black, Ezra, Roger Garside, and Geoffrey Leech, editors.</editor>
<location>Amsterdam.</location>
<marker>1993</marker>
<rawString>Black, Ezra, Roger Garside, and Geoffrey Leech, editors. 1993. Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach. Editions Rodopi, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Frederick jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="2590" citStr="Brown et al. 1990" startWordPosition="350" endWordPosition="353">hereby sidestepping the limitations of traditional transduction grammars. In tandem with the concept of bilingual language-modeling, we propose the concept of bilingual parsing, where the input is a sentence-pair rather than a sentence. Though inversion transduction grammars remain inadequate as full-fledged translation models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; * Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be </context>
</contexts>
<marker>Brown, Cocke, DellaPietra, DellaPietra, jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, Peter F., John Cocke, Stephen A. DellaPietra, Vincent J. DellaPietra, Frederick jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):29-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="2677" citStr="Brown et al. 1993" startWordPosition="365" endWordPosition="368">h the concept of bilingual language-modeling, we propose the concept of bilingual parsing, where the input is a sentence-pair rather than a sentence. Though inversion transduction grammars remain inadequate as full-fledged translation models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; * Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formalism&apos;s uniform inte</context>
<context position="55086" citStr="Brown et al. 1993" startWordPosition="8820" endWordPosition="8823">quent learning stages to acquire more interesting structures. Word alignment is difficult because correct matchings are not usually linearly ordered, i.e., there are crossings. Without some additional constraints, any word position in the source sentence can be matched to any position in the target sentence, an assumption that leads to high error rates. More sophisticated word alignment algorithms therefore attempt to model the intuition that proximate constituents in close relationships in one language remain proximate in the other. The later IBM models are formulated to prefer collocations (Brown et al. 1993). In the case of word_align (Dagan, Church, and Gale 1993; Dagan and Church 1994), a penalty is imposed according to the deviation from an ideal matching, as constructed by linear interpolation.&apos; From this point of view, the proposed technique is a word alignment method that imposes a more realistic distortion penalty. The tree structure reflects the assumption that crossings should not be penalized as long as they are consistent with constituent structure. Figure 7 gives theoretical upper bounds on the matching flexibility as the lengths of the sequences increase, where the constituent struct</context>
</contexts>
<marker>Brown, DellaPietra, DellaPietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<pages>169--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Berkeley, CA.</location>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>Brown, Peter F., Jennifer C. Lai, and Robert L. Mercer. 1991. Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Meeting, pages 169-176, Berkeley, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberta Catizone</author>
<author>Graham Russell</author>
<author>Susan Warwick</author>
</authors>
<title>Deriving translation data from bilingual texts.</title>
<date>1989</date>
<booktitle>In Proceedings of the First Lexical Acquisition Workshop,</booktitle>
<location>Detroit, MI.</location>
<marker>Catizone, Russell, Warwick, 1989</marker>
<rawString>Catizone, Roberta, Graham Russell, and Susan Warwick. 1989. Deriving translation data from bilingual texts. In Proceedings of the First Lexical Acquisition Workshop, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao-Huang Chang</author>
<author>Cheng-Der Chen</author>
</authors>
<title>HMM-based part-or-speech tagging for Chinese corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>40--47</pages>
<location>Columbus, OH,</location>
<marker>Chang, Cheng-Der Chen, 1993</marker>
<rawString>Chang, Chao-Huang and Cheng-Der Chen. 1993. HMM-based part-or-speech tagging for Chinese corpora. In Proceedings of the Workshop on Very Large Corpora, pages 40-47, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Aligning sentences in bilingual corpora using lexical information.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<contexts>
<context position="32827" citStr="Chen 1993" startWordPosition="5396" endWordPosition="5397">o it is up to the parser to decide when to break up potential collocations into individual words. The problem is particularly acute for English and Chinese because word boundaries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. (Sentences (2) and (5) demonstrate why the obvious trick of taking single characters as words is not a workable strategy.) The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omissions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentation in conjunction with the bracketing process. Note th</context>
<context position="48384" citStr="Chen 1993" startWordPosition="7801" endWordPosition="7802">) 8. Alignment 8.1 Phrasal Alignment Phrasal translation examples at the subsentential level are an essential resource for many MT and MAT architectures. This requirement is becoming increasingly direct for the example-based machine translation paradigm (Nagao 1984), whose translation flexibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, eit</context>
</contexts>
<marker>Chen, 1993</marker>
<rawString>Chen, Stanley F. 1993. Aligning sentences in bilingual corpora using lexical information. In Proceedings of the 31st Annual Meeting, pages 9-16, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tung-Hui Chiang</author>
<author>Jing-Shin Chang</author>
<author>Ming-Yu Lin</author>
<author>Keh-Yih Su</author>
</authors>
<title>Statistical models for word segmentation and unknown resolution.</title>
<date>1992</date>
<booktitle>In Proceedings of ROCLING-92,</booktitle>
<pages>121--146</pages>
<contexts>
<context position="32774" citStr="Chiang et al. 1992" startWordPosition="5384" endWordPosition="5387">ences do not come broken into appropriately matching chunks, so it is up to the parser to decide when to break up potential collocations into individual words. The problem is particularly acute for English and Chinese because word boundaries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. (Sentences (2) and (5) demonstrate why the obvious trick of taking single characters as words is not a workable strategy.) The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omissions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentatio</context>
</contexts>
<marker>Chiang, Chang, Lin, Su, 1992</marker>
<rawString>Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Lin, and Keh-Yih Su. 1992. Statistical models for word segmentation and unknown resolution. In Proceedings of ROCLING-92, pages 121-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Char-align: A program for aligning parallel texts at the character level.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<contexts>
<context position="2658" citStr="Church 1993" startWordPosition="363" endWordPosition="364">In tandem with the concept of bilingual language-modeling, we propose the concept of bilingual parsing, where the input is a sentence-pair rather than a sentence. Though inversion transduction grammars remain inadequate as full-fledged translation models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; * Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The forma</context>
<context position="48566" citStr="Church 1993" startWordPosition="7829" endWordPosition="7830">ng increasingly direct for the example-based machine translation paradigm (Nagao 1984), whose translation flexibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what we might c</context>
</contexts>
<marker>Church, 1993</marker>
<rawString>Church, Kenneth W. 1993. Char-align: A program for aligning parallel texts at the character level. In Proceedings of the 31st Annual Meeting, pages 1-8, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lambros Cranias</author>
<author>Harris Papageorgiou</author>
<author>Stelios Peperidis</author>
</authors>
<title>A matching technique in example-based machine translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>100--104</pages>
<location>Kyoto.</location>
<marker>Cranias, Papageorgiou, Peperidis, 1994</marker>
<rawString>Cranias, Lambros, Harris Papageorgiou, and Stelios Peperidis. 1994. A matching technique in example-based machine translation. In Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 100-104, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Kenneth W Church</author>
</authors>
<title>Termight: Identifying and translating technical terminology.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fourth Conference on Applied Natural Language Processing,</booktitle>
<pages>34--40</pages>
<location>Stuttgart,</location>
<contexts>
<context position="55167" citStr="Dagan and Church 1994" startWordPosition="8834" endWordPosition="8837">is difficult because correct matchings are not usually linearly ordered, i.e., there are crossings. Without some additional constraints, any word position in the source sentence can be matched to any position in the target sentence, an assumption that leads to high error rates. More sophisticated word alignment algorithms therefore attempt to model the intuition that proximate constituents in close relationships in one language remain proximate in the other. The later IBM models are formulated to prefer collocations (Brown et al. 1993). In the case of word_align (Dagan, Church, and Gale 1993; Dagan and Church 1994), a penalty is imposed according to the deviation from an ideal matching, as constructed by linear interpolation.&apos; From this point of view, the proposed technique is a word alignment method that imposes a more realistic distortion penalty. The tree structure reflects the assumption that crossings should not be penalized as long as they are consistent with constituent structure. Figure 7 gives theoretical upper bounds on the matching flexibility as the lengths of the sequences increase, where the constituent structure constraints are reflected by high flexibility up to length-4 sequences and a </context>
</contexts>
<marker>Dagan, Church, 1994</marker>
<rawString>Dagan, Ido and Kenneth W. Church. 1994. Termight: Identifying and translating technical terminology. In Proceedings of the Fourth Conference on Applied Natural Language Processing, pages 34-40, Stuttgart, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>Robust bilingual word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>1--8</pages>
<location>Columbus, OH,</location>
<marker>Dagan, Church, Gale, 1993</marker>
<rawString>Dagan, Ido, Kenneth W. Church, and William A. Gale. 1993. Robust bilingual word alignment for machine aided translation. In Proceedings of the Workshop on Very Large Corpora, pages 1-8, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<pages>13--2</pages>
<contexts>
<context position="60828" citStr="Earley 1970" startWordPosition="9699" endWordPosition="9700">es will never participate in any ML bibracketing. The running time reduction in this case depends heavily on the domain constraints. We have found this strategy to be useful for incorporating punctuation constraints. Certain punctuation characters give constituency indications with high reliability; &amp;quot;perfect separators&amp;quot; include colons and Chinese full stops, while &amp;quot;perfect delimiters&amp;quot; include parentheses and quotation marks. 10. Unrestricted-Form Grammars It is possible to construct a parser that accepts unrestricted-form, rather than normalform, grammars. In this case an Earley-style scheme (Earley 1970), employing an active chart, can be used. The time complexity remains the same as the normal-form case. We have found this to be useful in practice. For bracketing grammars of the type considered in this paper, there is no advantage. However, for more complex, linguistically structured grammars, the more flexible parser does not require the unreasonable numbers of productions that can easily arise from normal-form requirements. For most grammars, we have found performance to be comparable or faster than the normalform parser. 11. Conclusion The twin concepts of bilingual language modeling and </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2):94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kenneth W Church</author>
</authors>
<title>K-vec: A new approach for aligning parallel texts.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fifteenth International conference on Computational Linguistics,</booktitle>
<pages>1096--1102</pages>
<location>Kyoto.</location>
<contexts>
<context position="2948" citStr="Fung and Church 1994" startWordPosition="403" endWordPosition="406">mple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; * Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formalism&apos;s uniform integration of various types of bracketing and alignment constraints is one of its chief strengths. The paper is divided into two main parts. We begin in the first part below by laying out the basic formalism, then show that reduction to a normal form is possible. We then ra</context>
<context position="48624" citStr="Fung and Church 1994" startWordPosition="7837" endWordPosition="7840">ne translation paradigm (Nagao 1984), whose translation flexibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what we might call a &amp;quot;parse-parse-match&amp;quot; procedure. Each half of the para</context>
</contexts>
<marker>Fung, Church, 1994</marker>
<rawString>Fung, Pascale and Kenneth W. Church. 1994. K-vec: A new approach for aligning parallel texts. In Proceedings of the Fifteenth International conference on Computational Linguistics, pages 1096-1102, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aligning noisy parallel corpora across language groups: Word pair feature matching by dynamic time warping.</title>
<date>1994</date>
<booktitle>In AMTA-94, Association for Machine Translation in the Americas,</booktitle>
<pages>81--88</pages>
<location>Columbia, MD,</location>
<contexts>
<context position="2989" citStr="Fung and McKeown 1994" startWordPosition="411" endWordPosition="414">rns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; * Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formalism&apos;s uniform integration of various types of bracketing and alignment constraints is one of its chief strengths. The paper is divided into two main parts. We begin in the first part below by laying out the basic formalism, then show that reduction to a normal form is possible. We then raise several desiderata for the expressive</context>
<context position="48648" citStr="Fung and McKeown 1994" startWordPosition="7841" endWordPosition="7844">m (Nagao 1984), whose translation flexibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what we might call a &amp;quot;parse-parse-match&amp;quot; procedure. Each half of the parallel corpus is first par</context>
</contexts>
<marker>Fung, McKeown, 1994</marker>
<rawString>Fung, Pascale and Kathleen McKeown. 1994. Aligning noisy parallel corpora across language groups: Word pair feature matching by dynamic time warping. In AMTA-94, Association for Machine Translation in the Americas, pages 81-88, Columbia, MD, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Dekai Wu</author>
</authors>
<title>Statistical augmentation of a Chinese machine-readable dictionary.</title>
<date>1994</date>
<booktitle>In Proceedings of the Second Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<location>Kyoto,</location>
<contexts>
<context position="36895" citStr="Fung and Wu 1994" startWordPosition="6010" endWordPosition="6013">eting transduction grammar. Moreover, we also show how postprocessing using rotation and flattening operations restores the rank flexibility so that an output bracketing can hold more than two immediate constituents, as shown in Figure 11. The bu distribution actually encodes the English-Chinese translation lexicon with degrees of probability on each potential word translation. We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994). The latter stage gives us the probabilities directly. For the two singleton productions, which permit any word in either sentence to be unmatched, a small &amp;constant can be chosen for the probabilities b1, and b1, so that the optimal bracketing resorts to these productions only when it is 391 Computational Linguistics Volume 23, Number 3 otherwise impossible to match the singletons. The parameter a here is of no practical effect, and is chosen to be very small relative to the by probabilities of lexica</context>
</contexts>
<marker>Fung, Wu, 1994</marker>
<rawString>Fung, Pascale and Dekai Wu. 1994. Statistical augmentation of a Chinese machine-readable dictionary. In Proceedings of the Second Annual Workshop on Very Large Corpora, pages 69-85, Kyoto, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<pages>177--184</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="2612" citStr="Gale and Church 1991" startWordPosition="354" endWordPosition="357"> the limitations of traditional transduction grammars. In tandem with the concept of bilingual language-modeling, we propose the concept of bilingual parsing, where the input is a sentence-pair rather than a sentence. Though inversion transduction grammars remain inadequate as full-fledged translation models, bilingual parsing with simple inversion transduction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; * Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keepin</context>
<context position="48343" citStr="Gale and Church 1991" startWordPosition="7792" endWordPosition="7795">ng output examples. (&lt;&gt; = unrecognized input token.) 8. Alignment 8.1 Phrasal Alignment Phrasal translation examples at the subsentential level are an essential resource for many MT and MAT architectures. This requirement is becoming increasingly direct for the example-based machine translation paradigm (Nagao 1984), whose translation flexibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matchin</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>Gale, William A. and Kenneth W. Church. 1991. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th Annual Meeting, pages 177-184, Berkeley, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In TMI-92, Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>101--112</pages>
<location>Montreal.</location>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William A., Kenneth W. Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In TMI-92, Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 101-112, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Christopher S Mellish</author>
</authors>
<title>Natural Language Processing in LISP: An Introduction to Computational Linguistics.</title>
<date>1989</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="5443" citStr="Gazdar and Mellish 1989" startWordPosition="778" endWordPosition="781">guages are symmetrical, in contrast to the usual applications of syntax-directed transduction grammars. Moreover, the input-output view works better when a machine for accepting one of the languages (the input language) has a high degree of determinism, which is not the case here. Our transduction model is context-free, rather than finite-state. Finite-state transducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology (Koskenruiemi 1983), text-to-speech conversion (Kaplan and Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as wel</context>
</contexts>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>Gazdar, Gerald and Christopher S. Mellish. 1989. Natural Language Processing in LISP: An Introduction to Computational Linguistics. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Iterative alignment of syntactic structures for a bilingual corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the Second Annual Workshop on Very Large Corpora,</booktitle>
<pages>57--68</pages>
<location>Kyoto,</location>
<contexts>
<context position="20625" citStr="Grishman 1994" startWordPosition="3280" endWordPosition="3281"> discuss their relation to ITGs. Crossing Constraints. Arrangements where the matchings between subtrees cross each another are prohibited by crossing constraints, unless the subtrees&apos; immediate parent constituents are also matched to each other. For example, given the constituent matchings depicted as solid lines in Figure 4, the dotted-line matchings corresponding to potential lexical translations would be ruled illegal. Crossing constraints are implicit in many phrasal matching approaches, both constituency-oriented (Kaji, Kida, and Morimoto 1992; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994) and dependency-oriented (Sadler and Vendelmans 1990; Matsumoto, Ishimoto, and Utsuro 1993). The theoretical cross-linguistic hypothesis here is that the core arguments of frames tend to stay together over different languages. The constraint is also useful for computational reasons, since it helps avoid exponential bilingual matching times. ITGs inherently implement a crossing constraint; in fact, the version enforced by ITGs is even stronger. This is because even within a single constituent, immediate subtrees are only permitted to cross in exact inverted order. As we shall argue below, this </context>
<context position="49612" citStr="Grishman 1994" startWordPosition="7976" endWordPosition="7977">ototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what we might call a &amp;quot;parse-parse-match&amp;quot; procedure. Each half of the parallel corpus is first parsed individually using a monolingual grammar. Subsequently, the constituents of each sentence-pair are matched according to some heuristic procedure. A number of recent proposals can be cast in this framework (Sadler and Vendelmans 1990; Kaji, Kida, and Morimoto 1992; Matsumoto, Ishimoto, and Utsuro 1993; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994). The parse-parse-match procedure is susceptible to three weaknesses: • Appropriate, robust, monolingual grammars may not be available. This condition is particularly relevant for many non—Western European languages such as Chinese. A grammar for this purpose must be robust since it must still identify constituents for the subsequent matching process even for unanticipated or ill-formed input sentences. 396 Wu Bilingual Parsing • The grammars may be incompatible across languages. The best-matching constituent types between the two languages may not include the same core arguments. While gramma</context>
</contexts>
<marker>Grishman, 1994</marker>
<rawString>Grishman, Ralph. 1994. Iterative alignment of syntactic structures for a bilingual corpus. In Proceedings of the Second Annual Workshop on Very Large Corpora, pages 57-68, Kyoto, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Kaji</author>
<author>Yuuko Kida</author>
<author>Yasutsugu Morimoto</author>
</authors>
<title>Learning translation templates from bilingual text.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<pages>672--678</pages>
<location>Nantes.</location>
<marker>Kaji, Kida, Morimoto, 1992</marker>
<rawString>Kaji, Hiroyuki, Yuuko Kida, and Yasutsugu Morimoto. 1992. Learning translation templates from bilingual text. In Proceedings of the Fourteenth International Conference on Computational Linguistics, pages 672-678, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--3</pages>
<contexts>
<context position="5361" citStr="Kaplan and Kay 1994" startWordPosition="767" endWordPosition="770">view is more appropriate for our applications because the roles of the two languages are symmetrical, in contrast to the usual applications of syntax-directed transduction grammars. Moreover, the input-output view works better when a machine for accepting one of the languages (the input language) has a high degree of determinism, which is not the case here. Our transduction model is context-free, rather than finite-state. Finite-state transducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology (Koskenruiemi 1983), text-to-speech conversion (Kaplan and Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction gram</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Kaplan, Ronald M. and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-758,</tech>
<institution>Air Force Cambridge Research Laboratory,</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="27268" citStr="Kasami 1965" startWordPosition="4427" endWordPosition="4428">erminal symbol, as long as this introduces no confusion.) Then for every 1 &lt; i &lt; N, the production probabilities are subject to the constraint that E (aq+ai (jk)) b (x,y) = 1 j,k&lt;N We now introduce an algorithm for parsing with stochastic ITGs that computes an optimal parse given a sentence-pair using dynamic programming. In bilingual parsing, just as with ordinary monolingual parsing, probabilizing the grammar permits ambiguities to be resolved by choosing the maximum-likelihood parse. Our algorithm is similar in spirit to the recognition algorithm for HMMs (Viterbi 1967) and to CYK parsing (Kasami 1965; Younger 1967). Let the input English sentence be e1,. , eT and the corresponding input Chinese sentence be cl, , cv. As an abbreviation we write e, t for the sequence of words es+i,es+2, • • • , et, and similarly for cu v; also, es s = c is the empty string. It is convenient to use a 4-tuple of the form q = (s, t, u, v) to identify each node of the parse tree, where ITG all matchings ratio 1 1 2 2 3 6 4 22 5 90 6 394 7 1,806 8 8,558 9 41,586 10 206,098 11 1,037,718 12 5,293,446 13 27,297,738 14 142,078,746 15 745,387,038 16 3,937,603,038 1 1 2 6 24 120 720 5,040 40,320 362,880 3,628,800 39,9</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Kasami, T. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
<author>M Roscheisen</author>
</authors>
<title>Text-translation alignment.</title>
<date>1988</date>
<tech>Technical Report P90-00143,</tech>
<institution>Xerox Palo Alto Research Center.</institution>
<marker>Kay, Roscheisen, 1988</marker>
<rawString>Kay, Martin and M. Roscheisen. 1988. Text-translation alignment. Technical Report P90-00143, Xerox Palo Alto Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level morphology: A general computational model for word-form recognition and production.</title>
<date>1983</date>
<tech>Technical Report 11,</tech>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<marker>Koskenniemi, 1983</marker>
<rawString>Koskenniemi, Kimmo. 1983. Two-level morphology: A general computational model for word-form recognition and production. Technical Report 11, Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<pages>17--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<contexts>
<context position="48714" citStr="Kupiec 1993" startWordPosition="7851" endWordPosition="7852">examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what we might call a &amp;quot;parse-parse-match&amp;quot; procedure. Each half of the parallel corpus is first parsed individually using a monolingual grammar. Subsequently, the co</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Kupiec, Julian. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Meeting, pages 17-22, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Laporte</author>
</authors>
<title>Context-free parsing with finite-state transducers.</title>
<date>1996</date>
<booktitle>In String Processing Colloquium,</booktitle>
<location>Recife, Brazil.</location>
<contexts>
<context position="5560" citStr="Laporte 1996" startWordPosition="799" endWordPosition="800">ut view works better when a machine for accepting one of the languages (the input language) has a high degree of determinism, which is not the case here. Our transduction model is context-free, rather than finite-state. Finite-state transducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology (Koskenruiemi 1983), text-to-speech conversion (Kaplan and Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of ,the general class of context-free syntax-directed tran</context>
</contexts>
<marker>Laporte, 1996</marker>
<rawString>Laporte, Eric. 1996. Context-free parsing with finite-state transducers. In String Processing Colloquium, Recife, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Lewis</author>
<author>R E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>15--465</pages>
<contexts>
<context position="5989" citStr="Lewis and Stearns 1968" startWordPosition="864" endWordPosition="867"> nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of ,the general class of context-free syntax-directed transduction grammars (Aho and Ullman 1969a, 1969b, 1972); however, we will avoid the term syntax-directed here, so as to de-emphasize the input-output connotation as discussed above. A simple transduction grammar can be written by marking every terminal symbol for a particular output stream. Thus, each rewrite rule emits not one but two streams. For example, a rewrite rule of the form A —&gt; Bxiy2Czi means that the terminal symbol</context>
<context position="9884" citStr="Lewis and Stearns 1968" startWordPosition="1496" endWordPosition="1499">straint of simple transduction grammars. At the same time, any relaxation of constraints must be traded off against increases in the computational complexity of parsing, which may easily become exponential. The key is to make the relaxation relatively modest but still handle a wide range of ordering variations. The inversion transduction grammar (ITG) formalism only minimally extends the generative power of a simple transduction grammar, yet turns out to be surprisingly effective.1 Like simple transduction grammars, ITGs remain a subset of context-free (syntax-directed) transduction grammars (Lewis and Stearns 1968) but this view is too general to be of much help.&apos; The productions of an inversion transduction grammar are interpreted just as in a simple transduction grammar, except that two possible orientations are allowed. Pure simple transduction grammars have the implicit characteristic that for both output streams, the symbols generated by the right-hand-side constituents of a production are concatenated in the same left-to-right order. Inversion transduction grammars also allow such productions, which are said to have straight orientation. In addition, however, inversion transduction grammars allow </context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>Lewis, P. M. and R. E. Stearns. 1968. Syntax-directed transduction. Journal of the Association for Computing Machinery, 15:465-488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Chung Lin</author>
<author>Tung-Hui Chiang</author>
<author>Keh-Yih Su</author>
</authors>
<title>discrimination oriented probabilistic tagging.</title>
<date>1992</date>
<booktitle>In Proceedings of ROCLING-92,</booktitle>
<pages>85--96</pages>
<marker>Lin, Chiang, Su, 1992</marker>
<rawString>Lin, Yi-Chung, Tung-Hui Chiang, and Keh-Yih Su. 1992. discrimination oriented probabilistic tagging. In Proceedings of ROCLING-92, pages 85-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Yu Lin</author>
<author>Tung-Hui Chiang</author>
<author>Keh-Yih Su</author>
</authors>
<title>A preliminary study on unknown word problem in chinese word segmentation.</title>
<date>1993</date>
<booktitle>In Proceedings of ROCLING-93,</booktitle>
<pages>119--141</pages>
<marker>Lin, Chiang, Su, 1993</marker>
<rawString>Lin, Ming-Yu, Tung-Hui Chiang, and Keh-Yih Su. 1993. A preliminary study on unknown word problem in chinese word segmentation. In Proceedings of ROCLING-93, pages 119-141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Parsing a natural language using mutual information statistics.</title>
<date>1990</date>
<booktitle>In Proceedings of AAAI-90, Eighth National Conference on Artificial Intelligence,</booktitle>
<pages>984--989</pages>
<contexts>
<context position="35449" citStr="Magerman and Marcus 1990" startWordPosition="5791" endWordPosition="5795">ally when a fullcoverage grammar with which to parse a corpus is unavailable (for Chinese, an even more common situation than with English). Aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at constraining subsequent training of, for example, stochastic context-free grammars (Pereira and Schabes 1992; Black, Garside, and Leech 1993). Previous algorithms for automatic bracketing operate on monolingual texts and hence require more grammatical constraints; for example, tactics employing mutual information have been applied to tagged text (Magerman and Marcus 1990). Our method based on SITGs operates on the novel principle that lexical correspondences between parallel sentences yields information from which partial bracketings for both sentences can be extracted. The assumption that no grammar is available means that constituent categories are not differentiated. Instead, a generic bracketing transduction grammar is employed, containing only one nonterminal symbol, A, which rewrites either recursively as a pair of A&apos;s or as a single terminal-pair: a A [AA] a A (AA) A ui/vi for all i, j English-Chinese lexical translations A bi, for all i English vocabul</context>
<context position="46738" citStr="Magerman and Marcus (1990)" startWordPosition="7542" endWordPosition="7545"> under each criterion for correctness. Examples are shown in Figure 11. The bracket precision was 80% for the English sentences, and 78% for the Chinese sentences, as judged against manual bracketings. Inspection showed the errors to be due largely to imperfections of our translation lexicon, which contains approximately 6,500 English words and 5,500 Chinese words with about 86% translation accuracy (Wu and Xia 1994), so a better lexicon should yield substantial performance improvement. Moreover, if the resources for a good monolingual part-of-speech or grammar-based bracketer such as that of Magerman and Marcus (1990) are available, its output can readily be incorporated in complementary fashion as discussed in Section 9. 395 Computational Linguistics Volume 23, Number 3 [These/lait. arrangements/ M4P will/E E/TIJ enhance/UA our/RITI ([cill&apos;) ability/NE )3] [to/c c/19 lk maintain/a4 monetary/8 stability/ ff in the years to come/E]) ./. 1 [The / E Authority/V 4 N will/14 &apos;ft ( [be / c accountable/] [to the /€ € /relt Financial/ft&amp; Secretary/I-571) ./. ] [They/MI ( are/e right/EA c/-I-3). tole do/( Eatiff so/c) ./o ] [([ Even/e more/St important/IR ] [le however/ill ]) [If f /(fg, is/`1 to make the very best</context>
</contexts>
<marker>Magerman, Marcus, 1990</marker>
<rawString>Magerman, David M. and Mitchell P. Marcus. 1990. Parsing a natural language using mutual information statistics. In Proceedings of AAAI-90, Eighth National Conference on Artificial Intelligence, pages 984-989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Hiroyuki Ishimoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Structural matching of parallel texts.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<pages>23--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<marker>Matsumoto, Ishimoto, Utsuro, 1993</marker>
<rawString>Matsumoto, Yuji, Hiroyuki Ishimoto, and Takehito Utsuro. 1993. Structural matching of parallel texts. In Proceedings of the 31st Annual Meeting, pages 23-30, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
</authors>
<title>A framework of a mechanical translation between Japanese and English by analogy principle.</title>
<date>1984</date>
<booktitle>In Alick Elithorn and Ranan Banerji, editors, Artifiical and Human Intelligence: Edited Review Papers Presented at the International NATO Symposium on Artificial and Human Intelligence.</booktitle>
<pages>173--180</pages>
<publisher>North-Holland,</publisher>
<location>Amsterdam,</location>
<contexts>
<context position="48040" citStr="Nagao 1984" startWordPosition="7746" endWordPosition="7747">ake full/f c/5-j- A use/ fg [of/€ those/Ili -g.] (4e/Er31 who/A] [have acquired/e E/V-0.1 new/JI skills/RR ]) [through/&amp;114 this/201 programme/Pill]) ./o ] [I/ft have/E &lt;&gt; at/e length/:: ( on/f how/ frAl we/fill E/MI.) [can/JJ boost/e e/aie our/* 4 e/n prosperity/25k] ./ a ] Figure 11 Bracketing output examples. (&lt;&gt; = unrecognized input token.) 8. Alignment 8.1 Phrasal Alignment Phrasal translation examples at the subsentential level are an essential resource for many MT and MAT architectures. This requirement is becoming increasingly direct for the example-based machine translation paradigm (Nagao 1984), whose translation flexibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeo</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>Nagao, Makoto. 1984. A framework of a mechanical translation between Japanese and English by analogy principle. In Alick Elithorn and Ranan Banerji, editors, Artifiical and Human Intelligence: Edited Review Papers Presented at the International NATO Symposium on Artificial and Human Intelligence. North-Holland, Amsterdam, pages 173-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
</authors>
<title>Finite-state approximation of phrase structure grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="5533" citStr="Pereira 1991" startWordPosition="795" endWordPosition="796">. Moreover, the input-output view works better when a machine for accepting one of the languages (the input language) has a high degree of determinism, which is not the case here. Our transduction model is context-free, rather than finite-state. Finite-state transducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology (Koskenruiemi 1983), text-to-speech conversion (Kaplan and Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of ,the general class of contex</context>
</contexts>
<marker>Pereira, 1991</marker>
<rawString>Pereira, Fernando. 1991. Finite-state approximation of phrase structure grammars. In Proceedings of the 29th Annual Meeting, Berkeley, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Newark, DE.</location>
<contexts>
<context position="35183" citStr="Pereira and Schabes 1992" startWordPosition="5754" endWordPosition="5757">gical cases involving coordination constructions or lexicon coverage inadequacies. The method is also straightforward to employ in tandem with other applications, such as those below. 7. Bracketing Bracketing is another intermediate corpus annotation, useful especially when a fullcoverage grammar with which to parse a corpus is unavailable (for Chinese, an even more common situation than with English). Aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at constraining subsequent training of, for example, stochastic context-free grammars (Pereira and Schabes 1992; Black, Garside, and Leech 1993). Previous algorithms for automatic bracketing operate on monolingual texts and hence require more grammatical constraints; for example, tactics employing mutual information have been applied to tagged text (Magerman and Marcus 1990). Our method based on SITGs operates on the novel principle that lexical correspondences between parallel sentences yields information from which partial bracketings for both sentences can be extracted. The assumption that no grammar is available means that constituent categories are not differentiated. Instead, a generic bracketing</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Pereira, Fernando and Yves Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting, pages 128-135, Newark, DE. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
</authors>
<title>Two parsing algorithms by means of finite-state transducers.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics, Kyoto.</booktitle>
<contexts>
<context position="5545" citStr="Roche 1994" startWordPosition="797" endWordPosition="798">e input-output view works better when a machine for accepting one of the languages (the input language) has a high degree of determinism, which is not the case here. Our transduction model is context-free, rather than finite-state. Finite-state transducers, or FSTs, are well known to be useful for specific tasks such as analysis of inflectional morphology (Koskenruiemi 1983), text-to-speech conversion (Kaplan and Kay 1994), and nominal, number, and temporal phrase normalization (Gazdar and Mellish 1989). FSTs may also be used to parse restricted classes of context-free grammars (Pereira 1991; Roche 1994; Laporte 1996). However, the bilingual corpus analysis tasks we consider in this paper are quite different from the tasks for which FSTs are apparently well suited. Our domain is broader, and the model possesses very little a priori specific structural knowledge of the language. As a stepping stone to inversion transduction grammars, we first consider what a context-free model known as a simple transduction grammar (Lewis and Stearns 1968) would look like. Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of ,the general class of context-free synta</context>
</contexts>
<marker>Roche, 1994</marker>
<rawString>Roche, Emmanuel. 1994. Two parsing algorithms by means of finite-state transducers. In Proceedings of the Fifteenth International Conference on Computational Linguistics, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Sadler</author>
<author>Ronald Vendelmans</author>
</authors>
<title>Pilot implementation of a bilingual knowledge bank.</title>
<date>1990</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Wu Bilingual Parsing Computational Linguistics,</booktitle>
<pages>449--451</pages>
<location>Helsinki.</location>
<contexts>
<context position="20677" citStr="Sadler and Vendelmans 1990" startWordPosition="3284" endWordPosition="3287">g Constraints. Arrangements where the matchings between subtrees cross each another are prohibited by crossing constraints, unless the subtrees&apos; immediate parent constituents are also matched to each other. For example, given the constituent matchings depicted as solid lines in Figure 4, the dotted-line matchings corresponding to potential lexical translations would be ruled illegal. Crossing constraints are implicit in many phrasal matching approaches, both constituency-oriented (Kaji, Kida, and Morimoto 1992; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994) and dependency-oriented (Sadler and Vendelmans 1990; Matsumoto, Ishimoto, and Utsuro 1993). The theoretical cross-linguistic hypothesis here is that the core arguments of frames tend to stay together over different languages. The constraint is also useful for computational reasons, since it helps avoid exponential bilingual matching times. ITGs inherently implement a crossing constraint; in fact, the version enforced by ITGs is even stronger. This is because even within a single constituent, immediate subtrees are only permitted to cross in exact inverted order. As we shall argue below, this restriction reduces matching flexibility in a desira</context>
<context position="49484" citStr="Sadler and Vendelmans 1990" startWordPosition="7957" endWordPosition="7960">to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what we might call a &amp;quot;parse-parse-match&amp;quot; procedure. Each half of the parallel corpus is first parsed individually using a monolingual grammar. Subsequently, the constituents of each sentence-pair are matched according to some heuristic procedure. A number of recent proposals can be cast in this framework (Sadler and Vendelmans 1990; Kaji, Kida, and Morimoto 1992; Matsumoto, Ishimoto, and Utsuro 1993; Cranias, Papageorgiou, and Peperidis 1994; Grishman 1994). The parse-parse-match procedure is susceptible to three weaknesses: • Appropriate, robust, monolingual grammars may not be available. This condition is particularly relevant for many non—Western European languages such as Chinese. A grammar for this purpose must be robust since it must still identify constituents for the subsequent matching process even for unanticipated or ill-formed input sentences. 396 Wu Bilingual Parsing • The grammars may be incompatible acros</context>
</contexts>
<marker>Sadler, Vendelmans, 1990</marker>
<rawString>Sadler, Victor and Ronald Vendelmans. 1990. Pilot implementation of a bilingual knowledge bank. In Proceedings of the Thirteenth International Conference on Wu Bilingual Parsing Computational Linguistics, pages 449-451, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter J Savitch</author>
</authors>
<title>Abstract Machines and Grammars.</title>
<date>1982</date>
<location>Little, Brown, Boston, MA.</location>
<contexts>
<context position="11775" citStr="Savitch 1982" startWordPosition="1792" endWordPosition="1793">hin the parse tree. For example, if the inverted-orientation production of Figure 1(b) is added to the earlier simple transduction grammar, sentence-pair (3) can then be generated as follows: (4) a. [[[The Authority]Nrp [will Ube accountablelvv [to [the [[Financial Secretary1NN INNN INP lnr 1vr lvr lsr • Is b. [MIT in NI, iNti IP [RIM 1711lNN INNN INP lrr IAA ivy lvp lsr Is We can show the common structure of the two sentences more clearly and compactly with the aid of the () notation: 1 The expressiveness of simple transduction grammars is equivalent to nondeterministic pushdown transducers (Savitch 1982). 2 Also keep in mind that ITGs turn out to be especially suited for bilingual parsing applications, whereas pushdown transducers and syntax-directed transduction grammars are designed for monolingual parsing (in tandem with generation). 380 Wu Bilingual Parsing Figure 2 Inversion transduction grammar parse tree. (5) 111Theic Authority/Wit ffi, ]NP [Wi11/44ff ([be/c accountable/.]vv [to/PI 1theie [[Financial/1ft&amp; Secretary/f4iNN 1NNN IN!&apos; ]p, )vp bp ./. Is Alternatively, a graphical parse tree notation is shown in Figure 2, where the () level of bracketing is indicated by a horizontal line. Th</context>
</contexts>
<marker>Savitch, 1982</marker>
<rawString>Savitch, Walter J. 1982. Abstract Machines and Grammars. Little, Brown, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank A Smadja</author>
</authors>
<title>How to compile a bilingual collocational lexicon automatically.</title>
<date>1992</date>
<booktitle>In AAAI-92 Workshop on Statistically-Based NLP Techniques,</booktitle>
<pages>65--71</pages>
<location>San Jose, CA,</location>
<contexts>
<context position="48675" citStr="Smadja 1992" startWordPosition="7846" endWordPosition="7847">xibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications. Automatic approaches to identification of subsentential translation units have largely followed what we might call a &amp;quot;parse-parse-match&amp;quot; procedure. Each half of the parallel corpus is first parsed individually using a mo</context>
</contexts>
<marker>Smadja, 1992</marker>
<rawString>Smadja, Frank A. 1992. How to compile a bilingual collocational lexicon automatically. In AAAI-92 Workshop on Statistically-Based NLP Techniques, pages 65-71, San Jose, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic word segmentation algorithm for a Mandarin text-to-speech system.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting,</booktitle>
<pages>66--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Las Cruces, NM,</location>
<contexts>
<context position="32866" citStr="Sproat et al. 1994" startWordPosition="5402" endWordPosition="5405">cide when to break up potential collocations into individual words. The problem is particularly acute for English and Chinese because word boundaries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. (Sentences (2) and (5) demonstrate why the obvious trick of taking single characters as words is not a workable strategy.) The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omissions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentation in conjunction with the bracketing process. Note that the notion of a Chinese &amp;quot;word&amp;quot; is a </context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1994</marker>
<rawString>Sproat, Richard, Chilin Shih, William Gale, and Nancy Chang. 1994. A stochastic word segmentation algorithm for a Mandarin text-to-speech system. In Proceedings of the 32nd Annual Meeting, pages 66-72, Las Cruces, NM, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--260</pages>
<contexts>
<context position="27236" citStr="Viterbi 1967" startWordPosition="4421" endWordPosition="4422">hen we mean the corresponding nonterminal symbol, as long as this introduces no confusion.) Then for every 1 &lt; i &lt; N, the production probabilities are subject to the constraint that E (aq+ai (jk)) b (x,y) = 1 j,k&lt;N We now introduce an algorithm for parsing with stochastic ITGs that computes an optimal parse given a sentence-pair using dynamic programming. In bilingual parsing, just as with ordinary monolingual parsing, probabilizing the grammar permits ambiguities to be resolved by choosing the maximum-likelihood parse. Our algorithm is similar in spirit to the recognition algorithm for HMMs (Viterbi 1967) and to CYK parsing (Kasami 1965; Younger 1967). Let the input English sentence be e1,. , eT and the corresponding input Chinese sentence be cl, , cv. As an abbreviation we write e, t for the sequence of words es+i,es+2, • • • , et, and similarly for cu v; also, es s = c is the empty string. It is convenient to use a 4-tuple of the form q = (s, t, u, v) to identify each node of the parse tree, where ITG all matchings ratio 1 1 2 2 3 6 4 22 5 90 6 394 7 1,806 8 8,558 9 41,586 10 206,098 11 1,037,718 12 5,293,446 13 27,297,738 14 142,078,746 15 745,387,038 16 3,937,603,038 1 1 2 6 24 120 720 5,0</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi, Andrew J. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13:260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Aligning a parallel English-Chinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting,</booktitle>
<pages>80--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Las Cruces, NM,</location>
<contexts>
<context position="36821" citStr="Wu 1994" startWordPosition="6001" endWordPosition="6002"> in normal form is generatively equivalent to any reasonable bracketing transduction grammar. Moreover, we also show how postprocessing using rotation and flattening operations restores the rank flexibility so that an output bracketing can hold more than two immediate constituents, as shown in Figure 11. The bu distribution actually encodes the English-Chinese translation lexicon with degrees of probability on each potential word translation. We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994). The latter stage gives us the probabilities directly. For the two singleton productions, which permit any word in either sentence to be unmatched, a small &amp;constant can be chosen for the probabilities b1, and b1, so that the optimal bracketing resorts to these productions only when it is 391 Computational Linguistics Volume 23, Number 3 otherwise impossible to match the singletons. The parameter a here is of no practical effect,</context>
<context position="48450" citStr="Wu 1994" startWordPosition="7813" endWordPosition="7814"> the subsentential level are an essential resource for many MT and MAT architectures. This requirement is becoming increasingly direct for the example-based machine translation paradigm (Nagao 1984), whose translation flexibility is strongly restricted if the examples are only at the sentential level. It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy (Kay and Rocheisen 1988; Catizone, Russel, and Warwick 1989; Gale and Church 1991; Brown, Lai, and Mercer 1991; Chen 1993), even for languages as disparate as Chinese and English (Wu 1994). Algorithms for subsentential alignment have been developed as well as granularities of the character (Church 1993), word (Dagan, Church, and Gale 1993; Fung and Church 1994; Fung and McKeown 1994), collocation (Smadja 1992), and specially segmented (Kupiec 1993) levels. However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure. Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applicati</context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>Wu, Dekai. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In Proceedings of the 32nd Annual Meeting, pages 80-87, Las Cruces, NM, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts by aligning words.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting,</booktitle>
<pages>244--251</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="15096" citStr="Wu 1995" startWordPosition="2293" endWordPosition="2294">es of the two languages fortuitously parallel each other (though again the bracketing would be linguistically implausible). We will return to the subject of ITGs&apos; ordering flexibility in Section 4. We stress again that the primary purpose of ITGs is to maximize robustness for parallel corpus analysis rather than to verify grammaticality, and therefore writing grammars is made much easier since the grammars can be minimal and very leaky. We consider elsewhere an extreme special case of leaky ITGs, inversion-invariant transduction grammars, in which all productions occur with both orientations (Wu 1995). As the applications below demonstrate, the bilingual lexical constraints carry greater importance than the tightness of the grammar. Formally, an inversion transduction grammar, or ITG, is denoted by G = (Al, Wi, W2, S), where .AT is a finite set of nonterminals, Wi is a finite set of words (terminals) of language 1, W2 is a finite set of words (terminals) of language 2, R is a finite set of rewrite rules (productions), and S E Ai is the start symbol. The space of word-pairs (terminal-pairs) X = (WI U {E}) x (W2 U {e}) contains lexical translations denoted x/y and singletons denoted x/€ or c</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Wu, Dekai. 1995. An algorithm for simultaneously bracketing parallel texts by aligning words. In Proceedings of the 33rd Annual Meeting, pages 244-251, Cambridge, MA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Improving Chinese tokenization with linguistic filters on statistical lexical acquisition.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fourth Conference on Applied Natural Language Processing,</booktitle>
<pages>180--181</pages>
<location>Stuttgart,</location>
<contexts>
<context position="32885" citStr="Wu and Fung 1994" startWordPosition="5406" endWordPosition="5409">p potential collocations into individual words. The problem is particularly acute for English and Chinese because word boundaries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. (Sentences (2) and (5) demonstrate why the obvious trick of taking single characters as words is not a workable strategy.) The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omissions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentation in conjunction with the bracketing process. Note that the notion of a Chinese &amp;quot;word&amp;quot; is a longstanding lingui</context>
<context position="36914" citStr="Wu and Fung 1994" startWordPosition="6014" endWordPosition="6017"> grammar. Moreover, we also show how postprocessing using rotation and flattening operations restores the rank flexibility so that an output bracketing can hold more than two immediate constituents, as shown in Figure 11. The bu distribution actually encodes the English-Chinese translation lexicon with degrees of probability on each potential word translation. We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994). The latter stage gives us the probabilities directly. For the two singleton productions, which permit any word in either sentence to be unmatched, a small &amp;constant can be chosen for the probabilities b1, and b1, so that the optimal bracketing resorts to these productions only when it is 391 Computational Linguistics Volume 23, Number 3 otherwise impossible to match the singletons. The parameter a here is of no practical effect, and is chosen to be very small relative to the by probabilities of lexical translation pairs</context>
</contexts>
<marker>Wu, Fung, 1994</marker>
<rawString>Wu, Dekai and Pascale Fung. 1994. Improving Chinese tokenization with linguistic filters on statistical lexical acquisition. In Proceedings of the Fourth Conference on Applied Natural Language Processing, pages 180-181, Stuttgart, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Xuanyin Xia</author>
</authors>
<title>Learning an English-Chinese lexicon from a parallel corpus.</title>
<date>1994</date>
<booktitle>In AMTA-94, Association for Machine Translation in the Americas,</booktitle>
<pages>206--213</pages>
<location>Columbia, MD,</location>
<contexts>
<context position="2965" citStr="Wu and Xia 1994" startWordPosition="407" endWordPosition="410">ction grammars turns out to be very useful for parallel corpus analysis when the true grammar is not fully known. Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; * Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong. E-mail: dekai@cs.ust.hk © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Fung and Church 1994; Wu and Xia 1994; Fung and McKeown 1994). The primary purpose of bilingual parsing with inversion transduction grammars is not to flag ungrammatical inputs; rather, the aim is to extract structure from the input data, which is assumed to be grammatical, in keeping with the spirit of robust parsing. The formalism&apos;s uniform integration of various types of bracketing and alignment constraints is one of its chief strengths. The paper is divided into two main parts. We begin in the first part below by laying out the basic formalism, then show that reduction to a normal form is possible. We then raise several desid</context>
<context position="36987" citStr="Wu and Xia 1994" startWordPosition="6024" endWordPosition="6027">ttening operations restores the rank flexibility so that an output bracketing can hold more than two immediate constituents, as shown in Figure 11. The bu distribution actually encodes the English-Chinese translation lexicon with degrees of probability on each potential word translation. We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994). The latter stage gives us the probabilities directly. For the two singleton productions, which permit any word in either sentence to be unmatched, a small &amp;constant can be chosen for the probabilities b1, and b1, so that the optimal bracketing resorts to these productions only when it is 391 Computational Linguistics Volume 23, Number 3 otherwise impossible to match the singletons. The parameter a here is of no practical effect, and is chosen to be very small relative to the by probabilities of lexical translation pairs. The result is that the maximum-likelihood parser selects the parse tree</context>
<context position="46532" citStr="Wu and Xia 1994" startWordPosition="7513" endWordPosition="7516">lgorithm no discriminative leverage; such pairs accounted for less than 2% of the input data. A random sample of the bracketed sentence-pairs was then drawn, and the bracket precision was computed under each criterion for correctness. Examples are shown in Figure 11. The bracket precision was 80% for the English sentences, and 78% for the Chinese sentences, as judged against manual bracketings. Inspection showed the errors to be due largely to imperfections of our translation lexicon, which contains approximately 6,500 English words and 5,500 Chinese words with about 86% translation accuracy (Wu and Xia 1994), so a better lexicon should yield substantial performance improvement. Moreover, if the resources for a good monolingual part-of-speech or grammar-based bracketer such as that of Magerman and Marcus (1990) are available, its output can readily be incorporated in complementary fashion as discussed in Section 9. 395 Computational Linguistics Volume 23, Number 3 [These/lait. arrangements/ M4P will/E E/TIJ enhance/UA our/RITI ([cill&apos;) ability/NE )3] [to/c c/19 lk maintain/a4 monetary/8 stability/ ff in the years to come/E]) ./. 1 [The / E Authority/V 4 N will/14 &apos;ft ( [be / c accountable/] [to th</context>
</contexts>
<marker>Wu, Xia, 1994</marker>
<rawString>Wu, Dekai and Xuanyin Xia. 1994. Learning an English-Chinese lexicon from a parallel corpus. In AMTA-94, Association for Machine Translation in the Americas, pages 206-213, Columbia, MD, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>Journal of The American Society for Information Sciences,</journal>
<pages>44--9</pages>
<contexts>
<context position="32846" citStr="Wu and Tseng 1993" startWordPosition="5398" endWordPosition="5401">to the parser to decide when to break up potential collocations into individual words. The problem is particularly acute for English and Chinese because word boundaries are not orthographically marked in Chinese text, so not even a default chunking exists upon which word matchings could be postulated. (Sentences (2) and (5) demonstrate why the obvious trick of taking single characters as words is not a workable strategy.) The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al. 1994; Wu and Fung 1994), but, clearly, bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the Chinese segmentation is acceptable monolingually, it may not agree with the words present in the English sentence. Matters are made still worse by unpredictable omissions in the translation lexicon, even for valid compounds. We therefore extend the algorithm to optimize the Chinese sentence segmentation in conjunction with the bracketing process. Note that the notion of a </context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Wu, Zimin and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. Journal of The American Society for Information Sciences, 44(9):532-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages</title>
<date>1967</date>
<booktitle>in time n3. Information and Control,</booktitle>
<pages>10--2</pages>
<contexts>
<context position="27283" citStr="Younger 1967" startWordPosition="4429" endWordPosition="4430">l, as long as this introduces no confusion.) Then for every 1 &lt; i &lt; N, the production probabilities are subject to the constraint that E (aq+ai (jk)) b (x,y) = 1 j,k&lt;N We now introduce an algorithm for parsing with stochastic ITGs that computes an optimal parse given a sentence-pair using dynamic programming. In bilingual parsing, just as with ordinary monolingual parsing, probabilizing the grammar permits ambiguities to be resolved by choosing the maximum-likelihood parse. Our algorithm is similar in spirit to the recognition algorithm for HMMs (Viterbi 1967) and to CYK parsing (Kasami 1965; Younger 1967). Let the input English sentence be e1,. , eT and the corresponding input Chinese sentence be cl, , cv. As an abbreviation we write e, t for the sequence of words es+i,es+2, • • • , et, and similarly for cu v; also, es s = c is the empty string. It is convenient to use a 4-tuple of the form q = (s, t, u, v) to identify each node of the parse tree, where ITG all matchings ratio 1 1 2 2 3 6 4 22 5 90 6 394 7 1,806 8 8,558 9 41,586 10 206,098 11 1,037,718 12 5,293,446 13 27,297,738 14 142,078,746 15 745,387,038 16 3,937,603,038 1 1 2 6 24 120 720 5,040 40,320 362,880 3,628,800 39,916,800 479,001,</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Younger, David H. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>