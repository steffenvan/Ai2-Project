<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.991992">
Understanding the Value of Features for Coreference Resolution
</title>
<author confidence="0.998962">
Eric Bengtson Dan Roth
</author>
<affiliation confidence="0.999276">
Department of Computer Science
University of Illinois
</affiliation>
<address confidence="0.796055">
Urbana, IL 61801
</address>
<email confidence="0.999528">
{ebengt2,danr}@illinois.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999795818181818">
In recent years there has been substantial work
on the important problem of coreference res-
olution, most of which has concentrated on
the development of new models and algo-
rithmic techniques. These works often show
that complex models improve over a weak
pairwise baseline. However, less attention
has been given to the importance of selecting
strong features to support learning a corefer-
ence model.
This paper describes a rather simple pair-
wise classification model for coreference res-
olution, developed with a well-designed set
of features. We show that this produces a
state-of-the-art system that outperforms sys-
tems built with complex models. We suggest
that our system can be used as a baseline for
the development of more complex models –
which may have less impact when a more ro-
bust set of features is used. The paper also
presents an ablation study and discusses the
relative contributions of various features.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998885166666667">
Coreference resolution is the task of grouping all the
mentions of entities1 in a document into equivalence
classes so that all the mentions in a given class refer
to the same discourse entity. For example, given the
sentence (where the head noun of each mention is
subscripted)
</bodyText>
<footnote confidence="0.670219666666667">
1We follow the ACE (NIST, 2004) terminology: A noun
phrase referring to a discourse entity is called a mention, and
an equivalence class is called an entity.
</footnote>
<note confidence="0.59875925">
An American1 official2 announced that
American1 President3 Bill Clinton3 met
his3 Russian4 counterpart5, Vladimir
Putin5, today.
</note>
<bodyText confidence="0.999838866666667">
the task is to group the mentions so that those refer-
ring to the same entity are placed together into an
equivalence class.
Many NLP tasks detect attributes, actions, and
relations between discourse entities. In order to
discover all information about a given entity, tex-
tual mentions of that entity must be grouped to-
gether. Thus coreference is an important prerequi-
site to such tasks as textual entailment and informa-
tion extraction, among others.
Although coreference resolution has received
much attention, that attention has not focused on the
relative impact of high-quality features. Thus, while
many structural innovations in the modeling ap-
proach have been made, those innovations have gen-
erally been tested on systems with features whose
strength has not been established, and compared to
weak pairwise baselines. As a result, it is possible
that some modeling innovations may have less im-
pact or applicability when applied to a stronger base-
line system.
This paper introduces a rather simple but state-
of-the-art system, which we intend to be used as a
strong baseline to evaluate the impact of structural
innovations. To this end, we combine an effective
coreference classification model with a strong set of
features, and present an ablation study to show the
relative impact of a variety of features.
As we show, this combination of a pairwise
model and strong features produces a 1.5 percent-
</bodyText>
<page confidence="0.972443">
294
</page>
<note confidence="0.962047">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 294–303,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9987372">
age point increase in B-Cubed F-Score over a com-
plex model in the state-of-the-art system by Culotta
et al. (2007), although their system uses a complex,
non-pairwise model, computing features over partial
clusters of mentions.
</bodyText>
<sectionHeader confidence="0.984014" genericHeader="introduction">
2 A Pairwise Coreference Model
</sectionHeader>
<bodyText confidence="0.99998676">
Given a document and a set of mentions, corefer-
ence resolution is the task of grouping the mentions
into equivalence classes, so that each equivalence
class contains exactly those mentions that refer to
the same discourse entity. The number of equiv-
alence classes is not specified in advance, but is
bounded by the number of mentions.
In this paper, we view coreference resolution as
a graph problem: Given a set of mentions and their
context as nodes, generate a set of edges such that
any two mentions that belong in the same equiva-
lence class are connected by some path in the graph.
We construct this entity-mention graph by learning
to decide for each mention which preceding men-
tion, if any, belongs in the same equivalence class;
this approach is commonly called the pairwise coref-
erence model (Soon et al., 2001). To decide whether
two mentions should be linked in the graph, we learn
a pairwise coreference function pc that produces a
value indicating the probability that the two men-
tions should be placed in the same equivalence class.
The remainder of this section first discusses how
this function is used as part of a document-level
coreference decision model and then describes how
we learn the pc function.
</bodyText>
<subsectionHeader confidence="0.956125">
2.1 Document-Level Decision Model
</subsectionHeader>
<bodyText confidence="0.998188222222222">
Given a document d and a pairwise coreference scor-
ing function pc that maps an ordered pair of men-
tions to a value indicating the probability that they
are coreferential (see Section 2.2), we generate a
coreference graph Gd according to the Best-Link de-
cision model (Ng and Cardie, 2002b) as follows:
For each mention m in document d, let B,,t be the
set of mentions appearing before m in d. Let a be
the highest scoring antecedent:
</bodyText>
<equation confidence="0.875562666666667">
a = argmax (pc(b, m)).
bEB,
If pc(a, m) is above a threshold chosen as described
</equation>
<bodyText confidence="0.999146076923077">
in Section 4.4, we add the edge (a, m) to the coref-
erence graph Gd.
The resulting graph contains connected compo-
nents, each representing one equivalence class, with
all the mentions in the component referring to the
same entity. This technique permits us to learn to
detect some links between mentions while being ag-
nostic about whether other mentions are linked, and
yet via the transitive closure of all links we can still
determine the equivalence classes.
We also require that no non-pronoun can refer
back to a pronoun: If m is not a pronoun, we do
not consider pronouns as candidate antecedents.
</bodyText>
<sectionHeader confidence="0.811579" genericHeader="method">
2.1.1 Related Models
</sectionHeader>
<bodyText confidence="0.9999822">
For pairwise models, it is common to choose the
best antecedent for a given mention (thereby impos-
ing the constraint that each mention has at most one
antecedent); however, the method of deciding which
is the best antecedent varies.
Soon et al. (2001) use the Closest-Link method:
They select as an antecedent the closest preced-
ing mention that is predicted coreferential by a
pairwise coreference module; this is equivalent to
choosing the closest mention whose pc value is
above a threshold. Best-Link was shown to out-
perform Closest-Link in an experiment by Ng and
Cardie (2002b). Our model differs from that of Ng
and Cardie in that we impose the constraint that
non-pronouns cannot refer back to pronouns, and in
that we use as training examples all ordered pairs of
mentions, subject to the constraint above.
Culotta et al. (2007) introduced a model that pre-
dicts whether a pair of equivalence classes should be
merged, using features computed over all the men-
tions in both classes. Since the number of possi-
ble classes is exponential in the number of mentions,
they use heuristics to select training examples. Our
method does not require determining which equiva-
lence classes should be considered as examples.
</bodyText>
<subsectionHeader confidence="0.998002">
2.2 Pairwise Coreference Function
</subsectionHeader>
<bodyText confidence="0.985317666666667">
Learning the pairwise scoring function pc is a cru-
cial issue for the pairwise coreference model. We
apply machine learning techniques to learn from ex-
amples a function pc that takes as input an ordered
pair of mentions (a, m) such that a precedes m in
the document, and produces as output a value that is
</bodyText>
<page confidence="0.996808">
295
</page>
<bodyText confidence="0.997837">
interpreted as the conditional probability that m and
a belong in the same equivalence class.
</bodyText>
<subsectionHeader confidence="0.952604">
2.2.1 Training Example Selection
</subsectionHeader>
<bodyText confidence="0.99999025">
The ACE training data provides the equivalence
classes for mentions. However, for some pairs of
mentions from an equivalence class, there is little or
no direct evidence in the text that the mentions are
coreferential. Therefore, training pc on all pairs of
mentions within an equivalence class may not lead
to a good predictor. Thus, for each mention m we
select from m’s equivalence class the closest pre-
ceding mention a and present the pair (a, m) as a
positive training example, under the assumption that
there is more direct evidence in the text for the ex-
istence of this edge than for other edges. This is
similar to the technique of Ng and Cardie (2002b).
For each m, we generate negative examples (a, m)
for all mentions a that precede m and are not in the
same equivalence class. Note that in doing so we
generate more negative examples than positive ones.
Since we never apply pc to a pair where the first
mention is a pronoun and the second is not a pro-
noun, we do not train on examples of this form.
</bodyText>
<subsectionHeader confidence="0.954856">
2.2.2 Learning Pairwise Coreference Scoring
Model
</subsectionHeader>
<bodyText confidence="0.99994375">
We learn the pairwise coreference function using
an averaged perceptron learning algorithm (Freund
and Schapire, 1998) – we use the regularized version
in Learning Based Java2 (Rizzolo and Roth, 2007).
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.9998182">
The performance of the document-level coreference
model depends on the quality of the pairwise coref-
erence function pc. Beyond the training paradigm
described earlier, the quality of pc depends on the
features used.
We divide the features into categories, based on
their function. A full list of features and their cat-
egories is given in Table 2. In addition to these
boolean features, we also use the conjunctions of all
pairs of features.3
</bodyText>
<footnote confidence="0.9704334">
2LBJ code is available at http://L2R.cs.uiuc.edu/
-cogcomp/asoftware.php?skey=LBJ
3The package of all features used is available at
http://L2R.cs.uiuc.edu/-cogcomp/asoftware.
php?skey=LBJ#features.
</footnote>
<bodyText confidence="0.999955666666667">
In the following description, the term head means
the head noun phrase of a mention; the extent is the
largest noun phrase headed by the head noun phrase.
</bodyText>
<subsectionHeader confidence="0.99974">
3.1 Mention Types
</subsectionHeader>
<bodyText confidence="0.999972916666667">
The type of a mention indicates whether it is a proper
noun, a common noun, or a pronoun. This feature,
when conjoined with others, allows us to give dif-
ferent weight to a feature depending on whether it is
being applied to a proper name or a pronoun. For
our experiments in Section 5, we use gold mention
types as is done by Culotta et al. (2007) and Luo and
Zitouni (2005).
Note that in the experiments described in Sec-
tion 6 we predict the mention types as described
there and do not use any gold data. The mention
type feature is used in all experiments.
</bodyText>
<subsectionHeader confidence="0.999801">
3.2 String Relation Features
</subsectionHeader>
<bodyText confidence="0.978736666666667">
String relation features indicate whether two strings
share some property, such as one being the substring
of another or both sharing a modifier word. Features
are listed in Table 1. Modifiers are limited to those
occurring before the head.
Feature Definition
</bodyText>
<equation confidence="0.905671">
headi == headj
extenti == extentj
</equation>
<bodyText confidence="0.30552625">
headi substring of headj
modi == (headj or modj)
Alias acronym(headi) == headj
or lastnamei == lastnamej
</bodyText>
<tableCaption confidence="0.996069">
Table 1: String Relation Features
</tableCaption>
<subsectionHeader confidence="0.999345">
3.3 Semantic Features
</subsectionHeader>
<bodyText confidence="0.892314357142857">
Another class of features captures the semantic re-
lation between two words. Specifically, we check
whether gender or number match, or whether the
mentions are synonyms, antonyms, or hypernyms.
We also check the relationship of modifiers that
share a hypernym. Descriptions of the methods for
computing these features are described next.
Gender Match We determine the gender (male,
female, or neuter) of the two phrases, and report
whether they match (true, false, or unknown). For
Head match
Extent match
Substring
Modifiers Match
</bodyText>
<page confidence="0.987269">
296
</page>
<table confidence="0.999821954545455">
Category Feature Source
Mention Types Mention Type Pair Annotation and tokens
String Relations Head Match Tokens
Extent Match Tokens
Substring Tokens
Modifiers Match Tokens
Alias Tokens and lists
Semantic Gender Match WordNet and lists
Number Match WordNet and lists
Synonyms WordNet
Antonyms WordNet
Hypernyms WordNet
Both Speak Context
Relative Location Apposition Positions and context
Relative Pronoun Positions and tokens
Distances Positions
Learned Anaphoricity Learned
Name Modifiers Predicted Match Learned
Aligned Modifiers Aligned Modifiers Relation WordNet and lists
Memorization Last Words Tokens
Predicted Entity Types Entity Types Match Annotation and tokens
Entity Type Pair WordNet and tokens
</table>
<tableCaption confidence="0.98438">
Table 2: Features by Category
</tableCaption>
<bodyText confidence="0.999774783783784">
a proper name, gender is determined by the exis-
tence of mr, ms, mrs, or the gender of the first name.
If only a last name is found, the phrase is consid-
ered to refer to a person. If the name is found in
a comprehensive list of cities or countries, or ends
with an organization ending such as inc, then the
gender is neuter. In the case of a common noun
phrase, the phrase is looked up in WordNet (Fell-
baum, 1998), and it is assigned a gender according to
whether male, female, person, artifact, location, or
group (the last three correspond to neuter) is found
in the hypernym tree. The gender of a pronoun is
looked up in a table.
Number Match Number is determined as fol-
lows: Phrases starting with the words a, an, or this
are singular; those, these, or some indicate plural.
Names not containing and are singular. Common
nouns are checked against extensive lists of singular
and plural nouns – words found in neither or both
lists have unknown number. Finally, if the num-
ber is unknown yet the two mentions have the same
spelling, they are assumed to have the same number.
WordNet Features We check whether any sense
of one head noun phrase is a synonym, antonym, or
hypernym of any sense of the other. We also check
whether any sense of the phrases share a hypernym,
after dropping entity, abstraction, physical entity,
object, whole, artifact, and group from the senses,
since they are close to the root of the hypernym tree.
Modifiers Match Determines whether the text be-
fore the head of a mention matches the head or the
text before the head of the other mention.
Both Mentions Speak True if both mentions ap-
pear within two words of a verb meaning to say. Be-
ing in a window of size two is an approximation to
being a syntactic subject of such a verb. This feature
is a proxy for having similar semantic types.
</bodyText>
<subsectionHeader confidence="0.767954">
3.4 Relative Location Features
</subsectionHeader>
<bodyText confidence="0.998349333333333">
Additional evidence is derived from the relative lo-
cation of the two mentions. We thus measure dis-
tance (quantized as multiple boolean features of the
</bodyText>
<page confidence="0.982844">
297
</page>
<bodyText confidence="0.9986682">
form [distance &gt; i]) for all i up to the distance and
less than some maximum, using units of compatible
mentions, and whether the mentions are in the same
sentence. We also detect apposition (mentions sepa-
rated by a comma). For details, see Table 3.
</bodyText>
<table confidence="0.781108">
Feature Definition
Distance In same sentence
# compatible mentions
</table>
<subsectionHeader confidence="0.929891">
3.6 Aligned Modifiers
</subsectionHeader>
<bodyText confidence="0.999974142857143">
We determine the relationship of any pair of modi-
fiers that share a hypernym. Each aligned pair may
have one of the following relations: match, sub-
string, synonyms, hypernyms, antonyms, or mis-
match. Mismatch is defined as none of the above.
We restrict modifiers to single nouns and adjectives
occurring before the head noun phrase.
</bodyText>
<table confidence="0.5056625">
Apposition
m1, m2 found
3.7 Memorization Features
Relative Pronoun Apposition and m2 is PRO
</table>
<tableCaption confidence="0.9104025">
Table 3: Location Features. Compatible mentions are
those having the same gender and number.
</tableCaption>
<subsectionHeader confidence="0.871011">
3.5 Learned Features
</subsectionHeader>
<bodyText confidence="0.999933638888889">
Modifier Names If the mentions are both mod-
ified by other proper names, use a basic corefer-
ence classifier to determine whether the modifiers
are coreferential. This basic classifier is trained
using Mention Types, String Relations, Semantic
Features, Apposition, Relative Pronoun, and Both
Speak. For each mention m, examples are generated
with the closest antecedent a to form a positive ex-
ample, and every mention between a and m to form
negative examples.
Anaphoricity Ng and Cardie (2002a) and Denis
and Baldridge (2007) show that when used effec-
tively, explicitly predicting anaphoricity can be help-
ful. Thus, we learn a separate classifier to detect
whether a mention is anaphoric (that is, whether it
is not the first mention in its equivalence class), and
use that classifier’s output as a feature for the coref-
erence model. Features for the anaphoricity classi-
fier include the mention type, whether the mention
appears in a quotation, the text of the first word of
the extent, the text of the first word after the head
(if that word is part of the extent), whether there is
a longer mention preceding this mention and having
the same head text, whether any preceding mention
has the same extent text, and whether any preceding
mention has the same text from beginning of the ex-
tent to end of the head. Conjunctions of all pairs of
these features are also used. This classifier predicts
anaphoricity with about 82% accuracy.
We allow our system to learn which pairs of nouns
tend to be used to mention the same entity. For ex-
ample, President and he often refer to Bush but she
and Prime Minister rarely do, if ever. To enable the
system to learn such patterns, we treat the presence
or absence of each pair of final head nouns, one from
each mention of an example, as a feature.
</bodyText>
<subsectionHeader confidence="0.95658">
3.8 Predicted Entity Type
</subsectionHeader>
<bodyText confidence="0.999869678571428">
We predict the entity type (person, organization,
geo-political entity, location, facility, weapon, or ve-
hicle) as follows: If a proper name, we check a list of
personal first names, and a short list of honorary ti-
tles (e.g. mr) to determine if the mention is a person.
Otherwise we look in lists of personal last names
drawn from US census data, and in lists of cities,
states, countries, organizations, corporations, sports
teams, universities, political parties, and organiza-
tion endings (e.g. inc or corp). If found in exactly
one list, we return the appropriate type. We return
unknown if found in multiple lists because the lists
are quite comprehensive and may have significant
overlap.
For common nouns, we look at the hypernym tree
for one of the following: person, political unit, loca-
tion, organization, weapon, vehicle, industrial plant,
and facility. If any is found, we return the appropri-
ate type. If multiple are found, we sort as in the
above list.
For personal pronouns, we recognize the entity as
a person; otherwise we specify unknown.
This computation is used as part of the following
two features.
Entity Type Match This feature checks to see
whether the predicted entity types match. The result
is true if the types are identical, false if they are dif-
ferent, and unknown if at least one type is unknown.
</bodyText>
<page confidence="0.994525">
298
</page>
<bodyText confidence="0.977367625">
Entity Type Conjunctions This feature indicates
the presence of the pair of predicted entity types for
the two mentions, except that if either word is a pro-
noun, the word token replaces the type in the pair.
Since we do this replacement for entity types, we
also add a similar feature for mention types here.
These features are boolean: For any given pair, a
feature is active if that pair describes the example.
</bodyText>
<sectionHeader confidence="0.616379" genericHeader="method">
3.9 Related Work
</sectionHeader>
<bodyText confidence="0.999994307692308">
Many of our features are similar to those described
in Culotta et al. (2007). This includes Mention
Types, String Relation Features, Gender and Num-
ber Match, WordNet Features, Alias, Apposition,
Relative Pronoun, and Both Mentions Speak. The
implementations of those features may vary from
those of other systems. Anaphoricity has been pro-
posed as a part of the model in several systems, in-
cluding Ng and Cardie (2002a), but we are not aware
of it being used as a feature for a learning algorithm.
Distances have been used in e.g. Luo et al. (2004).
However, we are not aware of any system using the
number of compatible mentions as a distance.
</bodyText>
<sectionHeader confidence="0.99347" genericHeader="method">
4 Experimental Study
</sectionHeader>
<subsectionHeader confidence="0.981823">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999972333333333">
We use the official ACE 2004 English training
data (NIST, 2004). Much work has been done on
coreference in several languages, but for this work
we focus on English text. We split the corpus into
three sets: Train, Dev, and Test. Our test set contains
the same 107 documents as Culotta et al. (2007).
Our training set is a random 80% of the 336 doc-
uments in their training set and our Dev set is the
remaining 20%.
For our ablation study, we further randomly split
our development set into two evenly sized parts,
Dev-Tune and Dev-Eval. For each experiment, we
set the parameters of our algorithm to optimize B-
Cubed F-Score using Dev-Tune, and use those pa-
rameters to evaluate on the Dev-Eval data.
</bodyText>
<subsectionHeader confidence="0.992758">
4.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999669">
For the experiments in Section 5, following Culotta
et al. (2007), to make experiments more compara-
ble across systems, we assume that perfect mention
boundaries and mention type labels are given. We
do not use any other gold annotated input at evalu-
ation time. In Section 6 experiments we do not use
any gold annotated input and do not assume mention
types or boundaries are given. In all experiments we
automatically split words and sentences using our
preprocessing tools.4
</bodyText>
<subsectionHeader confidence="0.998985">
4.3 Evaluation Scores
</subsectionHeader>
<bodyText confidence="0.9999082">
B-Cubed F-Score We evaluate over the com-
monly used B-Cubed F-Score (Bagga and Baldwin,
1998), which is a measure of the overlap of predicted
clusters and true clusters. It is computed as the har-
monic mean of precision (P),
</bodyText>
<equation confidence="0.937465444444444">
⎛⎝X ( cm
)⎞
⎠
pm
mEd
and recall (R),
⎛ ⎝X ~cm �⎞
⎠ (2)
mEd tm &apos;
</equation>
<bodyText confidence="0.999898">
where cm is the number of mentions appearing
both in m’s predicted cluster and in m’s true clus-
ter, pm is the size of the predicted cluster containing
m, and tm is the size of m’s true cluster. Finally, d
represents a document from the set D, and N is the
total number of mentions in D.
B-Cubed F-Score has the advantage of being able
to measure the impact of singleton entities, and of
giving more weight to the splitting or merging of
larger entities. It also gives equal weight to all types
of entities and mentions. For these reasons, we re-
port our results using B-Cubed F-Score.
MUC F-Score We also provide results using the
official MUC scoring algorithm (Vilain et al., 1995).
The MUC F-score is also the harmonic mean of
precision and recall. However, the MUC precision
counts precision errors by computing the minimum
number of links that must be added to ensure that all
mentions referring to a given entity are connected
in the graph. Recall errors are the number of links
that must be removed to ensure that no two men-
tions referring to different entities are connected in
the graph.
</bodyText>
<footnote confidence="0.9699615">
4The code is available at http://L2R.cs.uiuc.edu/
˜cogcomp/tools.php
</footnote>
<equation confidence="0.998619285714286">
1 X P =
N dED
� (1)
1 X
R =
N
dED
</equation>
<page confidence="0.997543">
299
</page>
<subsectionHeader confidence="0.998026">
4.4 Learning Algorithm Details
</subsectionHeader>
<bodyText confidence="0.9998516">
We train a regularized average perceptron using ex-
amples selected as described in Section 2.2.1. The
learning rate is 0.1 and the regularization parameter
(separator thickness) is 3.5. At training time, we use
a threshold of 0.0, but when evaluating, we select pa-
rameters to optimize B-Cubed F-Score on a held-out
development set. We sample all even integer thresh-
olds from -16 to 8. We choose the number of rounds
of training similarly, allowing any number from one
to twenty.
</bodyText>
<sectionHeader confidence="0.784274" genericHeader="method">
5 Results
</sectionHeader>
<table confidence="0.999869">
Precision Recall B3 F
Culotta et al. 86.7 73.2 79.3
Current Work 88.3 74.5 80.8
</table>
<tableCaption confidence="0.5996448">
Table 4: Evaluation on unseen Test Data using B3 score.
Shows that our system outperforms the advanced system
of Culotta et al. The improvement is statistically signifi-
cant at the p = 0.05 level according to a non-parametric
bootstrapping percentile test.
</tableCaption>
<bodyText confidence="0.989342818181818">
In Table 4, we compare our performance against
a system that is comparable to ours: Both use gold
mention boundaries and types, evaluate using B-
Cubed F-Score, and have the same training and test
data split. Culotta et al. (2007) is the best compara-
ble system of which we are aware.
Our results show that a pairwise model with
strong features outperforms a state-of-the-art system
with a more complex model.
MUC Score We evaluate the performance of our
system using the official MUC score in Table 5.
</bodyText>
<tableCaption confidence="0.992932">
Table 5: Evaluation of our system on unseen Test Data
using MUC score.
</tableCaption>
<subsectionHeader confidence="0.999605">
5.1 Analysis of Feature Contributions
</subsectionHeader>
<bodyText confidence="0.97788736">
In Table 6 we show the relative impact of various
features. We report data on Dev-Eval, to avoid the
possibility of overfitting by feature selection. The
parameters of the algorithm are chosen to maximize
the BCubed F-Score on the Dev-Tune data. Note
that since we report results on Dev-Eval, the results
in Table 6 are not directly comparable with Culotta
et al. (2007). For comparable results, see Table 4
and the discussion above.
Our ablation study shows the impact of various
classes of features, indicating that almost all the fea-
tures help, although some more than others. It also
illustrates that some features contribute more to pre-
cision, others more to recall. For example, aligned
modifiers contribute primarily to precision, whereas
our learned features and our apposition features con-
tribute to recall. This information can be useful
when designing a coreference system in an applica-
tion where recall is more important than precision,
or vice versa.
We examine the effect of some important features,
selecting those that provide a substantial improve-
ment in precision, recall, or both. For each such
feature we examine the rate of coreference amongst
mention pairs for which the feature is active, com-
pared with the overall rate of coreference. We also
show examples on which the coreference systems
differ depending on the presence or absence of a fea-
ture.
Apposition This feature checks whether two men-
tions are separated by only a comma, and it in-
creases B-Cubed F-Score by about one percentage
point. We hypothesize that proper names and com-
mon noun phrases link primarily through apposition,
and that apposition is thus a significant feature for
good coreference resolution.
When this feature is active 36% of the examples
are coreferential, whereas only 6% of all examples
are coreferential. Looking at some examples our
system begins to get right when apposition is added,
we find the phrase
Israel’s Deputy Defense Minister,
Ephraim Sneh.
Upon adding apposition, our system begins to cor-
rectly associate Israel’s Deputy Defense Minister
with Ephraim Sneh. Likewise in the phrase
The court president, Ronald Sutherland,
the system correctly associates The court president
with Ronald Sutherland when they appear in an ap-
positive relation in the text. In addition, our system
</bodyText>
<table confidence="0.94501">
MUC Precision MUC Recall MUC F
82.7 69.9 75.8
300
Precision Recall B-Cubed F
String Similarity 86.88 67.17 75.76
+ Semantic Features 85.34 69.30 76.49
+ Apposition 89.77 67.53 77.07
+ Relative Pronoun 88.76 68.97 77.62
+ Distances 89.62 71.93 79.81
+ Learned Features 87.37 74.51 80.43
+ Aligned Modifiers 88.70 74.30 80.86
+ Memorization 86.57 75.59 80.71
+ Predicted Entity Types 87.92 76.46 81.79
</table>
<tableCaption confidence="0.994286">
Table 6: Contribution of Features as evaluated on a development set. Bold results are significantly better than the
</tableCaption>
<bodyText confidence="0.989264648148148">
previous line at the p = 0.05 level according to a paired non-parametric bootstrapping percentile test. These results
show the importance of Distance, Entity Type, and Apposition features.
begins correctly associating relative pronouns such
as who with their referents in phrases like
Sheikh Abbad, who died 500 years ago.
although an explicit relative pronoun feature is
added only later.
Although this feature may lead the system to link
comma separated lists of entities due to misinter-
pretation of the comma, for example Wyoming and
western South Dakota in a list of locations, we be-
lieve this can be avoided by refining the apposition
feature to ignore lists.
Relative Pronoun Next we investigate the relative
pronoun feature. With this feature active, 93% of
examples were positive, indicating the precision of
this feature. Looking to examples, we find who in
the official, who wished to remain anony-
mous
is properly linked, as is that in
nuclear warheads that can befitted to mis-
siles.
Distances Our distance features measure separa-
tion of two mentions in number of compatible men-
tions (quantized), and whether the mentions are in
the same sentence. Distance features are important
for a system that makes links based on the best pair-
wise coreference value rather than implicitly incor-
porating distance by linking only the closest pair
whose score is above a threshold, as done by e.g.
Soon et al. (2001).
Looking at examples, we find that adding dis-
tances allows the system to associate the pronoun
it with this missile not separated by any mentions,
rather than Tehran, which is separated from it by
many mentions.
Predicted Entity Types Since no two mentions
can have different entity types (person, organization,
geo-political entity, etc.) and be coreferential, this
feature has strong discriminative power. When the
entity types match, 13% of examples are positive
compared to only 6% of examples in general. Qual-
itatively, the entity type prediction correctly recog-
nizes the Gulf region as a geo-political entity, and
He as a person, and thus prevents linking the two.
Likewise, the system discerns Baghdad from am-
bassador due to the entity type. However, in some
cases an identity type match can cause the system to
be overly confident in a bad match, as in the case of
a palestinian state identified with holy Jerusalem on
the basis of proximity and shared entity type. This
type of example may require some additional world
knowledge or deeper comprehension of the docu-
ment.
</bodyText>
<sectionHeader confidence="0.999736" genericHeader="method">
6 End-to-End Coreference
</sectionHeader>
<bodyText confidence="0.999882333333333">
The ultimate goal for a coreference system is to
process unannotated text. We use the term end-to-
end coreference for a system capable of determin-
ing coreference on plain text. We describe the chal-
lenges associated with an end-to-end system, de-
scribe our approach, and report results below.
</bodyText>
<page confidence="0.995048">
301
</page>
<subsectionHeader confidence="0.915386">
6.1 Challenges
</subsectionHeader>
<bodyText confidence="0.9999408">
Developing an end-to-end system requires detecting
and classifying mentions, which may degrade coref-
erence results. One challenge in detecting mentions
is that they are often heavily nested. Additionally,
there are issues with evaluating an end-to-end sys-
tem against a gold standard corpus, resulting from
the possibility of mismatches in mention boundaries,
missing mentions, and additional mentions detected,
along with the need to align detected mentions to
their counterparts in the annotated data.
</bodyText>
<subsectionHeader confidence="0.988138">
6.2 Approach
</subsectionHeader>
<bodyText confidence="0.999987454545455">
We resolve coreference on unannotated text as fol-
lows: First we detect mention heads following a
state of the art chunking approach (Punyakanok and
Roth, 2001) using standard features. This results in
a 90% F1 head detector. Next, we detect the extent
boundaries for each head using a learned classifier.
This is followed by determining whether a mention
is a proper name, common noun phrase, prenominal
modifier, or pronoun using a learned mention type
classifier that. Finally, we apply our coreference al-
gorithm described above.
</bodyText>
<subsectionHeader confidence="0.983881">
6.3 Evaluation and Results
</subsectionHeader>
<bodyText confidence="0.999949461538462">
To evaluate, we align the heads of the detected men-
tions to the gold standard heads greedily based on
number of overlapping words. We choose not to
impute errors to the coreference system for men-
tions that were not detected or for spuriously de-
tected mentions (following Ji et al. (2005) and oth-
ers). Although this evaluation is lenient, given that
the mention detection component performs at over
90% F1, we believe it provides a realistic measure
for the performance of the end-to-end system and fo-
cuses the evaluation on the coreference component.
The results of our end-to-end coreference system are
shown in Table 7.
</bodyText>
<tableCaption confidence="0.921577">
Table 7: Coreference results using detected mentions on
unseen Test Data.
</tableCaption>
<sectionHeader confidence="0.997053" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999931153846154">
We described and evaluated a state-of-the-art coref-
erence system based on a pairwise model and strong
features. While previous work showed the impact
of complex models on a weak pairwise baseline, the
applicability and impact of such models on a strong
baseline system such as ours remains uncertain. We
also studied and demonstrated the relative value of
various types of features, showing in particular the
importance of distance and apposition features, and
showing which features impact precision or recall
more. Finally, we showed an end-to-end system ca-
pable of determining coreference in a plain text doc-
ument.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993815">
We would like to thank Ming-Wei Chang, Michael
Connor, Alexandre Klementiev, Nick Rizzolo,
Kevin Small, and the anonymous reviewers for their
insightful comments. This work is partly supported
by NSF grant SoD-HCER-0613885 and a grant from
Boeing.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99936448">
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT/NAACL, pages 81–88.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In HLT/NAACL, pages 236–243,
Rochester, New York.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Y. Freund and R. E. Schapire. 1998. Large margin clas-
sification using the Perceptron algorithm. In COLT,
pages 209–217.
H. Ji, D. Westbrook, and R. Grishman. 2005. Us-
ing semantic relations to refine coreference decisions.
In EMNLP/HLT, pages 17–24, Vancouver, British
Columbia, Canada.
X. Luo and I. Zitouni. 2005. Multi-lingual coreference
resolution with syntactic features. In HLT/EMNLP,
pages 660–667, Vancouver, British Columbia, Canada.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
ACL, page 135, Morristown, NJ, USA.
</reference>
<figure confidence="0.954848666666667">
Precision
Recall
B3 F
End-to-End System 84.91
72.53
78.24
</figure>
<page confidence="0.987175">
302
</page>
<reference confidence="0.999718727272727">
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In COLING-2002.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In ACL.
NIST. 2004. The ace evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995–1001. MIT Press.
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597–604, Irvine, California.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521–
544.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC6, pages 45–52.
</reference>
<page confidence="0.999469">
303
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.940481">
<title confidence="0.999896">Understanding the Value of Features for Coreference Resolution</title>
<author confidence="0.999992">Eric Bengtson Dan Roth</author>
<affiliation confidence="0.999832">Department of Computer University of</affiliation>
<address confidence="0.947799">Urbana, IL</address>
<abstract confidence="0.999659173913043">In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model. This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In MUC7.</booktitle>
<contexts>
<context position="20466" citStr="Bagga and Baldwin, 1998" startWordPosition="3417" endWordPosition="3420"> Dev-Eval data. 4.2 Preprocessing For the experiments in Section 5, following Culotta et al. (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. We do not use any other gold annotated input at evaluation time. In Section 6 experiments we do not use any gold annotated input and do not assume mention types or boundaries are given. In all experiments we automatically split words and sentences using our preprocessing tools.4 4.3 Evaluation Scores B-Cubed F-Score We evaluate over the commonly used B-Cubed F-Score (Bagga and Baldwin, 1998), which is a measure of the overlap of predicted clusters and true clusters. It is computed as the harmonic mean of precision (P), ⎛⎝X ( cm )⎞ ⎠ pm mEd and recall (R), ⎛ ⎝X ~cm �⎞ ⎠ (2) mEd tm &apos; where cm is the number of mentions appearing both in m’s predicted cluster and in m’s true cluster, pm is the size of the predicted cluster containing m, and tm is the size of m’s true cluster. Finally, d represents a document from the set D, and N is the total number of mentions in D. B-Cubed F-Score has the advantage of being able to measure the impact of singleton entities, and of giving more weight</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for scoring coreference chains. In MUC7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>M Wick</author>
<author>R Hall</author>
<author>A McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution. In</title>
<date>2007</date>
<booktitle>HLT/NAACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="3395" citStr="Culotta et al. (2007)" startWordPosition="532" endWordPosition="535">e to evaluate the impact of structural innovations. To this end, we combine an effective coreference classification model with a strong set of features, and present an ablation study to show the relative impact of a variety of features. As we show, this combination of a pairwise model and strong features produces a 1.5 percent294 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 294–303, Honolulu, October 2008.c�2008 Association for Computational Linguistics age point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. 2 A Pairwise Coreference Model Given a document and a set of mentions, coreference resolution is the task of grouping the mentions into equivalence classes, so that each equivalence class contains exactly those mentions that refer to the same discourse entity. The number of equivalence classes is not specified in advance, but is bounded by the number of mentions. In this paper, we view coreference resolution as a graph problem: Given a set of mentions and their context as nodes, ge</context>
<context position="6770" citStr="Culotta et al. (2007)" startWordPosition="1104" endWordPosition="1107">ntecedent varies. Soon et al. (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. Best-Link was shown to outperform Closest-Link in an experiment by Ng and Cardie (2002b). Our model differs from that of Ng and Cardie in that we impose the constraint that non-pronouns cannot refer back to pronouns, and in that we use as training examples all ordered pairs of mentions, subject to the constraint above. Culotta et al. (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. Since the number of possible classes is exponential in the number of mentions, they use heuristics to select training examples. Our method does not require determining which equivalence classes should be considered as examples. 2.2 Pairwise Coreference Function Learning the pairwise scoring function pc is a crucial issue for the pairwise coreference model. We apply machine learning techniques to learn from examples a function pc that takes as </context>
<context position="10056" citStr="Culotta et al. (2007)" startWordPosition="1658" endWordPosition="1661"> all features used is available at http://L2R.cs.uiuc.edu/-cogcomp/asoftware. php?skey=LBJ#features. In the following description, the term head means the head noun phrase of a mention; the extent is the largest noun phrase headed by the head noun phrase. 3.1 Mention Types The type of a mention indicates whether it is a proper noun, a common noun, or a pronoun. This feature, when conjoined with others, allows us to give different weight to a feature depending on whether it is being applied to a proper name or a pronoun. For our experiments in Section 5, we use gold mention types as is done by Culotta et al. (2007) and Luo and Zitouni (2005). Note that in the experiments described in Section 6 we predict the mention types as described there and do not use any gold data. The mention type feature is used in all experiments. 3.2 String Relation Features String relation features indicate whether two strings share some property, such as one being the substring of another or both sharing a modifier word. Features are listed in Table 1. Modifiers are limited to those occurring before the head. Feature Definition headi == headj extenti == extentj headi substring of headj modi == (headj or modj) Alias acronym(he</context>
<context position="18559" citStr="Culotta et al. (2007)" startWordPosition="3091" endWordPosition="3094"> is true if the types are identical, false if they are different, and unknown if at least one type is unknown. 298 Entity Type Conjunctions This feature indicates the presence of the pair of predicted entity types for the two mentions, except that if either word is a pronoun, the word token replaces the type in the pair. Since we do this replacement for entity types, we also add a similar feature for mention types here. These features are boolean: For any given pair, a feature is active if that pair describes the example. 3.9 Related Work Many of our features are similar to those described in Culotta et al. (2007). This includes Mention Types, String Relation Features, Gender and Number Match, WordNet Features, Alias, Apposition, Relative Pronoun, and Both Mentions Speak. The implementations of those features may vary from those of other systems. Anaphoricity has been proposed as a part of the model in several systems, including Ng and Cardie (2002a), but we are not aware of it being used as a feature for a learning algorithm. Distances have been used in e.g. Luo et al. (2004). However, we are not aware of any system using the number of compatible mentions as a distance. 4 Experimental Study 4.1 Corpus</context>
<context position="19941" citStr="Culotta et al. (2007)" startWordPosition="3331" endWordPosition="3334">ish text. We split the corpus into three sets: Train, Dev, and Test. Our test set contains the same 107 documents as Culotta et al. (2007). Our training set is a random 80% of the 336 documents in their training set and our Dev set is the remaining 20%. For our ablation study, we further randomly split our development set into two evenly sized parts, Dev-Tune and Dev-Eval. For each experiment, we set the parameters of our algorithm to optimize BCubed F-Score using Dev-Tune, and use those parameters to evaluate on the Dev-Eval data. 4.2 Preprocessing For the experiments in Section 5, following Culotta et al. (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. We do not use any other gold annotated input at evaluation time. In Section 6 experiments we do not use any gold annotated input and do not assume mention types or boundaries are given. In all experiments we automatically split words and sentences using our preprocessing tools.4 4.3 Evaluation Scores B-Cubed F-Score We evaluate over the commonly used B-Cubed F-Score (Bagga and Baldwin, 1998), which is a measure of the overlap of predicted clusters and true clusters</context>
<context position="22947" citStr="Culotta et al. (2007)" startWordPosition="3860" endWordPosition="3863">larly, allowing any number from one to twenty. 5 Results Precision Recall B3 F Culotta et al. 86.7 73.2 79.3 Current Work 88.3 74.5 80.8 Table 4: Evaluation on unseen Test Data using B3 score. Shows that our system outperforms the advanced system of Culotta et al. The improvement is statistically significant at the p = 0.05 level according to a non-parametric bootstrapping percentile test. In Table 4, we compare our performance against a system that is comparable to ours: Both use gold mention boundaries and types, evaluate using BCubed F-Score, and have the same training and test data split. Culotta et al. (2007) is the best comparable system of which we are aware. Our results show that a pairwise model with strong features outperforms a state-of-the-art system with a more complex model. MUC Score We evaluate the performance of our system using the official MUC score in Table 5. Table 5: Evaluation of our system on unseen Test Data using MUC score. 5.1 Analysis of Feature Contributions In Table 6 we show the relative impact of various features. We report data on Dev-Eval, to avoid the possibility of overfitting by feature selection. The parameters of the algorithm are chosen to maximize the BCubed F-S</context>
</contexts>
<marker>Culotta, Wick, Hall, McCallum, 2007</marker>
<rawString>A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007. First-order probabilistic models for coreference resolution. In HLT/NAACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In HLT/NAACL,</booktitle>
<pages>236--243</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="15432" citStr="Denis and Baldridge (2007)" startWordPosition="2547" endWordPosition="2550">tion Features. Compatible mentions are those having the same gender and number. 3.5 Learned Features Modifier Names If the mentions are both modified by other proper names, use a basic coreference classifier to determine whether the modifiers are coreferential. This basic classifier is trained using Mention Types, String Relations, Semantic Features, Apposition, Relative Pronoun, and Both Speak. For each mention m, examples are generated with the closest antecedent a to form a positive example, and every mention between a and m to form negative examples. Anaphoricity Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. Thus, we learn a separate classifier to detect whether a mention is anaphoric (that is, whether it is not the first mention in its equivalence class), and use that classifier’s output as a feature for the coreference model. Features for the anaphoricity classifier include the mention type, whether the mention appears in a quotation, the text of the first word of the extent, the text of the first word after the head (if that word is part of the extent), whether there is a longer mention preceding this mention a</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In HLT/NAACL, pages 236–243, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12438" citStr="Fellbaum, 1998" startWordPosition="2041" endWordPosition="2043">gned Modifiers Relation WordNet and lists Memorization Last Words Tokens Predicted Entity Types Entity Types Match Annotation and tokens Entity Type Pair WordNet and tokens Table 2: Features by Category a proper name, gender is determined by the existence of mr, ms, mrs, or the gender of the first name. If only a last name is found, the phrase is considered to refer to a person. If the name is found in a comprehensive list of cities or countries, or ends with an organization ending such as inc, then the gender is neuter. In the case of a common noun phrase, the phrase is looked up in WordNet (Fellbaum, 1998), and it is assigned a gender according to whether male, female, person, artifact, location, or group (the last three correspond to neuter) is found in the hypernym tree. The gender of a pronoun is looked up in a table. Number Match Number is determined as follows: Phrases starting with the words a, an, or this are singular; those, these, or some indicate plural. Names not containing and are singular. Common nouns are checked against extensive lists of singular and plural nouns – words found in neither or both lists have unknown number. Finally, if the number is unknown yet the two mentions ha</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the Perceptron algorithm.</title>
<date>1998</date>
<booktitle>In COLT,</booktitle>
<pages>209--217</pages>
<contexts>
<context position="8801" citStr="Freund and Schapire, 1998" startWordPosition="1453" endWordPosition="1456"> the existence of this edge than for other edges. This is similar to the technique of Ng and Cardie (2002b). For each m, we generate negative examples (a, m) for all mentions a that precede m and are not in the same equivalence class. Note that in doing so we generate more negative examples than positive ones. Since we never apply pc to a pair where the first mention is a pronoun and the second is not a pronoun, we do not train on examples of this form. 2.2.2 Learning Pairwise Coreference Scoring Model We learn the pairwise coreference function using an averaged perceptron learning algorithm (Freund and Schapire, 1998) – we use the regularized version in Learning Based Java2 (Rizzolo and Roth, 2007). 3 Features The performance of the document-level coreference model depends on the quality of the pairwise coreference function pc. Beyond the training paradigm described earlier, the quality of pc depends on the features used. We divide the features into categories, based on their function. A full list of features and their categories is given in Table 2. In addition to these boolean features, we also use the conjunctions of all pairs of features.3 2LBJ code is available at http://L2R.cs.uiuc.edu/ -cogcomp/asof</context>
</contexts>
<marker>Freund, Schapire, 1998</marker>
<rawString>Y. Freund and R. E. Schapire. 1998. Large margin classification using the Perceptron algorithm. In COLT, pages 209–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>D Westbrook</author>
<author>R Grishman</author>
</authors>
<title>Using semantic relations to refine coreference decisions.</title>
<date>2005</date>
<booktitle>In EMNLP/HLT,</booktitle>
<pages>17--24</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="30329" citStr="Ji et al. (2005)" startWordPosition="5054" endWordPosition="5057"> we detect the extent boundaries for each head using a learned classifier. This is followed by determining whether a mention is a proper name, common noun phrase, prenominal modifier, or pronoun using a learned mention type classifier that. Finally, we apply our coreference algorithm described above. 6.3 Evaluation and Results To evaluate, we align the heads of the detected mentions to the gold standard heads greedily based on number of overlapping words. We choose not to impute errors to the coreference system for mentions that were not detected or for spuriously detected mentions (following Ji et al. (2005) and others). Although this evaluation is lenient, given that the mention detection component performs at over 90% F1, we believe it provides a realistic measure for the performance of the end-to-end system and focuses the evaluation on the coreference component. The results of our end-to-end coreference system are shown in Table 7. Table 7: Coreference results using detected mentions on unseen Test Data. 7 Conclusion We described and evaluated a state-of-the-art coreference system based on a pairwise model and strong features. While previous work showed the impact of complex models on a weak </context>
</contexts>
<marker>Ji, Westbrook, Grishman, 2005</marker>
<rawString>H. Ji, D. Westbrook, and R. Grishman. 2005. Using semantic relations to refine coreference decisions. In EMNLP/HLT, pages 17–24, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>I Zitouni</author>
</authors>
<title>Multi-lingual coreference resolution with syntactic features.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP,</booktitle>
<pages>660--667</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="10083" citStr="Luo and Zitouni (2005)" startWordPosition="1663" endWordPosition="1666">lable at http://L2R.cs.uiuc.edu/-cogcomp/asoftware. php?skey=LBJ#features. In the following description, the term head means the head noun phrase of a mention; the extent is the largest noun phrase headed by the head noun phrase. 3.1 Mention Types The type of a mention indicates whether it is a proper noun, a common noun, or a pronoun. This feature, when conjoined with others, allows us to give different weight to a feature depending on whether it is being applied to a proper name or a pronoun. For our experiments in Section 5, we use gold mention types as is done by Culotta et al. (2007) and Luo and Zitouni (2005). Note that in the experiments described in Section 6 we predict the mention types as described there and do not use any gold data. The mention type feature is used in all experiments. 3.2 String Relation Features String relation features indicate whether two strings share some property, such as one being the substring of another or both sharing a modifier word. Features are listed in Table 1. Modifiers are limited to those occurring before the head. Feature Definition headi == headj extenti == extentj headi substring of headj modi == (headj or modj) Alias acronym(headi) == headj or lastnamei </context>
</contexts>
<marker>Luo, Zitouni, 2005</marker>
<rawString>X. Luo and I. Zitouni. 2005. Multi-lingual coreference resolution with syntactic features. In HLT/EMNLP, pages 660–667, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>135</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19031" citStr="Luo et al. (2004)" startWordPosition="3171" endWordPosition="3174">ature is active if that pair describes the example. 3.9 Related Work Many of our features are similar to those described in Culotta et al. (2007). This includes Mention Types, String Relation Features, Gender and Number Match, WordNet Features, Alias, Apposition, Relative Pronoun, and Both Mentions Speak. The implementations of those features may vary from those of other systems. Anaphoricity has been proposed as a part of the model in several systems, including Ng and Cardie (2002a), but we are not aware of it being used as a feature for a learning algorithm. Distances have been used in e.g. Luo et al. (2004). However, we are not aware of any system using the number of compatible mentions as a distance. 4 Experimental Study 4.1 Corpus We use the official ACE 2004 English training data (NIST, 2004). Much work has been done on coreference in several languages, but for this work we focus on English text. We split the corpus into three sets: Train, Dev, and Test. Our test set contains the same 107 documents as Culotta et al. (2007). Our training set is a random 80% of the 336 documents in their training set and our Dev set is the remaining 20%. For our ablation study, we further randomly split our dev</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In ACL, page 135, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution.</title>
<date>2002</date>
<booktitle>In COLING-2002.</booktitle>
<contexts>
<context position="5083" citStr="Ng and Cardie, 2002" startWordPosition="814" endWordPosition="817"> pc that produces a value indicating the probability that the two mentions should be placed in the same equivalence class. The remainder of this section first discusses how this function is used as part of a document-level coreference decision model and then describes how we learn the pc function. 2.1 Document-Level Decision Model Given a document d and a pairwise coreference scoring function pc that maps an ordered pair of mentions to a value indicating the probability that they are coreferential (see Section 2.2), we generate a coreference graph Gd according to the Best-Link decision model (Ng and Cardie, 2002b) as follows: For each mention m in document d, let B,,t be the set of mentions appearing before m in d. Let a be the highest scoring antecedent: a = argmax (pc(b, m)). bEB, If pc(a, m) is above a threshold chosen as described in Section 4.4, we add the edge (a, m) to the coreference graph Gd. The resulting graph contains connected components, each representing one equivalence class, with all the mentions in the component referring to the same entity. This technique permits us to learn to detect some links between mentions while being agnostic about whether other mentions are linked, and yet </context>
<context position="6514" citStr="Ng and Cardie (2002" startWordPosition="1060" endWordPosition="1063">date antecedents. 2.1.1 Related Models For pairwise models, it is common to choose the best antecedent for a given mention (thereby imposing the constraint that each mention has at most one antecedent); however, the method of deciding which is the best antecedent varies. Soon et al. (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. Best-Link was shown to outperform Closest-Link in an experiment by Ng and Cardie (2002b). Our model differs from that of Ng and Cardie in that we impose the constraint that non-pronouns cannot refer back to pronouns, and in that we use as training examples all ordered pairs of mentions, subject to the constraint above. Culotta et al. (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. Since the number of possible classes is exponential in the number of mentions, they use heuristics to select training examples. Our method does not require determining which equivalence class</context>
<context position="8280" citStr="Ng and Cardie (2002" startWordPosition="1362" endWordPosition="1365">alence classes for mentions. However, for some pairs of mentions from an equivalence class, there is little or no direct evidence in the text that the mentions are coreferential. Therefore, training pc on all pairs of mentions within an equivalence class may not lead to a good predictor. Thus, for each mention m we select from m’s equivalence class the closest preceding mention a and present the pair (a, m) as a positive training example, under the assumption that there is more direct evidence in the text for the existence of this edge than for other edges. This is similar to the technique of Ng and Cardie (2002b). For each m, we generate negative examples (a, m) for all mentions a that precede m and are not in the same equivalence class. Note that in doing so we generate more negative examples than positive ones. Since we never apply pc to a pair where the first mention is a pronoun and the second is not a pronoun, we do not train on examples of this form. 2.2.2 Learning Pairwise Coreference Scoring Model We learn the pairwise coreference function using an averaged perceptron learning algorithm (Freund and Schapire, 1998) – we use the regularized version in Learning Based Java2 (Rizzolo and Roth, 20</context>
<context position="15399" citStr="Ng and Cardie (2002" startWordPosition="2542" endWordPosition="2545">nd m2 is PRO Table 3: Location Features. Compatible mentions are those having the same gender and number. 3.5 Learned Features Modifier Names If the mentions are both modified by other proper names, use a basic coreference classifier to determine whether the modifiers are coreferential. This basic classifier is trained using Mention Types, String Relations, Semantic Features, Apposition, Relative Pronoun, and Both Speak. For each mention m, examples are generated with the closest antecedent a to form a positive example, and every mention between a and m to form negative examples. Anaphoricity Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. Thus, we learn a separate classifier to detect whether a mention is anaphoric (that is, whether it is not the first mention in its equivalence class), and use that classifier’s output as a feature for the coreference model. Features for the anaphoricity classifier include the mention type, whether the mention appears in a quotation, the text of the first word of the extent, the text of the first word after the head (if that word is part of the extent), whether there is a longer</context>
<context position="18900" citStr="Ng and Cardie (2002" startWordPosition="3145" endWordPosition="3148">lacement for entity types, we also add a similar feature for mention types here. These features are boolean: For any given pair, a feature is active if that pair describes the example. 3.9 Related Work Many of our features are similar to those described in Culotta et al. (2007). This includes Mention Types, String Relation Features, Gender and Number Match, WordNet Features, Alias, Apposition, Relative Pronoun, and Both Mentions Speak. The implementations of those features may vary from those of other systems. Anaphoricity has been proposed as a part of the model in several systems, including Ng and Cardie (2002a), but we are not aware of it being used as a feature for a learning algorithm. Distances have been used in e.g. Luo et al. (2004). However, we are not aware of any system using the number of compatible mentions as a distance. 4 Experimental Study 4.1 Corpus We use the official ACE 2004 English training data (NIST, 2004). Much work has been done on coreference in several languages, but for this work we focus on English text. We split the corpus into three sets: Train, Dev, and Test. Our test set contains the same 107 documents as Culotta et al. (2007). Our training set is a random 80% of the </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002a. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. In COLING-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5083" citStr="Ng and Cardie, 2002" startWordPosition="814" endWordPosition="817"> pc that produces a value indicating the probability that the two mentions should be placed in the same equivalence class. The remainder of this section first discusses how this function is used as part of a document-level coreference decision model and then describes how we learn the pc function. 2.1 Document-Level Decision Model Given a document d and a pairwise coreference scoring function pc that maps an ordered pair of mentions to a value indicating the probability that they are coreferential (see Section 2.2), we generate a coreference graph Gd according to the Best-Link decision model (Ng and Cardie, 2002b) as follows: For each mention m in document d, let B,,t be the set of mentions appearing before m in d. Let a be the highest scoring antecedent: a = argmax (pc(b, m)). bEB, If pc(a, m) is above a threshold chosen as described in Section 4.4, we add the edge (a, m) to the coreference graph Gd. The resulting graph contains connected components, each representing one equivalence class, with all the mentions in the component referring to the same entity. This technique permits us to learn to detect some links between mentions while being agnostic about whether other mentions are linked, and yet </context>
<context position="6514" citStr="Ng and Cardie (2002" startWordPosition="1060" endWordPosition="1063">date antecedents. 2.1.1 Related Models For pairwise models, it is common to choose the best antecedent for a given mention (thereby imposing the constraint that each mention has at most one antecedent); however, the method of deciding which is the best antecedent varies. Soon et al. (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. Best-Link was shown to outperform Closest-Link in an experiment by Ng and Cardie (2002b). Our model differs from that of Ng and Cardie in that we impose the constraint that non-pronouns cannot refer back to pronouns, and in that we use as training examples all ordered pairs of mentions, subject to the constraint above. Culotta et al. (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. Since the number of possible classes is exponential in the number of mentions, they use heuristics to select training examples. Our method does not require determining which equivalence class</context>
<context position="8280" citStr="Ng and Cardie (2002" startWordPosition="1362" endWordPosition="1365">alence classes for mentions. However, for some pairs of mentions from an equivalence class, there is little or no direct evidence in the text that the mentions are coreferential. Therefore, training pc on all pairs of mentions within an equivalence class may not lead to a good predictor. Thus, for each mention m we select from m’s equivalence class the closest preceding mention a and present the pair (a, m) as a positive training example, under the assumption that there is more direct evidence in the text for the existence of this edge than for other edges. This is similar to the technique of Ng and Cardie (2002b). For each m, we generate negative examples (a, m) for all mentions a that precede m and are not in the same equivalence class. Note that in doing so we generate more negative examples than positive ones. Since we never apply pc to a pair where the first mention is a pronoun and the second is not a pronoun, we do not train on examples of this form. 2.2.2 Learning Pairwise Coreference Scoring Model We learn the pairwise coreference function using an averaged perceptron learning algorithm (Freund and Schapire, 1998) – we use the regularized version in Learning Based Java2 (Rizzolo and Roth, 20</context>
<context position="15399" citStr="Ng and Cardie (2002" startWordPosition="2542" endWordPosition="2545">nd m2 is PRO Table 3: Location Features. Compatible mentions are those having the same gender and number. 3.5 Learned Features Modifier Names If the mentions are both modified by other proper names, use a basic coreference classifier to determine whether the modifiers are coreferential. This basic classifier is trained using Mention Types, String Relations, Semantic Features, Apposition, Relative Pronoun, and Both Speak. For each mention m, examples are generated with the closest antecedent a to form a positive example, and every mention between a and m to form negative examples. Anaphoricity Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. Thus, we learn a separate classifier to detect whether a mention is anaphoric (that is, whether it is not the first mention in its equivalence class), and use that classifier’s output as a feature for the coreference model. Features for the anaphoricity classifier include the mention type, whether the mention appears in a quotation, the text of the first word of the extent, the text of the first word after the head (if that word is part of the extent), whether there is a longer</context>
<context position="18900" citStr="Ng and Cardie (2002" startWordPosition="3145" endWordPosition="3148">lacement for entity types, we also add a similar feature for mention types here. These features are boolean: For any given pair, a feature is active if that pair describes the example. 3.9 Related Work Many of our features are similar to those described in Culotta et al. (2007). This includes Mention Types, String Relation Features, Gender and Number Match, WordNet Features, Alias, Apposition, Relative Pronoun, and Both Mentions Speak. The implementations of those features may vary from those of other systems. Anaphoricity has been proposed as a part of the model in several systems, including Ng and Cardie (2002a), but we are not aware of it being used as a feature for a learning algorithm. Distances have been used in e.g. Luo et al. (2004). However, we are not aware of any system using the number of compatible mentions as a distance. 4 Experimental Study 4.1 Corpus We use the official ACE 2004 English training data (NIST, 2004). Much work has been done on coreference in several languages, but for this work we focus on English text. We split the corpus into three sets: Train, Dev, and Test. Our test set contains the same 107 documents as Culotta et al. (2007). Our training set is a random 80% of the </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002b. Improving machine learning approaches to coreference resolution. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ace evaluation plan.</title>
<date>2004</date>
<note>www.nist.gov/speech/tests/ace/index.htm.</note>
<contexts>
<context position="1441" citStr="NIST, 2004" startWordPosition="227" endWordPosition="228">plex models. We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features. 1 Introduction Coreference resolution is the task of grouping all the mentions of entities1 in a document into equivalence classes so that all the mentions in a given class refer to the same discourse entity. For example, given the sentence (where the head noun of each mention is subscripted) 1We follow the ACE (NIST, 2004) terminology: A noun phrase referring to a discourse entity is called a mention, and an equivalence class is called an entity. An American1 official2 announced that American1 President3 Bill Clinton3 met his3 Russian4 counterpart5, Vladimir Putin5, today. the task is to group the mentions so that those referring to the same entity are placed together into an equivalence class. Many NLP tasks detect attributes, actions, and relations between discourse entities. In order to discover all information about a given entity, textual mentions of that entity must be grouped together. Thus coreference i</context>
<context position="19223" citStr="NIST, 2004" startWordPosition="3206" endWordPosition="3207">es, Gender and Number Match, WordNet Features, Alias, Apposition, Relative Pronoun, and Both Mentions Speak. The implementations of those features may vary from those of other systems. Anaphoricity has been proposed as a part of the model in several systems, including Ng and Cardie (2002a), but we are not aware of it being used as a feature for a learning algorithm. Distances have been used in e.g. Luo et al. (2004). However, we are not aware of any system using the number of compatible mentions as a distance. 4 Experimental Study 4.1 Corpus We use the official ACE 2004 English training data (NIST, 2004). Much work has been done on coreference in several languages, but for this work we focus on English text. We split the corpus into three sets: Train, Dev, and Test. Our test set contains the same 107 documents as Culotta et al. (2007). Our training set is a random 80% of the 336 documents in their training set and our Dev set is the remaining 20%. For our ablation study, we further randomly split our development set into two evenly sized parts, Dev-Tune and Dev-Eval. For each experiment, we set the parameters of our algorithm to optimize BCubed F-Score using Dev-Tune, and use those parameters</context>
</contexts>
<marker>NIST, 2004</marker>
<rawString>NIST. 2004. The ace evaluation plan. www.nist.gov/speech/tests/ace/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In The Conference on Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>995--1001</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="29642" citStr="Punyakanok and Roth, 2001" startWordPosition="4940" endWordPosition="4943">g and classifying mentions, which may degrade coreference results. One challenge in detecting mentions is that they are often heavily nested. Additionally, there are issues with evaluating an end-to-end system against a gold standard corpus, resulting from the possibility of mismatches in mention boundaries, missing mentions, and additional mentions detected, along with the need to align detected mentions to their counterparts in the annotated data. 6.2 Approach We resolve coreference on unannotated text as follows: First we detect mention heads following a state of the art chunking approach (Punyakanok and Roth, 2001) using standard features. This results in a 90% F1 head detector. Next, we detect the extent boundaries for each head using a learned classifier. This is followed by determining whether a mention is a proper name, common noun phrase, prenominal modifier, or pronoun using a learned mention type classifier that. Finally, we apply our coreference algorithm described above. 6.3 Evaluation and Results To evaluate, we align the heads of the detected mentions to the gold standard heads greedily based on number of overlapping words. We choose not to impute errors to the coreference system for mentions</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In The Conference on Advances in Neural Information Processing Systems (NIPS), pages 995–1001. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Modeling Discriminative Global Inference.</title>
<date>2007</date>
<booktitle>In Proceedings of the First International Conference on Semantic Computing (ICSC),</booktitle>
<pages>597--604</pages>
<location>Irvine, California.</location>
<contexts>
<context position="8883" citStr="Rizzolo and Roth, 2007" startWordPosition="1467" endWordPosition="1470"> Ng and Cardie (2002b). For each m, we generate negative examples (a, m) for all mentions a that precede m and are not in the same equivalence class. Note that in doing so we generate more negative examples than positive ones. Since we never apply pc to a pair where the first mention is a pronoun and the second is not a pronoun, we do not train on examples of this form. 2.2.2 Learning Pairwise Coreference Scoring Model We learn the pairwise coreference function using an averaged perceptron learning algorithm (Freund and Schapire, 1998) – we use the regularized version in Learning Based Java2 (Rizzolo and Roth, 2007). 3 Features The performance of the document-level coreference model depends on the quality of the pairwise coreference function pc. Beyond the training paradigm described earlier, the quality of pc depends on the features used. We divide the features into categories, based on their function. A full list of features and their categories is given in Table 2. In addition to these boolean features, we also use the conjunctions of all pairs of features.3 2LBJ code is available at http://L2R.cs.uiuc.edu/ -cogcomp/asoftware.php?skey=LBJ 3The package of all features used is available at http://L2R.cs</context>
</contexts>
<marker>Rizzolo, Roth, 2007</marker>
<rawString>N. Rizzolo and D. Roth. 2007. Modeling Discriminative Global Inference. In Proceedings of the First International Conference on Semantic Computing (ICSC), pages 597–604, Irvine, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>544</pages>
<contexts>
<context position="4360" citStr="Soon et al., 2001" startWordPosition="693" endWordPosition="696"> the same discourse entity. The number of equivalence classes is not specified in advance, but is bounded by the number of mentions. In this paper, we view coreference resolution as a graph problem: Given a set of mentions and their context as nodes, generate a set of edges such that any two mentions that belong in the same equivalence class are connected by some path in the graph. We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model (Soon et al., 2001). To decide whether two mentions should be linked in the graph, we learn a pairwise coreference function pc that produces a value indicating the probability that the two mentions should be placed in the same equivalence class. The remainder of this section first discusses how this function is used as part of a document-level coreference decision model and then describes how we learn the pc function. 2.1 Document-Level Decision Model Given a document d and a pairwise coreference scoring function pc that maps an ordered pair of mentions to a value indicating the probability that they are corefer</context>
<context position="6185" citStr="Soon et al. (2001)" startWordPosition="1007" endWordPosition="1010">o learn to detect some links between mentions while being agnostic about whether other mentions are linked, and yet via the transitive closure of all links we can still determine the equivalence classes. We also require that no non-pronoun can refer back to a pronoun: If m is not a pronoun, we do not consider pronouns as candidate antecedents. 2.1.1 Related Models For pairwise models, it is common to choose the best antecedent for a given mention (thereby imposing the constraint that each mention has at most one antecedent); however, the method of deciding which is the best antecedent varies. Soon et al. (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. Best-Link was shown to outperform Closest-Link in an experiment by Ng and Cardie (2002b). Our model differs from that of Ng and Cardie in that we impose the constraint that non-pronouns cannot refer back to pronouns, and in that we use as training examples all ordered pairs of mentions, subject to the constraint above. Culotta et al. (2007) introduced a m</context>
<context position="27552" citStr="Soon et al. (2001)" startWordPosition="4608" endWordPosition="4611">ating the precision of this feature. Looking to examples, we find who in the official, who wished to remain anonymous is properly linked, as is that in nuclear warheads that can befitted to missiles. Distances Our distance features measure separation of two mentions in number of compatible mentions (quantized), and whether the mentions are in the same sentence. Distance features are important for a system that makes links based on the best pairwise coreference value rather than implicitly incorporating distance by linking only the closest pair whose score is above a threshold, as done by e.g. Soon et al. (2001). Looking at examples, we find that adding distances allows the system to associate the pronoun it with this missile not separated by any mentions, rather than Tehran, which is separated from it by many mentions. Predicted Entity Types Since no two mentions can have different entity types (person, organization, geo-political entity, etc.) and be coreferential, this feature has strong discriminative power. When the entity types match, 13% of examples are positive compared to only 6% of examples in general. Qualitatively, the entity type prediction correctly recognizes the Gulf region as a geo-p</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521– 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In MUC6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="21343" citStr="Vilain et al., 1995" startWordPosition="3584" endWordPosition="3587">cted cluster and in m’s true cluster, pm is the size of the predicted cluster containing m, and tm is the size of m’s true cluster. Finally, d represents a document from the set D, and N is the total number of mentions in D. B-Cubed F-Score has the advantage of being able to measure the impact of singleton entities, and of giving more weight to the splitting or merging of larger entities. It also gives equal weight to all types of entities and mentions. For these reasons, we report our results using B-Cubed F-Score. MUC F-Score We also provide results using the official MUC scoring algorithm (Vilain et al., 1995). The MUC F-score is also the harmonic mean of precision and recall. However, the MUC precision counts precision errors by computing the minimum number of links that must be added to ensure that all mentions referring to a given entity are connected in the graph. Recall errors are the number of links that must be removed to ensure that no two mentions referring to different entities are connected in the graph. 4The code is available at http://L2R.cs.uiuc.edu/ ˜cogcomp/tools.php 1 X P = N dED � (1) 1 X R = N dED 299 4.4 Learning Algorithm Details We train a regularized average perceptron using </context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In MUC6, pages 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>