<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000709">
<title confidence="0.993947">
Analyzing the Errors of Unsupervised Learning
</title>
<author confidence="0.999666">
Percy Liang Dan Klein
</author>
<affiliation confidence="0.9982835">
Computer Science Division, EECS Department
University of California at Berkeley
</affiliation>
<address confidence="0.74876">
Berkeley, CA 94720
</address>
<email confidence="0.996272">
1pliang,kleinj@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997331" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998812666666667">
We identify four types of errors that unsu-
pervised induction systems make and study
each one in turn. Our contributions include
</bodyText>
<listItem confidence="0.8927893">
(1) using a meta-model to analyze the incor-
rect biases of a model in a systematic way,
(2) providing an efficient and robust method
of measuring distance between two parameter
settings of a model, and (3) showing that lo-
cal optima issues which typically plague EM
can be somewhat alleviated by increasing the
number of training examples. We conduct
our analyses on three models: the HMM, the
PCFG, and a simple dependency model.
</listItem>
<sectionHeader confidence="0.998074" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999685593220339">
The unsupervised induction of linguistic structure
from raw text is an important problem both for un-
derstanding language acquisition and for building
language processing systems such as parsers from
limited resources. Early work on inducing gram-
mars via EM encountered two serious obstacles: the
inappropriateness of the likelihood objective and the
tendency of EM to get stuck in local optima. With-
out additional constraints on bracketing (Pereira and
Shabes, 1992) or on allowable rewrite rules (Carroll
and Charniak, 1992), unsupervised grammar learn-
ing was ineffective.
Since then, there has been a large body of work
addressing the flaws of the EM-based approach.
Syntactic models empirically more learnable than
PCFGs have been developed (Clark, 2001; Klein
and Manning, 2004). Smith and Eisner (2005) pro-
posed a new objective function; Smith and Eis-
ner (2006) introduced a new training procedure.
Bayesian approaches can also improve performance
(Goldwater and Griffiths, 2007; Johnson, 2007;
Kurihara and Sato, 2006).
Though these methods have improved induction
accuracy, at the core they all still involve optimizing
non-convex objective functions related to the like-
lihood of some model, and thus are not completely
immune to the difficulties associated with early ap-
proaches. It is therefore important to better under-
stand the behavior of unsupervised induction sys-
tems in general.
In this paper, we take a step back and present
a more statistical view of unsupervised learning in
the context of grammar induction. We identify four
types of error that a system can make: approxima-
tion, identifiability, estimation, and optimization er-
rors (see Figure 1). We try to isolate each one in turn
and study its properties.
Approximation error is caused by a mis-match
between the likelihood objective optimized by EM
and the true relationship between sentences and their
syntactic structures. Our key idea for understand-
ing this mis-match is to “cheat” and initialize EM
with the true relationship and then study the ways
in which EM repurposes our desired syntactic struc-
tures to increase likelihood. We present a meta-
model of the changes that EM makes and show how
this tool can shed some light on the undesired biases
of the HMM, the PCFG, and the dependency model
with valence (Klein and Manning, 2004).
Identifiability error can be incurred when two dis-
tinct parameter settings yield the same probabil-
ity distribution over sentences. One type of non-
identifiability present in HMMs and PCFGs is label
symmetry, which even makes computing a mean-
ingful distance between parameters NP-hard. We
present a method to obtain lower and upper bounds
on such a distance.
Estimation error arises from having too few train-
ing examples, and optimization error stems from
</bodyText>
<page confidence="0.983697">
879
</page>
<note confidence="0.696034">
Proceedings ofACL-08: HLT, pages 879–887,
</note>
<page confidence="0.492431">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.9990795">
EM getting stuck in local optima. While it is to be
expected that estimation error should decrease as the
amount of data increases, we show that optimization
error can also decrease. We present striking experi-
ments showing that if our data actually comes from
the model family we are learning with, we can some-
times recover the true parameters by simply run-
ning EM without clever initialization. This result
runs counter to the conventional attitude that EM is
doomed to local optima; it suggests that increasing
the amount of data might be an effective way to par-
tially combat local optima.
</bodyText>
<sectionHeader confidence="0.994141" genericHeader="method">
2 Unsupervised models
</sectionHeader>
<bodyText confidence="0.99995705">
Let x denote an input sentence and y denote the un-
observed desired output (e.g., a parse tree). We con-
sider a model family P = {pg(x, y) : B E O}. For
example, if P is the set of all PCFGs, then the pa-
rameters B would specify all the rule probabilities of
a particular grammar. We sometimes use B and pg
interchangeably to simplify notation. In this paper,
we analyze the following three model families:
In the HMM, the input x is a sequence of words
and the output y is the corresponding sequence of
part-of-speech tags.
In the PCFG, the input x is a sequence of POS
tags and the output y is a binary parse tree with yield
x. We represent y as a multiset of binary rewrites of
the form (y —* y1 y2), where y is a nonterminal and
y1, y2 can be either nonterminals or terminals.
In the dependency model with valence (DMV)
(Klein and Manning, 2004), the input x =
(x1, ... , xm) is a sequence of POS tags and the out-
put y specifies the directed links of a projective de-
pendency tree. The generative model is as follows:
for each head xi, we generate an independent se-
quence of arguments to the left and to the right from
a direction-dependent distribution over tags. At each
point, we stop with a probability parametrized by the
direction and whether any arguments have already
been generated in that direction. See Klein and Man-
ning (2004) for a formal description.
In all our experiments, we used the Wall Street
Journal (WSJ) portion of the Penn Treebank. We bi-
narized the PCFG trees and created gold dependency
trees according to the Collins head rules. We trained
45-state HMMs on all 49208 sentences, 11-state
PCFGs on WSJ-10 (7424 sentences) and DMVs
on WSJ-20 (25523 sentences) (Klein and Manning,
2004). We ran EM for 100 iterations with the pa-
rameters initialized uniformly (always plus a small
amount of random noise). We evaluated the HMM
and PCFG by mapping model states to Treebank
tags to maximize accuracy.
</bodyText>
<sectionHeader confidence="0.885157" genericHeader="method">
3 Decomposition of errors
</sectionHeader>
<bodyText confidence="0.99995475">
Now we will describe the four types of errors (Fig-
ure 1) more formally. Let p∗(x, y) denote the distri-
bution which governs the true relationship between
the input x and output y. In general, p∗ does not
live in our model family P. We are presented with
a set of n unlabeled examples x(1),. . . , x(n) drawn
i.i.d. from the true p∗. In unsupervised induction,
our goal is to approximate p∗ by some model pg E P
in terms of strong generative capacity. A standard
approach is to use the EM algorithm to optimize
the empirical likelihood A log pg(x).1 However, EM
only finds a local maximum, which we denote BEM,
so there is a discrepancy between what we get (pˆgEM)
and what we want (p∗).
We will define this discrepancy later, but for now,
it suffices to remark that the discrepancy depends
on the distribution over y whereas learning depends
only on the distribution over x. This is an important
property that distinguishes unsupervised induction
from more standard supervised learning or density
estimation scenarios.
Now let us walk through the four types of er-
ror bottom up. First, BEM, the local maximum
found by EM, is in general different from B E
argmaxg ]E log pg(x), any global maximum, which
we could find given unlimited computational re-
sources. Optimization error refers to the discrep-
ancy between B and �BEM.
Second, our training data is only a noisy sam-
ple from the true p∗. If we had infinite data, we
would choose an optimal parameter setting under the
model, B∗2 E argmaxg ]E log pg(x), where now the
expectation ]E is taken with respect to the true p∗ in-
stead of the training data. The discrepancy between
B∗2 and B is the estimation error.
Note that B∗2 might not be unique. Let B∗1 denote
</bodyText>
<equation confidence="0.363283">
�n
1Here, the expectation ��f(x) def � 1 ��1 f(x(z)) denotes
n
</equation>
<bodyText confidence="0.260976">
averaging some function f over the training data.
</bodyText>
<page confidence="0.985479">
880
</page>
<figure confidence="0.982885">
p* = true model
Approximation error (Section 4)
log-likelihood -16.7 Labeled Fl 1.0
-17.2 0.8
-17.6 0.6
-18.0 0.4
-18.4 0.2
20 40 60 80 100 20 40 60 80 100
iteration iteration
</figure>
<figureCaption confidence="0.998030444444444">
Figure 2: For the PCFG, when we initialize EM with the
supervised estimate Bgen, the likelihood increases but the
accuracy decreases.
Figure 1: The discrepancy between what we get ( �BEM)
and what we want (p*) can be decomposed into four types
of errors. The box represents our model family P, which
is the set of possible parametrized distributions we can
represent. Best(S) returns the B E S which has the small-
est discrepancy with p*.
</figureCaption>
<bodyText confidence="0.999869">
the maximizer of ]E log pθ(x) that has the smallest
discrepancy with p∗. Since B∗1 and B∗2 have the same
value under the objective function, we would not be
able to choose B∗1 over B∗�, even with infinite data or
unlimited computation. Identifiability error refers to
the discrepancy between B∗1 and B∗�.
Finally, the model family P has fundamental lim-
itations. Approximation error refers to the discrep-
ancy between p∗ and pθ∗�. Note that B∗1 is not nec-
essarily the best in P. If we had labeled data, we
could find a parameter setting in P which is closer
to p∗ by optimizing joint likelihood ]E log pθ(x, y)
(generative training) or even conditional likelihood
]E log pθ(y  |x) (discriminative training).
In the remaining sections, we try to study each of
the four errors in isolation. In practice, since it is
difficult to work with some of the parameter settings
that participate in the error decomposition, we use
computationally feasible surrogates so that the error
under study remains the dominant effect.
</bodyText>
<sectionHeader confidence="0.964677" genericHeader="method">
4 Approximation error
</sectionHeader>
<bodyText confidence="0.999686178571428">
We start by analyzing approximation error, the dis-
crepancy between p∗ and pθ∗1 (the model found by
optimizing likelihood), a point which has been dis-
cussed by many authors (Merialdo, 1994; Smith and
Eisner, 2005; Haghighi and Klein, 2006).2
To confront the question of specifically how
the likelihood diverges from prediction accuracy,
we perform the following experiment: we ini-
tialize EM with the supervised estimate3 Bgen =
argmaxθ ]E log pθ(x, y), which acts as a surrogate
for p∗. As we run EM, the likelihood increases but
the accuracy decreases (Figure 2 shows this trend
for the PCFG; the HMM and DMV models behave
similarly). We believe that the initial iterations of
EM contain valuable information about the incor-
rect biases of these models. However, EM is chang-
ing hundreds of thousands of parameters at once in a
non-trivial way, so we need a way of characterizing
the important changes.
One broad observation we can make is that the
first iteration of EM reinforces the systematic mis-
takes of the supervised initializer. In the first E-step,
the posterior counts that are computed summarize
the predictions of the supervised system. If these
match the empirical counts, then the M-step does not
change the parameters. But if the supervised system
predicts too many JJs, for example, then the M-step
will update the parameters to reinforce this bias.
</bodyText>
<subsectionHeader confidence="0.986951">
4.1 A meta-model for analyzing EM
</subsectionHeader>
<bodyText confidence="0.972169777777778">
We would like to go further and characterize the
specific changes EM makes. An initial approach is
to find the parameters that changed the most dur-
ing the first iteration (weighted by the correspond-
2Here, we think of discrepancy between p and p0 as the error
incurred when using p0 for prediction on examples generated
from p; in symbols, E(,,y)∼Ploss(y, argmaxy, p0(y0 I x)).
3For all our models, the supervised estimate is solved in
closed form by taking ratios of counts.
</bodyText>
<equation confidence="0.995466333333333">
θ*1 = Best(argmaxe E log pe(x))
Identifiability error (Section 5)
θ* ∈ argmaxe E log pe (x)
2
Estimation error (Section 6)
ˆ Eˆ log pe(x)
θ ∈ argmaxe
Optimization error (Section 7)
ˆθEM = EM( ˆE log pe(x)) P
</equation>
<page confidence="0.969391">
881
</page>
<bodyText confidence="0.999840886363636">
ing expected counts computed in the E-step). For
the HMM, the three most changed parameters are
the transitions 2:DT—*8:JJ, START—*0:NNP, and
8:JJ—*3:NN.4 If we delve deeper, we can see that
2:DT—*3:NN (the parameter with the 10th largest
change) fell and 2:DT—*8:JJ rose. After checking
with a few examples, we can then deduce that some
nouns were retagged as adjectives. Unfortunately,
this type of ad-hoc reasoning requires considerable
manual effort and is rather subjective.
Instead, we propose using a general meta-model
to analyze the changes EM makes in an automatic
and objective way. Instead of treating parameters as
the primary object of study, we look at predictions
made by the model and study how they change over
time. While a model is a distribution over sentences,
a meta-model is a distribution over how the predic-
tions of the model change.
Let R(y) denote the set of parts of a predic-
tion y that we are interested in tracking. Each part
(c, l) E R(y) consists of a configuration c and a lo-
cation l. For a PCFG, we define a configuration to
be a rewrite rule (e.g., c = PP—*IN NP), and a loca-
tion l = [i, k, j] to be a span [i, j] split at k, where
the rewrite c is applied.
In this work, each configuration is associated with
a parameter of the model, but in general, a configu-
ration could be a larger unit such as a subtree, allow-
ing one to track more complex changes. The size of
a configuration governs how much the meta-model
generalizes from individual examples.
Let y(i,t) denote the model prediction on the i-th
training example after t iterations of EM. To sim-
plify notation, we write Rt = R(y(i,t)). The meta-
model explains how Rt became Rt+1.5
In general, we expect a part in Rt+1 to be ex-
plained by a part in Rt that has a similar location
and furthermore, we expect the locations of the two
parts to be related in some consistent way. The meta-
model uses two notions to formalize this idea: a dis-
tance d(l, l0) and a relation r(l, l0). For the PCFG,
d(l, l0) is the number of positions among i,j,k that
are the same as the corresponding ones in l0, and
r((i, k, j), (i0, k0, j0)) = (sign(i − i0), sign(j −
</bodyText>
<footnote confidence="0.99548325">
4Here 2:DT means state 2 of the HMM, which was greedily
mapped to DT.
5If the same part appears in both Rt and Rt+1, we remove
it from both sets.
</footnote>
<bodyText confidence="0.999201888888889">
j0), sign(k − k0)) is one of 33 values. We define a
migration as a triple (c, c0, r(l, l0)); this is the unit of
change we want to extract from the meta-model.
Our meta-model provides the following genera-
tive story of how Rt becomes Rt+1: each new part
(c0, l0) E Rt+1 chooses an old part (c, l) E Rt with
some probability that depends on (1) the distance be-
tween the locations l and l0 and (2) the likelihood of
the particular migration. Formally:
</bodyText>
<equation confidence="0.994012">
pmeta(Rt+1  |Rt) =
Z−1
l&apos; e−αd(l,l&apos;)p(c0  |c, r(l,l0)),
</equation>
<bodyText confidence="0.982846375">
where Zl = E(c,l)∈Rt e−αd(l,l&apos;) is a normalization
constant, and α is a hyperparameter controlling the
possibility of distant migrations (set to 3 in our ex-
periments).
We learn the parameters of the meta-model with
an EM algorithm similar to the one for IBM model
1. Fortunately, the likelihood objective is convex, so
we need not worry about local optima.
</bodyText>
<subsectionHeader confidence="0.973084">
4.2 Results of the meta-model
</subsectionHeader>
<bodyText confidence="0.999994708333333">
We used our meta-model to analyze the approxima-
tion errors of the HMM, DMV, and PCFG. For these
models, we initialized EM with the supervised es-
timate �θgen and collected the model predictions as
EM ran. We then trained the meta-model on the pre-
dictions between successive iterations. The meta-
model gives us an expected count for each migra-
tion. Figure 3 lists the migrations with the highest
expected counts.
From these migrations, we can see that EM tries
to explain x better by making the corresponding y
more regular. In fact, many of the HMM migra-
tions on the first iteration attempt to resolve incon-
sistencies in gold tags. For example, noun adjuncts
(e.g., stock-index), tagged as both nouns and adjec-
tives in the Treebank, tend to become consolidated
under adjectives, as captured by migration (B). EM
also re-purposes under-utilized states to better cap-
ture distributional similarities. For example, state 24
has migrated to state 40 (N), both of which are now
dominated by proper nouns. State 40 initially con-
tained only #, but was quickly overrun with distribu-
tionally similar proper nouns such as Oct. and Chap-
ter, which also precede numbers, just as # does.
</bodyText>
<equation confidence="0.877018">
ri �
(c&apos;,l&apos;)∈Rt+1 (c,l)∈Rt
</equation>
<page confidence="0.959191">
882
</page>
<table confidence="0.997049166666667">
Iteration 0→1 4:NN24:NNP Iteration 1→2 4:NN Iteration 2→3 U.S. Iteration 3→4 up Iteration 15:CD
(A) START 4:NN (D) 4:NN 4:NN 24:NNP 4:NN 11:RB U.S. 4→5 24:NNP
4:NN 24:NNP 8:JJ 24:NNP 24:NNP 11:RB 24
(B) 8:JJ (E) START 27:TO :NNP
(C) 24:NNP 8:JJ
(F) 11:RB
(G) 8:JJ (J) 32:RP 34:$
24:NNP 24:NNP 2:IN
11:RB
(H) 8:JJ (K) 8:JJ 40:NNP
(I) 3:DT (L) 19:, down
36:NNPS 8:JJ 32:RP 32:RP
</table>
<figure confidence="0.952490375">
(a) Top HMM migrations. Example: migration (D) means a NN→NN transition is replaced by JJ→NN.
Iteration 0→1 Iteration 1→2 Iteration 2→3 Iteration 3→4 Iteration 4→5
(A) DT NN NN (D) NNP NNP NNP (G) DT JJ NNS (J) DT JJ NN (M) POS JJ NN
(B) JJ NN NN (E) NNP NNP NNP (H) MD RB VB (K) DT NNP NN (N) NNS RB VBP
(C) NNP NNP (F) DT NNP NNP (I) VBP RB VB (L) PRP$ JJ NN (O) NNS RB VBD
(b) Top DMV migrations. Example: migration (A) means a DT attaches to the closer NN.
(c) Top PCFG migrations. Example: migration (D) means a NP→NNP NP rewrite is replaced by NP→NNP NNP,
where the new NNP right child spans less than the old NP right child.
</figure>
<figureCaption confidence="0.88309">
Figure 3: We show the prominent migrations that occur during the first 5 iterations of EM for the HMM, DMV, and
PCFG, as recovered by our meta-model. We sort the migrations across each iteration by their expected counts under
the meta-model and show the top 3. Iteration 0 corresponds to the correct outputs. Blue indicates the new iteration,
red indicates the old.
</figureCaption>
<figure confidence="0.999351630136986">
Iteration 0→1 Iteration 1→2 Iteration 2→3 Iteration 3→4 Iteration 4→5
(L)
1:VP
TO VB
1:VP
MD 1:VP
(J)
TO VB
2:PP
(K)
MD VB
1:VP
0:NP
NNP NNP
(O)
0:NP
CD NN
0:NP
0:NP NN
(M)
CD NN
3:ADJP
(N)
VBD 0:NP
1:VP
VBD 3:ADJP
1:VP
0:NP NN
0:NP
NNP NNP
6:NP
0:NP
0:NP
4:S
DT 0:NP
NNP 0:NP
RB 1:VP
(D)
(G)
NNP NNP
0:NP
RB 1:VP
1:VP
DT NN
0:NP
0:NP 1:VP
VBN 2:PP
1:VP
4:S
0:NP
0:NP 2:PP
(H)
1:VP 2:PP
1:VP
1:VP 2:PP
1:VP
0:NP 1:VP
4:S
4:S
1:VP
1:VP
VBZ 0:NP
0:NP 1:VP
TO VB
(F)
(I)
(E)
0:NP 1:VP
4:S
VBZ 0:NP
1:VP
TO VB
2:PP
</figure>
<bodyText confidence="0.99914294117647">
DMV migrations also try to regularize model pre-
dictions, but in a different way—in terms of the
number of arguments. Because the stop probability
is different for adjacent and non-adjacent arguments,
it is statistically much cheaper to generate one argu-
ment rather than two or more. For example, if we
train a DMV on only DT JJ NN, it can fit the data
perfectly by using a chain of single arguments, but
perfect fit is not possible if NN generates both DT
and JJ (which is the desired structure); this explains
migration (J). Indeed, we observed that the variance
of the number of arguments decreases with more EM
iterations (for NN, from 1.38 to 0.41).
In general, low-entropy conditional distributions
are preferred. Migration (H) explains how adverbs
now consistently attach to verbs rather than modals.
After a few iterations, the modal has committed
itself to generating exactly one verb to the right,
which is statistically advantageous because there
must be a verb after a modal, while the adverb is op-
tional. This leaves the verb to generate the adverb.
The PCFG migrations regularize categories in a
manner similar to the HMM, but with the added
complexity of changing bracketing structures. For
example, sentential adverbs are re-analyzed as VP
adverbs (A). Sometimes, multiple migrations ex-
plain the same phenomenon.6 For example, migra-
tions (B) and (C) indicate that PPs that previously
attached to NPs are now raised to the verbal level.
Tree rotation is another common phenomenon, lead-
ing to many left-branching structures (D,G,H). The
migrations that happen during one iteration can also
trigger additional migrations in the next. For exam-
ple, the raising of the PP (B,C) inspires more of the
</bodyText>
<footnote confidence="0.501528">
6We could consolidate these migrations by using larger con-
figurations, but at the risk of decreased generalization.
</footnote>
<page confidence="0.998257">
883
</page>
<bodyText confidence="0.9999558">
same raising (E). As another example, migration (I)
regularizes TO VB infinitival clauses into PPs, and
this momentum carries over to the next iteration with
even greater force (J).
In summary, the meta-model facilitates our anal-
yses by automatically identifying the broad trends.
We believe that the central idea of modeling the er-
rors of a system is a powerful one which can be used
to analyze a wide range of models, both supervised
and unsupervised.
</bodyText>
<sectionHeader confidence="0.991717" genericHeader="method">
5 Identifiability error
</sectionHeader>
<bodyText confidence="0.999942137931035">
While approximation error is incurred when likeli-
hood diverges from accuracy, identifiability error is
concerned with the case where likelihood is indiffer-
ent to accuracy.
We say a set of parameters 5 is identifiable (in
terms of x) if pg(x) =� pgg(x) for every 0, 00 E 5
where 0 =� 00.7 In general, identifiability error is
incurred when the set of maximizers of ]E log pg(x)
is non-identifiable.8
Label symmetry is perhaps the most familiar ex-
ample of non-identifiability and is intrinsic to mod-
els with hidden labels (HMM and PCFG, but not
DMV). We can permute the hidden labels without
changing the objective function or even the nature
of the solution, so there is no reason to prefer one
permutation over another. While seemingly benign,
this symmetry actually presents a serious challenge
in measuring discrepancy (Section 5.1).
Grenager et al. (2005) augments an HMM to al-
low emission from a generic stopword distribution at
any position with probability q. Their model would
definitely not be identifiable if q were a free param-
eter, since we can set q to 0 and just mix in the stop-
word distribution with each of the other emission
distributions to obtain a different parameter setting
yielding the same overall distribution. This is a case
where our notion of desired structure is absent in the
likelihood, and a prior over parameters could help
break ties.
</bodyText>
<footnote confidence="0.992074857142857">
7For our three model families, B is identifiable in terms of
(x, y), but not in terms of x alone.
8We emphasize that non-identifiability is in terms of x, so
two parameter settings could still induce the same marginal dis-
tribution on x (weak generative capacity) while having different
joint distributions on (x, y) (strong generative capacity). Recall
that discrepancy depends on the latter.
</footnote>
<bodyText confidence="0.9999686">
The above non-identifiabilities apply to all param-
eter settings, but another type of non-identifiability
concerns only the maximizers of ]E log pg(x). Sup-
pose the true data comes from a K-state HMM. If
we attempt to fit an HMM with K + 1 states, we
can split any one of the K states and maintain the
same distribution on x. Or, if we learn a PCFG on
the same HMM data, then we can choose either the
left- or right-branching chain structures, which both
mimic the true HMM equally well.
</bodyText>
<subsectionHeader confidence="0.995382">
5.1 Permutation-invariant distance
</subsectionHeader>
<bodyText confidence="0.9999725">
KL-divergence is a natural measure of discrepancy
between two distributions, but it is somewhat non-
trivial to compute—for our three recursive models, it
requires solving fixed point equations, and becomes
completely intractable in face of label symmetry.
Thus we propose a more manageable alternative:
</bodyText>
<equation confidence="0.9930215">
dµ(0   ||00) def Ej µj  |0j − 00j  |= ,(1)
j µj
</equation>
<bodyText confidence="0.9996962">
where we weight the difference between the j-th
component of the parameter vectors by µj, the j-
th expected sufficient statistic with respect to pg
(the expected counts computed in the E-step).9 Un-
like KL, our distance dµ is only defined on distri-
butions in the model family and is not invariant to
reparametrization. Like KL, dµ is asymmetric, with
the first argument holding the status of being the
“true” parameter setting. In our case, the parameters
are conditional probabilities, so 0 &lt; dµ(0 ||00) &lt; 1,
so we can interpret dµ as an expected difference be-
tween these probabilities.
Unfortunately, label symmetry can wreak havoc
on our distance measure dµ. Suppose we want to
measure the distance between 0 and 00. If 00 is
simply 0 with the labels permuted, then dµ(0  ||00)
would be substantial even though the distance ought
to be zero. We define a revised distance to correct
for this by taking the minimum distance over all la-
bel permutations:
</bodyText>
<equation confidence="0.954411">
Dµ(0  ||00) = min �dµ(0 ||7r(00)), (2)
</equation>
<footnote confidence="0.997255666666667">
9Without this factor, rarely used components could con-
tribute to the sum as much as frequently used ones, thus, making
the distance overly pessimistic.
</footnote>
<page confidence="0.997579">
884
</page>
<bodyText confidence="0.999986857142857">
where 7r(B0) denotes the parameter setting result-
ing from permuting the labels according to 7r. (The
DMV has no label symmetries, so just dµ works.)
For mixture models, we can compute Dµ(B ||B0)
efficiently as follows. Note that each term in the
summation of (1) is associated with one of the K
labels. We can form a K xK matrix M, where each
entry MZj is the distance between the parameters in-
volving label i of B and label j of B0. Dµ(B  ||B0) can
then be computed by finding a maximum weighted
bipartite matching on M using the O(K3) Hungar-
ian algorithm (Kuhn, 1955).
For models such as the HMM and PCFG, com-
puting Dµ is NP-hard, since the summation in dµ (1)
contains both first-order terms which depend on one
label (e.g., emission parameters) and higher-order
terms which depend on more than one label (e.g.,
transitions or rewrites). We cannot capture these
problematic higher-order dependencies in M.
However, we can bound Dµ(B  ||B0) as follows.
We create M using only first-order terms and find
the best matching (permutation) to obtain a lower
bound Dµ and an associated permutation 7r0 achiev-
ing it. Since Dµ(B ||B0) takes the minimum over all
permutations, dµ(B  ||7r(B0)) is an upper bound for
any 7r, in particular for 7r = 7r0. We then use a local
search procedure that changes 7r to further tighten
the upper bound. Let Dµ denote the final value.
</bodyText>
<sectionHeader confidence="0.967007" genericHeader="method">
6 Estimation error
</sectionHeader>
<bodyText confidence="0.9994755">
Thus far, we have considered approximation and
identifiability errors, which have to do with flaws of
the model. The remaining errors have to do with
how well we can fit the model. To focus on these
errors, we consider the case where the true model is
in our family (p∗ E P). To keep the setting as real-
istic as possible, we do supervised learning on real
labeled data to obtain B∗ = argmaxg ], logp(x, y).
We then throw away our real data and let p∗ = pg*.
Now we start anew: sample new artificial data from
B∗, learn a model using this artificial data, and see
how close we get to recovering B∗.
In order to compute estimation error, we need to
compare B∗ with B, the global maximizer of the like-
lihood on our generated data. However, we cannot
compute B exactly. Let us therefore first consider the
simpler supervised scenario. Here, �Bgen has a closed
form solution, so there is no optimization error. Us-
ing our distance Dµ (defined in Section 5.1) to quan-
tify estimation error, we see that, for the HMM, Bgen
quickly approaches B∗ as we increase the amount of
data (Table 1).
</bodyText>
<table confidence="0.994422">
# examples 500 5K 50K 500K
Dµ(B∗  ||Bgen) 0.003 6.3e-4 2.7e-4 8.5e-5
Dµ(B∗  ||Bgen) 0.005 0.001 5.2e-4 1.7e-4
Dµ(B∗  ||Bgen-EM) 0.022 0.018 0.008 0.002
Dµ(B∗  ||Ogen-EM) 0.049 0.039 0.016 0.004
</table>
<tableCaption confidence="0.992063">
Table 1: Lower and upper bounds on the distance from
</tableCaption>
<bodyText confidence="0.910486416666667">
the true B* for the HMM as we increase the number of
examples.
In the unsupervised case, we use the following
procedure to obtain a surrogate for B: initialize EM
with the supervised estimate Bgen and run EM for
100 iterations. Let �Bgen-EM denote the final param-
eters, which should be representative of B. Table 1
shows that the estimation error of Bgen-EM is an order
of magnitude higher than that of Bgen, which is to ex-
pected since Bgen-EM does not have access to labeled
data. However, this error can also be driven down
given a moderate number of examples.
</bodyText>
<sectionHeader confidence="0.988735" genericHeader="method">
7 Optimization error
</sectionHeader>
<bodyText confidence="0.9943819">
Finally, we study optimization error, which is the
discrepancy between the global maximizer B and
BEM, the result of running EM starting from a uni-
form initialization (plus some small noise). As be-
fore, we cannot compute B, so we use Bgen-EM as a
surrogate. Also, instead of comparing Bgen-EM and B
with each other, we compare each of their discrep-
ancies with respect to B∗.
Let us first consider optimization error in terms
of prediction error. The first observation is that
there is a gap between the prediction accuracies
of Bgen-EM and BEM, but this gap shrinks consider-
ably as we increase the number of examples. Fig-
ures 4(a,b,c) support this for all three model fami-
lies: for the HMM, both �Bgen-EM and BEM eventually
achieve around 90% accuracy; for the DMV, 85%.
For the PCFG, BEM still lags Bgen-EM by 10%, but we
believe that more data can further reduce this gap.
Figure 4(d) shows that these trends are not par-
ticular to artificial data. On real WSJ data, the gap
</bodyText>
<page confidence="0.99368">
885
</page>
<table confidence="0.860907285714286">
Accuracy 1.0 Directed F1 1.0 Labeled F1 1.0 Accuracy 0.8
0.9 0.9 0.9 0.7
0.8 0.8 0.8 0.6
0.7 0.7 0.6 0.4
0.6 0.6 0.5 0.3
500 5K 50K 500K 500 5K 50K 500K 500 5K 50K 1K 3K 10K 40K
# examples # examples # examples # examples
</table>
<figure confidence="0.9746825">
(a) HMM (artificial data) (b) DMV (artificial data) (c) PCFG (artificial data) (d) HMM (real data)
(e) HMM (artificial data) (f) HMM log-likelihood/accuracy on 500K examples
</figure>
<figureCaption confidence="0.975075666666667">
Figure 4: Compares the performance of �θEM (EM with a uniform initialization) against �θgen-EM (EM initialized with the
supervised estimate) on (a–c) various models, (d) real data. (e) measures distance instead of accuracy and (f) shows a
sample EM run.
</figureCaption>
<figure confidence="0.997975466666667">
1.0
-165.5
log-likelihood
-167.4
0.8
Accuracy
-169.4
0.6
0.4
-171.4
Sup. init.
Unif. init.
0.2
-173.3
ˆOgen-EM
ˆOEM (rand 1)
ˆOEM (rand 2)
ˆOEM (rand 3)
20 40 60 80 100
iteration
20 40 60 80 100
iteration
500 5K 50K 500K
# examples
Dµ(θ∗  ||·)
0.12
0.07
0.05
0.02
0.1
</figure>
<bodyText confidence="0.999595729729729">
between �θgen-EM and �θEM also diminishes for the
HMM. To reaffirm the trends, we also measure dis-
tance Dµ. Figure 4(e) shows that the distance from
�θEM to the true parameters θ* decreases, but the gap
between �θgen-EM and �θEM does not close as deci-
sively as it did for prediction error.
It is quite surprising that by simply running EM
with a neutral initialization, we can accurately learn
a complex model with thousands of parameters. Fig-
ures 4(f,g) show how both likelihood and accuracy,
which both start quite low, improve substantially
over time for the HMM on artificial data.
Carroll and Charniak (1992) report that EM fared
poorly with local optima. We do not claim that there
are no local optima, but only that the likelihood sur-
face that EM is optimizing can become smoother
with more examples. With more examples, there is
less noise in the aggregate statistics, so it might be
easier for EM to pick out the salient patterns.
Srebro et al. (2006) made a similar observation
in the context of learning Gaussian mixtures. They
characterized three regimes: one where EM was suc-
cessful in recovering the true clusters (given lots of
data), another where EM failed but the global opti-
mum was successful, and the last where both failed
(without much data).
There is also a rich body of theoretical work on
learning latent-variable models. Specialized algo-
rithms can provably learn certain constrained dis-
crete hidden-variable models, some in terms of weak
generative capacity (Ron et al., 1998; Clark and
Thollard, 2005; Adriaans, 1999), others in term of
strong generative capacity (Dasgupta, 1999; Feld-
man et al., 2005). But with the exception of Das-
gupta and Schulman (2007), there is little theoretical
understanding of EM, let alone on complex model
families such as the HMM, PCFG, and DMV.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999956357142857">
In recent years, many methods have improved unsu-
pervised induction, but these methods must still deal
with the four types of errors we have identified in
this paper. One of our main contributions of this pa-
per is the idea of using the meta-model to diagnose
the approximation error. Using this tool, we can bet-
ter understand model biases and hopefully correct
for them. We also introduced a method for mea-
suring distances in face of label symmetry and ran
experiments exploring the effectiveness of EM as a
function of the amount of data. Finally, we hope that
setting up the general framework to understand the
errors of unsupervised induction systems will aid the
development of better methods and further analyses.
</bodyText>
<page confidence="0.997727">
886
</page>
<sectionHeader confidence="0.99834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860403508772">
P. W. Adriaans. 1999. Learning shallow context-free lan-
guages under simple distributions. Technical report,
Stanford University.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. In Workshop Notes for Statistically-Based NLP
Techniques, pages 1–13.
A. Clark and F. Thollard. 2005. PAC-learnability
of probabilistic deterministic finite state automata.
JMLR, 5:473–497.
A. Clark. 2001. Unsupervised induction of stochastic
context free grammars with distributional clustering.
In CoNLL.
S. Dasgupta and L. Schulman. 2007. A probabilistic
analysis of EM for mixtures of separated, spherical
Gaussians. JMLR, 8.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
FOCS.
J. Feldman, R. O’Donnell, and R. A. Servedio. 2005.
Learning mixtures of product distributions over dis-
crete domains. In FOCS, pages 501–510.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
ACL.
T. Grenager, D. Klein, and C. D. Manning. 2005. Un-
supervised learning of field segmentation models for
information extraction. In ACL.
A. Haghighi and D. Klein. 2006. Prototype-based gram-
mar induction. In ACL.
M. Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In EMNLP/CoNLL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly, 2:83–97.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Interna-
tional Colloquium on Grammatical Inference.
B. Merialdo. 1994. Tagging English text with a prob-
abilistic model. Computational Linguistics, 20:155–
171.
F. Pereira and Y. Shabes. 1992. Inside-outside reestima-
tion from partially bracketed corpora. In ACL.
D. Ron, Y. Singer, and N. Tishby. 1998. On the learnabil-
ity and usage of acyclic probabilistic finite automata.
Journal of Computer and System Sciences, 56:133–
152.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
N. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In ACL.
N. Srebro, G. Shakhnarovich, and S. Roweis. 2006. An
investigation of computational and informational lim-
its in Gaussian mixture clustering. In ICML, pages
865–872.
</reference>
<page confidence="0.998016">
887
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951480">
<title confidence="0.999955">Analyzing the Errors of Unsupervised Learning</title>
<author confidence="0.999997">Percy Liang Dan Klein</author>
<affiliation confidence="0.999957">Computer Science Division, EECS Department University of California at Berkeley</affiliation>
<address confidence="0.998639">Berkeley, CA 94720</address>
<email confidence="0.976793">1pliang,kleinj@cs.berkeley.edu</email>
<abstract confidence="0.998177642857143">We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include using a analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P W Adriaans</author>
</authors>
<title>Learning shallow context-free languages under simple distributions.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="30860" citStr="Adriaans, 1999" startWordPosition="5354" endWordPosition="5355"> patterns. Srebro et al. (2006) made a similar observation in the context of learning Gaussian mixtures. They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data). There is also a rich body of theoretical work on learning latent-variable models. Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005). But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV. 8 Conclusion In recent years, many methods have improved unsupervised induction, but these methods must still deal with the four types of errors we have identified in this paper. One of our main contributions of this paper is the idea of using the meta-model to diagnose the approximation error. Using this tool, we can better understand mo</context>
</contexts>
<marker>Adriaans, 1999</marker>
<rawString>P. W. Adriaans. 1999. Learning shallow context-free languages under simple distributions. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>E Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<booktitle>In Workshop Notes for Statistically-Based NLP Techniques,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="1304" citStr="Carroll and Charniak, 1992" startWordPosition="196" endWordPosition="199">onduct our analyses on three models: the HMM, the PCFG, and a simple dependency model. 1 Introduction The unsupervised induction of linguistic structure from raw text is an important problem both for understanding language acquisition and for building language processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizin</context>
<context position="29929" citStr="Carroll and Charniak (1992)" startWordPosition="5197" endWordPosition="5200">0.05 0.02 0.1 between �θgen-EM and �θEM also diminishes for the HMM. To reaffirm the trends, we also measure distance Dµ. Figure 4(e) shows that the distance from �θEM to the true parameters θ* decreases, but the gap between �θgen-EM and �θEM does not close as decisively as it did for prediction error. It is quite surprising that by simply running EM with a neutral initialization, we can accurately learn a complex model with thousands of parameters. Figures 4(f,g) show how both likelihood and accuracy, which both start quite low, improve substantially over time for the HMM on artificial data. Carroll and Charniak (1992) report that EM fared poorly with local optima. We do not claim that there are no local optima, but only that the likelihood surface that EM is optimizing can become smoother with more examples. With more examples, there is less noise in the aggregate statistics, so it might be easier for EM to pick out the salient patterns. Srebro et al. (2006) made a similar observation in the context of learning Gaussian mixtures. They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, </context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>G. Carroll and E. Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Workshop Notes for Statistically-Based NLP Techniques, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
<author>F Thollard</author>
</authors>
<title>PAC-learnability of probabilistic deterministic finite state automata.</title>
<date>2005</date>
<journal>JMLR,</journal>
<pages>5--473</pages>
<contexts>
<context position="30843" citStr="Clark and Thollard, 2005" startWordPosition="5350" endWordPosition="5353">EM to pick out the salient patterns. Srebro et al. (2006) made a similar observation in the context of learning Gaussian mixtures. They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data). There is also a rich body of theoretical work on learning latent-variable models. Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005). But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV. 8 Conclusion In recent years, many methods have improved unsupervised induction, but these methods must still deal with the four types of errors we have identified in this paper. One of our main contributions of this paper is the idea of using the meta-model to diagnose the approximation error. Using this tool, we can bet</context>
</contexts>
<marker>Clark, Thollard, 2005</marker>
<rawString>A. Clark and F. Thollard. 2005. PAC-learnability of probabilistic deterministic finite state automata. JMLR, 5:473–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Unsupervised induction of stochastic context free grammars with distributional clustering.</title>
<date>2001</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="1535" citStr="Clark, 2001" startWordPosition="233" endWordPosition="234">ding language processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsup</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>A. Clark. 2001. Unsupervised induction of stochastic context free grammars with distributional clustering. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
<author>L Schulman</author>
</authors>
<title>A probabilistic analysis of EM for mixtures of separated, spherical Gaussians.</title>
<date>2007</date>
<journal>JMLR,</journal>
<volume>8</volume>
<contexts>
<context position="31001" citStr="Dasgupta and Schulman (2007)" startWordPosition="5375" endWordPosition="5379">e regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data). There is also a rich body of theoretical work on learning latent-variable models. Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005). But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV. 8 Conclusion In recent years, many methods have improved unsupervised induction, but these methods must still deal with the four types of errors we have identified in this paper. One of our main contributions of this paper is the idea of using the meta-model to diagnose the approximation error. Using this tool, we can better understand model biases and hopefully correct for them. We also introduced a method for measuring distances in face of label symmetry and ran experiments </context>
</contexts>
<marker>Dasgupta, Schulman, 2007</marker>
<rawString>S. Dasgupta and L. Schulman. 2007. A probabilistic analysis of EM for mixtures of separated, spherical Gaussians. JMLR, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
</authors>
<title>Learning mixtures of Gaussians.</title>
<date>1999</date>
<booktitle>In FOCS.</booktitle>
<contexts>
<context position="30922" citStr="Dasgupta, 1999" startWordPosition="5363" endWordPosition="5364">he context of learning Gaussian mixtures. They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data). There is also a rich body of theoretical work on learning latent-variable models. Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005). But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV. 8 Conclusion In recent years, many methods have improved unsupervised induction, but these methods must still deal with the four types of errors we have identified in this paper. One of our main contributions of this paper is the idea of using the meta-model to diagnose the approximation error. Using this tool, we can better understand model biases and hopefully correct for them. We also introduced </context>
</contexts>
<marker>Dasgupta, 1999</marker>
<rawString>S. Dasgupta. 1999. Learning mixtures of Gaussians. In FOCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Feldman</author>
<author>R O’Donnell</author>
<author>R A Servedio</author>
</authors>
<title>Learning mixtures of product distributions over discrete domains. In</title>
<date>2005</date>
<booktitle>FOCS,</booktitle>
<pages>501--510</pages>
<marker>Feldman, O’Donnell, Servedio, 2005</marker>
<rawString>J. Feldman, R. O’Donnell, and R. A. Servedio. 2005. Learning mixtures of product distributions over discrete domains. In FOCS, pages 501–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1762" citStr="Goldwater and Griffiths, 2007" startWordPosition="265" endWordPosition="268">ency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction. We identify four types of error that a system can make: </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Grenager</author>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Unsupervised learning of field segmentation models for information extraction.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="21216" citStr="Grenager et al. (2005)" startWordPosition="3668" endWordPosition="3671">pg(x) =� pgg(x) for every 0, 00 E 5 where 0 =� 00.7 In general, identifiability error is incurred when the set of maximizers of ]E log pg(x) is non-identifiable.8 Label symmetry is perhaps the most familiar example of non-identifiability and is intrinsic to models with hidden labels (HMM and PCFG, but not DMV). We can permute the hidden labels without changing the objective function or even the nature of the solution, so there is no reason to prefer one permutation over another. While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1). Grenager et al. (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q. Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution. This is a case where our notion of desired structure is absent in the likelihood, and a prior over parameters could help break ties. 7For our three model families, B is identifiable in terms of (x, y), but not in terms of x</context>
</contexts>
<marker>Grenager, Klein, Manning, 2005</marker>
<rawString>T. Grenager, D. Klein, and C. D. Manning. 2005. Unsupervised learning of field segmentation models for information extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-based grammar induction.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9945" citStr="Haghighi and Klein, 2006" startWordPosition="1679" endWordPosition="1682">nal likelihood ]E log pθ(y |x) (discriminative training). In the remaining sections, we try to study each of the four errors in isolation. In practice, since it is difficult to work with some of the parameter settings that participate in the error decomposition, we use computationally feasible surrogates so that the error under study remains the dominant effect. 4 Approximation error We start by analyzing approximation error, the discrepancy between p∗ and pθ∗1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmaxθ ]E log pθ(x, y), which acts as a surrogate for p∗. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly). We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models. However, EM is changing hundreds of thousands of parameters at once in a non-trivial </context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype-based grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In EMNLP/CoNLL.</booktitle>
<contexts>
<context position="1777" citStr="Johnson, 2007" startWordPosition="269" endWordPosition="270">l optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction. We identify four types of error that a system can make: approximation, </context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>M. Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In EMNLP/CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1561" citStr="Klein and Manning, 2004" startWordPosition="235" endWordPosition="238"> processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems </context>
<context position="3090" citStr="Klein and Manning, 2004" startWordPosition="483" endWordPosition="486">ch one in turn and study its properties. Approximation error is caused by a mis-match between the likelihood objective optimized by EM and the true relationship between sentences and their syntactic structures. Our key idea for understanding this mis-match is to “cheat” and initialize EM with the true relationship and then study the ways in which EM repurposes our desired syntactic structures to increase likelihood. We present a metamodel of the changes that EM makes and show how this tool can shed some light on the undesired biases of the HMM, the PCFG, and the dependency model with valence (Klein and Manning, 2004). Identifiability error can be incurred when two distinct parameter settings yield the same probability distribution over sentences. One type of nonidentifiability present in HMMs and PCFGs is label symmetry, which even makes computing a meaningful distance between parameters NP-hard. We present a method to obtain lower and upper bounds on such a distance. Estimation error arises from having too few training examples, and optimization error stems from 879 Proceedings ofACL-08: HLT, pages 879–887, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics EM getting stuck </context>
<context position="5133" citStr="Klein and Manning, 2004" startWordPosition="839" endWordPosition="842">ify all the rule probabilities of a particular grammar. We sometimes use B and pg interchangeably to simplify notation. In this paper, we analyze the following three model families: In the HMM, the input x is a sequence of words and the output y is the corresponding sequence of part-of-speech tags. In the PCFG, the input x is a sequence of POS tags and the output y is a binary parse tree with yield x. We represent y as a multiset of binary rewrites of the form (y —* y1 y2), where y is a nonterminal and y1, y2 can be either nonterminals or terminals. In the dependency model with valence (DMV) (Klein and Manning, 2004), the input x = (x1, ... , xm) is a sequence of POS tags and the output y specifies the directed links of a projective dependency tree. The generative model is as follows: for each head xi, we generate an independent sequence of arguments to the left and to the right from a direction-dependent distribution over tags. At each point, we stop with a probability parametrized by the direction and whether any arguments have already been generated in that direction. See Klein and Manning (2004) for a formal description. In all our experiments, we used the Wall Street Journal (WSJ) portion of the Penn</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistic Quarterly,</journal>
<pages>2--83</pages>
<contexts>
<context position="24699" citStr="Kuhn, 1955" startWordPosition="4273" endWordPosition="4274">aking the distance overly pessimistic. 884 where 7r(B0) denotes the parameter setting resulting from permuting the labels according to 7r. (The DMV has no label symmetries, so just dµ works.) For mixture models, we can compute Dµ(B ||B0) efficiently as follows. Note that each term in the summation of (1) is associated with one of the K labels. We can form a K xK matrix M, where each entry MZj is the distance between the parameters involving label i of B and label j of B0. Dµ(B ||B0) can then be computed by finding a maximum weighted bipartite matching on M using the O(K3) Hungarian algorithm (Kuhn, 1955). For models such as the HMM and PCFG, computing Dµ is NP-hard, since the summation in dµ (1) contains both first-order terms which depend on one label (e.g., emission parameters) and higher-order terms which depend on more than one label (e.g., transitions or rewrites). We cannot capture these problematic higher-order dependencies in M. However, we can bound Dµ(B ||B0) as follows. We create M using only first-order terms and find the best matching (permutation) to obtain a lower bound Dµ and an associated permutation 7r0 achieving it. Since Dµ(B ||B0) takes the minimum over all permutations, </context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>H. W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistic Quarterly, 2:83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kurihara</author>
<author>T Sato</author>
</authors>
<title>Variational Bayesian grammar induction for natural language.</title>
<date>2006</date>
<booktitle>In International Colloquium on Grammatical Inference.</booktitle>
<contexts>
<context position="1803" citStr="Kurihara and Sato, 2006" startWordPosition="271" endWordPosition="274">ut additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction. We identify four types of error that a system can make: approximation, identifiability, estimatio</context>
</contexts>
<marker>Kurihara, Sato, 2006</marker>
<rawString>K. Kurihara and T. Sato. 2006. Variational Bayesian grammar induction for natural language. In International Colloquium on Grammatical Inference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<pages>171</pages>
<contexts>
<context position="9894" citStr="Merialdo, 1994" startWordPosition="1673" endWordPosition="1674">) (generative training) or even conditional likelihood ]E log pθ(y |x) (discriminative training). In the remaining sections, we try to study each of the four errors in isolation. In practice, since it is difficult to work with some of the parameter settings that participate in the error decomposition, we use computationally feasible surrogates so that the error under study remains the dominant effect. 4 Approximation error We start by analyzing approximation error, the discrepancy between p∗ and pθ∗1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmaxθ ]E log pθ(x, y), which acts as a surrogate for p∗. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly). We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models. However, EM is changing hundreds o</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20:155– 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Shabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1245" citStr="Pereira and Shabes, 1992" startWordPosition="187" endWordPosition="190">iated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model. 1 Introduction The unsupervised induction of linguistic structure from raw text is an important problem both for understanding language acquisition and for building language processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induc</context>
</contexts>
<marker>Pereira, Shabes, 1992</marker>
<rawString>F. Pereira and Y. Shabes. 1992. Inside-outside reestimation from partially bracketed corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ron</author>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>On the learnability and usage of acyclic probabilistic finite automata.</title>
<date>1998</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>56</volume>
<pages>152</pages>
<contexts>
<context position="30817" citStr="Ron et al., 1998" startWordPosition="5346" endWordPosition="5349">ght be easier for EM to pick out the salient patterns. Srebro et al. (2006) made a similar observation in the context of learning Gaussian mixtures. They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data). There is also a rich body of theoretical work on learning latent-variable models. Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005). But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV. 8 Conclusion In recent years, many methods have improved unsupervised induction, but these methods must still deal with the four types of errors we have identified in this paper. One of our main contributions of this paper is the idea of using the meta-model to diagnose the approximation error. U</context>
</contexts>
<marker>Ron, Singer, Tishby, 1998</marker>
<rawString>D. Ron, Y. Singer, and N. Tishby. 1998. On the learnability and usage of acyclic probabilistic finite automata. Journal of Computer and System Sciences, 56:133– 152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1586" citStr="Smith and Eisner (2005)" startWordPosition="239" endWordPosition="242">s parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper</context>
<context position="9918" citStr="Smith and Eisner, 2005" startWordPosition="1675" endWordPosition="1678">aining) or even conditional likelihood ]E log pθ(y |x) (discriminative training). In the remaining sections, we try to study each of the four errors in isolation. In practice, since it is difficult to work with some of the parameter settings that participate in the error decomposition, we use computationally feasible surrogates so that the error under study remains the dominant effect. 4 Approximation error We start by analyzing approximation error, the discrepancy between p∗ and pθ∗1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmaxθ ]E log pθ(x, y), which acts as a surrogate for p∗. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly). We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models. However, EM is changing hundreds of thousands of parameter</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1645" citStr="Smith and Eisner (2006)" startWordPosition="249" endWordPosition="253">ammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view o</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>N. Smith and J. Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Srebro</author>
<author>G Shakhnarovich</author>
<author>S Roweis</author>
</authors>
<title>An investigation of computational and informational limits in Gaussian mixture clustering.</title>
<date>2006</date>
<booktitle>In ICML,</booktitle>
<pages>865--872</pages>
<contexts>
<context position="30276" citStr="Srebro et al. (2006)" startWordPosition="5260" endWordPosition="5263">ith a neutral initialization, we can accurately learn a complex model with thousands of parameters. Figures 4(f,g) show how both likelihood and accuracy, which both start quite low, improve substantially over time for the HMM on artificial data. Carroll and Charniak (1992) report that EM fared poorly with local optima. We do not claim that there are no local optima, but only that the likelihood surface that EM is optimizing can become smoother with more examples. With more examples, there is less noise in the aggregate statistics, so it might be easier for EM to pick out the salient patterns. Srebro et al. (2006) made a similar observation in the context of learning Gaussian mixtures. They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data). There is also a rich body of theoretical work on learning latent-variable models. Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term</context>
</contexts>
<marker>Srebro, Shakhnarovich, Roweis, 2006</marker>
<rawString>N. Srebro, G. Shakhnarovich, and S. Roweis. 2006. An investigation of computational and informational limits in Gaussian mixture clustering. In ICML, pages 865–872.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>