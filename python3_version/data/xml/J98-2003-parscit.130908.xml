<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9984315">
Contextual Grammars as Generative
Models of Natural Languages
</title>
<author confidence="0.997274">
Solomon Marcus* Carlos Martin-Videt
</author>
<affiliation confidence="0.7996815">
University of Bucharest Rovira i Virgili University
Gheorghe PAunI
Institute of Mathematics of the Roma-
nian Academy
</affiliation>
<bodyText confidence="0.99931035">
The paper discusses some classes of contextual grammars—mainly those with &amp;quot;maximal use of
selectors&amp;quot;—giving some arguments that these grammars can be considered a good model for
natural language syntax.
A contextual grammar produces a language starting from a finite set of words and iteratively
adding contexts to the currently generated words, according to a selection procedure: each context
has associated with it a selector, a set of words; the context is adjoined to any occurrence of such a
selector in the word to be derived. In grammars with maximal use of selectors, a context is adjoined
only to selectors for which no superword is a selector. Maximality can be defined either locally
or globally (with respect to all selectors in the grammar). The obtained families of languages are
incomparable with that of Chomsky context-free languages (and with other families of languages
that contain linear languages and that are not &amp;quot;too large&amp;quot;; see Section 5) and have a series of
properties supporting the assertion that these grammars are a possible adequate model for the
syntax of natural languages. They are able to straightforwardly describe all the usual restrictions
appearing in natural (and artificial) languages, which lead to the non-context-freeness of these
languages: reduplication, crossed dependencies, and multiple agreements; however, there are
center-embedded constructions that cannot be covered by these grammars.
While these assertions concern only the weak generative capacity of contextual grammars,
some ideas are also proposed for associating a structure to the generated words, in the form of a
tree, or of a dependence relation (as considered in descriptive linguistics and also similar to that
in link grammars).
</bodyText>
<sectionHeader confidence="0.991249" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99986225">
Contextual grammars were introduced by Marcus (1969), as &amp;quot;intrinsic grammars,&amp;quot;
without auxiliary symbols, based only on the fundamental linguistic operation of in-
serting words in given phrases, according to certain contextual dependencies. More
precisely, contextual grammars include contexts (pairs of words), associated with
</bodyText>
<affiliation confidence="0.4888962">
* University of Bucharest, Faculty of Mathematics, Str. Academiei 14, 70109 Bucharest, Romania. E-mail:
solomon@imarso
f Research Group on Mathematical Linguistics and Language Engineering (GRLMC), Rovira i Virgili
University, Pl. Imperial Tarraco 1, 43005 Tarragona, Spain. E-mail: cmv@astor.urv.es
I Institute of Mathematics of the Romanian Academy, P. 0. Box 1-764, 70700 Bucharest, Romania. E-mail:
</affiliation>
<footnote confidence="0.8664535">
gpaun@imar.ro. Research supported by the Academy of Finland, project 11281, and Spanish Secretaria
de Estado de Universidades e Investigacion, grant SAB 95-0357.
</footnote>
<note confidence="0.863342">
© 1998 Association for Computational Linguistics
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.995934782608696">
selectors (sets of words); a context can be adjoined to any associated word-selector. In
this way, starting from a finite set of words, we can generate a language.
This operation of iterated selective insertion of words is related to the basic com-
binatorics on words, as well as to the basic operations in rewriting systems of any
type. Indeed, contextual grammars, in the many variants considered in the literature,
were investigated mainly from a mathematical point of view; see Patin (1982, 1985,
1994), Nun, Rozenberg and Salomaa (1994), and their references. A complete source
of information is the monograph Nun (1997). A few applications of contextual gram-
mars were developed in connection with action theory (Pam 1979), with the study of
theatrical works (Patin 1976), and with computer program evolution (13alanescu and
Gheorghe 1987), but up to now no attempt has been made to check the relevance
of contextual grammars in the very field where they were motivated: linguistics, the
study of natural languages. A sort of a posteriori explanation is given: the variants
of contextual grammars investigated so far are not powerful enough, hence they are
not interesting enough; what they can do, a regular or a context-free grammar can
do as well. However, a recently introduced class of contextual grammars seems to be
quite appealing from this point of view: the grammars with a maximal use of selectors
(Martin-Vide et al. 1995). In these grammars, a context is adjoined to a word-selector
if this selector is the largest on that place (no other word containing it as a proper
subword can be a selector). Speaking strictly from a formal language theory point
of view, the behavior of these grammars is not spectacular: the family of generated
languages is incomparable with the family of context-free languages, incomparable
with many other families of contextual languages, and (strictly) included in the fam-
ily of context-sensitive languages, properties rather common in the area of contextual
grammars.
This type of grammar has a surprising property, however, important from a lin-
guistic point of view: all of the three basic features of natural (and artificial) languages
that lead to their non-context-freeness (reduplication, crossed dependencies, and mul-
tiple agreements) can be covered by such grammars (and no other class of contextual
grammars can do the same). Technically, the above mentioned non-context-free fea-
tures lead to formal languages of the forms {xcx I x E {a, b}* } (duplicated words of ar-
bitrary length), {anbmcndm n, m &gt; 1} (two crossed dependencies), and {anbncn I n &gt; 1}
gat least] three correlated positions). All of them are non-context-free languages and
all of them can be generated in a surprisingly simple way by contextual grammars
with selectors used in the maximal mode.
Examples of natural language constructions based on reduplication were found,
for instance, by Culy (1985), and Radzinski (1990), whereas crossed dependencies were
demonstrated for Swiss German by Shieber (1985); see also Partee, ter Meulen and
Wall (1990) or a number of contributions to Savitch et al. (1987). Multiple agreements
were identified early on in programming languages (see, for example, Floyd [1962]),
and certain constructions having such characteristics can also be found in natural
languages. We shall give some arguments in Section 4.
Some remarks are in order here. Although we mainly deal with the syntax of nat-
ural languages, we sometimes also mention artificial languages, mainly programming
languages. Without entering into details outside the scope of our paper,1 we adopt the
standpoint that natural and artificial languages have many common features (Man-
</bodyText>
<footnote confidence="0.963032">
1 A word of warning: When we invoke statements concerning various topics, some of which have been
debated for a long time, we do not necessarily argue for these statements and we do not consider the
adequacy of contextual grammars as either proved or disproved by them. We simply mention a
connection between a linguistic fact and a feature of our grammars.
</footnote>
<page confidence="0.990854">
246
</page>
<note confidence="0.915313">
Marcus, Martin-Vide, and Nun Contextual Grammars
</note>
<bodyText confidence="0.999921416666667">
aster Ramer 1993). For instance, we consider these languages infinite and organized on
successive levels of grammaticality, whose number is unlimited in principle, although
practically only a finite number of such levels can be approached. In Marcus (1981-
83), where an effective analysis of contextual ambiguity in English, French, Romanian,
and Hungarian is proposed, practical difficulties imposed a limitation to two levels
of grammaticality for English (one level excluding compound words, the other level
allowing the building of compound words) and Hungarian, but six levels for the anal-
ysis of French verbs. The reason for this situation is the &amp;quot;open&amp;quot; character of natural
languages, making it impossible to formulate a necessary and sufficient condition for a
sentence to be well-formed. As is pointed out by Hockett (1970), the set of well-formed
strings in a natural language should be both finite and infinite, a requirement that is
impossible to fulfill in the framework of classical set theory; for a related discussion,
see Savitch (1991, 1993). This can also be related to Chomsky&apos;s claim that a basic prob-
lem in linguistics is to find a grammar able to generate all and only the well-formed
strings in a natural language. Chomsky&apos;s claim presupposes that natural languages
have the status of formal languages, but not everyone agrees with this notion. Even
for programming languages, many authors reject the idea that well-formed strings
constitute a formal language; see, for instance, the various articles in the collective
volume Olivetti (1970), as well as Marcus (1979).
Returning to constructions specific to natural languages, we have found the sur-
prising fact that the language {ancbmcbmcan n,m &gt; 1} cannot be generated by contex-
tual grammars with a maximal global use of selectors. Observe the center-embedded
structure of this language and the fact that it is an &amp;quot;easy&amp;quot; linear language. As Manaster
Ramer (1994, 4) points out, &amp;quot;the Chomsky hierarchy is in fact highly misleading... ,
suggesting as it does, for example, that center-embedded structures (including mirror-
images) are simpler (since they are context-free) than cross-serial structures (including
reduplications). Yet we know that natural languages abound in reduplications but
abhor mirror-images (Rounds, Manaster Ramer, and Friedman 1987) and it also ap-
pears that, other things being equal, cross-serial structures are easier to process than
center-embedded ones.&amp;quot;
This point brings to mind Chomsky&apos;s arguments (1964, 120-25) that center-
embedded constructions can be handled by the grammar (the description of com-
petence), but not by the performance system. Here competence itself is not able to
cover the center-embedded construction. However, we have to mention the fact that
other similar constructions can be covered by contextual grammars (with or without
maximal use of selectors). This is the case with {wc mi(w) I w e {a, b}*}, where mi(w)
is the mirror image of w. Also the language {w mi(w) I w E {a, b}*1 can be gener-
ated when the maximal use of selectors is considered, but not without involving this
feature.
The difference between these last two languages suggests another point supporting
the adequacy of contextual grammars: from the Chomsky hierarchy point of view,
there is no difference between these languages; rather, their grammars are similar.
This is not the case in the contextual grammar framework, and this also corresponds
to our intuition: having a marker (the central c here) is helpful, it is significantly easier
to process a language when certain positions of its sentences are specified. (Further
illustrations of this point can be found in Section 4.) We conclude that contextual
grammars with a maximal use of selectors seem adequate from these points of view
for modeling natural languages.&apos;
</bodyText>
<footnote confidence="0.700309">
2 We do not claim and we do not intend to prove (because we cannot) that a contextual grammar with
</footnote>
<page confidence="0.9873">
247
</page>
<note confidence="0.879848">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.995744901960785">
In the architecture and the functioning of a contextual grammar one can note
two contradictory basic ingredients. On the one hand, because we use adjoining, not
rewriting (moreover, we do not use nonterminal symbols), the strings are always in-
creased. At every step, we preserve all previously introduced symbols and we add
new ones. This looks quite limiting for the power of these grammars. On the other
hand, in contextual grammars there is a clear context-sensing capability, the contexts
are adjoined to their selectors and depend on them. Context-sensitivity is in general
a powerful property. Context-sensitivity plus erasing produces everything. In many
cases in formal language theory, this combination leads to characterizations of recur-
sively enumerable languages. Such a result has been proved by Ehrenfeucht, Faun,
and Rozenberg (1997) for contextual grammars with unrestricted use of selectors. In
the last section of this paper, we prove that this is also true for the case of maximal
use of selectors. Specifically, we prove that every recursively enumerable language,
L, can be written in the form L = hi (h2-1(L&apos;)), where hi, h2 are morphisms and L&apos; is a
language generated by a contextual grammar with maximal use of selectors. The proof
uses the same construction as in Ehrenfeucht, Paun, and Rozenberg (1997), adapted
to our class of grammars. The effect of h1, h2-1 can also be achieved by a sequential
transducer (with finite memory), hence we may state the theorem in the form: every
recursively enumerable language is a sequential translation of a contextual language
(generated with maximal use of selectors). As a consequence, we find that our gram-
mars can generate languages outside any family of languages that is strictly included
in the family of recursively enumerable languages and is closed under direct and in-
verse morphisms or under finite sequential transducers. Important families in formal
language theory have these properties: the family of context-free languages, several
families in the regulated rewriting area (see Dassow and Faun [1989]), including in-
dexed languages and programmed languages. Together with the fact that the language
{ancbmcbmcan I n,m &gt; 1} mentioned above is linear, we get the incomparability of our
families with many families in the Chomsky hierarchy or in its refinements.
This relates to another statement of Manaster Ramer&apos;s (1994, 4): &amp;quot;The question as
posed by Chomsky [about the place of natural languages in a hierarchy of generative
devices] seems to suggest that the class of natural languages will be found somewhere
in the Chomsky hierarchy. Yet this need not be the case, and probably is not. It is en-
tirely possible, for example, that a realistic theory of natural languages would define
a class of languages which is incommensurate with the Chomsky types, e.g., a few
regular languages, a few non-regular context-free languages, a few non-context-free
context-sensitive languages, and so on. Indeed, it has been pointed out ... that, if finite
languages are to be excluded from linguistic theory as Chomsky himself has always
contended, then the class of natural languages will necessarily be a non-Chomsky
class, since all the Chomsky classes do contain finite languages.&amp;quot; Maybe contextual
grammars (with maximal use of selectors) are one example of such a realistic possi-
bility.
The discussion above has concerned the weak generative capacity of contextual
maximal use of selectors is the best model for natural language syntax, that these grammars can
describe all types of constructions in natural languages or in other languages, or that, for instance, we
can describe in a satisfactory manner the syntax of English. Maybe even other classes of contextual
grammars have to be imagined, which will be better than the existing ones. Further efforts should be
made to clarify the relevance of contextual grammars of various types for the study of natural
languages. For instance, we can report no practical experience in writing a contextual grammar for a
fragment of a natural language. In short, our goal is to acquaint the reader with contextual grammars
and to convince him or her that these grammars deserve further investigation—of a mathematical and,
more importantly, of a linguistic type.
</bodyText>
<page confidence="0.993964">
248
</page>
<note confidence="0.671792">
Marcus, Martin-Vide, and Nun Contextual Grammars
</note>
<bodyText confidence="0.9998564375">
grammars (with maximal use of selectors). Recently (see Martin-Vide and Nun [1998]),
some attempts were made to introduce a structure into the strings generated by con-
textual grammars. An easy way to do so is to associate a tree to a derivation (just add
a pair of parentheses to each context, then build a tree in the usual way: when reading
a left parenthesis add a new edge, when reading a right parenthesis go back along the
current edge, etc.) or a graph describing a dependence relation similar to those dis-
cussed in descriptive linguistics (see Chapter VI of Marcus [1967]) or in link grammars
(Sleator and Temperley 1991; Grinberg, Lafferty, and Sleator 1995). We briefly present
these possibilities here, although the linguistic relevance of the obtained structures is
still being researched.
Let us also mention that, by definition, contextual grammars are (fully) lexicalized
(in accordance with many current trends in formal syntax) and that their languages
have the bounded growth property.
In view of all these results and properties, we believe that contextual grammars
are an attractive model for natural language syntax, completing (but not necessarily
competing with) the existing models, and that they deserve further investigation.
</bodyText>
<sectionHeader confidence="0.973099" genericHeader="keywords">
2. Definitions
</sectionHeader>
<bodyText confidence="0.9981374">
In this section, we introduce the classes of grammars we shall investigate in this paper.
As usual, given an alphabet V (which we also call vocabulary), we denote by V* the
set of all words (equivalently: strings) over V. including the empty one, which is
denoted by A. The set of all nonempty words over V, hence V* — {A}, is denoted
by V. The length of x E V* is denoted by lx1 and its mirror image (also called the
reversal) by mi(x). The families of finite, regular, linear, context-free, context-sensitive,
and recursively enumerable languages are denoted by FIN, REG, LIN, CF, CS, RE,
respectively. For the elements of formal language theory we use, we refer to Harrison
(1978), Rozenberg and Salomaa (1997), and Salomaa (1973).3
A contextual grammar (with choice) is a construct:
</bodyText>
<equation confidence="0.974497">
G = (V, A, (Si,C1), • • • ,(Sn,Cn)), n &gt;1,
</equation>
<bodyText confidence="0.998962285714286">
where V is an alphabet, A is a finite language over V. Si, are languages over V.
and C1, , C are finite subsets of V* x V*.
The elements of A are called axioms (starting words), the sets Si are called selectors,
and the elements of sets C„ written in the form (u, v), are called contexts. The pairs
(S,, C,) are also called productions. The intuition behind this construction is that the
contexts in C, may be adjoined to words -in the associated set S. Formally, we define
the direct derivation relation on V* as follows:
</bodyText>
<equation confidence="0.6060655">
x y
iff x = xi x2x3, y = x1ta2vx3, where x2 E Si, (4, V) E C„ for some i,1 &lt;i &lt; n.
</equation>
<bodyText confidence="0.914343666666667">
Denoting by ==t*„ the reflexive and transitive closure of the relation the
language generated by G is:
= {z E V W 2, for some w E .
</bodyText>
<footnote confidence="0.9088865">
3 As general mathematical notations, we use: C (inclusion, not necessarily proper), c (proper inclusion),
E (&amp;quot;is an element of&amp;quot;), 0 (the empty set), 2x (the family of all subsets of the set X).
</footnote>
<page confidence="0.990529">
249
</page>
<note confidence="0.4287">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.985320333333333">
Consequently, L,„ (G) contains all words of A, as well as all words that can be obtained
from them by adjoining finitely many contexts, according to the selection imposed by
the pairing (S„
</bodyText>
<subsectionHeader confidence="0.679836">
Remark 1
</subsectionHeader>
<bodyText confidence="0.986475285714286">
The previous definition of a contextual grammar is called modular. Sometimes, it is
useful to present a contextual grammar in the so-called functional form, that is, as a
construct G = (V,A, C, (p), where V and A are as above, C is a finite set of contexts
over V. and yo: V* ---* 2c associates sets of contexts from C to strings in V*. Then we
write x == y iff x = xlx2x3, y = x1ux2vx3, for some (u, V) E (p(x2), xi, x2, X3 E V.
It is easy to see that starting from a contextual grammar in the modular pre-
sentation, G = (V, A, (Si,C1), , (Sn, CO), we can consider its functional counterpart
</bodyText>
<equation confidence="0.947036444444444">
G&apos; = (V, A,C, co), with:
C = Ci,
1=1
co(X)= {(U, V) (U, V) E Ci,X E Si,1 &lt;i &lt; n}, XE V. .
Conversely, from a grammar given as G = (V ,A,C,(,o) with:
C = {(u, vi),. •
we can pass, for instance, to G&apos; -= (17 , A, (Si, CO, • • • ,(Sn,Cn)), taking, for each i,1 &lt;
i &lt; n:
C, = {(u,,vi)},
</equation>
<bodyText confidence="0.658569">
and S, the set of strings in V* to which the context (u,, v) can be adjoined, that is:
</bodyText>
<equation confidence="0.846033">
Si = E V* I (141,V1) E gO(X)}.
</equation>
<bodyText confidence="0.989639666666667">
The two grammars G and G&apos; are clearly equivalent in both cases.
Thus, in the proofs below we shall use that presentation of a contextual grammar
which is more appropriate (economical) for that case.
</bodyText>
<subsectionHeader confidence="0.819418">
Remark 2
</subsectionHeader>
<bodyText confidence="0.999551545454545">
The derivation relation defined above has been denoted by &gt;in in order to distin-
guish it from the external derivation defined for G, where the context is adjoined at
the ends of the derived word: x &gt;ex y iff y = uxv for (u, v) E C,, x E S,, for some
i,1 &lt; i &lt; n. In Marcus (1969), only the external derivation is considered, for gram-
mars presented in the functional form, without restrictions on the selection mapping.
Contextual grammars with internal derivation were introduced in Paun and Nguyen
(1980).
We do not investigate the external derivation here.
Two natural variants of the relation
Martin-Vide et al. (1995):
in defined above were considered by
</bodyText>
<equation confidence="0.91810375">
X &gt;m, y
iff x = x1x2x3, y = xi ux2vx3, for x2 E Si, (u, v) c C„ for some 1 &lt;i &lt;n, and there are
no x&apos;2,4 E V* such that x x&apos;2 E Si, and I &lt; lx11, 141 &lt; ix31, ix121 &gt; 1.x21;
X &gt;MgY
</equation>
<page confidence="0.924857">
250
</page>
<bodyText confidence="0.855562">
Marcus, Martin-Vide, and Nun Contextual Grammars
iff x x1x2x3, y = x1ux2vx3, for x2 E Si, (u, V) E C,, for some 1 &lt; i &lt; n, and there
are no x&apos;3 E V* such that x = x&apos;2 E SI, for some 1 &lt;j &lt; n, and Ixc. I &lt;
1, 14i i 1x31, ix/21 &gt; 1x21.
We say that A,11 is a derivation in the maximal local mode (the word-selector x2
</bodyText>
<equation confidence="0.8080535">
is maximal in Si) and &gt;A4g is a derivation in the maximal global mode (the word-
selector x2 is maximal with respect to all selectors S1, • • • , SO.
For a E {M/, Mg}, we denote:
L„ (G) = z E V* I w = z, for some w E 21}.
</equation>
<listItem confidence="0.968106">
• If in a grammar G = , A, (Si, , C„)), all selectors Si,.. .,S are lan-
guages in a given family F, then we say that G is a contextual grammar with F choice
(or with F selection). The families of languages L,,. (G), for G a contextual grammar
with F choice, are denoted by CL (F), where a E {in, Ml, Mg} . Here we consider F one
of the families FIN, REG only. (It is natural to deal with selectors that are as simple
as possible, otherwise the grammar is no longer of &amp;quot;practical&amp;quot; interest. Still, for the
case of regular selectors we have here a sort of two-level grammar, because in order to
completely describe a contextual grammar, we also need a grammatical description for
the selector languages. However, using a selector S, means deciding the membership
of a substring of the current string with respect to Si; when S, is a regular language,
this question can be solved in real time, using the simplest type of recognizers: a de-
terministic finite automaton. Derivations where the selectors are used in the minimal
mode (no subword of a word-selector can be a selector) are introduced by Martin-Vide
et al. (1995); we do not discuss this variant here.
3. Generative Capacity
</listItem>
<bodyText confidence="0.999727769230769">
First, we recall some results from previous papers devoted to contextual grammars of
the basic type or with maximal use of selectors, then we prove new results about the
power of the latter classes of grammars.
The relations between families of contextual languages, defined above, and be-
tween these families and families in the Chomsky hierarchy, pictured in the diagram
in Figure 1, were proved by Martin-Vide et al. (1995). An arrow from a family F1 to
a family F2 indicates the strict inclusion F1 C F2; the dotted arrow indicates an inclu-
sion not known to be proper. Families not related by a path in this diagram are not
necessarily incomparable. The families CLA4g(REG) and CF are incomparable with all
families CL,,(F), a E {in, M1}, F E {FIN, REG}; CF is incomparable with CLA4g(REG),
too.
Here are three languages used by Martin-Vide et al. (1995) in order to prove some
of these strict inclusions and incomparabilities (we will need these languages later):
</bodyText>
<equation confidence="0.950720666666667">
= lanbmanbm I n,m &gt; 11 E CLO(REG) — CLo(FIN), a,/3 c {in, Ml, Mg},
L2 = {ant) I n &gt; 1} U {ab &amp;quot; n &gt; 1} E CLmg(FIN) — CL(REG), a E {in, M1},
L3 = fx Mi(X) I X E {a, b}*} E CL,(REG) — CLin(REG), a E {MI, Mg} .
</equation>
<bodyText confidence="0.847153">
Note that languages L2 and L3 are linear, but L1 is not context-free. In Faun (1985), it
is proved that CL(FIN) — CF 0.
Here is a grammar generating the language L2 in the Mg mode:
</bodyText>
<equation confidence="0.425978">
G= ({a,b}, lab, a2b21, (fabl, {(a, A)}), ({a2b2 in&gt; 1}, {(a,b)}))-
</equation>
<page confidence="0.983572">
251
</page>
<figure confidence="0.997962714285714">
Computational Linguistics Volume 24, Number 2
CS
CL,„ (REG) CLmi (REG) CLmg(REG)
CLA41(FIN)
CL, (FIN) CF
REG
FIN
</figure>
<figureCaption confidence="0.990767">
Figure 1
</figureCaption>
<bodyText confidence="0.97135876">
Relations between families of contextual languages and families in the Chomsky hierarchy.
Indeed, for any word anb with n &gt; 1 only the first context can be used, and for any
word an bn with n &gt; 2 only the second context can be used (here, the use of selectors
in the maximal global mode is essential, in order to prevent the adjoining of the first
context to words of the form anbn , n &gt; 2, in such a way as to destroy the equality of the
number of a and of b occurrences). However, L2 V CLA41(REG) U CL, (REG). Assume
the contrary and take G&apos; = ({a, b} , A, (Si, CO, , (Sn, CH)) such that L(G&apos;) = L2, a E
{in, M1}.
In order to generate all strings a&amp;quot; b, n &gt; 1, we need a context (ai , aj), with i + j &gt; 1,
either associated with as!) for s &gt; 0 (then j = 0), or with ak, k &gt; 0. For a = in, the
contradiction is clear: strings an&apos; bn with n&apos; &gt; n can be produced in both cases.
Assume that G&apos; is used in the maximal local mode. In order to generate the strings
,n &gt; 1, we also need a context (ak, bk), k &gt; 1. This context cannot be applied to a
k+
string to which (a&apos; , al) above can be applied (from am b we get am±kbi, which is not
in Li). Therefore (ak, bk) and (al , aj) can be used independently (these contexts belong
to sets Cs and Ct in G&apos;, respectively, with 1 &lt; s, t &lt; n, s t). This implies that (a&apos;, aJ)
can be applied to a string OM with large enough q, again producing strings that are
not in L2.
The relationships between the family CLmg(FIN) and other families CL,„ (F), a E
{in, M1}, F E {FIN, REG}, as well as between CLmg(FIN) and CF, are not settled by
Martin-Vide et al. (1995). We solve most of these problems here.
We start with two results having a linguistic relevance. The first one points out
a surprising limitation of contextual grammars with global maximal use of selectors:
there are center-embedded structures that cannot be generated by such grammars even
</bodyText>
<page confidence="0.955637">
252
</page>
<note confidence="0.550178">
Marcus, Martin-Vide, and Pun Contextual Grammars
</note>
<bodyText confidence="0.723652">
when regular selectors are used. Specifically, let us consider the language:
</bodyText>
<equation confidence="0.752587">
L4 = {an cbm cbm can I n, m &gt; 1}.
</equation>
<bodyText confidence="0.8673545">
Note that this is a linear language in Chomsky&apos;s sense and that it belongs to the
families CL, (FIN) and CLAN (FIN). For the grammar:
</bodyText>
<equation confidence="0.883611">
G = ({a, b, c} , facbcbcal, With}, {(b, b)}), ({acbcbca}, {(a, a)})),
</equation>
<bodyText confidence="0.9235032">
we have Lin (G) = L; the context (a, a) cannot be used after using the context (b, b),
hence the grammar G can first generate any word of the form an cbcbcan , n &gt; 1, then
any word dice cbm can , n, m &gt; 1; each selector consists of one word only, hence the local
maximal use of selectors imposes no restriction, L,n (G) = LA4/ (G).
In contrast to these observations, we have the following result:
Theorem 1
The language L4 is not in the family CLmg(REG).
Proof
Assume that L4 = Lmg(G) for some grammar G = ({a,b, c} , A, (Si , , (Sk, Ck)). In
order to generate strings a&amp;quot; cbm cbm can with arbitrarily large n and m we need:
</bodyText>
<listItem confidence="0.766479333333333">
• contexts (a&apos;, ai) associated with selectors of the form aPcbrcbrcaq, for some
p, q &gt; 0, r &gt; 1,
• contexts (bi b ) associated with selectors of the form bscbt , for some
s, t&gt; 1 (if one of s, t is zero, then we can introduce occurrences of b in
front of the first occurrence of c or after the third occurrence of c in
strings an cbm cbm can with large enough m).
</listItem>
<bodyText confidence="0.9304205">
If in a derivation we use an a-context, then no b-context can be used at a subsequent
step: either the a-context is still applicable or an a-context with a larger selector is
applicable, while the central subword cbm cbm c has not been changed; the b-contexts
use proper subwords of cbm cbm c, hence they are not allowed in the Mg mode.
Therefore, the derivations in G start by a phase:
W &gt;70g an&apos; cb&apos;n cb&apos;n can&apos;
where only b-contexts are used, then (possibly) continue by a phase:
ani cbm cbm can&apos; =t1g acbmcbmca,
where only a-contexts are used (and the subword cbm cbm c is not modified).
For a given n &gt; 1, denote:
</bodyText>
<equation confidence="0.975791">
M(n) = {x E Lmg(G) I w &gt;1/1g X by using only b—contexts,
w c A, w = an cbm cbm can , for some m&gt; 1}.
Let:
no = max{n M(n) is infinite}.
</equation>
<page confidence="0.937172">
253
</page>
<note confidence="0.36171">
Computational Linguistics Volume 24, Number 2
</note>
<equation confidence="0.8214885">
All strings in M(no) are of the form anocbm cbn cano ,m &gt; 1. Denote:
A/11(n0) = {ZU E LMg(G) I 4) Mg x by using a b-context, x e M(no)}.
</equation>
<bodyText confidence="0.99958416">
Because M(no) is infinite and we use a finite set of contexts, the set A/1/(n°) is also
infinite. Because each w E M(no) is derived using a b-context, it follows that no a-
context can be applied to w, otherwise the derivation is not done in the Mg mode.
However, for each m such that a&amp;quot;cbmcbmcano E AT(no), all strings z = an cbm cbm can ,n &gt;
1, are in L4. Let us denote their set with M&amp;quot; (no). In order to generate such strings
with arbitrarily large n, we have to use a-contexts. Because such a context (a&apos;, a&apos;) E
(p(aPCbmCbmCaq) cannot be applied to anocbmcbmcano, it follows that at least one of the
relations p &gt; no, q &gt; no holds. We have seen that a-contexts can be used only after
b-contexts. Therefore, the strings in M&amp;quot; (no) must be generated starting from axioms
anicbmicbmIcanl with n1 &gt; no. By the choice of no, such axioms are able to generate only
finitely many strings of the form anicbmcbmcani. The set M&amp;quot; (no) is infinite, the set of
axioms is finite, hence M&amp;quot; (no) cannot be covered by strings generated in this way, a
contradiction. The equality L4 =- Livig(G) is not possible, L4 0 CLA4g(REG).
Note that the type of selectors plays no role in the previous argument, hence
L4 CLmg(F), for any family F of languages.
The fact that L4 E CLa(FIN) — CLmg(REG), for a E {in, Mi}, should be contrasted
with the fact that L3 E CI„(REG) — CL,n(REG), for a E {MI, Mg}: there are center-
embedded constructions that cannot be handled by grammars with global maximal use
of selectors, but the &amp;quot;total mirror language&amp;quot; can be generated when using a maximal
restriction and not in the free case.
On the other hand, the family CLmg(FIN) goes surprisingly far in the Chomsky
hierarchy. The result will be stressed, indirectly, in Section 6, but we prefer to also
give an example of a language that belongs to the family CLmg (FIN) and looks quite
complex. Together with the previous theorem, this example settles the relationships
between the family CF and families CL,. (F), a E {Mg, M1}, F E {FIN, REG}.
</bodyText>
<subsectionHeader confidence="0.571637">
Theorem 2
</subsectionHeader>
<bodyText confidence="0.98988">
The family CLmg(FIN) contains non-context-free languages.
</bodyText>
<subsectionHeader confidence="0.822196">
Proof
</subsectionHeader>
<bodyText confidence="0.856978">
Consider the grammar:
</bodyText>
<equation confidence="0.837022666666667">
G = ({a, b} , laabl , ({b} , {(a,.\), (b, a)}), ({abb} , {(bb , a)}), ({ba, babb} f(A,A)1)).
Let us examine the intersection of the language Lmg(G) with the regular language:
R (bba)± a+ .
</equation>
<bodyText confidence="0.731332666666667">
The family CF is closed under intersection with regular languages; if Lmg(G) E CF,
then Lmg(G) n R E CF. However, this does not hold, because, as we shall prove below,
we obtain:
</bodyText>
<subsubsectionHeader confidence="0.214128">
Lmg(G) n R {(bba)r an I n &gt; 2}.
</subsubsectionHeader>
<bodyText confidence="0.94119375">
This is not a context-free language.
Indeed, examine the derivations in G, with a global maximal use of selectors,
starting from aab and leading to words of the form (bba)&amp;quot; am ,n,m &gt; 1 (such words are
elements of R).
</bodyText>
<page confidence="0.978414">
254
</page>
<note confidence="0.662773">
Marcus, Martin-Vide, and Nun Contextual Grammars
</note>
<bodyText confidence="0.995918">
As an illustration of the arguments that follow, let us consider a short example:
take n = 3. We have to proceed as follows:
</bodyText>
<construct confidence="0.70951725">
aaabba &gt;Mg aabbabbaa abbabbaabbaa
Mg bbabbaabbaabbaa &gt;mg bbabbabbabbaaabbaa
Mg bbabbabbabbaabbabbaaa &gt;mg bbabbabbabbabbabbaabbaaa
Mg bbabbabbabbabbabbabbabbaaaa = (bba)2&apos; (23.
</construct>
<bodyText confidence="0.997273193548387">
(The selector used at each step is underlined.)
Let us now examine a derivation of a general form. The first context of the first
production, (a, A), can be adjoined to occurrences of the symbol b only when these
occurrences do not have a symbol a to their right-hand side; in such a case, the selector
ba is present, which is larger than b, thus preventing the use of the first production.
Thus, from aab we can produce any word of the form anb, n &gt; 2.
To such a word we can adjoin the context (b, a), thus obtaining ab ba. From now
on, the selector b can never be used: the symbols b in pairs of b occurrences will not be
separated, and there can never be four adjacent occurrences of b. (This could happen
only when two symbols b are already present and two further ones are introduced
by the context (bb, a); this means that we would have started from a word xbbabbx&apos; ,
but with such a word we are not allowed to use the selector abb, because the longer
selector babb is present.) Therefore, the right occurrence of b in each pair bb is followed
by an occurrence of a, and thus the use of the selector b is forbidden by the selector
ba in the last production, whereas for the left b in a pair bb we cannot use the selector
b, because the selector abb is present. Thus, from a word of the form anbba we have
to obtain a word in R using only the second production of the grammar. This means
that every occurrence of a will go to the right, using this production. Crossing a pair
bb, each occurrence of a introduces one more pair bb, as well as one more a. Hence,
each use of the production doubles the number of occurrences of the pair bb. Since
we eventually get a word starting with bb, this means that one pair bb has crossed
all occurrences of a; at every step, one further a is introduced. One copy of a remains
in triples bba, the other must migrate to the suffix of the word. Consequently, the
obtained string is of the form (bba)maP, where m is the number of times of using the
production ({abb}, {(bb, a)}) minus one, and p is the number of initial occurrences of
a, that is p = n. This implies that the occurrence of a immediately to the left-hand side
of the initial pair bb has crossed one pair bb (doubling it), the next one has crossed
two pairs bb (doubling them), and so on until the leftmost occurrence of a, the n-th
one. In total, we have n doublings, because we started from anbba. This means that m
above is equal to 2n , that is the obtained word is of the form (bba)2&amp;quot; a&amp;quot; . This completes
the proof.
</bodyText>
<subsectionHeader confidence="0.595477">
Corollary 1
</subsectionHeader>
<bodyText confidence="0.970946">
The families CF, CLmg(FIN) are incomparable.
</bodyText>
<equation confidence="0.716624">
Proof
Theorem 1 shows that CF — CLmg(FIN) 0 0, whereas from Theorem 2 we know that
CLmg(FIN) — CF 0.
</equation>
<bodyText confidence="0.792683">
Returning to the diagram in Figure 1, we now know that any two families not
linked by a path in this diagram are incomparable, except the pairs (CL,n(REG),
CLmi(FIN)), (CL(REG), CLAN(REG)), and (REG, CLmg(FIN)), (REG, CLmg(REG)).
</bodyText>
<page confidence="0.979678">
255
</page>
<note confidence="0.396138">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.980664333333333">
For the convenience of the reader, we list all pairs (F1, F2) of families of contextual
languages from this diagram that are not known in be included in each other, specify-
ing in each case the known incomparability arguments in the form L, (F1, F2), L&apos;, with
the following meaning: L E F1 —F2, L&apos; E F2 — F1. When L or L&apos; is not specified, it means
that no such language is known. The languages L1, L2, L3, L4 used are those mentioned
above:
</bodyText>
<equation confidence="0.9821877">
L4, (CL,,, (FIN), CLA4g(FIN)), L2
L4, (CL,„ (FIN), CLA4g(REG)), L2
L1, (CLin(REG), CLmi (FIN))
(CL (REG), CLAv(REG)), L3
L4, (CL,( REG), CLA4g(FIN)), L2
L4, (CL( REG), CLA4g (REG)), L2
L4, (CLA4/ (FIN), CLA49 (FIN)), L2
L4, (CLA41(FIN), amg(REG)), L2
L4, (CLmi (REG), CLA4g(FIN)), L2
L4, (CLA4/ (REG), CLA4g(REG)), L2.
</equation>
<bodyText confidence="0.9993752">
Both families CLm(FIN) and CLmg(FIN) contain non-context-free languages, but there
are linear languages not in CL,,, (REG), CLA4/(REG) (for example: L2) or in CLA4g(REG)
(for example: L4). We conjecture that REG C CLA4g(FIN), however, the construction
from the proof of the inclusion REG C CL, (FIN) from Ehrenfeucht, Faun, and Rozen-
berg (1997) cannot be directly modified for the Mg case.
</bodyText>
<sectionHeader confidence="0.582638" genericHeader="introduction">
4. On the Linguistic Relevance of Contextual Grammars with Maximal Use of
Selectors
</sectionHeader>
<bodyText confidence="0.999949076923077">
With regard to their linguistic foundations, contextual grammars are closely related
to American distributional linguistics, the potential of which they try to exploit. Let
us quote some words of Manaster Ramer (1994, 4): &amp;quot;It is my contention that, until
the early 1960&apos;s, the situation, as revealed by a close mathematical analysis of the
underlying issues, was this: (a) there was no basis for concluding that &apos;in principle&apos;
natural languages were anything but context-sensitive (and it should have been clear
that nothing was likely to change that result), (b) it was clear that phrase structure was
inadequate in terms of its descriptive devices, and (c) it should have been clear (since
it had been admitted in print) that phrase structure left out some of the descriptive
devices of immediate constituent analysis. The right thing to have done would have
been to pursue a more accurate formalization of immediate constituent analysis, and
a more detailed analysis of just how much context-sensitivity was really required for
natural languages.&amp;quot;
The generative process in a contextual grammar is based on two dual linguistic
operations, which are among the most important in both natural and artificial lan-
guages: insertion of a string in a given context and adding a context to a given string.
Descriptive distributional linguistics developed in the U.S.A. in the 1940s and 1950s
is entirely based on these ideas. To some extent, a similar idea is behind some as-
pects of Chomsky grammars; for instance, the difference between a context-free and
a context-sensitive rule is that a certain substitution, generally valid in a context-free
grammar, becomes possible only in a given context as soon as the grammar is no
longer context-free, but context-sensitive.
Any derivation in a contextual grammar is a finite sequence of such operations,
starting from an initial finite stock of strings, simple enough to be considered primitive
well-formed strings (axioms).
Given a language L over the alphabet V. each context (u, v) over V selects a set of
</bodyText>
<page confidence="0.983119">
256
</page>
<note confidence="0.736002">
Marcus, Martin-Vide, and Paun Contextual Grammars
</note>
<bodyText confidence="0.99681793877551">
strings x such that uxv e L. We say in this case that x is accepted by (u, v) in L or that
(u, v) accepts x in L. Any set C of contexts over V selects the set X of those strings that
are accepted in L by any context in C. Obviously, X is here maximal, because it is the
set of all strings with the relevant property. The dual phenomenon is the following:
each string x over V selects the set C(x) of those contexts that accept x in L. To any
set E of strings over V we associate the set of contexts accepting in L any string in E.
In short, given a language L, each set of contexts (strings) selects, with respect to L,
a set of strings (contexts); in other words, each language over V determines a precise
interplay of strings and contexts over V.
A natural question can be raised: could we now follow an inverse itinerary, by
starting from a finite stock A of strings (over V) simple enough to be considered
primitive well-formed strings (axioms), and by considering a finite set of couples
(Si, C,), 1 &lt;i &lt; n, where S, is a set of strings, while C, is a finite set of contexts, to ask
what is (are) the language(s) with respect to which C, selects Si, 1 &lt; i &lt; n? The idea
of a contextual grammar, in its various forms, is born from the attempt to answer this
question. A series of details about this topic can be found in Marcus (1997).
Let us consider again the three non-context-free constructions in natural languages
mentioned in the introduction. The (non-)context-freeness of natural and programming
languages has been investigated since the early sixties (Bar-Hillel and Shamir [1964];
Floyd [1962], among others). While for Algol 60 and for all advanced programming
languages, the question has been settled from the very beginning—these languages
are not context-free—a long debate was necessary concerning natural languages. We
shall use information about this question from Gazdar and Pullum (1985); the reader
might also consult Pullum (1985, 1986, 1987) and Pullum and Gazdar (1982).
The general technique in approaching this problem is the same for both program-
ming and natural languages. Look for special constructions that seem, intuitively,
to require a non-context-free competence. In order to extract them from the studied
language, use an intersection with a regular language. Because CF is closed under
intersection with regular sets, if the result is not context-free, then we have a proof
that the initial language is not context-free.
The basic constructions of this type are duplication of arbitrarily long subwords,
dependencies (agreements) between crossed pairs of subwords, and dependencies act-
ing on (at least) three correlated subwords. The basic features of programming lan-
guages requiring dependencies are the necessity of declaring identifiers and names of
procedures, and of defining labels.
In natural languages, such replications and dependencies can appear either at the
level of the vocabulary or at the level of the sentences in a given language. The question
is not simple, because it might not be clear what is grammatical and what is not
grammatical with respect to a natural language. However, there are now convincing
examples of non-context-free constructions in many languages. At the level of the
vocabulary, the case of Bambara, a language from the Mande family in Africa (Culy
1985) is illustrative: compound words of the form string-of-words-o-string-of-words are
possible in this language. The corresponding formal language consists of words of the
form xcx, for x an arbitrarily long word over an alphabet not containing the symbol c
(this symbol corresponds to the separator o in the Bambara construction). Because we
can always codify words using two symbols, we work here with the language:
= {XCX X E 0,1311.
Another non-context-free construction has been found in a dialect of German spo-
ken around Zurich, Switzerland (Shieber 1985; Pullum 1985), which allows construc-
</bodyText>
<page confidence="0.980546">
257
</page>
<subsectionHeader confidence="0.285998">
Computational Linguistics Volume 24, Number 2
</subsectionHeader>
<bodyText confidence="0.96963425">
tions of the form NP; NP &apos;j Va&apos;n&apos; , where NP, are accusative noun phrases, NPd are
dative noun phrases, Va are accusative-demanding verbs, Vd are dative-demanding
verbs, and the numbers match up, that is m = n = n&apos;. This leads to languages of
the form:
</bodyText>
<equation confidence="0.980726">
M2 = fambnedn I n, m &gt; 11.
</equation>
<bodyText confidence="0.922255363636364">
Both of these constructions can be easily found in programming languages, too.
The proof of Floyd (1962) that Algol 60 is not context-free leads to a language of Mi
type. Intersecting any Algol-like language with a regular language consisting of strings
of the form:
begin; real x; ..., go to labell; y := 1; ...label2 : ...; end
we force the equalities x = y,labell = labe12, hence a language like M2 is obtained.
If, however, we intersect an Algol-like language with the regular set of strings of
the form:
begin; real x; y := z; end
then we force the equalities x = y z, which can be translated into a language of the
form:
</bodyText>
<equation confidence="0.976344">
M3 = faubncn I n &gt; 11.
</equation>
<bodyText confidence="0.993945107142857">
Concerning a natural language version of this form, Manaster Ramer (1993, 12) says:
&amp;quot;The interaction of two different constructions (coordination and serial-verb forma-
tion) gives rise to patterns essentially of the form an bnc&amp;quot; (and, more generally, (anb)m)
in Dutch and German, but there is no indication that any one construction in any
language has this property.&amp;quot; Also according to Manaster Ramer (1994, 21), &amp;quot;Columnar
structures like eV cn , an bn cndn , etc. (for all positive n) seem not to exist by themselves
as constructions but do appear as compositions of two constructions (in particular, the
serial verb construction of German or the cross-serial construction of Dutch together
with coordination of the verb clusters) ... in these terms, natural languages possess
an important property different from the usual formal languages. Namely, in natural
languages individual constructions often have forms which no natural language, taken
as a whole, can have. Thus, reduplication is common (probably universal), but there
is no natural language which is made up, in its entirety, of reduplications.&amp;quot;
Counterparts of these much-used examples of non-context-free languages can be
identified in other areas, such as the semiotics of folklore (Marcus 1978).
None of the languages M1, M2, M3 is context-free, and this is an easy exercise in
any formal language textbook. Moreover, M1 and M3 belong to no family CL, (F),
for arbitrary F (even more general than FIN and REG). The argument is similar in all
cases: in the free mode of using selectors, one cannot sense the place where the context
must be added without producing a parasitic word. Take, for instance, the case of M3.
If, in order to introduce arbitrarily many occurrences of a, we use a context (a&apos;, b&apos;c&apos;),
i &gt; 1, associated with words of the form aibk,j,k &gt; 0, then ai+k+lbi+k±iCi±k±1 &gt;in
ak±la1aibkbictbi+lc1±k±1 is a correct derivation, but the word produced is. not in M3.
A similar parasitic word is obtained if we use a context of the form ((el,&apos; , c&apos;), i &gt; 1,
associated with bi ck , j, k &gt; 1, and for contexts of the form (a&apos;bi bk cl), = j + k = 1 &gt; 1,
associated with words bP,p &gt; 0. At least one such context is necessary, hence no
grammar can generate M3 in the free mode without producing parasitic strings.
However, all three languages mentioned above can be generated by using the selectors in
</bodyText>
<page confidence="0.99274">
258
</page>
<note confidence="0.919215">
Marcus, Martin-Vide, and Nun Contextual Grammars
</note>
<tableCaption confidence="0.999199">
Table 1
</tableCaption>
<table confidence="0.925543">
Languages generated by various contextual grammars.
CL, „(FIN) CL, „(REG) CLAN(FIN) CLAN(REG) CLmg(FIN) CLmg(REG)
Mi No No No Yes No Yes
tA No No No No No ?
M2 No Yes No Yes No Yes
M3 No No No Yes No Yes
M4 Yes Yes Yes Yes Yes Yes
A&lt; No No No Yes No Yes
the maximal mode, both in the local and the global way. Here are grammars proving this
</table>
<equation confidence="0.915697">
assertion:
G1 = ({a, b, c}, {c}, ({c} fa, b}*, f(a, a), (b, b)1)),
G2 = ({a, b, c, d} , {abcd}, (ab+ c, {(a, c)}), (bc+d, f(b, d)1)),
G3 = ({a, b, c}, {abc} , (b+, {(a, bc)})).
</equation>
<bodyText confidence="0.999882315789474">
The reader can easily check that Lmi(Gi) = Lmg(G,) = M1, = 1, 2, 3. Notice how
simple these grammars are, even compared with regulated context-free grammars
(Dassow and Patin 1989), which, in some sense, are specially designed for handling
such languages.
What is significant here is that all of these languages, hence all of the subjacent
syntactic restrictions, can be handled by contextual grammars with both a local and
a global maximal use of selectors, although—as we have seen—the overall generative
power of such grammars is not &amp;quot;too large&amp;quot;: there are context-free languages (even
linear ones: remember the language in Theorem 1) that they cannot generate. On the
other hand, the power of these grammars is not &amp;quot;too small.&amp;quot; Theorem 2 from Section 3
and Theorems 3, and 4 from Section 6 explain the meaning of this statement.
At the beginning of Section 3, we mentioned that M2 E CL, (REG). This also fol-
lows from grammar G2, for which we have L, (G2) = Lmg (G2) = LM, (G2): the two
selectors are disjoint and their elements are &amp;quot;marked strings,&amp;quot; bounded by fixed sym-
bols, hence no selector string is the subword of another selector string. The maximality
feature is, however, essential for G1 and G3, because, as we have mentioned before,
the languages M1 and M3 cannot be generated by contextual grammars working in
the in mode.
Consider now the &amp;quot;unmarked&amp;quot; variant of the language M1 above, that is:
</bodyText>
<equation confidence="0.683988">
ATI = {xx I x E {a,b}*},
</equation>
<bodyText confidence="0.776412">
as well as the marked and unmarked mirror image languages:
</bodyText>
<equation confidence="0.9340835">
M4 = {XC Mi(X) I X E {a,b}*},
= mi(x) x E {a,b}*}.
</equation>
<bodyText confidence="0.95094175">
For reference, we indicate the possibility of generating these languages by contextual
grammars of various types in Table 1.
Proofs of the assertions represented in Table 1 can be found in Martin-Vide et al.
(1995), some of them were mentioned above, or can be easily found by the reader. For
</bodyText>
<page confidence="0.987164">
259
</page>
<note confidence="0.62781">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.999907571428571">
the sake of the completeness, some hints for the proofs not discussed here are given
in the appendix.
It is worth emphasizing the clear difference between marked and unmarked lan-
guages: the former are easier to handle than the latter. There is also a clear difference
between contextual grammars and Chomsky grammars, with respect to the languages
listed above. For instance, M4 and A are of the same complexity when they are gener-
ated by Chomsky grammars (both of them are linear and can be generated by almost
identical grammars); in the framework of contextual grammars, M4 and Mi4 are sig-
nificantly different. This also holds for MI and A. The case of contextual grammars
is closer to our intuition, because the existence of a marker makes it very easy to
check the property defining the strings in our languages (knowing the &amp;quot;center,&amp;quot; we
can directly check the relation between the two halves of the strings).
It is known that the language M3 mentioned above cannot be generated by a tree
adjoining grammar (TAG) in the pure form introduced by Joshi, Levy, and Takahashi
(1975), but CF c TAL, where TAL is the family of languages generated by TAGs
without additional features (see also Section 21.2 in Partee, ter Meulen, and Wall
[1990]). In view of the languages L2 and L3 in Section 3, which are context-free but not
in CLmg(REG), or CLmi(REG), respectively, it follows that TAL is incomparable with
each of the families CLmi (REG), and CLmg(REG). However, TAGs with constraints (for
instance, with null adjoining contraints; see, for example, Joshi [1987] and references
therein) can generate all languages M1, M2, and M3; hence, a proper superfamily of
TAL is obtained. The relationships between such enlarged TAL families and families
of contextual languages are not settled yet.
An important question in this framework is whether or not the languages in the
families CL (REG), a E {Ml, Mg}, are mildly context-sensitive. It is obvious that, by
definition, contextual languages have the bounded growth property: the set of contexts
is finite, passing from one string to another means adjoining of a context from a finite
set, and all generated strings belong to the language. However, we do not know
whether or not the languages in families CL (REG), c E {Ml, Mg} are parsable in
polynomial time.
In general, the parsing of languages generated by contextual grammars (of any
type, not only with maximal use of selectors) is a research area still open. There are
several attempts to define contextual automata (see, for example, Faun [1982], Janar et
al. [1996], and Miquel-Verges [1997]). Some of them characterize a number of families
of contextual languages, and some of them recognize families that do not correspond to
classes of contextual grammars. However, no systematic study of parsing complexity
has been done, even for basic classes of contextual grammars. (Of course, because
in contextual grammars we do not have erasing operations but only adjoining, we
always generate context-sensitive languages, hence membership is decidable.)
The only complexity results known at the moment concern external contextual
grammars with regular (even context-free) selectors, and a variant of internal contex-
tual grammars with regular selectors used in a &amp;quot;localized&amp;quot; manner: the selector used
at any derivation step should &amp;quot;touch&amp;quot; the context used at the previous step. Ilie (1997a,
1997b) proved that the parsing of the languages generated by such grammars can be
done in polynomial time.
Let us close this section with the observation that contextual grammars have
another property much discussed recently: they are lexicalized (we might say &amp;quot;fully
lexicalized&amp;quot;), as each of their productions (pair selector-context) consists of terminal
symbols only.
</bodyText>
<page confidence="0.994133">
260
</page>
<note confidence="0.895262">
Marcus, Martin-Vide, and Pun Contextual Grammars
</note>
<sectionHeader confidence="0.403438" genericHeader="method">
5. Attempts to Associate a Structure to Contextual Languages
</sectionHeader>
<bodyText confidence="0.999960235294117">
In this section, we investigate further the adequacy of contextual grammars for de-
scribing the syntax of natural languages.
One of the features of context-free grammars and of other grammars based on
context-free core rules (TAGs included) most useful for linguistics is the fact that a
derivation can be described by a tree defining a structure of the generated sentence.
On this basis, the difference between weak generative capacity and strong generative
capacity was introduced: the former refers to the set of sentences that a grammar
produces, while the latter refers to the set of pairs composed by a sentence and its
phrase-structure tree.
Only very recently (see Martin-Vide and Faun [1998]) some possibilities for intro-
ducing a structure to the words generated by contextual grammars were considered.
We present here some ideas from Martin-Vide and Faun (1998), without entering into
details; research is still in progress. We only want to show that various natural solu-
tions exist for structuring contextual languages. For instance, a tree can be associated
to a derivation in a contextual grammar, as we describe below.
Consider the parentheses [ and ] and denote by B their set. A string w E (Vu B)*,
where V is an alphabet, is said to be minimally Dyck covered if:
</bodyText>
<listItem confidence="0.978431666666667">
1. w can be reduced to A by using reduction rules of the form [x] A, for
x E V+;
2. if w = w1lw2[w3, with w1, W3 E (V B)* and w2 E V*, then w2 = A.
</listItem>
<bodyText confidence="0.99921975">
We denote by MDC(V) the language of all minimally Dyck covered strings over
the alphabet V.
To any string x E MDC(V) we can associate a tree r(x) with labeled edges in the
following way:
</bodyText>
<listItem confidence="0.967733545454545">
• draw a dot representing the root of the tree; the tree will be represented
with the root up and the leaves down;
• scan x from the left to the right and grow r(x) according to the following
two rules:
• for each maximal substring [w of x, for W E V* (hence after w we find
either [ or ]), we draw a new edge, starting from the current point of the
partially constructed r(x), marked with w on its left side, and placed to
the right of the currently constructed tree;
• for each maximal w], w E V*, not scanned yet (hence, either before w we
find ], or w = A and to the left of] we have a substring [z for some z E V*
already scanned), we climb the current edge, writing w on its right side.
</listItem>
<bodyText confidence="0.946508">
Here is a simple example. The tree corresponding to the string:
</bodyText>
<equation confidence="0.916496">
x = [a[ab][ab[ab[c]blb]a][a]
</equation>
<bodyText confidence="0.896835333333333">
(which is clearly in MDC({a, b, c})) is presented in Figure 2. The nodes are numbered
in the order of producing them (1 is the root).
A bracketed contextual grammar is a construct:
</bodyText>
<equation confidence="0.884877">
G = (V , A, (Si, CO • • • , n CO),
</equation>
<page confidence="0.951572">
261
</page>
<figure confidence="0.761">
Computational Linguistics Volume 24, Number 2
</figure>
<figureCaption confidence="0.991519">
Figure 2
</figureCaption>
<bodyText confidence="0.990802">
Tree corresponding to the string x = Ea[ab][ab[ab[c]b]bla][a].
where V is an alphabet, A is a finite subset of MDC(V), Si C MDC(V), and C, are finite
subsets of V* x V* — (A, A), for all 1 &lt; i &lt; n; in turn, n &gt; 1.
For x, y E MDC(V) we define:
</bodyText>
<equation confidence="0.6008025">
X y
iff x = x1x2x3, y = [ux2v]x3, where xi, x3 E (VUB)*, X2 E MDC(V), and x2 E Si, (14,V) E
</equation>
<bodyText confidence="0.919459">
Ci, for some 1 &lt; i &lt; n. (Clearly, if x E MDC(17) and X y, then y E MDC(V), hence
the definition above is consistent.)
The string language generated by a bracketed contextual grammar G = (V. A,
(S1, C1), . . . , (S, C)) is defined by:
</bodyText>
<equation confidence="0.948162">
L(G) = { prv(z) I w == z, for some w E A},
</equation>
<bodyText confidence="0.968954666666667">
where prv(z) denotes the projection of Z E (V U B)* on V. that is the string in V*
obtained by removing [ and I from z.
We can also associate to G the bracketed language BL(G) defined by:
</bodyText>
<equation confidence="0.869802">
BL(G) = {(prv(z), r(z)) I w ==. z, for some W E A}.
</equation>
<bodyText confidence="0.999321375">
Note the fact that each string in L(G) is paired with a tree in BL(G); however,
the string should be read on the edges of this tree, not on leaf nodes as in the case
of derivation trees of context-free grammars. The linguistic significance of such a tree
is not yet clear to us, hence we do not insist on this idea (the ambiguity of contex-
tual grammars and languages can be defined in this framework, but how the tree
illuminates the grammatical structure of a sentence remains to be clarified).
Another idea considered by Martin-Vide and Nun (1998), closer to linguistics,
is to introduce a dependence relation on the set of symbols appearing in axioms,
</bodyText>
<page confidence="0.988663">
262
</page>
<note confidence="0.762754">
Marcus, Martin-Vide, and Pun Contextual Grammars
</note>
<bodyText confidence="0.903243">
selectors, and contexts of a contextual grammar. We present some of the details of this
idea informally below.
Consider an alphabet V and a string x E V*. We denote:
</bodyText>
<equation confidence="0.985818">
M(x) -= {1, 2, ..., fx1}
</equation>
<bodyText confidence="0.998474708333333">
and we write x =-- x(1)x(2)...x(n), for n = (xi/x(0 E V,1 &lt; i &lt; n. Any antireflexive
relation on M(x) is called a dependence relation on x. Let p, be such a relation (antire-
flexivity means i Px i for no value of i). The pair (x, px) is called a structured string. If
Px j, then we say that x(j) depends on x(i). Let us denote by g the transitive closure
of px. If i p-xf- j, then we say that x(j) is subordinate to x(i). A structured string (x,Px)
can be represented in a graphical form by writing the elements x(1), ,x(n) of x in
a row and drawing above them arcs (x(i), x(j)) for i Px J. A structured string (x, p,) is
called a simple string of center x(io) if the graph associated to it as described above
is a tree with the root marked with x(i0) (the center corresponds to the predicative
element of a sentence).
The notion of a structured string is well-known in linguistics: see, for example,
Chapter VI of Marcus (1967). A related notion has been recently considered, that of
a link grammar: see Sleator and Temperley (1991), or Grinberg, Lafferty, and Sleator
(1995). In a link grammar, the elements of a sentence are correctly related in a linkage,
according to a pairing of left and right connectors given for each word in the dictionary,
providing that the obtained dependence relation has several properties: the associated
graph is connected, planar, etc. Because we do not investigate here the possibility of
producing correct linkages, in the sense of Sleator and Temperley (1991), by using
contextual grammars (such results appear in Martin-Vide and Faun [19981), we do not
formally define the notion of a link grammar.
For a structured string (x,Px), x E V+, and a substring y of x, we denote by Pxly
the restriction of p, to y, defined in the natural way (we remove the symbols of x not
appearing in y and we collect the remaining pairs of Px).
Now, a structured contextual grammar is a construct:
</bodyText>
<equation confidence="0.98606">
G = (V,A,P),
</equation>
<bodyText confidence="0.975201333333333">
where V is an alphabet, A is a finite set of structured strings over V. and P is a finite
set of triples of the form (x,(u,v); Puxv), with x E V+, (u, v) E V* x V*, and puxv a
dependence relation over uxv such that puI, = 0.
The elements of A are called axioms, the triples in P are called productions; in a
production (x, (u, v); Puxv), the string x is the selector, (u, v) is the context and puxv is a
relation defining the structure of uxv; note that no dependence is considered between
the elements of x. (Thus, we consider here only grammars with finite selectors.)
The derivation relation is defined (only for structured strings) as follows: for
(X,px),(y,py),X,y e v+, we write:
</bodyText>
<equation confidence="0.986359333333333">
Px) G (y, Py)
iff x -= x1x2x3, y = xi ux2vx3, for x1, X3 E V* and (x2, (u, v); pux,v) E P. such that o I
y,x1x2x3
</equation>
<bodyText confidence="0.994522">
and Py lux2o = Pux2v. In words, the string x is enlarged with the context (u, o) and
the structure of x is extended according to the dependencies imposed by pux2o; due
to the restriction pux,v Ix, = 0, the dependencies in x are not modified when adjoining
u, v. The elements of x2 can be linked to elements of x1, x3, but the elements of u,v
participate only in dependencies with elements of the selector string x2.
</bodyText>
<page confidence="0.995438">
263
</page>
<figure confidence="0.9972375">
Computational Linguistics Volume 24, Number 2
a
a
a
</figure>
<figureCaption confidence="0.818142">
Figure 3
First three strings generated by G1.
</figureCaption>
<bodyText confidence="0.9260008">
The string language generated by G is
L(G) = {w E V* I (x, px) (w, pw), for some (x,Px) c Al.
The language of structured strings generated by a grammar G as above is:
SL(G) = {(w, pu,) I (x, px) (w, pw), for some (x, px) E A}.
Let us examine two examples. For the grammar:
</bodyText>
<equation confidence="0.936453">
= ({a, b, c} , {(acb, f(2,1), (1,3)1)1, {(c, (a, b); 1(2,1),(1,3)1)1),
we obtain:
L(G1) = {a&amp;quot; cb I n &gt;1},
SL(G1) {(a&amp;quot; cbn , {(n + 1,i), (i,2n + 2 — i) 1&lt; i &lt; n}) jn 11.
</equation>
<bodyText confidence="0.9817578">
The first three strings generated by G1 are represented in Figure 3.
The structured strings generated by G1 are simple strings with center c; the struc-
ture graph is not planar if we preserve the order of elements of strings when writing
them in a row as above.
For the grammar:
</bodyText>
<equation confidence="0.99797075">
G2 = (fa, b, cl, {(acb, 1(2,1), (2,3)})} {(c,(a,b); {(2,1), (2,3)})}),
we obtain:
L(G2) = {an cb I n &gt; 1},
SL(G2) = {(ancb&amp;quot; {(n + (n + 1,2n + 2 — I 1 &lt; i n}) I n 11.
</equation>
<page confidence="0.997834">
264
</page>
<figure confidence="0.878904">
Marcus, Martin-Vide, and Nun Contextual Grammars
a
a a
a a
</figure>
<figureCaption confidence="0.7802705">
Figure 4
First three strings generated by G2.
</figureCaption>
<bodyText confidence="0.999957913043478">
One sees that G1 and G2 are weakly equivalent, they generate the same string language,
but they are not strongly equivalent, the structures of the same strings generated by
G1 and G2 are not identical. For instance, the first three strings generated by G2 are as
shown in Figure 4.
We again obtain simple strings with center c, but the graphs describing the struc-
ture of the strings in the representation above are planar.
These examples suggest that classes of structured contextual grammars should be
considered on the basis of a classification of the graphs associated to their generated
strings. Thus, a grammar G = (V, A, P) is said to be connected, simple, or planar if
the graphs associated to the relation describing the structure of the strings generated
by G is connected, a tree, or planar (when the string is written on a horizontal line,
as before), respectively. Moreover, we can use these properties as restrictions on the
grammar, selecting from the languages L(G), and SL(G) only the (structured) strings
whose structure graph has the properties mentioned above. Of course, many other
variants can be defined; for instance, we can consider the various types of projectivity
(progressive, regressive, strong, and so on), as investigated in Chapter VI of Marcus
(1967).
The above definitions of bracketed and structured contextual grammars can be
extended in an obvious way to grammars with maximal use of selectors. Some results
in this area can be found in Martin-Vide and Paun (1998), but a lot of questions
remain to be clarified. The main problem is to find the most useful and natural type
of structured contextual grammars for describing the structure of natural language
syntactic constructions.
</bodyText>
<page confidence="0.995855">
265
</page>
<note confidence="0.795581">
Computational Linguistics Volume 24, Number 2
</note>
<sectionHeader confidence="0.540927" genericHeader="method">
6. Representations of Recursively Enumerable Languages
</sectionHeader>
<bodyText confidence="0.9762620625">
Completing the study on (weak) generative power of contextual grammars from Sec-
tion 3, we now give a result proving the eccentric position of families of contextual
languages with regard to the Chomsky hierarchy.
Ehrenfeucht, Faun, and Rozenberg (1997) prove that each recursively enumerable
language L can be written in the form L h (h (L&apos;)), for L&apos; E CL, (FIN)and hi, h2 two
morphisms. In view of the inclusion CLin (FIN) C CLmi(FIN), this result is valid also for
L&apos; E CLAN(FIN) and L&apos; E CLAN(REG). Because CL(FIN) — CLmg(REG) 0, the result
of Ehrenfeucht, Faun, and Rozenberg (1997) is not directly valid for L&apos; E CLA4g(REG)
or L&apos; E CLmg(FIN). However, the result of Ehrenfeucht, Faun, and Rozenberg (1997)
can also be extended to these cases. Because we shall use it below, we outline here the
construction of Ehrenfeucht, PAun, and Rozenberg (1997).
Take L C T*, L E RE, and a type-0 Chomsky grammar Go = (N, T, S. P) for L.
Consider the new symbols [, I, [-, and construct the contextual grammar G with the
alphabet:
V =NUTuthl,F-1,
the starting string S. and the following productions:
</bodyText>
<listItem confidence="0.9077305">
1. ({u},{([, 1v)}), for u v E P,
2. ({4u]b{(1-, a)}), for a ENU T, u v E P,
3. ({ a E- {(I-, a)}), for a, /3 E NU T.
Consider also the set:
</listItem>
<equation confidence="0.52559325">
R={[ullu--4 vEP}U{HalaENUT}.
For each string w E R, consider a new symbol, bw; denote by D {bw I w E R} their
set. We define the coding hi : (D U T)* T* by:
hi(b) = A, w ER, hi(a) = a, a E T,
</equation>
<bodyText confidence="0.694362">
as well as the morphism h2 : (D U T)* V* by:
</bodyText>
<equation confidence="0.898317">
h2(b) = w, w ER, h2(a) = a, a c T.
</equation>
<bodyText confidence="0.999731153846154">
One obtains the equality L =
The idea is the following: h2-1 is defined on (R U T)*, hence all derivations in G
that do not produce words in (Ru T)* will be &amp;quot;lost&amp;quot;; thus, h2-1 acts like an intersection
with the regular language (R U T)*, plus the conversion of each string W E R into the
associated symbol bw. In order to obtain a string in (R u T)*, a derivation in G must
follow a derivation in Go, in the sense that each rule u v E P is simulated by a
production of type 1, ({u}, ({(I, ]v)}), thus replacing u with kilo. The parentheses [,
I &amp;quot;kill&amp;quot; the word u. Productions of types 2 and 3 allow &amp;quot;living&amp;quot; symbols a to go to
the right, across &amp;quot;dead&amp;quot; symbols; also I-. is a &amp;quot;killer,&amp;quot; specifically, of the symbol placed
immediately to its right. The requirement that a word in (R u T)* must eventually be
reached imposes the use of productions of type 1 for living u only, and the use of
productions of types 2 and 3 for living a and dead u and /3, respectively. After using
these rules, u is dead, v is living (type 1), the first a is dead, the new one is living
</bodyText>
<page confidence="0.995245">
266
</page>
<note confidence="0.880352">
Marcus, Martin-Vide, and Nun Contextual Grammars
</note>
<bodyText confidence="0.984559583333333">
(types 2 and 3). This ensures that the obtained word contains only dead symbols and
killers in words of R and living terminal symbols. By means of h1, h2-1, only the living
terminals remain.
Note that the construction of Ehrenfeucht, Nun, and Rozenberg (1997) does not
work directly for the global maximal case: the grammar Go can contain, for instance,
two rules u -4 v, u&apos; v&apos; with u a proper subword of /4&apos;; the first rule cannot be
simulated in G when u&apos; is present, because we are forced to use the maximal selector,
u&apos; in this case. However, the proof can be modified to cover the case of global maximal
selectors as well.
Theorem 3
Every language L E RE can be written in the form L = hi (h2-1(L&apos;)), where L&apos; E
CLmg(FIN) and h1, h2 are two morphisms.
</bodyText>
<subsectionHeader confidence="0.468927">
Proof
</subsectionHeader>
<bodyText confidence="0.964021">
Take L C T*,L E RE, and take a type-0 Chomsky grammar Go (N,T,S,P) for L in
the Kuroda normal form, that is containing rules of the forms:
</bodyText>
<listItem confidence="0.9755495">
1. X-YZ, X - a, X-+), for X, Y, Z N, a E T,
2. XY ZU, for X, Y, Z, L/ E N.
</listItem>
<bodyText confidence="0.938305">
(Context-free rules and non-context-free rules, respectively, all of them with left-hand
and right-hand members of length at most two.)
Take a new symbol, c T, and construct the Chomsky grammar G1 =- (NU {S&apos;}, TU
{c},S&apos;,P&apos;), where:
</bodyText>
<equation confidence="0.989589333333333">
{S&apos; Sc} U
U {XY ZU XY -+ ZU E P, X, Y , Z, U E N} U
U {Xa -&gt; xa I X -4 x is a rule of type 1 in P and a E NUTU {c}}.
</equation>
<bodyText confidence="0.99678825">
It is easy to see that L(G1) = L(G0){c}.
Now start the procedure of Ehrenfeucht, Nun, and Rozenberg (1997) from the
grammar G1, constructing the contextual grammar G exactly as in Ehrenfeucht, Nun,
and Rozenberg (1997) and extending the morphisms h1, h2 by:
</bodyText>
<equation confidence="0.999064">
hi(c) A,
h2(c) = c.
</equation>
<bodyText confidence="0.995886625">
Because all rules in P&apos;, excepting S&apos; -4 Sc, which is used only once, have left-hand
members of the same length, the maximal restriction of using the associated selectors
has no effect. Concerning selectors u and c[u], appearing in productions of type 1 and
type 2, respectively, the first selector for u is already dead (as is the case of the second
selector), so its use is illegal; it leads to nonsuccessful derivations. The symbol c is
preserved by h2-1 and it is erased by h1. Consequently, with the details of the proof
in Ehrenfeucht, Nun, and Rozenberg (1997), we obtain L = hi (h2-1(Lmg(G))), which
completes the proof.
</bodyText>
<footnote confidence="0.358654666666667">
Corollary 2
Every L E RE can be written in the form L = g(L&apos;), where L&apos; E CLmg(FIN) and g is a
generalized sequential transducer.
</footnote>
<page confidence="0.984823">
267
</page>
<note confidence="0.524402">
Computational Linguistics Volume 24, Number 2
</note>
<subsectionHeader confidence="0.517176">
Proof
</subsectionHeader>
<bodyText confidence="0.7541015">
A sequential transducer can simulate at the same time both the work of 111 and of h2-1.
These results have a rather interesting consequence.
</bodyText>
<subsectionHeader confidence="0.693195">
Theorem 4
</subsectionHeader>
<bodyText confidence="0.822202333333333">
Every family F of languages such that LIN C F c RE that is closed under direct and
inverse morphisms is incomparable with each family CLa(F&apos;), for a E {M1, Mg} and
E {FIN, REG} .
</bodyText>
<subsectionHeader confidence="0.83384">
Proof
</subsectionHeader>
<bodyText confidence="0.99974352631579">
Consider a family F with the properties mentioned above. Because LIN C F and
LIN - CL„ (REG) 0 for both a E {M1, Mg}, we get F - CL„(F1) 0 for a, F&apos; as above.
Let us now prove that also the assertion CL (F&apos;) - F 0 holds, for a and F&apos; as above.
Assume the contrary, that is, CL,(F&apos;) C F. The closure of CL,(F/) under direct and
inverse morphisms should be included in the closure of F under direct and inverse
morphisms. From Theorem 3 we know that the closure of CLa(F&apos;) under direct and
inverse morphisms is equal to RE. From the closure properties of F, the closure of F
under direct and inverse morphisms is equal to F. This implies RE C F, contradicting
the strictness of the inclusion F c RE from the theorem statement.
Important families in formal language theory that fulfill the conditions in Theorem
4 are: (1) languages generated by programmed grammars without appearance check-
ing but possibly using A-rules introduced by Rosenkrantz (1969) (they are equivalent to
many other grammars with context-free core rules applied in a regulated manner: see
Dassow and Faun [19891; (2) indexed languages (Aho 1968); (3) ETOL languages (gen-
erated by extended tabled interactionless Lindenmayer systems; ETOL is the largest
family in this area—see Rozenberg and Salomaa [1980]; and (4) other subfamilies of
ETOL (for instance, EOL). Each of the families CL„(FIN), c E {in,Mi,Mg-}, contains
(context-sensitive) languages outside these families. Therefore, the families CL,, (FIN)
occupy a quite eccentric position in the Chomsky hierarchy (Figure 5).
</bodyText>
<subsectionHeader confidence="0.412525">
7. Summary and Final Remarks
</subsectionHeader>
<bodyText confidence="0.999959933333333">
In this paper, we have continued the investigation of contextual grammars with (global
or local) maximal use of selectors, recently introduced by Martin-Vide et al. (1995). We
have mainly borne in mind issues concerning the adequacy of these grammars as an
alternative model (with respect to Chomsky grammars) for the syntax of natural lan-
guages, because &amp;quot;the arguments against the adequacy of phrase structure grammar (as
defined by Chomsky) are absolutely incontrovertible (although they also apply to full
context-sensitive grammars and to unrestricted grammars), that is, the constructions of
natural languages cannot be described in an adequate way using the descriptive mech-
anisms of such grammars.... Bizarre though it may sound, ... Bloomfield&apos;s theory of
constructions is probably the best point of departure for future work on the subject&amp;quot;
(Manaster Ramer 1994, 20). We need to keep in mind, as Manaster Ramer (1994) points
out, that &amp;quot;the kinds of mathematical models we are used to are, of course, largely de-
rived from Chomsky&apos;s early work on phrase structure, and this in turn represents ...
the formalization of a terribly diminished, impoverished, and even caricatured idea
of immediate constituent analysis, created by Leonard Bloomfield&amp;quot; (p. 22).
</bodyText>
<page confidence="0.996343">
268
</page>
<note confidence="0.93837">
Marcus, Martin-Vide, and Pain Contextual Grammars
</note>
<figureCaption confidence="0.970634">
Figure 5
</figureCaption>
<bodyText confidence="0.60357675">
Position of the families CL (FIN) in the Chomsky hierarchy.
Our essential arguments have been the following:
1. The families of contextual languages are incomparable with some basic
families in the Chomsky hierarchy (with LIN and CF) or in refinements
of this hierarchy (programmed languages, indexed languages, languages
generated by various classes of Lindenmayer systems). Some pieces of
evidence indicate that perhaps natural languages occupy a similar
incommensurate position with regard to Chomsky&apos;s classification.
</bodyText>
<listItem confidence="0.8617318125">
2. Contextual grammars with global maximal use of selectors cannot
generate all languages based on center-embedded constructions, as
Chomsky linear grammars (and TAGs) do. Such constructions seem not
to be very frequent in natural languages.
3. Contextual grammars with (global or local) maximal use of selectors can
generate, in a very easy way, the three basic non-context-free
constructions in natural languages: reduplication, crossed dependencies,
multiple agreements.
4. Contextual grammars are sensitive to using markers, languages of the
form {wcw 1 w e {a,b}*} and {wc mi(w) I w E {a, b}*} are handled more
easily (i.e., by classes of grammars with simpler features) than
{ww I w E {a, b}*} and {w mi(w) I w E {a, b}*}. This again corresponds to
our intuition, but it does not fit the Chomsky hierarchy.
5. By definition, contextual grammars are &amp;quot;fully&amp;quot; lexicalized (they use only
terminal symbols), and their languages have the bounded growth
property, which is specific to natural languages (and one of the main
</listItem>
<page confidence="0.994948">
269
</page>
<note confidence="0.726122">
Computational Linguistics Volume 24, Number 2
</note>
<bodyText confidence="0.997835833333333">
ideas behind the notion of mild context-sensitivity, see Joshi [1985]): each
word generated by a contextual grammar—excepting the axioms—is
obtained by adjoining a context from a finite set.
6. If we intend our model to convey some cognitive meaning, we must say
that the simple operation of adjoining might be closer than rewriting to
the way our brain may work when building a sentence. It is hard to
imagine our brain using auxiliary intermediate sentences of a
nonterminal type. Instead, it looks more natural, in the proper sense of
the word, to start with a collection of well-formed sentences, maybe
acquired from experience, and to produce new well-formed ones by
adding further words, in pairs that can observe dependencies and
agreements, and in accordance with specified selectors, which can ensure
the preservation of grammaticality Of course, this is only a speculation,
but it also fits with the general idea of &amp;quot;natural computation&amp;quot;: for
example, nature seems not to use the rewriting operation in the area of
genetics, where recombination (crossing over) of chromosomes is the
basic evolutionary operation (together with nondeterministic insertion
and deletion operations, which, again, are not rewriting) and where no
&amp;quot;nonterminal symbol&amp;quot; is used. Further discussion of this topic can be
found in Martin-Vide (1997).
7. A structure for the words generated by a contextual grammar can be
introduced in various ways. By parenthesizing the contexts, we get a
tree. Considering dependence relations on symbols appearing in axioms,
contexts, and selectors, we can obtain structured strings of a type well
investigated in descriptive linguistics and very similar to the
phrase-linkage structures produced by a link grammar.
A number of the previous points need further investigation. There are also several
topics that are important from a linguistic point of view and that are still poorly
investigated for contextual grammars. The main one concerns the parsing algorithms
and their complexity. Polynomial parsing algorithms were found for a few variants
of contextual grammars, which is encouraging, but the problem is still open for the
variants discussed in this paper.
The main aim of this paper was to call the reader&apos;s attention to contextual gram-
mars, to prove that they deserve further research efforts, especially in terms of their
linguistic adequacy and relevance. It is our (optimistic) belief that such efforts will be
rewarded.
</bodyText>
<sectionHeader confidence="0.7093255" genericHeader="method">
Appendix: Proofs of Some Assertions Represented in Table 1
Proof
</sectionHeader>
<bodyText confidence="0.9597835">
Assume that M1 E CL„ (FIN), a E {Ml, Mg}, take a grammar G for M1, and consider a
string of the form: zi abazbz ab&apos; caba2b2 ab 1,
for a large-enough integer i. In order to produce such a string, we need a derivation:
W = w1w2w3 &gt;ce wi uw2vw3 = z,.
It is obvious that jw21 depends on i, and so cannot be bounded; therefore G cannot
have finite selectors only.
</bodyText>
<page confidence="0.987655">
270
</page>
<note confidence="0.5929935">
Marcus, Martin-Vide, and Patin Contextual Grammars
Proof
</note>
<bodyText confidence="0.9824632">
Assume that MI e CL, „(F), for F E {FIN, REG} . Take G = ({a, b}, A, (S1, C1), . . (Sri, Ca))
such that L,, (G) = MI. There is at least one context (u, v) in G with uv A; all the
strings in M are of even length, so I uvj must be even. Take x in the selector of (u, v)
and consider the strings xa&apos; xat , i&gt; luvI. Then uxvai xai E Lin(G), so uxva&apos;xal = yy, for
y E {a, b}5. This implies:
</bodyText>
<equation confidence="0.823256375">
/ 2 = aluvi / 2 xai
that is, uxv = za , j &gt; 1. In the same way, starting from xbi xbi we get that uxv = z&apos; bk , k &gt;
1, a contradiction. As in point 1, we obtain that M CL. (FIN), a E {AN, Mg}.
Proof
Assume that M = LAN (G), for any G ({a, b} , A, (Si, C1), . . . , (Si,, Ca)) with regular
selectors. Let us denote:
co(X) = {(U,V) I (14,V) E Ci, X E S,,1 &lt;i &lt; n} , XE fa, by ,
= fx E fa , bl* I (u, v) E co(x)}, (u, v) E C,,1 &lt; i &lt; n.
</equation>
<bodyText confidence="0.982613727272727">
All strings am barn b, m &gt; 1, are in M. Take such a string with arbitrarily large m. If there
is a derivation step aq am barn b, then there is a context (u, v) = (ail ba`2 , ai3b) E co(aP),
for p &lt; q. As m = i2 + p + i3, it follows that p is arbitrarily large. The set co-1((u, v))
is regular (it is the union of a finite number of regular sets), so it contains an infinite
number of strings of the form as (we apply a pumping lemma to aP in yo-1((u, v))).
Therefore, (u, v) must be used for a maximal selector of the form at. In this way, a
string all bal2 bail can be produced, with bounded J1,J3 and arbitrarily large j2. Such a
string is not in M., a contradiction. Therefore, in the derivation of am barn b there exists
an arbitrary number of derivation steps of the form:
as bas b A41 as+kbas+k b,
with k&gt; 1 and aPbaq E v)-1 ((ak ak
</bodyText>
<equation confidence="0.75185825">
)) Consider now a string:
w = ali ba`2 bail ba12 b,
with arbitrarily large i1, i2. Each such string is in M. If aPbaq above or any other selector
ak \ \
</equation>
<bodyText confidence="0.999935666666667">
of the form arbar&apos; from (( ak)) is maximal in w, then we shall produce a string
which is not in M. On the other hand, aPbaq is a subword of w, so the selectors
included in (p-i ((dc, a&amp;quot;))
must contain a string that is a strict superword of aPbaq, in
order to prevent the generation of a parasitic word. Such a superword can only be
of the forms all be bal2 or all ball baI2 . In both cases, the middle subword, b or be b,
respectively, is arbitrarily long. As elements of a regular language, such strings have
pumping properties. Let us consider the case of ba&apos;213 (the second one is similar) This
means that all the strings of the form:
</bodyText>
<equation confidence="0.839156">
z = all bai2+rilbai2 ,
</equation>
<bodyText confidence="0.6184235">
for r &gt; 1 and all h &gt; 0, are in (p-1 ((u, v)). Take such a string z with h being large
enough to have:
</bodyText>
<equation confidence="0.965084">
i2 + rh &gt; +12.
Consider the string:
w = ai2+rh-i2bai2+rh bai2
</equation>
<page confidence="0.987286">
271
</page>
<figure confidence="0.3659435">
Computational Linguistics Volume 24, Number 2
Because:
</figure>
<equation confidence="0.8174734">
i2 + rh = (i2 + rh j2) + /2,
we have w E A. Because:
i2 + rh - j2 &gt;j1,
the context (a&apos;, 0k) is applicable to w. That is:
W &gt;M1 a12-1-rh-12+kbai2+rhbai2+k.
</equation>
<bodyText confidence="0.97273">
The string obtained is not in A, a contradiction. In conclusion, A cannot be in
CLAN (REG). The previous argument does not hold for the global maximal derivation,
so the relation A E CLmg(REG) remains open.
</bodyText>
<subsectionHeader confidence="0.624326">
Proof
</subsectionHeader>
<bodyText confidence="0.573274">
For the grammar G = ({a,b, c}, {c} , ({c} , {(a, a), (b, b)})), we have Le, (G) = M4 for all a.
0
</bodyText>
<subsectionHeader confidence="0.603426">
Proof
</subsectionHeader>
<bodyText confidence="0.9444985">
The fact that M&apos;4 CL,, (REG) is already proved in Faun (1982). As for MI, one can
easily prove that A cl CL,. (FIN), a E {MI, Mg} . On the other hand, for the grammar
</bodyText>
<equation confidence="0.8284975">
G = ({a, b} , {A},(0,1)1*, {(a, a), (b, b)1)),
we have LAN(G) = Lmg(G) = A. Hence A c CL,, (REG), a E {MI, Mg} . 0
</equation>
<sectionHeader confidence="0.834147" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999693666666667">
The authors are much indebted to three
anonymous reviewers, who very carefully
read a previous draft of the manuscript, and
who proposed a number of modifications
that have improved both the readability and
the content of this paper.
</bodyText>
<sectionHeader confidence="0.948191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99434592">
Aho, Alfred V. 1968. Indexed grammars. An
Extension of context-free grammars.
Journal of the ACM, 15:647-671.
Bar-Hillel, Yehoshua and Eliyahu Shamir.
1964. Finite state languages: Formal
representations and adequacy problems.
In Yehoshua Bar-Hillel, editor, Language
and Information. Addison-Wesley, Reading,
MA, pages 87-98.
Bala&apos;nescu, Tudor and Marian Gheorghe.
1987. Program tracing and languages of
action. Revue roumaine de
linguistique—Cahiers de linguistique
theorique et appliquee, 32:167-170.
Chomsky, Noam. 1964. On the Notion &amp;quot;Rule
of Grammar&amp;quot;. In Janet A. Fodor and
Jerrold J. Katz, editors, The Structure of
Language: Readings in the Philosophy of
Language. Prentice Hall, Englewood Cliffs,
N.J., pages 50-118.
Culy, Christopher. 1985. The complexity of
the vocabulary of Bambara. Linguistics and
Philosophy, 8:345-351.
Dassow, Jurgen and Gheorghe Pun. 1989.
Regulated Rewriting in Formal Language
Theory. Springer, Berlin.
Ehrenfeucht, Andrzej, Gheorghe Nun, and
Grzegorz Rozenberg. 1997. On
representing recursively enumerable
languages by internal contextual
languages. Theoretical Computer Science. To
appear.
Floyd, R. W. 1962. On the non-existence of a
phrase-structure grammar for Algol-60.
Communications of the ACM, 5:483-484.
Gazdar, Gerald and Geoffrey K. Pullum.
1985. Computationally relevant properties
of natural languages and their grammars.
New Generation Computing, 3:273-306.
Grinberg, Dennis, John Lafferty, and Daniel
Sleator. 1995. A robust parsing algorithm
for link grammars. In Proceedings of the
Fourth International Workshop on Parsing
Technologies, pages 111-125,
Prague/Karlovy Vary.
Harrison, Michael. 1978. Introduction to
Formal Language Theory. Addison-Wesley,
Reading, MA.
Hockett, Charles. 1970. The State of the Art.
Mouton, The Hague.
</reference>
<page confidence="0.989405">
272
</page>
<note confidence="0.874705">
Marcus, Martin-Vide, and Nun Contextual Grammars
</note>
<reference confidence="0.997604942622951">
Ilie, Lucian. 1997a. On computational
complexity of contextual languages.
Theoretical Computer Science. To appear.
Ilie, Lucian. 1997b. The computational
complexity of Marcus contextual
languages. Submitted.
Janar, Petr, Frantigek Mrdz, Martin Pldtek,
Martin Prochdzka, and Jorg Vogel. 1996.
Restarting automata, Marcus grammars
and context-free languages. In Jurgen
Dassow, Grzegorz Rozenberg, and Arto
Salomaa, editors, Developments in Language
Theory II. World Scientific, Singapore,
pages 102-111.
Joshi, Aravind K. 1985. How much
context-sensitivity is required to provide
structural descriptions: Tree adjoining
grammars. In David Dowty, Lauri
Karttunen, and Arnold Zwicky, editors,
Natural Language Processing:
Psycholinguistic, Computational, and
Theoretical Perspectives. Cambridge
University Press, New York,
pages 206-250.
Joshi, Aravind K. 1987. An introduction to
tree adjoining grammars. In Alexis
Manaster Ramer, editor, Mathematics of
Language. John Benjamins, Amsterdam,
pages 87-114.
Joshi, Aravind K., Leon S. Levy, and
M. Takahashi. 1975. Tree adjunct
grammars. Journal of Computer and Systems
Sciences, 10:136-163.
Manaster Ramer, Alexis. 1993. Capacity,
complexity, construction. Annals of
Mathematics and Artificial Intelligence,
8(1-2): Mathematics of language,
pages 1-16.
Manaster Ramer, Alexis. 1994. Uses and
misuses of mathematics in linguistics. X
Congreso de Lenguajes Naturales y
Lenguajes Formates, Sevilla.
Marcus, Solomon. 1967. Algebraic Linguistics.
Analytical Models. Academic Press, New
York.
Marcus, Solomon. 1969. Contextual
grammars. Revue roumaine des
mathematiques pures et apphquees,
14:1525-1534.
Marcus, Solomon, editor. 1978. La semiotique
formelle du folklore. Klincksieck,
Paris/Publishing House of the Romanian
Academy, Bucharest.
Marcus, Solomon. 1979. Linguistics for
programming languages. Revue roumaine
de linguistique-Cahiers de linguistique
theorique et appliquee, 16(1):29-38.
Marcus, Solomon, editor. 1981-83. Contextual
Ambiguities in Natural and Artificial
Languages. Communication and Cognition
Monographs, 2 volumes. Ghent.
Marcus, Solomon. 1997. Contextual
grammars and natural languages. In
Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages,
volume 2, pages 215-235.
Martin-Vide, Carlos. 1997. Natural
computation for natural language.
Fundamenta Informaticae, 31.2:117-124.
Martin-Vide, Carlos, Alexandru Mateescu,
Joan Miquel-Verges, and Gheorghe Paun.
1995. Internal contextual grammars:
Minimal, maximal and scattered use of
selectors. In M. Koppel and Eliyahu
Shamir, editors, Proceedings of The Fourth
Bar-Ilan Symposium on Foundations of
Artificial Intelligence (BISFAI&apos;95),
pages 132-142. Ramat Gan/Jerusalem.
Also in M. Koppel and Eliyahu Shamir,
editors, Proceedings of the fourth Bar-Ilan
Symposium on Foundations of Artificial
Intelligence. Focusing on Natural Languages
and Artificial Intelligence: Philosophical and
Computational Aspects. AAAI Press, Menlo
Park, CA, 1997, pages 159-168.
Martin-Vide, Carlos and Gheorghe Nun.
1998. Structured contextual grammars.
Grammars: To appear.
Miquel-Verges, Joan. 1997. Models Algebraics
Anahlics del Llenguatge. Un Exemple: les
Gramdtiques Contextuals; Formalitzacid i
Estudi de les Seves Aplicacions. Ph.D.
dissertation, Rovira i Virgili University,
Tarragona.
Olivetti, Convegno. 1970. Linguaggi nella
Societd e nella Tecnica. Edizzioni di
Comunita, Milano.
Partee, Barbara H., Alice ter Meulen, and
Robert E. Wall. 1990. Mathematical Methods
in Linguistics. Kluwer, Dordrecht.
Nun, Gheorghe. 1976. Languages associated
to a dramatic work. Revue roumaine de
linguistique: Cahiers de linguistique theorique
et appliquee, 13:605-611.
Nun, Gheorghe. 1979. A formal linguistic
model of action systems. Ars Semeiotica,
2:33-47.
Patin, Gheorghe. 1982. Contextual Grammars.
The Publishing House of the Romanian
Academy, Bucharest, in Romanian.
Nun, Gheorghe. 1985. On some open
problems about Marcus contextual
grammars. International Journal of Computer
Mathematics, 17:9-23.
Patin, Gheorghe. 1994. Marcus contextual
grammars. After 25 years. Bulletin of the
EATCS, 52:263-273.
Nun, Gheorghe. 1997. Marcus Contextual
Grammars. Kluwer, Boston, Dordrecht.
Nun, Gheorghe and Xuan My Nguyen.
1980. On the inner contextual grammars.
Revue roumaine des mat hematiques pures et
</reference>
<page confidence="0.965769">
273
</page>
<note confidence="0.351161">
Computational Linguistics Volume 24, Number 2
</note>
<reference confidence="0.999332876923077">
appliquees, 25:641-651.
Nun, Gheorghe, Grzegorz Rozenberg, and
Arto Salomaa. 1994. Marcus contextual
grammars: Modularity and leftmost
derivation. In Gheorghe Nun, editor,
Mathematical Aspects of Natural and Formal
Languages. World Scientific, Singapore,
pages 375-392.
Pullum, Geoffrey K. 1985. On two recent
attempts to show that English is not a
CFL. Computational Linguistics, 10:182-186.
Pullum, Geoffrey K. 1986. Footloose and
context-free. Natural Language and
Linguistic Theory, 4(3):409-414.
Pullum, Geoffrey K. 1987. Nobody goes
around at LSA meetings offering odds.
Natural Language and Linguistic Theory,
5(2):303-309.
Pullum, Geoffrey K. and Gerald Gazdar.
1982. Natural languages and context-free
languages. Linguistics and Philosophy,
4:471-504.
Radzinski, Daniel. 1990. Unbounded
syntactic copying in Mandarin Chinese.
Linguistics and Philosophy, 13:113-127.
Rosenkrantz, Daniel J. 1969. Programmed
grammars and classes of formal
languages. Journal of the ACM, 16:107-131.
Rounds, William C., Alexis Manaster Ramer,
and Joyce Friedman. 1987. Finding natural
languages a home in formal language
theory. In Alexis Manaster Ramer, editor,
Mathematics of Language. John Benjamins,
Amsterdam, pages 375-392.
Rozenberg, Grzegorz and Arto Salomaa.
1980. The Mathematical Theory of L Systems.
Academic Press, New York.
Rozenberg, Grzegorz and Arto Salomaa,
editors. 1997. Handbook of Formal
Languages, 3 volumes. Springer, Berlin.
Salomaa, Arto. 1973. Formal Languages.
Academic Press, New York.
Savitch, Walter J. 1991. Infinity is in the eyes
of the beholder. In C. Georgopoulos and
R. Ishihara, editors, Interdisciplinary
Approaches to Language: Essays in Honor of
S.-Y. Kuroda. Kluwer, Dordrecht,
pages 487-500.
Savitch, Walter J. 1993. Why it might pay to
assume that languages are infinite. Annals
of Mathematics and Artificial Intelligence,
8(1-2): Mathematics of language,
pages 17-25.
Savitch, Walter J., Emmon Bach, William
Marsh, and Gila Safran-Naveh, editors.
1987. The Formal Complexity of Natural
Language. Kluwer, Dordrecht.
Shieber, Stuart M. 1985. Evidence against
the context-freeness of natural language.
Linguistics and Philosophy, 8:333-343.
Sleator, Daniel and D. Temperley. 1991.
Parsing English with a link grammar.
Technical Report CMU-CS-91-196, School
of Computer Science, Carnegie Mellon
University, Pittsburgh.
</reference>
<page confidence="0.998298">
274
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.362688">
<title confidence="0.9962625">Contextual Grammars as Generative Models of Natural Languages</title>
<author confidence="0.999724">Solomon Marcus Carlos Martin-Videt</author>
<affiliation confidence="0.65109">University of Bucharest Rovira i Virgili University Gheorghe PAunI Institute of Mathematics of the Roma-nian Academy</affiliation>
<abstract confidence="0.99729715">The paper discusses some classes of contextual grammars—mainly those with &amp;quot;maximal use of selectors&amp;quot;—giving some arguments that these grammars can be considered a good model for natural language syntax. A contextual grammar produces a language starting from a finite set of words and iteratively adding contexts to the currently generated words, according to a selection procedure: each context has associated with it a selector, a set of words; the context is adjoined to any occurrence of such a selector in the word to be derived. In grammars with maximal use of selectors, a context is adjoined only to selectors for which no superword is a selector. Maximality can be defined either locally or globally (with respect to all selectors in the grammar). The obtained families of languages are incomparable with that of Chomsky context-free languages (and with other families of languages that contain linear languages and that are not &amp;quot;too large&amp;quot;; see Section 5) and have a series of properties supporting the assertion that these grammars are a possible adequate model for the syntax of natural languages. They are able to straightforwardly describe all the usual restrictions appearing in natural (and artificial) languages, which lead to the non-context-freeness of these languages: reduplication, crossed dependencies, and multiple agreements; however, there are center-embedded constructions that cannot be covered by these grammars. While these assertions concern only the weak generative capacity of contextual grammars, some ideas are also proposed for associating a structure to the generated words, in the form of a tree, or of a dependence relation (as considered in descriptive linguistics and also similar to that in link grammars).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
</authors>
<title>Indexed grammars. An Extension of context-free grammars.</title>
<date>1968</date>
<journal>Journal of the ACM,</journal>
<pages>15--647</pages>
<contexts>
<context position="68133" citStr="Aho 1968" startWordPosition="12016" endWordPosition="12017">is equal to RE. From the closure properties of F, the closure of F under direct and inverse morphisms is equal to F. This implies RE C F, contradicting the strictness of the inclusion F c RE from the theorem statement. Important families in formal language theory that fulfill the conditions in Theorem 4 are: (1) languages generated by programmed grammars without appearance checking but possibly using A-rules introduced by Rosenkrantz (1969) (they are equivalent to many other grammars with context-free core rules applied in a regulated manner: see Dassow and Faun [19891; (2) indexed languages (Aho 1968); (3) ETOL languages (generated by extended tabled interactionless Lindenmayer systems; ETOL is the largest family in this area—see Rozenberg and Salomaa [1980]; and (4) other subfamilies of ETOL (for instance, EOL). Each of the families CL„(FIN), c E {in,Mi,Mg-}, contains (context-sensitive) languages outside these families. Therefore, the families CL,, (FIN) occupy a quite eccentric position in the Chomsky hierarchy (Figure 5). 7. Summary and Final Remarks In this paper, we have continued the investigation of contextual grammars with (global or local) maximal use of selectors, recently intro</context>
</contexts>
<marker>Aho, 1968</marker>
<rawString>Aho, Alfred V. 1968. Indexed grammars. An Extension of context-free grammars. Journal of the ACM, 15:647-671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
<author>Eliyahu Shamir</author>
</authors>
<title>Finite state languages: Formal representations and adequacy problems.</title>
<date>1964</date>
<booktitle>In Yehoshua Bar-Hillel, editor, Language and Information.</booktitle>
<pages>87--98</pages>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA,</location>
<marker>Bar-Hillel, Shamir, 1964</marker>
<rawString>Bar-Hillel, Yehoshua and Eliyahu Shamir. 1964. Finite state languages: Formal representations and adequacy problems. In Yehoshua Bar-Hillel, editor, Language and Information. Addison-Wesley, Reading, MA, pages 87-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tudor Bala&apos;nescu</author>
<author>Marian Gheorghe</author>
</authors>
<title>Program tracing and languages of action. Revue roumaine de linguistique—Cahiers de linguistique theorique et appliquee,</title>
<date>1987</date>
<pages>32--167</pages>
<marker>Bala&apos;nescu, Gheorghe, 1987</marker>
<rawString>Bala&apos;nescu, Tudor and Marian Gheorghe. 1987. Program tracing and languages of action. Revue roumaine de linguistique—Cahiers de linguistique theorique et appliquee, 32:167-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>On the Notion &amp;quot;Rule of Grammar&amp;quot;.</title>
<date>1964</date>
<booktitle>The Structure of Language: Readings in the Philosophy of Language.</booktitle>
<pages>50--118</pages>
<editor>In Janet A. Fodor and Jerrold J. Katz, editors,</editor>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, N.J.,</location>
<marker>Chomsky, 1964</marker>
<rawString>Chomsky, Noam. 1964. On the Notion &amp;quot;Rule of Grammar&amp;quot;. In Janet A. Fodor and Jerrold J. Katz, editors, The Structure of Language: Readings in the Philosophy of Language. Prentice Hall, Englewood Cliffs, N.J., pages 50-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Culy</author>
</authors>
<title>The complexity of the vocabulary of Bambara. Linguistics and Philosophy,</title>
<date>1985</date>
<pages>8--345</pages>
<contexts>
<context position="5889" citStr="Culy (1985)" startWordPosition="898" endWordPosition="899">rammars (and no other class of contextual grammars can do the same). Technically, the above mentioned non-context-free features lead to formal languages of the forms {xcx I x E {a, b}* } (duplicated words of arbitrary length), {anbmcndm n, m &gt; 1} (two crossed dependencies), and {anbncn I n &gt; 1} gat least] three correlated positions). All of them are non-context-free languages and all of them can be generated in a surprisingly simple way by contextual grammars with selectors used in the maximal mode. Examples of natural language constructions based on reduplication were found, for instance, by Culy (1985), and Radzinski (1990), whereas crossed dependencies were demonstrated for Swiss German by Shieber (1985); see also Partee, ter Meulen and Wall (1990) or a number of contributions to Savitch et al. (1987). Multiple agreements were identified early on in programming languages (see, for example, Floyd [1962]), and certain constructions having such characteristics can also be found in natural languages. We shall give some arguments in Section 4. Some remarks are in order here. Although we mainly deal with the syntax of natural languages, we sometimes also mention artificial languages, mainly prog</context>
<context position="40855" citStr="Culy 1985" startWordPosition="7068" endWordPosition="7069">dependencies are the necessity of declaring identifiers and names of procedures, and of defining labels. In natural languages, such replications and dependencies can appear either at the level of the vocabulary or at the level of the sentences in a given language. The question is not simple, because it might not be clear what is grammatical and what is not grammatical with respect to a natural language. However, there are now convincing examples of non-context-free constructions in many languages. At the level of the vocabulary, the case of Bambara, a language from the Mande family in Africa (Culy 1985) is illustrative: compound words of the form string-of-words-o-string-of-words are possible in this language. The corresponding formal language consists of words of the form xcx, for x an arbitrarily long word over an alphabet not containing the symbol c (this symbol corresponds to the separator o in the Bambara construction). Because we can always codify words using two symbols, we work here with the language: = {XCX X E 0,1311. Another non-context-free construction has been found in a dialect of German spoken around Zurich, Switzerland (Shieber 1985; Pullum 1985), which allows construc257 Co</context>
</contexts>
<marker>Culy, 1985</marker>
<rawString>Culy, Christopher. 1985. The complexity of the vocabulary of Bambara. Linguistics and Philosophy, 8:345-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Dassow</author>
<author>Gheorghe Pun</author>
</authors>
<title>Regulated Rewriting in Formal Language Theory.</title>
<date>1989</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<marker>Dassow, Pun, 1989</marker>
<rawString>Dassow, Jurgen and Gheorghe Pun. 1989. Regulated Rewriting in Formal Language Theory. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrzej Ehrenfeucht</author>
<author>Gheorghe Nun</author>
<author>Grzegorz Rozenberg</author>
</authors>
<title>On representing recursively enumerable languages by internal contextual languages. Theoretical Computer Science.</title>
<date>1997</date>
<note>To appear.</note>
<marker>Ehrenfeucht, Nun, Rozenberg, 1997</marker>
<rawString>Ehrenfeucht, Andrzej, Gheorghe Nun, and Grzegorz Rozenberg. 1997. On representing recursively enumerable languages by internal contextual languages. Theoretical Computer Science. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Floyd</author>
</authors>
<title>On the non-existence of a phrase-structure grammar for Algol-60.</title>
<date>1962</date>
<journal>Communications of the ACM,</journal>
<pages>5--483</pages>
<contexts>
<context position="41883" citStr="Floyd (1962)" startWordPosition="7239" endWordPosition="7240"> {XCX X E 0,1311. Another non-context-free construction has been found in a dialect of German spoken around Zurich, Switzerland (Shieber 1985; Pullum 1985), which allows construc257 Computational Linguistics Volume 24, Number 2 tions of the form NP; NP &apos;j Va&apos;n&apos; , where NP, are accusative noun phrases, NPd are dative noun phrases, Va are accusative-demanding verbs, Vd are dative-demanding verbs, and the numbers match up, that is m = n = n&apos;. This leads to languages of the form: M2 = fambnedn I n, m &gt; 11. Both of these constructions can be easily found in programming languages, too. The proof of Floyd (1962) that Algol 60 is not context-free leads to a language of Mi type. Intersecting any Algol-like language with a regular language consisting of strings of the form: begin; real x; ..., go to labell; y := 1; ...label2 : ...; end we force the equalities x = y,labell = labe12, hence a language like M2 is obtained. If, however, we intersect an Algol-like language with the regular set of strings of the form: begin; real x; y := z; end then we force the equalities x = y z, which can be translated into a language of the form: M3 = faubncn I n &gt; 11. Concerning a natural language version of this form, Ma</context>
</contexts>
<marker>Floyd, 1962</marker>
<rawString>Floyd, R. W. 1962. On the non-existence of a phrase-structure grammar for Algol-60. Communications of the ACM, 5:483-484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Geoffrey K Pullum</author>
</authors>
<title>Computationally relevant properties of natural languages and their grammars.</title>
<date>1985</date>
<journal>New Generation Computing,</journal>
<pages>3--273</pages>
<contexts>
<context position="39432" citStr="Gazdar and Pullum (1985)" startWordPosition="6844" endWordPosition="6847">ails about this topic can be found in Marcus (1997). Let us consider again the three non-context-free constructions in natural languages mentioned in the introduction. The (non-)context-freeness of natural and programming languages has been investigated since the early sixties (Bar-Hillel and Shamir [1964]; Floyd [1962], among others). While for Algol 60 and for all advanced programming languages, the question has been settled from the very beginning—these languages are not context-free—a long debate was necessary concerning natural languages. We shall use information about this question from Gazdar and Pullum (1985); the reader might also consult Pullum (1985, 1986, 1987) and Pullum and Gazdar (1982). The general technique in approaching this problem is the same for both programming and natural languages. Look for special constructions that seem, intuitively, to require a non-context-free competence. In order to extract them from the studied language, use an intersection with a regular language. Because CF is closed under intersection with regular sets, if the result is not context-free, then we have a proof that the initial language is not context-free. The basic constructions of this type are duplicati</context>
</contexts>
<marker>Gazdar, Pullum, 1985</marker>
<rawString>Gazdar, Gerald and Geoffrey K. Pullum. 1985. Computationally relevant properties of natural languages and their grammars. New Generation Computing, 3:273-306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Grinberg</author>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
</authors>
<title>A robust parsing algorithm for link grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourth International Workshop on Parsing Technologies,</booktitle>
<pages>111--125</pages>
<location>Prague/Karlovy Vary.</location>
<marker>Grinberg, Lafferty, Sleator, 1995</marker>
<rawString>Grinberg, Dennis, John Lafferty, and Daniel Sleator. 1995. A robust parsing algorithm for link grammars. In Proceedings of the Fourth International Workshop on Parsing Technologies, pages 111-125, Prague/Karlovy Vary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Harrison</author>
</authors>
<title>Introduction to Formal Language Theory.</title>
<date>1978</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="17328" citStr="Harrison (1978)" startWordPosition="2717" endWordPosition="2718">tigate in this paper. As usual, given an alphabet V (which we also call vocabulary), we denote by V* the set of all words (equivalently: strings) over V. including the empty one, which is denoted by A. The set of all nonempty words over V, hence V* — {A}, is denoted by V. The length of x E V* is denoted by lx1 and its mirror image (also called the reversal) by mi(x). The families of finite, regular, linear, context-free, context-sensitive, and recursively enumerable languages are denoted by FIN, REG, LIN, CF, CS, RE, respectively. For the elements of formal language theory we use, we refer to Harrison (1978), Rozenberg and Salomaa (1997), and Salomaa (1973).3 A contextual grammar (with choice) is a construct: G = (V, A, (Si,C1), • • • ,(Sn,Cn)), n &gt;1, where V is an alphabet, A is a finite language over V. Si, are languages over V. and C1, , C are finite subsets of V* x V*. The elements of A are called axioms (starting words), the sets Si are called selectors, and the elements of sets C„ written in the form (u, v), are called contexts. The pairs (S,, C,) are also called productions. The intuition behind this construction is that the contexts in C, may be adjoined to words -in the associated set S.</context>
</contexts>
<marker>Harrison, 1978</marker>
<rawString>Harrison, Michael. 1978. Introduction to Formal Language Theory. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Hockett</author>
</authors>
<title>The State of the Art.</title>
<date>1970</date>
<location>Mouton, The Hague.</location>
<contexts>
<context position="7907" citStr="Hockett (1970)" startWordPosition="1211" endWordPosition="1212">s can be approached. In Marcus (1981- 83), where an effective analysis of contextual ambiguity in English, French, Romanian, and Hungarian is proposed, practical difficulties imposed a limitation to two levels of grammaticality for English (one level excluding compound words, the other level allowing the building of compound words) and Hungarian, but six levels for the analysis of French verbs. The reason for this situation is the &amp;quot;open&amp;quot; character of natural languages, making it impossible to formulate a necessary and sufficient condition for a sentence to be well-formed. As is pointed out by Hockett (1970), the set of well-formed strings in a natural language should be both finite and infinite, a requirement that is impossible to fulfill in the framework of classical set theory; for a related discussion, see Savitch (1991, 1993). This can also be related to Chomsky&apos;s claim that a basic problem in linguistics is to find a grammar able to generate all and only the well-formed strings in a natural language. Chomsky&apos;s claim presupposes that natural languages have the status of formal languages, but not everyone agrees with this notion. Even for programming languages, many authors reject the idea th</context>
</contexts>
<marker>Hockett, 1970</marker>
<rawString>Hockett, Charles. 1970. The State of the Art. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Ilie</author>
</authors>
<title>On computational complexity of contextual languages. Theoretical Computer Science.</title>
<date>1997</date>
<note>To appear.</note>
<contexts>
<context position="50659" citStr="Ilie (1997" startWordPosition="8748" endWordPosition="8749">ystematic study of parsing complexity has been done, even for basic classes of contextual grammars. (Of course, because in contextual grammars we do not have erasing operations but only adjoining, we always generate context-sensitive languages, hence membership is decidable.) The only complexity results known at the moment concern external contextual grammars with regular (even context-free) selectors, and a variant of internal contextual grammars with regular selectors used in a &amp;quot;localized&amp;quot; manner: the selector used at any derivation step should &amp;quot;touch&amp;quot; the context used at the previous step. Ilie (1997a, 1997b) proved that the parsing of the languages generated by such grammars can be done in polynomial time. Let us close this section with the observation that contextual grammars have another property much discussed recently: they are lexicalized (we might say &amp;quot;fully lexicalized&amp;quot;), as each of their productions (pair selector-context) consists of terminal symbols only. 260 Marcus, Martin-Vide, and Pun Contextual Grammars 5. Attempts to Associate a Structure to Contextual Languages In this section, we investigate further the adequacy of contextual grammars for describing the syntax of natural</context>
</contexts>
<marker>Ilie, 1997</marker>
<rawString>Ilie, Lucian. 1997a. On computational complexity of contextual languages. Theoretical Computer Science. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Ilie</author>
</authors>
<title>The computational complexity of Marcus contextual languages.</title>
<date>1997</date>
<note>Submitted.</note>
<contexts>
<context position="50659" citStr="Ilie (1997" startWordPosition="8748" endWordPosition="8749">ystematic study of parsing complexity has been done, even for basic classes of contextual grammars. (Of course, because in contextual grammars we do not have erasing operations but only adjoining, we always generate context-sensitive languages, hence membership is decidable.) The only complexity results known at the moment concern external contextual grammars with regular (even context-free) selectors, and a variant of internal contextual grammars with regular selectors used in a &amp;quot;localized&amp;quot; manner: the selector used at any derivation step should &amp;quot;touch&amp;quot; the context used at the previous step. Ilie (1997a, 1997b) proved that the parsing of the languages generated by such grammars can be done in polynomial time. Let us close this section with the observation that contextual grammars have another property much discussed recently: they are lexicalized (we might say &amp;quot;fully lexicalized&amp;quot;), as each of their productions (pair selector-context) consists of terminal symbols only. 260 Marcus, Martin-Vide, and Pun Contextual Grammars 5. Attempts to Associate a Structure to Contextual Languages In this section, we investigate further the adequacy of contextual grammars for describing the syntax of natural</context>
</contexts>
<marker>Ilie, 1997</marker>
<rawString>Ilie, Lucian. 1997b. The computational complexity of Marcus contextual languages. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Janar</author>
<author>Frantigek Mrdz</author>
<author>Martin Pldtek</author>
<author>Martin Prochdzka</author>
<author>Jorg Vogel</author>
</authors>
<title>Restarting automata, Marcus grammars and context-free languages.</title>
<date>1996</date>
<booktitle>In Jurgen Dassow, Grzegorz Rozenberg, and Arto Salomaa, editors, Developments in Language Theory II. World Scientific, Singapore,</booktitle>
<pages>102--111</pages>
<marker>Janar, Mrdz, Pldtek, Prochdzka, Vogel, 1996</marker>
<rawString>Janar, Petr, Frantigek Mrdz, Martin Pldtek, Martin Prochdzka, and Jorg Vogel. 1996. Restarting automata, Marcus grammars and context-free languages. In Jurgen Dassow, Grzegorz Rozenberg, and Arto Salomaa, editors, Developments in Language Theory II. World Scientific, Singapore, pages 102-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>How much context-sensitivity is required to provide structural descriptions: Tree adjoining grammars.</title>
<date>1985</date>
<booktitle>Natural Language Processing: Psycholinguistic, Computational, and Theoretical Perspectives.</booktitle>
<pages>206--250</pages>
<editor>In David Dowty, Lauri Karttunen, and Arnold Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>New York,</location>
<marker>Joshi, 1985</marker>
<rawString>Joshi, Aravind K. 1985. How much context-sensitivity is required to provide structural descriptions: Tree adjoining grammars. In David Dowty, Lauri Karttunen, and Arnold Zwicky, editors, Natural Language Processing: Psycholinguistic, Computational, and Theoretical Perspectives. Cambridge University Press, New York, pages 206-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars.</title>
<date>1987</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<pages>87--114</pages>
<editor>In Alexis Manaster Ramer, editor,</editor>
<location>Amsterdam,</location>
<marker>Joshi, 1987</marker>
<rawString>Joshi, Aravind K. 1987. An introduction to tree adjoining grammars. In Alexis Manaster Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam, pages 87-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and Systems Sciences,</journal>
<pages>10--136</pages>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, Aravind K., Leon S. Levy, and M. Takahashi. 1975. Tree adjunct grammars. Journal of Computer and Systems Sciences, 10:136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaster Ramer</author>
<author>Alexis</author>
</authors>
<title>Capacity, complexity, construction.</title>
<date>1993</date>
<journal>Annals of Mathematics and Artificial Intelligence, 8(1-2): Mathematics of language,</journal>
<pages>1--16</pages>
<marker>Ramer, Alexis, 1993</marker>
<rawString>Manaster Ramer, Alexis. 1993. Capacity, complexity, construction. Annals of Mathematics and Artificial Intelligence, 8(1-2): Mathematics of language, pages 1-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaster Ramer</author>
<author>Alexis</author>
</authors>
<title>Uses and misuses of mathematics in linguistics. X Congreso de Lenguajes Naturales y Lenguajes Formates,</title>
<date>1994</date>
<location>Sevilla.</location>
<marker>Ramer, Alexis, 1994</marker>
<rawString>Manaster Ramer, Alexis. 1994. Uses and misuses of mathematics in linguistics. X Congreso de Lenguajes Naturales y Lenguajes Formates, Sevilla.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Marcus</author>
</authors>
<title>Algebraic Linguistics. Analytical Models.</title>
<date>1967</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="56500" citStr="Marcus (1967)" startWordPosition="9848" endWordPosition="9849">t us denote by g the transitive closure of px. If i p-xf- j, then we say that x(j) is subordinate to x(i). A structured string (x,Px) can be represented in a graphical form by writing the elements x(1), ,x(n) of x in a row and drawing above them arcs (x(i), x(j)) for i Px J. A structured string (x, p,) is called a simple string of center x(io) if the graph associated to it as described above is a tree with the root marked with x(i0) (the center corresponds to the predicative element of a sentence). The notion of a structured string is well-known in linguistics: see, for example, Chapter VI of Marcus (1967). A related notion has been recently considered, that of a link grammar: see Sleator and Temperley (1991), or Grinberg, Lafferty, and Sleator (1995). In a link grammar, the elements of a sentence are correctly related in a linkage, according to a pairing of left and right connectors given for each word in the dictionary, providing that the obtained dependence relation has several properties: the associated graph is connected, planar, etc. Because we do not investigate here the possibility of producing correct linkages, in the sense of Sleator and Temperley (1991), by using contextual grammars </context>
<context position="61099" citStr="Marcus (1967)" startWordPosition="10693" endWordPosition="10694">ar if the graphs associated to the relation describing the structure of the strings generated by G is connected, a tree, or planar (when the string is written on a horizontal line, as before), respectively. Moreover, we can use these properties as restrictions on the grammar, selecting from the languages L(G), and SL(G) only the (structured) strings whose structure graph has the properties mentioned above. Of course, many other variants can be defined; for instance, we can consider the various types of projectivity (progressive, regressive, strong, and so on), as investigated in Chapter VI of Marcus (1967). The above definitions of bracketed and structured contextual grammars can be extended in an obvious way to grammars with maximal use of selectors. Some results in this area can be found in Martin-Vide and Paun (1998), but a lot of questions remain to be clarified. The main problem is to find the most useful and natural type of structured contextual grammars for describing the structure of natural language syntactic constructions. 265 Computational Linguistics Volume 24, Number 2 6. Representations of Recursively Enumerable Languages Completing the study on (weak) generative power of contextu</context>
</contexts>
<marker>Marcus, 1967</marker>
<rawString>Marcus, Solomon. 1967. Algebraic Linguistics. Analytical Models. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Marcus</author>
</authors>
<title>Contextual grammars. Revue roumaine des mathematiques pures et apphquees,</title>
<date>1969</date>
<pages>14--1525</pages>
<contexts>
<context position="2030" citStr="Marcus (1969)" startWordPosition="304" endWordPosition="305">nd artificial) languages, which lead to the non-context-freeness of these languages: reduplication, crossed dependencies, and multiple agreements; however, there are center-embedded constructions that cannot be covered by these grammars. While these assertions concern only the weak generative capacity of contextual grammars, some ideas are also proposed for associating a structure to the generated words, in the form of a tree, or of a dependence relation (as considered in descriptive linguistics and also similar to that in link grammars). 1. Introduction Contextual grammars were introduced by Marcus (1969), as &amp;quot;intrinsic grammars,&amp;quot; without auxiliary symbols, based only on the fundamental linguistic operation of inserting words in given phrases, according to certain contextual dependencies. More precisely, contextual grammars include contexts (pairs of words), associated with * University of Bucharest, Faculty of Mathematics, Str. Academiei 14, 70109 Bucharest, Romania. E-mail: solomon@imarso f Research Group on Mathematical Linguistics and Language Engineering (GRLMC), Rovira i Virgili University, Pl. Imperial Tarraco 1, 43005 Tarragona, Spain. E-mail: cmv@astor.urv.es I Institute of Mathematic</context>
<context position="20136" citStr="Marcus (1969)" startWordPosition="3280" endWordPosition="3281">,,vi)}, and S, the set of strings in V* to which the context (u,, v) can be adjoined, that is: Si = E V* I (141,V1) E gO(X)}. The two grammars G and G&apos; are clearly equivalent in both cases. Thus, in the proofs below we shall use that presentation of a contextual grammar which is more appropriate (economical) for that case. Remark 2 The derivation relation defined above has been denoted by &gt;in in order to distinguish it from the external derivation defined for G, where the context is adjoined at the ends of the derived word: x &gt;ex y iff y = uxv for (u, v) E C,, x E S,, for some i,1 &lt; i &lt; n. In Marcus (1969), only the external derivation is considered, for grammars presented in the functional form, without restrictions on the selection mapping. Contextual grammars with internal derivation were introduced in Paun and Nguyen (1980). We do not investigate the external derivation here. Two natural variants of the relation Martin-Vide et al. (1995): in defined above were considered by X &gt;m, y iff x = x1x2x3, y = xi ux2vx3, for x2 E Si, (u, v) c C„ for some 1 &lt;i &lt;n, and there are no x&apos;2,4 E V* such that x x&apos;2 E Si, and I &lt; lx11, 141 &lt; ix31, ix121 &gt; 1.x21; X &gt;MgY 250 Marcus, Martin-Vide, and Nun Context</context>
</contexts>
<marker>Marcus, 1969</marker>
<rawString>Marcus, Solomon. 1969. Contextual grammars. Revue roumaine des mathematiques pures et apphquees, 14:1525-1534.</rawString>
</citation>
<citation valid="true">
<title>La semiotique formelle du folklore. Klincksieck, Paris/Publishing House of the Romanian Academy,</title>
<date>1978</date>
<editor>Marcus, Solomon, editor.</editor>
<location>Bucharest.</location>
<contexts>
<context position="17328" citStr="(1978)" startWordPosition="2718" endWordPosition="2718"> this paper. As usual, given an alphabet V (which we also call vocabulary), we denote by V* the set of all words (equivalently: strings) over V. including the empty one, which is denoted by A. The set of all nonempty words over V, hence V* — {A}, is denoted by V. The length of x E V* is denoted by lx1 and its mirror image (also called the reversal) by mi(x). The families of finite, regular, linear, context-free, context-sensitive, and recursively enumerable languages are denoted by FIN, REG, LIN, CF, CS, RE, respectively. For the elements of formal language theory we use, we refer to Harrison (1978), Rozenberg and Salomaa (1997), and Salomaa (1973).3 A contextual grammar (with choice) is a construct: G = (V, A, (Si,C1), • • • ,(Sn,Cn)), n &gt;1, where V is an alphabet, A is a finite language over V. Si, are languages over V. and C1, , C are finite subsets of V* x V*. The elements of A are called axioms (starting words), the sets Si are called selectors, and the elements of sets C„ written in the form (u, v), are called contexts. The pairs (S,, C,) are also called productions. The intuition behind this construction is that the contexts in C, may be adjoined to words -in the associated set S.</context>
</contexts>
<marker>1978</marker>
<rawString>Marcus, Solomon, editor. 1978. La semiotique formelle du folklore. Klincksieck, Paris/Publishing House of the Romanian Academy, Bucharest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Marcus</author>
</authors>
<title>Linguistics for programming languages. Revue roumaine de linguistique-Cahiers de linguistique theorique et appliquee,</title>
<date>1979</date>
<pages>16--1</pages>
<contexts>
<context position="8666" citStr="Marcus (1979)" startWordPosition="1333" endWordPosition="1334">ework of classical set theory; for a related discussion, see Savitch (1991, 1993). This can also be related to Chomsky&apos;s claim that a basic problem in linguistics is to find a grammar able to generate all and only the well-formed strings in a natural language. Chomsky&apos;s claim presupposes that natural languages have the status of formal languages, but not everyone agrees with this notion. Even for programming languages, many authors reject the idea that well-formed strings constitute a formal language; see, for instance, the various articles in the collective volume Olivetti (1970), as well as Marcus (1979). Returning to constructions specific to natural languages, we have found the surprising fact that the language {ancbmcbmcan n,m &gt; 1} cannot be generated by contextual grammars with a maximal global use of selectors. Observe the center-embedded structure of this language and the fact that it is an &amp;quot;easy&amp;quot; linear language. As Manaster Ramer (1994, 4) points out, &amp;quot;the Chomsky hierarchy is in fact highly misleading... , suggesting as it does, for example, that center-embedded structures (including mirrorimages) are simpler (since they are context-free) than cross-serial structures (including redup</context>
</contexts>
<marker>Marcus, 1979</marker>
<rawString>Marcus, Solomon. 1979. Linguistics for programming languages. Revue roumaine de linguistique-Cahiers de linguistique theorique et appliquee, 16(1):29-38.</rawString>
</citation>
<citation valid="false">
<booktitle>Contextual Ambiguities in Natural and Artificial Languages. Communication and Cognition Monographs,</booktitle>
<volume>2</volume>
<pages>1981--83</pages>
<editor>Marcus, Solomon, editor.</editor>
<publisher>Ghent.</publisher>
<marker></marker>
<rawString>Marcus, Solomon, editor. 1981-83. Contextual Ambiguities in Natural and Artificial Languages. Communication and Cognition Monographs, 2 volumes. Ghent.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Marcus</author>
</authors>
<title>Contextual grammars and natural languages.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages,</booktitle>
<volume>2</volume>
<pages>215--235</pages>
<contexts>
<context position="38859" citStr="Marcus (1997)" startWordPosition="6767" endWordPosition="6768">gs and contexts over V. A natural question can be raised: could we now follow an inverse itinerary, by starting from a finite stock A of strings (over V) simple enough to be considered primitive well-formed strings (axioms), and by considering a finite set of couples (Si, C,), 1 &lt;i &lt; n, where S, is a set of strings, while C, is a finite set of contexts, to ask what is (are) the language(s) with respect to which C, selects Si, 1 &lt; i &lt; n? The idea of a contextual grammar, in its various forms, is born from the attempt to answer this question. A series of details about this topic can be found in Marcus (1997). Let us consider again the three non-context-free constructions in natural languages mentioned in the introduction. The (non-)context-freeness of natural and programming languages has been investigated since the early sixties (Bar-Hillel and Shamir [1964]; Floyd [1962], among others). While for Algol 60 and for all advanced programming languages, the question has been settled from the very beginning—these languages are not context-free—a long debate was necessary concerning natural languages. We shall use information about this question from Gazdar and Pullum (1985); the reader might also con</context>
</contexts>
<marker>Marcus, 1997</marker>
<rawString>Marcus, Solomon. 1997. Contextual grammars and natural languages. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages, volume 2, pages 215-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Martin-Vide</author>
</authors>
<title>Natural computation for natural language. Fundamenta Informaticae,</title>
<date>1997</date>
<pages>31--2</pages>
<contexts>
<context position="72849" citStr="Martin-Vide (1997)" startWordPosition="12740" endWordPosition="12741">serve dependencies and agreements, and in accordance with specified selectors, which can ensure the preservation of grammaticality Of course, this is only a speculation, but it also fits with the general idea of &amp;quot;natural computation&amp;quot;: for example, nature seems not to use the rewriting operation in the area of genetics, where recombination (crossing over) of chromosomes is the basic evolutionary operation (together with nondeterministic insertion and deletion operations, which, again, are not rewriting) and where no &amp;quot;nonterminal symbol&amp;quot; is used. Further discussion of this topic can be found in Martin-Vide (1997). 7. A structure for the words generated by a contextual grammar can be introduced in various ways. By parenthesizing the contexts, we get a tree. Considering dependence relations on symbols appearing in axioms, contexts, and selectors, we can obtain structured strings of a type well investigated in descriptive linguistics and very similar to the phrase-linkage structures produced by a link grammar. A number of the previous points need further investigation. There are also several topics that are important from a linguistic point of view and that are still poorly investigated for contextual gr</context>
</contexts>
<marker>Martin-Vide, 1997</marker>
<rawString>Martin-Vide, Carlos. 1997. Natural computation for natural language. Fundamenta Informaticae, 31.2:117-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Martin-Vide</author>
</authors>
<title>Alexandru Mateescu, Joan Miquel-Verges, and Gheorghe Paun.</title>
<date>1995</date>
<booktitle>Proceedings of The Fourth Bar-Ilan Symposium on Foundations of Artificial Intelligence (BISFAI&apos;95),</booktitle>
<editor>In M. Koppel and Eliyahu Shamir, editors,</editor>
<marker>Martin-Vide, 1995</marker>
<rawString>Martin-Vide, Carlos, Alexandru Mateescu, Joan Miquel-Verges, and Gheorghe Paun. 1995. Internal contextual grammars: Minimal, maximal and scattered use of selectors. In M. Koppel and Eliyahu Shamir, editors, Proceedings of The Fourth Bar-Ilan Symposium on Foundations of Artificial Intelligence (BISFAI&apos;95),</rawString>
</citation>
<citation valid="true">
<title>Ramat Gan/Jerusalem. Also</title>
<date>1997</date>
<booktitle>Proceedings of the fourth Bar-Ilan Symposium on Foundations of Artificial Intelligence. Focusing on Natural Languages and Artificial Intelligence: Philosophical and Computational Aspects.</booktitle>
<pages>132--142</pages>
<editor>in M. Koppel and Eliyahu Shamir, editors,</editor>
<publisher>AAAI Press,</publisher>
<location>Menlo Park, CA,</location>
<contexts>
<context position="49867" citStr="[1997]" startWordPosition="8631" endWordPosition="8631">ve the bounded growth property: the set of contexts is finite, passing from one string to another means adjoining of a context from a finite set, and all generated strings belong to the language. However, we do not know whether or not the languages in families CL (REG), c E {Ml, Mg} are parsable in polynomial time. In general, the parsing of languages generated by contextual grammars (of any type, not only with maximal use of selectors) is a research area still open. There are several attempts to define contextual automata (see, for example, Faun [1982], Janar et al. [1996], and Miquel-Verges [1997]). Some of them characterize a number of families of contextual languages, and some of them recognize families that do not correspond to classes of contextual grammars. However, no systematic study of parsing complexity has been done, even for basic classes of contextual grammars. (Of course, because in contextual grammars we do not have erasing operations but only adjoining, we always generate context-sensitive languages, hence membership is decidable.) The only complexity results known at the moment concern external contextual grammars with regular (even context-free) selectors, and a varian</context>
</contexts>
<marker>1997</marker>
<rawString>pages 132-142. Ramat Gan/Jerusalem. Also in M. Koppel and Eliyahu Shamir, editors, Proceedings of the fourth Bar-Ilan Symposium on Foundations of Artificial Intelligence. Focusing on Natural Languages and Artificial Intelligence: Philosophical and Computational Aspects. AAAI Press, Menlo Park, CA, 1997, pages 159-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Martin-Vide</author>
<author>Gheorghe Nun</author>
</authors>
<date>1998</date>
<note>Structured contextual grammars. Grammars: To appear.</note>
<contexts>
<context position="55223" citStr="Martin-Vide and Nun (1998)" startWordPosition="9606" endWordPosition="9609">nguage BL(G) defined by: BL(G) = {(prv(z), r(z)) I w ==. z, for some W E A}. Note the fact that each string in L(G) is paired with a tree in BL(G); however, the string should be read on the edges of this tree, not on leaf nodes as in the case of derivation trees of context-free grammars. The linguistic significance of such a tree is not yet clear to us, hence we do not insist on this idea (the ambiguity of contextual grammars and languages can be defined in this framework, but how the tree illuminates the grammatical structure of a sentence remains to be clarified). Another idea considered by Martin-Vide and Nun (1998), closer to linguistics, is to introduce a dependence relation on the set of symbols appearing in axioms, 262 Marcus, Martin-Vide, and Pun Contextual Grammars selectors, and contexts of a contextual grammar. We present some of the details of this idea informally below. Consider an alphabet V and a string x E V*. We denote: M(x) -= {1, 2, ..., fx1} and we write x =-- x(1)x(2)...x(n), for n = (xi/x(0 E V,1 &lt; i &lt; n. Any antireflexive relation on M(x) is called a dependence relation on x. Let p, be such a relation (antireflexivity means i Px i for no value of i). The pair (x, px) is called a struc</context>
</contexts>
<marker>Martin-Vide, Nun, 1998</marker>
<rawString>Martin-Vide, Carlos and Gheorghe Nun. 1998. Structured contextual grammars. Grammars: To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Miquel-Verges</author>
</authors>
<title>Models Algebraics Anahlics del Llenguatge. Un Exemple: les Gramdtiques Contextuals; Formalitzacid i Estudi de les Seves Aplicacions.</title>
<date>1997</date>
<institution>Rovira i Virgili University,</institution>
<location>Tarragona.</location>
<note>Ph.D. dissertation,</note>
<marker>Miquel-Verges, 1997</marker>
<rawString>Miquel-Verges, Joan. 1997. Models Algebraics Anahlics del Llenguatge. Un Exemple: les Gramdtiques Contextuals; Formalitzacid i Estudi de les Seves Aplicacions. Ph.D. dissertation, Rovira i Virgili University, Tarragona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Convegno Olivetti</author>
</authors>
<date>1970</date>
<booktitle>Linguaggi nella Societd e nella Tecnica. Edizzioni di Comunita,</booktitle>
<location>Milano.</location>
<contexts>
<context position="8640" citStr="Olivetti (1970)" startWordPosition="1328" endWordPosition="1329">sible to fulfill in the framework of classical set theory; for a related discussion, see Savitch (1991, 1993). This can also be related to Chomsky&apos;s claim that a basic problem in linguistics is to find a grammar able to generate all and only the well-formed strings in a natural language. Chomsky&apos;s claim presupposes that natural languages have the status of formal languages, but not everyone agrees with this notion. Even for programming languages, many authors reject the idea that well-formed strings constitute a formal language; see, for instance, the various articles in the collective volume Olivetti (1970), as well as Marcus (1979). Returning to constructions specific to natural languages, we have found the surprising fact that the language {ancbmcbmcan n,m &gt; 1} cannot be generated by contextual grammars with a maximal global use of selectors. Observe the center-embedded structure of this language and the fact that it is an &amp;quot;easy&amp;quot; linear language. As Manaster Ramer (1994, 4) points out, &amp;quot;the Chomsky hierarchy is in fact highly misleading... , suggesting as it does, for example, that center-embedded structures (including mirrorimages) are simpler (since they are context-free) than cross-serial s</context>
</contexts>
<marker>Olivetti, 1970</marker>
<rawString>Olivetti, Convegno. 1970. Linguaggi nella Societd e nella Tecnica. Edizzioni di Comunita, Milano.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara H Partee</author>
<author>Alice ter Meulen</author>
<author>Robert E Wall</author>
</authors>
<date>1990</date>
<booktitle>Mathematical Methods in Linguistics.</booktitle>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Partee, Meulen, Wall, 1990</marker>
<rawString>Partee, Barbara H., Alice ter Meulen, and Robert E. Wall. 1990. Mathematical Methods in Linguistics. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gheorghe Nun</author>
</authors>
<title>Languages associated to a dramatic work. Revue roumaine de linguistique: Cahiers de linguistique theorique et appliquee,</title>
<date>1976</date>
<pages>13--605</pages>
<marker>Nun, 1976</marker>
<rawString>Nun, Gheorghe. 1976. Languages associated to a dramatic work. Revue roumaine de linguistique: Cahiers de linguistique theorique et appliquee, 13:605-611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gheorghe Nun</author>
</authors>
<title>A formal linguistic model of action systems.</title>
<date>1979</date>
<journal>Ars Semeiotica,</journal>
<pages>2--33</pages>
<marker>Nun, 1979</marker>
<rawString>Nun, Gheorghe. 1979. A formal linguistic model of action systems. Ars Semeiotica, 2:33-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gheorghe Patin</author>
</authors>
<title>Contextual Grammars. The Publishing House of the Romanian Academy,</title>
<date>1982</date>
<location>Bucharest, in Romanian.</location>
<contexts>
<context position="3457" citStr="Patin (1982" startWordPosition="509" endWordPosition="510">ion, grant SAB 95-0357. © 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 2 selectors (sets of words); a context can be adjoined to any associated word-selector. In this way, starting from a finite set of words, we can generate a language. This operation of iterated selective insertion of words is related to the basic combinatorics on words, as well as to the basic operations in rewriting systems of any type. Indeed, contextual grammars, in the many variants considered in the literature, were investigated mainly from a mathematical point of view; see Patin (1982, 1985, 1994), Nun, Rozenberg and Salomaa (1994), and their references. A complete source of information is the monograph Nun (1997). A few applications of contextual grammars were developed in connection with action theory (Pam 1979), with the study of theatrical works (Patin 1976), and with computer program evolution (13alanescu and Gheorghe 1987), but up to now no attempt has been made to check the relevance of contextual grammars in the very field where they were motivated: linguistics, the study of natural languages. A sort of a posteriori explanation is given: the variants of contextual </context>
</contexts>
<marker>Patin, 1982</marker>
<rawString>Patin, Gheorghe. 1982. Contextual Grammars. The Publishing House of the Romanian Academy, Bucharest, in Romanian.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gheorghe Nun</author>
</authors>
<title>On some open problems about Marcus contextual grammars.</title>
<date>1985</date>
<journal>International Journal of Computer Mathematics,</journal>
<pages>17--9</pages>
<marker>Nun, 1985</marker>
<rawString>Nun, Gheorghe. 1985. On some open problems about Marcus contextual grammars. International Journal of Computer Mathematics, 17:9-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gheorghe Patin</author>
</authors>
<title>Marcus contextual grammars. After 25 years.</title>
<date>1994</date>
<journal>Bulletin of the EATCS,</journal>
<pages>52--263</pages>
<marker>Patin, 1994</marker>
<rawString>Patin, Gheorghe. 1994. Marcus contextual grammars. After 25 years. Bulletin of the EATCS, 52:263-273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gheorghe Nun</author>
</authors>
<title>Marcus Contextual Grammars.</title>
<date>1997</date>
<publisher>Kluwer,</publisher>
<location>Boston, Dordrecht.</location>
<contexts>
<context position="3589" citStr="Nun (1997)" startWordPosition="529" endWordPosition="530">s of words); a context can be adjoined to any associated word-selector. In this way, starting from a finite set of words, we can generate a language. This operation of iterated selective insertion of words is related to the basic combinatorics on words, as well as to the basic operations in rewriting systems of any type. Indeed, contextual grammars, in the many variants considered in the literature, were investigated mainly from a mathematical point of view; see Patin (1982, 1985, 1994), Nun, Rozenberg and Salomaa (1994), and their references. A complete source of information is the monograph Nun (1997). A few applications of contextual grammars were developed in connection with action theory (Pam 1979), with the study of theatrical works (Patin 1976), and with computer program evolution (13alanescu and Gheorghe 1987), but up to now no attempt has been made to check the relevance of contextual grammars in the very field where they were motivated: linguistics, the study of natural languages. A sort of a posteriori explanation is given: the variants of contextual grammars investigated so far are not powerful enough, hence they are not interesting enough; what they can do, a regular or a contex</context>
</contexts>
<marker>Nun, 1997</marker>
<rawString>Nun, Gheorghe. 1997. Marcus Contextual Grammars. Kluwer, Boston, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gheorghe Nun</author>
<author>Xuan My Nguyen</author>
</authors>
<title>On the inner contextual grammars. Revue roumaine des mat hematiques pures et appliquees,</title>
<date>1980</date>
<pages>25--641</pages>
<marker>Nun, Nguyen, 1980</marker>
<rawString>Nun, Gheorghe and Xuan My Nguyen. 1980. On the inner contextual grammars. Revue roumaine des mat hematiques pures et appliquees, 25:641-651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gheorghe Nun</author>
<author>Grzegorz Rozenberg</author>
<author>Arto Salomaa</author>
</authors>
<title>Marcus contextual grammars: Modularity and leftmost derivation.</title>
<date>1994</date>
<booktitle>In Gheorghe Nun, editor, Mathematical Aspects of Natural and Formal Languages. World Scientific, Singapore,</booktitle>
<pages>375--392</pages>
<marker>Nun, Rozenberg, Salomaa, 1994</marker>
<rawString>Nun, Gheorghe, Grzegorz Rozenberg, and Arto Salomaa. 1994. Marcus contextual grammars: Modularity and leftmost derivation. In Gheorghe Nun, editor, Mathematical Aspects of Natural and Formal Languages. World Scientific, Singapore, pages 375-392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
</authors>
<title>On two recent attempts to show that English is not a CFL.</title>
<date>1985</date>
<journal>Computational Linguistics,</journal>
<pages>10--182</pages>
<contexts>
<context position="39432" citStr="Pullum (1985)" startWordPosition="6846" endWordPosition="6847">this topic can be found in Marcus (1997). Let us consider again the three non-context-free constructions in natural languages mentioned in the introduction. The (non-)context-freeness of natural and programming languages has been investigated since the early sixties (Bar-Hillel and Shamir [1964]; Floyd [1962], among others). While for Algol 60 and for all advanced programming languages, the question has been settled from the very beginning—these languages are not context-free—a long debate was necessary concerning natural languages. We shall use information about this question from Gazdar and Pullum (1985); the reader might also consult Pullum (1985, 1986, 1987) and Pullum and Gazdar (1982). The general technique in approaching this problem is the same for both programming and natural languages. Look for special constructions that seem, intuitively, to require a non-context-free competence. In order to extract them from the studied language, use an intersection with a regular language. Because CF is closed under intersection with regular sets, if the result is not context-free, then we have a proof that the initial language is not context-free. The basic constructions of this type are duplicati</context>
<context position="41426" citStr="Pullum 1985" startWordPosition="7157" endWordPosition="7158">om the Mande family in Africa (Culy 1985) is illustrative: compound words of the form string-of-words-o-string-of-words are possible in this language. The corresponding formal language consists of words of the form xcx, for x an arbitrarily long word over an alphabet not containing the symbol c (this symbol corresponds to the separator o in the Bambara construction). Because we can always codify words using two symbols, we work here with the language: = {XCX X E 0,1311. Another non-context-free construction has been found in a dialect of German spoken around Zurich, Switzerland (Shieber 1985; Pullum 1985), which allows construc257 Computational Linguistics Volume 24, Number 2 tions of the form NP; NP &apos;j Va&apos;n&apos; , where NP, are accusative noun phrases, NPd are dative noun phrases, Va are accusative-demanding verbs, Vd are dative-demanding verbs, and the numbers match up, that is m = n = n&apos;. This leads to languages of the form: M2 = fambnedn I n, m &gt; 11. Both of these constructions can be easily found in programming languages, too. The proof of Floyd (1962) that Algol 60 is not context-free leads to a language of Mi type. Intersecting any Algol-like language with a regular language consisting of s</context>
</contexts>
<marker>Pullum, 1985</marker>
<rawString>Pullum, Geoffrey K. 1985. On two recent attempts to show that English is not a CFL. Computational Linguistics, 10:182-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
</authors>
<title>Footloose and context-free.</title>
<date>1986</date>
<booktitle>Natural Language and Linguistic Theory,</booktitle>
<pages>4--3</pages>
<marker>Pullum, 1986</marker>
<rawString>Pullum, Geoffrey K. 1986. Footloose and context-free. Natural Language and Linguistic Theory, 4(3):409-414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
</authors>
<title>Nobody goes around at LSA meetings offering odds. Natural Language and Linguistic Theory,</title>
<date>1987</date>
<pages>5--2</pages>
<marker>Pullum, 1987</marker>
<rawString>Pullum, Geoffrey K. 1987. Nobody goes around at LSA meetings offering odds. Natural Language and Linguistic Theory, 5(2):303-309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>Gerald Gazdar</author>
</authors>
<title>Natural languages and context-free languages. Linguistics and Philosophy,</title>
<date>1982</date>
<pages>4--471</pages>
<contexts>
<context position="39518" citStr="Pullum and Gazdar (1982)" startWordPosition="6858" endWordPosition="6861">on-context-free constructions in natural languages mentioned in the introduction. The (non-)context-freeness of natural and programming languages has been investigated since the early sixties (Bar-Hillel and Shamir [1964]; Floyd [1962], among others). While for Algol 60 and for all advanced programming languages, the question has been settled from the very beginning—these languages are not context-free—a long debate was necessary concerning natural languages. We shall use information about this question from Gazdar and Pullum (1985); the reader might also consult Pullum (1985, 1986, 1987) and Pullum and Gazdar (1982). The general technique in approaching this problem is the same for both programming and natural languages. Look for special constructions that seem, intuitively, to require a non-context-free competence. In order to extract them from the studied language, use an intersection with a regular language. Because CF is closed under intersection with regular sets, if the result is not context-free, then we have a proof that the initial language is not context-free. The basic constructions of this type are duplication of arbitrarily long subwords, dependencies (agreements) between crossed pairs of su</context>
</contexts>
<marker>Pullum, Gazdar, 1982</marker>
<rawString>Pullum, Geoffrey K. and Gerald Gazdar. 1982. Natural languages and context-free languages. Linguistics and Philosophy, 4:471-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Radzinski</author>
</authors>
<title>Unbounded syntactic copying in Mandarin Chinese. Linguistics and Philosophy,</title>
<date>1990</date>
<pages>13--113</pages>
<contexts>
<context position="5911" citStr="Radzinski (1990)" startWordPosition="901" endWordPosition="902">ther class of contextual grammars can do the same). Technically, the above mentioned non-context-free features lead to formal languages of the forms {xcx I x E {a, b}* } (duplicated words of arbitrary length), {anbmcndm n, m &gt; 1} (two crossed dependencies), and {anbncn I n &gt; 1} gat least] three correlated positions). All of them are non-context-free languages and all of them can be generated in a surprisingly simple way by contextual grammars with selectors used in the maximal mode. Examples of natural language constructions based on reduplication were found, for instance, by Culy (1985), and Radzinski (1990), whereas crossed dependencies were demonstrated for Swiss German by Shieber (1985); see also Partee, ter Meulen and Wall (1990) or a number of contributions to Savitch et al. (1987). Multiple agreements were identified early on in programming languages (see, for example, Floyd [1962]), and certain constructions having such characteristics can also be found in natural languages. We shall give some arguments in Section 4. Some remarks are in order here. Although we mainly deal with the syntax of natural languages, we sometimes also mention artificial languages, mainly programming languages. Wit</context>
</contexts>
<marker>Radzinski, 1990</marker>
<rawString>Radzinski, Daniel. 1990. Unbounded syntactic copying in Mandarin Chinese. Linguistics and Philosophy, 13:113-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel J Rosenkrantz</author>
</authors>
<title>Programmed grammars and classes of formal languages.</title>
<date>1969</date>
<journal>Journal of the ACM,</journal>
<pages>16--107</pages>
<contexts>
<context position="67968" citStr="Rosenkrantz (1969)" startWordPosition="11990" endWordPosition="11991">se morphisms should be included in the closure of F under direct and inverse morphisms. From Theorem 3 we know that the closure of CLa(F&apos;) under direct and inverse morphisms is equal to RE. From the closure properties of F, the closure of F under direct and inverse morphisms is equal to F. This implies RE C F, contradicting the strictness of the inclusion F c RE from the theorem statement. Important families in formal language theory that fulfill the conditions in Theorem 4 are: (1) languages generated by programmed grammars without appearance checking but possibly using A-rules introduced by Rosenkrantz (1969) (they are equivalent to many other grammars with context-free core rules applied in a regulated manner: see Dassow and Faun [19891; (2) indexed languages (Aho 1968); (3) ETOL languages (generated by extended tabled interactionless Lindenmayer systems; ETOL is the largest family in this area—see Rozenberg and Salomaa [1980]; and (4) other subfamilies of ETOL (for instance, EOL). Each of the families CL„(FIN), c E {in,Mi,Mg-}, contains (context-sensitive) languages outside these families. Therefore, the families CL,, (FIN) occupy a quite eccentric position in the Chomsky hierarchy (Figure 5). 7</context>
</contexts>
<marker>Rosenkrantz, 1969</marker>
<rawString>Rosenkrantz, Daniel J. 1969. Programmed grammars and classes of formal languages. Journal of the ACM, 16:107-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Rounds</author>
<author>Alexis Manaster Ramer</author>
<author>Joyce Friedman</author>
</authors>
<title>Finding natural languages a home in formal language theory.</title>
<date>1987</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<pages>375--392</pages>
<editor>In Alexis Manaster Ramer, editor,</editor>
<location>Amsterdam,</location>
<marker>Rounds, Ramer, Friedman, 1987</marker>
<rawString>Rounds, William C., Alexis Manaster Ramer, and Joyce Friedman. 1987. Finding natural languages a home in formal language theory. In Alexis Manaster Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam, pages 375-392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Rozenberg</author>
<author>Arto Salomaa</author>
</authors>
<title>The Mathematical Theory of L Systems.</title>
<date>1980</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>Rozenberg, Salomaa, 1980</marker>
<rawString>Rozenberg, Grzegorz and Arto Salomaa. 1980. The Mathematical Theory of L Systems. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rozenberg</author>
</authors>
<title>Grzegorz and Arto Salomaa, editors.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="11893" citStr="Rozenberg (1997)" startWordPosition="1840" endWordPosition="1841">e always increased. At every step, we preserve all previously introduced symbols and we add new ones. This looks quite limiting for the power of these grammars. On the other hand, in contextual grammars there is a clear context-sensing capability, the contexts are adjoined to their selectors and depend on them. Context-sensitivity is in general a powerful property. Context-sensitivity plus erasing produces everything. In many cases in formal language theory, this combination leads to characterizations of recursively enumerable languages. Such a result has been proved by Ehrenfeucht, Faun, and Rozenberg (1997) for contextual grammars with unrestricted use of selectors. In the last section of this paper, we prove that this is also true for the case of maximal use of selectors. Specifically, we prove that every recursively enumerable language, L, can be written in the form L = hi (h2-1(L&apos;)), where hi, h2 are morphisms and L&apos; is a language generated by a contextual grammar with maximal use of selectors. The proof uses the same construction as in Ehrenfeucht, Paun, and Rozenberg (1997), adapted to our class of grammars. The effect of h1, h2-1 can also be achieved by a sequential transducer (with finite</context>
<context position="35287" citStr="Rozenberg (1997)" startWordPosition="6145" endWordPosition="6147">N), CLA4g(REG)), L2 L1, (CLin(REG), CLmi (FIN)) (CL (REG), CLAv(REG)), L3 L4, (CL,( REG), CLA4g(FIN)), L2 L4, (CL( REG), CLA4g (REG)), L2 L4, (CLA4/ (FIN), CLA49 (FIN)), L2 L4, (CLA41(FIN), amg(REG)), L2 L4, (CLmi (REG), CLA4g(FIN)), L2 L4, (CLA4/ (REG), CLA4g(REG)), L2. Both families CLm(FIN) and CLmg(FIN) contain non-context-free languages, but there are linear languages not in CL,,, (REG), CLA4/(REG) (for example: L2) or in CLA4g(REG) (for example: L4). We conjecture that REG C CLA4g(FIN), however, the construction from the proof of the inclusion REG C CL, (FIN) from Ehrenfeucht, Faun, and Rozenberg (1997) cannot be directly modified for the Mg case. 4. On the Linguistic Relevance of Contextual Grammars with Maximal Use of Selectors With regard to their linguistic foundations, contextual grammars are closely related to American distributional linguistics, the potential of which they try to exploit. Let us quote some words of Manaster Ramer (1994, 4): &amp;quot;It is my contention that, until the early 1960&apos;s, the situation, as revealed by a close mathematical analysis of the underlying issues, was this: (a) there was no basis for concluding that &apos;in principle&apos; natural languages were anything but context</context>
<context position="61892" citStr="Rozenberg (1997)" startWordPosition="10815" endWordPosition="10816">n be found in Martin-Vide and Paun (1998), but a lot of questions remain to be clarified. The main problem is to find the most useful and natural type of structured contextual grammars for describing the structure of natural language syntactic constructions. 265 Computational Linguistics Volume 24, Number 2 6. Representations of Recursively Enumerable Languages Completing the study on (weak) generative power of contextual grammars from Section 3, we now give a result proving the eccentric position of families of contextual languages with regard to the Chomsky hierarchy. Ehrenfeucht, Faun, and Rozenberg (1997) prove that each recursively enumerable language L can be written in the form L h (h (L&apos;)), for L&apos; E CL, (FIN)and hi, h2 two morphisms. In view of the inclusion CLin (FIN) C CLmi(FIN), this result is valid also for L&apos; E CLAN(FIN) and L&apos; E CLAN(REG). Because CL(FIN) — CLmg(REG) 0, the result of Ehrenfeucht, Faun, and Rozenberg (1997) is not directly valid for L&apos; E CLA4g(REG) or L&apos; E CLmg(FIN). However, the result of Ehrenfeucht, Faun, and Rozenberg (1997) can also be extended to these cases. Because we shall use it below, we outline here the construction of Ehrenfeucht, PAun, and Rozenberg (199</context>
<context position="64522" citStr="Rozenberg (1997)" startWordPosition="11334" endWordPosition="11335">t that a word in (R u T)* must eventually be reached imposes the use of productions of type 1 for living u only, and the use of productions of types 2 and 3 for living a and dead u and /3, respectively. After using these rules, u is dead, v is living (type 1), the first a is dead, the new one is living 266 Marcus, Martin-Vide, and Nun Contextual Grammars (types 2 and 3). This ensures that the obtained word contains only dead symbols and killers in words of R and living terminal symbols. By means of h1, h2-1, only the living terminals remain. Note that the construction of Ehrenfeucht, Nun, and Rozenberg (1997) does not work directly for the global maximal case: the grammar Go can contain, for instance, two rules u -4 v, u&apos; v&apos; with u a proper subword of /4&apos;; the first rule cannot be simulated in G when u&apos; is present, because we are forced to use the maximal selector, u&apos; in this case. However, the proof can be modified to cover the case of global maximal selectors as well. Theorem 3 Every language L E RE can be written in the form L = hi (h2-1(L&apos;)), where L&apos; E CLmg(FIN) and h1, h2 are two morphisms. Proof Take L C T*,L E RE, and take a type-0 Chomsky grammar Go (N,T,S,P) for L in the Kuroda normal fo</context>
<context position="65807" citStr="Rozenberg (1997)" startWordPosition="11598" endWordPosition="11599">X, Y, Z N, a E T, 2. XY ZU, for X, Y, Z, L/ E N. (Context-free rules and non-context-free rules, respectively, all of them with left-hand and right-hand members of length at most two.) Take a new symbol, c T, and construct the Chomsky grammar G1 =- (NU {S&apos;}, TU {c},S&apos;,P&apos;), where: {S&apos; Sc} U U {XY ZU XY -+ ZU E P, X, Y , Z, U E N} U U {Xa -&gt; xa I X -4 x is a rule of type 1 in P and a E NUTU {c}}. It is easy to see that L(G1) = L(G0){c}. Now start the procedure of Ehrenfeucht, Nun, and Rozenberg (1997) from the grammar G1, constructing the contextual grammar G exactly as in Ehrenfeucht, Nun, and Rozenberg (1997) and extending the morphisms h1, h2 by: hi(c) A, h2(c) = c. Because all rules in P&apos;, excepting S&apos; -4 Sc, which is used only once, have left-hand members of the same length, the maximal restriction of using the associated selectors has no effect. Concerning selectors u and c[u], appearing in productions of type 1 and type 2, respectively, the first selector for u is already dead (as is the case of the second selector), so its use is illegal; it leads to nonsuccessful derivations. The symbol c is preserved by h2-1 and it is erased by h1. Consequently, with the details of the proof in Ehrenfeucht</context>
</contexts>
<marker>Rozenberg, 1997</marker>
<rawString>Rozenberg, Grzegorz and Arto Salomaa, editors. 1997. Handbook of Formal Languages, 3 volumes. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arto Salomaa</author>
</authors>
<title>Formal Languages.</title>
<date>1973</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="17378" citStr="Salomaa (1973)" startWordPosition="2724" endWordPosition="2725"> (which we also call vocabulary), we denote by V* the set of all words (equivalently: strings) over V. including the empty one, which is denoted by A. The set of all nonempty words over V, hence V* — {A}, is denoted by V. The length of x E V* is denoted by lx1 and its mirror image (also called the reversal) by mi(x). The families of finite, regular, linear, context-free, context-sensitive, and recursively enumerable languages are denoted by FIN, REG, LIN, CF, CS, RE, respectively. For the elements of formal language theory we use, we refer to Harrison (1978), Rozenberg and Salomaa (1997), and Salomaa (1973).3 A contextual grammar (with choice) is a construct: G = (V, A, (Si,C1), • • • ,(Sn,Cn)), n &gt;1, where V is an alphabet, A is a finite language over V. Si, are languages over V. and C1, , C are finite subsets of V* x V*. The elements of A are called axioms (starting words), the sets Si are called selectors, and the elements of sets C„ written in the form (u, v), are called contexts. The pairs (S,, C,) are also called productions. The intuition behind this construction is that the contexts in C, may be adjoined to words -in the associated set S. Formally, we define the direct derivation relatio</context>
</contexts>
<marker>Salomaa, 1973</marker>
<rawString>Salomaa, Arto. 1973. Formal Languages. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter J Savitch</author>
</authors>
<title>Infinity is in the eyes of the beholder.</title>
<date>1991</date>
<booktitle>Interdisciplinary Approaches to Language: Essays in Honor of S.-Y.</booktitle>
<pages>487--500</pages>
<editor>In C. Georgopoulos and R. Ishihara, editors,</editor>
<publisher>Kuroda. Kluwer,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="8127" citStr="Savitch (1991" startWordPosition="1247" endWordPosition="1248">cality for English (one level excluding compound words, the other level allowing the building of compound words) and Hungarian, but six levels for the analysis of French verbs. The reason for this situation is the &amp;quot;open&amp;quot; character of natural languages, making it impossible to formulate a necessary and sufficient condition for a sentence to be well-formed. As is pointed out by Hockett (1970), the set of well-formed strings in a natural language should be both finite and infinite, a requirement that is impossible to fulfill in the framework of classical set theory; for a related discussion, see Savitch (1991, 1993). This can also be related to Chomsky&apos;s claim that a basic problem in linguistics is to find a grammar able to generate all and only the well-formed strings in a natural language. Chomsky&apos;s claim presupposes that natural languages have the status of formal languages, but not everyone agrees with this notion. Even for programming languages, many authors reject the idea that well-formed strings constitute a formal language; see, for instance, the various articles in the collective volume Olivetti (1970), as well as Marcus (1979). Returning to constructions specific to natural languages, w</context>
</contexts>
<marker>Savitch, 1991</marker>
<rawString>Savitch, Walter J. 1991. Infinity is in the eyes of the beholder. In C. Georgopoulos and R. Ishihara, editors, Interdisciplinary Approaches to Language: Essays in Honor of S.-Y. Kuroda. Kluwer, Dordrecht, pages 487-500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter J Savitch</author>
</authors>
<title>Why it might pay to assume that languages are infinite.</title>
<date>1993</date>
<journal>Annals of Mathematics and Artificial Intelligence, 8(1-2): Mathematics of language,</journal>
<pages>17--25</pages>
<marker>Savitch, 1993</marker>
<rawString>Savitch, Walter J. 1993. Why it might pay to assume that languages are infinite. Annals of Mathematics and Artificial Intelligence, 8(1-2): Mathematics of language, pages 17-25.</rawString>
</citation>
<citation valid="true">
<date>1987</date>
<booktitle>The Formal Complexity of Natural Language.</booktitle>
<editor>Savitch, Walter J., Emmon Bach, William Marsh, and Gila Safran-Naveh, editors.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="48834" citStr="[1987]" startWordPosition="8461" endWordPosition="8461">above cannot be generated by a tree adjoining grammar (TAG) in the pure form introduced by Joshi, Levy, and Takahashi (1975), but CF c TAL, where TAL is the family of languages generated by TAGs without additional features (see also Section 21.2 in Partee, ter Meulen, and Wall [1990]). In view of the languages L2 and L3 in Section 3, which are context-free but not in CLmg(REG), or CLmi(REG), respectively, it follows that TAL is incomparable with each of the families CLmi (REG), and CLmg(REG). However, TAGs with constraints (for instance, with null adjoining contraints; see, for example, Joshi [1987] and references therein) can generate all languages M1, M2, and M3; hence, a proper superfamily of TAL is obtained. The relationships between such enlarged TAL families and families of contextual languages are not settled yet. An important question in this framework is whether or not the languages in the families CL (REG), a E {Ml, Mg}, are mildly context-sensitive. It is obvious that, by definition, contextual languages have the bounded growth property: the set of contexts is finite, passing from one string to another means adjoining of a context from a finite set, and all generated strings b</context>
</contexts>
<marker>1987</marker>
<rawString>Savitch, Walter J., Emmon Bach, William Marsh, and Gila Safran-Naveh, editors. 1987. The Formal Complexity of Natural Language. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Evidence against the context-freeness of natural language.</title>
<date>1985</date>
<journal>Linguistics and Philosophy,</journal>
<pages>8--333</pages>
<contexts>
<context position="5994" citStr="Shieber (1985)" startWordPosition="912" endWordPosition="913">non-context-free features lead to formal languages of the forms {xcx I x E {a, b}* } (duplicated words of arbitrary length), {anbmcndm n, m &gt; 1} (two crossed dependencies), and {anbncn I n &gt; 1} gat least] three correlated positions). All of them are non-context-free languages and all of them can be generated in a surprisingly simple way by contextual grammars with selectors used in the maximal mode. Examples of natural language constructions based on reduplication were found, for instance, by Culy (1985), and Radzinski (1990), whereas crossed dependencies were demonstrated for Swiss German by Shieber (1985); see also Partee, ter Meulen and Wall (1990) or a number of contributions to Savitch et al. (1987). Multiple agreements were identified early on in programming languages (see, for example, Floyd [1962]), and certain constructions having such characteristics can also be found in natural languages. We shall give some arguments in Section 4. Some remarks are in order here. Although we mainly deal with the syntax of natural languages, we sometimes also mention artificial languages, mainly programming languages. Without entering into details outside the scope of our paper,1 we adopt the standpoint</context>
<context position="41412" citStr="Shieber 1985" startWordPosition="7155" endWordPosition="7156"> a language from the Mande family in Africa (Culy 1985) is illustrative: compound words of the form string-of-words-o-string-of-words are possible in this language. The corresponding formal language consists of words of the form xcx, for x an arbitrarily long word over an alphabet not containing the symbol c (this symbol corresponds to the separator o in the Bambara construction). Because we can always codify words using two symbols, we work here with the language: = {XCX X E 0,1311. Another non-context-free construction has been found in a dialect of German spoken around Zurich, Switzerland (Shieber 1985; Pullum 1985), which allows construc257 Computational Linguistics Volume 24, Number 2 tions of the form NP; NP &apos;j Va&apos;n&apos; , where NP, are accusative noun phrases, NPd are dative noun phrases, Va are accusative-demanding verbs, Vd are dative-demanding verbs, and the numbers match up, that is m = n = n&apos;. This leads to languages of the form: M2 = fambnedn I n, m &gt; 11. Both of these constructions can be easily found in programming languages, too. The proof of Floyd (1962) that Algol 60 is not context-free leads to a language of Mi type. Intersecting any Algol-like language with a regular language c</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, Stuart M. 1985. Evidence against the context-freeness of natural language. Linguistics and Philosophy, 8:333-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>D Temperley</author>
</authors>
<title>Parsing English with a link grammar.</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196,</tech>
<institution>School of Computer Science, Carnegie Mellon University,</institution>
<location>Pittsburgh.</location>
<contexts>
<context position="16006" citStr="Sleator and Temperley 1991" startWordPosition="2504" endWordPosition="2507">ammars (with maximal use of selectors). Recently (see Martin-Vide and Nun [1998]), some attempts were made to introduce a structure into the strings generated by contextual grammars. An easy way to do so is to associate a tree to a derivation (just add a pair of parentheses to each context, then build a tree in the usual way: when reading a left parenthesis add a new edge, when reading a right parenthesis go back along the current edge, etc.) or a graph describing a dependence relation similar to those discussed in descriptive linguistics (see Chapter VI of Marcus [1967]) or in link grammars (Sleator and Temperley 1991; Grinberg, Lafferty, and Sleator 1995). We briefly present these possibilities here, although the linguistic relevance of the obtained structures is still being researched. Let us also mention that, by definition, contextual grammars are (fully) lexicalized (in accordance with many current trends in formal syntax) and that their languages have the bounded growth property. In view of all these results and properties, we believe that contextual grammars are an attractive model for natural language syntax, completing (but not necessarily competing with) the existing models, and that they deserve</context>
<context position="56605" citStr="Sleator and Temperley (1991)" startWordPosition="9863" endWordPosition="9866">rdinate to x(i). A structured string (x,Px) can be represented in a graphical form by writing the elements x(1), ,x(n) of x in a row and drawing above them arcs (x(i), x(j)) for i Px J. A structured string (x, p,) is called a simple string of center x(io) if the graph associated to it as described above is a tree with the root marked with x(i0) (the center corresponds to the predicative element of a sentence). The notion of a structured string is well-known in linguistics: see, for example, Chapter VI of Marcus (1967). A related notion has been recently considered, that of a link grammar: see Sleator and Temperley (1991), or Grinberg, Lafferty, and Sleator (1995). In a link grammar, the elements of a sentence are correctly related in a linkage, according to a pairing of left and right connectors given for each word in the dictionary, providing that the obtained dependence relation has several properties: the associated graph is connected, planar, etc. Because we do not investigate here the possibility of producing correct linkages, in the sense of Sleator and Temperley (1991), by using contextual grammars (such results appear in Martin-Vide and Faun [19981), we do not formally define the notion of a link gram</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Sleator, Daniel and D. Temperley. 1991. Parsing English with a link grammar. Technical Report CMU-CS-91-196, School of Computer Science, Carnegie Mellon University, Pittsburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>