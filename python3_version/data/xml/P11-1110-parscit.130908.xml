<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.99809">
Learning From Collective Human Behavior to
Introduce Diversity in Lexical Choice
</title>
<author confidence="0.99096">
Vahed Qazvinian
</author>
<affiliation confidence="0.99647">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.951914">
Ann Arbor, MI
</address>
<email confidence="0.998952">
vahed@umich.edu
</email>
<author confidence="0.982982">
Dragomir R. Radev
</author>
<affiliation confidence="0.997598">
School of Information
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.952361">
Ann Arbor, MI
</address>
<email confidence="0.999411">
radev@umich.edu
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962857142857">
We analyze collective discourse, a collective
human behavior in content generation, and
show that it exhibits diversity, a property of
general collective systems. Using extensive
analysis, we propose a novel paradigm for de-
signing summary generation systems that re-
flect the diversity of perspectives seen in real-
life collective summarization. We analyze 50
sets of summaries written by human about the
same story or artifact and investigate the diver-
sity of perspectives across these summaries.
We show how different summaries use vari-
ous phrasal information units (i.e., nuggets) to
express the same atomic semantic units, called
factoids. Finally, we present a ranker that em-
ploys distributional similarities to build a net-
work of words, and captures the diversity of
perspectives by detecting communities in this
network. Our experiments show how our sys-
tem outperforms a wide range of other docu-
ment ranking systems that leverage diversity.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871155555556">
In sociology, the term collective behavior is used to
denote mass activities that are not centrally coordi-
nated (Blumer, 1951). Collective behavior is dif-
ferent from group behavior in the following ways:
(a) it involves limited social interaction, (b) mem-
bership is fluid, and (c) it generates weak and un-
conventional norms (Smelser, 1963). In this paper,
we focus on the computational analysis of collective
discourse, a collective behavior seen in interactive
content contribution and text summarization in on-
line social media. In collective discourse each in-
dividual’s behavior is largely independent of that of
other individuals.
In social media, discourse (Grosz and Sidner,
1986) is often a collective reaction to an event. One
scenario leading to collective reaction to a well-
defined subject is when an event occurs (a movie is
released, a story occurs, a paper is published) and
people independently write about it (movie reviews,
news headlines, citation sentences). This process of
content generation happens over time, and each per-
son chooses the aspects to cover. Each event has
an onset and a time of death after which nothing is
written about it. Tracing the generation of content
over many instances will reveal temporal patterns
that will allow us to make sense of the text gener-
ated around a particular event.
To understand collective discourse, we are inter-
ested in behavior that happens over a short period
of time. We focus on topics that are relatively well-
defined in scope such as a particular event or a single
news event that does not evolve over time. This can
eventually be extended to events and issues that are
evolving either in time or scope such as elections,
wars, or the economy.
In social sciences and the study of complex sys-
tems a lot of work has been done to study such col-
lective systems, and their properties such as self-
organization (Page, 2007) and diversity (Hong and
Page, 2009; Fisher, 2009). However, there is little
work that studies a collective system in which mem-
bers individually write summaries.
In most of this paper, we will be concerned with
developing a complex systems view of the set of col-
lectively written summaries, and give evidence of
</bodyText>
<page confidence="0.958024">
1098
</page>
<note confidence="0.9792595">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997009125">
the diversity of perspectives and its cause. We be-
lieve that out experiments will give insight into new
models of text generation, which is aimed at model-
ing the process of producing natural language texts,
and is best characterized as the process of mak-
ing choices between alternate linguistic realizations,
also known as lexical choice (Elhadad, 1995; Barzi-
lay and Lee, 2002; Stede, 1995).
</bodyText>
<sectionHeader confidence="0.995825" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.999887271604939">
In summarization, a number of previous methods
have focused on diversity. (Mei et al., 2010) in-
troduce a diversity-focused ranking methodology
based on reinforced random walks in information
networks. Their random walk model introduces the
rich-gets-richer mechanism to PageRank with rein-
forcements on transition probabilities between ver-
tices. A similar ranking model is the Grasshopper
ranking model (Zhu et al., 2007), which leverages
an absorbing random walk. This model starts with
a regular time-homogeneous random walk, and in
each step the node with the highest weight is set
as an absorbing state. The multi-view point sum-
marization of opinionated text is discussed in (Paul
et al., 2010). Paul et al. introduce Compar-
ative LexRank, based on the LexRank ranking
model (Erkan and Radev, 2004). Their random walk
formulation is to score sentences and pairs of sen-
tences from opposite viewpoints (clusters) based on
both their representativeness of the collection as well
as their contrastiveness with each other. Once a lex-
ical similarity graph is built, they modify the graph
based on cluster information and perform LexRank
on the modified cosine similarity graph.
The most well-known paper that address diver-
sity in summarization is (Carbonell and Goldstein,
1998), which introduces Maximal Marginal Rele-
vance (MMR). This method is based on a greedy
algorithm that picks sentences in each step that are
the least similar to the summary so far. There are
a few other diversity-focused summarization sys-
tems like C-LexRank (Qazvinian and Radev, 2008),
which employs document clustering. These papers
try to increase diversity in summarizing documents,
but do not explain the type of the diversity in their in-
puts. In this paper, we give an insightful discussion
on the nature of the diversity seen in collective dis-
course, and will explain why some of the mentioned
methods may not work under such environments.
In prior work on evaluating independent contri-
butions in content generation, Voorhees (Voorhees,
1998) studied IR systems and showed that rele-
vance judgments differ significantly between hu-
mans but relative rankings show high degrees of sta-
bility across annotators. However, perhaps the clos-
est work to this paper is (van Halteren and Teufel,
2004) in which 40 Dutch students and 10 NLP re-
searchers were asked to summarize a BBC news re-
port, resulting in 50 different summaries. Teufel
and van Halteren also used 6 DUC1-provided sum-
maries, and annotations from 10 student participants
and 4 additional researchers, to create 20 summaries
for another news article in the DUC datasets. They
calculated the Kappa statistic (Carletta, 1996; Krip-
pendorff, 1980) and observed high agreement, indi-
cating that the task of atomic semantic unit (factoid)
extraction can be robustly performed in naturally oc-
curring text, without any copy-editing.
The diversity of perspectives and the unprece-
dented growth of the factoid inventory also affects
evaluation in text summarization. Evaluation meth-
ods are either extrinsic, in which the summaries are
evaluated based on their quality in performing a spe-
cific task (Sp¨arck-Jones, 1999) or intrinsic where the
quality of the summary itself is evaluated, regardless
of any applied task (van Halteren and Teufel, 2003;
Nenkova and Passonneau, 2004). These evaluation
methods assess the information content in the sum-
maries that are generated automatically.
Finally, recent research on analyzing online so-
cial media shown a growing interest in mining news
stories and headlines because of its broad appli-
cations ranging from “meme” tracking and spike
detection (Leskovec et al., 2009) to text summa-
rization (Barzilay and McKeown, 2005). In sim-
ilar work on blogs, it is shown that detecting top-
ics (Kumar et al., 2003; Adar et al., 2007) and sen-
timent (Pang and Lee, 2004) in the blogosphere can
help identify influential bloggers (Adar et al., 2004;
Java et al., 2006) and mine opinions about prod-
ucts (Mishne and Glance, 2006).
</bodyText>
<footnote confidence="0.654998">
1Document Understanding Conference
</footnote>
<page confidence="0.99764">
1099
</page>
<sectionHeader confidence="0.990751" genericHeader="method">
3 Data Annotation
</sectionHeader>
<bodyText confidence="0.999913">
The datasets used in our experiments represent two
completely different categories: news headlines, and
scientific citation sentences. The headlines datasets
consist of 25 clusters of news headlines collected
from Google News2, and the citations datasets have
25 clusters of citations to specific scientific papers
from the ACL Anthology Network (AAN)3. Each
cluster consists of a number of unique summaries
(headlines or citations) about the same artifact (non-
evolving news story or scientific paper) written by
different people. Table 1 lists some of the clusters
with the number of summaries in them.
</bodyText>
<table confidence="0.9991524">
ID type Name Story/Title #
1 hdl miss Miss Venezuela wins miss universe’09 125
2 hdl typhoon Second typhoon hit philippines 100
3 hdl russian Accident at Russian hydro-plant 101
4 hdl redsox Boston Red Sox win world series 99
5 hdl gervais “Invention of Lying” movie reviewed 97
� � � � � � � � �
25 hdl yale Yale lab tech in court 10
26 cit N03-1017 Statistical Phrase-Based Translation 172
27 cit P02-1006 Learning Surface Text Patterns ... 72
28 cit P05-1012 On-line Large-Margin Training ... 71
29 cit C96-1058 Three New Probabilistic Models ... 66
30 cit P05-1033 A Hierarchical Phrase-Based Model ... 65
� � � � � � � � �
50 cit H05-1047 A Semantic Approach to Recognizing ... 7
</table>
<tableCaption confidence="0.963214333333333">
Table 1: Some of the annotated datasets and the number
of summaries in each of them (hdl = headlines; cit = cita-
tions)
</tableCaption>
<subsectionHeader confidence="0.998208">
3.1 Nuggets vs. Factoids
</subsectionHeader>
<bodyText confidence="0.999525833333333">
We define an annotation task that requires explicit
definitions that distinguish between phrases that rep-
resent the same or different information units. Un-
fortunately, there is little consensus in the literature
on such definitions. Therefore, we follow (van Hal-
teren and Teufel, 2003) and make the following dis-
tinction. We define a nugget to be a phrasal infor-
mation unit. Different nuggets may all represent
the same atomic semantic unit, which we call as a
factoid. In the following headlines, which are ran-
domly extracted from the redsox dataset, nuggets
are manually underlined.
</bodyText>
<footnote confidence="0.806244333333333">
red sox win 2007 world series
boston red sox blank rockies to clinch world series
2news.google.com
3http://clair.si.umich.edu/clair/anthology/
boston fans celebrate world series win; 37 arrests re-
ported
</footnote>
<bodyText confidence="0.986414">
These 3 headlines contain 9 nuggets, which rep-
resent 5 factoids or classes of equivalent nuggets.
</bodyText>
<equation confidence="0.9992864">
f1 : {red sox, boston, boston red sox}
f2 : {2007 world series, world series win, world series}
f3 : {rockies}
f4 : {37 arrests}
f5 : {fans celebrate}
</equation>
<bodyText confidence="0.950581125">
This example suggests that different headlines on
the same story written independently of one an-
other use different phrases (nuggets) to refer to the
same semantic unit (e.g., “red sox” vs. “boston” vs.
“boston red sox”) or to semantic units corresponding
to different aspects of the story (e.g., “37 arrests” vs.
“rockies”). In the former case different nuggets are
used to represent the same factoid, while in the latter
case different nuggets are used to express different
factoids. This analogy is similar to the definition of
factoids in (van Halteren and Teufel, 2004).
The following citation sentences to Koehn’s work
suggest that a similar phenomenon also happens in
citations.
We also compared our model with pharaoh (Koehn et al,
2003).
</bodyText>
<figureCaption confidence="0.810846285714286">
Koehn et al (2003) find that
phrases longer than three words improve per-
formance little.
Koehn et al (2003) suggest limiting phrase length
to three words or less.
For further information on these parameter settings,
confer (koehn et al, 2003).
</figureCaption>
<bodyText confidence="0.999945363636363">
where the first author mentions “pharaoh” as a
contribution of Koehn et al, but the second and third
use different nuggets to represent the same contribu-
tion: use of trigrams. However, as the last citation
shows, a citation sentence, unlike news headlines,
may cover no information about the target paper.
The use of phrasal information as nuggets is an es-
sential element to our experiments, since some head-
line writers often try to use uncommon terms to re-
fer to a factoid. For instance, two headlines from the
redsox cluster are:
</bodyText>
<footnote confidence="0.4283495">
Short wait for bossox this time
Soxcess started upstairs
</footnote>
<page confidence="0.969083">
1100
</page>
<bodyText confidence="0.99992575">
Following these examples, we asked two anno-
tators to annotate all 1, 390 headlines, and 926 ci-
tations. The annotators were asked to follow pre-
cise guidelines in nugget extraction. Our guidelines
instructed annotators to extract non-overlapping
phrases from each headline as nuggets. Therefore,
each nugget should be a substring of the headline
that represents a semantic unit4.
Previously (Lin and Hovy, 2002) had shown that
information overlap judgment is a difficult task for
human annotators. To avoid such a difficulty, we
enforced our annotators to extract non-overlapping
nuggets from a summary to make sure that they are
mutually independent and that information overlap
between them is minimized.
Finding agreement between annotated well-
defined nuggets is straightforward and can be cal-
culated in terms of Kappa. However, when nuggets
themselves are to be extracted by annotators, the
task becomes less obvious. To calculate the agree-
ment, we annotated 10 randomly selected head-
line clusters twice and designed a simple evalua-
tion scheme based on Kappa5. For each n-gram,
w, in a given headline, we look if w is part of any
nugget in either human annotations. If w occurs
in both or neither, then the two annotators agree
on it, and otherwise they do not. Based on this
agreement setup, we can formalize the n statistic
</bodyText>
<equation confidence="0.644559">
Pr(a)−Pr(e)
</equation>
<bodyText confidence="0.80032">
as n = where Pr(a) is the relative ob-
</bodyText>
<equation confidence="0.590827">
1−Pr(e)
</equation>
<bodyText confidence="0.999968666666667">
served agreement among annotators, and Pr(e) is
the probability that annotators agree by chance if
each annotator is randomly assigning categories.
Table 2 shows the unigram, bigram, and trigram-
based average n between the two human annotators
(Human1, Human2). These results suggest that
human annotators can reach substantial agreement
when bigram and trigram nuggets are examined, and
has reasonable agreement for unigram nuggets.
</bodyText>
<sectionHeader confidence="0.997577" genericHeader="method">
4 Diversity
</sectionHeader>
<bodyText confidence="0.999974666666667">
We study the diversity of ways with which human
summarizers talk about the same story or event and
explain why such a diversity exists.
</bodyText>
<footnote confidence="0.96847">
4Before the annotations, we lower-cased all summaries and
removed duplicates
5Previously (Qazvinian and Radev, 2010) have shown high
agreement in human judgments in a similar task on citation an-
notation
</footnote>
<figure confidence="0.615275142857143">
Average n
unigram bigram trigram
Human1 vs. Human2
0.76 ± 0.4 0.80 ± 0.4 0.89 ± 0.3
Table 2: Agreement between different annotators in terms
of average Kappa in 25 headline clusters.
c c
</figure>
<figureCaption confidence="0.9978235">
Figure 1: The cumulative probability distribution for the
frequency of factoids (i.e., the probability that a factoid
will be mentioned in c different summaries) across in
each category.
</figureCaption>
<subsectionHeader confidence="0.988968">
4.1 Skewed Distributions
</subsectionHeader>
<bodyText confidence="0.999960777777778">
Our first experiment is to analyze the popularity of
different factoids. For each factoid in the annotated
clusters, we extract its count, X, which is equal to
the number of summaries it has been mentioned in,
and then we look at the distribution of X. Fig-
ure 1 shows the cumulative probability distribution
for these counts (i.e., the probability that a factoid
will be mentioned in at least c different summaries)
in both categories.
These highly skewed distributions indicate that a
large number of factoids (more than 28%) are only
mentioned once across different clusters (e.g., “poor
pitching of colorado” in the redsox cluster), and
that a few factoids are mentioned in a large number
of headlines (likely using different nuggets). The
large number of factoids that are only mentioned in
one headline indicates that different summarizers in-
crease diversity by focusing on different aspects of
a story or a paper. The set of nuggets also exhibit
similar skewed distributions. If we look at individ-
ual nuggets, the redsox set shows that about 63
(or 80%) of the nuggets get mentioned in only one
headline, resulting in a right-skewed distribution.
The factoid analysis of the datasets reveals two
main causes for the content diversity seen in head-
lines: (1) writers focus on different aspects of the
story and therefore write about different factoids
</bodyText>
<figure confidence="0.996312142857143">
Pr(X ≥ c)
100
10−1
10−2
100 101 102
headlines
Pr(X ≥ c)
Pr(X ≥ c)
100
citations
Pr(X ≥ c)
10−1
10−2
100 101 102
</figure>
<page confidence="0.984387">
1101
</page>
<bodyText confidence="0.999737">
(e.g., “celebrations” vs. “poor pitching of col-
orado”). (2) writer use different nuggets to represent
the same factoid (e.g., “redsox” vs. “bosox”). In the
following sections we analyze the extent at which
each scenario happens.
</bodyText>
<figure confidence="0.530968">
headlines
number of summaries
</figure>
<figureCaption confidence="0.997708">
Figure 2: The number of unique factoids and nuggets ob-
served by reading n random summaries in all the clusters
of each category
</figureCaption>
<subsectionHeader confidence="0.974438">
4.2 Factoid Inventory
</subsectionHeader>
<bodyText confidence="0.999872628571429">
The emergence of diversity in covering different fac-
toids suggests that looking at more summaries will
capture a larger number of factoids. In order to ana-
lyze the growth of the factoid inventory, we perform
a simple experiment. We shuffle the set of sum-
maries from all 25 clusters in each category, and then
look at the number of unique factoids and nuggets
seen after reading nth summary. This number shows
the amount of information that a randomly selected
subset of n writers represent. This is important to
study in order to find out whether we need a large
number of summaries to capture all aspects of a
story and build a complete factoid inventory. The
plot in Figure 4.1 shows, at each n, the number of
unique factoids and nuggets observed by reading n
random summaries from the 25 clusters in each cat-
egory. These curves are plotted on a semi-log scale
to emphasize the difference between the growth pat-
terns of the nugget inventories and the factoid inven-
tories6.
This finding numerically confirms a similar ob-
servation on human summary annotations discussed
in (van Halteren and Teufel, 2003; van Halteren
and Teufel, 2004). In their work, van Halteren and
Teufel indicated that more than 10-20 human sum-
maries are needed for a full factoid inventory. How-
ever, our experiments with nuggets of nearly 2, 400
independent human summaries suggest that neither
the nugget inventory nor the number of factoids will
be likely to show asymptotic behavior. However,
these plots show that the nugget inventory grows at
a much faster rate than factoids. This means that a
lot of the diversity seen in human summarization is
a result of the so called different lexical choices that
represent the same semantic units or factoids.
</bodyText>
<subsectionHeader confidence="0.997178">
4.3 Summary Quality
</subsectionHeader>
<bodyText confidence="0.999987592592593">
In previous sections we gave evidence for the diver-
sity seen in human summaries. However, a more
important question to answer is whether these sum-
maries all cover important aspects of the story. Here,
we examine the quality of these summaries, study
the distribution of information coverage in them,
and investigate the number of summaries required
to build a complete factoid inventory.
The information covered in each summary can be
determined by the set of factoids (and not nuggets)
and their frequencies across the datasets. For exam-
ple, in the redsox dataset, “red sox”, “boston”, and
“boston red sox” are nuggets that all represent the
same piece of information: the red sox team. There-
fore, different summaries that use these nuggets to
refer to the red sox team should not be seen as very
different.
We use the Pyramid model (Nenkova and Pas-
sonneau, 2004) to value different summary factoids.
Intuitively, factoids that are mentioned more fre-
quently are more salient aspects of the story. There-
fore, our pyramid model uses the normalized fre-
quency at which a factoid is mentioned across a
dataset as its weight. In the pyramid model, the in-
dividual factoids fall in tiers. If a factoid appears in
more summaries, it falls in a higher tier. In princi-
ple, if the term wi appears IwiI times in the set of
</bodyText>
<footnote confidence="0.9460235">
6Similar experiment using individual clusters exhibit similar
behavior
</footnote>
<figure confidence="0.99872428">
1000
Nuggets
Factoids
800
600
400
200
0
100 101 102 103
number of summaries
Inventory size
350
300
250
200
150
100
Nuggets
Factoids
50
0
100 101
102 103
Inventory size
citations
</figure>
<page confidence="0.995492">
1102
</page>
<bodyText confidence="0.999833833333334">
headlines it is assigned to the tier T�w,�. The pyra-
mid score that we use is computed as follows. Sup-
pose the pyramid has n tiers, Ti, where tier Tn is
the top tier and T1 is the bottom. The weight of
the factoids in tier Ti will be i (i.e. they appeared
in i summaries). If |Ti |denotes the number of fac-
toids in tier Ti, and Di is the number of factoids in
the summary that appear in Ti, then the total factoid
weight for the summary is D = Eni�1 i × Di. Ad-
ditionally, the optimal pyramid score for a summary
is Max = Eni�1 i × |Ti|. Finally, the pyramid score
for a summary can be calculated as
</bodyText>
<equation confidence="0.958661">
D
P = Max
</equation>
<bodyText confidence="0.995737547619047">
Based on this scoring scheme, we can use the an-
notated datasets to determine the quality of individ-
ual headlines. First, for each set we look at the vari-
ation in pyramid scores that individual summaries
obtain in their set. Figure 3 shows, for each clus-
ter, the variation in the pyramid scores (25th to 75th
percentile range) of individual summaries evaluated
against the factoids of that cluster. This figure in-
dicates that the pyramid score of almost all sum-
maries obtain values with high variations in most of
the clusters For instance, individual headlines from
redsox obtain pyramid scores as low as 0.00 and
as high as 0.93. This high variation confirms the pre-
vious observations on diversity of information cov-
erage in different summaries.
Additionally, this figure shows that headlines gen-
erally obtain higher values than citations when con-
sidered as summaries. One reason, as explained be-
fore, is that a citation may not cover any important
contribution of the paper it is citing, when headlines
generally tend to cover some aspects of the story.
High variation in quality means that in order to
capture a larger information content we need to read
a greater number of summaries. But how many
headlines should one read to capture a desired level
of information content? To answer this question,
we perform an experiment based on drawing random
summaries from the pool of all the clusters in each
category. We perform a Monte Carlo simulation, in
which for each n, we draw n random summaries,
and look at the pyramid score achieved by reading
these headlines. The pyramid score is calculated us-
ing the factoids from all 25 clusters in each cate-
gory7. Each experiment is repeated 1, 000 times to
find the statistical significance of the experiment and
the variation from the average pyramid scores.
Figure 4.3 shows the average pyramid scores over
different n values in each category on a log-log
scale. This figure shows how pyramid score grows
and approaches 1.00 rapidly as more randomly se-
lected summaries are seen.
number of summaries
</bodyText>
<figureCaption confidence="0.917189">
Figure 4: Average pyramid score obtained by reading n
random summaries shows rapid asymptotic behavior.
</figureCaption>
<sectionHeader confidence="0.994927" genericHeader="method">
5 Diversity-based Ranking
</sectionHeader>
<bodyText confidence="0.999981833333333">
In previous sections we showed that the diversity
seen in human summaries could be according to dif-
ferent nuggets or phrases that represent the same fac-
toid. Ideally, a summarizer that seeks to increase di-
versity should capture this phenomenon and avoid
covering redundant nuggets. In this section, we use
different state of the art summarization systems to
rank the set of summaries in each cluster with re-
spect to information content and diversity. To evalu-
ate each system, we cut the ranked list at a constant
length (in terms of the number of words) and calcu-
late the pyramid score of the remaining text.
</bodyText>
<subsectionHeader confidence="0.991181">
5.1 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.999772714285714">
We have designed a summary ranker that will pro-
duce a ranked list of documents with respect to the
diversity of their contents. Our model works based
on ranking individual words and using the ranked
list of words to rank documents that contain them.
In order to capture the nuggets of equivalent se-
mantic classes, we use a distributional similarity of
</bodyText>
<footnote confidence="0.9289635">
7Similar experiment using individual clusters exhibit similar
results
</footnote>
<figure confidence="0.996731641791045">
100
headlines
citations
100 101 102 103
10−2
Pyramid Score
10−1
1103
Pyramid Score
1
0.8
0.6
0.4
0.2
0
headlines
citations
abortion
amazon
babies
burger
colombia
england
gervais
google
ireland
maine
mercury
miss
monkey
mozart
nobel
priest
ps3slim
radiation
redsox
russian
scientist
soupy
sweden
typhoon
yale
A00_1023
A00_1043
A00_2024
C00_1072
C96_1058
D03_1017
D04_9907
H05_1047
H05_1079
J04_4002
N03_1017
N04_1033
P02_1006
P03_1001
P05_1012
P05_1013
P05_1014
P05_1033
P97_1003
P99_1065
W00_0403
W00_0603
W03_0301
W03_0510
W05_1203
</figure>
<figureCaption confidence="0.999991">
Figure 3: The 25th to 75th percentile pyramid score range in individual clusters
</figureCaption>
<bodyText confidence="0.999822888888889">
words that is inspired by (Lee, 1999). We represent
each word by its context in the cluster and find the
similarity of such contexts. Particularly, each word
wi is represented by a bag of words, fi, that have a
surface distance of 3 or smaller to wi anywhere in
the cluster. In other words, fi contains any word that
co-occurs with wi in a 4-gram in the cluster. This
bag of words representation of words enables us to
find the word-pair similarities.
</bodyText>
<figureCaption confidence="0.930811">
Figure 5: Part of the word similarity graph in the redsox
cluster
</figureCaption>
<figure confidence="0.917983173913043">
police
celebrations
arrest
fan poorer
unhappy
sweeps
second
sox jump
red
title
2nd
poor
glory
hitting
dynasty
victory
pitching
baseball
Pajek
Sim(wi,wj) =
(1)
�|�fivj|
�fi · �fj
</figure>
<bodyText confidence="0.999822862068965">
We use the pair-wise similarities of words in each
cluster, and build a network of words and their simi-
larities. Intuitively, words that appear in similar con-
texts are more similar to each other and will have a
stronger edge between them in the network. There-
fore, similar words, or words that appear in similar
contexts, will form communities in this graph. Ide-
ally, each community in the word similarity network
would represent a factoid. To find the communities
in the word network we use (Clauset et al., 2004), a
hierarchical agglomeration algorithm which works
by greedily optimizing the modularity in a linear
running time for sparse graphs.
The community detection algorithm will assign
to each word wi, a community label Ci. For each
community, we use LexRank to rank the words us-
ing the similarities in Equation 1, and assign a score
to each word wi as S(wi) = ��
|��|, where Ri is the
rank of wi in its community, and |Ci |is the number
of words that belong to Ci. Figure 5.1 shows part
of the word similarity graph in the redsox cluster,
in which each node is color-coded with its commu-
nity. This figure illustrates how words that are se-
mantically related to the same aspects of the story
fall in the same communities (e.g., “police” and “ar-
rest”). Finally, to rank sentences, we define the score
of each document Dj as the sum of the scores of its
words.
</bodyText>
<equation confidence="0.990326">
pds(Dj) = � S(wi)
w;EDj
</equation>
<bodyText confidence="0.9999266">
Intuitively, sentences that contain higher ranked
words in highly populated communities will have a
smaller score. To rank the sentences, we sort them
in an ascending order, and cut the list when its size
is greater than the length limit.
</bodyText>
<subsectionHeader confidence="0.912781">
5.2 Other Methods
5.2.1 Random
</subsectionHeader>
<bodyText confidence="0.9905075">
For each cluster in each category (citations and
headlines), this method simply gets a random per-
</bodyText>
<page confidence="0.994609">
1104
</page>
<bodyText confidence="0.99997">
mutations of the summaries. In the headlines
datasets, where most of the headlines cover some
factoids about the story, we expect this method to
perform reasonably well since randomization will
increase the chances of covering headlines that fo-
cus on different factoids. However, in the citations
dataset, where a citing sentence may cover no infor-
mation about the cited paper, randomization has the
drawback of selecting citations that have no valuable
information in them.
</bodyText>
<subsectionHeader confidence="0.935131">
5.2.2 LexRank
</subsectionHeader>
<bodyText confidence="0.999905125">
LexRank (Erkan and Radev, 2004) works by first
building a graph of all the documents (Di) in a
cluster. The edges between corresponding nodes
(di) represent the cosine similarity between them is
above a threshold (0.10 following (Erkan and Radev,
2004)). Once the network is built, the system finds
the most central sentences by performing a random
walk on the graph.
</bodyText>
<equation confidence="0.9263805">
p(dj) = (1 − λ) |D |+ λEp(di)P(di → dj) (2)
d;
</equation>
<subsectionHeader confidence="0.732398">
5.2.3 MMR
</subsectionHeader>
<bodyText confidence="0.9969175">
Maximal Marginal Relevance (MMR) (Carbonell
and Goldstein, 1998) uses the pairwise cosine simi-
larity matrix and greedily chooses sentences that are
the least similar to those already in the summary. In
</bodyText>
<equation confidence="0.775558">
particular,
L i
MMR = arg minD;ED−A maxDjEA Sim(Di, Dj)
</equation>
<bodyText confidence="0.936903">
where A is the set of documents in the summary,
initialized to A = 0.
</bodyText>
<subsectionHeader confidence="0.814869">
5.2.4 DivRank
</subsectionHeader>
<bodyText confidence="0.999871636363637">
Unlike other time-homogeneous random walks
(e.g., PageRank), DivRank does not assume that
the transition probabilities remain constant over
time. DivRank uses a vertex-reinforced random
walk model to rank graph nodes based on a diversity
based centrality. The basic assumption in DivRank
is that the transition probability from a node to other
is reinforced by the number of previous visits to the
target node (Mei et al., 2010). Particularly, let’s as-
sume pT(u, v) is the transition probability from any
node u to node v at time T. Then,
</bodyText>
<equation confidence="0.978406">
pT (di, dj) = (1 − λ).p*(dj) + λ.p0(di, dj).NT (dj) (3)
DT (di)
</equation>
<bodyText confidence="0.9972495">
where NT (dj) is the number of times the walk has
visited dj up to time T and
</bodyText>
<equation confidence="0.979865">
DT (di) = E p0(di, dj)NT (dj) (4)
djEV
</equation>
<bodyText confidence="0.99635">
Here, p*(dj) is the prior distribution that deter-
mines the preference of visiting vertex dj. We try
two variants of this algorithm: DivRank, in which
p*(dj) is uniform, and DivRank with priors in
which p*(dj) a l(Dj)−1, where l(Dj) is the num-
ber of the words in the document Dj and β is a pa-
rameter (β = 0.8).
</bodyText>
<subsectionHeader confidence="0.925319">
5.2.5 C-LexRank
</subsectionHeader>
<bodyText confidence="0.99997125">
C-LexRank is a clustering-based model in which
the cosine similarities of document pairs are used to
build a network of documents. Then the the network
is split into communities, and the most salient doc-
uments in each community are selected (Qazvinian
and Radev, 2008). C-LexRank focuses on finding
communities of documents using their cosine simi-
larity. The intuition is that documents that are more
similar to each other contain similar factoids. We ex-
pect C-LexRank to be a strong ranker, but incapable
of capturing the diversity caused by using different
phrases to express the same meaning. The reason is
that different nuggets that represent the same factoid
often have no words in common (e.g., “victory” and
“glory”) and won’t be captured by a lexical measure
like cosine similarity.
</bodyText>
<subsectionHeader confidence="0.875224">
5.3 Experiments
</subsectionHeader>
<bodyText confidence="0.9994606875">
We use each of the systems explained above to rank
the summaries in each cluster. Each ranked list is
then cut at a certain length (50 words for headlines,
and 150 for citations) and the information content
in the remaining text is examined using the pyramid
score.
Table 3 shows the average pyramid score achieved
by different methods in each category. The method
based on the distributional similarities of words out-
performs other methods in the citations category. All
methods show similar results in the headlines cate-
gory, where most headlines cover at least 1 factoid
about the story and a random ranker performs rea-
sonably well. Table 4 shows top 3 headlines from
3 rankers: word distributional similarity (WDS), C-
LexRank, and MMR. In this example, the first 3
</bodyText>
<page confidence="0.973312">
1105
</page>
<table confidence="0.998395">
Method headlines citations Mean
pyramid 95% C.I. pyramid 95% C.I.
R 0.928 [0.896, 0.959] 0.716 [0.625, 0.807] 0.822
MMR 0.930 [0.902, 0.960] 0.766 [0.684, 0.847] 0.848
LR 0.918 [0.891, 0.945] 0.728 [0.635, 0.822] 0.823
DR 0.927 [0.900, 0.955] 0.736 [0.667, 0.804] 0.832
DR(p) 0.916 [0.884, 0.949] 0.764 [0.697, 0.831] 0.840
C-LR 0.942 [0.919, 0.965] 0.781 [0.710, 0.852] 0.862
WDS 0.931 [0.905, 0.958] 0.813 [0.738, 0.887] 0.872
R=Random; LR=LexRank; DR=DivRank; DR(p)=DivRank with Priors; C-
LR=C-LexRank; WDS=Word Distributional Similarity; C.I.=Confidence In-
terval
</table>
<tableCaption confidence="0.99943">
Table 3: Comparison of different ranking systems
</tableCaption>
<table confidence="0.761163454545455">
Method Top 3 headlines
1: how sweep it is
WDS 2: fans celebrate red sox win
3: red sox take title
1: world series: red sox sweep rockies
C-LR 2: red sox take world series
3: red sox win world series
1:red sox scale the rockies
MMR 2: boston sweep colorado to win world series
3: rookies respond in first crack at the big time
C-LR=C-LexRank; WDS=Word Distributional Similarity
</table>
<tableCaption confidence="0.973642">
Table 4: Top 3 ranked summaries of the redsox cluster
using different methods
</tableCaption>
<bodyText confidence="0.9997635">
headlines produced by WDS cover two important
factoids: “red sox winning the title” and “fans cel-
ebrating”. However, the second factoid is absent in
the other two.
</bodyText>
<sectionHeader confidence="0.996433" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999974823529412">
Our experiments on two different categories of
human-written summaries (headlines and citations)
showed that a lot of the diversity seen in human
summarization comes from different nuggets that
may actually represent the same semantic informa-
tion (i.e., factoids). We showed that the factoids ex-
hibit a skewed distribution model, and that the size
of the nugget inventory asymptotic behavior even
with a large number of summaries. We also showed
high variation in summary quality across different
summaries in terms of pyramid score, and that the
information covered by reading n summaries has a
rapidly growing asymptotic behavior as n increases.
Finally, we proposed a ranking system that employs
word distributional similarities to identify semanti-
cally equivalent words, and compared it with a wide
range of summarization systems that leverage diver-
sity.
In the future, we plan to move to content from
other collective systems on Web. In order to gen-
eralize our findings, we plan to examine blog com-
ments, online reviews, and tweets (that discuss the
same URL). We also plan to build a generation sys-
tem that employs the Yule model (Yule, 1925) to de-
termine the importance of each aspect (e.g. who,
when, where, etc.) in order to produce summaries
that include diverse aspects of a story.
Our work has resulted in a publicly available
dataset 8 of 25 annotated news clusters with nearly
1, 400 headlines, and 25 clusters of citation sen-
tences with more than 900 citations. We believe that
this dataset can open new dimensions in studying di-
versity and other aspects of automatic text genera-
tion.
</bodyText>
<sectionHeader confidence="0.998742" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999325">
This work is supported by the National Science
Foundation grant number IIS-0705832 and grant
number IIS-0968489. Any opinions, findings, and
conclusions or recommendations expressed in this
paper are those of the authors and do not necessarily
reflect the views of the supporters.
</bodyText>
<sectionHeader confidence="0.996076" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.839218">
Eytan Adar, Li Zhang, Lada A. Adamic, and Rajan M.
Lukose. 2004. Implicit structure and the dynamics of
</reference>
<footnote confidence="0.93732">
8http://www-personal.umich.edu/˜vahed/
data.html
</footnote>
<page confidence="0.98822">
1106
</page>
<reference confidence="0.998849981308411">
Blogspace. In WWW’04, Workshop on the Weblogging
Ecosystem.
Eytan Adar, Daniel S. Weld, Brian N. Bershad, and
Steven S. Gribble. 2007. Why we search: visualiz-
ing and predicting user behavior. In WWW’07, pages
161–170, New York, NY, USA.
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing - Volume 10,
EMNLP ’02, pages 164–171.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Comput. Linguist., 31(3):297–328.
Herbert Blumer. 1951. Collective behavior. In Lee, Al-
fred McClung, Ed., Principles of Sociology.
Jaime G. Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In SIGIR’98, pages
335–336.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22(2):249–254.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70(6).
Michael Elhadad. 1995. Using argumentation in text
generation. Journal of Pragmatics, 24:189–220.
G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Len Fisher. 2009. The Perfect Swarm: The Science of
Complexity in Everyday Life. Basic Books.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12:175–204, July.
Lu Hong and Scott Page. 2009. Interpreted and
generated signals. Journal of Economic Theory,
144(5):2174–2196.
Akshay Java, Pranam Kolari, Tim Finin, and Tim Oates.
2006. Modeling the spread of influence on the blogo-
sphere. In WWW’06.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Beverly Hills: Sage Pub-
lications.
Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and
Andrew Tomkins. 2003. On the bursty evolution of
blogspace. In WWW’03, pages 568–576, New York,
NY, USA.
Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, pages 25–32.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD ’09: Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 497–506.
Chin-Yew Lin and Eduard Hovy. 2002. Manual and au-
tomatic evaluation of summaries. In ACL-Workshop
on Automatic Summarization.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Di-
vrank: the interplay of prestige and diversity in infor-
mation networks. In Proceedings of the 16th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 1009–1018.
Gilad Mishne and Natalie Glance. 2006. Predicting
movie sales from blogger sentiment. In AAAI 2006
Spring Symposium on Computational Approaches to
Analysing Weblogs (AAAI-CAAW 2006).
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. Proceedings of the HLT-NAACL conference.
Scott E. Page. 2007. The Difference: How the Power of
Diversity Creates Better Groups, Firms, Schools, and
Societies. Princeton University Press.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL’04, Morristown,
NJ, USA.
Michael Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 66–76.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In COLING 2008, Manchester, UK.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555–564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Neil J. Smelser. 1963. Theory of Collective Behavior.
Free Press.
Karen Sp¨arck-Jones. 1999. Automatic summarizing:
factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in automatic text summa-
rization, chapter 1, pages 1 – 12. The MIT Press.
Manfred Stede. 1995. Lexicalization in natural language
generation: a survey. Artificial Intelligence Review,
(8):309–336.
Hans van Halteren and Simone Teufel. 2003. Examin-
ing the consensus between human summaries: initial
experiments with factoid analysis. In Proceedings of
</reference>
<page confidence="0.906344">
1107
</page>
<reference confidence="0.996979956521739">
the HLT-NAACL 03 on Text summarization workshop,
pages 57–64, Morristown, NJ, USA. Association for
Computational Linguistics.
Hans van Halteren and Simone Teufel. 2004. Evaluating
information content by factoid analysis: human anno-
tation and stability. In EMNLP’04, Barcelona.
Ellen M. Voorhees. 1998. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
In SIGIR ’98: Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 315–323.
G. Udny Yule. 1925. A mathematical theory of evo-
lution, based on the conclusions of dr. j. c. willis,
f.r.s. Philosophical Transactions of the Royal Society
of London. Series B, Containing Papers of a Biological
Character, 213:21–87.
Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and
David Andrzejewski. 2007. Improving diversity in
ranking using absorbing random walks. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 97–104.
</reference>
<page confidence="0.996259">
1108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.275280">
<title confidence="0.966203">Learning From Collective Human Behavior Introduce Diversity in Lexical Choice</title>
<author confidence="0.845781">Vahed</author>
<affiliation confidence="0.9832905">Department of University of</affiliation>
<author confidence="0.63713">Ann Arbor</author>
<email confidence="0.99887">vahed@umich.edu</email>
<author confidence="0.914755">R Dragomir</author>
<affiliation confidence="0.985597666666667">School of Department of University of</affiliation>
<author confidence="0.622436">Ann Arbor</author>
<email confidence="0.999766">radev@umich.edu</email>
<abstract confidence="0.998833727272727">We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use variphrasal information units (i.e., to express the same atomic semantic units, called Finally, we present a ranker that emsimilarities build a network of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
<author>Li Zhang</author>
<author>Lada A Adamic</author>
<author>Rajan M Lukose</author>
</authors>
<title>Implicit structure and the dynamics of Blogspace.</title>
<date>2004</date>
<booktitle>In WWW’04, Workshop on the Weblogging Ecosystem.</booktitle>
<contexts>
<context position="7948" citStr="Adar et al., 2004" startWordPosition="1260" endWordPosition="1263"> Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1Document Understanding Conference 1099 3 Data Annotation The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headlines collected from Google News2, and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL Anthology Network (AAN)3. Each cluster consists of a number of unique summaries (headlines or citations) about the same artifact (non</context>
</contexts>
<marker>Adar, Zhang, Adamic, Lukose, 2004</marker>
<rawString>Eytan Adar, Li Zhang, Lada A. Adamic, and Rajan M. Lukose. 2004. Implicit structure and the dynamics of Blogspace. In WWW’04, Workshop on the Weblogging Ecosystem.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
<author>Daniel S Weld</author>
<author>Brian N Bershad</author>
<author>Steven S Gribble</author>
</authors>
<title>Why we search: visualizing and predicting user behavior.</title>
<date>2007</date>
<booktitle>In WWW’07,</booktitle>
<pages>161--170</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="7836" citStr="Adar et al., 2007" startWordPosition="1241" endWordPosition="1244">ty of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1Document Understanding Conference 1099 3 Data Annotation The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headlines collected from Google News2, and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL Anthology Network (AA</context>
</contexts>
<marker>Adar, Weld, Bershad, Gribble, 2007</marker>
<rawString>Eytan Adar, Daniel S. Weld, Brian N. Bershad, and Steven S. Gribble. 2007. Why we search: visualizing and predicting user behavior. In WWW’07, pages 161–170, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple-sequence alignment.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02,</booktitle>
<pages>164--171</pages>
<contexts>
<context position="4020" citStr="Barzilay and Lee, 2002" startWordPosition="637" endWordPosition="641">lectively written summaries, and give evidence of 1098 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the diversity of perspectives and its cause. We believe that out experiments will give insight into new models of text generation, which is aimed at modeling the process of producing natural language texts, and is best characterized as the process of making choices between alternate linguistic realizations, also known as lexical choice (Elhadad, 1995; Barzilay and Lee, 2002; Stede, 1995). 2 Prior Work In summarization, a number of previous methods have focused on diversity. (Mei et al., 2010) introduce a diversity-focused ranking methodology based on reinforced random walks in information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the node with the highest w</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02, pages 164–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="7735" citStr="Barzilay and McKeown, 2005" startWordPosition="1220" endWordPosition="1223">luated based on their quality in performing a specific task (Sp¨arck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1Document Understanding Conference 1099 3 Data Annotation The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headlines collected from Google News2, and the citations da</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Comput. Linguist., 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Blumer</author>
</authors>
<title>Collective behavior. In</title>
<date>1951</date>
<location>Lee, Alfred McClung, Ed., Principles</location>
<note>of Sociology.</note>
<contexts>
<context position="1377" citStr="Blumer, 1951" startWordPosition="206" endWordPosition="207">ross these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a network of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity. 1 Introduction In sociology, the term collective behavior is used to denote mass activities that are not centrally coordinated (Blumer, 1951). Collective behavior is different from group behavior in the following ways: (a) it involves limited social interaction, (b) membership is fluid, and (c) it generates weak and unconventional norms (Smelser, 1963). In this paper, we focus on the computational analysis of collective discourse, a collective behavior seen in interactive content contribution and text summarization in online social media. In collective discourse each individual’s behavior is largely independent of that of other individuals. In social media, discourse (Grosz and Sidner, 1986) is often a collective reaction to an eve</context>
</contexts>
<marker>Blumer, 1951</marker>
<rawString>Herbert Blumer. 1951. Collective behavior. In Lee, Alfred McClung, Ed., Principles of Sociology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In SIGIR’98,</booktitle>
<pages>335--336</pages>
<contexts>
<context position="5323" citStr="Carbonell and Goldstein, 1998" startWordPosition="840" endWordPosition="843"> opinionated text is discussed in (Paul et al., 2010). Paul et al. introduce Comparative LexRank, based on the LexRank ranking model (Erkan and Radev, 2004). Their random walk formulation is to score sentences and pairs of sentences from opposite viewpoints (clusters) based on both their representativeness of the collection as well as their contrastiveness with each other. Once a lexical similarity graph is built, they modify the graph based on cluster information and perform LexRank on the modified cosine similarity graph. The most well-known paper that address diversity in summarization is (Carbonell and Goldstein, 1998), which introduces Maximal Marginal Relevance (MMR). This method is based on a greedy algorithm that picks sentences in each step that are the least similar to the summary so far. There are a few other diversity-focused summarization systems like C-LexRank (Qazvinian and Radev, 2008), which employs document clustering. These papers try to increase diversity in summarizing documents, but do not explain the type of the diversity in their inputs. In this paper, we give an insightful discussion on the nature of the diversity seen in collective discourse, and will explain why some of the mentioned </context>
<context position="27804" citStr="Carbonell and Goldstein, 1998" startWordPosition="4588" endWordPosition="4591">mation about the cited paper, randomization has the drawback of selecting citations that have no valuable information in them. 5.2.2 LexRank LexRank (Erkan and Radev, 2004) works by first building a graph of all the documents (Di) in a cluster. The edges between corresponding nodes (di) represent the cosine similarity between them is above a threshold (0.10 following (Erkan and Radev, 2004)). Once the network is built, the system finds the most central sentences by performing a random walk on the graph. p(dj) = (1 − λ) |D |+ λEp(di)P(di → dj) (2) d; 5.2.3 MMR Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) uses the pairwise cosine similarity matrix and greedily chooses sentences that are the least similar to those already in the summary. In particular, L i MMR = arg minD;ED−A maxDjEA Sim(Di, Dj) where A is the set of documents in the summary, initialized to A = 0. 5.2.4 DivRank Unlike other time-homogeneous random walks (e.g., PageRank), DivRank does not assume that the transition probabilities remain constant over time. DivRank uses a vertex-reinforced random walk model to rank graph nodes based on a diversity based centrality. The basic assumption in DivRank is that the transition probability</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime G. Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR’98, pages 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="6704" citStr="Carletta, 1996" startWordPosition="1065" endWordPosition="1066"> and showed that relevance judgments differ significantly between humans but relative rankings show high degrees of stability across annotators. However, perhaps the closest work to this paper is (van Halteren and Teufel, 2004) in which 40 Dutch students and 10 NLP researchers were asked to summarize a BBC news report, resulting in 50 different summaries. Teufel and van Halteren also used 6 DUC1-provided summaries, and annotations from 10 student participants and 4 additional researchers, to create 20 summaries for another news article in the DUC datasets. They calculated the Kappa statistic (Carletta, 1996; Krippendorff, 1980) and observed high agreement, indicating that the task of atomic semantic unit (factoid) extraction can be robustly performed in naturally occurring text, without any copy-editing. The diversity of perspectives and the unprecedented growth of the factoid inventory also affects evaluation in text summarization. Evaluation methods are either extrinsic, in which the summaries are evaluated based on their quality in performing a specific task (Sp¨arck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren an</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Comput. Linguist., 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Mark E J Newman</author>
<author>Cristopher Moore</author>
</authors>
<title>Finding community structure in very large networks.</title>
<date>2004</date>
<journal>Phys. Rev. E,</journal>
<volume>70</volume>
<issue>6</issue>
<contexts>
<context position="25576" citStr="Clauset et al., 2004" startWordPosition="4207" endWordPosition="4210">d title 2nd poor glory hitting dynasty victory pitching baseball Pajek Sim(wi,wj) = (1) �|�fivj| �fi · �fj We use the pair-wise similarities of words in each cluster, and build a network of words and their similarities. Intuitively, words that appear in similar contexts are more similar to each other and will have a stronger edge between them in the network. Therefore, similar words, or words that appear in similar contexts, will form communities in this graph. Ideally, each community in the word similarity network would represent a factoid. To find the communities in the word network we use (Clauset et al., 2004), a hierarchical agglomeration algorithm which works by greedily optimizing the modularity in a linear running time for sparse graphs. The community detection algorithm will assign to each word wi, a community label Ci. For each community, we use LexRank to rank the words using the similarities in Equation 1, and assign a score to each word wi as S(wi) = �� |��|, where Ri is the rank of wi in its community, and |Ci |is the number of words that belong to Ci. Figure 5.1 shows part of the word similarity graph in the redsox cluster, in which each node is color-coded with its community. This figur</context>
</contexts>
<marker>Clauset, Newman, Moore, 2004</marker>
<rawString>Aaron Clauset, Mark E. J. Newman, and Cristopher Moore. 2004. Finding community structure in very large networks. Phys. Rev. E, 70(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>Using argumentation in text generation.</title>
<date>1995</date>
<journal>Journal of Pragmatics,</journal>
<pages>24--189</pages>
<contexts>
<context position="3996" citStr="Elhadad, 1995" startWordPosition="635" endWordPosition="636"> the set of collectively written summaries, and give evidence of 1098 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the diversity of perspectives and its cause. We believe that out experiments will give insight into new models of text generation, which is aimed at modeling the process of producing natural language texts, and is best characterized as the process of making choices between alternate linguistic realizations, also known as lexical choice (Elhadad, 1995; Barzilay and Lee, 2002; Stede, 1995). 2 Prior Work In summarization, a number of previous methods have focused on diversity. (Mei et al., 2010) introduce a diversity-focused ranking methodology based on reinforced random walks in information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the</context>
</contexts>
<marker>Elhadad, 1995</marker>
<rawString>Michael Elhadad. 1995. Using argumentation in text generation. Journal of Pragmatics, 24:189–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR).</journal>
<contexts>
<context position="4849" citStr="Erkan and Radev, 2004" startWordPosition="767" endWordPosition="770"> information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the node with the highest weight is set as an absorbing state. The multi-view point summarization of opinionated text is discussed in (Paul et al., 2010). Paul et al. introduce Comparative LexRank, based on the LexRank ranking model (Erkan and Radev, 2004). Their random walk formulation is to score sentences and pairs of sentences from opposite viewpoints (clusters) based on both their representativeness of the collection as well as their contrastiveness with each other. Once a lexical similarity graph is built, they modify the graph based on cluster information and perform LexRank on the modified cosine similarity graph. The most well-known paper that address diversity in summarization is (Carbonell and Goldstein, 1998), which introduces Maximal Marginal Relevance (MMR). This method is based on a greedy algorithm that picks sentences in each s</context>
<context position="27346" citStr="Erkan and Radev, 2004" startWordPosition="4510" endWordPosition="4513">For each cluster in each category (citations and headlines), this method simply gets a random per1104 mutations of the summaries. In the headlines datasets, where most of the headlines cover some factoids about the story, we expect this method to perform reasonably well since randomization will increase the chances of covering headlines that focus on different factoids. However, in the citations dataset, where a citing sentence may cover no information about the cited paper, randomization has the drawback of selecting citations that have no valuable information in them. 5.2.2 LexRank LexRank (Erkan and Radev, 2004) works by first building a graph of all the documents (Di) in a cluster. The edges between corresponding nodes (di) represent the cosine similarity between them is above a threshold (0.10 following (Erkan and Radev, 2004)). Once the network is built, the system finds the most central sentences by performing a random walk on the graph. p(dj) = (1 − λ) |D |+ λEp(di)P(di → dj) (2) d; 5.2.3 MMR Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) uses the pairwise cosine similarity matrix and greedily chooses sentences that are the least similar to those already in the summary. In part</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Len Fisher</author>
</authors>
<title>The Perfect Swarm: The Science of Complexity in Everyday Life.</title>
<date>2009</date>
<publisher>Basic Books.</publisher>
<contexts>
<context position="3186" citStr="Fisher, 2009" startWordPosition="509" endWordPosition="510">ent. To understand collective discourse, we are interested in behavior that happens over a short period of time. We focus on topics that are relatively welldefined in scope such as a particular event or a single news event that does not evolve over time. This can eventually be extended to events and issues that are evolving either in time or scope such as elections, wars, or the economy. In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009). However, there is little work that studies a collective system in which members individually write summaries. In most of this paper, we will be concerned with developing a complex systems view of the set of collectively written summaries, and give evidence of 1098 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the diversity of perspectives and its cause. We believe that out experiments will give insight into new models of text generation, which is ai</context>
</contexts>
<marker>Fisher, 2009</marker>
<rawString>Len Fisher. 2009. The Perfect Swarm: The Science of Complexity in Everyday Life. Basic Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Comput. Linguist.,</journal>
<pages>12--175</pages>
<contexts>
<context position="1936" citStr="Grosz and Sidner, 1986" startWordPosition="289" endWordPosition="292">ass activities that are not centrally coordinated (Blumer, 1951). Collective behavior is different from group behavior in the following ways: (a) it involves limited social interaction, (b) membership is fluid, and (c) it generates weak and unconventional norms (Smelser, 1963). In this paper, we focus on the computational analysis of collective discourse, a collective behavior seen in interactive content contribution and text summarization in online social media. In collective discourse each individual’s behavior is largely independent of that of other individuals. In social media, discourse (Grosz and Sidner, 1986) is often a collective reaction to an event. One scenario leading to collective reaction to a welldefined subject is when an event occurs (a movie is released, a story occurs, a paper is published) and people independently write about it (movie reviews, news headlines, citation sentences). This process of content generation happens over time, and each person chooses the aspects to cover. Each event has an onset and a time of death after which nothing is written about it. Tracing the generation of content over many instances will reveal temporal patterns that will allow us to make sense of the </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Comput. Linguist., 12:175–204, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Hong</author>
<author>Scott Page</author>
</authors>
<title>Interpreted and generated signals.</title>
<date>2009</date>
<journal>Journal of Economic Theory,</journal>
<volume>144</volume>
<issue>5</issue>
<contexts>
<context position="3171" citStr="Hong and Page, 2009" startWordPosition="505" endWordPosition="508">round a particular event. To understand collective discourse, we are interested in behavior that happens over a short period of time. We focus on topics that are relatively welldefined in scope such as a particular event or a single news event that does not evolve over time. This can eventually be extended to events and issues that are evolving either in time or scope such as elections, wars, or the economy. In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009). However, there is little work that studies a collective system in which members individually write summaries. In most of this paper, we will be concerned with developing a complex systems view of the set of collectively written summaries, and give evidence of 1098 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the diversity of perspectives and its cause. We believe that out experiments will give insight into new models of text generati</context>
</contexts>
<marker>Hong, Page, 2009</marker>
<rawString>Lu Hong and Scott Page. 2009. Interpreted and generated signals. Journal of Economic Theory, 144(5):2174–2196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshay Java</author>
<author>Pranam Kolari</author>
<author>Tim Finin</author>
<author>Tim Oates</author>
</authors>
<title>Modeling the spread of influence on the blogosphere.</title>
<date>2006</date>
<booktitle>In WWW’06.</booktitle>
<contexts>
<context position="7968" citStr="Java et al., 2006" startWordPosition="1264" endWordPosition="1267"> These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1Document Understanding Conference 1099 3 Data Annotation The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headlines collected from Google News2, and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL Anthology Network (AAN)3. Each cluster consists of a number of unique summaries (headlines or citations) about the same artifact (nonevolving news story </context>
</contexts>
<marker>Java, Kolari, Finin, Oates, 2006</marker>
<rawString>Akshay Java, Pranam Kolari, Tim Finin, and Tim Oates. 2006. Modeling the spread of influence on the blogosphere. In WWW’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology. Beverly Hills:</title>
<date>1980</date>
<publisher>Sage Publications.</publisher>
<contexts>
<context position="6725" citStr="Krippendorff, 1980" startWordPosition="1067" endWordPosition="1069"> relevance judgments differ significantly between humans but relative rankings show high degrees of stability across annotators. However, perhaps the closest work to this paper is (van Halteren and Teufel, 2004) in which 40 Dutch students and 10 NLP researchers were asked to summarize a BBC news report, resulting in 50 different summaries. Teufel and van Halteren also used 6 DUC1-provided summaries, and annotations from 10 student participants and 4 additional researchers, to create 20 summaries for another news article in the DUC datasets. They calculated the Kappa statistic (Carletta, 1996; Krippendorff, 1980) and observed high agreement, indicating that the task of atomic semantic unit (factoid) extraction can be robustly performed in naturally occurring text, without any copy-editing. The diversity of perspectives and the unprecedented growth of the factoid inventory also affects evaluation in text summarization. Evaluation methods are either extrinsic, in which the summaries are evaluated based on their quality in performing a specific task (Sp¨arck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenko</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to its Methodology. Beverly Hills: Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kumar</author>
<author>Jasmine Novak</author>
<author>Prabhakar Raghavan</author>
<author>Andrew Tomkins</author>
</authors>
<title>On the bursty evolution of blogspace.</title>
<date>2003</date>
<booktitle>In WWW’03,</booktitle>
<pages>568--576</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="7816" citStr="Kumar et al., 2003" startWordPosition="1237" endWordPosition="1240">nsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1Document Understanding Conference 1099 3 Data Annotation The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headlines collected from Google News2, and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL A</context>
</contexts>
<marker>Kumar, Novak, Raghavan, Tomkins, 2003</marker>
<rawString>Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and Andrew Tomkins. 2003. On the bursty evolution of blogspace. In WWW’03, pages 568–576, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="24403" citStr="Lee, 1999" startWordPosition="4005" endWordPosition="4006">0−2 Pyramid Score 10−1 1103 Pyramid Score 1 0.8 0.6 0.4 0.2 0 headlines citations abortion amazon babies burger colombia england gervais google ireland maine mercury miss monkey mozart nobel priest ps3slim radiation redsox russian scientist soupy sweden typhoon yale A00_1023 A00_1043 A00_2024 C00_1072 C96_1058 D03_1017 D04_9907 H05_1047 H05_1079 J04_4002 N03_1017 N04_1033 P02_1006 P03_1001 P05_1012 P05_1013 P05_1014 P05_1033 P97_1003 P99_1065 W00_0403 W00_0603 W03_0301 W03_0510 W05_1203 Figure 3: The 25th to 75th percentile pyramid score range in individual clusters words that is inspired by (Lee, 1999). We represent each word by its context in the cluster and find the similarity of such contexts. Particularly, each word wi is represented by a bag of words, fi, that have a surface distance of 3 or smaller to wi anywhere in the cluster. In other words, fi contains any word that co-occurs with wi in a 4-gram in the cluster. This bag of words representation of words enables us to find the word-pair similarities. Figure 5: Part of the word similarity graph in the redsox cluster police celebrations arrest fan poorer unhappy sweeps second sox jump red title 2nd poor glory hitting dynasty victory p</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jure Leskovec</author>
<author>Lars Backstrom</author>
<author>Jon Kleinberg</author>
</authors>
<title>Meme-tracking and the dynamics of the news cycle.</title>
<date>2009</date>
<booktitle>In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>497--506</pages>
<contexts>
<context position="7684" citStr="Leskovec et al., 2009" startWordPosition="1212" endWordPosition="1215">ther extrinsic, in which the summaries are evaluated based on their quality in performing a specific task (Sp¨arck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1Document Understanding Conference 1099 3 Data Annotation The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headline</context>
</contexts>
<marker>Leskovec, Backstrom, Kleinberg, 2009</marker>
<rawString>Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 497–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Manual and automatic evaluation of summaries.</title>
<date>2002</date>
<booktitle>In ACL-Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="12551" citStr="Lin and Hovy, 2002" startWordPosition="2016" endWordPosition="2019">iments, since some headline writers often try to use uncommon terms to refer to a factoid. For instance, two headlines from the redsox cluster are: Short wait for bossox this time Soxcess started upstairs 1100 Following these examples, we asked two annotators to annotate all 1, 390 headlines, and 926 citations. The annotators were asked to follow precise guidelines in nugget extraction. Our guidelines instructed annotators to extract non-overlapping phrases from each headline as nuggets. Therefore, each nugget should be a substring of the headline that represents a semantic unit4. Previously (Lin and Hovy, 2002) had shown that information overlap judgment is a difficult task for human annotators. To avoid such a difficulty, we enforced our annotators to extract non-overlapping nuggets from a summary to make sure that they are mutually independent and that information overlap between them is minimized. Finding agreement between annotated welldefined nuggets is straightforward and can be calculated in terms of Kappa. However, when nuggets themselves are to be extracted by annotators, the task becomes less obvious. To calculate the agreement, we annotated 10 randomly selected headline clusters twice and</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2002. Manual and automatic evaluation of summaries. In ACL-Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Jian Guo</author>
<author>Dragomir Radev</author>
</authors>
<title>Divrank: the interplay of prestige and diversity in information networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1009--1018</pages>
<contexts>
<context position="4141" citStr="Mei et al., 2010" startWordPosition="658" endWordPosition="661">onal Linguistics, pages 1098–1108, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the diversity of perspectives and its cause. We believe that out experiments will give insight into new models of text generation, which is aimed at modeling the process of producing natural language texts, and is best characterized as the process of making choices between alternate linguistic realizations, also known as lexical choice (Elhadad, 1995; Barzilay and Lee, 2002; Stede, 1995). 2 Prior Work In summarization, a number of previous methods have focused on diversity. (Mei et al., 2010) introduce a diversity-focused ranking methodology based on reinforced random walks in information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the node with the highest weight is set as an absorbing state. The multi-view point summarization of opinionated text is discussed in (Paul et al., </context>
<context position="28510" citStr="Mei et al., 2010" startWordPosition="4705" endWordPosition="4708">east similar to those already in the summary. In particular, L i MMR = arg minD;ED−A maxDjEA Sim(Di, Dj) where A is the set of documents in the summary, initialized to A = 0. 5.2.4 DivRank Unlike other time-homogeneous random walks (e.g., PageRank), DivRank does not assume that the transition probabilities remain constant over time. DivRank uses a vertex-reinforced random walk model to rank graph nodes based on a diversity based centrality. The basic assumption in DivRank is that the transition probability from a node to other is reinforced by the number of previous visits to the target node (Mei et al., 2010). Particularly, let’s assume pT(u, v) is the transition probability from any node u to node v at time T. Then, pT (di, dj) = (1 − λ).p*(dj) + λ.p0(di, dj).NT (dj) (3) DT (di) where NT (dj) is the number of times the walk has visited dj up to time T and DT (di) = E p0(di, dj)NT (dj) (4) djEV Here, p*(dj) is the prior distribution that determines the preference of visiting vertex dj. We try two variants of this algorithm: DivRank, in which p*(dj) is uniform, and DivRank with priors in which p*(dj) a l(Dj)−1, where l(Dj) is the number of the words in the document Dj and β is a parameter (β = 0.8)</context>
</contexts>
<marker>Mei, Guo, Radev, 2010</marker>
<rawString>Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Divrank: the interplay of prestige and diversity in information networks. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1009–1018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilad Mishne</author>
<author>Natalie Glance</author>
</authors>
<title>Predicting movie sales from blogger sentiment.</title>
<date>2006</date>
<booktitle>In AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW</booktitle>
<contexts>
<context position="8027" citStr="Mishne and Glance, 2006" startWordPosition="1274" endWordPosition="1277">ent in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1Document Understanding Conference 1099 3 Data Annotation The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headlines collected from Google News2, and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL Anthology Network (AAN)3. Each cluster consists of a number of unique summaries (headlines or citations) about the same artifact (nonevolving news story or scientific paper) written by different people. Table 1 l</context>
</contexts>
<marker>Mishne, Glance, 2006</marker>
<rawString>Gilad Mishne and Natalie Glance. 2006. Predicting movie sales from blogger sentiment. In AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>Proceedings of the HLT-NAACL conference.</booktitle>
<contexts>
<context position="7349" citStr="Nenkova and Passonneau, 2004" startWordPosition="1161" endWordPosition="1164">1980) and observed high agreement, indicating that the task of atomic semantic unit (factoid) extraction can be robustly performed in naturally occurring text, without any copy-editing. The diversity of perspectives and the unprecedented growth of the factoid inventory also affects evaluation in text summarization. Evaluation methods are either extrinsic, in which the summaries are evaluated based on their quality in performing a specific task (Sp¨arck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004;</context>
<context position="19205" citStr="Nenkova and Passonneau, 2004" startWordPosition="3120" endWordPosition="3124">mmaries, study the distribution of information coverage in them, and investigate the number of summaries required to build a complete factoid inventory. The information covered in each summary can be determined by the set of factoids (and not nuggets) and their frequencies across the datasets. For example, in the redsox dataset, “red sox”, “boston”, and “boston red sox” are nuggets that all represent the same piece of information: the red sox team. Therefore, different summaries that use these nuggets to refer to the red sox team should not be seen as very different. We use the Pyramid model (Nenkova and Passonneau, 2004) to value different summary factoids. Intuitively, factoids that are mentioned more frequently are more salient aspects of the story. Therefore, our pyramid model uses the normalized frequency at which a factoid is mentioned across a dataset as its weight. In the pyramid model, the individual factoids fall in tiers. If a factoid appears in more summaries, it falls in a higher tier. In principle, if the term wi appears IwiI times in the set of 6Similar experiment using individual clusters exhibit similar behavior 1000 Nuggets Factoids 800 600 400 200 0 100 101 102 103 number of summaries Invent</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. Proceedings of the HLT-NAACL conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott E Page</author>
</authors>
<title>The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies.</title>
<date>2007</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="3136" citStr="Page, 2007" startWordPosition="501" endWordPosition="502">nse of the text generated around a particular event. To understand collective discourse, we are interested in behavior that happens over a short period of time. We focus on topics that are relatively welldefined in scope such as a particular event or a single news event that does not evolve over time. This can eventually be extended to events and issues that are evolving either in time or scope such as elections, wars, or the economy. In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009). However, there is little work that studies a collective system in which members individually write summaries. In most of this paper, we will be concerned with developing a complex systems view of the set of collectively written summaries, and give evidence of 1098 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the diversity of perspectives and its cause. We believe that out experiments will give insig</context>
</contexts>
<marker>Page, 2007</marker>
<rawString>Scott E. Page. 2007. The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In ACL’04,</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7871" citStr="Pang and Lee, 2004" startWordPosition="1248" endWordPosition="1251">ted, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1Document Understanding Conference 1099 3 Data Annotation The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headlines collected from Google News2, and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL Anthology Network (AAN)3. Each cluster consists of a num</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL’04, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>ChengXiang Zhai</author>
<author>Roxana Girju</author>
</authors>
<title>Summarizing contrastive viewpoints in opinionated text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>66--76</pages>
<contexts>
<context position="4746" citStr="Paul et al., 2010" startWordPosition="750" endWordPosition="753">et al., 2010) introduce a diversity-focused ranking methodology based on reinforced random walks in information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the node with the highest weight is set as an absorbing state. The multi-view point summarization of opinionated text is discussed in (Paul et al., 2010). Paul et al. introduce Comparative LexRank, based on the LexRank ranking model (Erkan and Radev, 2004). Their random walk formulation is to score sentences and pairs of sentences from opposite viewpoints (clusters) based on both their representativeness of the collection as well as their contrastiveness with each other. Once a lexical similarity graph is built, they modify the graph based on cluster information and perform LexRank on the modified cosine similarity graph. The most well-known paper that address diversity in summarization is (Carbonell and Goldstein, 1998), which introduces Maxi</context>
</contexts>
<marker>Paul, Zhai, Girju, 2010</marker>
<rawString>Michael Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 66–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<booktitle>In COLING 2008,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="5607" citStr="Qazvinian and Radev, 2008" startWordPosition="886" endWordPosition="889"> representativeness of the collection as well as their contrastiveness with each other. Once a lexical similarity graph is built, they modify the graph based on cluster information and perform LexRank on the modified cosine similarity graph. The most well-known paper that address diversity in summarization is (Carbonell and Goldstein, 1998), which introduces Maximal Marginal Relevance (MMR). This method is based on a greedy algorithm that picks sentences in each step that are the least similar to the summary so far. There are a few other diversity-focused summarization systems like C-LexRank (Qazvinian and Radev, 2008), which employs document clustering. These papers try to increase diversity in summarizing documents, but do not explain the type of the diversity in their inputs. In this paper, we give an insightful discussion on the nature of the diversity seen in collective discourse, and will explain why some of the mentioned methods may not work under such environments. In prior work on evaluating independent contributions in content generation, Voorhees (Voorhees, 1998) studied IR systems and showed that relevance judgments differ significantly between humans but relative rankings show high degrees of s</context>
<context position="29396" citStr="Qazvinian and Radev, 2008" startWordPosition="4872" endWordPosition="4875"> = E p0(di, dj)NT (dj) (4) djEV Here, p*(dj) is the prior distribution that determines the preference of visiting vertex dj. We try two variants of this algorithm: DivRank, in which p*(dj) is uniform, and DivRank with priors in which p*(dj) a l(Dj)−1, where l(Dj) is the number of the words in the document Dj and β is a parameter (β = 0.8). 5.2.5 C-LexRank C-LexRank is a clustering-based model in which the cosine similarities of document pairs are used to build a network of documents. Then the the network is split into communities, and the most salient documents in each community are selected (Qazvinian and Radev, 2008). C-LexRank focuses on finding communities of documents using their cosine similarity. The intuition is that documents that are more similar to each other contain similar factoids. We expect C-LexRank to be a strong ranker, but incapable of capturing the diversity caused by using different phrases to express the same meaning. The reason is that different nuggets that represent the same factoid often have no words in common (e.g., “victory” and “glory”) and won’t be captured by a lexical measure like cosine similarity. 5.3 Experiments We use each of the systems explained above to rank the summa</context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In COLING 2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Identifying non-explicit citing sentences for citation-based summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>555--564</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="14224" citStr="Qazvinian and Radev, 2010" startWordPosition="2284" endWordPosition="2287">notators agree by chance if each annotator is randomly assigning categories. Table 2 shows the unigram, bigram, and trigrambased average n between the two human annotators (Human1, Human2). These results suggest that human annotators can reach substantial agreement when bigram and trigram nuggets are examined, and has reasonable agreement for unigram nuggets. 4 Diversity We study the diversity of ways with which human summarizers talk about the same story or event and explain why such a diversity exists. 4Before the annotations, we lower-cased all summaries and removed duplicates 5Previously (Qazvinian and Radev, 2010) have shown high agreement in human judgments in a similar task on citation annotation Average n unigram bigram trigram Human1 vs. Human2 0.76 ± 0.4 0.80 ± 0.4 0.89 ± 0.3 Table 2: Agreement between different annotators in terms of average Kappa in 25 headline clusters. c c Figure 1: The cumulative probability distribution for the frequency of factoids (i.e., the probability that a factoid will be mentioned in c different summaries) across in each category. 4.1 Skewed Distributions Our first experiment is to analyze the popularity of different factoids. For each factoid in the annotated cluster</context>
</contexts>
<marker>Qazvinian, Radev, 2010</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2010. Identifying non-explicit citing sentences for citation-based summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil J Smelser</author>
</authors>
<title>Theory of Collective Behavior.</title>
<date>1963</date>
<publisher>Free Press.</publisher>
<contexts>
<context position="1590" citStr="Smelser, 1963" startWordPosition="240" endWordPosition="241">stributional similarities to build a network of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity. 1 Introduction In sociology, the term collective behavior is used to denote mass activities that are not centrally coordinated (Blumer, 1951). Collective behavior is different from group behavior in the following ways: (a) it involves limited social interaction, (b) membership is fluid, and (c) it generates weak and unconventional norms (Smelser, 1963). In this paper, we focus on the computational analysis of collective discourse, a collective behavior seen in interactive content contribution and text summarization in online social media. In collective discourse each individual’s behavior is largely independent of that of other individuals. In social media, discourse (Grosz and Sidner, 1986) is often a collective reaction to an event. One scenario leading to collective reaction to a welldefined subject is when an event occurs (a movie is released, a story occurs, a paper is published) and people independently write about it (movie reviews, </context>
</contexts>
<marker>Smelser, 1963</marker>
<rawString>Neil J. Smelser. 1963. Theory of Collective Behavior. Free Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck-Jones</author>
</authors>
<title>Automatic summarizing: factors and directions.</title>
<date>1999</date>
<booktitle>Advances in automatic text summarization, chapter 1,</booktitle>
<pages>1--12</pages>
<editor>In Inderjeet Mani and Mark T. Maybury, editors,</editor>
<publisher>The MIT Press.</publisher>
<marker>Sp¨arck-Jones, 1999</marker>
<rawString>Karen Sp¨arck-Jones. 1999. Automatic summarizing: factors and directions. In Inderjeet Mani and Mark T. Maybury, editors, Advances in automatic text summarization, chapter 1, pages 1 – 12. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>Lexicalization in natural language generation: a survey.</title>
<date>1995</date>
<journal>Artificial Intelligence Review,</journal>
<pages>8--309</pages>
<contexts>
<context position="4034" citStr="Stede, 1995" startWordPosition="642" endWordPosition="643">ies, and give evidence of 1098 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the diversity of perspectives and its cause. We believe that out experiments will give insight into new models of text generation, which is aimed at modeling the process of producing natural language texts, and is best characterized as the process of making choices between alternate linguistic realizations, also known as lexical choice (Elhadad, 1995; Barzilay and Lee, 2002; Stede, 1995). 2 Prior Work In summarization, a number of previous methods have focused on diversity. (Mei et al., 2010) introduce a diversity-focused ranking methodology based on reinforced random walks in information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the node with the highest weight is set a</context>
</contexts>
<marker>Stede, 1995</marker>
<rawString>Manfred Stede. 1995. Lexicalization in natural language generation: a survey. Artificial Intelligence Review, (8):309–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Simone Teufel</author>
</authors>
<title>Examining the consensus between human summaries: initial experiments with factoid analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 03 on Text summarization workshop,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>van Halteren, Teufel, 2003</marker>
<rawString>Hans van Halteren and Simone Teufel. 2003. Examining the consensus between human summaries: initial experiments with factoid analysis. In Proceedings of the HLT-NAACL 03 on Text summarization workshop, pages 57–64, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Simone Teufel</author>
</authors>
<title>Evaluating information content by factoid analysis: human annotation and stability.</title>
<date>2004</date>
<booktitle>In EMNLP’04,</booktitle>
<location>Barcelona.</location>
<marker>van Halteren, Teufel, 2004</marker>
<rawString>Hans van Halteren and Simone Teufel. 2004. Evaluating information content by factoid analysis: human annotation and stability. In EMNLP’04, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Variations in relevance judgments and the measurement of retrieval effectiveness.</title>
<date>1998</date>
<booktitle>In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>315--323</pages>
<contexts>
<context position="6071" citStr="Voorhees, 1998" startWordPosition="962" endWordPosition="963">t are the least similar to the summary so far. There are a few other diversity-focused summarization systems like C-LexRank (Qazvinian and Radev, 2008), which employs document clustering. These papers try to increase diversity in summarizing documents, but do not explain the type of the diversity in their inputs. In this paper, we give an insightful discussion on the nature of the diversity seen in collective discourse, and will explain why some of the mentioned methods may not work under such environments. In prior work on evaluating independent contributions in content generation, Voorhees (Voorhees, 1998) studied IR systems and showed that relevance judgments differ significantly between humans but relative rankings show high degrees of stability across annotators. However, perhaps the closest work to this paper is (van Halteren and Teufel, 2004) in which 40 Dutch students and 10 NLP researchers were asked to summarize a BBC news report, resulting in 50 different summaries. Teufel and van Halteren also used 6 DUC1-provided summaries, and annotations from 10 student participants and 4 additional researchers, to create 20 summaries for another news article in the DUC datasets. They calculated th</context>
</contexts>
<marker>Voorhees, 1998</marker>
<rawString>Ellen M. Voorhees. 1998. Variations in relevance judgments and the measurement of retrieval effectiveness. In SIGIR ’98: Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Udny Yule</author>
</authors>
<title>A mathematical theory of evolution, based on the conclusions of dr. j. c. willis, f.r.s.</title>
<date>1925</date>
<journal>Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character,</journal>
<pages>213--21</pages>
<contexts>
<context position="33120" citStr="Yule, 1925" startWordPosition="5477" endWordPosition="5478"> that the information covered by reading n summaries has a rapidly growing asymptotic behavior as n increases. Finally, we proposed a ranking system that employs word distributional similarities to identify semantically equivalent words, and compared it with a wide range of summarization systems that leverage diversity. In the future, we plan to move to content from other collective systems on Web. In order to generalize our findings, we plan to examine blog comments, online reviews, and tweets (that discuss the same URL). We also plan to build a generation system that employs the Yule model (Yule, 1925) to determine the importance of each aspect (e.g. who, when, where, etc.) in order to produce summaries that include diverse aspects of a story. Our work has resulted in a publicly available dataset 8 of 25 annotated news clusters with nearly 1, 400 headlines, and 25 clusters of citation sentences with more than 900 citations. We believe that this dataset can open new dimensions in studying diversity and other aspects of automatic text generation. 7 Acknowledgments This work is supported by the National Science Foundation grant number IIS-0705832 and grant number IIS-0968489. Any opinions, fin</context>
</contexts>
<marker>Yule, 1925</marker>
<rawString>G. Udny Yule. 1925. A mathematical theory of evolution, based on the conclusions of dr. j. c. willis, f.r.s. Philosophical Transactions of the Royal Society of London. Series B, Containing Papers of a Biological Character, 213:21–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Andrew Goldberg</author>
<author>Jurgen Van Gael</author>
<author>David Andrzejewski</author>
</authors>
<title>Improving diversity in ranking using absorbing random walks.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>97--104</pages>
<marker>Zhu, Goldberg, Van Gael, Andrzejewski, 2007</marker>
<rawString>Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski. 2007. Improving diversity in ranking using absorbing random walks. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 97–104.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>