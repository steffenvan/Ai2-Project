<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996593">
Extremely Lexicalized Models for Accurate and Fast HPSG Parsing
</title>
<author confidence="0.96718">
Takashi Ninomiya
</author>
<affiliation confidence="0.960675">
Information Technology Center
University of Tokyo
</affiliation>
<author confidence="0.968417">
Yoshimasa Tsuruoka
</author>
<affiliation confidence="0.99096">
School of Informatics
University of Manchester
</affiliation>
<author confidence="0.991977">
Takuya Matsuzaki
</author>
<affiliation confidence="0.995638">
Department of Computer Science
University of Tokyo
</affiliation>
<author confidence="0.992333">
Yusuke Miyao
</author>
<affiliation confidence="0.9956075">
Department of Computer Science
University of Tokyo
</affiliation>
<author confidence="0.983216">
Jun’ichi Tsujii
</author>
<affiliation confidence="0.880570666666667">
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
SORST, Japan Science and Technology Agency
</affiliation>
<address confidence="0.761026">
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
</address>
<email confidence="0.924326">
{ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.981442" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997015">
This paper describes an extremely lexi-
calized probabilistic model for fast and
accurate HPSG parsing. In this model,
the probabilities of parse trees are de-
fined with only the probabilities of select-
ing lexical entries. The proposed model
is very simple, and experiments revealed
that the implemented parser runs around
four times faster than the previous model
and that the proposed model has a high
accuracy comparable to that of the previ-
ous model for probabilistic HPSG, which
is defined over phrase structures. We
also developed a hybrid of our probabilis-
tic model and the conventional phrase-
structure-based model. The hybrid model
is not only significantly faster but also sig-
nificantly more accurate by two points of
precision and recall compared to the pre-
vious model.
</bodyText>
<sectionHeader confidence="0.995169" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948952380952">
For the last decade, accurate and wide-coverage
parsing for real-world text has been intensively
and extensively pursued. In most of state-of-the-
art parsers, probabilistic events are defined over
phrase structures because phrase structures are
supposed to dominate syntactic configurations of
sentences. For example, probabilities were de-
fined over grammar rules in probabilistic CFG
(Collins, 1999; Klein and Manning, 2003; Char-
niak and Johnson, 2005) or over complex phrase
structures of head-driven phrase structure gram-
mar (HPSG) or combinatory categorial grammar
(CCG) (Clark and Curran, 2004b; Malouf and van
Noord, 2004; Miyao and Tsujii, 2005). Although
these studies vary in the design of the probabilistic
models, the fundamental conception of probabilis-
tic modeling is intended to capture characteristics
of phrase structures or grammar rules. Although
lexical information, such as head words, is known
to significantly improve the parsing accuracy, it
was also used to augment information on phrase
structures.
Another interesting approach to this problem
was using supertagging (Clark and Curran, 2004b;
Clark and Curran, 2004a; Wang and Harper, 2004;
Nasr and Rambow, 2004), which was originally
developed for lexicalized tree adjoining grammars
(LTAG) (Bangalore and Joshi, 1999). Supertag-
ging is a process where words in an input sen-
tence are tagged with ‘supertags,’ which are lex-
ical entries in lexicalized grammars, e.g., elemen-
tary trees in LTAG, lexical categories in CCG,
and lexical entries in HPSG. Supertagging was,
in the first place, a technique to reduce the cost
of parsing with lexicalized grammars; ambiguity
in assigning lexical entries to words is reduced
by the light-weight process of supertagging be-
fore the heavy process of parsing. Bangalore and
Joshi (1999) claimed that if words can be assigned
correct supertags, syntactic parsing is almost triv-
ial. What this means is that if supertags are cor-
rectly assigned, syntactic structures are almost de-
</bodyText>
<page confidence="0.559025">
155
</page>
<note confidence="0.964603">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155–163,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999970641791045">
termined because supertags include rich syntac-
tic information such as subcategorization frames.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated
in the case of a CCG parser (Clark and Curran,
2004a) with the result of a drastic improvement in
the parsing speed. Wang and Harper (2004) also
demonstrated the effects of supertagging with a
statistical constraint dependency grammar (CDG)
parser. They achieved accuracy as high as the
state-of-the-art parsers. However, a supertagger it-
self was used as an external tagger that enumerates
candidates of lexical entries or filters out unlikely
lexical entries just to help parsing, and the best
parse trees were selected mainly according to the
probabilistic model for phrase structures or depen-
dencies with/without the probabilistic model for
supertagging.
We investigate an extreme case of HPSG pars-
ing in which the probabilistic model is defined
with only the probabilities of lexical entry selec-
tion; i.e., the model is never sensitive to charac-
teristics of phrase structures. The model is simply
defined as the product of the supertagging proba-
bilities, which are provided by the discriminative
method with machine learning features of word
trigrams and part-of-speech (POS) 5-grams as de-
fined in the CCG supertagging (Clark and Curran,
2004a). The model is implemented in an HPSG
parser instead of the phrase-structure-based prob-
abilistic model; i.e., the parser returns the parse
tree assigned the highest probability of supertag-
ging among the parse trees licensed by an HPSG.
Though the model uses only the probabilities of
lexical entry selection, the experiments revealed
that it was as accurate as the previous phrase-
structure-based model. Interestingly, this means
that accurate parsing is possible using rather sim-
ple mechanisms.
We also tested a hybrid model of the su-
pertagging and the previous phrase-structure-
based probabilistic model. In the hybrid model,
the probabilities of the previous model are mul-
tiplied by the supertagging probabilities instead
of a preliminary probabilistic model, which is in-
troduced to help the process of estimation by fil-
tering unlikely lexical entries (Miyao and Tsujii,
2005). In the previous model, the preliminary
probabilistic model is defined as the probability
of unigram supertagging. So, the hybrid model
can be regarded as an extension of supertagging
from unigram to n-gram. The hybrid model can
also be regarded as a variant of the statistical CDG
parser (Wang, 2003; Wang and Harper, 2004), in
which the parse tree probabilities are defined as
the product of the supertagging probabilities and
the dependency probabilities. In the experiments,
we observed that the hybrid model significantly
improved the parsing speed, by around three to
four times speed-ups, and accuracy, by around two
points in both precision and recall, over the pre-
vious model. This implies that finer probabilistic
model of lexical entry selection can improve the
phrase-structure-based model.
</bodyText>
<sectionHeader confidence="0.503333" genericHeader="introduction">
2 HPSG and probabilistic models
</sectionHeader>
<bodyText confidence="0.995956708333333">
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemata describe gen-
eral construction rules, and a large number of
lexical entries express word-specific characteris-
tics. The structures of sentences are explained us-
ing combinations of schemata and lexical entries.
Both schemata and lexical entries are represented
by typed feature structures, and constraints repre-
sented by feature structures are checked with uni-
fication.
An example of HPSG parsing of the sentence
“Spring has come” is shown in Figure 1. First,
each of the lexical entries for “has” and “come”
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
parse result is output as a phrasal sign that domi-
nates the sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
(L, R), where
</bodyText>
<equation confidence="0.828512">
L = {l = (w, F)|w E W, F E FJ is a set of
lexical entries, and
R is a set of schemata; i.e., r E R is a partial
function: F x F — F.
</equation>
<bodyText confidence="0.999438">
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries de-
termine the dominant syntactic structures.
</bodyText>
<figure confidence="0.992021272727273">
156
has
come
Spring
HEAD noun
1 SUBJ &lt; &gt;
COMPS &lt; &gt;
&gt;
HEAD verb
SUBJ &lt;
COMPS &lt; &gt;
HEAD noun
SUBJ &lt; &gt;
&lt; &gt;
COMPS &lt; &gt; COMPS 2
HEAD verb
SUBJ &lt; &gt;
1
head-comp
HEAD verb
2 SUBJ &lt; &gt;
1
COMPS &lt; &gt;
HEAD verb
SUBJ &lt; &gt;
COMPS &lt; &gt;
subject-head
HEAD verb
1
COMPS &lt; &gt;
head-comp
HEAD noun
SUBJ &lt; &gt;
COMPS &lt; &gt;
HEAD verb
1
2
HEAD verb
1
COMPS &lt; &gt;
1
has
Spring
come
</figure>
<figureCaption confidence="0.991569">
Figure 1: HPSG parsing.
</figureCaption>
<figure confidence="0.955351446808511">
HEAD noun
1 SUBJ &lt;&gt;
COMPS &lt;&gt;
froot= &lt;S, has, VBZ,
Spring/NN has/VBZ come/VBN
HEAD verb
SUBJ &lt;&gt;
COMPS &lt;&gt;
subject-head
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt; &gt;
2
flex= &lt;spring, NN,
HEAD verb
SUBJ &lt;NP&gt;
COMPS &lt;VP&gt;
&gt;
fbinary=
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt;&gt;
head-comp
head-comp, 1, 0,
1, VP, has, VBZ,
HEAD verb
2 SUBJ &lt; &gt;
1
COMPS &lt;&gt;
1, VP, come, VBN,
HEAD noun
SUBJ &lt;&gt;
COMPS &lt;&gt;
&gt;
HEAD verb
SUBJ &lt;NP&gt;
COMPS &lt;VP&gt;
,
HEAD verb
SUBJ &lt;NP&gt;
COMPS &lt;&gt;
SUBJ &lt; &gt;
SUBJ &lt; &gt;
COMPS &lt; &gt;
2 SUBJ &lt; &gt;
</figure>
<bodyText confidence="0.99796925">
Previous studies (Abney, 1997; Johnson et al.,
1999; Riezler et al., 2000; Malouf and van Noord,
2004; Kaplan et al., 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model
or maximum entropy model (Berger et al., 1996).
The probability that a parse result T is assigned to
a given sentence w = (w1, ... , wn) is
</bodyText>
<equation confidence="0.595491">
phpsg (T  |w) = 1exp ÃX λufu(T)
ZW u
</equation>
<figureCaption confidence="0.998486">
Figure 2: Example of features.
</figureCaption>
<bodyText confidence="0.999447285714286">
(2005) also introduced a preliminary probabilistic
model p0(T |w) whose estimation does not require
the parsing of a treebank. This model is intro-
duced as a reference distribution of the probabilis-
tic HPSG model; i.e., the computation of parse
trees given low probabilities by the model is omit-
ted in the estimation stage. We have
</bodyText>
<equation confidence="0.84340175">
(Previous probabilistic HPSG)
XZW = ÃX ! ÃX !
T&apos; exp λufu(T&apos;) , phpsg&apos;(T|w) = p0(T|w) 1exp λufu(T)
u ZW u
</equation>
<bodyText confidence="0.999982095238095">
where λu is a model parameter, fu is a feature
function that represents a characteristic of parse
tree T, and Z, is the sum over the set of all pos-
sible parse trees for the sentence. Intuitively, the
probability is defined as the normalized product
of the weights exp(λu) when a characteristic cor-
responding to fu appears in parse result T. The
model parameters, λu, are estimated using numer-
ical optimization methods (Malouf, 2002) to max-
imize the log-likelihood of the training data.
However, the above model cannot be easily es-
timated because the estimation requires the com-
putation of p(T|w) for all parse candidates as-
signed to sentence w. Because the number of
parse candidates is exponentially related to the
length of the sentence, the estimation is intractable
for long sentences. To make the model estimation
tractable, Geman and Johnson (Geman and John-
son, 2002) and Miyao and Tsujii (Miyao and Tsu-
jii, 2002) proposed a dynamic programming algo-
rithm for estimating p(T|w). Miyao and Tsujii
</bodyText>
<equation confidence="0.9927126">
ÃX !
p0(T&apos;|w) exp λufu(T&apos;)
u
p0(T|w) = Yn p(lz|wz),
z=1
</equation>
<bodyText confidence="0.991513866666667">
where lz is a lexical entry assigned to word wz in T
and
is the probability of selecting lexical
entry lz for wz.
In the experiments, we compared our model
with the probabilistic HPSG model of Miyao and
Tsujii (2005). The features used in their model are
combinations of the feature templates listed in Ta-
ble 1. The feature templates fbznary and
are defined for constituents at binary and unary
branches,
is a feature template set for the
root nodes of parse trees, and
is a feature tem-
plate set for calculating the preliminary
</bodyText>
<equation confidence="0.85562725">
p(lz|wz)
funary
froot
flex
</equation>
<bodyText confidence="0.99665125">
probabilis-
tic model. An example of features applied to the
parse tree for the sentence “Spring has come” is
shown in Figure 2.
</bodyText>
<equation confidence="0.9926544">
XZW =
T&apos;
157
*r, d, c, +
fbinary = spl, syl, hwl, hpl, hll,
</equation>
<bodyText confidence="0.2295544">
spr, syr, hwr, hpr, hlr
funary = hr, sy, hw, hp, hli
froot = hsy, hw, hp, hli
flex = hwi, pi, lii
combinations of feature templates for fbinary
</bodyText>
<construct confidence="0.9980518">
hr, d, c, hw, hp, hli, hr, d, c, hw, hpi, hr, d, c, hw, hli,
hr, d, c, sy, hwi, hr, c, sp, hw, hp, hli, hr, c, sp, hw, hpi,
hr, c, sp, hw, hli, hr, c, sp, sy, hwi, hr, d, c, hp, hli,
hr, d, c, hpi, hr, d, c, hli, hr, d, c, syi, hr, c, sp, hp, hli,
hr, c, sp, hpi, hr, c, sp, hli, hr, c, sp, syi
</construct>
<table confidence="0.24858115">
combinations of feature templates for funary
hr, hw, hp, hli, hr, hw, hpi, hr, hw, hli, hr, sy, hwi,
hr, hp, hli, hr, hpi, hr, hli, hr, syi
combinations of feature templates for froot
hhw, hp, hli, hhw, hpi, hhw, hli,
hsy, hwi, hhp, hli, hhpi, hhli, hsyi
combinations of feature templates for flex
hwi, pi, lii, hpi, lii
r name of the applied schema
d distance between the head words of the daughters
whether a comma exists between daughters
c and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
</table>
<tableCaption confidence="0.995175">
Table 1: Features.
</tableCaption>
<sectionHeader confidence="0.8456545" genericHeader="method">
3 Extremely lexicalized probabilistic
models
</sectionHeader>
<bodyText confidence="0.989034866666667">
In the experiments, we tested parsing with the pre-
vious model for the probabilistic HPSG explained
in Section 2 and other three types of probabilis-
tic models defined with the probabilities of lexi-
cal entry selection. The first one is the simplest
probabilistic model, which is defined with only
the probabilities of lexical entry selection. It is
defined simply as the product of the probabilities
of selecting all lexical entries in the sentence; i.e.,
the model does not use the probabilities of phrase
structures like the previous models.
Given a set of lexical entries, L, a sentence,
w = (w1, ... , wr,,), and the probabilistic model
of lexical entry selection, p(li E L|w, i), the first
model is formally defined as follows:
</bodyText>
<equation confidence="0.997143333333333">
(Model 1)
pmodel1(T|w) = Yn p(li|w, i),
i=1
</equation>
<bodyText confidence="0.999654111111111">
where li is a lexical entry assigned to word wi
in T and p(li|w, i) is the probability of selecting
lexical entry li for wi.
The second model is defined as the product of
the probabilities of selecting all lexical entries in
the sentence and the root node probability of the
parse tree. That is, the second model is also de-
fined without the probabilities on phrase struc-
tures:
</bodyText>
<equation confidence="0.996377368421053">
(Model 2)
pmodel2(T |w) =
pmodel1(T|w) exp ⎜ X ⎝ λufu(T)
u
(fu∈froot�
Zmodel2 =
⎛ ⎞
X
⎜ ⎟
pmodel1(T 0|w) exp ⎝λufu(T0) ⎠ ,
u(fu∈froot�
pmodel3(T|w) =
ÃX !
pmodel1(T |w) exp λufu(T )
u
Zmodel3 =
ÃX !
pmodel1(T 0|w) exp λufu(T 0) .
u
</equation>
<bodyText confidence="0.9999198">
In this study, the same model parameters used
in the previous model were used for phrase struc-
tures.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
</bodyText>
<subsectionHeader confidence="0.922455">
(Probabilistic Model of Lexical Entry Selection)
</subsectionHeader>
<bodyText confidence="0.996090142857143">
where
is the sum over the set of all pos-
sible parse trees for the sentence.
The third model is a hybrid of model 1 and the
previous model. The probabilities of the lexical
entries in the previous model are replaced with the
probabilities of lexical entry selection:
</bodyText>
<equation confidence="0.751789895833333">
(Model 3)
Zry,todel2
TI
ÃX !
1
p(li|w, i) = exp λufu(li, w, i)
Zw
u
1
Zmodel2
X
TI
1
Zmodel3
X
158
hwi−1i, hwii, hwi+1i,
hpi−2i, hpi−1i, hpii, hpi+1i, hpi+2i, hpi+3i,
hwi−1, wii, hwi, wi+1i,
hpi−1, wii, hpi, wii, hpi+1, wii,
hpi, pi+1, pi+2, pi+3i, hpi−2, pi−1, pii,
hpi−1, pi, pi+1i, hpi, pi+1, pi+2i
hpi−2, pi−1i, hpi−1, pii, hpi, pi+1i, hpi+1,pi+2i
Table 2: Features for the probabilities of lexical
entry selection.
procedure Parsing((w1, ... , wn), (L, R), α, β, κ, δ, θ)
for i = 1 ton
foreach F&apos; E {FI(w;, F) E L}
p = Eu λufu(F&apos;)
π[i - 1, i] - π[i - 1, i] U {F&apos;}
if (p &gt; ρ[i - 1, i, F&apos;]) then
ρ[i - 1, i, F&apos;] - p
LocalThresholding(i - 1, i,α, β)
for d = 1 to n
for i = 0 to n - d
j= i + d
for k=i+1toj-1
foreach F. E φ[i, k], Ft E φ[k, j], r E R
if F = r(F., Ft) has succeeded
p = ρ[i, k, F.] + ρ[k, j, Ft] + Eu λufu(F)
π[i, j] - π[i, j] U {F}
if (p &gt; ρ[i, j, F]) then
ρ[i, j, F] - p
LocalThresholding(i, j,κ, δ)
GlobalThresholding(i, n, θ)
procedure IterativeParsing(w, G, α0, β0, κ0, δ0, θ0, Aα, Aβ, Aκ,
Aδ, Aθ, αlast, βlast, κlast, δlast, θlast)
α - α0; β - β0; κ - κ0; δ - δ0; θ - θ0;
</equation>
<construct confidence="0.5726975">
loop while α &lt; αlast and β &lt; βlast and κ &lt; κlast and δ &lt; δlast
and θ &lt; θlast
call Parsing(w, G, α, β, κ, δ, θ)
if π[1, n] =� 0 then exit
α - α + Aα; β - β + Aβ;
κ - κ + Aκ; δ - δ + Aδ; θ - θ + Aθ;
</construct>
<figureCaption confidence="0.9160905">
Figure 3: Pseudo-code of iterative parsing for
HPSG.
</figureCaption>
<bodyText confidence="0.9999515">
where Z,,, is the sum over all possible lexical en-
tries for the word wi. The feature templates used
in our model are listed in Table 2 and are word
trigrams and POS 5-grams.
</bodyText>
<sectionHeader confidence="0.999023" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.957164">
4.1 Implementation
</subsectionHeader>
<bodyText confidence="0.999935151515152">
We implemented the iterative parsing algorithm
(Ninomiya et al., 2005) for the probabilistic HPSG
models. It first starts parsing with a narrow beam.
If the parsing fails, then the beam is widened, and
parsing continues until the parser outputs results
or the beam width reaches some limit. Though
the probabilities of lexical entry selection are in-
troduced, the algorithm for the presented proba-
bilistic models is almost the same as the original
iterative parsing algorithm.
The pseudo-code of the algorithm is shown in
Figure 3. In the figure, the 7r[i, j] represents
the set of partial parse results that cover words
wi+1, ... , wj, and p[i, j, F] stores the maximum
figure-of-merit (FOM) of partial parse result F
at cell (i, j). The probability of lexical entry
F is computed as Eu Aufu(F) for the previous
model, as shown in the figure. The probability
of a lexical entry for models 1, 2, and 3 is com-
puted as the probability of lexical entry selection,
p(Flw, i). The FOM of a newly created partial
parse, F, is computed by summing the values of
p of the daughters and an additional FOM of F if
the model is the previous model or model 3. The
FOM for models 1 and 2 is computed by only sum-
ming the values of p of the daughters; i.e., weights
exp(Au) in the figure are assigned zero. The terms
K and S are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs
in the chart cell. The terms α and Q are the thresh-
olds of the number and the beam width of lexical
entries, and 0 is the beam width for global thresh-
olding (Goodman, 1997).
</bodyText>
<subsectionHeader confidence="0.94587">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.998954086956522">
We evaluated the speed and accuracy of parsing
with extremely lexicalized models by using Enju
2.1, the HPSG grammar for English (Miyao et al.,
2005; Miyao and Tsujii, 2005). The lexicon of
the grammar was extracted from Sections 02-21 of
the Penn Treebank (Marcus et al., 1994) (39,832
sentences). The grammar consisted of 3,797 lex-
ical entries for 10,536 words1. The probabilis-
tic models were trained using the same portion of
the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al., 2005) and other tech-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by ap-
plying lexical rules to observed lexical entries in the HPSG
treebank (Nakanishi et al., 2004). The lexicon, however, in-
cluded many lexical entries that do not appear in the HPSG
treebank. The HPSG treebank is used for training the prob-
abilistic model for lexical entry selection, and hence, those
lexical entries that do not appear in the treebank are rarely
selected by the probabilistic model. The ‘effective’ tag set
size, therefore, is around 1,361, the number of lexical entries
without those never-seen lexical entries.
</bodyText>
<equation confidence="0.759294428571429">
wi−1, wi, wi+1,
pi−2, pi−1, pi, pi+1, pi+2
combinations of feature templates
\fexlex =
�Z. = exp I E λufu(lr, W, i)J
l&apos; \,
u
</equation>
<page confidence="0.395252">
159
</page>
<table confidence="0.85003025">
No. of tested sentences Total No. of Avg. length of tested sentences
&lt; 40 words &lt; 100 words sentences &lt; 40 words &lt; 100 words
Section 23 2,162 (94.04%) 2,299 (100.00%) 2,299 20.7 22.2
Section 24 1,157 (92.78%) 1,245 (99.84%) 1,247 21.2 23.0
</table>
<tableCaption confidence="0.991962">
Table 3: Statistics of the Penn Treebank.
</tableCaption>
<table confidence="0.999412357142857">
Section 23 (&lt; 40 + Gold POSs) Section 23 (&lt; 100 + Gold POSs)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 87.65 86.97 91.13 90.42 468 87.26 86.50 90.73 89.93 604
model 1 87.54 86.85 90.38 89.66 111 87.23 86.47 90.05 89.27 129
model 2 87.71 87.02 90.51 89.80 109 87.38 86.62 90.17 89.39 130
model 3 89.79 88.97 92.66 91.81 132 89.48 88.58 92.33 91.40 152
Section 23 (&lt; 40 + POS tagger) Section 23 (&lt; 100 + POS tagger)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 85.33 84.83 89.93 89.41 509 84.96 84.25 89.55 88.80 674
model 1 85.26 84.31 89.17 88.18 133 85.00 84.01 88.85 87.82 154
model 2 85.37 84.42 89.25 88.26 134 85.08 84.09 88.91 87.88 155
model 3 87.66 86.53 91.61 90.43 155 87.35 86.29 91.24 90.13 183
</table>
<tableCaption confidence="0.999654">
Table 4: Experimental results for Section 23.
</tableCaption>
<bodyText confidence="0.998943757575757">
niques for deep parsing2. The parameters for beam
searching were determined manually by trial and
error using Section 22: α0 = 4, Oα = 4, αlast =
20,β0 = 1.0,Oβ = 2.5, βlast = 11.0,δ0 =
12, Oδ = 4, δlast = 28, κ0 = 6.0, Oκ =
2.25, κlast = 15.0, θ0 = 8.0, Oθ = 3.0, and
θlast = 20.0. With these thresholding parame-
ters, the parser iterated at most five times for each
sentence.
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tu-
ple (σ, wh, a, wa), where σ is the predicate type
(e.g., adjective, intransitive verb), wh is the head
word of the predicate, a is the argument label
(MODARG, ARG1, ..., ARG4), and wa is the
head word of the argument. Labeled precision
(LP)/labeled recall (LR) is the ratio of tuples cor-
rectly identified by the parser3. Unlabeled pre-
cision (UP)/unlabeled recall (UR) is the ratio of
tuples without the predicate type and the argu-
ment label. This evaluation scheme was the
same as used in previous evaluations of lexicalized
grammars (Hockenmaier, 2003; Clark and Cur-
2Deep parsing techniques include quick check (Malouf
et al., 2000) and large constituent inhibition (Kaplan et al.,
2004) as described by Ninomiya et al. (2005), but hybrid
parsing with a CFG chunk parser was not used. This is be-
cause we did not observe a significant improvement for the
development set by the hybrid parsing and observed only a
small improvement in the parsing speed by around 10 ms.
3When parsing fails, precision and recall are evaluated,
although nothing is output by the parser; i.e., recall decreases
greatly.
ran, 2004b; Miyao and Tsujii, 2005). The ex-
periments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the
Treebank was used as the development set, and
the performance was evaluated using sentences of
&lt; 40 and 100 words in Section 23. The perfor-
mance of each parsing technique was analyzed us-
ing the sentences in Section 24 of &lt; 100 words.
Table 3 details the numbers and average lengths of
the tested sentences of &lt; 40 and 100 words in Sec-
tions 23 and 24, and the total numbers of sentences
in Sections 23 and 24.
The parsing performance for Section 23 is
shown in Table 4. The upper half of the table
shows the performance using the correct POSs in
the Penn Treebank, and the lower half shows the
performance using the POSs given by a POS tag-
ger (Tsuruoka and Tsujii, 2005). The left and
right sides of the table show the performances for
the sentences of &lt; 40 and &lt; 100 words. Our
models significantly increased not only the pars-
ing speed but also the parsing accuracy. Model
3 was around three to four times faster and had
around two points higher precision and recall than
the previous model. Surprisingly, model 1, which
used only lexical information, was very fast and
as accurate as the previous model. Model 2 also
improved the accuracy slightly without informa-
tion of phrase structures. When the automatic POS
tagger was introduced, both precision and recall
dropped by around 2 points, but the tendency to-
wards improved speed and accuracy was again ob-
</bodyText>
<table confidence="0.9548728">
160
88.00% previous model
86.00% model 1
84.00% model 2
82.00% model 3
80.00%
78.00%
76.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
</table>
<figureCaption confidence="0.990066">
Figure 4: F-score versus average parsing time for sentences in Section 24 of &lt; 100 words.
</figureCaption>
<bodyText confidence="0.998461631578947">
served.
The unlabeled precisions and recalls of the pre-
vious model and models 1, 2, and 3 were signifi-
cantly different as measured using stratified shuf-
fling tests (Cohen, 1995) with p-values &lt; 0.05.
The labeled precisions and recalls were signifi-
cantly different among models 1, 2, and 3 and
between the previous model and model 3, but
were not significantly different between the previ-
ous model and model 1 and between the previous
model and model 2.
The average parsing time and labeled F-score
curves of each probabilistic model for the sen-
tences in Section 24 of &lt; 100 words are graphed in
Figure 4. The superiority of our models is clearly
observed in the figure. Model 3 performed sig-
nificantly better than the previous model. Models
1 and 2 were significantly faster with almost the
same accuracy as the previous model.
</bodyText>
<sectionHeader confidence="0.996077" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.995571">
5.1 Supertagging
</subsectionHeader>
<bodyText confidence="0.999369666666667">
Our probabilistic model of lexical entry selection
can be used as an independent classifier for select-
ing lexical entries, which is called the supertag-
ger (Bangalore and Joshi, 1999; Clark and Curran,
2004b). The CCG supertagger uses a maximum
entropy classifier and is similar to our model.
We evaluated the performance of our probabilis-
tic model as a supertagger. The accuracy of the re-
sulting supertagger on our development set (Sec-
tion 22) is given in Table 5 and Table 6. The test
sentences were automatically POS-tagged. Re-
sults of other supertaggers for automatically ex-
</bodyText>
<table confidence="0.999023857142857">
test data accuracy (%)
HPSG supertagger 22 87.51
(this paper)
CCG supertagger 00/23 91.70 / 91.45
(Curran and Clark, 2003)
LTAG supertagger 22/23 86.01 / 86.27
(Shen and Joshi, 2003)
</table>
<tableCaption confidence="0.996408666666667">
Table 5: Accuracy of single-tag supertaggers. The
numbers under “test data” are the PTB section
numbers of the test data.
</tableCaption>
<table confidence="0.999486166666667">
-y tags/word word acc. (%) sentence acc. (%)
1e-1 1.30 92.64 34.98
1e-2 2.11 95.08 46.11
1e-3 4.66 96.22 51.95
1e-4 10.72 96.83 55.66
1e-5 19.93 96.95 56.20
</table>
<tableCaption confidence="0.998428">
Table 6: Accuracy of multi-supertagging.
</tableCaption>
<bodyText confidence="0.988550875">
tracted lexicalized grammars are listed in Table 5.
Table 6 gives the average number of supertags as-
signed to a word, the per-word accuracy, and the
sentence accuracy for several values of -y, which is
a parameter to determine how many lexical entries
are assigned.
When compared with other supertag sets of au-
tomatically extracted lexicalized grammars, the
(effective) size of our supertag set, 1,361 lexical
entries, is between the CCG supertag set (398 cat-
egories) used by Curran and Clark (2003) and the
LTAG supertag set (2920 elementary trees) used
by Shen and Joshi (2003). The relative order based
on the sizes of the tag sets exactly matches the or-
der based on the accuracies of corresponding su-
pertaggers.
</bodyText>
<page confidence="0.685253">
161
</page>
<subsectionHeader confidence="0.9809">
5.2 Efficacy of extremely lexicalized models
</subsectionHeader>
<bodyText confidence="0.999986070175439">
The implemented parsers of models 1 and 2 were
around four times faster than the previous model
without a loss of accuracy. However, what sur-
prised us is not the speed of the models, but
the fact that they were as accurate as the previ-
ous model, though they do not use any phrase-
structure-based probabilities. We think that the
correct parse is more likely to be selected if the
correct lexical entries are assigned high probabil-
ities because lexical entries include specific infor-
mation about subcategorization frames and syn-
tactic alternation, such as wh-movement and pas-
sivization, that likely determines the dominant
structures of parse trees. Another possible rea-
son for the accuracy is the constraints placed by
unification-based grammars. That is, incorrect
parse trees were suppressed by the constraints.
The best performer in terms of speed and ac-
curacy was model 3. The increased speed was,
of course, possible for the same reasons as the
speeds of models 1 and 2. An unexpected but
very impressive result was the significant improve-
ment of accuracy by two points in precision and
recall, which is hard to attain by tweaking param-
eters or hacking features. This may be because
the phrase structure information and lexical in-
formation complementarily improved the model.
The lexical information includes more specific in-
formation about the syntactic alternation, and the
phrase structure information includes information
about the syntactic structures, such as the dis-
tances of head words or the sizes of phrases.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. We ex-
emplified the dominance of lexical information in
real syntactic parsing, i.e., syntactic parsing with-
out gold-supertags, by showing that the proba-
bilities of lexical entry selection dominantly con-
tributed to syntactic parsing.
The CCG supertagging demonstrated fast and
accurate parsing for the probabilistic CCG (Clark
and Curran, 2004a). They used the supertag-
ger for eliminating candidates of lexical entries,
and the probabilities of parse trees were calcu-
lated using the phrase-structure-based model with-
out the probabilities of lexical entry selection. Our
study is essentially different from theirs in that the
probabilities of lexical entry selection have been
demonstrated to dominantly contribute to the dis-
ambiguation of phrase structures.
We have not yet investigated whether our results
can be reproduced with other lexicalized gram-
mars. Our results might hold only for HPSG be-
cause HPSG has strict feature constraints and has
lexical entries with rich syntactic information such
as wh-movement.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994103448276">
We developed an extremely lexicalized probabilis-
tic model for fast and accurate HPSG parsing.
The model is very simple. The probabilities of
parse trees are defined with only the probabili-
ties of selecting lexical entries, which are trained
by the discriminative methods in the log-linear
model with features of word trigrams and POS 5-
grams as defined in the CCG supertagging. Ex-
periments revealed that the model achieved im-
pressive accuracy as high as that of the previous
model for the probabilistic HPSG and that the im-
plemented parser runs around four times faster.
This indicates that accurate and fast parsing is pos-
sible using rather simple mechanisms. In addi-
tion, we provided another probabilistic model, in
which the probabilities for the leaf nodes in a parse
tree are given by the probabilities of supertag-
ging, and the probabilities for the intermediate
nodes are given by the previous phrase-structure-
based model. The experiments demonstrated not
only speeds significantly increased by three to four
times but also impressive improvement in parsing
accuracy by around two points in precision and re-
call.
We hope that this research provides a novel ap-
proach to deterministic parsing in which only lex-
ical selection and little phrasal information with-
out packed representations dominates the parsing
strategy.
</bodyText>
<sectionHeader confidence="0.996466" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995359637168142">
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23(4):597–
618.
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237–265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39–71.
162
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. ofACL’05, pages 173–180.
Stephen Clark and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Proc.
ofACL’04, pages 104–111.
Paul R. Cohen. 1995. Empirical Methods for Arti�cial
Intelligence. The MIT Press.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Univ. of Pennsylvania.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proc. of EACL’03, pages 91–98.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proc. of ACL’02,
pages 279–286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11–25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL’03, pages 359–366.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic “unification-based” grammars. In Proc.
ofACL ’99, pages 535–541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL’04.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL’03,
pages 423–430.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Proc. of IJCNLP-04 Workshop Be-
yond Shallow Analyses”.
Robert Malouf, John Carroll, and Ann Copestake.
2000. Efficient feature structure operations with-
out compilation. Journal of Natural Language En-
gineering, 6(1):29–46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49–55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002, pages 292–297.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. ofACL’05, pages 83–90.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii,
2005. Keh-Yih Su, Jun’ichi Tsujii, Jong-Hyeok Lee
and Oi Yee Kwong (Eds.), Natural Language Pro-
cessing - IJCNLP 2004 LNAI 3248, chapter Corpus-
oriented Grammar Development for Acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684–693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2004. An empirical investigation of the effect of lex-
ical rules on parsing with a treebank grammar. In
Proc. of TLT’04, pages 103–114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun’ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic hpsg parsing. In Proc. of IWPT
2005, pages 103–114.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling
of constraint-based grammars using log-linear mea-
sures and EM training. In Proc. of ACL’00, pages
480–487.
Libin Shen and Aravind K. Joshi. 2003. A SNoW
based supertagger with application to NP chunking.
In Proc. ofACL’03, pages 505–512.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proc. of HLT/EMNLP
2005, pages 467–474.
Wen Wang and Mary P. Harper. 2004. A statisti-
cal constraint dependency grammar (CDG) parser.
In Proc. of ACL’04 Incremental Parsing work-
shop: Bringing Engineering and Cognition To-
gether, pages 42–49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Gram-
mar. Ph.D. thesis, Purdue University.
</reference>
<page confidence="0.963277">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.087369">
<title confidence="0.998539">Extremely Lexicalized Models for Accurate and Fast HPSG Parsing</title>
<author confidence="0.933562">Takashi</author>
<affiliation confidence="0.8835826">Information Technology University of Tokyo Yoshimasa School of University of Manchester</affiliation>
<email confidence="0.606792">Takuya</email>
<affiliation confidence="0.999768">Department of Computer University of Tokyo</affiliation>
<author confidence="0.935492">Yusuke Miyao</author>
<affiliation confidence="0.9997375">Department of Computer University of Tokyo</affiliation>
<email confidence="0.47719">Jun’ichi</email>
<affiliation confidence="0.997067666666667">Department of Computer Science, University of School of Informatics, University of SORST, Japan Science and Technology</affiliation>
<address confidence="0.943385">Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan</address>
<email confidence="0.888367">matuzaki,tsuruoka,yusuke,</email>
<abstract confidence="0.997585333333333">This paper describes an extremely lexicalized probabilistic model for fast and accurate HPSG parsing. In this model, the probabilities of parse trees are defined with only the probabilities of selecting lexical entries. The proposed model is very simple, and experiments revealed that the implemented parser runs around four times faster than the previous model and that the proposed model has a high accuracy comparable to that of the previous model for probabilistic HPSG, which is defined over phrase structures. We also developed a hybrid of our probabilistic model and the conventional phrasestructure-based model. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<pages>618</pages>
<contexts>
<context position="9061" citStr="Abney, 1997" startWordPosition="1474" endWordPosition="1475">head-comp HEAD noun SUBJ &lt; &gt; COMPS &lt; &gt; HEAD verb 1 2 HEAD verb 1 COMPS &lt; &gt; 1 has Spring come Figure 1: HPSG parsing. HEAD noun 1 SUBJ &lt;&gt; COMPS &lt;&gt; froot= &lt;S, has, VBZ, Spring/NN has/VBZ come/VBN HEAD verb SUBJ &lt;&gt; COMPS &lt;&gt; subject-head HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 flex= &lt;spring, NN, HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; &gt; fbinary= HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;&gt; head-comp head-comp, 1, 0, 1, VP, has, VBZ, HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt;&gt; 1, VP, come, VBN, HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; &gt; HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; , HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; 2 SUBJ &lt; &gt; Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is phpsg (T |w) = 1exp ÃX λufu(T) ZW u Figure 2: Example of features. (2005) also introduced a preliminary probabilistic model p0(T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distributi</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597– 618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2694" citStr="Bangalore and Joshi, 1999" startWordPosition="387" endWordPosition="390">studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing. Bangalore and Joshi (1999) claimed that if words can be assigned correct supertags, syntactic parsing is almost trivial. Wha</context>
<context position="24846" citStr="Bangalore and Joshi, 1999" startWordPosition="4370" endWordPosition="4373">etween the previous model and model 2. The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of &lt; 100 words are graphed in Figure 4. The superiority of our models is clearly observed in the figure. Model 3 performed significantly better than the previous model. Models 1 and 2 were significantly faster with almost the same accuracy as the previous model. 5 Discussion 5.1 Supertagging Our probabilistic model of lexical entry selection can be used as an independent classifier for selecting lexical entries, which is called the supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004b). The CCG supertagger uses a maximum entropy classifier and is similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically extest data accuracy (%) HPSG supertagger 22 87.51 (this paper) CCG supertagger 00/23 91.70 / 91.45 (Curran and Clark, 2003) LTAG supertagger 22/23 86.01 / 86.27 (Shen and Joshi, 2003) Table 5: Accuracy of </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="9323" citStr="Berger et al., 1996" startWordPosition="1514" endWordPosition="1517">1 COMPS &lt; &gt; 2 flex= &lt;spring, NN, HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; &gt; fbinary= HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;&gt; head-comp head-comp, 1, 0, 1, VP, has, VBZ, HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt;&gt; 1, VP, come, VBN, HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; &gt; HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; , HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; 2 SUBJ &lt; &gt; Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is phpsg (T |w) = 1exp ÃX λufu(T) ZW u Figure 2: Example of features. (2005) also introduced a preliminary probabilistic model p0(T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have (Previous probabilistic HPSG) XZW = ÃX ! ÃX ! T&apos; exp λufu(T&apos;) , phpsg&apos;(T|w) = p0(T|w) 1exp λufu(T) u ZW u wh</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL’05,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1852" citStr="Charniak and Johnson, 2005" startWordPosition="263" endWordPosition="267">l. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model. 1 Introduction For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Charniak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proc. ofACL’05, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proc. of COLING-04.</booktitle>
<contexts>
<context position="1997" citStr="Clark and Curran, 2004" startWordPosition="285" endWordPosition="288">us model. 1 Introduction For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Charniak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which wa</context>
<context position="3958" citStr="Clark and Curran, 2004" startWordPosition="587" endWordPosition="590">ectly assigned, syntactic structures are almost de155 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155–163, Sydney, July 2006. c�2006 Association for Computational Linguistics termined because supertags include rich syntactic information such as subcategorization frames. Nasr and Rambow (2004) showed that the accuracy of LTAG parsing reached about 97%, assuming that the correct supertags were given. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser. They achieved accuracy as high as the state-of-the-art parsers. However, a supertagger itself was used as an external tagger that enumerates candidates of lexical entries or filters out unlikely lexical entries just to help parsing, and the best parse trees were selected mainly according to the probabilistic model for phrase structures or dependencies with/without the probabilistic model for superta</context>
<context position="24870" citStr="Clark and Curran, 2004" startWordPosition="4374" endWordPosition="4377">nd model 2. The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of &lt; 100 words are graphed in Figure 4. The superiority of our models is clearly observed in the figure. Model 3 performed significantly better than the previous model. Models 1 and 2 were significantly faster with almost the same accuracy as the previous model. 5 Discussion 5.1 Supertagging Our probabilistic model of lexical entry selection can be used as an independent classifier for selecting lexical entries, which is called the supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004b). The CCG supertagger uses a maximum entropy classifier and is similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically extest data accuracy (%) HPSG supertagger 22 87.51 (this paper) CCG supertagger 00/23 91.70 / 91.45 (Curran and Clark, 2003) LTAG supertagger 22/23 86.01 / 86.27 (Shen and Joshi, 2003) Table 5: Accuracy of single-tag supertaggers.</context>
<context position="28506" citStr="Clark and Curran, 2004" startWordPosition="4966" endWordPosition="4969">d the phrase structure information includes information about the syntactic structures, such as the distances of head words or the sizes of phrases. Nasr and Rambow (2004) showed that the accuracy of LTAG parsing reached about 97%, assuming that the correct supertags were given. We exemplified the dominance of lexical information in real syntactic parsing, i.e., syntactic parsing without gold-supertags, by showing that the probabilities of lexical entry selection dominantly contributed to syntactic parsing. The CCG supertagging demonstrated fast and accurate parsing for the probabilistic CCG (Clark and Curran, 2004a). They used the supertagger for eliminating candidates of lexical entries, and the probabilities of parse trees were calculated using the phrase-structure-based model without the probabilities of lexical entry selection. Our study is essentially different from theirs in that the probabilities of lexical entry selection have been demonstrated to dominantly contribute to the disambiguation of phrase structures. We have not yet investigated whether our results can be reproduced with other lexicalized grammars. Our results might hold only for HPSG because HPSG has strict feature constraints and </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing. In Proc. of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proc. ofACL’04,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1997" citStr="Clark and Curran, 2004" startWordPosition="285" endWordPosition="288">us model. 1 Introduction For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Charniak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which wa</context>
<context position="3958" citStr="Clark and Curran, 2004" startWordPosition="587" endWordPosition="590">ectly assigned, syntactic structures are almost de155 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155–163, Sydney, July 2006. c�2006 Association for Computational Linguistics termined because supertags include rich syntactic information such as subcategorization frames. Nasr and Rambow (2004) showed that the accuracy of LTAG parsing reached about 97%, assuming that the correct supertags were given. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser. They achieved accuracy as high as the state-of-the-art parsers. However, a supertagger itself was used as an external tagger that enumerates candidates of lexical entries or filters out unlikely lexical entries just to help parsing, and the best parse trees were selected mainly according to the probabilistic model for phrase structures or dependencies with/without the probabilistic model for superta</context>
<context position="24870" citStr="Clark and Curran, 2004" startWordPosition="4374" endWordPosition="4377">nd model 2. The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of &lt; 100 words are graphed in Figure 4. The superiority of our models is clearly observed in the figure. Model 3 performed significantly better than the previous model. Models 1 and 2 were significantly faster with almost the same accuracy as the previous model. 5 Discussion 5.1 Supertagging Our probabilistic model of lexical entry selection can be used as an independent classifier for selecting lexical entries, which is called the supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004b). The CCG supertagger uses a maximum entropy classifier and is similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically extest data accuracy (%) HPSG supertagger 22 87.51 (this paper) CCG supertagger 00/23 91.70 / 91.45 (Curran and Clark, 2003) LTAG supertagger 22/23 86.01 / 86.27 (Shen and Joshi, 2003) Table 5: Accuracy of single-tag supertaggers.</context>
<context position="28506" citStr="Clark and Curran, 2004" startWordPosition="4966" endWordPosition="4969">d the phrase structure information includes information about the syntactic structures, such as the distances of head words or the sizes of phrases. Nasr and Rambow (2004) showed that the accuracy of LTAG parsing reached about 97%, assuming that the correct supertags were given. We exemplified the dominance of lexical information in real syntactic parsing, i.e., syntactic parsing without gold-supertags, by showing that the probabilities of lexical entry selection dominantly contributed to syntactic parsing. The CCG supertagging demonstrated fast and accurate parsing for the probabilistic CCG (Clark and Curran, 2004a). They used the supertagger for eliminating candidates of lexical entries, and the probabilities of parse trees were calculated using the phrase-structure-based model without the probabilities of lexical entry selection. Our study is essentially different from theirs in that the probabilities of lexical entry selection have been demonstrated to dominantly contribute to the disambiguation of phrase structures. We have not yet investigated whether our results can be reproduced with other lexicalized grammars. Our results might hold only for HPSG because HPSG has strict feature constraints and </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proc. ofACL’04, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Arti�cial Intelligence.</title>
<date>1995</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="23984" citStr="Cohen, 1995" startWordPosition="4226" endWordPosition="4227"> of phrase structures. When the automatic POS tagger was introduced, both precision and recall dropped by around 2 points, but the tendency towards improved speed and accuracy was again ob160 88.00% previous model 86.00% model 1 84.00% model 2 82.00% model 3 80.00% 78.00% 76.00% 0 100 200 300 400 500 600 700 800 900 Parsing time (ms/sentence) Figure 4: F-score versus average parsing time for sentences in Section 24 of &lt; 100 words. served. The unlabeled precisions and recalls of the previous model and models 1, 2, and 3 were significantly different as measured using stratified shuffling tests (Cohen, 1995) with p-values &lt; 0.05. The labeled precisions and recalls were significantly different among models 1, 2, and 3 and between the previous model and model 3, but were not significantly different between the previous model and model 1 and between the previous model and model 2. The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of &lt; 100 words are graphed in Figure 4. The superiority of our models is clearly observed in the figure. Model 3 performed significantly better than the previous model. Models 1 and 2 were significantly faster wi</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R. Cohen. 1995. Empirical Methods for Arti�cial Intelligence. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Pennsylvania.</institution>
<contexts>
<context position="1798" citStr="Collins, 1999" startWordPosition="257" endWordPosition="258"> conventional phrasestructure-based model. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model. 1 Introduction For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Charniak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment informati</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proc. of EACL’03,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="25364" citStr="Curran and Clark, 2003" startWordPosition="4456" endWordPosition="4459">dent classifier for selecting lexical entries, which is called the supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004b). The CCG supertagger uses a maximum entropy classifier and is similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically extest data accuracy (%) HPSG supertagger 22 87.51 (this paper) CCG supertagger 00/23 91.70 / 91.45 (Curran and Clark, 2003) LTAG supertagger 22/23 86.01 / 86.27 (Shen and Joshi, 2003) Table 5: Accuracy of single-tag supertaggers. The numbers under “test data” are the PTB section numbers of the test data. -y tags/word word acc. (%) sentence acc. (%) 1e-1 1.30 92.64 34.98 1e-2 2.11 95.08 46.11 1e-3 4.66 96.22 51.95 1e-4 10.72 96.83 55.66 1e-5 19.93 96.95 56.20 Table 6: Accuracy of multi-supertagging. tracted lexicalized grammars are listed in Table 5. Table 6 gives the average number of supertags assigned to a word, the per-word accuracy, and the sentence accuracy for several values of -y, which is a parameter to de</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proc. of EACL’03, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Mark Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02,</booktitle>
<pages>279--286</pages>
<contexts>
<context position="10794" citStr="Geman and Johnson, 2002" startWordPosition="1769" endWordPosition="1773">of the weights exp(λu) when a characteristic corresponding to fu appears in parse result T. The model parameters, λu, are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T|w). Miyao and Tsujii ÃX ! p0(T&apos;|w) exp λufu(T&apos;) u p0(T|w) = Yn p(lz|wz), z=1 where lz is a lexical entry assigned to word wz in T and is the probability of selecting lexical entry lz for wz. In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). The features used in their model are combinations of the feature templates listed in Table 1. The feature templates fbznary and are defined for constituents at binary and unary branches, is a featur</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Stuart Geman and Mark Johnson. 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In Proc. of ACL’02, pages 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple pass parsing.</title>
<date>1997</date>
<booktitle>In Proc. of EMNLP-1997,</booktitle>
<pages>11--25</pages>
<contexts>
<context position="17959" citStr="Goodman, 1997" startWordPosition="3157" endWordPosition="3158">lw, i). The FOM of a newly created partial parse, F, is computed by summing the values of p of the daughters and an additional FOM of F if the model is the previous model or model 3. The FOM for models 1 and 2 is computed by only summing the values of p of the daughters; i.e., weights exp(Au) in the figure are assigned zero. The terms K and S are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and Q are the thresholds of the number and the beam width of lexical entries, and 0 is the beam width for global thresholding (Goodman, 1997). 4.2 Evaluation We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other tech1An HPSG t</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and multiple pass parsing. In Proc. of EMNLP-1997, pages 11–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Parsing with generative models of predicate-argument structure.</title>
<date>2003</date>
<booktitle>In Proc. of ACL’03,</booktitle>
<pages>359--366</pages>
<contexts>
<context position="21525" citStr="Hockenmaier, 2003" startWordPosition="3793" endWordPosition="3794">t of the parser. A predicate-argument relation is defined as a tuple (σ, wh, a, wa), where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser3. Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Cur2Deep parsing techniques include quick check (Malouf et al., 2000) and large constituent inhibition (Kaplan et al., 2004) as described by Ninomiya et al. (2005), but hybrid parsing with a CFG chunk parser was not used. This is because we did not observe a significant improvement for the development set by the hybrid parsing and observed only a small improvement in the parsing speed by around 10 ms. 3When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. ran, 2004b; Miyao and Tsujii, 2005). The experiments</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Parsing with generative models of predicate-argument structure. In Proc. of ACL’03, pages 359–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proc. ofACL ’99,</booktitle>
<pages>535--541</pages>
<contexts>
<context position="9083" citStr="Johnson et al., 1999" startWordPosition="1476" endWordPosition="1479">D noun SUBJ &lt; &gt; COMPS &lt; &gt; HEAD verb 1 2 HEAD verb 1 COMPS &lt; &gt; 1 has Spring come Figure 1: HPSG parsing. HEAD noun 1 SUBJ &lt;&gt; COMPS &lt;&gt; froot= &lt;S, has, VBZ, Spring/NN has/VBZ come/VBN HEAD verb SUBJ &lt;&gt; COMPS &lt;&gt; subject-head HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 flex= &lt;spring, NN, HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; &gt; fbinary= HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;&gt; head-comp head-comp, 1, 0, 1, VP, has, VBZ, HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt;&gt; 1, VP, come, VBN, HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; &gt; HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; , HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; 2 SUBJ &lt; &gt; Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is phpsg (T |w) = 1exp ÃX λufu(T) ZW u Figure 2: Example of features. (2005) also introduced a preliminary probabilistic model p0(T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilisti</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proc. ofACL ’99, pages 535–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>S Riezler</author>
<author>T H King</author>
<author>J T Maxwell</author>
<author>A Vasserman</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL’04.</booktitle>
<contexts>
<context position="9154" citStr="Kaplan et al., 2004" startWordPosition="1489" endWordPosition="1492">ng come Figure 1: HPSG parsing. HEAD noun 1 SUBJ &lt;&gt; COMPS &lt;&gt; froot= &lt;S, has, VBZ, Spring/NN has/VBZ come/VBN HEAD verb SUBJ &lt;&gt; COMPS &lt;&gt; subject-head HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 flex= &lt;spring, NN, HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; &gt; fbinary= HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;&gt; head-comp head-comp, 1, 0, 1, VP, has, VBZ, HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt;&gt; 1, VP, come, VBN, HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; &gt; HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; , HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; 2 SUBJ &lt; &gt; Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is phpsg (T |w) = 1exp ÃX λufu(T) ZW u Figure 2: Example of features. (2005) also introduced a preliminary probabilistic model p0(T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilit</context>
<context position="21661" citStr="Kaplan et al., 2004" startWordPosition="3812" endWordPosition="3815">intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser3. Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Cur2Deep parsing techniques include quick check (Malouf et al., 2000) and large constituent inhibition (Kaplan et al., 2004) as described by Ninomiya et al. (2005), but hybrid parsing with a CFG chunk parser was not used. This is because we did not observe a significant improvement for the development set by the hybrid parsing and observed only a small improvement in the parsing speed by around 10 ms. 3When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. ran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and the perform</context>
</contexts>
<marker>Kaplan, Riezler, King, Maxwell, Vasserman, 2004</marker>
<rawString>R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell III, and A. Vasserman. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proc. of HLT/NAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL’03,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="1823" citStr="Klein and Manning, 2003" startWordPosition="259" endWordPosition="262">hrasestructure-based model. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model. 1 Introduction For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Charniak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL’03, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proc. of IJCNLP-04 Workshop Beyond Shallow Analyses”.</booktitle>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proc. of IJCNLP-04 Workshop Beyond Shallow Analyses”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>John Carroll</author>
<author>Ann Copestake</author>
</authors>
<title>Efficient feature structure operations without compilation.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="21606" citStr="Malouf et al., 2000" startWordPosition="3804" endWordPosition="3807">, wa), where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser3. Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Cur2Deep parsing techniques include quick check (Malouf et al., 2000) and large constituent inhibition (Kaplan et al., 2004) as described by Ninomiya et al. (2005), but hybrid parsing with a CFG chunk parser was not used. This is because we did not observe a significant improvement for the development set by the hybrid parsing and observed only a small improvement in the parsing speed by around 10 ms. 3When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. ran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Tr</context>
</contexts>
<marker>Malouf, Carroll, Copestake, 2000</marker>
<rawString>Robert Malouf, John Carroll, and Ann Copestake. 2000. Efficient feature structure operations without compilation. Journal of Natural Language Engineering, 6(1):29–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-2002,</booktitle>
<pages>49--55</pages>
<contexts>
<context position="10353" citStr="Malouf, 2002" startWordPosition="1700" endWordPosition="1701">low probabilities by the model is omitted in the estimation stage. We have (Previous probabilistic HPSG) XZW = ÃX ! ÃX ! T&apos; exp λufu(T&apos;) , phpsg&apos;(T|w) = p0(T|w) 1exp λufu(T) u ZW u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T, and Z, is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu) when a characteristic corresponding to fu appears in parse result T. The model parameters, λu, are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T|w). Miyao and Tsujii ÃX ! p0(T&apos;|w) exp λufu(T&apos;) u p0</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proc. of CoNLL-2002, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="18254" citStr="Marcus et al., 1994" startWordPosition="3204" endWordPosition="3207">(Au) in the figure are assigned zero. The terms K and S are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and Q are the thresholds of the number and the beam width of lexical entries, and 0 is the beam width for global thresholding (Goodman, 1997). 4.2 Evaluation We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other tech1An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG treebank. The H</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proc. of HLT</booktitle>
<pages>292--297</pages>
<contexts>
<context position="10840" citStr="Miyao and Tsujii, 2002" startWordPosition="1778" endWordPosition="1782">rresponding to fu appears in parse result T. The model parameters, λu, are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T|w). Miyao and Tsujii ÃX ! p0(T&apos;|w) exp λufu(T&apos;) u p0(T|w) = Yn p(lz|wz), z=1 where lz is a lexical entry assigned to word wz in T and is the probability of selecting lexical entry lz for wz. In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). The features used in their model are combinations of the feature templates listed in Table 1. The feature templates fbznary and are defined for constituents at binary and unary branches, is a feature template set for the root nodes of parse tre</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proc. of HLT 2002, pages 292–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proc. ofACL’05,</booktitle>
<pages>83--90</pages>
<contexts>
<context position="2051" citStr="Miyao and Tsujii, 2005" startWordPosition="294" endWordPosition="297">e and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Charniak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining </context>
<context position="5932" citStr="Miyao and Tsujii, 2005" startWordPosition="891" endWordPosition="894">ses only the probabilities of lexical entry selection, the experiments revealed that it was as accurate as the previous phrasestructure-based model. Interestingly, this means that accurate parsing is possible using rather simple mechanisms. We also tested a hybrid model of the supertagging and the previous phrase-structurebased probabilistic model. In the hybrid model, the probabilities of the previous model are multiplied by the supertagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). In the previous model, the preliminary probabilistic model is defined as the probability of unigram supertagging. So, the hybrid model can be regarded as an extension of supertagging from unigram to n-gram. The hybrid model can also be regarded as a variant of the statistical CDG parser (Wang, 2003; Wang and Harper, 2004), in which the parse tree probabilities are defined as the product of the supertagging probabilities and the dependency probabilities. In the experiments, we observed that the hybrid model significantly improved the parsing speed, by around three to four times speed-ups, and</context>
<context position="9179" citStr="Miyao and Tsujii, 2005" startWordPosition="1493" endWordPosition="1496">G parsing. HEAD noun 1 SUBJ &lt;&gt; COMPS &lt;&gt; froot= &lt;S, has, VBZ, Spring/NN has/VBZ come/VBN HEAD verb SUBJ &lt;&gt; COMPS &lt;&gt; subject-head HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 flex= &lt;spring, NN, HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; &gt; fbinary= HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;&gt; head-comp head-comp, 1, 0, 1, VP, has, VBZ, HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt;&gt; 1, VP, come, VBN, HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; &gt; HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; , HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; 2 SUBJ &lt; &gt; Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is phpsg (T |w) = 1exp ÃX λufu(T) ZW u Figure 2: Example of features. (2005) also introduced a preliminary probabilistic model p0(T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitt</context>
<context position="11194" citStr="Miyao and Tsujii (2005)" startWordPosition="1842" endWordPosition="1845">se the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T|w). Miyao and Tsujii ÃX ! p0(T&apos;|w) exp λufu(T&apos;) u p0(T|w) = Yn p(lz|wz), z=1 where lz is a lexical entry assigned to word wz in T and is the probability of selecting lexical entry lz for wz. In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). The features used in their model are combinations of the feature templates listed in Table 1. The feature templates fbznary and are defined for constituents at binary and unary branches, is a feature template set for the root nodes of parse trees, and is a feature template set for calculating the preliminary p(lz|wz) funary froot flex probabilistic model. An example of features applied to the parse tree for the sentence “Spring has come” is shown in Figure 2. XZW = T&apos; 157 *r, d, c, + fbinary = spl, syl, hwl, hpl, hll, spr, syr, hwr, hpr, hlr funary = hr, sy, hw, hp, hli froot = hsy, hw, hp, </context>
<context position="18149" citStr="Miyao and Tsujii, 2005" startWordPosition="3186" endWordPosition="3189">. The FOM for models 1 and 2 is computed by only summing the values of p of the daughters; i.e., weights exp(Au) in the figure are assigned zero. The terms K and S are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and Q are the thresholds of the number and the beam width of lexical entries, and 0 is the beam width for global thresholding (Goodman, 1997). 4.2 Evaluation We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other tech1An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., </context>
<context position="22108" citStr="Miyao and Tsujii, 2005" startWordPosition="3888" endWordPosition="3891"> lexicalized grammars (Hockenmaier, 2003; Clark and Cur2Deep parsing techniques include quick check (Malouf et al., 2000) and large constituent inhibition (Kaplan et al., 2004) as described by Ninomiya et al. (2005), but hybrid parsing with a CFG chunk parser was not used. This is because we did not observe a significant improvement for the development set by the hybrid parsing and observed only a small improvement in the parsing speed by around 10 ms. 3When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. ran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of &lt; 40 and 100 words in Section 23. The performance of each parsing technique was analyzed using the sentences in Section 24 of &lt; 100 words. Table 3 details the numbers and average lengths of the tested sentences of &lt; 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is shown in Table 4. The upper half of the table shows t</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proc. ofACL’05, pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Keh-Yih Su, Jun’ichi Tsujii, Jong-Hyeok Lee and Oi Yee Kwong (Eds.), Natural Language Processing - IJCNLP</title>
<date>2005</date>
<pages>684--693</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="18124" citStr="Miyao et al., 2005" startWordPosition="3182" endWordPosition="3185">ous model or model 3. The FOM for models 1 and 2 is computed by only summing the values of p of the daughters; i.e., weights exp(Au) in the figure are assigned zero. The terms K and S are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and Q are the thresholds of the number and the beam width of lexical entries, and 0 is the beam width for global thresholding (Goodman, 1997). 4.2 Evaluation We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other tech1An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG tre</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2005</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii, 2005. Keh-Yih Su, Jun’ichi Tsujii, Jong-Hyeok Lee and Oi Yee Kwong (Eds.), Natural Language Processing - IJCNLP 2004 LNAI 3248, chapter Corpusoriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank, pages 684–693. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>An empirical investigation of the effect of lexical rules on parsing with a treebank grammar.</title>
<date>2004</date>
<booktitle>In Proc. of TLT’04,</booktitle>
<pages>103--114</pages>
<contexts>
<context position="18754" citStr="Nakanishi et al., 2004" startWordPosition="3280" endWordPosition="3283"> and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other tech1An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG treebank. The HPSG treebank is used for training the probabilistic model for lexical entry selection, and hence, those lexical entries that do not appear in the treebank are rarely selected by the probabilistic model. The ‘effective’ tag set size, therefore, is around 1,361, the number of lexical entries without those never-seen lexical entries. wi−1, wi, wi+1, pi−2, pi−1, pi, pi+1, pi+2 combinations of feature templates \fexlex = �Z. = exp I E λufu(lr, W, i)J l&apos; \, u 159 No. of tested sentences Total No. of A</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2004</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2004. An empirical investigation of the effect of lexical rules on parsing with a treebank grammar. In Proc. of TLT’04, pages 103–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Supertagging and full parsing.</title>
<date>2004</date>
<booktitle>In Proc. of the 7th International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7).</booktitle>
<contexts>
<context position="2587" citStr="Nasr and Rambow, 2004" startWordPosition="373" endWordPosition="376">ar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing. Bangalore and Jos</context>
<context position="28055" citStr="Nasr and Rambow (2004)" startWordPosition="4897" endWordPosition="4900">r the same reasons as the speeds of models 1 and 2. An unexpected but very impressive result was the significant improvement of accuracy by two points in precision and recall, which is hard to attain by tweaking parameters or hacking features. This may be because the phrase structure information and lexical information complementarily improved the model. The lexical information includes more specific information about the syntactic alternation, and the phrase structure information includes information about the syntactic structures, such as the distances of head words or the sizes of phrases. Nasr and Rambow (2004) showed that the accuracy of LTAG parsing reached about 97%, assuming that the correct supertags were given. We exemplified the dominance of lexical information in real syntactic parsing, i.e., syntactic parsing without gold-supertags, by showing that the probabilities of lexical entry selection dominantly contributed to syntactic parsing. The CCG supertagging demonstrated fast and accurate parsing for the probabilistic CCG (Clark and Curran, 2004a). They used the supertagger for eliminating candidates of lexical entries, and the probabilities of parse trees were calculated using the phrase-st</context>
</contexts>
<marker>Nasr, Rambow, 2004</marker>
<rawString>Alexis Nasr and Owen Rambow. 2004. Supertagging and full parsing. In Proc. of the 7th International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic hpsg parsing.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT</booktitle>
<pages>103--114</pages>
<contexts>
<context position="16452" citStr="Ninomiya et al., 2005" startWordPosition="2874" endWordPosition="2877">Aα, Aβ, Aκ, Aδ, Aθ, αlast, βlast, κlast, δlast, θlast) α - α0; β - β0; κ - κ0; δ - δ0; θ - θ0; loop while α &lt; αlast and β &lt; βlast and κ &lt; κlast and δ &lt; δlast and θ &lt; θlast call Parsing(w, G, α, β, κ, δ, θ) if π[1, n] =� 0 then exit α - α + Aα; β - β + Aβ; κ - κ + Aκ; δ - δ + Aδ; θ - θ + Aθ; Figure 3: Pseudo-code of iterative parsing for HPSG. where Z,,, is the sum over all possible lexical entries for the word wi. The feature templates used in our model are listed in Table 2 and are word trigrams and POS 5-grams. 4 Experiments 4.1 Implementation We implemented the iterative parsing algorithm (Ninomiya et al., 2005) for the probabilistic HPSG models. It first starts parsing with a narrow beam. If the parsing fails, then the beam is widened, and parsing continues until the parser outputs results or the beam width reaches some limit. Though the probabilities of lexical entry selection are introduced, the algorithm for the presented probabilistic models is almost the same as the original iterative parsing algorithm. The pseudo-code of the algorithm is shown in Figure 3. In the figure, the 7r[i, j] represents the set of partial parse results that cover words wi+1, ... , wj, and p[i, j, F] stores the maximum </context>
<context position="18534" citStr="Ninomiya et al., 2005" startWordPosition="3245" endWordPosition="3248">m width for global thresholding (Goodman, 1997). 4.2 Evaluation We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1. The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other tech1An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG treebank. The HPSG treebank is used for training the probabilistic model for lexical entry selection, and hence, those lexical entries that do not appear in the treebank are rarely selected by the probabilistic model. The ‘effective’ tag set size, therefore, is around 1,361, the number of lexic</context>
<context position="21700" citStr="Ninomiya et al. (2005)" startWordPosition="3819" endWordPosition="3822">rd of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser3. Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Cur2Deep parsing techniques include quick check (Malouf et al., 2000) and large constituent inhibition (Kaplan et al., 2004) as described by Ninomiya et al. (2005), but hybrid parsing with a CFG chunk parser was not used. This is because we did not observe a significant improvement for the development set by the hybrid parsing and observed only a small improvement in the parsing speed by around 10 ms. 3When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. ran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of &lt;</context>
</contexts>
<marker>Ninomiya, Tsuruoka, Miyao, Tsujii, 2005</marker>
<rawString>Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic hpsg parsing. In Proc. of IWPT 2005, pages 103–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="6796" citStr="Pollard and Sag, 1994" startWordPosition="1025" endWordPosition="1028">rded as a variant of the statistical CDG parser (Wang, 2003; Wang and Harper, 2004), in which the parse tree probabilities are defined as the product of the supertagging probabilities and the dependency probabilities. In the experiments, we observed that the hybrid model significantly improved the parsing speed, by around three to four times speed-ups, and accuracy, by around two points in both precision and recall, over the previous model. This implies that finer probabilistic model of lexical entry selection can improve the phrase-structure-based model. 2 HPSG and probabilistic models HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics. The structures of sentences are explained using combinations of schemata and lexical entries. Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structures are checked with unification. An example of HPSG parsing of the sentence “Spring has come” is shown in Figure 1. First, each of the lexical entries for “has” and “c</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proc. of ACL’00,</booktitle>
<pages>480--487</pages>
<contexts>
<context position="9105" citStr="Riezler et al., 2000" startWordPosition="1480" endWordPosition="1483">&lt; &gt; HEAD verb 1 2 HEAD verb 1 COMPS &lt; &gt; 1 has Spring come Figure 1: HPSG parsing. HEAD noun 1 SUBJ &lt;&gt; COMPS &lt;&gt; froot= &lt;S, has, VBZ, Spring/NN has/VBZ come/VBN HEAD verb SUBJ &lt;&gt; COMPS &lt;&gt; subject-head HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 flex= &lt;spring, NN, HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; &gt; fbinary= HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;&gt; head-comp head-comp, 1, 0, 1, VP, has, VBZ, HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt;&gt; 1, VP, come, VBN, HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; &gt; HEAD verb SUBJ &lt;NP&gt; COMPS &lt;VP&gt; , HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; 2 SUBJ &lt; &gt; Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = (w1, ... , wn) is phpsg (T |w) = 1exp ÃX λufu(T) ZW u Figure 2: Example of features. (2005) also introduced a preliminary probabilistic model p0(T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., th</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training. In Proc. of ACL’00, pages 480–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>A SNoW based supertagger with application to NP chunking.</title>
<date>2003</date>
<booktitle>In Proc. ofACL’03,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="25424" citStr="Shen and Joshi, 2003" startWordPosition="4466" endWordPosition="4469"> the supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004b). The CCG supertagger uses a maximum entropy classifier and is similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically extest data accuracy (%) HPSG supertagger 22 87.51 (this paper) CCG supertagger 00/23 91.70 / 91.45 (Curran and Clark, 2003) LTAG supertagger 22/23 86.01 / 86.27 (Shen and Joshi, 2003) Table 5: Accuracy of single-tag supertaggers. The numbers under “test data” are the PTB section numbers of the test data. -y tags/word word acc. (%) sentence acc. (%) 1e-1 1.30 92.64 34.98 1e-2 2.11 95.08 46.11 1e-3 4.66 96.22 51.95 1e-4 10.72 96.83 55.66 1e-5 19.93 96.95 56.20 Table 6: Accuracy of multi-supertagging. tracted lexicalized grammars are listed in Table 5. Table 6 gives the average number of supertags assigned to a word, the per-word accuracy, and the sentence accuracy for several values of -y, which is a parameter to determine how many lexical entries are assigned. When compared</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. A SNoW based supertagger with application to NP chunking. In Proc. ofACL’03, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP</booktitle>
<pages>467--474</pages>
<contexts>
<context position="22873" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="4032" endWordPosition="4035">nd the performance was evaluated using sentences of &lt; 40 and 100 words in Section 23. The performance of each parsing technique was analyzed using the sentences in Section 24 of &lt; 100 words. Table 3 details the numbers and average lengths of the tested sentences of &lt; 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is shown in Table 4. The upper half of the table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger (Tsuruoka and Tsujii, 2005). The left and right sides of the table show the performances for the sentences of &lt; 40 and &lt; 100 words. Our models significantly increased not only the parsing speed but also the parsing accuracy. Model 3 was around three to four times faster and had around two points higher precision and recall than the previous model. Surprisingly, model 1, which used only lexical information, was very fast and as accurate as the previous model. Model 2 also improved the accuracy slightly without information of phrase structures. When the automatic POS tagger was introduced, both precision and recall droppe</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proc. of HLT/EMNLP 2005, pages 467–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
</authors>
<title>A statistical constraint dependency grammar (CDG) parser.</title>
<date>2004</date>
<booktitle>In Proc. of ACL’04 Incremental Parsing workshop: Bringing Engineering and Cognition Together,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="2563" citStr="Wang and Harper, 2004" startWordPosition="369" endWordPosition="372">natory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of pa</context>
<context position="4046" citStr="Wang and Harper (2004)" startWordPosition="602" endWordPosition="605"> on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155–163, Sydney, July 2006. c�2006 Association for Computational Linguistics termined because supertags include rich syntactic information such as subcategorization frames. Nasr and Rambow (2004) showed that the accuracy of LTAG parsing reached about 97%, assuming that the correct supertags were given. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser. They achieved accuracy as high as the state-of-the-art parsers. However, a supertagger itself was used as an external tagger that enumerates candidates of lexical entries or filters out unlikely lexical entries just to help parsing, and the best parse trees were selected mainly according to the probabilistic model for phrase structures or dependencies with/without the probabilistic model for supertagging. We investigate an extreme case of HPSG parsing in which the probabilistic model i</context>
<context position="6257" citStr="Wang and Harper, 2004" startWordPosition="944" endWordPosition="947">sed probabilistic model. In the hybrid model, the probabilities of the previous model are multiplied by the supertagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). In the previous model, the preliminary probabilistic model is defined as the probability of unigram supertagging. So, the hybrid model can be regarded as an extension of supertagging from unigram to n-gram. The hybrid model can also be regarded as a variant of the statistical CDG parser (Wang, 2003; Wang and Harper, 2004), in which the parse tree probabilities are defined as the product of the supertagging probabilities and the dependency probabilities. In the experiments, we observed that the hybrid model significantly improved the parsing speed, by around three to four times speed-ups, and accuracy, by around two points in both precision and recall, over the previous model. This implies that finer probabilistic model of lexical entry selection can improve the phrase-structure-based model. 2 HPSG and probabilistic models HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism</context>
</contexts>
<marker>Wang, Harper, 2004</marker>
<rawString>Wen Wang and Mary P. Harper. 2004. A statistical constraint dependency grammar (CDG) parser. In Proc. of ACL’04 Incremental Parsing workshop: Bringing Engineering and Cognition Together, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
</authors>
<title>Statistical Parsing and Language Modeling based on Constraint Dependency Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Purdue University.</institution>
<contexts>
<context position="6233" citStr="Wang, 2003" startWordPosition="942" endWordPosition="943">-structurebased probabilistic model. In the hybrid model, the probabilities of the previous model are multiplied by the supertagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). In the previous model, the preliminary probabilistic model is defined as the probability of unigram supertagging. So, the hybrid model can be regarded as an extension of supertagging from unigram to n-gram. The hybrid model can also be regarded as a variant of the statistical CDG parser (Wang, 2003; Wang and Harper, 2004), in which the parse tree probabilities are defined as the product of the supertagging probabilities and the dependency probabilities. In the experiments, we observed that the hybrid model significantly improved the parsing speed, by around three to four times speed-ups, and accuracy, by around two points in both precision and recall, over the previous model. This implies that finer probabilistic model of lexical entry selection can improve the phrase-structure-based model. 2 HPSG and probabilistic models HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexic</context>
</contexts>
<marker>Wang, 2003</marker>
<rawString>Wen Wang. 2003. Statistical Parsing and Language Modeling based on Constraint Dependency Grammar. Ph.D. thesis, Purdue University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>