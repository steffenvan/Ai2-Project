<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9981085">
Learning Semantic Word Embeddings based on
Ordinal Knowledge Constraints
</title>
<author confidence="0.981156">
Quan Liu† and Hui Jiang$ and Si Wei§ and Zhen-Hua Ling† and Yu Hu§
</author>
<affiliation confidence="0.871228">
† National Engineering Laboratory for Speech and Language Information Processing
University of Science and Technology of China, Hefei, China
$ Department of Electrical Engineering and Computer Science, York University, Canada
§ iFLYTEK Research, Hefei, China
</affiliation>
<email confidence="0.892503">
emails: quanliu@mail.ustc.edu.cn, hj@cse.yorku.ca,
siwei@iflytek.com, zhling@ustc.edu.cn, yuhu@iflytek.com
</email>
<sectionHeader confidence="0.994491" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981464285714">
In this paper, we propose a general frame-
work to incorporate semantic knowledge
into the popular data-driven learning pro-
cess of word embeddings to improve the
quality of them. Under this framework,
we represent semantic knowledge as many
ordinal ranking inequalities and formu-
late the learning of semantic word embed-
dings (SWE) as a constrained optimiza-
tion problem, where the data-derived ob-
jective function is optimized subject to all
ordinal knowledge inequality constraints
extracted from available knowledge re-
sources such as Thesaurus and Word-
Net. We have demonstrated that this con-
strained optimization problem can be ef-
ficiently solved by the stochastic gradient
descent (SGD) algorithm, even for a large
number of inequality constraints. Experi-
mental results on four standard NLP tasks,
including word similarity measure, sen-
tence completion, name entity recogni-
tion, and the TOEFL synonym selection,
have all demonstrated that the quality of
learned word vectors can be significantly
improved after semantic knowledge is in-
corporated as inequality constraints during
the learning process of word embeddings.
</bodyText>
<sectionHeader confidence="0.998125" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.965863673076923">
Distributed word representation (i.e., word embed-
ding) is a technique that represents words as con-
tinuous vectors, which is an important research
topic in natural language processing (NLP) (Hin-
ton et al., 1986; Turney et al., 2010). In re-
cent years, it has been widely used in various
NLP tasks, including neural language model (Ben-
gio et al., 2003; Schwenk, 2007), sequence la-
belling tasks (Collobert and Weston, 2008; Col-
lobert et al., 2011), machine translation (Devlin et
al., 2014; Sutskever et al., 2014), and antonym se-
lection (Chen et al., 2015). Typically, word vectors
are learned based on the distributional hypothesis
(Harris, 1954; Miller and Charles, 1991), which
assumes that words with a similar context tend
to have a similar meaning. Under this hypothe-
sis, various models, such as the skip-gram model
(Mikolov et al., 2013a; Mikolov et al., 2013b;
Levy and Goldberg, 2014) and GloVe model (Pen-
nington et al., 2014), have been proposed to lever-
age the context of each word in large corpora to
learn word embeddings. These methods can ef-
ficiently estimate the co-occurrence statistics to
model contextual distributions from very large text
corpora and they have been demonstrated to be
quite effective in a number of NLP tasks. How-
ever, they still suffer from some major limitations.
In particular, these corpus-based methods usu-
ally fail to capture the precise meanings for many
words. For example, some semantically related
but dissimilar words may have similar contexts,
such as synonyms and antonyms. As a result, these
corpus-based methods may lead to some antony-
mous word vectors being located much closer in
the learned embedding space than many synony-
mous words. Moreover, as word representations
are mainly learned based on the co-occurrence in-
formation, the learned word embeddings do not
capture the accurate relationship between two se-
mantically similar words if either one appears less
frequently in the corpus.
To address these issues, some recent work has
been proposed to incorporate prior lexical knowl-
edge (WordNet, PPDB, etc.) or knowledge graph
(Freebase, etc.) into word representations. Such
knowledge enhanced word embedding methods
have achieved considerable improvements on var-
ious natural language processing tasks, like (Yu
and Dredze, 2014; Bian et al., 2014; Xu et al.,
2014). These methods attempt to increase the se-
mantic similarities between words belonging to
1501
</bodyText>
<note confidence="0.997502666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1501–1511,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999963490196079">
one semantic category or to explicitly model the
semantic relationships between different words.
For example, Yu and Dredze (2014) have proposed
a new learning objective function to enhance
word embeddings by combining neural models
and a prior knowledge measure from semantic re-
sources. Bian et. al (2014) have recently proposed
to leverage morphological, syntactic, and semantic
knowledge to improve the learning of word em-
beddings. Besides, a novel framework has been
proposed in (Xu et al., 2014) to take advantage of
both relational and categorical knowledge to learn
high-quality word representations, where two reg-
ularization functions are used to model the re-
lational and categorical knowledge respectively.
More recently, a retrofitting technique has been in-
troduced in (Faruqui et al., 2014) to improve se-
mantic vectors by leveraging lexicon-derived rela-
tional information in a post-processing stage.
In this paper, we propose a new and flexible
method to incorporate semantic knowledge into
the corpus-based learning of word embeddings.
In our approach, we propose to represent seman-
tic knowledge as many word ordinal ranking in-
equalities. Furthermore, these inequalities are cast
as semantic constraints in the optimization pro-
cess to learn semantically sensible word embed-
dings. The proposed method has several advan-
tages. Firstly, many different types of seman-
tic knowledge can all be represented as a num-
ber of such ranking inequalities, such as synonym-
antonym, hyponym-hypernym and etc. Secondly,
these inequalities can be easily extracted from
many existing knowledge resources, such as The-
saurus, WordNet (Miller, 1995) and knowledge
graphs. Moreover, the ranking inequalities can
also be manually generated by human annotation
because ranking orders is much easier for human
annotators than assigning specific scores. Next,
we present a flexible learning framework to learn
distributed word representation based on the ordi-
nal semantic knowledge. By solving a constrained
optimization problem using the efficient stochas-
tic gradient descent algorithm, we can obtain se-
mantic word embedding enhanced by the ordinal
knowledge constraints. Experiments on four pop-
ular natural language processing tasks, including
word similarity, sentence completion, name en-
tity recognition and synonym selection, have all
demonstrated that the proposed method can learn
good semantically sensible word embeddings.
</bodyText>
<sectionHeader confidence="0.854004" genericHeader="method">
2 Representing Knowledge By Ranking
</sectionHeader>
<bodyText confidence="0.994667333333334">
Many types of lexical semantic knowledge can be
quantitatively represented by a large number of
ranking inequalities such as:
similarity(wi, wj) &gt; similarity(wi, wk) (1)
where wi, wj and wk denote any three words in
vocabulary. For example, eq.(1) holds if wj is a
synonym of wi and wk is an antonym of wi. In
general, the similarity between a word and its syn-
onymous word should be larger than the similar-
ity between the word and its antonymous word.
Moreover, a particular word should be more sim-
ilar to the words belonging to the same semantic
category as this word than other words belonging
to a different category. Besides, eq.(1) holds if wi
and wj have shorter distance in a semantic hierar-
chy than wi and wk do in the same hierarchy (Lea-
cock and Chodorow, 1998; Jurafsky and Martin,
2000).
Equivalently, each of the above similarity in-
equalities may be represented as the following
constraint in the embedding space:
</bodyText>
<equation confidence="0.997711625">
sim(w(1)
i , w(1)
j ) &gt; sim(w(1)
i , w(1)
k ) (2)
where w(1)
i , w(1)
j and w(1)
</equation>
<bodyText confidence="0.998091166666667">
k denote the embedding
vectors of the words, wi, wj and wk.
In this paper, we use the following three rules to
gather the ordinal semantic knowledge from avail-
able lexical knowledge resources, such as The-
saurus and WordNet.
</bodyText>
<listItem confidence="0.9996682">
• Synonym Antonym Rule: Similarities be-
tween a word and its synonymous words
are always larger than similarities be-
tween the word and its antonymous words.
For example, the similarity between fool-
ish and stupid is expected to be bigger
than the similarity between foolish and
clever, i.e., similarity(foolish, stupid) &gt;
similarity(foolish, clever).
• Semantic Category Rule: Similarities of
</listItem>
<bodyText confidence="0.965044625">
words that belong to the same semantic cat-
egory would be larger than similarities of
words that belong to different categories.
This rule refers to the idea of Fisher lin-
ear discriminant algorithm in (Fisher, 1936).
A semantic category may be defined as a
synset in WordNet, a hypernym in a se-
mantic hierarchy, or an entity category in
</bodyText>
<figure confidence="0.731737071428571">
1502
Tool model aims to maximize the following objective
Input Word
function:
Hyponyms Hammer Chisel Saw Co-Hyponyms
Mallet Plessor Hacksaw Jigsaw
−c≤j≤c,j�=0
M
Hypernym
log p(wt+j|wt) (3)
1
Q = T
T
t=1
</figure>
<figureCaption confidence="0.995956">
Figure 1: An example of hyponym and hypernym.
</figureCaption>
<bodyText confidence="0.911574166666667">
knowledge graphs. Figure 1 shows a sim-
ple example of the relationship between hy-
ponyms and hypernyms. From there, it is
reasonable to assume the following similar-
ity inequality: similarity(Mallet, Plessor) &gt;
similarity(Mallet, Hacksaw).
</bodyText>
<listItem confidence="0.981275">
• Semantic Hierarchy Rule: Similarities be-
tween words that have shorter distances in
a semantic hierarchy should be larger than
similarities of words that have longer dis-
tances. In this work, the semantic hi-
erarchy refers to the hypernym and hy-
</listItem>
<subsectionHeader confidence="0.573915">
sim(wt,w &gt; simwj,wk)
She Embedng
</subsectionHeader>
<bodyText confidence="0.770315">
ponym structure in WordNet. From Fig-
</bodyText>
<equation confidence="0.537693">
W(1
</equation>
<bodyText confidence="0.998592833333333">
ure 1, this rule may suggest several inequal-
ities like: similarity(Mallet, Hammer) &gt;
similarity(Mallet, Tool).
In addition, we may generate many such se-
mantically ranking similarity inequalities by hu-
man annotation through crowdsourcing.
</bodyText>
<sectionHeader confidence="0.985507" genericHeader="method">
3 Semantic Word Embedding
</sectionHeader>
<bodyText confidence="0.9999366">
In this section, we first briefly review the conven-
tional skip-gram model (Mikolov et al., 2013b).
Next, we study how to incorporate the ordinal sim-
ilarity inequalities to learn semantic word embed-
dings.
</bodyText>
<subsectionHeader confidence="0.999088">
3.1 The skip-gram model
</subsectionHeader>
<bodyText confidence="0.999108944444445">
The skip-gram model is a recently proposed learn-
ing framework (Mikolov et al., 2013b; Mikolov et
al., 2013a) to learn continuous word vectors from
text corpora based on the aforementioned distribu-
tional hypothesis, where each word in vocabulary
(size of V ) is mapped to a continuous embedding
space by looking up an embedding matrix W(1).
And W(1) is learned by maximizing the predic-
tion probability, calculated by another prediction
matrix W(2), of its neighbouring words within a
context window.
Given a sequence of training data, denoted as
w1, w2, w3, ..., wT with T words, the skip-gram
where c is the size of context windows, wt de-
notes the input central word and wt+j for its neigh-
bouring word. The skip-gram model computes
the above conditional probability p(wt+j|wt) us-
ing the following softmax function:
</bodyText>
<equation confidence="0.970562166666667">
(4)
Ek=1 exp(w(2)
k · w(1)
t )
where w(1)
t and w(2)
</equation>
<bodyText confidence="0.909532">
k denotes row vectors in ma-
trices W(1) and W(2), corresponding to word wt
and wk respectively.
</bodyText>
<subsectionHeader confidence="0.993673">
Semantic Knowl
</subsectionHeader>
<bodyText confidence="0.9999585">
The training process of the skip-gram model
can be formulated as an optimization problem to
</bodyText>
<subsectionHeader confidence="0.804132">
WordNet
</subsectionHeader>
<bodyText confidence="0.999956">
maximize the above objective function Q. As in
(Mikolovet al., 2013b), this optimization problem
</bodyText>
<subsectionHeader confidence="0.983679">
Knowledge Human Labelng
</subsectionHeader>
<bodyText confidence="0.9999315">
is solved by the stochastic gradient descent (SGD)
method and the learned embedding matrix W(1)
k is used as the word embeddings for all words in
vocabulary.
</bodyText>
<subsectionHeader confidence="0.9982685">
3.2 Semantic Word Embedding (SWE) as
Constrained Optimization
</subsectionHeader>
<bodyText confidence="0.9978821">
Here we consider how to combine the ordinal
knowledge representation in section 2 and the
skip-gram model in 3.1 to learn semantic word
embeddings (SWE).
As shown in section 2, each ranking inequal-
ity involves a triplet, (i, j, k), of three words,
{wi, wj, wk}. Assume the ordinal knowledge is
represented by a large number of such inequalities,
denoted as the inequality set S. For ∀(i, j, k) ∈ S,
we have:
</bodyText>
<equation confidence="0.980407222222222">
similarity(wi, wj) &gt; similarity(wi, wk)
⇔ sim(w(1)
i , w(1)
j ) &gt; sim(w(1)
i , w(1)
k ).
For notational simplicity, we denote sij =
sim(w(1)
i , w(1)
</equation>
<bodyText confidence="0.97588175">
j ) hereafter.
Next, we propose to use the following con-
strained optimization problem to learn semantic
word embeddings (SWE):
</bodyText>
<equation confidence="0.921408333333333">
{W(1), W(2)} = arg max
W(1),W(2)
p(wt+j|wt) =
exp(w(2)
t+j · w(1)
t )
Q(W(1), W(2))
(5)
1503
</equation>
<figureCaption confidence="0.993544">
Figure 2: The proposed semantic word embedding (SWE) learning framework (The left part denotes the
state-of-the-art skip-gram model; The right part represents the semantic constraints).
</figureCaption>
<figure confidence="0.905986933333333">
wt-c wt-c+1 wt+c-1 wt+c
Distributional
Hypothesis
R(2)
Share Embedding
Joint
Optimization
R(1)
wi wj wk
Knowledge Constraints
sim(wi,wj) &gt; sim(wi,wk)
R(1)
wt
subject to
sij &gt; sik b(i, j, k) E S. (6)
</figure>
<bodyText confidence="0.997353">
In this work, we formulate the above con-
strained optimization problem into an uncon-
strained one by casting all the constraints as a
penalty term in the objective function. The penalty
term can be expressed as follows:
</bodyText>
<equation confidence="0.976266">
�D = f(i, j, k) (7)
(i,j,k)ES
</equation>
<bodyText confidence="0.999936083333333">
where the function f(·) is a normalization func-
tion. It can be a sigmoid function like f(i, j, k) =
σ(sik − sij) with σ(x) = 1/(1 + exp(−x)). Al-
ternatively, it may be a hinge loss function like
f(i, j, k) = h(sik−sij) where h(x) = max(δ0, x)
with δ0 denoting a parameter to control the de-
cision margin. In this work, we adopt to use
the hinge function to compute the penalty term in
eq.(7) and δ0 is set to be 0 for all experiments.
Finally, the proposed semantic word embed-
ding (SWE) model aims to maximize the follow-
ing combined objective function:
</bodyText>
<equation confidence="0.991954">
2&apos; = 2 − β · D (8)
</equation>
<bodyText confidence="0.999968666666667">
where β is a control parameter to balance the con-
tribution of the penalty term in the optimization
process. It balances between the semantic infor-
mation estimated from the corpus based on the
distributional hypothesis and the semantic knowl-
edge encoded in the ordinal ranking inequalities.
In Rockt¨aschel et al. (2014), a similar approach
was proposed to capture knowledge constraint as
extra terms in the objective function for optimiza-
tion.
In Figure 2, we show a diagram for the the over-
all SWE learning framework to incorporate se-
mantic knowledge into the basic skip-gram word
embeddings. Comparing with the previous work
in (Xu et al., 2014) and (Faruqui et al., 2014),
the proposed SWE framework is more general in
terms of encoding the semantic knowledge for
learning word embeddings. It is straightforward
to show that the work in (Xu et al., 2014; Zweig,
2014; Faruqui et al., 2014) can be viewed as some
special cases under our SWE learning framework.
</bodyText>
<subsectionHeader confidence="0.98576">
3.3 Optimization algorithm for SWE
</subsectionHeader>
<bodyText confidence="0.999754">
In this work, the proposed semantic word em-
beddings (SWE) are learned using the standard
mini-batch stochastic gradient descent (SGD) al-
gorithm. Furthermore, we adopt to use the cosine
distance of the embedding vectors to compute the
similarity between two words in the penalty term.
In the following, we show how to compute the
derivatives of the penalty term for the SWE learn-
ing.
</bodyText>
<equation confidence="0.9978943">
∂w(1)
∂D �
t (i,j,k)ES
= ∂f (sik − sij)
∂w(1)
t
�
δij(t) ∂sij (9)
∂w(1)
t
</equation>
<bodyText confidence="0.94043">
where δik(t) and δij(t) are computed as
</bodyText>
<equation confidence="0.93509975">
�
1 t = i or t = k
δik(t) = (10)
0 otherwise
</equation>
<bodyText confidence="0.9913695">
and for the hinge loss function f(x), we have
�
</bodyText>
<equation confidence="0.996623666666667">
1 (sik − sij) &gt; δ0
f&apos; = (11)
0 (sik − sij) :5 δ0
</equation>
<bodyText confidence="0.9813025">
and the derivatives of the cosine similarity mea-
sure, sij = %w(1) w(1) , with respect to a word vec-
</bodyText>
<equation confidence="0.984991">
w(1)·w(1)
7
�= ∂sik
(i,j,k)ES f&apos;
δik (t) ∂w(1)
t
1504
tor, i.e., ∂sik
∂w(1) , which can be derived as follows:
z
w(1)
j
|w(1)i ||(1)|. (12)wj
</equation>
<bodyText confidence="0.9999673">
The learning rate used for the SWE learning
is the same as that for the skip-gram model. In
each mini-batch of SGD, we sample terms in the
same way as the skip-gram model. As for the con-
straints, we do not sample them but use all inequal-
ities relevant to any words in a minibatch to update
the model for the minibatch. Finally, by jointly op-
timizing the two terms in the combined objective
function, we may learn a new set of word vectors
encoding with ordinal semantic knowledge.
</bodyText>
<sectionHeader confidence="0.999682" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999988333333333">
In this section, we report all experiments con-
ducted to evaluate the effectiveness of the pro-
posed semantic word embeddings (SWE). Here
we compare the performance of the proposed
SWE model with the conventional skip-gram
baseline model on four popular natural language
processing tasks, including word similarity mea-
sure, sentence completion, name entity recogni-
tion, and synonym selection. In the following,
we first describe the experimental setup, training
corpora, semantic knowledge databases. Next,
we report the experimental results on these four
NLP tasks. Note that the SWE training codes
and scripts are made publicly available at http:
//home.ustc.edu.cn/˜quanliu/.
</bodyText>
<subsectionHeader confidence="0.984887">
4.1 Experimental setup
4.1.1 Training corpora
</subsectionHeader>
<bodyText confidence="0.999983416666667">
In this work, we use the popular Wikipedia cor-
pus as our training data to learn word embeddings
for experiments on the word similarity task and
the TOEFL synonym selection task. Particularly,
we utilize two Wikipedia corpora with different
sizes. The first corpus with a smaller size is a
data set including the first one billion characters
from Wikipedia1, named as Wiki-Small in our ex-
periments. The second corpus with a relatively
large size is the latest Wikipedia dump2, named
as Wiki-Large in our experiments. Both Wikipedia
corpora have been pre-processed by removing all
</bodyText>
<equation confidence="0.496934666666667">
1http://mattmahoney.net/dc/enwik9.zip
2http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-
pages-articles.xml.bz2
</equation>
<bodyText confidence="0.999901857142857">
the HTML meta-data and hyper-links and replac-
ing the digit numbers with English words using the
perl script from the Matt Mahoney’s page3. After
text normalization, the Wiki-Small corpus contains
totally 130 million words, for which we create a
lexicon of 225,909 distinct words appearing more
than 5 times in the corpus. Similarly, the Wiki-
Large corpus contains about 5 billion words, for
which we create a lexicon of 228,069 words ap-
pearing more than 200 times.
For the other two tasks, sentence completion
and name entity recognition, we use the same
training corpora from the previous state-of-the-art
work for fair comparisons. The training corpus for
the sentence completion is the Holmes text (Zweig
and Burges, 2011; Mikolov et al., 2013a). The
training corpus for the name entity recognition
task is the Reuters English newswire from RCV1
(Turian et al., 2010; Lewis et al., 2004). Refer
to section 4.4 and section 4.5 for detailed descrip-
tions respectively.
</bodyText>
<subsubsectionHeader confidence="0.509588">
4.1.2 Semantic constraint collections
</subsubsectionHeader>
<bodyText confidence="0.999846076923077">
In this work, we use WordNet as the resource to
collect ordinal semantic knowledge. WordNet is a
large semantic lexicon database of English words
(Miller, 1995), where nouns, verbs, adjectives and
adverbs are grouped into sets of cognitive syn-
onyms (usually called synsets). Each synset usu-
ally expresses a distinct semantic concept. All
synsets in WordNet are interlinked by means of
conceptual-semantic and/or lexical relations such
as synonyms and antonyms, hypernyms and hy-
ponyms.
In our experiments, we use the version
WordNet-3.1 for creating the corresponding se-
mantic constraints. In detail, we follow the fol-
lowing process to extract semantic similarity in-
equalities from WordNet and Thesaurus:
1. Based on the Synonym Antonym Rule de-
scribed in section 2, for each word in vo-
cabulary, find its synset and use the syn-
onym and antonym relations to find all re-
lated synonymous and antonymous synsets.
Note that the antonymous synset is selected
as long as there exists an antonymous rela-
tion between any word in this synset and any
word in an synonymous synset. After find-
ing the synonymous and antonymous synsets
</bodyText>
<equation confidence="0.927402666666667">
3http://mattmahoney.net/dc/textdata.html
asij sijw(1)
aw(1) i
i |w(1) |2
i
1505
</equation>
<bodyText confidence="0.999945363636364">
of the current word, the similarity inequali-
ties could be generated according to the rank-
ing rule. After processing all words, we
have collected about 30,000 inequalities re-
lated to the synonym and antonym relations.
Furthermore, we extract additional 320,000
inequalities from an old English dictionary
(Fernald, 1896). In total, we have about
345,000 inequalities related to the synonym
and antonym relations. This set of inequali-
ties is denoted as Synon-Anton constraints in
our experiments.
2. Based on the Semantic Category Rule and Se-
mantic Hierarchy Rule, we extract another in-
equality set consisting of 75,000 inequalities
from WordNet. We defined this collection as
Hyper-Hypon constraints in our experiments.
In the following experiments, we just use all of
these collected inequality constraints as is without
further manually checking or cleaning-up. They
may contain a very small percentage of errors or
conflicts (due to multiple senses of a word).
</bodyText>
<subsectionHeader confidence="0.942278">
4.1.3 Training parameter setting
</subsectionHeader>
<bodyText confidence="0.99997856">
Here we describe the control parameters used to
learn the baseline skip-gram model and the pro-
posed SWE model. In our experiments, we use the
open-source word2vec toolkit4 to train the base-
line skip-gram model, where the context window
size is set to be 5. The initial learning rate is set
as 0.025 and the learning rate is decreased lin-
early during the SGD model training process. We
use the popular negative sampling technique to
speed up model training and set the negative sam-
ple number as 5.
To train the proposed SWE model, we use the
same configuration as the skip-gram model to
maximize 2&apos;. For the penalty term in eq. (7), we
set 60 = 0 for the hinge loss function. The seman-
tic similarity between words is computed by the
cosine distance. The combination coefficient Q in
eq. (8) is usually set to be a number between 0.001
and 0.3 in our experiments.
In the following four NLP tasks, the dimension-
ality of embedding vectors is different since we
try to use the same settings from the state-of-the-
art work for the comparison purpose. In the Word
Similarity task and the TOEFL Synonym Selec-
tion task, we followed the state of the art work
</bodyText>
<figure confidence="0.93974875">
4https://code.google.com/p/word2vec.
Semantic Inequality Satisfied Rate Curve
0.00 0.05 0.10 0.15 0.20 0.25 0.30
beta (β)
</figure>
<figureCaption confidence="0.999384">
Figure 3: A curve of inequality satisfied rates (All
</figureCaption>
<bodyText confidence="0.872540090909091">
models trained on the Wiki-Small corpus. Hyper-
Hypon and Synon-Anton stand for different seman-
tic constraint sets employed for training semantic
word embeddings).
in (Xu et al., 2014), to set word embeddings to
300-dimension. Similarly, we refer to Bian et al.
(2014) to set the dimensionality of word vectors to
600 for the Sentence Completion task. And we set
the dimensionality of word vectors to 50 for the
NER task according to (Turian et al., 2010; Pen-
nington et al., 2014).
</bodyText>
<subsectionHeader confidence="0.982956">
4.2 Semantic inequality satisfied rates
</subsectionHeader>
<bodyText confidence="0.999924875">
Here we first examine the inequality satisfied rates
of various word embeddings. The inequality sat-
isfied rate is defined as how many percentage of
semantic inequalities are satisfied based on the un-
derlying word embedding vectors. In Figure 3,
we show a typical curve of the inequality satis-
fied rates as a function of Q used in model train-
ing. This figure is plotted based on the Wiki-Small
corpus. Two semantic constraint sets Synon-Anton
and Hyper-Hypon created in section 4.1.2 are em-
ployed to learn semantic word embeddings.
In the framework of the proposed semantic
word embedding method, we just need to tune
one more parameter Q, comparing with the skip-
gram model. It shows that the baseline skip-gram
(Q = 0) can only satisfy about 50-60% of inequal-
ities in the training set. As we choose a proper
value for Q, we may significantly improve the in-
equality satisfied rate, up to 85-95%. Although
we can get higher inequality satisfying rate on the
training set by increasing beta continuously, how-
ever, we do not suggest to use a big beta value
because it would make the model overfitting. The
major reason for this is that the constraints only
</bodyText>
<figure confidence="0.984750266666667">
Satisfied Rate (%)
100
95
90
85
80
75
70
65
60
55
50
Hyper-Hypon
Synon-Anton
1506
</figure>
<bodyText confidence="0.999285785714286">
cover a subset of words in vocabulary. Increasing
the rate too much may screw up the entire word
embeddings due to the sparsity of the constraints.
Meanwhile, we have found that the proposed
SGD method is very efficient to handle a large
number of inequalities in model training. When
we use the total 345,000 inequalities, the SWE
training is comparable with the baseline skip-gram
model in terms of training speed. In the following,
we continue to examine the SWE model on four
popular natural language processing tasks, includ-
ing word similarity, sentence completion, name
entity recognition and the TOEFL synonym selec-
tion.
</bodyText>
<subsectionHeader confidence="0.9638445">
4.3 Task 1: Word Similarity Task
4.3.1 Task description
</subsectionHeader>
<bodyText confidence="0.9999826875">
Measuring word similarity is a traditional NLP
task (Rubenstein and Goodenough, 1965). Here
we compare several word embedding models on
a popular word similarity task, namely WordSim-
353 (Finkelstein et al., 2001), which contains 353
English word pairs along with human-assigned
similarity scores, which measure the relatedness
of each word pair on a scale from 0 (totally unre-
lated words) to 10 (very much related or identical
words). The final similarity score for each pair is
the average across 13 to 16 human judges. When
evaluating word embeddings on this task, we mea-
sure the performance by calculating the Spearman
rank correlation between the human judgments
and the similarity scores computed based on the
learned word embeddings.
</bodyText>
<subsectionHeader confidence="0.820622">
4.3.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999882125">
Here we compare the proposed SWE model with
the baseline skip-gram model on the WordSim-
353 task. Both word embedding models are
trained using the Wikipedia corpora. We set the di-
mension of word embedding vectors to be 300. In
Table 1, we have shown all the Spearman rank cor-
relation results. The baseline results on this task
include PPMI (Levy and Goldberg, 2014), GloVe
(Pennington et al., 2014), and ESA-Wikipedia
(Gabrilovich and Markovitch, 2007).
From the results in Table 1, we can see that the
proposed SWE model can achieve consistent im-
provements over the baseline skip-gram model, no
matter which training corpus is used. These re-
sults have demonstrated that, by incorporating se-
mantic ordinal knowledge into the word vectors,
</bodyText>
<table confidence="0.999246615384616">
Word Embeddings Result
SPPMI 0.6870
Others GloVe (6 billion) 0.6580
GloVe (42 billion) 0.7590
ESA-Wikipedia 0.7500
Skip-gram 0.6326
Wiki-Small SWE + Synon-Anton 0.6584
(0.13 billion) SWE + Hyper-Hypon 0.6407
SWE + Both 0.6442
Skip-gram 0.7085
Wiki-Large SWE + Synon-Anton 0.7274
(5 billion) SWE + Hyper-Hypon 0.7213
SWE + Both 0.7236
</table>
<tableCaption confidence="0.968976">
Table 1: Spearman results on the WordSim-353
Task.
</tableCaption>
<bodyText confidence="0.999155777777778">
the proposed semantic word embedding frame-
work can capture much better semantics for many
words. The SWE model using the Wiki-Large
corpus has achieved the state-of-the-art perfor-
mance on this task, significantly outperforming
other popular word embedding methods, such as
skip-gram and GloVe. Moreover, we also find that
the Synon-Anton constraint set is more relevant
than Hyper-Hypon for the word similarity task.
</bodyText>
<subsectionHeader confidence="0.9810715">
4.4 Task 2: Sentence Completion Task
4.4.1 Task description
</subsectionHeader>
<bodyText confidence="0.9999245">
The Microsoft sentence completion challenge has
recently been introduced as a standard benchmark
task for language modeling and other NLP tech-
niques (Zweig and Burges, 2011). This task con-
sists of 1040 sentences, each of which misses one
word. The goal is to select a word that is the
most coherent with the rest of the sentence, from
a list of five candidates. Many NLP techniques
have already been reported on this task, includ-
ing N-gram model and LSA-based model pro-
posed in (Zweig and Burges, 2011), log-bilinear
model (Mnih and Teh, 2012), recurrent neural
networks (RNN) (Mikolov, 2012), the skip-gram
model (Mikolov et al., 2013a), a combination of
the skip-gram and RNN model, and a knowledge
enhanced word embedding model proposed by
Bian et. al. (2014). The performance of all these
techniques is listed in Table 2 for comparison.
In this work, we follow the the same proce-
dure as in (Mikolov et al., 2013a) to examine the
performance of our proposed semantic word em-
beddings (SWE) on this task. We first train 600-
</bodyText>
<page confidence="0.539735">
1507
</page>
<bodyText confidence="0.9996272">
dimension word embeddings based on a training
corpus of 50M words provided by (Zweig and
Burges, 2011), with and without using the col-
lected ordinal knowledge. Then, for each sen-
tence in the test set, we use the learned word em-
beddings to compute a sentence score for predict-
ing all surrounding words based on each candidate
word in the list. Finally, we use the computed sen-
tence prediction scores to choose the most likely
word from the given list to answer the question.
</bodyText>
<table confidence="0.999585263157895">
System Acc
N-gram model 39.0
LSA-based model 49.0
Others Log-bilinear model 54.8
RNN 55.4
Skip-gram 48.0
Skip-gram + RNN 58.9
Skip-gram 41.2
Bian et al. + Syntactic knowledge 41.9
+ Semantic knowledge 45.2
+ Both knowledge 44.2
Skip-gram 44.1
SWE + Synon-Anton 47.9
1 Iteration SWE + Hyper-Hypon 47.5
SWE + Both 48.3
Skip-gram 51.5
SWE + Synon-Anton 55.7
5 Iterations SWE + Hyper-Hypon 55.4
SWE + Both 56.2
</table>
<tableCaption confidence="0.999335">
Table 2: Results on Sentence Completion Task.
</tableCaption>
<subsectionHeader confidence="0.874217">
4.4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999969583333333">
In Table 2, we have shown the sentence comple-
tion accuracy on this task for various word em-
bedding models. We can see that the proposed
SWE model has achieved considerable improve-
ments over the baseline skip-gram model. Once
again, this suggests that the semantic knowledge
represented by the ordinal inequalities can signif-
icantly improve the quality of the word embed-
dings. Besides, the SWE model significantly out-
performs the recent work in (Bian et al., 2014),
which considers syntactics and semantics of the
sentence contexts.
</bodyText>
<subsectionHeader confidence="0.9729175">
4.5 Task 3: Name Entity Recognition
4.5.1 Task description
</subsectionHeader>
<bodyText confidence="0.999901">
To further investigate the performance of seman-
tic word embeddings, we have further conducted
some experiments on the standard CoNLL03
name entity recognition (NER) task. The
CoNLL03 NER dataset is drawn from the Reuters
newswire. The training set contains 204K words
(14K sentences, 946 documents), the test set
contains 46K words (3.5K sentences, 231 doc-
uments), and the development set contains 51K
words (3.3K sentences, 216 documents). We have
listed the state-of-the-art performance in Table 3
for this task (Turian et al., 2010).
To make a fair comparison, we have used the
exactly same experimental configurations as in
(Turian et al., 2010), including the used training
algorithm, the baseline discrete features and so on.
Like the C&amp;W model, we use the same training
text resource to learn word vectors, which contains
one year of Reuters English newswire from RCV1,
from August 1996 to August 1997, having about
810,000 news stories (Lewis et al., 2004). Mean-
while, the dimension of word embeddings is set to
50 for all experiments on this task.
</bodyText>
<subsectionHeader confidence="0.719833">
4.5.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.9981925">
In our experiments, we compare the proposed
SWE model with the baseline skip-gram model for
name entity recognition, measured by the standard
F1 scores. We present the final NER F1 scores
on the CoNLL03 NER task in Table 3. The nota-
tion “Gaz” stands for gazetteers that are added into
the NER system as an auxiliary feature. For the
SWE model, we experiment two configurations by
adding gazetteers or not (denoted by “IsGaz” and
“NoGaz” respectively).
</bodyText>
<table confidence="0.999821727272727">
System Dev Test MUC7
Others C&amp;W 92.3 87.9 75.7
C&amp;W + Gaz 93.0 88.9 81.4
Skip-gram 92.6 88.3 76.7
NoGaz + Synon-Anton 92.5 88.4 77.2
+ Hyper-Hypon 92.6 88.6 77.7
+ Both 92.6 88.4 77.5
Skip-gram 93.3 89.5 80.0
IsGaz + Synon-Anton 93.1 89.6 80.7
+ Hyper-Hypon 93.1 89.7 80.7
+ Both 93.0 89.5 80.8
</table>
<tableCaption confidence="0.999639">
Table 3: F1 scores on the CoNLL03 NER task.
</tableCaption>
<bodyText confidence="0.956753416666667">
From the results shown in Table 3, we could find
the proposed semantic word embedding (SWE)
model can consistently achieve 0.8% (or more) ab-
solute improvements on the MUC7 task no mat-
1508
ter whether the gazetteers features are used or not.
The proposed SWE model can also obtain 0.3%
improvement in the CoNLL03 test set when no
gazetteers is added into the NER system. How-
ever, no significant improvement is observed in
this test set for the proposed SWE model after we
add the gazetteers feature.
</bodyText>
<subsectionHeader confidence="0.856336">
4.6 Task 4: TOEFL Synonym Selection
4.6.1 Task description
</subsectionHeader>
<bodyText confidence="0.99994325">
The goal of a synonym selection task is to se-
lect, from a list of candidate words, the semanti-
cally closest word for each given target word. The
dataset we use for this task is the standard TOEFL
dataset (Landauer and Dumais, 1997), which con-
tains 80 questions. Each question consists of a tar-
get word along with 4 candidate lexical substitutes
for selection.
The evaluation criterion on this task is the
synonym selection accuracy which indicates how
many synonyms are correctly selected for all 80
questions. Similar to the configurations on the
word similarity task, all the experiments on this
task are conducted on the English Wikipedia cor-
pora. In our experiments, we set all the vector di-
mensions to 300.
</bodyText>
<subsubsectionHeader confidence="0.439789">
4.6.2 Experimental Results
</subsubsectionHeader>
<table confidence="0.999471888888889">
Corpus Model Accuracy (%)
Skip-gram 61.25
Wiki-Small + Synon-Anton 70.00
+ Hyper-Hypon 66.25
+ Both 71.25
Skip-gram 83.75
Wiki-Large + Synon-Anton 87.50
+ Hyper-Hypon 85.00
+ Both 88.75
</table>
<tableCaption confidence="0.999794">
Table 4: The TOEFL synonym selection task.
</tableCaption>
<bodyText confidence="0.9999844375">
In Table 4, we have shown the experimen-
tal results for different word embedding models,
learned from different Wikipedia corpora: Wiki-
Small or Wiki-Large. We compare the proposed
SWE with the baseline skip-gram model. From
the experimental results in Table 4, we can see that
the proposed SWE model can achieve consistent
improvements over the baseline skip-gram model
on the TOEFL synonym selection task, about 5-
8% improvements on the selection accuracy. We
find the similar performance differences between
the SWE model trained with the Synon-Anton and
Hyper-Hypon constraint set. The main reason
would be that the synonym selection task is mainly
related to lexical level similarity and less relevant
to the hypernym-hyponym relations.
</bodyText>
<sectionHeader confidence="0.997827" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999989133333333">
Word embedding models with good semantic rep-
resentations are quite invaluable to many natu-
ral language processing tasks. However, the cur-
rent data-driven methods that learn word vectors
from corpora based on the distributional hypoth-
esis tend to suffer from some major limitations.
In this paper, we propose a general and flexible
framework to incorporate various types of seman-
tic knowledge into the popular data-driven learn-
ing procedure for word embeddings. Our main
contributions are to represent semantic knowledge
as a number of ordinal similarity inequalities as
well as to formulate the entire learning process as
a constrained optimization problem. Meanwhile,
the optimization problem could be solved by effi-
cient stochastic gradient descend algorithm. Ex-
perimental results on four popular NLP tasks have
all demonstrated that the propose semantic word
embedding framework can significantly improve
the quality of word representations.
As for the future work, we would incorpo-
rate more types of knowledge, such as knowledge
graphs and FrameNet, into the learning process
for more powerful word representations. We also
expect that some common sense related semantic
knowledge may be generated as ordinal inequality
constraints by human annotators for learning se-
mantic word embeddings. At the end, we plan to
apply the SWE word embedding models for more
natural language processing tasks.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999453">
This work was supported in part by the Science
and Technology Development of Anhui Province,
China (Grants No. 2014z02006) and the Funda-
mental Research Funds for the Central Universi-
ties. Special thanks to Zhigang Chen, Ruiji Fu and
the anonymous reviewers for their insightful com-
ments as well as suggestions. The authors also
want to thank Profs. Wu Guo and Li-Rong Dai
for their wonderful help and supports during the
experiments.
</bodyText>
<page confidence="0.773019">
1509
</page>
<sectionHeader confidence="0.995058" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999336514925373">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Machine Learning and Knowledge Discov-
ery in Databases, pages 132–148. Springer.
Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,
Si Wei, Xiaodan Zhu, and Hui Jiang. 2015. Revis-
iting word embedding for contrasting meaning. In
Proceedings of ACL.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL,
pages 1370–1380.
Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2014.
Retrofitting word vectors to semantic lexicons. In
Proceedings of the NIPS Deep learning and repre-
sentation learning workshop.
James Champlin Fernald. 1896. English synonyms and
antonyms. Funk &amp; Wagnalls Company.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of WWW, pages
406–414. ACM.
Ronald A Fisher. 1936. The use of multiple measure-
ments in taxonomic problems. Annals of eugenics,
7(2):179–188.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
IJCAI, volume 7, pages 1606–1611.
Zellig S Harris. 1954. Distributional structure. Word,
10(23):146–162.
Geoffrey E Hinton, James L McClelland, and David E
Rumelhart. 1986. Distributed representations. In
Parallel distributed processing: Explorations in the
microstructure of cognition. Volume 1: Foundations,
pages 77–109. MIT Press.
Dan Jurafsky and James H Martin. 2000. Speech &amp;
language processing. Pearson Education India.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Pro-
ceedings of NIPS, pages 2177–2185.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. RCV1: A new benchmark collection
for text categorization research. Journal of Machine
Learning Research, 5:361–397.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.
Tom´aˇs Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Ph. D. the-
sis, Brno University of Technology.
George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
cognitive processes, 6(1):1–28.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of ICML.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. Proceedings of EMNLP, 12:1532–
1543.
Tim Rockt¨aschel, Matko Boˇsnjak, Sameer Singh, and
Sebastian Riedel. 2014. Low-dimensional embed-
dings of logic. In Proceedings of the ACL 2014
Workshop on Semantic Parsing, pages 45–49, Bal-
timore, MD, June. Association for Computational
Linguistics.
Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
1510
Holger Schwenk. 2007. Continuous space language
models. Computer Speech &amp; Language, 21(3):492–
518.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384–394. Association for Computa-
tional Linguistics.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-
net: A general framework for incorporating knowl-
edge into word representations. In Proceedings of
CIKM, pages 1219–1228. ACM.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings
of ACL, volume 2, pages 545–550.
Geoffrey Zweig and Christopher JC Burges. 2011. The
microsoft research sentence completion challenge.
Technical report, Technical Report MSR-TR-2011-
129, Microsoft.
Geoffrey Zweig. 2014. Explicit representation of
antonymy in language modelling. Technical report,
Technical Report MSR-TR-2014-52, Microsoft.
</reference>
<page confidence="0.752645">
1511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.321629">
<title confidence="0.992137">Learning Semantic Word Embeddings based Ordinal Knowledge Constraints</title>
<affiliation confidence="0.877476">Engineering Laboratory for Speech and Language Information University of Science and Technology of China, Hefei, of Electrical Engineering and Computer Science, York University,</affiliation>
<address confidence="0.978261">Research, Hefei,</address>
<email confidence="0.961047">emails:quanliu@mail.ustc.edu.cn,siwei@iflytek.com,zhling@ustc.edu.cn,yuhu@iflytek.com</email>
<abstract confidence="0.999323862068966">In this paper, we propose a general framework to incorporate semantic knowledge into the popular data-driven learning process of word embeddings to improve the quality of them. Under this framework, we represent semantic knowledge as many ordinal ranking inequalities and formulate the learning of semantic word embeddings (SWE) as a constrained optimization problem, where the data-derived objective function is optimized subject to all ordinal knowledge inequality constraints extracted from available knowledge resources such as Thesaurus and Word- Net. We have demonstrated that this constrained optimization problem can be efficiently solved by the stochastic gradient descent (SGD) algorithm, even for a large number of inequality constraints. Experimental results on four standard NLP tasks, including word similarity measure, sentence completion, name entity recognition, and the TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be significantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1997" citStr="Bengio et al., 2003" startWordPosition="289" endWordPosition="293">y recognition, and the TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be significantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings. 1 Introduction Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been prop</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Knowledge-powered deep learning for word embedding.</title>
<date>2014</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>132--148</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3951" citStr="Bian et al., 2014" startWordPosition="596" endWordPosition="599">. Moreover, as word representations are mainly learned based on the co-occurrence information, the learned word embeddings do not capture the accurate relationship between two semantically similar words if either one appears less frequently in the corpus. To address these issues, some recent work has been proposed to incorporate prior lexical knowledge (WordNet, PPDB, etc.) or knowledge graph (Freebase, etc.) into word representations. Such knowledge enhanced word embedding methods have achieved considerable improvements on various natural language processing tasks, like (Yu and Dredze, 2014; Bian et al., 2014; Xu et al., 2014). These methods attempt to increase the semantic similarities between words belonging to 1501 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1501–1511, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics one semantic category or to explicitly model the semantic relationships between different words. For example, Yu and Dredze (2014) have proposed a new learning objective function to enhance word embeddings by combining neural</context>
<context position="22012" citStr="Bian et al. (2014)" startWordPosition="3553" endWordPosition="3556"> same settings from the state-of-theart work for the comparison purpose. In the Word Similarity task and the TOEFL Synonym Selection task, we followed the state of the art work 4https://code.google.com/p/word2vec. Semantic Inequality Satisfied Rate Curve 0.00 0.05 0.10 0.15 0.20 0.25 0.30 beta (β) Figure 3: A curve of inequality satisfied rates (All models trained on the Wiki-Small corpus. HyperHypon and Synon-Anton stand for different semantic constraint sets employed for training semantic word embeddings). in (Xu et al., 2014), to set word embeddings to 300-dimension. Similarly, we refer to Bian et al. (2014) to set the dimensionality of word vectors to 600 for the Sentence Completion task. And we set the dimensionality of word vectors to 50 for the NER task according to (Turian et al., 2010; Pennington et al., 2014). 4.2 Semantic inequality satisfied rates Here we first examine the inequality satisfied rates of various word embeddings. The inequality satisfied rate is defined as how many percentage of semantic inequalities are satisfied based on the underlying word embedding vectors. In Figure 3, we show a typical curve of the inequality satisfied rates as a function of Q used in model training. </context>
<context position="29000" citStr="Bian et al., 2014" startWordPosition="4711" endWordPosition="4714">m 51.5 SWE + Synon-Anton 55.7 5 Iterations SWE + Hyper-Hypon 55.4 SWE + Both 56.2 Table 2: Results on Sentence Completion Task. 4.4.2 Experimental results In Table 2, we have shown the sentence completion accuracy on this task for various word embedding models. We can see that the proposed SWE model has achieved considerable improvements over the baseline skip-gram model. Once again, this suggests that the semantic knowledge represented by the ordinal inequalities can significantly improve the quality of the word embeddings. Besides, the SWE model significantly outperforms the recent work in (Bian et al., 2014), which considers syntactics and semantics of the sentence contexts. 4.5 Task 3: Name Entity Recognition 4.5.1 Task description To further investigate the performance of semantic word embeddings, we have further conducted some experiments on the standard CoNLL03 name entity recognition (NER) task. The CoNLL03 NER dataset is drawn from the Reuters newswire. The training set contains 204K words (14K sentences, 946 documents), the test set contains 46K words (3.5K sentences, 231 documents), and the development set contains 51K words (3.3K sentences, 216 documents). We have listed the state-of-the</context>
</contexts>
<marker>Bian, Gao, Liu, 2014</marker>
<rawString>Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Knowledge-powered deep learning for word embedding. In Machine Learning and Knowledge Discovery in Databases, pages 132–148. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhigang Chen</author>
<author>Wei Lin</author>
<author>Qian Chen</author>
<author>Xiaoping Chen</author>
<author>Si Wei</author>
<author>Xiaodan Zhu</author>
<author>Hui Jiang</author>
</authors>
<title>Revisiting word embedding for contrasting meaning.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2202" citStr="Chen et al., 2015" startWordPosition="323" endWordPosition="326">s during the learning process of word embeddings. 1 Introduction Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurrence statistics to model contextual distributions from very large te</context>
</contexts>
<marker>Chen, Lin, Chen, Chen, Wei, Zhu, Jiang, 2015</marker>
<rawString>Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen, Si Wei, Xiaodan Zhu, and Hui Jiang. 2015. Revisiting word embedding for contrasting meaning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2067" citStr="Collobert and Weston, 2008" startWordPosition="300" endWordPosition="303">trated that the quality of learned word vectors can be significantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings. 1 Introduction Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn wo</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2092" citStr="Collobert et al., 2011" startWordPosition="304" endWordPosition="308">earned word vectors can be significantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings. 1 Introduction Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These meth</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1370--1380</pages>
<contexts>
<context position="2134" citStr="Devlin et al., 2014" startWordPosition="311" endWordPosition="314">ved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings. 1 Introduction Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurr</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of ACL, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Jesse Dodge</author>
<author>Sujay K Jauhar</author>
<author>Chris Dyer</author>
<author>Eduard Hovy</author>
<author>Noah A Smith</author>
</authors>
<title>Retrofitting word vectors to semantic lexicons.</title>
<date>2014</date>
<booktitle>In Proceedings of the NIPS Deep</booktitle>
<contexts>
<context position="5127" citStr="Faruqui et al., 2014" startWordPosition="769" endWordPosition="772">o enhance word embeddings by combining neural models and a prior knowledge measure from semantic resources. Bian et. al (2014) have recently proposed to leverage morphological, syntactic, and semantic knowledge to improve the learning of word embeddings. Besides, a novel framework has been proposed in (Xu et al., 2014) to take advantage of both relational and categorical knowledge to learn high-quality word representations, where two regularization functions are used to model the relational and categorical knowledge respectively. More recently, a retrofitting technique has been introduced in (Faruqui et al., 2014) to improve semantic vectors by leveraging lexicon-derived relational information in a post-processing stage. In this paper, we propose a new and flexible method to incorporate semantic knowledge into the corpus-based learning of word embeddings. In our approach, we propose to represent semantic knowledge as many word ordinal ranking inequalities. Furthermore, these inequalities are cast as semantic constraints in the optimization process to learn semantically sensible word embeddings. The proposed method has several advantages. Firstly, many different types of semantic knowledge can all be re</context>
<context position="14104" citStr="Faruqui et al., 2014" startWordPosition="2253" endWordPosition="2256">ution of the penalty term in the optimization process. It balances between the semantic information estimated from the corpus based on the distributional hypothesis and the semantic knowledge encoded in the ordinal ranking inequalities. In Rockt¨aschel et al. (2014), a similar approach was proposed to capture knowledge constraint as extra terms in the objective function for optimization. In Figure 2, we show a diagram for the the overall SWE learning framework to incorporate semantic knowledge into the basic skip-gram word embeddings. Comparing with the previous work in (Xu et al., 2014) and (Faruqui et al., 2014), the proposed SWE framework is more general in terms of encoding the semantic knowledge for learning word embeddings. It is straightforward to show that the work in (Xu et al., 2014; Zweig, 2014; Faruqui et al., 2014) can be viewed as some special cases under our SWE learning framework. 3.3 Optimization algorithm for SWE In this work, the proposed semantic word embeddings (SWE) are learned using the standard mini-batch stochastic gradient descent (SGD) algorithm. Furthermore, we adopt to use the cosine distance of the embedding vectors to compute the similarity between two words in the penalt</context>
</contexts>
<marker>Faruqui, Dodge, Jauhar, Dyer, Hovy, Smith, 2014</marker>
<rawString>Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris Dyer, Eduard Hovy, and Noah A Smith. 2014. Retrofitting word vectors to semantic lexicons. In Proceedings of the NIPS Deep learning and representation learning workshop.</rawString>
</citation>
<citation valid="false">
<title>English synonyms and antonyms. Funk &amp;</title>
<publisher>Wagnalls Company.</publisher>
<marker></marker>
<rawString>James Champlin Fernald. 1896. English synonyms and antonyms. Funk &amp; Wagnalls Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>406--414</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24397" citStr="Finkelstein et al., 2001" startWordPosition="3956" endWordPosition="3959">raining. When we use the total 345,000 inequalities, the SWE training is comparable with the baseline skip-gram model in terms of training speed. In the following, we continue to examine the SWE model on four popular natural language processing tasks, including word similarity, sentence completion, name entity recognition and the TOEFL synonym selection. 4.3 Task 1: Word Similarity Task 4.3.1 Task description Measuring word similarity is a traditional NLP task (Rubenstein and Goodenough, 1965). Here we compare several word embedding models on a popular word similarity task, namely WordSim353 (Finkelstein et al., 2001), which contains 353 English word pairs along with human-assigned similarity scores, which measure the relatedness of each word pair on a scale from 0 (totally unrelated words) to 10 (very much related or identical words). The final similarity score for each pair is the average across 13 to 16 human judges. When evaluating word embeddings on this task, we measure the performance by calculating the Spearman rank correlation between the human judgments and the similarity scores computed based on the learned word embeddings. 4.3.2 Experimental results Here we compare the proposed SWE model with t</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of WWW, pages 406–414. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald A Fisher</author>
</authors>
<title>The use of multiple measurements in taxonomic problems.</title>
<date>1936</date>
<journal>Annals of eugenics,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="8613" citStr="Fisher, 1936" startWordPosition="1328" endWordPosition="1329">ordNet. • Synonym Antonym Rule: Similarities between a word and its synonymous words are always larger than similarities between the word and its antonymous words. For example, the similarity between foolish and stupid is expected to be bigger than the similarity between foolish and clever, i.e., similarity(foolish, stupid) &gt; similarity(foolish, clever). • Semantic Category Rule: Similarities of words that belong to the same semantic category would be larger than similarities of words that belong to different categories. This rule refers to the idea of Fisher linear discriminant algorithm in (Fisher, 1936). A semantic category may be defined as a synset in WordNet, a hypernym in a semantic hierarchy, or an entity category in 1502 Tool model aims to maximize the following objective Input Word function: Hyponyms Hammer Chisel Saw Co-Hyponyms Mallet Plessor Hacksaw Jigsaw −c≤j≤c,j�=0 M Hypernym log p(wt+j|wt) (3) 1 Q = T T t=1 Figure 1: An example of hyponym and hypernym. knowledge graphs. Figure 1 shows a simple example of the relationship between hyponyms and hypernyms. From there, it is reasonable to assume the following similarity inequality: similarity(Mallet, Plessor) &gt; similarity(Mallet, Ha</context>
</contexts>
<marker>Fisher, 1936</marker>
<rawString>Ronald A Fisher. 1936. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7(2):179–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<volume>7</volume>
<pages>1606--1611</pages>
<contexts>
<context position="25403" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="4119" endWordPosition="4122">formance by calculating the Spearman rank correlation between the human judgments and the similarity scores computed based on the learned word embeddings. 4.3.2 Experimental results Here we compare the proposed SWE model with the baseline skip-gram model on the WordSim353 task. Both word embedding models are trained using the Wikipedia corpora. We set the dimension of word embedding vectors to be 300. In Table 1, we have shown all the Spearman rank correlation results. The baseline results on this task include PPMI (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014), and ESA-Wikipedia (Gabrilovich and Markovitch, 2007). From the results in Table 1, we can see that the proposed SWE model can achieve consistent improvements over the baseline skip-gram model, no matter which training corpus is used. These results have demonstrated that, by incorporating semantic ordinal knowledge into the word vectors, Word Embeddings Result SPPMI 0.6870 Others GloVe (6 billion) 0.6580 GloVe (42 billion) 0.7590 ESA-Wikipedia 0.7500 Skip-gram 0.6326 Wiki-Small SWE + Synon-Anton 0.6584 (0.13 billion) SWE + Hyper-Hypon 0.6407 SWE + Both 0.6442 Skip-gram 0.7085 Wiki-Large SWE + Synon-Anton 0.7274 (5 billion) SWE + Hyper-Hypon 0.72</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In Proceedings of IJCAI, volume 7, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="2292" citStr="Harris, 1954" startWordPosition="337" endWordPosition="338"> (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurrence statistics to model contextual distributions from very large text corpora and they have been demonstrated to be quite effective in a number of NLP tasks.</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>James L McClelland</author>
<author>David E Rumelhart</author>
</authors>
<title>Distributed representations.</title>
<date>1986</date>
<booktitle>In Parallel distributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,</booktitle>
<pages>77--109</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1858" citStr="Hinton et al., 1986" startWordPosition="263" endWordPosition="267">inequality constraints. Experimental results on four standard NLP tasks, including word similarity measure, sentence completion, name entity recognition, and the TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be significantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings. 1 Introduction Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-g</context>
</contexts>
<marker>Hinton, McClelland, Rumelhart, 1986</marker>
<rawString>Geoffrey E Hinton, James L McClelland, and David E Rumelhart. 1986. Distributed representations. In Parallel distributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations, pages 77–109. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech &amp; language processing.</title>
<date>2000</date>
<publisher>Pearson Education</publisher>
<contexts>
<context position="7574" citStr="Jurafsky and Martin, 2000" startWordPosition="1151" endWordPosition="1154">here wi, wj and wk denote any three words in vocabulary. For example, eq.(1) holds if wj is a synonym of wi and wk is an antonym of wi. In general, the similarity between a word and its synonymous word should be larger than the similarity between the word and its antonymous word. Moreover, a particular word should be more similar to the words belonging to the same semantic category as this word than other words belonging to a different category. Besides, eq.(1) holds if wi and wj have shorter distance in a semantic hierarchy than wi and wk do in the same hierarchy (Leacock and Chodorow, 1998; Jurafsky and Martin, 2000). Equivalently, each of the above similarity inequalities may be represented as the following constraint in the embedding space: sim(w(1) i , w(1) j ) &gt; sim(w(1) i , w(1) k ) (2) where w(1) i , w(1) j and w(1) k denote the embedding vectors of the words, wi, wj and wk. In this paper, we use the following three rules to gather the ordinal semantic knowledge from available lexical knowledge resources, such as Thesaurus and WordNet. • Synonym Antonym Rule: Similarities between a word and its synonymous words are always larger than similarities between the word and its antonymous words. For exampl</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Dan Jurafsky and James H Martin. 2000. Speech &amp; language processing. Pearson Education India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="31791" citStr="Landauer and Dumais, 1997" startWordPosition="5182" endWordPosition="5185">the MUC7 task no mat1508 ter whether the gazetteers features are used or not. The proposed SWE model can also obtain 0.3% improvement in the CoNLL03 test set when no gazetteers is added into the NER system. However, no significant improvement is observed in this test set for the proposed SWE model after we add the gazetteers feature. 4.6 Task 4: TOEFL Synonym Selection 4.6.1 Task description The goal of a synonym selection task is to select, from a list of candidate words, the semantically closest word for each given target word. The dataset we use for this task is the standard TOEFL dataset (Landauer and Dumais, 1997), which contains 80 questions. Each question consists of a target word along with 4 candidate lexical substitutes for selection. The evaluation criterion on this task is the synonym selection accuracy which indicates how many synonyms are correctly selected for all 80 questions. Similar to the configurations on the word similarity task, all the experiments on this task are conducted on the English Wikipedia corpora. In our experiments, we set all the vector dimensions to 300. 4.6.2 Experimental Results Corpus Model Accuracy (%) Skip-gram 61.25 Wiki-Small + Synon-Anton 70.00 + Hyper-Hypon 66.25</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="7546" citStr="Leacock and Chodorow, 1998" startWordPosition="1146" endWordPosition="1150">) &gt; similarity(wi, wk) (1) where wi, wj and wk denote any three words in vocabulary. For example, eq.(1) holds if wj is a synonym of wi and wk is an antonym of wi. In general, the similarity between a word and its synonymous word should be larger than the similarity between the word and its antonymous word. Moreover, a particular word should be more similar to the words belonging to the same semantic category as this word than other words belonging to a different category. Besides, eq.(1) holds if wi and wj have shorter distance in a semantic hierarchy than wi and wk do in the same hierarchy (Leacock and Chodorow, 1998; Jurafsky and Martin, 2000). Equivalently, each of the above similarity inequalities may be represented as the following constraint in the embedding space: sim(w(1) i , w(1) j ) &gt; sim(w(1) i , w(1) k ) (2) where w(1) i , w(1) j and w(1) k denote the embedding vectors of the words, wi, wj and wk. In this paper, we use the following three rules to gather the ordinal semantic knowledge from available lexical knowledge resources, such as Thesaurus and WordNet. • Synonym Antonym Rule: Similarities between a word and its synonymous words are always larger than similarities between the word and its </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="2539" citStr="Levy and Goldberg, 2014" startWordPosition="376" endWordPosition="379">ly used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurrence statistics to model contextual distributions from very large text corpora and they have been demonstrated to be quite effective in a number of NLP tasks. However, they still suffer from some major limitations. In particular, these corpus-based methods usually fail to capture the precise meanings for many words. For example, some semantically related but dissimilar words may have similar contexts, </context>
<context position="25316" citStr="Levy and Goldberg, 2014" startWordPosition="4108" endWordPosition="4111">human judges. When evaluating word embeddings on this task, we measure the performance by calculating the Spearman rank correlation between the human judgments and the similarity scores computed based on the learned word embeddings. 4.3.2 Experimental results Here we compare the proposed SWE model with the baseline skip-gram model on the WordSim353 task. Both word embedding models are trained using the Wikipedia corpora. We set the dimension of word embedding vectors to be 300. In Table 1, we have shown all the Spearman rank correlation results. The baseline results on this task include PPMI (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014), and ESA-Wikipedia (Gabrilovich and Markovitch, 2007). From the results in Table 1, we can see that the proposed SWE model can achieve consistent improvements over the baseline skip-gram model, no matter which training corpus is used. These results have demonstrated that, by incorporating semantic ordinal knowledge into the word vectors, Word Embeddings Result SPPMI 0.6870 Others GloVe (6 billion) 0.6580 GloVe (42 billion) 0.7590 ESA-Wikipedia 0.7500 Skip-gram 0.6326 Wiki-Small SWE + Synon-Anton 0.6584 (0.13 billion) SWE + Hyper-Hypon 0.6407 SWE + Both 0.6442 </context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In Proceedings of NIPS, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="18110" citStr="Lewis et al., 2004" startWordPosition="2920" endWordPosition="2923">ct words appearing more than 5 times in the corpus. Similarly, the WikiLarge corpus contains about 5 billion words, for which we create a lexicon of 228,069 words appearing more than 200 times. For the other two tasks, sentence completion and name entity recognition, we use the same training corpora from the previous state-of-the-art work for fair comparisons. The training corpus for the sentence completion is the Holmes text (Zweig and Burges, 2011; Mikolov et al., 2013a). The training corpus for the name entity recognition task is the Reuters English newswire from RCV1 (Turian et al., 2010; Lewis et al., 2004). Refer to section 4.4 and section 4.5 for detailed descriptions respectively. 4.1.2 Semantic constraint collections In this work, we use WordNet as the resource to collect ordinal semantic knowledge. WordNet is a large semantic lexicon database of English words (Miller, 1995), where nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (usually called synsets). Each synset usually expresses a distinct semantic concept. All synsets in WordNet are interlinked by means of conceptual-semantic and/or lexical relations such as synonyms and antonyms, hypernyms and hyponyms</context>
<context position="30092" citStr="Lewis et al., 2004" startWordPosition="4883" endWordPosition="4886">es, 231 documents), and the development set contains 51K words (3.3K sentences, 216 documents). We have listed the state-of-the-art performance in Table 3 for this task (Turian et al., 2010). To make a fair comparison, we have used the exactly same experimental configurations as in (Turian et al., 2010), including the used training algorithm, the baseline discrete features and so on. Like the C&amp;W model, we use the same training text resource to learn word vectors, which contains one year of Reuters English newswire from RCV1, from August 1996 to August 1997, having about 810,000 news stories (Lewis et al., 2004). Meanwhile, the dimension of word embeddings is set to 50 for all experiments on this task. 4.5.2 Experimental results In our experiments, we compare the proposed SWE model with the baseline skip-gram model for name entity recognition, measured by the standard F1 scores. We present the final NER F1 scores on the CoNLL03 NER task in Table 3. The notation “Gaz” stands for gazetteers that are added into the NER system as an auxiliary feature. For the SWE model, we experiment two configurations by adding gazetteers or not (denoted by “IsGaz” and “NoGaz” respectively). System Dev Test MUC7 Others </context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. 2004. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="2489" citStr="Mikolov et al., 2013" startWordPosition="368" endWordPosition="371"> al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurrence statistics to model contextual distributions from very large text corpora and they have been demonstrated to be quite effective in a number of NLP tasks. However, they still suffer from some major limitations. In particular, these corpus-based methods usually fail to capture the precise meanings for many words. For example, some semantically relate</context>
<context position="9897" citStr="Mikolov et al., 2013" startWordPosition="1530" endWordPosition="1533">at have shorter distances in a semantic hierarchy should be larger than similarities of words that have longer distances. In this work, the semantic hierarchy refers to the hypernym and hysim(wt,w &gt; simwj,wk) She Embedng ponym structure in WordNet. From FigW(1 ure 1, this rule may suggest several inequalities like: similarity(Mallet, Hammer) &gt; similarity(Mallet, Tool). In addition, we may generate many such semantically ranking similarity inequalities by human annotation through crowdsourcing. 3 Semantic Word Embedding In this section, we first briefly review the conventional skip-gram model (Mikolov et al., 2013b). Next, we study how to incorporate the ordinal similarity inequalities to learn semantic word embeddings. 3.1 The skip-gram model The skip-gram model is a recently proposed learning framework (Mikolov et al., 2013b; Mikolov et al., 2013a) to learn continuous word vectors from text corpora based on the aforementioned distributional hypothesis, where each word in vocabulary (size of V ) is mapped to a continuous embedding space by looking up an embedding matrix W(1). And W(1) is learned by maximizing the prediction probability, calculated by another prediction matrix W(2), of its neighbouring</context>
<context position="17966" citStr="Mikolov et al., 2013" startWordPosition="2896" endWordPosition="2899">oney’s page3. After text normalization, the Wiki-Small corpus contains totally 130 million words, for which we create a lexicon of 225,909 distinct words appearing more than 5 times in the corpus. Similarly, the WikiLarge corpus contains about 5 billion words, for which we create a lexicon of 228,069 words appearing more than 200 times. For the other two tasks, sentence completion and name entity recognition, we use the same training corpora from the previous state-of-the-art work for fair comparisons. The training corpus for the sentence completion is the Holmes text (Zweig and Burges, 2011; Mikolov et al., 2013a). The training corpus for the name entity recognition task is the Reuters English newswire from RCV1 (Turian et al., 2010; Lewis et al., 2004). Refer to section 4.4 and section 4.5 for detailed descriptions respectively. 4.1.2 Semantic constraint collections In this work, we use WordNet as the resource to collect ordinal semantic knowledge. WordNet is a large semantic lexicon database of English words (Miller, 1995), where nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (usually called synsets). Each synset usually expresses a distinct semantic concept. All s</context>
<context position="27187" citStr="Mikolov et al., 2013" startWordPosition="4401" endWordPosition="4404">entence completion challenge has recently been introduced as a standard benchmark task for language modeling and other NLP techniques (Zweig and Burges, 2011). This task consists of 1040 sentences, each of which misses one word. The goal is to select a word that is the most coherent with the rest of the sentence, from a list of five candidates. Many NLP techniques have already been reported on this task, including N-gram model and LSA-based model proposed in (Zweig and Burges, 2011), log-bilinear model (Mnih and Teh, 2012), recurrent neural networks (RNN) (Mikolov, 2012), the skip-gram model (Mikolov et al., 2013a), a combination of the skip-gram and RNN model, and a knowledge enhanced word embedding model proposed by Bian et. al. (2014). The performance of all these techniques is listed in Table 2 for comparison. In this work, we follow the the same procedure as in (Mikolov et al., 2013a) to examine the performance of our proposed semantic word embeddings (SWE) on this task. We first train 600- 1507 dimension word embeddings based on a training corpus of 50M words provided by (Zweig and Burges, 2011), with and without using the collected ordinal knowledge. Then, for each sentence in the test set, we </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2489" citStr="Mikolov et al., 2013" startWordPosition="368" endWordPosition="371"> al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurrence statistics to model contextual distributions from very large text corpora and they have been demonstrated to be quite effective in a number of NLP tasks. However, they still suffer from some major limitations. In particular, these corpus-based methods usually fail to capture the precise meanings for many words. For example, some semantically relate</context>
<context position="9897" citStr="Mikolov et al., 2013" startWordPosition="1530" endWordPosition="1533">at have shorter distances in a semantic hierarchy should be larger than similarities of words that have longer distances. In this work, the semantic hierarchy refers to the hypernym and hysim(wt,w &gt; simwj,wk) She Embedng ponym structure in WordNet. From FigW(1 ure 1, this rule may suggest several inequalities like: similarity(Mallet, Hammer) &gt; similarity(Mallet, Tool). In addition, we may generate many such semantically ranking similarity inequalities by human annotation through crowdsourcing. 3 Semantic Word Embedding In this section, we first briefly review the conventional skip-gram model (Mikolov et al., 2013b). Next, we study how to incorporate the ordinal similarity inequalities to learn semantic word embeddings. 3.1 The skip-gram model The skip-gram model is a recently proposed learning framework (Mikolov et al., 2013b; Mikolov et al., 2013a) to learn continuous word vectors from text corpora based on the aforementioned distributional hypothesis, where each word in vocabulary (size of V ) is mapped to a continuous embedding space by looking up an embedding matrix W(1). And W(1) is learned by maximizing the prediction probability, calculated by another prediction matrix W(2), of its neighbouring</context>
<context position="17966" citStr="Mikolov et al., 2013" startWordPosition="2896" endWordPosition="2899">oney’s page3. After text normalization, the Wiki-Small corpus contains totally 130 million words, for which we create a lexicon of 225,909 distinct words appearing more than 5 times in the corpus. Similarly, the WikiLarge corpus contains about 5 billion words, for which we create a lexicon of 228,069 words appearing more than 200 times. For the other two tasks, sentence completion and name entity recognition, we use the same training corpora from the previous state-of-the-art work for fair comparisons. The training corpus for the sentence completion is the Holmes text (Zweig and Burges, 2011; Mikolov et al., 2013a). The training corpus for the name entity recognition task is the Reuters English newswire from RCV1 (Turian et al., 2010; Lewis et al., 2004). Refer to section 4.4 and section 4.5 for detailed descriptions respectively. 4.1.2 Semantic constraint collections In this work, we use WordNet as the resource to collect ordinal semantic knowledge. WordNet is a large semantic lexicon database of English words (Miller, 1995), where nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (usually called synsets). Each synset usually expresses a distinct semantic concept. All s</context>
<context position="27187" citStr="Mikolov et al., 2013" startWordPosition="4401" endWordPosition="4404">entence completion challenge has recently been introduced as a standard benchmark task for language modeling and other NLP techniques (Zweig and Burges, 2011). This task consists of 1040 sentences, each of which misses one word. The goal is to select a word that is the most coherent with the rest of the sentence, from a list of five candidates. Many NLP techniques have already been reported on this task, including N-gram model and LSA-based model proposed in (Zweig and Burges, 2011), log-bilinear model (Mnih and Teh, 2012), recurrent neural networks (RNN) (Mikolov, 2012), the skip-gram model (Mikolov et al., 2013a), a combination of the skip-gram and RNN model, and a knowledge enhanced word embedding model proposed by Bian et. al. (2014). The performance of all these techniques is listed in Table 2 for comparison. In this work, we follow the the same procedure as in (Mikolov et al., 2013a) to examine the performance of our proposed semantic word embeddings (SWE) on this task. We first train 600- 1507 dimension word embeddings based on a training corpus of 50M words provided by (Zweig and Burges, 2011), with and without using the collected ordinal knowledge. Then, for each sentence in the test set, we </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
</authors>
<title>Statistical language models based on neural networks.</title>
<date>2012</date>
<tech>Ph.D. thesis, Ph. D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="27144" citStr="Mikolov, 2012" startWordPosition="4396" endWordPosition="4397">.4.1 Task description The Microsoft sentence completion challenge has recently been introduced as a standard benchmark task for language modeling and other NLP techniques (Zweig and Burges, 2011). This task consists of 1040 sentences, each of which misses one word. The goal is to select a word that is the most coherent with the rest of the sentence, from a list of five candidates. Many NLP techniques have already been reported on this task, including N-gram model and LSA-based model proposed in (Zweig and Burges, 2011), log-bilinear model (Mnih and Teh, 2012), recurrent neural networks (RNN) (Mikolov, 2012), the skip-gram model (Mikolov et al., 2013a), a combination of the skip-gram and RNN model, and a knowledge enhanced word embedding model proposed by Bian et. al. (2014). The performance of all these techniques is listed in Table 2 for comparison. In this work, we follow the the same procedure as in (Mikolov et al., 2013a) to examine the performance of our proposed semantic word embeddings (SWE) on this task. We first train 600- 1507 dimension word embeddings based on a training corpus of 50M words provided by (Zweig and Burges, 2011), with and without using the collected ordinal knowledge. T</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´aˇs Mikolov. 2012. Statistical language models based on neural networks. Ph.D. thesis, Ph. D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity. Language and cognitive processes,</title>
<date>1991</date>
<pages>6--1</pages>
<contexts>
<context position="2319" citStr="Miller and Charles, 1991" startWordPosition="339" endWordPosition="342">mbedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurrence statistics to model contextual distributions from very large text corpora and they have been demonstrated to be quite effective in a number of NLP tasks. However, they still suffer</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and cognitive processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="5963" citStr="Miller, 1995" startWordPosition="897" endWordPosition="898">ing of word embeddings. In our approach, we propose to represent semantic knowledge as many word ordinal ranking inequalities. Furthermore, these inequalities are cast as semantic constraints in the optimization process to learn semantically sensible word embeddings. The proposed method has several advantages. Firstly, many different types of semantic knowledge can all be represented as a number of such ranking inequalities, such as synonymantonym, hyponym-hypernym and etc. Secondly, these inequalities can be easily extracted from many existing knowledge resources, such as Thesaurus, WordNet (Miller, 1995) and knowledge graphs. Moreover, the ranking inequalities can also be manually generated by human annotation because ranking orders is much easier for human annotators than assigning specific scores. Next, we present a flexible learning framework to learn distributed word representation based on the ordinal semantic knowledge. By solving a constrained optimization problem using the efficient stochastic gradient descent algorithm, we can obtain semantic word embedding enhanced by the ordinal knowledge constraints. Experiments on four popular natural language processing tasks, including word sim</context>
<context position="18387" citStr="Miller, 1995" startWordPosition="2964" endWordPosition="2965"> training corpora from the previous state-of-the-art work for fair comparisons. The training corpus for the sentence completion is the Holmes text (Zweig and Burges, 2011; Mikolov et al., 2013a). The training corpus for the name entity recognition task is the Reuters English newswire from RCV1 (Turian et al., 2010; Lewis et al., 2004). Refer to section 4.4 and section 4.5 for detailed descriptions respectively. 4.1.2 Semantic constraint collections In this work, we use WordNet as the resource to collect ordinal semantic knowledge. WordNet is a large semantic lexicon database of English words (Miller, 1995), where nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (usually called synsets). Each synset usually expresses a distinct semantic concept. All synsets in WordNet are interlinked by means of conceptual-semantic and/or lexical relations such as synonyms and antonyms, hypernyms and hyponyms. In our experiments, we use the version WordNet-3.1 for creating the corresponding semantic constraints. In detail, we follow the following process to extract semantic similarity inequalities from WordNet and Thesaurus: 1. Based on the Synonym Antonym Rule described in sectio</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="27095" citStr="Mnih and Teh, 2012" startWordPosition="4388" endWordPosition="4391">imilarity task. 4.4 Task 2: Sentence Completion Task 4.4.1 Task description The Microsoft sentence completion challenge has recently been introduced as a standard benchmark task for language modeling and other NLP techniques (Zweig and Burges, 2011). This task consists of 1040 sentences, each of which misses one word. The goal is to select a word that is the most coherent with the rest of the sentence, from a list of five candidates. Many NLP techniques have already been reported on this task, including N-gram model and LSA-based model proposed in (Zweig and Burges, 2011), log-bilinear model (Mnih and Teh, 2012), recurrent neural networks (RNN) (Mikolov, 2012), the skip-gram model (Mikolov et al., 2013a), a combination of the skip-gram and RNN model, and a knowledge enhanced word embedding model proposed by Bian et. al. (2014). The performance of all these techniques is listed in Table 2 for comparison. In this work, we follow the the same procedure as in (Mikolov et al., 2013a) to examine the performance of our proposed semantic word embeddings (SWE) on this task. We first train 600- 1507 dimension word embeddings based on a training corpus of 50M words provided by (Zweig and Burges, 2011), with and</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>12--1532</pages>
<contexts>
<context position="2581" citStr="Pennington et al., 2014" startWordPosition="383" endWordPosition="387">ural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurrence statistics to model contextual distributions from very large text corpora and they have been demonstrated to be quite effective in a number of NLP tasks. However, they still suffer from some major limitations. In particular, these corpus-based methods usually fail to capture the precise meanings for many words. For example, some semantically related but dissimilar words may have similar contexts, such as synonyms and antonyms. As a result</context>
<context position="22224" citStr="Pennington et al., 2014" startWordPosition="3591" endWordPosition="3595">rd2vec. Semantic Inequality Satisfied Rate Curve 0.00 0.05 0.10 0.15 0.20 0.25 0.30 beta (β) Figure 3: A curve of inequality satisfied rates (All models trained on the Wiki-Small corpus. HyperHypon and Synon-Anton stand for different semantic constraint sets employed for training semantic word embeddings). in (Xu et al., 2014), to set word embeddings to 300-dimension. Similarly, we refer to Bian et al. (2014) to set the dimensionality of word vectors to 600 for the Sentence Completion task. And we set the dimensionality of word vectors to 50 for the NER task according to (Turian et al., 2010; Pennington et al., 2014). 4.2 Semantic inequality satisfied rates Here we first examine the inequality satisfied rates of various word embeddings. The inequality satisfied rate is defined as how many percentage of semantic inequalities are satisfied based on the underlying word embedding vectors. In Figure 3, we show a typical curve of the inequality satisfied rates as a function of Q used in model training. This figure is plotted based on the Wiki-Small corpus. Two semantic constraint sets Synon-Anton and Hyper-Hypon created in section 4.1.2 are employed to learn semantic word embeddings. In the framework of the pro</context>
<context position="25349" citStr="Pennington et al., 2014" startWordPosition="4113" endWordPosition="4116">d embeddings on this task, we measure the performance by calculating the Spearman rank correlation between the human judgments and the similarity scores computed based on the learned word embeddings. 4.3.2 Experimental results Here we compare the proposed SWE model with the baseline skip-gram model on the WordSim353 task. Both word embedding models are trained using the Wikipedia corpora. We set the dimension of word embedding vectors to be 300. In Table 1, we have shown all the Spearman rank correlation results. The baseline results on this task include PPMI (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014), and ESA-Wikipedia (Gabrilovich and Markovitch, 2007). From the results in Table 1, we can see that the proposed SWE model can achieve consistent improvements over the baseline skip-gram model, no matter which training corpus is used. These results have demonstrated that, by incorporating semantic ordinal knowledge into the word vectors, Word Embeddings Result SPPMI 0.6870 Others GloVe (6 billion) 0.6580 GloVe (42 billion) 0.7590 ESA-Wikipedia 0.7500 Skip-gram 0.6326 Wiki-Small SWE + Synon-Anton 0.6584 (0.13 billion) SWE + Hyper-Hypon 0.6407 SWE + Both 0.6442 Skip-gram 0.7085 Wiki-Large SWE +</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of EMNLP, 12:1532– 1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Rockt¨aschel</author>
<author>Matko Boˇsnjak</author>
<author>Sameer Singh</author>
<author>Sebastian Riedel</author>
</authors>
<title>Low-dimensional embeddings of logic.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Workshop on Semantic Parsing,</booktitle>
<pages>45--49</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, MD,</location>
<marker>Rockt¨aschel, Boˇsnjak, Singh, Riedel, 2014</marker>
<rawString>Tim Rockt¨aschel, Matko Boˇsnjak, Sameer Singh, and Sebastian Riedel. 2014. Low-dimensional embeddings of logic. In Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 45–49, Baltimore, MD, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="24270" citStr="Rubenstein and Goodenough, 1965" startWordPosition="3936" endWordPosition="3939">nstraints. Meanwhile, we have found that the proposed SGD method is very efficient to handle a large number of inequalities in model training. When we use the total 345,000 inequalities, the SWE training is comparable with the baseline skip-gram model in terms of training speed. In the following, we continue to examine the SWE model on four popular natural language processing tasks, including word similarity, sentence completion, name entity recognition and the TOEFL synonym selection. 4.3 Task 1: Word Similarity Task 4.3.1 Task description Measuring word similarity is a traditional NLP task (Rubenstein and Goodenough, 1965). Here we compare several word embedding models on a popular word similarity task, namely WordSim353 (Finkelstein et al., 2001), which contains 353 English word pairs along with human-assigned similarity scores, which measure the relatedness of each word pair on a scale from 0 (totally unrelated words) to 10 (very much related or identical words). The final similarity score for each pair is the average across 13 to 16 human judges. When evaluating word embeddings on this task, we measure the performance by calculating the Spearman rank correlation between the human judgments and the similarity</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>518</pages>
<contexts>
<context position="2013" citStr="Schwenk, 2007" startWordPosition="294" endWordPosition="295">e TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be significantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings. 1 Introduction Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech &amp; Language, 21(3):492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="2159" citStr="Sutskever et al., 2014" startWordPosition="315" endWordPosition="318">owledge is incorporated as inequality constraints during the learning process of word embeddings. 1 Introduction Distributed word representation (i.e., word embedding) is a technique that represents words as continuous vectors, which is an important research topic in natural language processing (NLP) (Hinton et al., 1986; Turney et al., 2010). In recent years, it has been widely used in various NLP tasks, including neural language model (Bengio et al., 2003; Schwenk, 2007), sequence labelling tasks (Collobert and Weston, 2008; Collobert et al., 2011), machine translation (Devlin et al., 2014; Sutskever et al., 2014), and antonym selection (Chen et al., 2015). Typically, word vectors are learned based on the distributional hypothesis (Harris, 1954; Miller and Charles, 1991), which assumes that words with a similar context tend to have a similar meaning. Under this hypothesis, various models, such as the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014) and GloVe model (Pennington et al., 2014), have been proposed to leverage the context of each word in large corpora to learn word embeddings. These methods can efficiently estimate the co-occurrence statistics to model </context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of NIPS, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18089" citStr="Turian et al., 2010" startWordPosition="2916" endWordPosition="2919">con of 225,909 distinct words appearing more than 5 times in the corpus. Similarly, the WikiLarge corpus contains about 5 billion words, for which we create a lexicon of 228,069 words appearing more than 200 times. For the other two tasks, sentence completion and name entity recognition, we use the same training corpora from the previous state-of-the-art work for fair comparisons. The training corpus for the sentence completion is the Holmes text (Zweig and Burges, 2011; Mikolov et al., 2013a). The training corpus for the name entity recognition task is the Reuters English newswire from RCV1 (Turian et al., 2010; Lewis et al., 2004). Refer to section 4.4 and section 4.5 for detailed descriptions respectively. 4.1.2 Semantic constraint collections In this work, we use WordNet as the resource to collect ordinal semantic knowledge. WordNet is a large semantic lexicon database of English words (Miller, 1995), where nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (usually called synsets). Each synset usually expresses a distinct semantic concept. All synsets in WordNet are interlinked by means of conceptual-semantic and/or lexical relations such as synonyms and antonyms, h</context>
<context position="22198" citStr="Turian et al., 2010" startWordPosition="3587" endWordPosition="3590">/code.google.com/p/word2vec. Semantic Inequality Satisfied Rate Curve 0.00 0.05 0.10 0.15 0.20 0.25 0.30 beta (β) Figure 3: A curve of inequality satisfied rates (All models trained on the Wiki-Small corpus. HyperHypon and Synon-Anton stand for different semantic constraint sets employed for training semantic word embeddings). in (Xu et al., 2014), to set word embeddings to 300-dimension. Similarly, we refer to Bian et al. (2014) to set the dimensionality of word vectors to 600 for the Sentence Completion task. And we set the dimensionality of word vectors to 50 for the NER task according to (Turian et al., 2010; Pennington et al., 2014). 4.2 Semantic inequality satisfied rates Here we first examine the inequality satisfied rates of various word embeddings. The inequality satisfied rate is defined as how many percentage of semantic inequalities are satisfied based on the underlying word embedding vectors. In Figure 3, we show a typical curve of the inequality satisfied rates as a function of Q used in model training. This figure is plotted based on the Wiki-Small corpus. Two semantic constraint sets Synon-Anton and Hyper-Hypon created in section 4.1.2 are employed to learn semantic word embeddings. I</context>
<context position="29663" citStr="Turian et al., 2010" startWordPosition="4812" endWordPosition="4815">of the sentence contexts. 4.5 Task 3: Name Entity Recognition 4.5.1 Task description To further investigate the performance of semantic word embeddings, we have further conducted some experiments on the standard CoNLL03 name entity recognition (NER) task. The CoNLL03 NER dataset is drawn from the Reuters newswire. The training set contains 204K words (14K sentences, 946 documents), the test set contains 46K words (3.5K sentences, 231 documents), and the development set contains 51K words (3.3K sentences, 216 documents). We have listed the state-of-the-art performance in Table 3 for this task (Turian et al., 2010). To make a fair comparison, we have used the exactly same experimental configurations as in (Turian et al., 2010), including the used training algorithm, the baseline discrete features and so on. Like the C&amp;W model, we use the same training text resource to learn word vectors, which contains one year of Reuters English newswire from RCV1, from August 1996 to August 1997, having about 810,000 news stories (Lewis et al., 2004). Meanwhile, the dimension of word embeddings is set to 50 for all experiments on this task. 4.5.2 Experimental results In our experiments, we compare the proposed SWE mod</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Xu</author>
<author>Yalong Bai</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Gang Wang</author>
<author>Xiaoguang Liu</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Rcnet: A general framework for incorporating knowledge into word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>1219--1228</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3969" citStr="Xu et al., 2014" startWordPosition="600" endWordPosition="603"> representations are mainly learned based on the co-occurrence information, the learned word embeddings do not capture the accurate relationship between two semantically similar words if either one appears less frequently in the corpus. To address these issues, some recent work has been proposed to incorporate prior lexical knowledge (WordNet, PPDB, etc.) or knowledge graph (Freebase, etc.) into word representations. Such knowledge enhanced word embedding methods have achieved considerable improvements on various natural language processing tasks, like (Yu and Dredze, 2014; Bian et al., 2014; Xu et al., 2014). These methods attempt to increase the semantic similarities between words belonging to 1501 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1501–1511, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics one semantic category or to explicitly model the semantic relationships between different words. For example, Yu and Dredze (2014) have proposed a new learning objective function to enhance word embeddings by combining neural models and a prio</context>
<context position="14077" citStr="Xu et al., 2014" startWordPosition="2248" endWordPosition="2251">to balance the contribution of the penalty term in the optimization process. It balances between the semantic information estimated from the corpus based on the distributional hypothesis and the semantic knowledge encoded in the ordinal ranking inequalities. In Rockt¨aschel et al. (2014), a similar approach was proposed to capture knowledge constraint as extra terms in the objective function for optimization. In Figure 2, we show a diagram for the the overall SWE learning framework to incorporate semantic knowledge into the basic skip-gram word embeddings. Comparing with the previous work in (Xu et al., 2014) and (Faruqui et al., 2014), the proposed SWE framework is more general in terms of encoding the semantic knowledge for learning word embeddings. It is straightforward to show that the work in (Xu et al., 2014; Zweig, 2014; Faruqui et al., 2014) can be viewed as some special cases under our SWE learning framework. 3.3 Optimization algorithm for SWE In this work, the proposed semantic word embeddings (SWE) are learned using the standard mini-batch stochastic gradient descent (SGD) algorithm. Furthermore, we adopt to use the cosine distance of the embedding vectors to compute the similarity betw</context>
<context position="21928" citStr="Xu et al., 2014" startWordPosition="3539" endWordPosition="3542">asks, the dimensionality of embedding vectors is different since we try to use the same settings from the state-of-theart work for the comparison purpose. In the Word Similarity task and the TOEFL Synonym Selection task, we followed the state of the art work 4https://code.google.com/p/word2vec. Semantic Inequality Satisfied Rate Curve 0.00 0.05 0.10 0.15 0.20 0.25 0.30 beta (β) Figure 3: A curve of inequality satisfied rates (All models trained on the Wiki-Small corpus. HyperHypon and Synon-Anton stand for different semantic constraint sets employed for training semantic word embeddings). in (Xu et al., 2014), to set word embeddings to 300-dimension. Similarly, we refer to Bian et al. (2014) to set the dimensionality of word vectors to 600 for the Sentence Completion task. And we set the dimensionality of word vectors to 50 for the NER task according to (Turian et al., 2010; Pennington et al., 2014). 4.2 Semantic inequality satisfied rates Here we first examine the inequality satisfied rates of various word embeddings. The inequality satisfied rate is defined as how many percentage of semantic inequalities are satisfied based on the underlying word embedding vectors. In Figure 3, we show a typical</context>
</contexts>
<marker>Xu, Bai, Bian, Gao, Wang, Liu, Liu, 2014</marker>
<rawString>Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rcnet: A general framework for incorporating knowledge into word representations. In Proceedings of CIKM, pages 1219–1228. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>2</volume>
<pages>545--550</pages>
<contexts>
<context position="3932" citStr="Yu and Dredze, 2014" startWordPosition="592" endWordPosition="595">many synonymous words. Moreover, as word representations are mainly learned based on the co-occurrence information, the learned word embeddings do not capture the accurate relationship between two semantically similar words if either one appears less frequently in the corpus. To address these issues, some recent work has been proposed to incorporate prior lexical knowledge (WordNet, PPDB, etc.) or knowledge graph (Freebase, etc.) into word representations. Such knowledge enhanced word embedding methods have achieved considerable improvements on various natural language processing tasks, like (Yu and Dredze, 2014; Bian et al., 2014; Xu et al., 2014). These methods attempt to increase the semantic similarities between words belonging to 1501 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1501–1511, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics one semantic category or to explicitly model the semantic relationships between different words. For example, Yu and Dredze (2014) have proposed a new learning objective function to enhance word embeddings </context>
</contexts>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Proceedings of ACL, volume 2, pages 545–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Zweig</author>
<author>Christopher JC Burges</author>
</authors>
<title>The microsoft research sentence completion challenge.</title>
<date>2011</date>
<tech>Technical report, Technical Report MSR-TR-2011-129, Microsoft.</tech>
<contexts>
<context position="17944" citStr="Zweig and Burges, 2011" startWordPosition="2892" endWordPosition="2895">script from the Matt Mahoney’s page3. After text normalization, the Wiki-Small corpus contains totally 130 million words, for which we create a lexicon of 225,909 distinct words appearing more than 5 times in the corpus. Similarly, the WikiLarge corpus contains about 5 billion words, for which we create a lexicon of 228,069 words appearing more than 200 times. For the other two tasks, sentence completion and name entity recognition, we use the same training corpora from the previous state-of-the-art work for fair comparisons. The training corpus for the sentence completion is the Holmes text (Zweig and Burges, 2011; Mikolov et al., 2013a). The training corpus for the name entity recognition task is the Reuters English newswire from RCV1 (Turian et al., 2010; Lewis et al., 2004). Refer to section 4.4 and section 4.5 for detailed descriptions respectively. 4.1.2 Semantic constraint collections In this work, we use WordNet as the resource to collect ordinal semantic knowledge. WordNet is a large semantic lexicon database of English words (Miller, 1995), where nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (usually called synsets). Each synset usually expresses a distinct s</context>
<context position="26725" citStr="Zweig and Burges, 2011" startWordPosition="4321" endWordPosition="4324">embedding framework can capture much better semantics for many words. The SWE model using the Wiki-Large corpus has achieved the state-of-the-art performance on this task, significantly outperforming other popular word embedding methods, such as skip-gram and GloVe. Moreover, we also find that the Synon-Anton constraint set is more relevant than Hyper-Hypon for the word similarity task. 4.4 Task 2: Sentence Completion Task 4.4.1 Task description The Microsoft sentence completion challenge has recently been introduced as a standard benchmark task for language modeling and other NLP techniques (Zweig and Burges, 2011). This task consists of 1040 sentences, each of which misses one word. The goal is to select a word that is the most coherent with the rest of the sentence, from a list of five candidates. Many NLP techniques have already been reported on this task, including N-gram model and LSA-based model proposed in (Zweig and Burges, 2011), log-bilinear model (Mnih and Teh, 2012), recurrent neural networks (RNN) (Mikolov, 2012), the skip-gram model (Mikolov et al., 2013a), a combination of the skip-gram and RNN model, and a knowledge enhanced word embedding model proposed by Bian et. al. (2014). The perfo</context>
</contexts>
<marker>Zweig, Burges, 2011</marker>
<rawString>Geoffrey Zweig and Christopher JC Burges. 2011. The microsoft research sentence completion challenge. Technical report, Technical Report MSR-TR-2011-129, Microsoft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Zweig</author>
</authors>
<title>Explicit representation of antonymy in language modelling.</title>
<date>2014</date>
<tech>Technical report, Technical Report MSR-TR-2014-52, Microsoft.</tech>
<contexts>
<context position="14299" citStr="Zweig, 2014" startWordPosition="2288" endWordPosition="2289">e ordinal ranking inequalities. In Rockt¨aschel et al. (2014), a similar approach was proposed to capture knowledge constraint as extra terms in the objective function for optimization. In Figure 2, we show a diagram for the the overall SWE learning framework to incorporate semantic knowledge into the basic skip-gram word embeddings. Comparing with the previous work in (Xu et al., 2014) and (Faruqui et al., 2014), the proposed SWE framework is more general in terms of encoding the semantic knowledge for learning word embeddings. It is straightforward to show that the work in (Xu et al., 2014; Zweig, 2014; Faruqui et al., 2014) can be viewed as some special cases under our SWE learning framework. 3.3 Optimization algorithm for SWE In this work, the proposed semantic word embeddings (SWE) are learned using the standard mini-batch stochastic gradient descent (SGD) algorithm. Furthermore, we adopt to use the cosine distance of the embedding vectors to compute the similarity between two words in the penalty term. In the following, we show how to compute the derivatives of the penalty term for the SWE learning. ∂w(1) ∂D � t (i,j,k)ES = ∂f (sik − sij) ∂w(1) t � δij(t) ∂sij (9) ∂w(1) t where δik(t) a</context>
</contexts>
<marker>Zweig, 2014</marker>
<rawString>Geoffrey Zweig. 2014. Explicit representation of antonymy in language modelling. Technical report, Technical Report MSR-TR-2014-52, Microsoft.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>