<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.9992415">
A Comparative Study of Parameter Estimation Methods for
Statistical Natural Language Processing
</title>
<author confidence="0.861232">
Jianfeng Gao&amp;quot;, Galen Andrew&amp;quot;, Mark Johnson&amp;quot;&amp;, Kristina Toutanova&amp;quot;
</author>
<affiliation confidence="0.5747955">
*Microsoft Research, Redmond WA 98052, {jfgao,galena,kristout}@microsoft.com
&amp;Brown University, Providence, RI 02912, mj@cs.brown.edu
</affiliation>
<sectionHeader confidence="0.988766" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983260869565">
This paper presents a comparative study of
five parameter estimation algorithms on four
NLP tasks. Three of the five algorithms are
well-known in the computational linguistics
community: Maximum Entropy (ME) estima-
tion with L2 regularization, the Averaged
Perceptron (AP), and Boosting. We also in-
vestigate ME estimation with L1 regularization
using a novel optimization algorithm, and
BLasso, which is a version of Boosting with
Lasso (L1) regularization. We first investigate
all of our estimators on two re-ranking tasks: a
parse selection task and a language model
(LM) adaptation task. Then we apply the best
of these estimators to two additional tasks
involving conditional sequence models: a
Conditional Markov Model (CMM) for part of
speech tagging and a Conditional Random
Field (CRF) for Chinese word segmentation.
Our experiments show that across tasks, three
of the estimators — ME estimation with L1 or
L2 regularization, and AP — are in a near sta-
tistical tie for first place.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999561517857143">
Parameter estimation is fundamental to many sta-
tistical approaches to NLP. Because of the
high-dimensional nature of natural language, it is
often easy to generate an extremely large number of
features. The challenge of parameter estimation is
to find a combination of the typically noisy, re-
dundant features that accurately predicts the target
output variable and avoids overfitting. Intuitively,
this can be achieved either by selecting a small
number of highly-effective features and ignoring
the others, or by averaging over a large number of
weakly informative features. The first intuition
motivates feature selection methods such as
Boosting and BLasso (e.g., Collins 2000; Zhao and
Yu, 2004), which usually work best when many
features are completely irrelevant. L1 or Lasso
regularization of linear models, introduced by
Tibshirani (1996), embeds feature selection into
regularization so that both an assessment of the
reliability of a feature and the decision about
whether to remove it are done in the same frame-
work, and has generated a large amount of interest
in the NLP community recently (e.g., Goodman
2003; Riezler and Vasserman 2004). If on the other
hand most features are noisy but at least weakly
correlated with the target, it may be reasonable to
attempt to reduce noise by averaging over all of the
features. ME estimators with L2 regularization,
which have been widely used in NLP tasks (e.g.,
Chen and Rosenfeld 2000; Charniak and Johnson
2005; Johnson et al. 1999), tend to produce models
that have this property. In addition, the perceptron
algorithm and its variants, e.g., the voted or aver-
aged perceptron, is becoming increasingly popular
due to their competitive performance, simplicity in
implementation and low computational cost in
training (e.g., Collins 2002).
While recent studies claim advantages for L1
regularization, this study is the first of which we are
aware to systematically compare it to a range of
estimators on a diverse set of NLP tasks. Gao et al.
(2006) showed that BLasso, due to its explicit use of
L1 regularization, outperformed Boosting in the LM
adaptation task. Ng (2004) showed that for logistic
regression, L1 regularization outperforms L2 regu-
larization on artificial datasets which contain many
completely irrelevant features. Goodman (2003)
showed that in two out of three tasks, an ME esti-
mator with a one-sided Laplacian prior (i.e., L1
regularization with the constraint that all feature
weights are positive) outperformed a comparable
estimator using a Gaussian prior (i.e., L2 regulari-
zation). Riezler and Vasserman (2004) showed that
an L1-regularized ME estimator outperformed an
L2-regularized estimator for ranking the parses of a
stochastic unification-based grammar.
</bodyText>
<page confidence="0.977418">
824
</page>
<note confidence="0.925555">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824–831,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99982975">
While these individual estimators are well de-
scribed in the literature, little is known about the
relative performance of these methods because the
published results are generally not directly compa-
rable. For example, in the parse re-ranking task,
one cannot tell whether the L2- regularized ME
approach used by Charniak and Johnson (2005)
significantly outperforms the Boosting method by
Collins (2000) because different feature sets and
n-best parses were used in the evaluations of these
methods.
This paper conducts a much-needed comparative
study of these five parameter estimation algorithms
on four NLP tasks: ME estimation with L1 and L2
regularization, the Averaged Perceptron (AP),
Boosting, and BLasso, a version of Boosting with
Lasso (L1) regularization. We first investigate all of
our estimators on two re-ranking tasks: a parse
selection task and a language model adaptation task.
Then we apply the best of these estimators to two
additional tasks involving conditional sequence
models: a CMM for POS tagging and a CRF for
Chinese word segmentation. Our results show that
ME estimation with L2 regularization achieves the
best performing estimators in all of the tasks, and
AP achieves almost as well and requires much less
training time. L1 (Lasso) regularization also per-
forms well and leads to sparser models.
</bodyText>
<sectionHeader confidence="0.994653" genericHeader="method">
2 Estimators
</sectionHeader>
<bodyText confidence="0.996265">
All the four NLP tasks studied in this paper are
based on linear models (Collins 2000) which re-
quire learning a mapping from inputs 𝑥 ∈ 𝑋 to
outputs 𝑦 ∈ 𝑌. We are given:
</bodyText>
<listItem confidence="0.996278">
• Training samples (𝑥𝑖, 𝑦𝑖) for 𝑖 = 1 ... 𝑁,
• A procedure 𝑮𝑬𝑵 to generate a set of candi-
dates 𝑮𝑬𝑵(𝑥) for an input x,
• A feature mapping (D: 𝑋 × 𝑌 ↦ ℝ𝐷 to map
each (𝑥, 𝑦) to a vector of feature values, and
• A parameter vector 𝒘 ∈ ℝ𝐷, which assigns a
real-valued weight to each feature.
</listItem>
<bodyText confidence="0.56946675">
For all models except the CMM sequence model for
POS tagging, the components 𝑮𝑬𝑵, (D and 𝒘 di-
rectly define a mapping from an input 𝑥 to an output
𝐹(𝑥) as follows:
</bodyText>
<equation confidence="0.961392">
𝐹 𝑥 = arg max𝑦∈𝑮𝑬𝑵 𝑋 (D 𝑥, 𝑦 ⋅ 𝒘. (1)
</equation>
<bodyText confidence="0.99977188">
In the CMM sequence classifier, locally normalized
linear models to predict the tag of each word token
are chained together to arrive at a probability esti-
mate for the entire tag sequence, resulting in a
slightly different decision rule.
Linear models, though simple, can capture very
complex dependencies because the features can be
arbitrary functions of the input/output pair. For
example, we can define a feature to be the log con-
ditional probability of the output as estimated by
some other model, which may in turn depend on
arbitrarily complex interactions of „basic‟ features.
In practice, with an appropriate feature set, linear
models achieve very good empirical results on
various NLP tasks. The focus of this paper however
is not on feature definition (which requires domain
knowledge and varies from task to task), but on
parameter estimation (which is generic across
tasks). We assume we are given fixed feature
templates from which a large number of features are
generated. The task of the estimator is to use the
training samples to choose a parameter vector 𝒘,
such that the mapping 𝐹(𝑥) is capable of correctly
classifying unseen examples. We will describe the
five estimators in our study individually.
</bodyText>
<subsectionHeader confidence="0.975905">
2.1 ME estimation with L2 regularization
</subsectionHeader>
<bodyText confidence="0.99992">
Like many linear models, the ME estimator chooses
𝒘 to minimize the sum of the empirical loss on the
training set and a regularization term:
</bodyText>
<equation confidence="0.949189">
𝒘 = arg min𝒘 𝐿 𝒘 + 𝑅 𝒘 . (2)
</equation>
<bodyText confidence="0.9977785">
In this case, the loss term L(w) is the negative con-
ditional log-likelihood of the training data,
</bodyText>
<equation confidence="0.9481628">
𝐿 𝒘 = 𝑛 , where
𝑃 𝑦 𝑥) = − log𝑃 𝑦𝑖 𝑥𝑖)
𝑖=1
exp (D 𝑥,𝑦 ⋅ 𝒘
𝑦 ′∈𝐺𝐸𝑁 𝑥 exp((D 𝑥, 𝑦′ ⋅ 𝒘)
</equation>
<bodyText confidence="0.998545533333333">
and the regularizer term 𝑅 𝒘 = 𝛼 𝑗 𝑤𝑗2 is the
weighted squared L2 norm of the parameters. Here,
a is a parameter that controls the amount of regu-
larization, optimized on held-out data.
This is one of the most popular estimators,
largely due to its appealing computational proper-
ties: both 𝐿 𝒘 and 𝑅 (𝒘) are convex and differen-
tiable, so gradient-based numerical algorithms can
be used to find the global minimum efficiently.
In our experiments, we used the limited memory
quasi-Newton algorithm (or L-BFGS, Nocedal and
Wright 1999) to find the optimal 𝒘 because this
method has been shown to be substantially faster
than other methods such as Generalized Iterative
Scaling (Malouf 2002).
</bodyText>
<page confidence="0.997518">
825
</page>
<bodyText confidence="0.982575166666667">
Because for some sentences there are multiple
best parses (i.e., parses with the same F-Score), we
used the variant of ME estimator described in
Riezler et al. (2002), where L(w) is defined as the
likelihood of the best parses y E Y(x) relative to
the n-best parser output GEN(x), (i.e., Y(x) 9;
GEN(x)): L(w) = − Zn 1logZyiGY(xi)P(yi|xi).
We applied this variant in our experiments of
parse re-ranking and LM adaptation, and found that
on both tasks it leads to a significant improvement
in performance for the L2-regularied ME estimator
but not for the L1-regularied ME estimator.
</bodyText>
<subsectionHeader confidence="0.992958">
2.2 ME estimation with L1 regularization
</subsectionHeader>
<bodyText confidence="0.999964060606061">
This estimator also minimizes the negative condi-
tional log-likelihood, but uses an L1 (or Lasso)
penalty. That is, R (w) in Equation (2) is defined
according to R (w) = a jZI wj I. L1 regularization
typically leads to sparse solutions in which many
feature weights are exactly zero, so it is a natural
candidate when feature selection is desirable. By
contrast, L2 regularization produces solutions in
which most weights are small but non-zero.
Optimizing the L1-regularized objective function
is challenging because its gradient is discontinuous
whenever some parameter equals zero. Kazama and
Tsujii (2003) described an estimation method that
constructs an equivalent constrained optimization
problem with twice the number of variables.
However, we found that this method is impracti-
cally slow for large-scale NLP tasks. In this work
we use the orthant-wise limited-memory qua-
si-Newton algorithm (OWL-QN), which is a mod-
ification of L-BFGS that allows it to effectively
handle the discontinuity of the gradient (Andrew
and Gao 2007). We provide here a high-level de-
scription of the algorithm.
A quasi-Newton method such as L-BFGS uses
first order information at each iterate to build an
approximation to the Hessian matrix, H, thus mod-
eling the local curvature of the function. At each
step, a search direction is chosen by minimizing a
quadratic approximation to the function:
L-BFGS maintains vectors of the change in gradient
gk − gk−1 from the most recent iterations, and uses
them to construct an estimate of the inverse Hessian
H−1. Furthermore, it does so in such a way that
H−1g0 can be computed without expanding out the
full matrix, which is typically unmanageably large.
The computation requires a number of operations
linear in the number of variables.
OWL-QN is based on the observation that when
restricted to a single orthant, the L1 regularizer is
differentiable, and is in fact a linear function of w.
Thus, so long as each coordinate of any two con-
secutive search points does not pass through zero,
R (w) does not contribute at all to the curvature of
the function on the segment joining them. There-
fore, we can use L-BFGS to approximate the Hes-
sian of L(w) alone, and use it to build an approxi-
mation to the full regularized objective that is valid
on a given orthant. To ensure that the next point is in
the valid region, we project each point during the
line search back onto the chosen orthant.1 At each
iteration, we choose the orthant containing the
current point and into which the direction giving the
greatest local rate of function decrease points.
This algorithm, although only a simple modifi-
cation of L-BFGS, works quite well in practice. It
typically reaches convergence in even fewer itera-
tions than standard L-BFGS takes on the analogous
L2-regularized objective (which translates to less
training time, since the time per iteration is only
negligibly higher, and total time is dominated by
function evaluations). We describe OWL-QN more
fully in (Andrew and Gao 2007). We also show that
it is significantly faster than Kazama and Tsujii‟s
algorithm for L1 regularization and prove that it is
guaranteed converge to a parameter vector that
globally optimizes the L1-regularized objective.
</bodyText>
<subsectionHeader confidence="0.9996">
2.3 Boosting
</subsectionHeader>
<bodyText confidence="0.9974816">
The Boosting algorithm we used is based on Collins
(2000). It optimizes the pairwise exponential loss
(ExpLoss) function (rather than the logarithmic loss
optimized by ME). Given a training sample
(xi, yi), for each possible output yj E GEN (xi), we
</bodyText>
<equation confidence="0.988939">
Q(x) =
2 (x − x0 ) ′H (x − x0 ) + g0′ (x − x0)
1
</equation>
<bodyText confidence="0.841191">
where x0 is the current iterate, and g0 is the func-
tion gradient at x0. If H is positive definite, the
minimizing value of x can be computed analytically
according to: x* = x0 − H−1g0.
1 This projection just entails zeroing-out any coordinates
that change sign. Note that it is possible for a variable to
change sign in two iterations, by moving from a negative
value to zero, and on a the next iteration moving from
zero to a positive value.
</bodyText>
<page confidence="0.991012">
826
</page>
<figure confidence="0.9942035">
1 Set w0 = argminw0ExpLoss(w); and wd = 0 for d=1...D
2 Select a feature fk» which has largest estimated
impact on reducing ExpLoss of Equation (3)
3 Update ilk»  ilk» + δ*, and return to Step 2
</figure>
<figureCaption confidence="0.999933">
Figure 1: The boosting algorithm
</figureCaption>
<bodyText confidence="0.880916666666667">
define the margin of the pair (𝑦𝑖, 𝑦𝑗 ) with respect to
𝒘 as 𝑀 𝑦𝑖, 𝑦𝑗 = Φ 𝑥𝑖, 𝑦𝑖 ⋅ 𝒘− Φ 𝑥𝑖, 𝑦𝑗 ⋅ 𝒘.
Then ExpLoss is defined as
</bodyText>
<figure confidence="0.5402475">
ExpLoss 𝒘 = exp −M yi,yj (3)
𝑖 𝑦𝑗 ∈𝑮𝑬𝑵 𝑥𝑖
</figure>
<figureCaption confidence="0.9952316">
Figure 1 summarizes the Boosting algorithm we
used. It is an incremental feature selection proce-
dure. After initialization, Steps 2 and 3 are repeated
T times; at each iteration, a feature is chosen and its
weight is updated as follows.
</figureCaption>
<bodyText confidence="0.935997333333333">
First, we define Upd(𝒘, 𝑘, 𝛿) as an updated
model, with the same parameter values as 𝑤 with
the exception of 𝑤𝑘, which is incremented by 𝛿:
Upd 𝒘,𝑘,𝛿 = (𝑤1,..., 𝑤𝑘 + 𝛿,...,𝑤𝐷)
Then, Steps 2 and 3 in Figure 1 can be rewritten as
Equations (4) and (5), respectively.
</bodyText>
<equation confidence="0.9995095">
𝑘∗, 𝛿∗ = arg min ExpLoss(Upd 𝒘, 𝑘, 𝛿 ) (4)
𝑘,𝛿𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗, 𝛿∗) (5)
</equation>
<bodyText confidence="0.999441">
Because Boosting can overfit we update the weight
of 𝑓𝑘∗ by a small fixed step size , as in Equation (6),
following the FSLR algorithm (Hastie et al. 2001).
</bodyText>
<equation confidence="0.980968">
𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗, 𝜖 × sign 𝛿∗ ) (6)
</equation>
<bodyText confidence="0.9995958">
By taking such small steps, Boosting imposes a
kind of implicit regularization, and can closely
approximate the effect of L1 regularization in a local
sense (Hastie et al. 2001). Empirically, smaller
values of 𝜖 lead to smaller numbers of test errors.
</bodyText>
<subsectionHeader confidence="0.993847">
2.4 Boosted Lasso
</subsectionHeader>
<bodyText confidence="0.993501166666667">
The Boosted Lasso (BLasso) algorithm was origi-
nally proposed in Zhao and Yu (2004), and was
adapted for language modeling by Gao et al. (2006).
BLasso can be viewed as a version of Boosting with
L1 regularization. It optimizes an L1-regularized
ExpLoss function:
</bodyText>
<equation confidence="0.845243">
LassoLoss 𝒘 = ExpLoss(𝒘) + 𝑅(𝒘) (7)
</equation>
<bodyText confidence="0.9934367">
where 𝑅 𝒘 = 𝛼 𝑗 𝑤𝑗 .
BLasso also uses an incremental feature selec-
tion procedure to learn parameter vector 𝒘, just as
Boosting does. Due to the explicit use of the regu-
larization term 𝑅 (𝒘), however, there are two major
differences from Boosting.
At each iteration, BLasso takes either a forward
step or a backward step. Similar to Boosting, at
each forward step, a feature is selected and its
weight is updated according to Eq. (8) and (9).
</bodyText>
<equation confidence="0.989639">
𝑘∗, 𝛿∗ = 𝑎𝑟𝑔 𝑚𝑖𝑛
𝑘,𝛿=±𝜖 ExpLoss(Upd 𝒘, 𝑘, 𝛿 ) (8)
𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗, 𝜖 × sign 𝛿∗ ) (9)
</equation>
<bodyText confidence="0.9979696">
There is a small but important difference between
Equations (8) and (4). In Boosting, as shown in
Equation (4), a feature is selected by its impact on
reducing the loss with its optimal update 𝛿∗. By
contrast, in BLasso, as shown in Equation (8),
rather than optimizing over 𝛿 for each feature, the
loss is calculated with an update of either +𝜖 or −𝜖,
i.e., grid search is used for feature weight estima-
tion. We found in our experiments that this mod-
ification brings a consistent improvement.
The backward step is unique to BLasso. At each
iteration, a feature is selected and the absolute value
of its weight is reduced by 𝜖 if and only if it leads to
a decrease of the LassoLoss, as shown in Equations
(10) and (11), where  is a tolerance parameter.
</bodyText>
<equation confidence="0.999738">
𝑘∗ = arg min ExpLoss(Upd(𝒘, 𝑘, −𝜖sign 𝑤𝑘 ) (10)
𝑘:𝑤𝑘≠0
𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗,sign(𝑤𝑘∗) × 𝜖) (11)
</equation>
<bodyText confidence="0.914244391304348">
if LassoLoss 𝒘𝑡−1, 𝛼𝑡−1 − LassoLoss 𝒘𝑡, 𝛼𝑡 &gt; 𝜃
Figure 2 summarizes the BLasso algorithm we
used. After initialization, Steps 4 and 5 are repeated
T times; at each iteration, a feature is chosen and its
weight is updated either backward or forward by a
fixed amount 𝜖. Notice that the value of 𝛼 is adap-
tively chosen according to the reduction of ExpLoss
during training. The algorithm starts with a large
initial 𝛼, and then at each forward step the value of
𝛼 decreases until ExpLoss stops decreasing. This is
intuitively desirable: it is expected that most highly
effective features are selected in early stages of
training, so the reduction of ExpLoss at each step in
early stages are more substantial than in later stages.
These early steps coincide with the Boosting steps
most of the time. In other words, the effect of
backward steps is more visible at later stages. It can
be proved that for a finite number of features and
𝜃 =0, the BLasso algorithm shown in Figure 2
converges to the Lasso solution when 𝜖 → 0. See
Gao et al. (2006) for implementation details, and
Zhao and Yu (2004) for a theoretical justification
for BLasso.
</bodyText>
<page confidence="0.976648">
827
</page>
<table confidence="0.808129428571429">
1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0 the mapping from 𝑥 to 𝑦 by the linear model may
for d=1...D. make use of arbitrary global features of the output
2 Take a forward step according to Eq. (8) and (9), and and is performed “all at once”, we call such a linear
the updated model is denoted by w1 model a global model.
3 Initialize a = (ExpLoss(w0)-ExpLoss(w1))/e In the other two tasks (i.e., Chinese word seg-
4 Take a backward step if and only if it leads to a de- mentation and POS tagging), there is no explicit
crease of LassoLoss according to Eq. (10) and (11), enumeration of 𝑮𝑬𝑵(𝑥). The mapping from 𝑥 to 𝑦
</table>
<bodyText confidence="0.887798285714286">
where B = 0; otherwise is determined by a sequence model which aggre-
5 Take a forward step according to Eq. (8) and (9); gates the decisions of local linear models via a
update a = min(a, (ExpLoss(wt-1)-ExpLoss(wt))/e ); dynamic program. In the CMM, the local linear
and return to Step 4. models are trained independently, while in the CRF
model, the local models are trained jointly. We call
these two linear models local models because they
dynamically combine the output of models that use
only local features.
While it is straightforward to apply the five es-
timators to global models in the re-ranking
framework, the application of some estimators to
the local models is problematic. Boosting and
BLasso are too computationally expensive to be
applied to CRF training and we compared the other
three better performing estimation methods for this
model. The CMM is a probabilistic sequence model
and the log-loss used by ME estimation is most
natural for it; thus we limit the comparison to the
two kinds of ME models for CMMs. Note that our
goal is not to compare locally trained models to
globally trained ones; for a study which focuses on
this issue, see (Punyakanok et al. 2005).
In each task we compared the performance of
different estimators using task-specific measures.
We used the Wilcoxon signed rank test to test the
statistical significance of the difference among the
competing estimators. We also report other results
such as number of non-zero features after estima-
tion, number of training iterations, and computation
time (in minutes of elapsed time on an XEONTM MP
3.6GHz machine).
3.1 Parse re-ranking
We follow the experimental paradigm of parse
re-ranking outlined in Charniak and Johnson
(2005), and fed the features extracted by their pro-
gram to the five rerankers we developed. Each uses
a linear model trained using one of the five esti-
mators. These rerankers attempt to select the best
parse 𝑦 for a sentence 𝑥 from the 50-best list of
possible parses 𝑮𝑬𝑵 𝑥 for the sentence. The li-
near model combines the log probability calculated
by the Charniak (2000) parser as a feature with
</bodyText>
<figureCaption confidence="0.8804035">
1,219,272 additional features. We trained the fea-
Figure 2: The BLasso algorithm
</figureCaption>
<figure confidence="0.851416875">
1 Set w0 = 1 and wd = 0 for d=1...D
2 For t = 1...T (T = the total number of iterations)
3 For each training sample (xi, yi), i = 1...N
4 Choose the best candidate zi from GEN(xi) using
the current model w,
𝑧𝑖 = arg max Φ 𝑥𝑖, 𝑧 ⋅ 𝑤
𝑧∈𝐺𝐸𝑁 𝑥 _𝑖
5 w = w + η((D(xi, yi) – (D(xi, zi)), where η is the size of
</figure>
<figureCaption confidence="0.94431032">
learning step, optimized on held-out data.
Figure 3: The perceptron algorithm
2.5 Averaged Perceptron
The perceptron algorithm can be viewed as a form
of incremental training procedure (e.g., using sto-
chastic approximation) that optimizes a minimum
square error (MSE) loss function (Mitchell, 1997).
As shown in Figure 3, it starts with an initial pa-
rameter setting and updates it for each training
example. In our experiments, we used the Averaged
Perceptron algorithm of Freund and Schapire
(1999), a variation that has been shown to be more
effective than the standard algorithm (Collins
2002). Let 𝒘𝑡,𝑖 be the parameter vector after the 𝑖th
training sample has been processed in pass 𝑡 over
the training data. The average parameters are de-
fined as𝒘 = 𝟏 𝑻𝑵 𝒘𝒕,𝒊
𝒕 𝒊 where T is the number of
epochs, and N is the number of training samples.
3 Evaluations
From the four tasks we consider, parsing and lan-
guage model adaptation are both examples of
re-ranking. In these tasks, we assume that we have
been given a list of candidates 𝑮𝑬𝑵(𝑥) for each
training or test sample 𝑥, 𝑦 , generated using a
</figureCaption>
<table confidence="0.881731916666667">
baseline model. Then, a linear model of the form in
Equation (1) is used to discriminatively re-rank the
candidate list using additional features which may
or may not be included in the baseline model. Since
828
F-Score # features time (min) # train iter
Baseline 0.8986
ME/L2 0.9176 1,211,026 62 129
ME/L1 0.9165 19,121 37 174
AP 0.9164 939,248 2 8
Boosting 0.9131 6,714 495 92,600
BLasso 0.9133 8,085 239 56,500
</table>
<tableCaption confidence="0.964488">
Table 1: Performance summary of estimators on
parsing re-ranking (ME/L2: ME with L2 regulari-
zation; ME/L1: ME with L1 regularization)
</tableCaption>
<table confidence="0.999381333333333">
ME/L2 ME/L1 AP Boost BLasso
ME/L2 &gt;&gt; — &gt;&gt; &gt;&gt;
ME/L1 &lt;&lt; — &gt; —
AP — — &gt;&gt; &gt;
Boost &lt;&lt; &lt; &lt;&lt; —
Blasso &lt;&lt; — &lt; —
</table>
<tableCaption confidence="0.785338666666667">
Table 2: Statistical significance test results (“&gt;&gt;”
or “&lt;&lt;” means P-value &lt; 0.01; &gt; or &lt; means 0.01 &lt;
P-value  0.05; “~” means P-value &gt; 0.05)
</tableCaption>
<bodyText confidence="0.999941842105263">
ture weights w on Sections 2-19 of the Penn Tree-
bank, adjusted the regularizer constant a to max-
imize the F-Score on Sections 20-21 of the Tree-
bank, and evaluated the rerankers on Section 22.
The results are presented in Tables 12 and 2, where
Baseline results were obtained using the parser by
Charniak (2000).
The ME estimation with L2 regularization out-
performs all of the other estimators significantly
except for the AP, which performs almost as well
and requires an order of magnitude less time in
training. Boosting and BLasso are feature selection
methods in nature, so they achieve the sparsest
models, but at the cost of slightly lower perfor-
mance and much longer training time. The
L1-regularized ME estimator also produces a rela-
tively sparse solution whereas the Averaged Per-
ceptron and the L2-regularized ME estimator assign
almost all features a non-zero weight.
</bodyText>
<subsectionHeader confidence="0.999649">
3.2 Language model adaptation
</subsectionHeader>
<bodyText confidence="0.998623142857143">
Our experiments with LM adaptation are based on
the work described in Gao et al. (2006). The va-
riously trained language models were evaluated
according to their impact on Japanese text input
accuracy, where input phonetic symbols x are
mapped into a word string y. Performance of the
application is measured in terms of character error
</bodyText>
<footnote confidence="0.990515">
2 The result of ME/L2 is better than that reported in
Andrew and Gao (2007) due to the use of the variant of
L2-regularized ME estimator, as described in Section 2.1.
</footnote>
<table confidence="0.999717375">
CER # features time (min) #train iter
Baseline 10.24%
MAP 7.98%
ME/L2 6.99% 295,337 27 665
ME/L1 7.01% 53,342 25 864
AP 7.23% 167,591 6 56
Boost 7.54% 32,994 175 71,000
BLasso 7.20% 33,126 238 250,000
</table>
<tableCaption confidence="0.984507">
Table 3. Performance summary of estimators
(lower is better) on language model adaptation
</tableCaption>
<table confidence="0.993394166666667">
ME/L2 ME/L1 AP Boost BLasso
ME/L2 — &gt;&gt; &gt;&gt; &gt;&gt;
ME/L1 — &gt;&gt; &gt;&gt; &gt;&gt;
AP &lt;&lt; &lt;&lt; &gt;&gt; —
Boost &lt;&lt; &lt;&lt; &lt;&lt; &lt;&lt;
BLasso &lt;&lt; &lt;&lt; — &gt;&gt;
</table>
<tableCaption confidence="0.999687">
Table 4. Statistical significance test results.
</tableCaption>
<bodyText confidence="0.999789575757576">
rate (CER), which is the number of characters
wrongly converted from x divided by the number of
characters in the correct transcript.
Again we evaluated five linear rerankers, one for
each estimator. These rerankers attempt to select the
best conversions y for an input phonetic string x
from a 100-best list GEN(x)of possible conver-
sions proposed by a baseline system. The linear
model combines the log probability under a trigram
language model as base feature and additional
865,190 word uni/bi-gram features. These
uni/bi-gram features were already included in the
trigram model which was trained on a background
domain corpus (Nikkei Newspaper). But in the
linear model their feature weights were trained
discriminatively on an adaptation domain corpus
(Encarta Encyclopedia). Thus, this forms a cross
domain adaptation paradigm. This also implies that
the portion of redundant features in this task could
be much larger than that in the parse re-ranking
task, especially because the background domain is
reasonably similar to the adaptation domain.
We divided the Encarta corpus into three sets
that do not overlap. A 72K-sentences set was used
as training data, a 5K-sentence set as development
data, and another 5K-sentence set as testing data.
The results are presented in Tables 3 and 4, where
Baseline is the word-based trigram model trained
on background domain corpus, and MAP (maxi-
mum a posteriori) is a traditional model adaptation
method, where the parameters of the background
model are adjusted so as to maximize the likelihood
of the adaptation data.
</bodyText>
<page confidence="0.99618">
829
</page>
<table confidence="0.99979925">
Test F1 # features # train iter
ME/L2 0.9719 8,084,086 713
ME/L1 0.9713 317,146 201
AP 0.9703 1,965,719 162
</table>
<tableCaption confidence="0.9415535">
Table 5. Performance summary of estimators on
CWS
</tableCaption>
<bodyText confidence="0.999975866666667">
The results are more or less similar to those in
the parsing task with one visible difference: L1
regularization achieved relatively better perfor-
mance in this task. For example, while in the
parsing task ME with L2 regularization significantly
outperforms ME with L1 regularization, their per-
formance difference is not significant in this task.
While in the parsing task the performance differ-
ence between BLasso and Boosting is not signifi-
cant, BLasso outperforms Boosting significantly in
this task. Considering that a much higher propor-
tion of the features are redundant in this task than
the parsing task, the results seem to corroborate the
observation that L1 regularization is robust to the
presence of many redundant features.
</bodyText>
<subsectionHeader confidence="0.997059">
3.3 Chinese word segmentation
</subsectionHeader>
<bodyText confidence="0.999947128205128">
Our third task is Chinese word segmentation
(CWS). The goal of CWS is to determine the
boundaries between words in a section of Chinese
text. The model we used is the hybrid Mar-
kov/semi- Markov CRF described by Andrew
(2006), which was shown to have state-of-the-art
accuracy. We tested models trained with the various
estimation methods on the Microsoft Research Asia
corpus from the Second International Chinese Word
Segmentation, and we used the same train/test split
used in the competition. The model and experi-
mental setup is identical with that of Andrew (2006)
except for two differences. First, we extracted
features from both positive and negative training
examples, while Andrew (2006) uses only features
that occur in some positive training example.
Second, we used the last 4K sentences of the
training data to select the weight of the regularizers
and to determine when to stop perceptron training.
We compared three of the best performing es-
timation procedures on this task: ME with L2 regu-
larization, ME with L1 regularization, and the Av-
eraged Perceptron. In this case, ME refers to mi-
nimizing the negative log-probability of the correct
segmentation, which is globally normalized, while
the perceptron is trained using at each iteration the
exact maximum-scoring segmentation with the
current weights. We observed the same pattern as in
the other tasks: the three algorithms have nearly
identical performance, while L1 uses only 6% of the
features, and the Averaged Perceptron requires
significantly fewer training iterations. In this case,
L1 was also several times faster than L2. The results
are summarized in Table 5.3
We note that all three algorithms performed
slightly better than the model used by Andrew
(2006), which also used L2 regularization (96.84
F1). We believe the difference is due to the use of
features derived from negative training examples.
</bodyText>
<subsectionHeader confidence="0.969849">
3.4 POS tagging
</subsectionHeader>
<bodyText confidence="0.999789333333333">
Finally we studied the impact of the regularization
methods on a Maximum Entropy conditional
Markov Model (MEMM, McCallum et al. 2000) for
POS tagging. MEMMs decompose the conditional
probability of a tag sequence given a word sequence
as follows:
</bodyText>
<equation confidence="0.981782666666667">
n
P 4 ... tnIW1 . ..Wn) = fjP(ti|ti−1 ... ti−k,W1 . ..Wn)
i=1
</equation>
<bodyText confidence="0.998170333333333">
where the probability distributions for each tag
given its context are ME models. Following pre-
vious work (Ratnaparkhi, 1996), we assume that the
tag of a word is independent of the tags of all pre-
ceding words given the tags of the previous two
words (i.e., k=2 in the equation above). The local
models at each position include features of the
current word, the previous word, the next word, and
features of the previous two tags. In addition to
lexical identity of the words, we used features of
word suffixes, capitalization, and number/special
character signatures of the words.
We used the standard splits of the Penn Treebank
from the tagging literature (Toutanova et al. 2003)
for training, development and test sets. The training
set comprises Sections 0-18, the development set —
Sections 19-21, and the test set — Sections 22-24.
We compared training the ME models using L1 and
L2 regularization. For each of the two types of
regularization we selected the best value of the
regularization constant using grid search to optim-
ize the accuracy on the development set. We report
final accuracy measures on the test set in Table 6.
The results on this task confirm the trends we
have seen so far. There is almost no difference in
3 Only the L2 vs. AP comparison is significant at a 0.05
level according to the Wilcoxon signed rank test.
</bodyText>
<page confidence="0.988603">
830
</page>
<table confidence="0.995244">
Accuracy (%) # features # train iter
MEMM/L2 96.39 926,350 467
MEMM/L1 96.41 84,070 85
</table>
<tableCaption confidence="0.7785345">
Table 6. Performance summary of estimators on
POS tagging
</tableCaption>
<bodyText confidence="0.999911333333333">
accuracy of the two kinds of regularizations, and
indeed the differences were not statistically signif-
icant. Estimation with L1 regularization required
considerably less time than estimation with L2, and
resulted in a model which is more than ten times
smaller.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999990185185185">
We compared five of the most competitive para-
meter estimation methods on four NLP tasks em-
ploying a variety of models, and the results were
remarkably consistent across tasks. Three of the
methods — ME estimation with L2 regularization,
ME estimation with L1 regularization, and the Av-
eraged Perceptron — were nearly indistinguishable
in terms of test set accuracy, with ME estimation
with L2 regularization perhaps enjoying a slight
lead. Meanwhile, ME estimation with L1 regulari-
zation achieves the same level of performance while
at the same time producing sparse models, and the
Averaged Perceptron provides an excellent com-
promise of high performance and fast training.
These results suggest that when deciding which
type of parameter estimation to use on these or
similar NLP tasks, one may choose any of these
three popular methods and expect to achieve com-
parable performance. The choice of which to im-
plement should come down to other considerations:
if model sparsity is desired, choose ME estimation
with L1 regularization (or feature selection methods
such as BLasso); if quick implementation and
training is necessary, use the Averaged Perceptron;
and ME estimation with L2 regularization may be
used if it is important to achieve the highest ob-
tainable level of performance.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870415384615">
Andrew, G. 2006. A hybrid Markov/semi-Markov condi-
tional random field for sequence segmentation. In EMNLP,
465-472.
Andrew, G. and Gao, J. 2007. Scalable training of
L1-regularized log-linear models. In ICML.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
NAACL, 132-139.
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative re-ranking. In ACL.
173-180.
Chen, S.F., and Rosenfeld, R. 2000. A survey of smoothing
techniques for ME models. IEEE Trans. On Speech and Audio
Processing, 8(2): 37-50.
Collins, M. 2000. Discriminative re-ranking for natural
language parsing. In ICML, 175-182.
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with per-
ceptron algorithms. In EMNLP, 1-8.
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An
efficient boosting algorithm for combining preferences. In
ICML’98.
Freund, Y. and Schapire, R. E. 1999. Large margin classifica-
tion using the perceptron algorithm. In Machine Learning,
37(3): 277-296.
Hastie, T., R. Tibshirani and J. Friedman. 2001. The elements of
statistical learning. Springer-Verlag, New York.
Gao, J., Suzuki, H., and Yu, B. 2006. Approximation lasso
methods for language modeling. In ACL.
Goodman, J. 2004. Exponential priors for maximum entropy
models. In NAACL.
Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S.
1999. Estimators for stochastic “Unification-based”
grammars. In ACL.
Kazama, J. and Tsujii, J. 2003. Evaluation and extension of
maximum entropy models with inequality constraints. In
EMNLP.
Malouf, R. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In HLT.
McCallum A, D. Freitag and F. Pereira. 2000. Maximum
entropy markov models for information extraction and
segmentation. In ICML.
Mitchell, T. M. 1997. Machine learning. The McGraw-Hill
Companies, Inc.
Ng, A. Y. 2004. Feature selection, L1 vs. L2 regularization,
and rotational invariance. In ICML.
Nocedal, J., and Wright, S. J. 1999. Numerical Optimization.
Springer, New York.
Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In IJCAI.
Ratnaparkhi, A. 1996. A maximum entropy part-of-speech
tagger. In EMNLP.
Riezler, S., and Vasserman, A. 2004. Incremental feature
selection and L1 regularization for relax maximum entro-
py modeling. In EMNLP.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J.,
and Johnson, M. 2002. Parsing the wall street journal using
a lexical-functional grammar and discriminative estima-
tion techniques. In ACL. 271-278.
Tibshirani, R. 1996. Regression shrinkage and selection via
the lasso. J. R. Statist. Soc. B, 58(1): 267-288.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
2003. Feature-rich Part-of-Speech tagging with a cyclic
dependency network. In HLT-NAACL, 252-259.
Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, Statistics
Department, U. C. Berkeley.
</reference>
<page confidence="0.998407">
831
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000271">
<title confidence="0.999785">A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing</title>
<author confidence="0.998349">Galen Mark Kristina</author>
<address confidence="0.957133">Research, Redmond WA 98052, University, Providence, RI 02912,</address>
<abstract confidence="0.999778062499999">This paper presents a comparative study of five parameter estimation algorithms on four NLP tasks. Three of the five algorithms are well-known in the computational linguistics community: Maximum Entropy (ME) estimawith the Averaged Perceptron (AP), and Boosting. We also in- ME estimation with using a novel optimization algorithm, and BLasso, which is a version of Boosting with regularization. We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model (LM) adaptation task. Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a Conditional Markov Model (CMM) for part of speech tagging and a Conditional Random Field (CRF) for Chinese word segmentation. Our experiments show that across tasks, three the estimators estimation with and AP in a near statistical tie for first place. Parameter estimation is fundamental to many statistical approaches to NLP. Because of the high-dimensional nature of natural language, it is often easy to generate an extremely large number of features. The challenge of parameter estimation is to find a combination of the typically noisy, redundant features that accurately predicts the target output variable and avoids overfitting. Intuitively, this can be achieved either by selecting a small number of highly-effective features and ignoring the others, or by averaging over a large number of weakly informative features. The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many are completely irrelevant. Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the ME estimators with which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). recent studies claim advantages for regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic outperforms regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estiwith a one-sided Laplacian prior (i.e., regularization with the constraint that all feature weights are positive) outperformed a comparable using a Gaussian prior (i.e., regularization). Riezler and Vasserman (2004) showed that ME estimator outperformed an estimator for ranking the parses of a stochastic unification-based grammar.</abstract>
<note confidence="0.560282666666667">824 of the 45th Annual Meeting of the Association of Computational pages 824–831, Czech Republic, June 2007. Association for Computational Linguistics</note>
<abstract confidence="0.994105551851851">While these individual estimators are well described in the literature, little is known about the relative performance of these methods because the published results are generally not directly comparable. For example, in the parse re-ranking task, cannot tell whether the regularized ME approach used by Charniak and Johnson (2005) significantly outperforms the Boosting method by Collins (2000) because different feature sets and parses were used in the evaluations of these methods. This paper conducts a much-needed comparative study of these five parameter estimation algorithms four NLP tasks: ME estimation with regularization, the Averaged Perceptron (AP), Boosting, and BLasso, a version of Boosting with regularization. We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model adaptation task. Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a CMM for POS tagging and a CRF for Chinese word segmentation. Our results show that estimation with achieves the best performing estimators in all of the tasks, and AP achieves almost as well and requires much less time. regularization also performs well and leads to sparser models. All the four NLP tasks studied in this paper are based on linear models (Collins 2000) which relearning a mapping from inputs ∈ 𝑋 ∈ We are given: Training samples 1 A procedure generate a set of candian input A feature mapping ↦ map a vector of feature values, and A parameter vector ∈ which assigns a real-valued weight to each feature. For all models except the CMM sequence model for tagging, the components didefine a mapping from an input an output follows: 𝑥 arg 𝑋 ⋅ (1) In the CMM sequence classifier, locally normalized linear models to predict the tag of each word token chained together to arrive at a probability estimate for the entire tag sequence, resulting in a slightly different decision rule. Linear models, though simple, can capture very complex dependencies because the features can be arbitrary functions of the input/output pair. For example, we can define a feature to be the log conditional probability of the output as estimated by some other model, which may in turn depend on complex interactions of features. In practice, with an appropriate feature set, linear models achieve very good empirical results on various NLP tasks. The focus of this paper however is not on feature definition (which requires domain knowledge and varies from task to task), but on parameter estimation (which is generic across tasks). We assume we are given fixed feature templates from which a large number of features are generated. The task of the estimator is to use the samples to choose a parameter vector that the mapping capable of correctly classifying unseen examples. We will describe the five estimators in our study individually. estimation with Like many linear models, the ME estimator chooses minimize the sum of the empirical loss on the training set and a regularization term: arg 𝒘 𝒘 (2) this case, the loss term is the negative conditional log-likelihood of the training data, 𝐿 𝒘 = where 𝑃 𝑦 = 𝑖=1 ⋅ 𝒘 𝑥 the regularizer term 𝒘 the squared of the parameters. Here, a parameter that controls the amount of regularization, optimized on held-out data. This is one of the most popular estimators, largely due to its appealing computational properboth 𝒘 convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. our experiments, we used the memory (or L-BFGS, Nocedal and 1999) to find the optimal this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). 825 Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in et al. (2002), where defined as the of the best parses E to parser output 9; We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement performance for the ME estimator not for the ME estimator. estimation with This estimator also minimizes the negative condilog-likelihood, but uses an Lasso) That is, Equation (2) is defined to (w) typically leads to sparse solutions in which many feature weights are exactly zero, so it is a natural candidate when feature selection is desirable. By produces solutions in which most weights are small but non-zero. the objective function is challenging because its gradient is discontinuous whenever some parameter equals zero. Kazama and Tsujii (2003) described an estimation method that constructs an equivalent constrained optimization problem with twice the number of variables. However, we found that this method is impractically slow for large-scale NLP tasks. In this work use the limited-memory qua- (OWL-QN), which is a modification of L-BFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao 2007). We provide here a high-level description of the algorithm. A quasi-Newton method such as L-BFGS uses first order information at each iterate to build an to the Hessian matrix, thus modeling the local curvature of the function. At each step, a search direction is chosen by minimizing a quadratic approximation to the function: L-BFGS maintains vectors of the change in gradient the most recent iterations, and uses them to construct an estimate of the inverse Hessian Furthermore, it does so in such a way that be computed without expanding out the full matrix, which is typically unmanageably large. The computation requires a number of operations linear in the number of variables. OWL-QN is based on the observation that when to a single orthant, the and is in fact a linear function of Thus, so long as each coordinate of any two consecutive search points does not pass through zero, not contribute at all to the curvature of the function on the segment joining them. Therefore, we can use L-BFGS to approximate the Hesof and use it to build an approximation to the full regularized objective that is valid on a given orthant. To ensure that the next point is in the valid region, we project each point during the search back onto the chosen At each iteration, we choose the orthant containing the current point and into which the direction giving the greatest local rate of function decrease points. This algorithm, although only a simple modification of L-BFGS, works quite well in practice. It typically reaches convergence in even fewer iterations than standard L-BFGS takes on the analogous objective (which translates to less training time, since the time per iteration is only negligibly higher, and total time is dominated by function evaluations). We describe OWL-QN more fully in (Andrew and Gao 2007). We also show that is significantly faster than and Tsujii‟s for and prove that it is guaranteed converge to a parameter vector that optimizes the objective. The Boosting algorithm we used is based on Collins It optimizes the pairwise loss (ExpLoss) function (rather than the logarithmic loss optimized by ME). Given a training sample for each possible output GEN we − (x − − 1 the current iterate, and the funcgradient at If positive definite, the value of be computed analytically to: = projection just entails zeroing-out any coordinates that change sign. Note that it is possible for a variable to change sign in two iterations, by moving from a negative value to zero, and on a the next iteration moving from zero to a positive value. 826 Set and 0 for Select a feature which has largest estimated impact on reducing ExpLoss of Equation (3) Update and return to Step 2 Figure 1: The boosting algorithm the margin of the pair respect to 𝒘− Then ExpLoss is defined as exp Figure 1 summarizes the Boosting algorithm we used. It is an incremental feature selection procedure. After initialization, Steps 2 and 3 are repeated at each iteration, a feature is chosen and its weight is updated as follows. we define an updated with the same parameter values as exception of which is incremented by Then, Steps 2 and 3 in Figure 1 can be rewritten as Equations (4) and (5), respectively. arg min ExpLoss(Upd Because Boosting can overfit we update the weight a small fixed step size as in Equation (6), following the FSLR algorithm (Hastie et al. 2001). sign By taking such small steps, Boosting imposes a kind of implicit regularization, and can closely the effect of in a local sense (Hastie et al. 2001). Empirically, smaller of to smaller numbers of test errors. Lasso The Boosted Lasso (BLasso) algorithm was originally proposed in Zhao and Yu (2004), and was adapted for language modeling by Gao et al. (2006). BLasso can be viewed as a version of Boosting with It optimizes an ExpLoss function: + 𝒘 BLasso also uses an incremental feature selecprocedure to learn parameter vector just as Boosting does. Due to the explicit use of the reguterm however, there are two major differences from Boosting. each iteration, BLasso takes either a a Similar to Boosting, at each forward step, a feature is selected and its weight is updated according to Eq. (8) and (9). 𝑚𝑖𝑛 sign There is a small but important difference between Equations (8) and (4). In Boosting, as shown in Equation (4), a feature is selected by its impact on the loss with its optimal update By contrast, in BLasso, as shown in Equation (8), than optimizing over each feature, the is calculated with an update of either i.e., grid search is used for feature weight estimation. We found in our experiments that this modification brings a consistent improvement. The backward step is unique to BLasso. At each iteration, a feature is selected and the absolute value its weight is reduced by and only if it leads to a decrease of the LassoLoss, as shown in Equations and (11), where a tolerance parameter. arg min × LassoLoss Figure 2 summarizes the BLasso algorithm we used. After initialization, Steps 4 and 5 are repeated at each iteration, a feature is chosen and its weight is updated either backward or forward by a amount Notice that the value of adaptively chosen according to the reduction of ExpLoss during training. The algorithm starts with a large and then at each forward step the value of until ExpLoss stops decreasing. This is intuitively desirable: it is expected that most highly effective features are selected in early stages of training, so the reduction of ExpLoss at each step in early stages are more substantial than in later stages. These early steps coincide with the Boosting steps most of the time. In other words, the effect of backward steps is more visible at later stages. It can be proved that for a finite number of features and the BLasso algorithm shown in Figure 2 to the Lasso solution when → See Gao et al. (2006) for implementation details, and Zhao and Yu (2004) for a theoretical justification for BLasso. 827 Initialize set and 0 mapping from the linear model may make use of arbitrary global features of the output is performed “all at once”, call such a linear a In the other two tasks (i.e., Chinese word seg-mentation and POS tagging), there is no explicit of The mapping from is determined by a sequence model which aggre-gates the decisions of local linear models via a dynamic program. In the CMM, the local linear models are trained independently, while in the CRF model, the local models are trained jointly. We call two linear models models they dynamically combine the output of models that use only local features. 2 Take a forward step according to Eq. (8) and (9), and While it is straightforward to apply the five es-timators to global models in the re-ranking framework, the application of some estimators to the local models is problematic. Boosting and BLasso are too computationally expensive to be applied to CRF training and we compared the other three better performing estimation methods for this model. The CMM is a probabilistic sequence model and the log-loss used by ME estimation is most natural for it; thus we limit the comparison to the two kinds of ME models for CMMs. Note that our goal is not to compare locally trained models to globally trained ones; for a study which focuses on this issue, see (Punyakanok et al. 2005). updated model is denoted by In each task we compared the performance of different estimators using task-specific measures. We used the Wilcoxon signed rank test to test the statistical significance of the difference among the competing estimators. We also report other results such as number of non-zero features after estima-tion, number of training iterations, and computation (in minutes of elapsed time on an MP 3.6GHz machine). Initialize re-ranking Take a backward step if and only if it leads to a de- We follow the experimental paradigm of parse re-ranking outlined in Charniak and Johnson (2005), and fed the features extracted by their pro-gram to the five rerankers we developed. Each uses a linear model trained using one of the five esti-mators. These rerankers attempt to select the best a sentence the 50-best list of parses 𝑥 the sentence. The li-near model combines the log probability calculated by the Charniak (2000) parser as a feature with additional features. We trained the feacrease of LassoLoss according to Eq. (10) and (11), 0; otherwise 5 Take a forward step according to Eq. (8) and (9); and return to Step 4. Figure 2: The BLasso algorithm Set 1 and 0 for For the total number of iterations) For each training sample Choose the best candidate using the current model w, arg max ⋅ 𝑤 𝑥 w = w + where the size of learning step, optimized on held-out data. Figure 3: The perceptron algorithm Perceptron The perceptron algorithm can be viewed as a form of incremental training procedure (e.g., using stoapproximation) that optimizes a error loss function (Mitchell, 1997). shown in Figure starts with an initial pa-rameter setting and updates it for each training example. In our experiments, we used the Averaged Perceptron algorithm of Freund and Schapire (1999), a variation that has been shown to be more effective than the standard algorithm (Collins Let the parameter vector after the sample has been processed in pass training data. The average parameters are de- 𝑻𝑵 𝒊 the number of and the number of training samples. F-Score # features time (min) # train iter Baseline 0.8986 ME/L2 0.9176 1,211,026 62 129 ME/L1 0.9165 19,121 37 174 AP 0.9164 939,248 2 8 Boosting 0.9131 6,714 495 92,600 BLasso 0.9133 8,085 239 56,500 Table 1: Performance summary of estimators on re-ranking (ME/L2: ME with regulari- ME/L1: ME with ME/L2 ME/L1 AP Boost BLasso ME/L2 &gt;&gt; — &gt;&gt; &gt;&gt; ME/L1 &lt;&lt; — &gt; — AP — — &gt;&gt; &gt; Boost &lt;&lt; &lt; &lt;&lt; — Blasso &lt;&lt; — &lt; — 2: significance test results (“&gt;&gt;” “&lt;&lt;” means &lt; 0.01; &gt; or &lt; means 0.01 &lt; “~” means &gt; 0.05) weights Sections 2-19 of the Penn Treeadjusted the regularizer constant maximize the F-Score on Sections 20-21 of the Treebank, and evaluated the rerankers on Section 22. results are presented in Tables and 2, where were obtained using the parser by Charniak (2000). ME estimation with outperforms all of the other estimators significantly except for the AP, which performs almost as well and requires an order of magnitude less time in training. Boosting and BLasso are feature selection methods in nature, so they achieve the sparsest models, but at the cost of slightly lower performance and much longer training time. The ME estimator also produces a relatively sparse solution whereas the Averaged Perand the ME estimator assign almost all features a non-zero weight. model adaptation Our experiments with LM adaptation are based on the work described in Gao et al. (2006). The variously trained language models were evaluated according to their impact on Japanese text input where input phonetic symbols into a word string Performance of the application is measured in terms of character error 2The result of ME/L2 is better than that reported in Andrew and Gao (2007) due to the use of the variant of ME estimator, as described in Section 2.1. CER # features time (min) #train iter Baseline 10.24% MAP 7.98% ME/L2 6.99% 295,337 27 665 ME/L1 7.01% 53,342 25 864 AP 7.23% 167,591 6 56 Boost 7.54% 32,994 175 71,000 BLasso 7.20% 33,126 238 250,000 Table 3. Performance summary of estimators (lower is better) on language model adaptation ME/L2 ME/L1 AP Boost BLasso ME/L2 — &gt;&gt; &gt;&gt; &gt;&gt; ME/L1 — &gt;&gt; &gt;&gt; &gt;&gt; AP &lt;&lt; &lt;&lt; &gt;&gt; — Boost &lt;&lt; &lt;&lt; &lt;&lt; &lt;&lt; BLasso &lt;&lt; &lt;&lt; — &gt;&gt; Table 4. Statistical significance test results. rate (CER), which is the number of characters converted from by the number of characters in the correct transcript. Again we evaluated five linear rerankers, one for each estimator. These rerankers attempt to select the conversions an input phonetic string a 100-best list possible conversions proposed by a baseline system. The linear model combines the log probability under a trigram language model as base feature and additional 865,190 word uni/bi-gram features. These uni/bi-gram features were already included in the model which was trained on a domain corpus (Nikkei Newspaper). But in the linear model their feature weights were trained on an corpus (Encarta Encyclopedia). Thus, this forms a cross domain adaptation paradigm. This also implies that the portion of redundant features in this task could be much larger than that in the parse re-ranking task, especially because the background domain is reasonably similar to the adaptation domain. We divided the Encarta corpus into three sets that do not overlap. A 72K-sentences set was used as training data, a 5K-sentence set as development data, and another 5K-sentence set as testing data. The results are presented in Tables 3 and 4, where the word-based trigram model trained background domain corpus, and (maxiis a traditional model adaptation method, where the parameters of the background model are adjusted so as to maximize the likelihood of the adaptation data. 829 ME/L2 0.9719 8,084,086 713 ME/L1 0.9713 317,146 201 AP 0.9703 1,965,719 162 Table 5. Performance summary of estimators on CWS The results are more or less similar to those in parsing task with one visible difference: regularization achieved relatively better performance in this task. For example, while in the task ME with significantly ME with their performance difference is not significant in this task. While in the parsing task the performance difference between BLasso and Boosting is not significant, BLasso outperforms Boosting significantly in this task. Considering that a much higher proportion of the features are redundant in this task than the parsing task, the results seem to corroborate the that is robust to the presence of many redundant features. word segmentation Our third task is Chinese word segmentation (CWS). The goal of CWS is to determine the boundaries between words in a section of Chinese text. The model we used is the hybrid Markov/semi- Markov CRF described by Andrew (2006), which was shown to have state-of-the-art accuracy. We tested models trained with the various estimation methods on the Microsoft Research Asia corpus from the Second International Chinese Word Segmentation, and we used the same train/test split used in the competition. The model and experimental setup is identical with that of Andrew (2006) except for two differences. First, we extracted features from both positive and negative training examples, while Andrew (2006) uses only features that occur in some positive training example. Second, we used the last 4K sentences of the training data to select the weight of the regularizers and to determine when to stop perceptron training. We compared three of the best performing esprocedures on this task: ME with regu- ME with and the Averaged Perceptron. In this case, ME refers to minimizing the negative log-probability of the correct segmentation, which is globally normalized, while the perceptron is trained using at each iteration the exact maximum-scoring segmentation with the current weights. We observed the same pattern as in the other tasks: the three algorithms have nearly performance, while only 6% of the features, and the Averaged Perceptron requires significantly fewer training iterations. In this case, also several times faster than The results summarized in Table We note that all three algorithms performed slightly better than the model used by Andrew which also used (96.84 We believe the difference is due to the use of features derived from negative training examples. tagging Finally we studied the impact of the regularization methods on a Maximum Entropy conditional Markov Model (MEMM, McCallum et al. 2000) for POS tagging. MEMMs decompose the conditional probability of a tag sequence given a word sequence as follows: n 4 ... . . where the probability distributions for each tag given its context are ME models. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two (i.e., the equation above). The local models at each position include features of the current word, the previous word, the next word, and features of the previous two tags. In addition to lexical identity of the words, we used features of word suffixes, capitalization, and number/special character signatures of the words. We used the standard splits of the Penn Treebank from the tagging literature (Toutanova et al. 2003) for training, development and test sets. The training comprises Sections 0-18, the development set 19-21, and the test set 22-24. compared training the ME models using For each of the two types of regularization we selected the best value of the regularization constant using grid search to optimize the accuracy on the development set. We report final accuracy measures on the test set in Table 6. The results on this task confirm the trends we have seen so far. There is almost no difference in the L2 vs. AP comparison is significant at a 0.05 level according to the Wilcoxon signed rank test. 830 Accuracy (%) # features # train iter MEMM/L2 96.39 926,350 467 MEMM/L1 96.41 84,070 85 Table 6. Performance summary of estimators on POS tagging accuracy of the two kinds of regularizations, and indeed the differences were not statistically signif- Estimation with required less time than estimation with and resulted in a model which is more than ten times smaller. We compared five of the most competitive parameter estimation methods on four NLP tasks employing a variety of models, and the results were remarkably consistent across tasks. Three of the estimation with estimation with and the Av- Perceptron nearly indistinguishable in terms of test set accuracy, with ME estimation perhaps enjoying a slight Meanwhile, ME estimation with regularization achieves the same level of performance while at the same time producing sparse models, and the Averaged Perceptron provides an excellent compromise of high performance and fast training. These results suggest that when deciding which type of parameter estimation to use on these or similar NLP tasks, one may choose any of these three popular methods and expect to achieve comparable performance. The choice of which to implement should come down to other considerations: if model sparsity is desired, choose ME estimation (or feature selection methods such as BLasso); if quick implementation and training is necessary, use the Averaged Perceptron; ME estimation with may be used if it is important to achieve the highest obtainable level of performance.</abstract>
<note confidence="0.8244183">References Andrew, G. 2006. A hybrid Markov/semi-Markov condirandom field for sequence segmentation. In 465-472. Andrew, G. and Gao, J. 2007. Scalable training of log-linear models. In Charniak, E. 2000. A maximum-entropy-inspired parser. In 132-139. E. and Johnson, M. 2005. Coarse-to-fine and MaxEnt discriminative re-ranking. In 173-180. Chen, S.F., and Rosenfeld, R. 2000. A survey of smoothing for ME models. Trans. On Speech and Audio 8(2): 37-50. Collins, M. 2000. Discriminative re-ranking for natural parsing. In 175-182. Collins, M. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with peralgorithms. In 1-8. Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An</note>
<abstract confidence="0.9098686">efficient boosting algorithm for combining preferences. In ICML’98. Freund, Y. and Schapire, R. E. 1999. Large margin classificausing the perceptron algorithm. In 37(3): 277-296. T., R. Tibshirani and J. Friedman. 2001. elements of learning. New York. Gao, J., Suzuki, H., and Yu, B. 2006. Approximation lasso for language modeling. In Goodman, J. 2004. Exponential priors for maximum entropy In Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S. Estimators for stochastic In Kazama, J. and Tsujii, J. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Malouf, R. 2002. A comparison of algorithms for maximum parameter estimation. In McCallum A, D. Freitag and F. Pereira. 2000. Maximum entropy markov models for information extraction and</abstract>
<note confidence="0.708069625">In T. M. 1997. The McGraw-Hill Companies, Inc. A. Y. 2004. Feature selection, rotational invariance. In J., and Wright, S. J. 1999. Springer, New York. Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005.</note>
<abstract confidence="0.929975916666666">and inference over constrained output. In Ratnaparkhi, A. 1996. A maximum entropy part-of-speech tagger. In EMNLP. Riezler, S., and Vasserman, A. 2004. Incremental feature and for relax maximum entromodeling. In Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J., and Johnson, M. 2002. Parsing the wall street journal using a lexical-functional grammar and discriminative estimatechniques. In 271-278. Tibshirani, R. 1996. Regression shrinkage and selection via lasso. R. Statist. Soc. 58(1): 267-288.</abstract>
<note confidence="0.5673335">Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. 2003. Feature-rich Part-of-Speech tagging with a cyclic network. In P. and B. Yu. 2004. Boosted lasso. Statistics</note>
<affiliation confidence="0.789087">Department, U. C. Berkeley.</affiliation>
<address confidence="0.587158">831</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Andrew</author>
</authors>
<title>A hybrid Markov/semi-Markov conditional random field for sequence segmentation.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>465--472</pages>
<contexts>
<context position="27024" citStr="Andrew (2006)" startWordPosition="4611" endWordPosition="4612">erformance difference between BLasso and Boosting is not significant, BLasso outperforms Boosting significantly in this task. Considering that a much higher proportion of the features are redundant in this task than the parsing task, the results seem to corroborate the observation that L1 regularization is robust to the presence of many redundant features. 3.3 Chinese word segmentation Our third task is Chinese word segmentation (CWS). The goal of CWS is to determine the boundaries between words in a section of Chinese text. The model we used is the hybrid Markov/semi- Markov CRF described by Andrew (2006), which was shown to have state-of-the-art accuracy. We tested models trained with the various estimation methods on the Microsoft Research Asia corpus from the Second International Chinese Word Segmentation, and we used the same train/test split used in the competition. The model and experimental setup is identical with that of Andrew (2006) except for two differences. First, we extracted features from both positive and negative training examples, while Andrew (2006) uses only features that occur in some positive training example. Second, we used the last 4K sentences of the training data to </context>
<context position="28537" citStr="Andrew (2006)" startWordPosition="4849" endWordPosition="4850">log-probability of the correct segmentation, which is globally normalized, while the perceptron is trained using at each iteration the exact maximum-scoring segmentation with the current weights. We observed the same pattern as in the other tasks: the three algorithms have nearly identical performance, while L1 uses only 6% of the features, and the Averaged Perceptron requires significantly fewer training iterations. In this case, L1 was also several times faster than L2. The results are summarized in Table 5.3 We note that all three algorithms performed slightly better than the model used by Andrew (2006), which also used L2 regularization (96.84 F1). We believe the difference is due to the use of features derived from negative training examples. 3.4 POS tagging Finally we studied the impact of the regularization methods on a Maximum Entropy conditional Markov Model (MEMM, McCallum et al. 2000) for POS tagging. MEMMs decompose the conditional probability of a tag sequence given a word sequence as follows: n P 4 ... tnIW1 . ..Wn) = fjP(ti|ti−1 ... ti−k,W1 . ..Wn) i=1 where the probability distributions for each tag given its context are ME models. Following previous work (Ratnaparkhi, 1996), we</context>
</contexts>
<marker>Andrew, 2006</marker>
<rawString>Andrew, G. 2006. A hybrid Markov/semi-Markov conditional random field for sequence segmentation. In EMNLP, 465-472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Andrew</author>
<author>J Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="10215" citStr="Andrew and Gao 2007" startWordPosition="1654" endWordPosition="1657">eights are small but non-zero. Optimizing the L1-regularized objective function is challenging because its gradient is discontinuous whenever some parameter equals zero. Kazama and Tsujii (2003) described an estimation method that constructs an equivalent constrained optimization problem with twice the number of variables. However, we found that this method is impractically slow for large-scale NLP tasks. In this work we use the orthant-wise limited-memory quasi-Newton algorithm (OWL-QN), which is a modification of L-BFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao 2007). We provide here a high-level description of the algorithm. A quasi-Newton method such as L-BFGS uses first order information at each iterate to build an approximation to the Hessian matrix, H, thus modeling the local curvature of the function. At each step, a search direction is chosen by minimizing a quadratic approximation to the function: L-BFGS maintains vectors of the change in gradient gk − gk−1 from the most recent iterations, and uses them to construct an estimate of the inverse Hessian H−1. Furthermore, it does so in such a way that H−1g0 can be computed without expanding out the fu</context>
<context position="12185" citStr="Andrew and Gao 2007" startWordPosition="1988" endWordPosition="1991">arch back onto the chosen orthant.1 At each iteration, we choose the orthant containing the current point and into which the direction giving the greatest local rate of function decrease points. This algorithm, although only a simple modification of L-BFGS, works quite well in practice. It typically reaches convergence in even fewer iterations than standard L-BFGS takes on the analogous L2-regularized objective (which translates to less training time, since the time per iteration is only negligibly higher, and total time is dominated by function evaluations). We describe OWL-QN more fully in (Andrew and Gao 2007). We also show that it is significantly faster than Kazama and Tsujii‟s algorithm for L1 regularization and prove that it is guaranteed converge to a parameter vector that globally optimizes the L1-regularized objective. 2.3 Boosting The Boosting algorithm we used is based on Collins (2000). It optimizes the pairwise exponential loss (ExpLoss) function (rather than the logarithmic loss optimized by ME). Given a training sample (xi, yi), for each possible output yj E GEN (xi), we Q(x) = 2 (x − x0 ) ′H (x − x0 ) + g0′ (x − x0) 1 where x0 is the current iterate, and g0 is the function gradient at</context>
<context position="23759" citStr="Andrew and Gao (2007)" startWordPosition="4073" endWordPosition="4076">gularized ME estimator also produces a relatively sparse solution whereas the Averaged Perceptron and the L2-regularized ME estimator assign almost all features a non-zero weight. 3.2 Language model adaptation Our experiments with LM adaptation are based on the work described in Gao et al. (2006). The variously trained language models were evaluated according to their impact on Japanese text input accuracy, where input phonetic symbols x are mapped into a word string y. Performance of the application is measured in terms of character error 2 The result of ME/L2 is better than that reported in Andrew and Gao (2007) due to the use of the variant of L2-regularized ME estimator, as described in Section 2.1. CER # features time (min) #train iter Baseline 10.24% MAP 7.98% ME/L2 6.99% 295,337 27 665 ME/L1 7.01% 53,342 25 864 AP 7.23% 167,591 6 56 Boost 7.54% 32,994 175 71,000 BLasso 7.20% 33,126 238 250,000 Table 3. Performance summary of estimators (lower is better) on language model adaptation ME/L2 ME/L1 AP Boost BLasso ME/L2 — &gt;&gt; &gt;&gt; &gt;&gt; ME/L1 — &gt;&gt; &gt;&gt; &gt;&gt; AP &lt;&lt; &lt;&lt; &gt;&gt; — Boost &lt;&lt; &lt;&lt; &lt;&lt; &lt;&lt; BLasso &lt;&lt; &lt;&lt; — &gt;&gt; Table 4. Statistical significance test results. rate (CER), which is the number of characters wrongly con</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Andrew, G. and Gao, J. 2007. Scalable training of L1-regularized log-linear models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In NAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="20136" citStr="Charniak (2000)" startWordPosition="3428" endWordPosition="3429">es after estimation, number of training iterations, and computation time (in minutes of elapsed time on an XEONTM MP 3.6GHz machine). 3.1 Parse re-ranking We follow the experimental paradigm of parse re-ranking outlined in Charniak and Johnson (2005), and fed the features extracted by their program to the five rerankers we developed. Each uses a linear model trained using one of the five estimators. These rerankers attempt to select the best parse 𝑦 for a sentence 𝑥 from the 50-best list of possible parses 𝑮𝑬𝑵 𝑥 for the sentence. The linear model combines the log probability calculated by the Charniak (2000) parser as a feature with 1,219,272 additional features. We trained the feaFigure 2: The BLasso algorithm 1 Set w0 = 1 and wd = 0 for d=1...D 2 For t = 1...T (T = the total number of iterations) 3 For each training sample (xi, yi), i = 1...N 4 Choose the best candidate zi from GEN(xi) using the current model w, 𝑧𝑖 = arg max Φ 𝑥𝑖, 𝑧 ⋅ 𝑤 𝑧∈𝐺𝐸𝑁 𝑥 _𝑖 5 w = w + η((D(xi, yi) – (D(xi, zi)), where η is the size of learning step, optimized on held-out data. Figure 3: The perceptron algorithm 2.5 Averaged Perceptron The perceptron algorithm can be viewed as a form of incremental training procedure (e.g.</context>
<context position="22750" citStr="Charniak (2000)" startWordPosition="3910" endWordPosition="3911">th L2 regularization; ME/L1: ME with L1 regularization) ME/L2 ME/L1 AP Boost BLasso ME/L2 &gt;&gt; — &gt;&gt; &gt;&gt; ME/L1 &lt;&lt; — &gt; — AP — — &gt;&gt; &gt; Boost &lt;&lt; &lt; &lt;&lt; — Blasso &lt;&lt; — &lt; — Table 2: Statistical significance test results (“&gt;&gt;” or “&lt;&lt;” means P-value &lt; 0.01; &gt; or &lt; means 0.01 &lt; P-value  0.05; “~” means P-value &gt; 0.05) ture weights w on Sections 2-19 of the Penn Treebank, adjusted the regularizer constant a to maximize the F-Score on Sections 20-21 of the Treebank, and evaluated the rerankers on Section 22. The results are presented in Tables 12 and 2, where Baseline results were obtained using the parser by Charniak (2000). The ME estimation with L2 regularization outperforms all of the other estimators significantly except for the AP, which performs almost as well and requires an order of magnitude less time in training. Boosting and BLasso are feature selection methods in nature, so they achieve the sparsest models, but at the cost of slightly lower performance and much longer training time. The L1-regularized ME estimator also produces a relatively sparse solution whereas the Averaged Perceptron and the L2-regularized ME estimator assign almost all features a non-zero weight. 3.2 Language model adaptation Ou</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. 2000. A maximum-entropy-inspired parser. In NAACL, 132-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative re-ranking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<pages>173--180</pages>
<contexts>
<context position="2781" citStr="Charniak and Johnson 2005" startWordPosition="420" endWordPosition="423">6), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularizat</context>
<context position="4586" citStr="Charniak and Johnson (2005)" startWordPosition="690" endWordPosition="693">ed an L2-regularized estimator for ranking the parses of a stochastic unification-based grammar. 824 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824–831, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics While these individual estimators are well described in the literature, little is known about the relative performance of these methods because the published results are generally not directly comparable. For example, in the parse re-ranking task, one cannot tell whether the L2- regularized ME approach used by Charniak and Johnson (2005) significantly outperforms the Boosting method by Collins (2000) because different feature sets and n-best parses were used in the evaluations of these methods. This paper conducts a much-needed comparative study of these five parameter estimation algorithms on four NLP tasks: ME estimation with L1 and L2 regularization, the Averaged Perceptron (AP), Boosting, and BLasso, a version of Boosting with Lasso (L1) regularization. We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model adaptation task. Then we apply the best of these estimators</context>
<context position="19771" citStr="Charniak and Johnson (2005)" startWordPosition="3360" endWordPosition="3363"> globally trained ones; for a study which focuses on this issue, see (Punyakanok et al. 2005). In each task we compared the performance of different estimators using task-specific measures. We used the Wilcoxon signed rank test to test the statistical significance of the difference among the competing estimators. We also report other results such as number of non-zero features after estimation, number of training iterations, and computation time (in minutes of elapsed time on an XEONTM MP 3.6GHz machine). 3.1 Parse re-ranking We follow the experimental paradigm of parse re-ranking outlined in Charniak and Johnson (2005), and fed the features extracted by their program to the five rerankers we developed. Each uses a linear model trained using one of the five estimators. These rerankers attempt to select the best parse 𝑦 for a sentence 𝑥 from the 50-best list of possible parses 𝑮𝑬𝑵 𝑥 for the sentence. The linear model combines the log probability calculated by the Charniak (2000) parser as a feature with 1,219,272 additional features. We trained the feaFigure 2: The BLasso algorithm 1 Set w0 = 1 and wd = 0 for d=1...D 2 For t = 1...T (T = the total number of iterations) 3 For each training sample (xi, yi), i =</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative re-ranking. In ACL. 173-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for ME models.</title>
<date>2000</date>
<journal>IEEE Trans. On Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>2</issue>
<pages>37--50</pages>
<contexts>
<context position="2754" citStr="Chen and Rosenfeld 2000" startWordPosition="416" endWordPosition="419">oduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its exp</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Chen, S.F., and Rosenfeld, R. 2000. A survey of smoothing techniques for ME models. IEEE Trans. On Speech and Audio Processing, 8(2): 37-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative re-ranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In ICML,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="1990" citStr="Collins 2000" startWordPosition="291" endWordPosition="292">to NLP. Because of the high-dimensional nature of natural language, it is often easy to generate an extremely large number of features. The challenge of parameter estimation is to find a combination of the typically noisy, redundant features that accurately predicts the target output variable and avoids overfitting. Intuitively, this can be achieved either by selecting a small number of highly-effective features and ignoring the others, or by averaging over a large number of weakly informative features. The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many features are completely irrelevant. L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt </context>
<context position="4650" citStr="Collins (2000)" startWordPosition="700" endWordPosition="701">tion-based grammar. 824 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824–831, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics While these individual estimators are well described in the literature, little is known about the relative performance of these methods because the published results are generally not directly comparable. For example, in the parse re-ranking task, one cannot tell whether the L2- regularized ME approach used by Charniak and Johnson (2005) significantly outperforms the Boosting method by Collins (2000) because different feature sets and n-best parses were used in the evaluations of these methods. This paper conducts a much-needed comparative study of these five parameter estimation algorithms on four NLP tasks: ME estimation with L1 and L2 regularization, the Averaged Perceptron (AP), Boosting, and BLasso, a version of Boosting with Lasso (L1) regularization. We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model adaptation task. Then we apply the best of these estimators to two additional tasks involving conditional sequence models: </context>
<context position="12476" citStr="Collins (2000)" startWordPosition="2035" endWordPosition="2036">lly reaches convergence in even fewer iterations than standard L-BFGS takes on the analogous L2-regularized objective (which translates to less training time, since the time per iteration is only negligibly higher, and total time is dominated by function evaluations). We describe OWL-QN more fully in (Andrew and Gao 2007). We also show that it is significantly faster than Kazama and Tsujii‟s algorithm for L1 regularization and prove that it is guaranteed converge to a parameter vector that globally optimizes the L1-regularized objective. 2.3 Boosting The Boosting algorithm we used is based on Collins (2000). It optimizes the pairwise exponential loss (ExpLoss) function (rather than the logarithmic loss optimized by ME). Given a training sample (xi, yi), for each possible output yj E GEN (xi), we Q(x) = 2 (x − x0 ) ′H (x − x0 ) + g0′ (x − x0) 1 where x0 is the current iterate, and g0 is the function gradient at x0. If H is positive definite, the minimizing value of x can be computed analytically according to: x* = x0 − H−1g0. 1 This projection just entails zeroing-out any coordinates that change sign. Note that it is possible for a variable to change sign in two iterations, by moving from a negat</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Collins, M. 2000. Discriminative re-ranking for natural language parsing. In ICML, 175-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3108" citStr="Collins 2002" startWordPosition="470" endWordPosition="471">ures are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior (</context>
<context position="21142" citStr="Collins 2002" startWordPosition="3612" endWordPosition="3613">η is the size of learning step, optimized on held-out data. Figure 3: The perceptron algorithm 2.5 Averaged Perceptron The perceptron algorithm can be viewed as a form of incremental training procedure (e.g., using stochastic approximation) that optimizes a minimum square error (MSE) loss function (Mitchell, 1997). As shown in Figure 3, it starts with an initial parameter setting and updates it for each training example. In our experiments, we used the Averaged Perceptron algorithm of Freund and Schapire (1999), a variation that has been shown to be more effective than the standard algorithm (Collins 2002). Let 𝒘𝑡,𝑖 be the parameter vector after the 𝑖th training sample has been processed in pass 𝑡 over the training data. The average parameters are defined as𝒘 = 𝟏 𝑻𝑵 𝒘𝒕,𝒊 𝒕 𝒊 where T is the number of epochs, and N is the number of training samples. 3 Evaluations From the four tasks we consider, parsing and language model adaptation are both examples of re-ranking. In these tasks, we assume that we have been given a list of candidates 𝑮𝑬𝑵(𝑥) for each training or test sample 𝑥, 𝑦 , generated using a baseline model. Then, a linear model of the form in Equation (1) is used to discriminatively re-ran</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, M. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In EMNLP, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Iyer</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>In ICML’98.</booktitle>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An efficient boosting algorithm for combining preferences. In ICML’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<pages>277--296</pages>
<contexts>
<context position="21045" citStr="Freund and Schapire (1999)" startWordPosition="3594" endWordPosition="3597">i) using the current model w, 𝑧𝑖 = arg max Φ 𝑥𝑖, 𝑧 ⋅ 𝑤 𝑧∈𝐺𝐸𝑁 𝑥 _𝑖 5 w = w + η((D(xi, yi) – (D(xi, zi)), where η is the size of learning step, optimized on held-out data. Figure 3: The perceptron algorithm 2.5 Averaged Perceptron The perceptron algorithm can be viewed as a form of incremental training procedure (e.g., using stochastic approximation) that optimizes a minimum square error (MSE) loss function (Mitchell, 1997). As shown in Figure 3, it starts with an initial parameter setting and updates it for each training example. In our experiments, we used the Averaged Perceptron algorithm of Freund and Schapire (1999), a variation that has been shown to be more effective than the standard algorithm (Collins 2002). Let 𝒘𝑡,𝑖 be the parameter vector after the 𝑖th training sample has been processed in pass 𝑡 over the training data. The average parameters are defined as𝒘 = 𝟏 𝑻𝑵 𝒘𝒕,𝒊 𝒕 𝒊 where T is the number of epochs, and N is the number of training samples. 3 Evaluations From the four tasks we consider, parsing and language model adaptation are both examples of re-ranking. In these tasks, we assume that we have been given a list of candidates 𝑮𝑬𝑵(𝑥) for each training or test sample 𝑥, 𝑦 , generated using a ba</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Freund, Y. and Schapire, R. E. 1999. Large margin classification using the perceptron algorithm. In Machine Learning, 37(3): 277-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J Friedman</author>
</authors>
<title>The elements of statistical learning.</title>
<date>2001</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="14294" citStr="Hastie et al. 2001" startWordPosition="2390" endWordPosition="2393">ialization, Steps 2 and 3 are repeated T times; at each iteration, a feature is chosen and its weight is updated as follows. First, we define Upd(𝒘, 𝑘, 𝛿) as an updated model, with the same parameter values as 𝑤 with the exception of 𝑤𝑘, which is incremented by 𝛿: Upd 𝒘,𝑘,𝛿 = (𝑤1,..., 𝑤𝑘 + 𝛿,...,𝑤𝐷) Then, Steps 2 and 3 in Figure 1 can be rewritten as Equations (4) and (5), respectively. 𝑘∗, 𝛿∗ = arg min ExpLoss(Upd 𝒘, 𝑘, 𝛿 ) (4) 𝑘,𝛿𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗, 𝛿∗) (5) Because Boosting can overfit we update the weight of 𝑓𝑘∗ by a small fixed step size , as in Equation (6), following the FSLR algorithm (Hastie et al. 2001). 𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗, 𝜖 × sign 𝛿∗ ) (6) By taking such small steps, Boosting imposes a kind of implicit regularization, and can closely approximate the effect of L1 regularization in a local sense (Hastie et al. 2001). Empirically, smaller values of 𝜖 lead to smaller numbers of test errors. 2.4 Boosted Lasso The Boosted Lasso (BLasso) algorithm was originally proposed in Zhao and Yu (2004), and was adapted for language modeling by Gao et al. (2006). BLasso can be viewed as a version of Boosting with L1 regularization. It optimizes an L1-regularized ExpLoss function: LassoLoss 𝒘 = ExpLoss(𝒘) + 𝑅</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>Hastie, T., R. Tibshirani and J. Friedman. 2001. The elements of statistical learning. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>H Suzuki</author>
<author>B Yu</author>
</authors>
<title>Approximation lasso methods for language modeling.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3319" citStr="Gao et al. (2006)" startWordPosition="506" endWordPosition="509">ely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior (i.e., L1 regularization with the constraint that all feature weights are positive) outperformed a comparable estimator using a Gaussian prior (i.e., L2 regularization). Riezler and Vasserman (2004) showed that a</context>
<context position="14745" citStr="Gao et al. (2006)" startWordPosition="2469" endWordPosition="2472">, 𝛿∗) (5) Because Boosting can overfit we update the weight of 𝑓𝑘∗ by a small fixed step size , as in Equation (6), following the FSLR algorithm (Hastie et al. 2001). 𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗, 𝜖 × sign 𝛿∗ ) (6) By taking such small steps, Boosting imposes a kind of implicit regularization, and can closely approximate the effect of L1 regularization in a local sense (Hastie et al. 2001). Empirically, smaller values of 𝜖 lead to smaller numbers of test errors. 2.4 Boosted Lasso The Boosted Lasso (BLasso) algorithm was originally proposed in Zhao and Yu (2004), and was adapted for language modeling by Gao et al. (2006). BLasso can be viewed as a version of Boosting with L1 regularization. It optimizes an L1-regularized ExpLoss function: LassoLoss 𝒘 = ExpLoss(𝒘) + 𝑅(𝒘) (7) where 𝑅 𝒘 = 𝛼 𝑗 𝑤𝑗 . BLasso also uses an incremental feature selection procedure to learn parameter vector 𝒘, just as Boosting does. Due to the explicit use of the regularization term 𝑅 (𝒘), however, there are two major differences from Boosting. At each iteration, BLasso takes either a forward step or a backward step. Similar to Boosting, at each forward step, a feature is selected and its weight is updated according to Eq. (8) and (9). 𝑘</context>
<context position="17320" citStr="Gao et al. (2006)" startWordPosition="2940" endWordPosition="2943">at each forward step the value of 𝛼 decreases until ExpLoss stops decreasing. This is intuitively desirable: it is expected that most highly effective features are selected in early stages of training, so the reduction of ExpLoss at each step in early stages are more substantial than in later stages. These early steps coincide with the Boosting steps most of the time. In other words, the effect of backward steps is more visible at later stages. It can be proved that for a finite number of features and 𝜃 =0, the BLasso algorithm shown in Figure 2 converges to the Lasso solution when 𝜖 → 0. See Gao et al. (2006) for implementation details, and Zhao and Yu (2004) for a theoretical justification for BLasso. 827 1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0 the mapping from 𝑥 to 𝑦 by the linear model may for d=1...D. make use of arbitrary global features of the output 2 Take a forward step according to Eq. (8) and (9), and and is performed “all at once”, we call such a linear the updated model is denoted by w1 model a global model. 3 Initialize a = (ExpLoss(w0)-ExpLoss(w1))/e In the other two tasks (i.e., Chinese word seg4 Take a backward step if and only if it leads to a de- mentation and PO</context>
<context position="23435" citStr="Gao et al. (2006)" startWordPosition="4018" endWordPosition="4021">her estimators significantly except for the AP, which performs almost as well and requires an order of magnitude less time in training. Boosting and BLasso are feature selection methods in nature, so they achieve the sparsest models, but at the cost of slightly lower performance and much longer training time. The L1-regularized ME estimator also produces a relatively sparse solution whereas the Averaged Perceptron and the L2-regularized ME estimator assign almost all features a non-zero weight. 3.2 Language model adaptation Our experiments with LM adaptation are based on the work described in Gao et al. (2006). The variously trained language models were evaluated according to their impact on Japanese text input accuracy, where input phonetic symbols x are mapped into a word string y. Performance of the application is measured in terms of character error 2 The result of ME/L2 is better than that reported in Andrew and Gao (2007) due to the use of the variant of L2-regularized ME estimator, as described in Section 2.1. CER # features time (min) #train iter Baseline 10.24% MAP 7.98% ME/L2 6.99% 295,337 27 665 ME/L1 7.01% 53,342 25 864 AP 7.23% 167,591 6 56 Boost 7.54% 32,994 175 71,000 BLasso 7.20% 33</context>
</contexts>
<marker>Gao, Suzuki, Yu, 2006</marker>
<rawString>Gao, J., Suzuki, H., and Yu, B. 2006. Approximation lasso methods for language modeling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In NAACL.</booktitle>
<marker>Goodman, 2004</marker>
<rawString>Goodman, J. 2004. Exponential priors for maximum entropy models. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Geman</author>
<author>S Canon</author>
<author>Z Chi</author>
<author>S Riezler</author>
</authors>
<title>Estimators for stochastic “Unification-based” grammars.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2803" citStr="Johnson et al. 1999" startWordPosition="424" endWordPosition="427">n into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boos</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S. 1999. Estimators for stochastic “Unification-based” grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>J Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="9789" citStr="Kazama and Tsujii (2003)" startWordPosition="1589" endWordPosition="1592"> regularization This estimator also minimizes the negative conditional log-likelihood, but uses an L1 (or Lasso) penalty. That is, R (w) in Equation (2) is defined according to R (w) = a jZI wj I. L1 regularization typically leads to sparse solutions in which many feature weights are exactly zero, so it is a natural candidate when feature selection is desirable. By contrast, L2 regularization produces solutions in which most weights are small but non-zero. Optimizing the L1-regularized objective function is challenging because its gradient is discontinuous whenever some parameter equals zero. Kazama and Tsujii (2003) described an estimation method that constructs an equivalent constrained optimization problem with twice the number of variables. However, we found that this method is impractically slow for large-scale NLP tasks. In this work we use the orthant-wise limited-memory quasi-Newton algorithm (OWL-QN), which is a modification of L-BFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao 2007). We provide here a high-level description of the algorithm. A quasi-Newton method such as L-BFGS uses first order information at each iterate to build an approximation to th</context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Kazama, J. and Tsujii, J. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In HLT.</booktitle>
<contexts>
<context position="8551" citStr="Malouf 2002" startWordPosition="1392" endWordPosition="1393">the parameters. Here, a is a parameter that controls the amount of regularization, optimized on held-out data. This is one of the most popular estimators, largely due to its appealing computational properties: both 𝐿 𝒘 and 𝑅 (𝒘) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal 𝒘 because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). 825 Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where L(w) is defined as the likelihood of the best parses y E Y(x) relative to the n-best parser output GEN(x), (i.e., Y(x) 9; GEN(x)): L(w) = − Zn 1logZyiGY(xi)P(yi|xi). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME esti</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Malouf, R. 2002. A comparison of algorithms for maximum entropy parameter estimation. In HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="28832" citStr="McCallum et al. 2000" startWordPosition="4894" endWordPosition="4897">cal performance, while L1 uses only 6% of the features, and the Averaged Perceptron requires significantly fewer training iterations. In this case, L1 was also several times faster than L2. The results are summarized in Table 5.3 We note that all three algorithms performed slightly better than the model used by Andrew (2006), which also used L2 regularization (96.84 F1). We believe the difference is due to the use of features derived from negative training examples. 3.4 POS tagging Finally we studied the impact of the regularization methods on a Maximum Entropy conditional Markov Model (MEMM, McCallum et al. 2000) for POS tagging. MEMMs decompose the conditional probability of a tag sequence given a word sequence as follows: n P 4 ... tnIW1 . ..Wn) = fjP(ti|ti−1 ... ti−k,W1 . ..Wn) i=1 where the probability distributions for each tag given its context are ME models. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e., k=2 in the equation above). The local models at each position include features of the current word, the previous word, the next word, and features of the previous two</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>McCallum A, D. Freitag and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine learning. The McGraw-Hill Companies, Inc.</booktitle>
<contexts>
<context position="20844" citStr="Mitchell, 1997" startWordPosition="3562" endWordPosition="3563">so algorithm 1 Set w0 = 1 and wd = 0 for d=1...D 2 For t = 1...T (T = the total number of iterations) 3 For each training sample (xi, yi), i = 1...N 4 Choose the best candidate zi from GEN(xi) using the current model w, 𝑧𝑖 = arg max Φ 𝑥𝑖, 𝑧 ⋅ 𝑤 𝑧∈𝐺𝐸𝑁 𝑥 _𝑖 5 w = w + η((D(xi, yi) – (D(xi, zi)), where η is the size of learning step, optimized on held-out data. Figure 3: The perceptron algorithm 2.5 Averaged Perceptron The perceptron algorithm can be viewed as a form of incremental training procedure (e.g., using stochastic approximation) that optimizes a minimum square error (MSE) loss function (Mitchell, 1997). As shown in Figure 3, it starts with an initial parameter setting and updates it for each training example. In our experiments, we used the Averaged Perceptron algorithm of Freund and Schapire (1999), a variation that has been shown to be more effective than the standard algorithm (Collins 2002). Let 𝒘𝑡,𝑖 be the parameter vector after the 𝑖th training sample has been processed in pass 𝑡 over the training data. The average parameters are defined as𝒘 = 𝟏 𝑻𝑵 𝒘𝒕,𝒊 𝒕 𝒊 where T is the number of epochs, and N is the number of training samples. 3 Evaluations From the four tasks we consider, parsing </context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Mitchell, T. M. 1997. Machine learning. The McGraw-Hill Companies, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Y Ng</author>
</authors>
<title>Feature selection, L1 vs. L2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="3444" citStr="Ng (2004)" startWordPosition="528" endWordPosition="529">ve this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior (i.e., L1 regularization with the constraint that all feature weights are positive) outperformed a comparable estimator using a Gaussian prior (i.e., L2 regularization). Riezler and Vasserman (2004) showed that an L1-regularized ME estimator outperformed an L2-regularized estimator for ranking the parses of a stochastic unification-bas</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Ng, A. Y. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>1999</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="8396" citStr="Nocedal and Wright 1999" startWordPosition="1365" endWordPosition="1368"> data, 𝐿 𝒘 = 𝑛 , where 𝑃 𝑦 𝑥) = − log𝑃 𝑦𝑖 𝑥𝑖) 𝑖=1 exp (D 𝑥,𝑦 ⋅ 𝒘 𝑦 ′∈𝐺𝐸𝑁 𝑥 exp((D 𝑥, 𝑦′ ⋅ 𝒘) and the regularizer term 𝑅 𝒘 = 𝛼 𝑗 𝑤𝑗2 is the weighted squared L2 norm of the parameters. Here, a is a parameter that controls the amount of regularization, optimized on held-out data. This is one of the most popular estimators, largely due to its appealing computational properties: both 𝐿 𝒘 and 𝑅 (𝒘) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal 𝒘 because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). 825 Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where L(w) is defined as the likelihood of the best parses y E Y(x) relative to the n-best parser output GEN(x), (i.e., Y(x) 9; GEN(x)): L(w) = − Zn 1logZyiGY(xi)P(yi|xi). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Nocedal, J., and Wright, S. J. 1999. Numerical Optimization. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Learning and inference over constrained output.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="19237" citStr="Punyakanok et al. 2005" startWordPosition="3279" endWordPosition="3282">imators to global models in the re-ranking framework, the application of some estimators to the local models is problematic. Boosting and BLasso are too computationally expensive to be applied to CRF training and we compared the other three better performing estimation methods for this model. The CMM is a probabilistic sequence model and the log-loss used by ME estimation is most natural for it; thus we limit the comparison to the two kinds of ME models for CMMs. Note that our goal is not to compare locally trained models to globally trained ones; for a study which focuses on this issue, see (Punyakanok et al. 2005). In each task we compared the performance of different estimators using task-specific measures. We used the Wilcoxon signed rank test to test the statistical significance of the difference among the competing estimators. We also report other results such as number of non-zero features after estimation, number of training iterations, and computation time (in minutes of elapsed time on an XEONTM MP 3.6GHz machine). 3.1 Parse re-ranking We follow the experimental paradigm of parse re-ranking outlined in Charniak and Johnson (2005), and fed the features extracted by their program to the five rera</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2005</marker>
<rawString>Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005. Learning and inference over constrained output. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="29133" citStr="Ratnaparkhi, 1996" startWordPosition="4947" endWordPosition="4948"> used by Andrew (2006), which also used L2 regularization (96.84 F1). We believe the difference is due to the use of features derived from negative training examples. 3.4 POS tagging Finally we studied the impact of the regularization methods on a Maximum Entropy conditional Markov Model (MEMM, McCallum et al. 2000) for POS tagging. MEMMs decompose the conditional probability of a tag sequence given a word sequence as follows: n P 4 ... tnIW1 . ..Wn) = fjP(ti|ti−1 ... ti−k,W1 . ..Wn) i=1 where the probability distributions for each tag given its context are ME models. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e., k=2 in the equation above). The local models at each position include features of the current word, the previous word, the next word, and features of the previous two tags. In addition to lexical identity of the words, we used features of word suffixes, capitalization, and number/special character signatures of the words. We used the standard splits of the Penn Treebank from the tagging literature (Toutanova et al. 2003) for training, development and test sets. T</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A. 1996. A maximum entropy part-of-speech tagger. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>A Vasserman</author>
</authors>
<title>Incremental feature selection and L1 regularization for relax maximum entropy modeling.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2463" citStr="Riezler and Vasserman 2004" startWordPosition="365" endWordPosition="368">ver a large number of weakly informative features. The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many features are completely irrelevant. L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low comput</context>
<context position="3905" citStr="Riezler and Vasserman (2004)" startWordPosition="593" endWordPosition="596">iverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior (i.e., L1 regularization with the constraint that all feature weights are positive) outperformed a comparable estimator using a Gaussian prior (i.e., L2 regularization). Riezler and Vasserman (2004) showed that an L1-regularized ME estimator outperformed an L2-regularized estimator for ranking the parses of a stochastic unification-based grammar. 824 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824–831, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics While these individual estimators are well described in the literature, little is known about the relative performance of these methods because the published results are generally not directly comparable. For example, in the parse re-ranking task, one cannot</context>
</contexts>
<marker>Riezler, Vasserman, 2004</marker>
<rawString>Riezler, S., and Vasserman, A. 2004. Incremental feature selection and L1 regularization for relax maximum entropy modeling. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T H King</author>
<author>R M Kaplan</author>
<author>R Crouch</author>
<author>J Maxwell</author>
<author>M Johnson</author>
</authors>
<title>Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<pages>271--278</pages>
<contexts>
<context position="8723" citStr="Riezler et al. (2002)" startWordPosition="1419" endWordPosition="1422">due to its appealing computational properties: both 𝐿 𝒘 and 𝑅 (𝒘) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal 𝒘 because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). 825 Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where L(w) is defined as the likelihood of the best parses y E Y(x) relative to the n-best parser output GEN(x), (i.e., Y(x) 9; GEN(x)): L(w) = − Zn 1logZyiGY(xi)P(yi|xi). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME estimation with L1 regularization This estimator also minimizes the negative conditional log-likelihood, but uses an L1 (or Lasso) penalty. That is, R (w) in Equation (2) is de</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J., and Johnson, M. 2002. Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques. In ACL. 271-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>J. R. Statist. Soc. B,</journal>
<volume>58</volume>
<issue>1</issue>
<pages>267--288</pages>
<contexts>
<context position="2158" citStr="Tibshirani (1996)" startWordPosition="316" endWordPosition="317">estimation is to find a combination of the typically noisy, redundant features that accurately predicts the target output variable and avoids overfitting. Intuitively, this can be achieved either by selecting a small number of highly-effective features and ignoring the others, or by averaging over a large number of weakly informative features. The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many features are completely irrelevant. L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Ch</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Tibshirani, R. 1996. Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B, 58(1): 267-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich Part-of-Speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="29690" citStr="Toutanova et al. 2003" startWordPosition="5041" endWordPosition="5044">ontext are ME models. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e., k=2 in the equation above). The local models at each position include features of the current word, the previous word, the next word, and features of the previous two tags. In addition to lexical identity of the words, we used features of word suffixes, capitalization, and number/special character signatures of the words. We used the standard splits of the Penn Treebank from the tagging literature (Toutanova et al. 2003) for training, development and test sets. The training set comprises Sections 0-18, the development set — Sections 19-21, and the test set — Sections 22-24. We compared training the ME models using L1 and L2 regularization. For each of the two types of regularization we selected the best value of the regularization constant using grid search to optimize the accuracy on the development set. We report final accuracy measures on the test set in Table 6. The results on this task confirm the trends we have seen so far. There is almost no difference in 3 Only the L2 vs. AP comparison is significant </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. 2003. Feature-rich Part-of-Speech tagging with a cyclic dependency network. In HLT-NAACL, 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Zhao</author>
<author>B Yu</author>
</authors>
<title>Boosted lasso.</title>
<date>2004</date>
<tech>Tech Report, Statistics</tech>
<contexts>
<context position="2010" citStr="Zhao and Yu, 2004" startWordPosition="293" endWordPosition="296">e of the high-dimensional nature of natural language, it is often easy to generate an extremely large number of features. The challenge of parameter estimation is to find a combination of the typically noisy, redundant features that accurately predicts the target output variable and avoids overfitting. Intuitively, this can be achieved either by selecting a small number of highly-effective features and ignoring the others, or by averaging over a large number of weakly informative features. The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many features are completely irrelevant. L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by a</context>
<context position="14685" citStr="Zhao and Yu (2004)" startWordPosition="2458" endWordPosition="2461">, 𝛿∗ = arg min ExpLoss(Upd 𝒘, 𝑘, 𝛿 ) (4) 𝑘,𝛿𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗, 𝛿∗) (5) Because Boosting can overfit we update the weight of 𝑓𝑘∗ by a small fixed step size , as in Equation (6), following the FSLR algorithm (Hastie et al. 2001). 𝒘𝑡 = Upd(𝒘𝑡−1, 𝑘∗, 𝜖 × sign 𝛿∗ ) (6) By taking such small steps, Boosting imposes a kind of implicit regularization, and can closely approximate the effect of L1 regularization in a local sense (Hastie et al. 2001). Empirically, smaller values of 𝜖 lead to smaller numbers of test errors. 2.4 Boosted Lasso The Boosted Lasso (BLasso) algorithm was originally proposed in Zhao and Yu (2004), and was adapted for language modeling by Gao et al. (2006). BLasso can be viewed as a version of Boosting with L1 regularization. It optimizes an L1-regularized ExpLoss function: LassoLoss 𝒘 = ExpLoss(𝒘) + 𝑅(𝒘) (7) where 𝑅 𝒘 = 𝛼 𝑗 𝑤𝑗 . BLasso also uses an incremental feature selection procedure to learn parameter vector 𝒘, just as Boosting does. Due to the explicit use of the regularization term 𝑅 (𝒘), however, there are two major differences from Boosting. At each iteration, BLasso takes either a forward step or a backward step. Similar to Boosting, at each forward step, a feature is select</context>
<context position="17371" citStr="Zhao and Yu (2004)" startWordPosition="2948" endWordPosition="2951">l ExpLoss stops decreasing. This is intuitively desirable: it is expected that most highly effective features are selected in early stages of training, so the reduction of ExpLoss at each step in early stages are more substantial than in later stages. These early steps coincide with the Boosting steps most of the time. In other words, the effect of backward steps is more visible at later stages. It can be proved that for a finite number of features and 𝜃 =0, the BLasso algorithm shown in Figure 2 converges to the Lasso solution when 𝜖 → 0. See Gao et al. (2006) for implementation details, and Zhao and Yu (2004) for a theoretical justification for BLasso. 827 1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0 the mapping from 𝑥 to 𝑦 by the linear model may for d=1...D. make use of arbitrary global features of the output 2 Take a forward step according to Eq. (8) and (9), and and is performed “all at once”, we call such a linear the updated model is denoted by w1 model a global model. 3 Initialize a = (ExpLoss(w0)-ExpLoss(w1))/e In the other two tasks (i.e., Chinese word seg4 Take a backward step if and only if it leads to a de- mentation and POS tagging), there is no explicit crease of LassoLos</context>
</contexts>
<marker>Zhao, Yu, 2004</marker>
<rawString>Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, Statistics Department, U. C. Berkeley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>