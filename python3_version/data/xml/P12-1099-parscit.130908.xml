<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000233">
<title confidence="0.919369">
Mixing Multiple Translation Models in Statistical Machine Translation
</title>
<author confidence="0.987953">
Majid Razmara1 George Foster2 Baskaran Sankaran1 Anoop Sarkar1
</author>
<affiliation confidence="0.965012">
1 Simon Fraser University, 8888 University Dr., Burnaby, BC, Canada
</affiliation>
<email confidence="0.907829">
{razmara,baskaran,anoop}@sfu.ca
</email>
<affiliation confidence="0.403825">
2 National Research Council Canada, 283 Alexandre-Tach´e Blvd, Gatineau, QC, Canada
</affiliation>
<email confidence="0.979214">
george.foster@nrc.gc.ca
</email>
<sectionHeader confidence="0.99723" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999263933333334">
Statistical machine translation is often faced
with the problem of combining training data
from many diverse sources into a single trans-
lation model which then has to translate sen-
tences in a new domain. We propose a novel
approach, ensemble decoding, which com-
bines a number of translation systems dynam-
ically at the decoding step. In this paper,
we evaluate performance on a domain adap-
tation setting where we translate sentences
from the medical domain. Our experimental
results show that ensemble decoding outper-
forms various strong baselines including mix-
ture models, the current state-of-the-art for do-
main adaptation in machine translation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939203389831">
Statistical machine translation (SMT) systems re-
quire large parallel corpora in order to be able to
obtain a reasonable translation quality. In statisti-
cal learning theory, it is assumed that the training
and test datasets are drawn from the same distribu-
tion, or in other words, they are from the same do-
main. However, bilingual corpora are only available
in very limited domains and building bilingual re-
sources in a new domain is usually very expensive.
It is an interesting question whether a model that is
trained on an existing large bilingual corpus in a spe-
cific domain can be adapted to another domain for
which little parallel data is present. Domain adap-
tation techniques aim at finding ways to adjust an
out-of-domain (OUT) model to represent a target do-
main (in-domain or IN).
Common techniques for model adaptation adapt
two main components of contemporary state-of-the-
art SMT systems: the language model and the trans-
lation model. However, language model adapta-
tion is a more straight-forward problem compared to
translation model adaptation, because various mea-
sures such as perplexity of adapted language models
can be easily computed on data in the target domain.
As a result, language model adaptation has been well
studied in various work (Clarkson and Robinson,
1997; Seymore and Rosenfeld, 1997; Bacchiani and
Roark, 2003; Eck et al., 2004) both for speech recog-
nition and for machine translation. It is also easier to
obtain monolingual data in the target domain, com-
pared to bilingual data which is required for transla-
tion model adaptation. In this paper, we focused on
adapting only the translation model by fixing a lan-
guage model for all the experiments. We expect do-
main adaptation for machine translation can be im-
proved further by combining orthogonal techniques
for translation model adaptation combined with lan-
guage model adaptation.
In this paper, a new approach for adapting the
translation model is proposed. We use a novel sys-
tem combination approach called ensemble decod-
ing in order to combine two or more translation
models with the goal of constructing a system that
outperforms all the component models. The strength
of this system combination method is that the sys-
tems are combined in the decoder. This enables
the decoder to pick the best hypotheses for each
span of the input. The main applications of en-
semble models are domain adaptation, domain mix-
ing and system combination. We have modified
Kriya (Sankaran et al., 2012), an in-house imple-
mentation of hierarchical phrase-based translation
system (Chiang, 2005), to implement ensemble de-
coding using multiple translation models.
We compare the results of ensemble decoding
with a number of baselines for domain adaptation.
In addition to the basic approach of concatenation of
in-domain and out-of-domain data, we also trained
a log-linear mixture model (Foster and Kuhn, 2007)
</bodyText>
<page confidence="0.960188">
940
</page>
<note confidence="0.9876825">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940–949,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.6636604">
as well as the linear mixture model of (Foster et al., Xλ� =argmax �p(e, f) log M λmpm(�e |f)
2010) for conditional phrase-pair probabilities over a fe, X
IN and OUT. Furthermore, within the framework of m
ensemble decoding, we study and evaluate various
methods for combining translation tables.
</bodyText>
<sectionHeader confidence="0.98873" genericHeader="introduction">
2 Baselines
</sectionHeader>
<bodyText confidence="0.999752833333333">
The natural baseline for model adaption is to con-
catenate the IN and OUT data into a single paral-
lel corpus and train a model on it. In addition to
this baseline, we have experimented with two more
sophisticated baselines which are based on mixture
techniques.
</bodyText>
<subsectionHeader confidence="0.683437">
2.1 Log-Linear Mixture
</subsectionHeader>
<bodyText confidence="0.9410075">
Log-linear translation model (TM) mixtures are of
the form:
</bodyText>
<equation confidence="0.985327">
M 1l
p(e |f) oc exp (Xλm log pm(e|�f)J
m
</equation>
<bodyText confidence="0.999874666666666">
where m ranges over IN and OUT, pm(e |f) is an
estimate from a component phrase table, and each
λm is a weight in the top-level log-linear model, set
so as to maximize dev-set BLEU using minimum
error rate training (Och, 2003). We learn separate
weights for relative-frequency and lexical estimates
for both pm(e |f) and pm(�f|e). Thus, for 2 compo-
nent models (from IN and OUT training corpora),
there are 4 * 2 = 8 TM weights to tune. Whenever
a phrase pair does not appear in a component phrase
table, we set the corresponding pm(e |f) to a small
epsilon value.
</bodyText>
<subsectionHeader confidence="0.997748">
2.2 Linear Mixture
</subsectionHeader>
<equation confidence="0.677487">
Linear TM mixtures are of the form:
λmpm(�e |f)
</equation>
<bodyText confidence="0.999950421052631">
Our technique for setting λm is similar to that
outlined in Foster et al. (2010). We first extract a
joint phrase-pair distribution p(e, f) from the de-
velopment set using standard techniques (HMM
word alignment with grow-diag-and symmeteriza-
tion (Koehn et al., 2003)). We then find the set
of weights λ� that minimize the cross-entropy of the
mixture p(e |f) with respect to ,5(e, f):
For efficiency and stability, we use the EM algo-
rithm to find �λ, rather than L-BFGS as in (Foster et
al., 2010). Whenever a phrase pair does not appear
in a component phrase table, we set the correspond-
ing pm(e |f) to 0; pairs in p(e, f) that do not appear
in at least one component table are discarded. We
learn separate linear mixtures for relative-frequency
and lexical estimates for both p(e|�f) and p(�f|e).
These four features then appear in the top-level
model as usual – there is no runtime cost for the lin-
ear mixture.
</bodyText>
<sectionHeader confidence="0.996537" genericHeader="method">
3 Ensemble Decoding
</sectionHeader>
<bodyText confidence="0.999982366666667">
Ensemble decoding is a way to combine the exper-
tise of different models in one single model. The
current implementation is able to combine hierar-
chical phrase-based systems (Chiang, 2005) as well
as phrase-based translation systems (Koehn et al.,
2003). However, the method can be easily extended
to support combining a number of heterogeneous
translation systems e.g. phrase-based, hierarchical
phrase-based, and/or syntax-based systems. This
section explains how such models can be combined
during the decoding.
Given a number of translation models which are
already trained and tuned, the ensemble decoder
uses hypotheses constructed from all of the models
in order to translate a sentence. We use the bottom-
up CKY parsing algorithm for decoding. For each
sentence, a CKY chart is constructed. The cells of
the CKY chart are populated with appropriate rules
from all the phrase tables of different components.
As in the Hiero SMT system (Chiang, 2005), the
cells which span up to a certain length (i.e. the max-
imum span length) are populated from the phrase-
tables and the rest of the chart uses glue rules as de-
fined in (Chiang, 2005).
The rules suggested from the component models
are combined in a single set. Some of the rules may
be unique and others may be common with other
component model rule sets, though with different
scores. Therefore, we need to combine the scores
of such common rules and assign a single score to
</bodyText>
<equation confidence="0.993029">
M
X
m
f) =
p(�e|
</equation>
<page confidence="0.969392">
941
</page>
<bodyText confidence="0.9991677">
them. Depending on the mixture operation used for
combining the scores, we would get different mix-
ture scores. The choice of mixture operation will be
discussed in Section 3.1.
Figure 1 illustrates how the CKY chart is filled
with the rules. Each cell, covering a span, is popu-
lated with rules from all component models as well
as from cells covering a sub-span of it.
In the typical log-linear model SMT, the posterior
probability for each phrase pair (e, f) is given by:
</bodyText>
<listItem confidence="0.984185666666667">
• Model Switching (Switch): in model switch-
ing, each cell in the CKY chart gets populated
only by rules from one of the models and the
</listItem>
<bodyText confidence="0.865526714285714">
other models’ rules are discarded. This is based
on the hypothesis that each component model
is an expert on certain parts of sentence. In this
method, we need to define a binary indicator
function δ(�f, m) for each span and component
model to specify rules of which model to retain
for each span.
</bodyText>
<figure confidence="0.903723818181818">
p(�e |� X � δ( f, m) = ⎧ 1, m = argmax ψ( f,n)
�f) ∝ exp wiφi(�e, �f) ⎨ nEM
i ⎩ 0, otherwise
 |{z }
W&apos;φ
The criteria for choosing a model for each cell,
ψ( f, n), could be based on:
– Max: for each cell, the model that has the
highest weighted best-rule score wins:
ψ( f, n) = λn max (wn · φn(�e, f))
e
</figure>
<bodyText confidence="0.931052285714286">
– Sum: Instead of comparing only the
scores of the best rules, the model with
the highest weighted sum of the probabil-
ities of the rules wins. This sum has to
take into account the translation table limit
(ttl), on the number of rules suggested by
each model for each cell:
</bodyText>
<equation confidence="0.903755">
Xψ( �f, n) = λn exp (wn · φn(�e,�f))
e
</equation>
<bodyText confidence="0.9978662">
The probability of each phrase-pair (e, f) is
computed as:
Ensemble decoding uses the same framework for
each individual system. Therefore, the score of a
phrase-pair (e, f) in the ensemble model is:
</bodyText>
<equation confidence="0.91674925">
�
p(�e  |f) ∝ exp w1 · φ1
 |{z }
1st model
</equation>
<bodyText confidence="0.9992345">
where ⊕ denotes the mixture operation between two
or more model scores.
</bodyText>
<subsectionHeader confidence="0.999423">
3.1 Mixture Operations
</subsectionHeader>
<bodyText confidence="0.99896">
Mixture operations receive two or more scores
(probabilities) and return the mixture score (prob-
ability). In this section, we explore different options
for mixture operation and discuss some of the char-
acteristics of these mixture operations.
</bodyText>
<listItem confidence="0.99168775">
• Weighted Sum (wsum): in wsum the ensemble
probability is proportional to the weighted sum
of all individual model probabilities (i.e. linear
mixture).
</listItem>
<equation confidence="0.968845142857143">
⊕ w2 · φ2
 |{z }
2nd model
�⊕ ···
p(�e |f) ∝ M λm exp (wm · φm) p(�e |f) = M δ( f, m) pm(�e  |f)
X X
m m
</equation>
<bodyText confidence="0.998952666666667">
where m denotes the index of component mod-
els, M is the total number of them and λi is the
weight for component i.
</bodyText>
<listItem confidence="0.874276">
• Weighted Max (wmax): where the ensemble
score is the weighted max of all model scores.
f) ∝ max (λm exp (wm · φm ��
m
• Product (prod): in Product models or Prod-
uct of Experts (Hinton, 1999), the probability
of the ensemble model or a rule is computed as
the product of the probabilities of all compo-
nents (or equally the sum of log-probabilities,
i.e. log-linear mixture). Product models can
also make use of weights to control the contri-
bution of each component. These models are
</listItem>
<page confidence="0.791596">
p(�e|
942
</page>
<figureCaption confidence="0.999844">
Figure 1: The cells in the CKY chart are populated using rules from all component models and sub-span cells.
</figureCaption>
<bodyText confidence="0.5037785">
generally known as Logarithmic Opinion Pools
(LOPs) where:
</bodyText>
<equation confidence="0.961798333333333">
M
p(e f) a exp ( E Am (wm - om))
m
</equation>
<bodyText confidence="0.999939466666667">
Product models have been used in combining
LMs and TMs in SMT as well as some other
NLP tasks such as ensemble parsing (Petrov,
2010).
Each of these mixture operations has a specific
property that makes it work in specific domain adap-
tation or system combination scenarios. For in-
stance, LOPs may not be optimal for domain adapta-
tion in the setting where there are two or more mod-
els trained on heterogeneous corpora. As discussed
in (Smith et al., 2005), LOPs work best when all the
models accuracies are high and close to each other
with some degree of diversity. LOPs give veto power
to any of the component models and this perfectly
works for settings such as the one in (Petrov, 2010)
where a number of parsers are trained by changing
the randomization seeds but having the same base
parser and using the same training set. They no-
ticed that parsers trained using different randomiza-
tion seeds have high accuracies but there are some
diversities among them and they used product mod-
els for their advantage to get an even better parser.
We assume that each of the models is expert in some
parts and so they do not necessarily agree on cor-
rect hypotheses. In other words, product models (or
LOPs) tend to have intersection-style effects while
we are more interested in union-style effects.
In Section 4.2, we compare the BLEU scores of
different mixture operations on a French-English ex-
perimental setup.
</bodyText>
<subsectionHeader confidence="0.994733">
3.2 Normalization
</subsectionHeader>
<bodyText confidence="0.999989035714286">
Since in log-linear models, the model scores are
not normalized to form probability distributions, the
scores that different models assign to each phrase-
pair may not be in the same scale. Therefore, mixing
their scores might wash out the information in one
(or some) of the models. We experimented with two
different ways to deal with this normalization issue.
A practical but inexact heuristic is to normalize the
scores over a shorter list. So the list of rules coming
from each model for a cell in CKY chart is normal-
ized before getting mixed with other phrase-table
rules. However, experiments showed changing the
scores with the normalized scores hurts the BLEU
score radically. So we use the normalized scores
only for pruning and the actual scores are intact.
We could also globally normalize the scores to ob-
tain posterior probabilities using the inside-outside
algorithm. However, we did not try it as the BLEU
scores we got using the normalization heuristic was
not promissing and it would impose a cost in de-
coding as well. More investigation on this issue has
been left for future work.
A more principled way is to systematically find
the most appropriate model weights that can avoid
this problem by scaling the scores properly. We
used a publicly available toolkit, CONDOR (Van-
den Berghen and Bersini, 2005), a direct optimizer
based on Powell’s algorithm, that does not require
</bodyText>
<page confidence="0.995657">
943
</page>
<bodyText confidence="0.999905666666667">
explicit gradient information for the objective func-
tion. Component weights for each mixture operation
are optimized on the dev-set using CONDOR.
</bodyText>
<sectionHeader confidence="0.996593" genericHeader="method">
4 Experiments &amp; Results
</sectionHeader>
<subsectionHeader confidence="0.92999">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.938639166666667">
We carried out translation experiments using the Eu-
ropean Medicines Agency (EMEA) corpus (Tiede-
mann, 2009) as IN, and the Europarl (EP) corpus1 as
OUT, for French to English translation. The dev and
test sets were randomly chosen from the EMEA cor-
pus.2 The details of datasets used are summarized in
</bodyText>
<tableCaption confidence="0.990058">
Table 1.
</tableCaption>
<table confidence="0.999911833333333">
Dataset Sents Words
French English
EMEA 11770 168K 144K
Europarl 1.3M 40M 37M
Dev 1533 29K 25K
Test 1522 29K 25K
</table>
<tableCaption confidence="0.999946">
Table 1: Training, dev and test sets for EMEA.
</tableCaption>
<bodyText confidence="0.998615695652174">
For the mixture baselines, we used a standard
one-pass phrase-based system (Koehn et al., 2003),
Portage (Sadat et al., 2005), with the following 7
features: relative-frequency and lexical translation
model (TM) probabilities in both directions; word-
displacement distortion model; language model
(LM) and word count. The corpus was word-aligned
using both HMM and IBM2 models, and the phrase
table was the union of phrases extracted from these
separate alignments, with a length limit of 7. It
was filtered to retain the top 20 translations for each
source phrase using the TM part of the current log-
linear model.
For ensemble decoding, we modified an in-house
implementation of hierarchical phrase-based sys-
tem, Kriya (Sankaran et al., 2012) which uses the
same features mentioned in (Chiang, 2005): for-
ward and backward relative-frequency and lexical
TM probabilities; LM; word, phrase and glue-rules
penalty. GIZA++(Och and Ney, 2000) has been used
for word alignment with phrase length limit of 7.
In both systems, feature weights were optimized
using MERT (Och, 2003) and with a 5-gram lan-
</bodyText>
<footnote confidence="0.9878975">
1www.statmt.org/europarl
2Please contact the authors to access the data-sets.
</footnote>
<bodyText confidence="0.9988324">
guage model and Kneser-Ney smoothing was used
in all the experiments. We used SRILM (Stolcke,
2002) as the langugage model toolkit. Fixing the
language model allows us to compare various trans-
lation model combination techniques.
</bodyText>
<sectionHeader confidence="0.695961" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999682916666667">
Table 2 shows the results of the baselines. The first
group are the baseline results on the phrase-based
system discussed in Section 2 and the second group
are those of our hierarchical MT system. Since the
Hiero baselines results were substantially better than
those of the phrase-based model, we also imple-
mented the best-performing baseline, linear mixture,
in our Hiero-style MT system and in fact it achieves
the hights BLEU score among all the baselines as
shown in Table 2. This baseline is run three times
the score is averaged over the BLEU scores with
standard deviation of 0.34.
</bodyText>
<table confidence="0.999730833333333">
Baseline PBS Hiero
IN 31.84 33.69
OUT 24.08 25.32
IN + OUT 31.75 33.76
LOGLIN 32.21 –
LINMIX 33.81 35.57
</table>
<tableCaption confidence="0.9995115">
Table 2: The results of various baselines implemented in
a phrase-based (PBS) and a Hiero SMT on EMEA.
</tableCaption>
<bodyText confidence="0.998919947368421">
Table 3 shows the results of ensemble decoding
with different mixture operations and model weight
settings. Each mixture operation has been evalu-
ated on the test-set by setting the component weights
uniformly (denoted by uniform) and by tuning the
weights using CONDOR (denoted by tuned) on a
held-out set. The tuned scores (3rd column in Ta-
ble 3) are averages of three runs with different initial
points as in Clark et al. (2011). We also reported the
BLEU scores when we applied the span-wise nor-
malization heuristic. All of these mixture operations
were able to significantly improve over the concate-
nation baseline. In particular, Switching:Max could
gain up to 2.2 BLEU points over the concatenation
baseline and 0.39 BLEU points over the best per-
forming baseline (i.e. linear mixture model imple-
mented in Hiero) which is statistically significant
based on Clark et al. (2011) (p = 0.02).
Prod when using with uniform weights gets the
</bodyText>
<page confidence="0.996587">
944
</page>
<table confidence="0.999263666666667">
Mixture Operation Uniform Tuned Norm.
WMAX 35.39 35.47 (s=0.03) 35.47
WSUM 35.35 35.53 (s=0.04) 35.45
SWITCHING:MAX 35.93 35.96 (s=0.01) 32.62
SWITCHING:SUM 34.90 34.72 (s=0.23) 34.90
PROD 33.93 35.24 (s=0.05) 35.02
</table>
<tableCaption confidence="0.982219333333333">
Table 3: The results of ensemble decoding on EMEA for Fr2En when using uniform weights, tuned weights and
normalization heuristic. The tuned BLEU scores are averaged over three runs with multiple initial points, as in (Clark
et al., 2011), with the standard deviations in brackets .
</tableCaption>
<bodyText confidence="0.999882416666667">
lowest score among the mixture operations, how-
ever after tuning, it learns to bias the weights to-
wards one of the models and hence improves by
1.31 BLEU points. Although Switching:Sum outper-
forms the concatenation baseline, it is substantially
worse than other mixture operations. One explana-
tion that Switching:Max is the best performing op-
eration and Switching:Sum is the worst one, despite
their similarities, is that Switching:Max prefers more
peaked distributions while Switching:Sum favours a
model that has fewer hypotheses for each span.
An interesting observation based on the results in
Table 3 is that uniform weights are doing reasonably
well given that the component weights are not opti-
mized and therefore model scores may not be in the
same scope (refer to discussion in §3.2). We suspect
this is because a single LM is shared between both
models. This shared component controls the vari-
ance of the weights in the two models when com-
bined with the standard L-1 normalization of each
model’s weights and hence prohibits models to have
too varied scores for the same input. Though, it may
not be the case when multiple LMs are used which
are not shared.
Two sample sentences from the EMEA test-set
along with their translations by the IN, OUT and En-
semble models are shown in Figure 2. The boxes
show how the Ensemble model is able to use n-
grams from the IN and OUT models to construct
a better translation than both of them. In the first
example, there are two OOVs one for each of the
IN and OUT models. Our approach is able to re-
solve the OOV issues by taking advantage of the
other model’s presence. Similarly, the second exam-
ple shows how ensemble decoding improves lexical
choices as well as word re-orderings.
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="related work">
5 Related Work
</sectionHeader>
<subsectionHeader confidence="0.995653">
5.1 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999980676470588">
Early approaches to domain adaptation involved in-
formation retrieval techniques where sentence pairs
related to the target domain were retrieved from the
training corpus using IR methods (Eck et al., 2004;
Hildebrand et al., 2005). Foster et al. (2010), how-
ever, uses a different approach to select related sen-
tences from OUT. They use language model per-
plexities from IN to select relavant sentences from
OUT. These sentences are used to enrich the IN
training set.
Other domain adaptation methods involve tech-
niques that distinguish between general and domain-
specific examples (Daum´e and Marcu, 2006). Jiang
and Zhai (2007) introduce a general instance weight-
ing framework for model adaptation. This approach
tries to penalize misleading training instances from
OUT and assign more weight to IN-like instances
than OUT instances. Foster et al. (2010) propose a
similar method for machine translation that uses fea-
tures to capture degrees of generality. Particularly,
they include the output from an SVM classifier that
uses the intersection between IN and OUT as pos-
itive examples. Unlike previous work on instance
weighting in machine translation, they use phrase-
level instances instead of sentences.
A large body of work uses interpolation tech-
niques to create a single TM/LM from interpolating
a number of LMs/TMs. Two famous examples of
such methods are linear mixtures and log-linear mix-
tures (Koehn and Schroeder, 2007; Civera and Juan,
2007; Foster and Kuhn, 2007) which were used as
baselines and discussed in Section 2. Other meth-
ods include using self-training techniques to exploit
monolingual in-domain data (Ueffing et al., 2007;
</bodyText>
<page confidence="0.998389">
945
</page>
<figureCaption confidence="0.995017">
Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems.
</figureCaption>
<figure confidence="0.95979925">
IN
amenorrhoea , menstruations irr´eguli`eres
ENSEMBLE
amenorrhoea ,
irregular menstruation
metabolic disease .
in the management of patients with mps vi or other hereditary
by a doctor with the experience of the care
OUT naglazyme ’s treatment must be supervised
supervised
by a physician experienced
ENSEMBLE naglazyme treatment should be
metabolic disease .
in the management of patients with mps vi or other hereditary
SOURCE am´enorrh´ee , menstruations irr´eguli`eres
REF amenorrhoea , irregular menstruation
OUT am´enorrh´ee , irregular menstruation
SOURCE le traitement par naglazyme doit ˆetre supervis´e par un m´edecin ayant l’ exp´erience de
la prise en charge des patients atteints de mps vi ou d’ une autre maladie m´etabolique
h´er´editaire .
REF naglazyme treatment should be supervised by a physician experienced in the manage-
ment of patients with mps vi or other inherited metabolic diseases .
IN naglazyme treatment should be supervis´e by a doctor the with
of patients with mps vi. or another disease hereditary metabolic .
</figure>
<figureCaption confidence="0.575988166666667">
Bertoldi and Federico, 2009). In this approach, a
system is trained on the parallel OUT and IN data
and it is used to translate the monolingual IN data
set. Iteratively, most confident sentence pairs are se-
lected and added to the training corpus on which a
new system is trained.
</figureCaption>
<subsectionHeader confidence="0.998591">
5.2 System Combination
</subsectionHeader>
<bodyText confidence="0.999960261904762">
Tackling the model adaptation problem using sys-
tem combination approaches has been experimented
in various work (Koehn and Schroeder, 2007; Hilde-
brand and Vogel, 2009). Among these approaches
are sentence-based, phrase-based and word-based
output combination methods. In a similar approach,
Koehn and Schroeder (2007) use a feature of the fac-
tored translation model framework in Moses SMT
system (Koehn and Schroeder, 2007) to use multiple
alternative decoding paths. Two decoding paths, one
for each translation table (IN and OUT), were used
during decoding. The weights are set with minimum
error rate training (Och, 2003).
Our work is closely related to Koehn and
Schroeder (2007) but uses a different approach to
deal with multiple translation tables. The Moses
SMT system implements (Koehn and Schroeder,
2007) and can treat multiple translation tables in
two different ways: intersection and union. In in-
tersection, for each span only the hypotheses would
be used that are present in all phrase tables. For
each set of hypothesis with the same source and
target phrases, a new hypothesis is created whose
feature-set is the union of feature sets of all corre-
sponding hypotheses. Union, on the other hand, uses
hypotheses from all the phrase tables. The feature
set of these hypotheses are expanded to include one
feature set for each table. However, for the corre-
sponding feature values of those phrase-tables that
did not have a particular phrase-pair, a default log
probability value of 0 is assumed (Bertoldi and Fed-
erico, 2009) which is counter-intuitive as it boosts
the score of hypotheses with phrase-pairs that do not
belong to all of the translation tables.
Our approach is different from Koehn and
Schroeder (2007) in a number of ways. Firstly, un-
like the multi-table support of Moses which only
supports phrase-based translation table combination,
our approach supports ensembles of both hierarchi-
cal and phrase-based systems. With little modifica-
tion, it can also support ensemble of syntax-based
systems with the other two state-of-the-art SMT sys-
</bodyText>
<page confidence="0.997044">
946
</page>
<bodyText confidence="0.999986071428572">
tems. Secondly, our combining method uses the
union option, but instead of preserving the features
of all phrase-tables, it only combines their scores
using various mixture operations. This enables us
to experiment with a number of different opera-
tions as opposed to sticking to only one combination
method. Finally, by avoiding increasing the number
of features we can add as many translation models
as we need without serious performance drop. In
addition, MERT would not be an appropriate opti-
mizer when the number of features increases a cer-
tain amount (Chiang et al., 2008).
Our approach differs from the model combina-
tion approach of DeNero et al. (2010), a generaliza-
tion of consensus or minimum Bayes risk decoding
where the search space consists of those of multi-
ple systems, in that model combination uses forest
of derivations of all component models to do the
combination. In other words, it requires all compo-
nent models to fully decode each sentence, compute
n-gram expectations from each component model
and calculate posterior probabilities over transla-
tion derivations. While, in our approach we only
use partial hypotheses from component models and
the derivation forest is constructed by the ensemble
model. A major difference is that in the model com-
bination approach the component search spaces are
conjoined and they are not intermingled as opposed
to our approach where these search spaces are inter-
mixed on spans. This enables us to generate new
sentences that cannot be generated by component
models. Furthermore, various combination methods
can be explored in our approach. Finally, main tech-
niques used in this work are orthogonal to our ap-
proach such as Minimum Bayes Risk decoding, us-
ing n-gram features and tuning using MERT.
Finally, our work is most similar to that of
Liu et al. (2009) where max-derivation and max-
translation decoding have been used. Max-
derivation finds a derivation with highest score and
max-translation finds the highest scoring translation
by summing the score of all derivations with the
same yield. The combination can be done in two
levels: translation-level and derivation-level. Their
derivation-level max-translation decoding is similar
to our ensemble decoding with wsum as the mixture
operation. We did not restrict ourself to this par-
ticular mixture operation and experimented with a
number of different mixing techniques and as Ta-
ble 3 shows we could improve over wsum in our
experimental setup. Liu et al. (2009) used a mod-
ified version of MERT to tune max-translation de-
coding weights, while we use a two-step approach
using MERT for tuning each component model sep-
arately and then using CONDOR to tune component
weights on top of them.
</bodyText>
<sectionHeader confidence="0.998866" genericHeader="conclusions">
6 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999981517241379">
In this paper, we presented a new approach for do-
main adaptation using ensemble decoding. In this
approach a number of MT systems are combined at
decoding time in order to form an ensemble model.
The model combination can be done using various
mixture operations. We showed that this approach
can gain up to 2.2 BLEU points over its concatena-
tion baseline and 0.39 BLEU points over a powerful
mixture model.
Future work includes extending this approach to
use multiple translation models with multiple lan-
guage models in ensemble decoding. Different
mixture operations can be investigated and the be-
haviour of each operation can be studied in more
details. We will also add capability of support-
ing syntax-based ensemble decoding and experi-
ment how a phrase-based system can benefit from
syntax information present in a syntax-aware MT
system. Furthermore, ensemble decoding can be ap-
plied on domain mixing settings in which develop-
ment sets and test sets include sentences from dif-
ferent domains and genres, and this is a very suit-
able setting for an ensemble model which can adapt
to new domains at test time. In addition, we can
extend our approach by applying some of the tech-
niques used in other system combination approaches
such as consensus decoding, using n-gram features,
tuning using forest-based MERT, among other pos-
sible extensions.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999630857142857">
This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Fac-
ulty Award to the last author. We would like to
thank Philipp Koehn and the anonymous reviewers
for their valuable comments. We also thank the de-
velopers of GIZA++ and Condor which we used for
our experiments.
</bodyText>
<page confidence="0.996204">
947
</page>
<sectionHeader confidence="0.996376" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999737219047619">
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In Acoustics, Speech, and
Signal Processing, 2003. Proceedings. (ICASSP ’03).
2003 IEEE International Conference on, volume 1,
pages I–224 – I–227 vol.1, april.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
’09, pages 182–189, Stroudsburg, PA, USA. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. ACL.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ’05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263–270, Mor-
ristown, NJ, USA. ACL.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, StatMT ’07, pages
177–180, Stroudsburg, PA, USA. ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume 2,
HLT ’11, pages 176–181. ACL.
P. Clarkson and A. Robinson. 1997. Language model
adaptation using mixtures and an exponentially decay-
ing cache. In Proceedings of the 1997 IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP ’97)-Volume 2 - Volume 2,
ICASSP ’97, pages 799–, Washington, DC, USA.
IEEE Computer Society.
Hal Daum´e, III and Daniel Marcu. 2006. Domain
adaptation for statistical classifiers. J. Artif. Int. Res.,
26:101–126, May.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ’10,
pages 975–983, Stroudsburg, PA, USA. ACL.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In In Pro-
ceedings of LREC.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for smt. In Proceedings of the Second
Workshop on Statistical Machine Translation, StatMT
’07, pages 128–135, Stroudsburg, PA, USA. ACL.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’10, pages 451–
459, Stroudsburg, PA, USA. ACL.
Almut Silja Hildebrand and Stephan Vogel. 2009. CMU
system combination for WMT’09. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, StatMT ’09, pages 47–50, Stroudsburg, PA, USA.
ACL.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th EAMT
2005, Budapest, Hungary, May.
Geoffrey E. Hinton. 1999. Products of experts. In Artifi-
cial Neural Networks, 1999. ICANN 99. Ninth Interna-
tional Conference on (Conf. Publ. No. 470), volume 1,
pages 1–6.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264–271, Prague, Czech
Republic, June. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ’07, pages 224–
227, Stroudsburg, PA, USA. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, pages 127–133, Edmonton, May.
NAACL.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ’09, pages
576–584, Stroudsburg, PA, USA. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual Meet-
ing of the ACL, pages 440–447, Hongkong, China, Oc-
tober.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the ACL, Sapporo, July. ACL.
</reference>
<page confidence="0.979481">
948
</page>
<reference confidence="0.998625386363637">
Slav Petrov. 2010. Products of random latent variable
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ’10, pages 19–27, Stroudsburg, PA, USA. ACL.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Joel Martin, and Aaron Tikuisis. 2005.
Portage: A phrase-based machine translation system.
In In Proceedings of the ACL Worskhop on Building
and Using Parallel Texts, Ann Arbor. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, 97(97), April.
Kristie Seymore and Ronald Rosenfeld. 1997. Us-
ing story topics for language model adaptation. In
George Kokkinakis, Nikos Fakotakis, and Evangelos
Dermatas, editors, EUROSPEECH. ISCA.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 18–25, Stroudsburg, PA, USA. ACL.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257–
286.
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237–248. John
Benjamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 25–32, Prague, Czech Republic, June. ACL.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell’s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157–175,
September.
</reference>
<page confidence="0.999003">
949
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839586">
<title confidence="0.999932">Mixing Multiple Translation Models in Statistical Machine Translation</title>
<author confidence="0.994812">George Baskaran Anoop</author>
<affiliation confidence="0.885117">Fraser University, 8888 University Dr., Burnaby, BC,</affiliation>
<address confidence="0.952158">Research Council Canada, 283 Alexandre-Tach´e Blvd, Gatineau, QC,</address>
<email confidence="0.994392">george.foster@nrc.gc.ca</email>
<abstract confidence="0.998295875">Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bacchiani</author>
<author>B Roark</author>
</authors>
<title>Unsupervised language model adaptation.</title>
<date>2003</date>
<booktitle>In Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP ’03). 2003 IEEE International Conference on,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<contexts>
<context position="2356" citStr="Bacchiani and Roark, 2003" startWordPosition="357" endWordPosition="360">t-of-domain (OUT) model to represent a target domain (in-domain or IN). Common techniques for model adaptation adapt two main components of contemporary state-of-theart SMT systems: the language model and the translation model. However, language model adaptation is a more straight-forward problem compared to translation model adaptation, because various measures such as perplexity of adapted language models can be easily computed on data in the target domain. As a result, language model adaptation has been well studied in various work (Clarkson and Robinson, 1997; Seymore and Rosenfeld, 1997; Bacchiani and Roark, 2003; Eck et al., 2004) both for speech recognition and for machine translation. It is also easier to obtain monolingual data in the target domain, compared to bilingual data which is required for translation model adaptation. In this paper, we focused on adapting only the translation model by fixing a language model for all the experiments. We expect domain adaptation for machine translation can be improved further by combining orthogonal techniques for translation model adaptation combined with language model adaptation. In this paper, a new approach for adapting the translation model is propose</context>
</contexts>
<marker>Bacchiani, Roark, 2003</marker>
<rawString>M. Bacchiani and B. Roark. 2003. Unsupervised language model adaptation. In Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP ’03). 2003 IEEE International Conference on, volume 1, pages I–224 – I–227 vol.1, april.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09,</booktitle>
<pages>182--189</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22716" citStr="Bertoldi and Federico, 2009" startWordPosition="3794" endWordPosition="3797"> menstruations irr´eguli`eres REF amenorrhoea , irregular menstruation OUT am´enorrh´ee , irregular menstruation SOURCE le traitement par naglazyme doit ˆetre supervis´e par un m´edecin ayant l’ exp´erience de la prise en charge des patients atteints de mps vi ou d’ une autre maladie m´etabolique h´er´editaire . REF naglazyme treatment should be supervised by a physician experienced in the management of patients with mps vi or other inherited metabolic diseases . IN naglazyme treatment should be supervis´e by a doctor the with of patients with mps vi. or another disease hereditary metabolic . Bertoldi and Federico, 2009). In this approach, a system is trained on the parallel OUT and IN data and it is used to translate the monolingual IN data set. Iteratively, most confident sentence pairs are selected and added to the training corpus on which a new system is trained. 5.2 System Combination Tackling the model adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009). Among these approaches are sentence-based, phrase-based and word-based output combination methods. In a similar approach, Koehn and Schroeder (2007) use a f</context>
<context position="24528" citStr="Bertoldi and Federico, 2009" startWordPosition="4087" endWordPosition="4091">ntersection, for each span only the hypotheses would be used that are present in all phrase tables. For each set of hypothesis with the same source and target phrases, a new hypothesis is created whose feature-set is the union of feature sets of all corresponding hypotheses. Union, on the other hand, uses hypotheses from all the phrase tables. The feature set of these hypotheses are expanded to include one feature set for each table. However, for the corresponding feature values of those phrase-tables that did not have a particular phrase-pair, a default log probability value of 0 is assumed (Bertoldi and Federico, 2009) which is counter-intuitive as it boosts the score of hypotheses with phrase-pairs that do not belong to all of the translation tables. Our approach is different from Koehn and Schroeder (2007) in a number of ways. Firstly, unlike the multi-table support of Moses which only supports phrase-based translation table combination, our approach supports ensembles of both hierarchical and phrase-based systems. With little modification, it can also support ensemble of syntax-based systems with the other two state-of-the-art SMT sys946 tems. Secondly, our combining method uses the union option, but ins</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, pages 182–189, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features. In</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="25639" citStr="Chiang et al., 2008" startWordPosition="4264" endWordPosition="4267">the other two state-of-the-art SMT sys946 tems. Secondly, our combining method uses the union option, but instead of preserving the features of all phrase-tables, it only combines their scores using various mixture operations. This enables us to experiment with a number of different operations as opposed to sticking to only one combination method. Finally, by avoiding increasing the number of features we can add as many translation models as we need without serious performance drop. In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al., 2008). Our approach differs from the model combination approach of DeNero et al. (2010), a generalization of consensus or minimum Bayes risk decoding where the search space consists of those of multiple systems, in that model combination uses forest of derivations of all component models to do the combination. In other words, it requires all component models to fully decode each sentence, compute n-gram expectations from each component model and calculate posterior probabilities over translation derivations. While, in our approach we only use partial hypotheses from component models and the derivat</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In In Proceedings of the Conference on Empirical Methods in Natural Language Processing. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3576" citStr="Chiang, 2005" startWordPosition="559" endWordPosition="560">se a novel system combination approach called ensemble decoding in order to combine two or more translation models with the goal of constructing a system that outperforms all the component models. The strength of this system combination method is that the systems are combined in the decoder. This enables the decoder to pick the best hypotheses for each span of the input. The main applications of ensemble models are domain adaptation, domain mixing and system combination. We have modified Kriya (Sankaran et al., 2012), an in-house implementation of hierarchical phrase-based translation system (Chiang, 2005), to implement ensemble decoding using multiple translation models. We compare the results of ensemble decoding with a number of baselines for domain adaptation. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940–949, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics as well as the linear mixture model of (Foster et al., Xλ� =argmax �p(e, f) log M λmp</context>
<context position="6537" citStr="Chiang, 2005" startWordPosition="1062" endWordPosition="1063">a phrase pair does not appear in a component phrase table, we set the corresponding pm(e |f) to 0; pairs in p(e, f) that do not appear in at least one component table are discarded. We learn separate linear mixtures for relative-frequency and lexical estimates for both p(e|�f) and p(�f|e). These four features then appear in the top-level model as usual – there is no runtime cost for the linear mixture. 3 Ensemble Decoding Ensemble decoding is a way to combine the expertise of different models in one single model. The current implementation is able to combine hierarchical phrase-based systems (Chiang, 2005) as well as phrase-based translation systems (Koehn et al., 2003). However, the method can be easily extended to support combining a number of heterogeneous translation systems e.g. phrase-based, hierarchical phrase-based, and/or syntax-based systems. This section explains how such models can be combined during the decoding. Given a number of translation models which are already trained and tuned, the ensemble decoder uses hypotheses constructed from all of the models in order to translate a sentence. We use the bottomup CKY parsing algorithm for decoding. For each sentence, a CKY chart is con</context>
<context position="15254" citStr="Chiang, 2005" startWordPosition="2590" endWordPosition="2591">al translation model (TM) probabilities in both directions; worddisplacement distortion model; language model (LM) and word count. The corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sankaran et al., 2012) which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty. GIZA++(Och and Ney, 2000) has been used for word alignment with phrase length limit of 7. In both systems, feature weights were optimized using MERT (Och, 2003) and with a 5-gram lan1www.statmt.org/europarl 2Please contact the authors to access the data-sets. guage model and Kneser-Ney smoothing was used in all the experiments. We used SRILM (Stolcke, 2002) as the langugage model toolkit. Fixing the language model allows us to compare various translation model combination techniques</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Civera</author>
<author>Alfons Juan</author>
</authors>
<title>Domain adaptation in statistical machine translation with mixture modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>177--180</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21326" citStr="Civera and Juan, 2007" startWordPosition="3582" endWordPosition="3585">es. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; 945 Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems. IN amenorrhoea , menstruations irr´eguli`eres ENSEMBLE amenorrhoea , irregular menstruation metabolic disease . in the management of patients with mps vi or other hereditary by a doctor with the experience of the care OUT naglazyme ’s treatment must be supervised supervised by a ph</context>
</contexts>
<marker>Civera, Juan, 2007</marker>
<rawString>Jorge Civera and Alfons Juan. 2007. Domain adaptation in statistical machine translation with mixture modelling. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 177–180, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>176--181</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="17096" citStr="Clark et al. (2011)" startWordPosition="2891" endWordPosition="2894">IN 31.84 33.69 OUT 24.08 25.32 IN + OUT 31.75 33.76 LOGLIN 32.21 – LINMIX 33.81 35.57 Table 2: The results of various baselines implemented in a phrase-based (PBS) and a Hiero SMT on EMEA. Table 3 shows the results of ensemble decoding with different mixture operations and model weight settings. Each mixture operation has been evaluated on the test-set by setting the component weights uniformly (denoted by uniform) and by tuning the weights using CONDOR (denoted by tuned) on a held-out set. The tuned scores (3rd column in Table 3) are averages of three runs with different initial points as in Clark et al. (2011). We also reported the BLEU scores when we applied the span-wise normalization heuristic. All of these mixture operations were able to significantly improve over the concatenation baseline. In particular, Switching:Max could gain up to 2.2 BLEU points over the concatenation baseline and 0.39 BLEU points over the best performing baseline (i.e. linear mixture model implemented in Hiero) which is statistically significant based on Clark et al. (2011) (p = 0.02). Prod when using with uniform weights gets the 944 Mixture Operation Uniform Tuned Norm. WMAX 35.39 35.47 (s=0.03) 35.47 WSUM 35.35 35.53</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 176–181. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>A Robinson</author>
</authors>
<title>Language model adaptation using mixtures and an exponentially decaying cache.</title>
<date>1997</date>
<booktitle>In Proceedings of the 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’97)-Volume 2 - Volume 2, ICASSP ’97,</booktitle>
<pages>799</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="2300" citStr="Clarkson and Robinson, 1997" startWordPosition="349" endWordPosition="352"> adaptation techniques aim at finding ways to adjust an out-of-domain (OUT) model to represent a target domain (in-domain or IN). Common techniques for model adaptation adapt two main components of contemporary state-of-theart SMT systems: the language model and the translation model. However, language model adaptation is a more straight-forward problem compared to translation model adaptation, because various measures such as perplexity of adapted language models can be easily computed on data in the target domain. As a result, language model adaptation has been well studied in various work (Clarkson and Robinson, 1997; Seymore and Rosenfeld, 1997; Bacchiani and Roark, 2003; Eck et al., 2004) both for speech recognition and for machine translation. It is also easier to obtain monolingual data in the target domain, compared to bilingual data which is required for translation model adaptation. In this paper, we focused on adapting only the translation model by fixing a language model for all the experiments. We expect domain adaptation for machine translation can be improved further by combining orthogonal techniques for translation model adaptation combined with language model adaptation. In this paper, a ne</context>
</contexts>
<marker>Clarkson, Robinson, 1997</marker>
<rawString>P. Clarkson and A. Robinson. 1997. Language model adaptation using mixtures and an exponentially decaying cache. In Proceedings of the 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’97)-Volume 2 - Volume 2, ICASSP ’97, pages 799–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>J. Artif. Int. Res.,</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. J. Artif. Int. Res., 26:101–126, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Shankar Kumar</author>
<author>Ciprian Chelba</author>
<author>Franz Och</author>
</authors>
<title>Model combination for machine translation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>975--983</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="25721" citStr="DeNero et al. (2010)" startWordPosition="4278" endWordPosition="4281">s the union option, but instead of preserving the features of all phrase-tables, it only combines their scores using various mixture operations. This enables us to experiment with a number of different operations as opposed to sticking to only one combination method. Finally, by avoiding increasing the number of features we can add as many translation models as we need without serious performance drop. In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al., 2008). Our approach differs from the model combination approach of DeNero et al. (2010), a generalization of consensus or minimum Bayes risk decoding where the search space consists of those of multiple systems, in that model combination uses forest of derivations of all component models to do the combination. In other words, it requires all component models to fully decode each sentence, compute n-gram expectations from each component model and calculate posterior probabilities over translation derivations. While, in our approach we only use partial hypotheses from component models and the derivation forest is constructed by the ensemble model. A major difference is that in the</context>
</contexts>
<marker>DeNero, Kumar, Chelba, Och, 2010</marker>
<rawString>John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 975–983, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Language model adaptation for statistical machine translation based on information retrieval.</title>
<date>2004</date>
<booktitle>In In Proceedings of LREC.</booktitle>
<contexts>
<context position="2375" citStr="Eck et al., 2004" startWordPosition="361" endWordPosition="364">represent a target domain (in-domain or IN). Common techniques for model adaptation adapt two main components of contemporary state-of-theart SMT systems: the language model and the translation model. However, language model adaptation is a more straight-forward problem compared to translation model adaptation, because various measures such as perplexity of adapted language models can be easily computed on data in the target domain. As a result, language model adaptation has been well studied in various work (Clarkson and Robinson, 1997; Seymore and Rosenfeld, 1997; Bacchiani and Roark, 2003; Eck et al., 2004) both for speech recognition and for machine translation. It is also easier to obtain monolingual data in the target domain, compared to bilingual data which is required for translation model adaptation. In this paper, we focused on adapting only the translation model by fixing a language model for all the experiments. We expect domain adaptation for machine translation can be improved further by combining orthogonal techniques for translation model adaptation combined with language model adaptation. In this paper, a new approach for adapting the translation model is proposed. We use a novel s</context>
<context position="20080" citStr="Eck et al., 2004" startWordPosition="3385" endWordPosition="3388">ams from the IN and OUT models to construct a better translation than both of them. In the first example, there are two OOVs one for each of the IN and OUT models. Our approach is able to resolve the OOV issues by taking advantage of the other model’s presence. Similarly, the second example shows how ensemble decoding improves lexical choices as well as word re-orderings. 5 Related Work 5.1 Domain Adaptation Early approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005). Foster et al. (2010), however, uses a different approach to select related sentences from OUT. They use language model perplexities from IN to select relavant sentences from OUT. These sentences are used to enrich the IN training set. Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum´e and Marcu, 2006). Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like i</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2004</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2004. Language model adaptation for statistical machine translation based on information retrieval. In In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixture-model adaptation for smt.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>128--135</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3892" citStr="Foster and Kuhn, 2007" startWordPosition="605" endWordPosition="608">he decoder to pick the best hypotheses for each span of the input. The main applications of ensemble models are domain adaptation, domain mixing and system combination. We have modified Kriya (Sankaran et al., 2012), an in-house implementation of hierarchical phrase-based translation system (Chiang, 2005), to implement ensemble decoding using multiple translation models. We compare the results of ensemble decoding with a number of baselines for domain adaptation. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940–949, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics as well as the linear mixture model of (Foster et al., Xλ� =argmax �p(e, f) log M λmpm(�e |f) 2010) for conditional phrase-pair probabilities over a fe, X IN and OUT. Furthermore, within the framework of m ensemble decoding, we study and evaluate various methods for combining translation tables. 2 Baselines The natural baseline for model adaption is to concatenate the IN and OUT data into a single </context>
<context position="21350" citStr="Foster and Kuhn, 2007" startWordPosition="3586" endWordPosition="3589">) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; 945 Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems. IN amenorrhoea , menstruations irr´eguli`eres ENSEMBLE amenorrhoea , irregular menstruation metabolic disease . in the management of patients with mps vi or other hereditary by a doctor with the experience of the care OUT naglazyme ’s treatment must be supervised supervised by a physician experienced ENSE</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 128–135, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>451--459</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5496" citStr="Foster et al. (2010)" startWordPosition="881" endWordPosition="884">each λm is a weight in the top-level log-linear model, set so as to maximize dev-set BLEU using minimum error rate training (Och, 2003). We learn separate weights for relative-frequency and lexical estimates for both pm(e |f) and pm(�f|e). Thus, for 2 component models (from IN and OUT training corpora), there are 4 * 2 = 8 TM weights to tune. Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm(e |f) to a small epsilon value. 2.2 Linear Mixture Linear TM mixtures are of the form: λmpm(�e |f) Our technique for setting λm is similar to that outlined in Foster et al. (2010). We first extract a joint phrase-pair distribution p(e, f) from the development set using standard techniques (HMM word alignment with grow-diag-and symmeterization (Koehn et al., 2003)). We then find the set of weights λ� that minimize the cross-entropy of the mixture p(e |f) with respect to ,5(e, f): For efficiency and stability, we use the EM algorithm to find �λ, rather than L-BFGS as in (Foster et al., 2010). Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm(e |f) to 0; pairs in p(e, f) that do not appear in at least one component table are d</context>
<context position="20128" citStr="Foster et al. (2010)" startWordPosition="3393" endWordPosition="3396">a better translation than both of them. In the first example, there are two OOVs one for each of the IN and OUT models. Our approach is able to resolve the OOV issues by taking advantage of the other model’s presence. Similarly, the second example shows how ensemble decoding improves lexical choices as well as word re-orderings. 5 Related Work 5.1 Domain Adaptation Early approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005). Foster et al. (2010), however, uses a different approach to select related sentences from OUT. They use language model perplexities from IN to select relavant sentences from OUT. These sentences are used to enrich the IN training set. Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum´e and Marcu, 2006). Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like instances than OUT instances. Foster et al. (2010</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 451– 459, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>CMU system combination for WMT’09.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09,</booktitle>
<pages>47--50</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23158" citStr="Hildebrand and Vogel, 2009" startWordPosition="3867" endWordPosition="3871">ted metabolic diseases . IN naglazyme treatment should be supervis´e by a doctor the with of patients with mps vi. or another disease hereditary metabolic . Bertoldi and Federico, 2009). In this approach, a system is trained on the parallel OUT and IN data and it is used to translate the monolingual IN data set. Iteratively, most confident sentence pairs are selected and added to the training corpus on which a new system is trained. 5.2 System Combination Tackling the model adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009). Among these approaches are sentence-based, phrase-based and word-based output combination methods. In a similar approach, Koehn and Schroeder (2007) use a feature of the factored translation model framework in Moses SMT system (Koehn and Schroeder, 2007) to use multiple alternative decoding paths. Two decoding paths, one for each translation table (IN and OUT), were used during decoding. The weights are set with minimum error rate training (Och, 2003). Our work is closely related to Koehn and Schroeder (2007) but uses a different approach to deal with multiple translation tables. The Moses S</context>
</contexts>
<marker>Hildebrand, Vogel, 2009</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2009. CMU system combination for WMT’09. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, pages 47–50, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th EAMT 2005,</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="20106" citStr="Hildebrand et al., 2005" startWordPosition="3389" endWordPosition="3392">d OUT models to construct a better translation than both of them. In the first example, there are two OOVs one for each of the IN and OUT models. Our approach is able to resolve the OOV issues by taking advantage of the other model’s presence. Similarly, the second example shows how ensemble decoding improves lexical choices as well as word re-orderings. 5 Related Work 5.1 Domain Adaptation Early approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005). Foster et al. (2010), however, uses a different approach to select related sentences from OUT. They use language model perplexities from IN to select relavant sentences from OUT. These sentences are used to enrich the IN training set. Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum´e and Marcu, 2006). Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like instances than OUT instance</context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of the 10th EAMT 2005, Budapest, Hungary, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Products of experts.</title>
<date>1999</date>
<booktitle>In Artificial Neural Networks,</booktitle>
<volume>470</volume>
<pages>1--6</pages>
<contexts>
<context position="10465" citStr="Hinton, 1999" startWordPosition="1784" endWordPosition="1785"> the characteristics of these mixture operations. • Weighted Sum (wsum): in wsum the ensemble probability is proportional to the weighted sum of all individual model probabilities (i.e. linear mixture). ⊕ w2 · φ2 |{z } 2nd model �⊕ ··· p(�e |f) ∝ M λm exp (wm · φm) p(�e |f) = M δ( f, m) pm(�e |f) X X m m where m denotes the index of component models, M is the total number of them and λi is the weight for component i. • Weighted Max (wmax): where the ensemble score is the weighted max of all model scores. f) ∝ max (λm exp (wm · φm �� m • Product (prod): in Product models or Product of Experts (Hinton, 1999), the probability of the ensemble model or a rule is computed as the product of the probabilities of all components (or equally the sum of log-probabilities, i.e. log-linear mixture). Product models can also make use of weights to control the contribution of each component. These models are p(�e| 942 Figure 1: The cells in the CKY chart are populated using rules from all component models and sub-span cells. generally known as Logarithmic Opinion Pools (LOPs) where: M p(e f) a exp ( E Am (wm - om)) m Product models have been used in combining LMs and TMs in SMT as well as some other NLP tasks s</context>
</contexts>
<marker>Hinton, 1999</marker>
<rawString>Geoffrey E. Hinton. 1999. Products of experts. In Artificial Neural Networks, 1999. ICANN 99. Ninth International Conference on (Conf. Publ. No. 470), volume 1, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>264--271</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="20502" citStr="Jiang and Zhai (2007)" startWordPosition="3453" endWordPosition="3456"> approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005). Foster et al. (2010), however, uses a different approach to select related sentences from OUT. They use language model perplexities from IN to select relavant sentences from OUT. These sentences are used to enrich the IN training set. Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum´e and Marcu, 2006). Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like instances than OUT instances. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work </context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>224--227</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21303" citStr="Koehn and Schroeder, 2007" startWordPosition="3578" endWordPosition="3581"> instances than OUT instances. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; 945 Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems. IN amenorrhoea , menstruations irr´eguli`eres ENSEMBLE amenorrhoea , irregular menstruation metabolic disease . in the management of patients with mps vi or other hereditary by a doctor with the experience of the care OUT naglazyme ’s treatment must be superv</context>
<context position="23129" citStr="Koehn and Schroeder, 2007" startWordPosition="3863" endWordPosition="3866">with mps vi or other inherited metabolic diseases . IN naglazyme treatment should be supervis´e by a doctor the with of patients with mps vi. or another disease hereditary metabolic . Bertoldi and Federico, 2009). In this approach, a system is trained on the parallel OUT and IN data and it is used to translate the monolingual IN data set. Iteratively, most confident sentence pairs are selected and added to the training corpus on which a new system is trained. 5.2 System Combination Tackling the model adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009). Among these approaches are sentence-based, phrase-based and word-based output combination methods. In a similar approach, Koehn and Schroeder (2007) use a feature of the factored translation model framework in Moses SMT system (Koehn and Schroeder, 2007) to use multiple alternative decoding paths. Two decoding paths, one for each translation table (IN and OUT), were used during decoding. The weights are set with minimum error rate training (Och, 2003). Our work is closely related to Koehn and Schroeder (2007) but uses a different approach to deal with multiple tr</context>
<context position="24721" citStr="Koehn and Schroeder (2007)" startWordPosition="4119" endWordPosition="4122"> whose feature-set is the union of feature sets of all corresponding hypotheses. Union, on the other hand, uses hypotheses from all the phrase tables. The feature set of these hypotheses are expanded to include one feature set for each table. However, for the corresponding feature values of those phrase-tables that did not have a particular phrase-pair, a default log probability value of 0 is assumed (Bertoldi and Federico, 2009) which is counter-intuitive as it boosts the score of hypotheses with phrase-pairs that do not belong to all of the translation tables. Our approach is different from Koehn and Schroeder (2007) in a number of ways. Firstly, unlike the multi-table support of Moses which only supports phrase-based translation table combination, our approach supports ensembles of both hierarchical and phrase-based systems. With little modification, it can also support ensemble of syntax-based systems with the other two state-of-the-art SMT sys946 tems. Secondly, our combining method uses the union option, but instead of preserving the features of all phrase-tables, it only combines their scores using various mixture operations. This enables us to experiment with a number of different operations as oppo</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 224– 227, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>127--133</pages>
<publisher>NAACL.</publisher>
<location>Edmonton,</location>
<contexts>
<context position="5682" citStr="Koehn et al., 2003" startWordPosition="909" endWordPosition="912">nd lexical estimates for both pm(e |f) and pm(�f|e). Thus, for 2 component models (from IN and OUT training corpora), there are 4 * 2 = 8 TM weights to tune. Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm(e |f) to a small epsilon value. 2.2 Linear Mixture Linear TM mixtures are of the form: λmpm(�e |f) Our technique for setting λm is similar to that outlined in Foster et al. (2010). We first extract a joint phrase-pair distribution p(e, f) from the development set using standard techniques (HMM word alignment with grow-diag-and symmeterization (Koehn et al., 2003)). We then find the set of weights λ� that minimize the cross-entropy of the mixture p(e |f) with respect to ,5(e, f): For efficiency and stability, we use the EM algorithm to find �λ, rather than L-BFGS as in (Foster et al., 2010). Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm(e |f) to 0; pairs in p(e, f) that do not appear in at least one component table are discarded. We learn separate linear mixtures for relative-frequency and lexical estimates for both p(e|�f) and p(�f|e). These four features then appear in the top-level model as usual – t</context>
<context position="14550" citStr="Koehn et al., 2003" startWordPosition="2478" endWordPosition="2481">CONDOR. 4 Experiments &amp; Results 4.1 Experimental Setup We carried out translation experiments using the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus1 as OUT, for French to English translation. The dev and test sets were randomly chosen from the EMEA corpus.2 The details of datasets used are summarized in Table 1. Dataset Sents Words French English EMEA 11770 168K 144K Europarl 1.3M 40M 37M Dev 1533 29K 25K Test 1522 29K 25K Table 1: Training, dev and test sets for EMEA. For the mixture baselines, we used a standard one-pass phrase-based system (Koehn et al., 2003), Portage (Sadat et al., 2005), with the following 7 features: relative-frequency and lexical translation model (TM) probabilities in both directions; worddisplacement distortion model; language model (LM) and word count. The corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phr</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference of the NAACL, pages 127–133, Edmonton, May. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Haitao Mi</author>
<author>Yang Feng</author>
<author>Qun Liu</author>
</authors>
<title>Joint decoding with multiple translation models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09,</booktitle>
<pages>576--584</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26879" citStr="Liu et al. (2009)" startWordPosition="4467" endWordPosition="4470">ed by the ensemble model. A major difference is that in the model combination approach the component search spaces are conjoined and they are not intermingled as opposed to our approach where these search spaces are intermixed on spans. This enables us to generate new sentences that cannot be generated by component models. Furthermore, various combination methods can be explored in our approach. Finally, main techniques used in this work are orthogonal to our approach such as Minimum Bayes Risk decoding, using n-gram features and tuning using MERT. Finally, our work is most similar to that of Liu et al. (2009) where max-derivation and maxtranslation decoding have been used. Maxderivation finds a derivation with highest score and max-translation finds the highest scoring translation by summing the score of all derivations with the same yield. The combination can be done in two levels: translation-level and derivation-level. Their derivation-level max-translation decoding is similar to our ensemble decoding with wsum as the mixture operation. We did not restrict ourself to this particular mixture operation and experimented with a number of different mixing techniques and as Table 3 shows we could imp</context>
</contexts>
<marker>Liu, Mi, Feng, Liu, 2009</marker>
<rawString>Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009. Joint decoding with multiple translation models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 576–584, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="15392" citStr="Och and Ney, 2000" startWordPosition="2607" endWordPosition="2610">e corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sankaran et al., 2012) which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty. GIZA++(Och and Ney, 2000) has been used for word alignment with phrase length limit of 7. In both systems, feature weights were optimized using MERT (Och, 2003) and with a 5-gram lan1www.statmt.org/europarl 2Please contact the authors to access the data-sets. guage model and Kneser-Ney smoothing was used in all the experiments. We used SRILM (Stolcke, 2002) as the langugage model toolkit. Fixing the language model allows us to compare various translation model combination techniques. 4.2 Results Table 2 shows the results of the baselines. The first group are the baseline results on the phrase-based system discussed in</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the ACL, pages 440–447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Meeting of the ACL,</booktitle>
<publisher>ACL.</publisher>
<location>Sapporo,</location>
<contexts>
<context position="5011" citStr="Och, 2003" startWordPosition="795" endWordPosition="796">he natural baseline for model adaption is to concatenate the IN and OUT data into a single parallel corpus and train a model on it. In addition to this baseline, we have experimented with two more sophisticated baselines which are based on mixture techniques. 2.1 Log-Linear Mixture Log-linear translation model (TM) mixtures are of the form: M 1l p(e |f) oc exp (Xλm log pm(e|�f)J m where m ranges over IN and OUT, pm(e |f) is an estimate from a component phrase table, and each λm is a weight in the top-level log-linear model, set so as to maximize dev-set BLEU using minimum error rate training (Och, 2003). We learn separate weights for relative-frequency and lexical estimates for both pm(e |f) and pm(�f|e). Thus, for 2 component models (from IN and OUT training corpora), there are 4 * 2 = 8 TM weights to tune. Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm(e |f) to a small epsilon value. 2.2 Linear Mixture Linear TM mixtures are of the form: λmpm(�e |f) Our technique for setting λm is similar to that outlined in Foster et al. (2010). We first extract a joint phrase-pair distribution p(e, f) from the development set using standard techniques (HMM</context>
<context position="15527" citStr="Och, 2003" startWordPosition="2632" endWordPosition="2633">s, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sankaran et al., 2012) which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty. GIZA++(Och and Ney, 2000) has been used for word alignment with phrase length limit of 7. In both systems, feature weights were optimized using MERT (Och, 2003) and with a 5-gram lan1www.statmt.org/europarl 2Please contact the authors to access the data-sets. guage model and Kneser-Ney smoothing was used in all the experiments. We used SRILM (Stolcke, 2002) as the langugage model toolkit. Fixing the language model allows us to compare various translation model combination techniques. 4.2 Results Table 2 shows the results of the baselines. The first group are the baseline results on the phrase-based system discussed in Section 2 and the second group are those of our hierarchical MT system. Since the Hiero baselines results were substantially better th</context>
<context position="23615" citStr="Och, 2003" startWordPosition="3939" endWordPosition="3940">odel adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009). Among these approaches are sentence-based, phrase-based and word-based output combination methods. In a similar approach, Koehn and Schroeder (2007) use a feature of the factored translation model framework in Moses SMT system (Koehn and Schroeder, 2007) to use multiple alternative decoding paths. Two decoding paths, one for each translation table (IN and OUT), were used during decoding. The weights are set with minimum error rate training (Och, 2003). Our work is closely related to Koehn and Schroeder (2007) but uses a different approach to deal with multiple translation tables. The Moses SMT system implements (Koehn and Schroeder, 2007) and can treat multiple translation tables in two different ways: intersection and union. In intersection, for each span only the hypotheses would be used that are present in all phrase tables. For each set of hypothesis with the same source and target phrases, a new hypothesis is created whose feature-set is the union of feature sets of all corresponding hypotheses. Union, on the other hand, uses hypothes</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41th Annual Meeting of the ACL, Sapporo, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of random latent variable grammars. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>pages</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11103" citStr="Petrov, 2010" startWordPosition="1899" endWordPosition="1900">ensemble model or a rule is computed as the product of the probabilities of all components (or equally the sum of log-probabilities, i.e. log-linear mixture). Product models can also make use of weights to control the contribution of each component. These models are p(�e| 942 Figure 1: The cells in the CKY chart are populated using rules from all component models and sub-span cells. generally known as Logarithmic Opinion Pools (LOPs) where: M p(e f) a exp ( E Am (wm - om)) m Product models have been used in combining LMs and TMs in SMT as well as some other NLP tasks such as ensemble parsing (Petrov, 2010). Each of these mixture operations has a specific property that makes it work in specific domain adaptation or system combination scenarios. For instance, LOPs may not be optimal for domain adaptation in the setting where there are two or more models trained on heterogeneous corpora. As discussed in (Smith et al., 2005), LOPs work best when all the models accuracies are high and close to each other with some degree of diversity. LOPs give veto power to any of the component models and this perfectly works for settings such as the one in (Petrov, 2010) where a number of parsers are trained by ch</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of random latent variable grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 19–27, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fatiha Sadat</author>
</authors>
<location>Howard Johnson, Akakpo Agbago, George</location>
<marker>Sadat, </marker>
<rawString>Fatiha Sadat, Howard Johnson, Akakpo Agbago, George</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Martin Foster</author>
<author>Aaron Tikuisis</author>
</authors>
<title>Portage: A phrase-based machine translation system. In</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Worskhop on Building and Using Parallel Texts,</booktitle>
<publisher>ACL.</publisher>
<location>Ann Arbor.</location>
<marker>Foster, Tikuisis, 2005</marker>
<rawString>Foster, Joel Martin, and Aaron Tikuisis. 2005. Portage: A phrase-based machine translation system. In In Proceedings of the ACL Worskhop on Building and Using Parallel Texts, Ann Arbor. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baskaran Sankaran</author>
<author>Majid Razmara</author>
<author>Anoop Sarkar</author>
</authors>
<title>Kriya an end-to-end hierarchical phrase-based mt system.</title>
<date>2012</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<volume>97</volume>
<issue>97</issue>
<contexts>
<context position="3485" citStr="Sankaran et al., 2012" startWordPosition="546" endWordPosition="549">model adaptation. In this paper, a new approach for adapting the translation model is proposed. We use a novel system combination approach called ensemble decoding in order to combine two or more translation models with the goal of constructing a system that outperforms all the component models. The strength of this system combination method is that the systems are combined in the decoder. This enables the decoder to pick the best hypotheses for each span of the input. The main applications of ensemble models are domain adaptation, domain mixing and system combination. We have modified Kriya (Sankaran et al., 2012), an in-house implementation of hierarchical phrase-based translation system (Chiang, 2005), to implement ensemble decoding using multiple translation models. We compare the results of ensemble decoding with a number of baselines for domain adaptation. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940–949, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Lingui</context>
<context position="15197" citStr="Sankaran et al., 2012" startWordPosition="2579" endWordPosition="2582">2005), with the following 7 features: relative-frequency and lexical translation model (TM) probabilities in both directions; worddisplacement distortion model; language model (LM) and word count. The corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sankaran et al., 2012) which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty. GIZA++(Och and Ney, 2000) has been used for word alignment with phrase length limit of 7. In both systems, feature weights were optimized using MERT (Och, 2003) and with a 5-gram lan1www.statmt.org/europarl 2Please contact the authors to access the data-sets. guage model and Kneser-Ney smoothing was used in all the experiments. We used SRILM (Stolcke, 2002) as the langugage model toolkit. Fixing the language model allows us to</context>
</contexts>
<marker>Sankaran, Razmara, Sarkar, 2012</marker>
<rawString>Baskaran Sankaran, Majid Razmara, and Anoop Sarkar. 2012. Kriya an end-to-end hierarchical phrase-based mt system. The Prague Bulletin of Mathematical Linguistics, 97(97), April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Using story topics for language model adaptation.</title>
<date>1997</date>
<editor>In George Kokkinakis, Nikos Fakotakis, and Evangelos Dermatas, editors, EUROSPEECH.</editor>
<publisher>ISCA.</publisher>
<contexts>
<context position="2329" citStr="Seymore and Rosenfeld, 1997" startWordPosition="353" endWordPosition="356"> finding ways to adjust an out-of-domain (OUT) model to represent a target domain (in-domain or IN). Common techniques for model adaptation adapt two main components of contemporary state-of-theart SMT systems: the language model and the translation model. However, language model adaptation is a more straight-forward problem compared to translation model adaptation, because various measures such as perplexity of adapted language models can be easily computed on data in the target domain. As a result, language model adaptation has been well studied in various work (Clarkson and Robinson, 1997; Seymore and Rosenfeld, 1997; Bacchiani and Roark, 2003; Eck et al., 2004) both for speech recognition and for machine translation. It is also easier to obtain monolingual data in the target domain, compared to bilingual data which is required for translation model adaptation. In this paper, we focused on adapting only the translation model by fixing a language model for all the experiments. We expect domain adaptation for machine translation can be improved further by combining orthogonal techniques for translation model adaptation combined with language model adaptation. In this paper, a new approach for adapting the t</context>
</contexts>
<marker>Seymore, Rosenfeld, 1997</marker>
<rawString>Kristie Seymore and Ronald Rosenfeld. 1997. Using story topics for language model adaptation. In George Kokkinakis, Nikos Fakotakis, and Evangelos Dermatas, editors, EUROSPEECH. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Smith</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Logarithmic opinion pools for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>18--25</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11424" citStr="Smith et al., 2005" startWordPosition="1953" endWordPosition="1956">re populated using rules from all component models and sub-span cells. generally known as Logarithmic Opinion Pools (LOPs) where: M p(e f) a exp ( E Am (wm - om)) m Product models have been used in combining LMs and TMs in SMT as well as some other NLP tasks such as ensemble parsing (Petrov, 2010). Each of these mixture operations has a specific property that makes it work in specific domain adaptation or system combination scenarios. For instance, LOPs may not be optimal for domain adaptation in the setting where there are two or more models trained on heterogeneous corpora. As discussed in (Smith et al., 2005), LOPs work best when all the models accuracies are high and close to each other with some degree of diversity. LOPs give veto power to any of the component models and this perfectly works for settings such as the one in (Petrov, 2010) where a number of parsers are trained by changing the randomization seeds but having the same base parser and using the same training set. They noticed that parsers trained using different randomization seeds have high accuracies but there are some diversities among them and they used product models for their advantage to get an even better parser. We assume tha</context>
</contexts>
<marker>Smith, Cohn, Osborne, 2005</marker>
<rawString>Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Logarithmic opinion pools for conditional random fields. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 18–25, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings International Conference on Spoken Language Processing,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="15726" citStr="Stolcke, 2002" startWordPosition="2662" endWordPosition="2663">use implementation of hierarchical phrase-based system, Kriya (Sankaran et al., 2012) which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty. GIZA++(Och and Ney, 2000) has been used for word alignment with phrase length limit of 7. In both systems, feature weights were optimized using MERT (Och, 2003) and with a 5-gram lan1www.statmt.org/europarl 2Please contact the authors to access the data-sets. guage model and Kneser-Ney smoothing was used in all the experiments. We used SRILM (Stolcke, 2002) as the langugage model toolkit. Fixing the language model allows us to compare various translation model combination techniques. 4.2 Results Table 2 shows the results of the baselines. The first group are the baseline results on the phrase-based system discussed in Section 2 and the second group are those of our hierarchical MT system. Since the Hiero baselines results were substantially better than those of the phrase-based model, we also implemented the best-performing baseline, linear mixture, in our Hiero-style MT system and in fact it achieves the hights BLEU score among all the baseline</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings International Conference on Spoken Language Processing, pages 257– 286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorg Tiedemann</author>
</authors>
<title>News from opus - a collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing,</booktitle>
<volume>volume V,</volume>
<pages>237--248</pages>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<publisher>John Benjamins, Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="14092" citStr="Tiedemann, 2009" startWordPosition="2397" endWordPosition="2399">t for future work. A more principled way is to systematically find the most appropriate model weights that can avoid this problem by scaling the scores properly. We used a publicly available toolkit, CONDOR (Vanden Berghen and Bersini, 2005), a direct optimizer based on Powell’s algorithm, that does not require 943 explicit gradient information for the objective function. Component weights for each mixture operation are optimized on the dev-set using CONDOR. 4 Experiments &amp; Results 4.1 Experimental Setup We carried out translation experiments using the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus1 as OUT, for French to English translation. The dev and test sets were randomly chosen from the EMEA corpus.2 The details of datasets used are summarized in Table 1. Dataset Sents Words French English EMEA 11770 168K 144K Europarl 1.3M 40M 37M Dev 1533 29K 25K Test 1522 29K 25K Table 1: Training, dev and test sets for EMEA. For the mixture baselines, we used a standard one-pass phrase-based system (Koehn et al., 2003), Portage (Sadat et al., 2005), with the following 7 features: relative-frequency and lexical translation model (TM) probabilities in both dir</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>Jorg Tiedemann. 2009. News from opus - a collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>25--32</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="21520" citStr="Ueffing et al., 2007" startWordPosition="3612" endWordPosition="3615">es the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; 945 Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems. IN amenorrhoea , menstruations irr´eguli`eres ENSEMBLE amenorrhoea , irregular menstruation metabolic disease . in the management of patients with mps vi or other hereditary by a doctor with the experience of the care OUT naglazyme ’s treatment must be supervised supervised by a physician experienced ENSEMBLE naglazyme treatment should be metabolic disease . in the management of patients with mps vi or other hereditary SOURCE am´enorrh´ee , menstruations irr´eguli`eres RE</context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Transductive learning for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 25–32, Prague, Czech Republic, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Vanden Berghen</author>
<author>Hugues Bersini</author>
</authors>
<title>CONDOR, a new parallel, constrained extension of powell’s UOBYQA algorithm: Experimental results and comparison with the DFO algorithm.</title>
<date>2005</date>
<journal>Journal of Computational and Applied Mathematics,</journal>
<pages>181--157</pages>
<contexts>
<context position="13717" citStr="Berghen and Bersini, 2005" startWordPosition="2340" endWordPosition="2343">he normalized scores only for pruning and the actual scores are intact. We could also globally normalize the scores to obtain posterior probabilities using the inside-outside algorithm. However, we did not try it as the BLEU scores we got using the normalization heuristic was not promissing and it would impose a cost in decoding as well. More investigation on this issue has been left for future work. A more principled way is to systematically find the most appropriate model weights that can avoid this problem by scaling the scores properly. We used a publicly available toolkit, CONDOR (Vanden Berghen and Bersini, 2005), a direct optimizer based on Powell’s algorithm, that does not require 943 explicit gradient information for the objective function. Component weights for each mixture operation are optimized on the dev-set using CONDOR. 4 Experiments &amp; Results 4.1 Experimental Setup We carried out translation experiments using the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus1 as OUT, for French to English translation. The dev and test sets were randomly chosen from the EMEA corpus.2 The details of datasets used are summarized in Table 1. Dataset Sents Words Fr</context>
</contexts>
<marker>Berghen, Bersini, 2005</marker>
<rawString>Frank Vanden Berghen and Hugues Bersini. 2005. CONDOR, a new parallel, constrained extension of powell’s UOBYQA algorithm: Experimental results and comparison with the DFO algorithm. Journal of Computational and Applied Mathematics, 181:157–175, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>