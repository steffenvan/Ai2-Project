<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.998226">
Improved Statistical Machine Translation for Resource-Poor Languages
Using Related Resource-Rich Languages
</title>
<author confidence="0.992728">
Preslav Nakov
</author>
<affiliation confidence="0.999783">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.9719635">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.998967">
nakov@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988210526316">
We propose a novel language-independent
approach for improving statistical ma-
chine translation for resource-poor lan-
guages by exploiting their similarity to
resource-rich ones. More precisely, we
improve the translation from a resource-
poor source language X1 into a resource-
rich language Y given a bi-text contain-
ing a limited number of parallel sentences
for X1-Y and a larger bi-text for X2-Y
for some resource-rich language X2 that
is closely related to X1. The evaluation
for Indonesian→English (using Malay)
and Spanish→English (using Portuguese
and pretending Spanish is resource-poor)
shows an absolute gain of up to 1.35 and
3.37 Bleu points, respectively, which is an
improvement over the rivaling approaches,
while using much less additional data.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999339111111111">
Recent developments in statistical machine trans-
lation (SMT), e.g., the availability of efficient im-
plementations of integrated open-source toolkits
like Moses (Koehn et al., 2007), have made it pos-
sible to build a prototype system with decent trans-
lation quality for any language pair in a few days
or even hours. In theory. In practice, doing so
requires having a large set of parallel sentence-
aligned bi-lingual texts (a bi-text) for that lan-
guage pair, which is often unavailable. Large high-
quality bi-texts are rare; except for Arabic, Chi-
nese, and some official languages of the European
Union (EU), most of the 6,500+ world languages
remain resource-poor from an SMT viewpoint.
While manually creating a small bi-text could
be relatively easy, building a large one is hard,
e.g., because of copyright. Most bi-texts for SMT
come from parliament debates and legislation of
</bodyText>
<author confidence="0.760544">
Hwee Tou Ng
</author>
<affiliation confidence="0.997813">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.955861">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.992276">
nght@comp.nus.edu.sg
</email>
<bodyText confidence="0.999834128205128">
multi-lingual countries (e.g., French-English from
Canada, and Chinese-English from Hong Kong),
or from international organizations like the United
Nations and the European Union. For exam-
ple, the Europarl corpus of parliament proceed-
ings consists of about 1.3M parallel sentences (up
to 44M words) per language for 11 languages
(Koehn, 2005), and the JRC-Acquis corpus pro-
vides a comparable amount of European legisla-
tion in 22 languages (Steinberger et al., 2006).
The official languages of the EU are especially
lucky in that respect; while this includes such
“classic SMT languages” like English and French,
and some important international ones like Span-
ish and Portuguese, most of the rest have a limited
number of speakers and were resource-poor until
recently; this is changing quickly because of the
increasing volume of EU parliament debates and
the ever-growing European legislation. Thus, be-
coming an official language of the EU has turned
out to be an easy recipe for getting resource-rich in
bi-texts quickly. Of course, not all languages are
that ‘lucky’, but many can still benefit.
In this paper, we propose using bi-texts for
resource-rich language pairs to build better SMT
systems for resource-poor ones by exploiting the
similarity between a resource-poor language and a
resource-rich one.
The proposed method allows non-EU languages
to benefit from being closely related to one or
more official languages of the EU, the most
obvious candidates being Norwegian (related to
Swedish), Moldavian1 (related to Romanian), and
Macedonian2 (related to Bulgarian). After Croa-
tia joins the EU, Serbian, Bosnian and Montene-
grin will be able to benefit from Croatian gradually
turning resource-rich (all four split from Serbo-
Croatian after the breakup of Yugoslavia). The
newly-made EU-official (and thus not as resource-
</bodyText>
<footnote confidence="0.996703">
1Not recognized by Romania.
2Not recognized by Bulgaria and Greece.
</footnote>
<page confidence="0.891718">
1358
</page>
<note confidence="0.996683">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1358–1367,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.992957133333333">
rich) Czech and Slovak are another possible pair
of candidates. As we will see below, even such
resource-rich languages like Spanish and Por-
tuguese can benefit from the proposed method. Of
course, many pairs of closely related languages
can be also found outside of Europe, Malay and
Indonesian being just one such example we will
experiment with.
The remainder of the present paper is organized
as follows: Section 2 presents our method, Sec-
tion 3 describes the experiments, and Section 4
discusses the results and the general applicability
of the approach. Section 5 provides an overview
of the related work. Finally, Section 6 concludes
and suggests possible directions for future work.
</bodyText>
<sectionHeader confidence="0.987179" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999965058823529">
We propose a novel language-independent ap-
proach for improving statistical machine trans-
lation for resource-poor languages by exploiting
their similarity to resource-rich ones. More pre-
cisely, we improve the translation from a resource-
poor source language X1 into a resource-rich tar-
get language Y given a bi-text containing a limited
number of parallel sentences for X1-Y and a much
larger bi-text for X2-Y for some resource-rich lan-
guage X2 that is closely related to X1.
Our method exploits the similarity between re-
lated languages with respect to word order, syntax,
and, most importantly, vocabulary overlap – re-
lated languages share a large number of cognates.
Before we present the method, we will describe
two simple strategies for integrating the bi-text for
X2-Y into a phrase-based SMT system for X1-Y .
</bodyText>
<subsectionHeader confidence="0.991611">
2.1 Merging Bi-texts
</subsectionHeader>
<bodyText confidence="0.999936325">
We can simply concatenate the bi-texts for X1-Y
and X2-Y into one large bi-text and use it to train
an SMT system.
This offers several advantages. First, it can
yield improved word alignments for the sentences
that came from the X1-Y bi-text, e.g., since the
additional sentences can provide new contexts for
the rare words in that bi-text; rare words are
hard to align, which could have a disastrous ef-
fect on the subsequent phrase extraction stage.
Second, it can provide new source-language side
translation options, thus increasing the lexical
coverage and reducing the number of unknown
words at translation time; it can also provide new
useful non-compositional phrases on the source-
language side, thus yielding more fluent transla-
tion output. Third, it can offer new target-language
side phrases for known source phrases, which
could improve fluency by providing more trans-
lation options for the language model (LM) to
choose from. Fourth, bad phrases including words
from X2 that do not exist in X1 will be effectively
ignored at translation time since they could never
possibly match the input, while bad new target-
language translations still have the chance to be
filtered out by the language model.
However, simple concatenation can be problem-
atic. First, when concatenating the small bi-text
for X1-Y with the much larger one for X2-Y , the
latter will dominate during word alignment and
phrase extraction, thus hugely influencing both
lexical and phrase translation probabilities, which
can yield poor performance. This can be counter-
acted by repeating the small bi-text several times
so that the large one does not dominate. Sec-
ond, since the bi-texts are merged mechanically,
there is no way to distinguish between phrases ex-
tracted from the bi-text for X1-Y (which should
be good), from those coming from the bi-text for
X2-Y (whose quality might be questionable).
</bodyText>
<subsectionHeader confidence="0.999859">
2.2 Combining Phrase Tables
</subsectionHeader>
<bodyText confidence="0.99995125">
An alternative way of making use of the additional
bi-text for X2-Y to train an improved SMT sys-
tem for X1 → Y is to build separate phrase ta-
bles from X1-Y and X2-Y , which can then be
(a) used together, e.g., as alternative decoding
paths, (b) merged, e.g., using one or more extra
features to indicate the bi-text each phrase came
from, or (c) interpolated, e.g., using simple linear
interpolation.
Building two separate phrase tables offers sev-
eral advantages. First, the good phrases from the
bi-text for X1-Y are clearly distinguished from (or
given a higher weight in the linear interpolation
compared to) the potentially bad ones from the
X2-Y bi-text. Second, the lexical and the phrase
translation probabilities are combined in a princi-
pled manner. Third, using an X2-Y bi-text that is
much larger than that for X1-Y is not problematic
any more. Fourth, as with bi-text merging, there
are many additional source- and target-language
phrases, which offer new translation options.
On the negative side, the opportunity is lost
to obtain improved word alignments for the sen-
tences in the X1-Y bi-text.
</bodyText>
<page confidence="0.991865">
1359
</page>
<subsectionHeader confidence="0.996517">
2.3 Proposed Method
</subsectionHeader>
<bodyText confidence="0.9999784">
Taking into account the potential advantages and
disadvantages of the above strategies, we pro-
pose a method that tries to get the best of both:
(i) increased lexical coverage by using additional
phrase pairs independently extracted from X2-Y ,
and (ii) improved word alignments for X1-Y by
biasing the word alignment process with addi-
tional sentence pairs from X2-Y (possibly also re-
peating X1-Y several times). A detailed descrip-
tion of the method follows:
</bodyText>
<listItem confidence="0.929818857142857">
1. Build a bi-text Bcat that is a concatenation
of the bi-texts for X1-Y and X2-Y . Gener-
ate word alignments for Bcat, extract phrases,
and build a phrase table Tcat.
2. Build a bi-text Brep from the X1-Y bi-text
repeated k times followed by one copy of the
X2-Y bi-text. Generate word alignments for
Brep, then truncate them, only keeping word
alignments for one copy of the X1-Y bi-text.
Use these word alignments to extract phrases,
and build a phrase table Trep trunc.
3. Produce a phrase table Tmerged by combin-
ing Tcat and Trep trunc, giving priority to the
latter, and use it in an X1 → Y SMT system.
</listItem>
<subsectionHeader confidence="0.992278">
2.4 Transliteration
</subsectionHeader>
<bodyText confidence="0.984188421052631">
As we mentioned above, our method relies on the
existence of a large number of cognates between
related languages. While linguists define cognates
as words derived from a common root3 (Bickford
and Tuggy, 2002), computational linguists typi-
cally ignore origin, defining them as words in dif-
ferent languages that are mutual translations and
have a similar orthography (Bergsma and Kon-
drak, 2007; Mann and Yarowsky, 2001; Melamed,
1999). In this paper, we adopt the latter definition.
Cognates between related languages often ex-
hibit minor spelling variations, which can be sim-
ply due to different rules of orthography, (e.g.,
senhor vs. se˜nor in Portuguese and Spanish), but
often stem from real phonological differences. For
example, the Portuguese suffix -c¸˜ao corresponds
to the Spanish suffix -ci´on, e.g., evoluc¸˜ao vs.
evoluci´on. Such correspondences can be quite fre-
quent and thus easy to learn automatically4. Even
3E.g., Latin tu, Old English thou, Spanish t´u, Greek s´u and
German du are all cognates meaning ‘2nd person singular’.
4Not all such differences are systematic; many apply to a
particular word only, e.g., kerana vs. karena in Malay and
Indonesian, or dizer vs. decir in Portuguese and Spanish.
more frequent can be the inflectional variations.
For example, in Portuguese and Spanish respec-
tively, verb endings like -ou vs. -´o (for 3rd person
singular, simple past tense), e.g., visitou vs. visit´o,
or -ei vs. -´e (for 1st person singular, simple past
tense), e.g., visitei vs. visit´e.
If such systematic differences exist between the
languages X1 and X2, it might be useful to learn
and to use them as a pre-processing step in order
to transliterate the X2 side of the X2-Y bi-text
and thus increase its vocabulary overlap with the
source language X1.
We will describe our approach to automatic
transliteration in more detail in Section 3.4 below.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998056">
3.1 Language Pairs
</subsectionHeader>
<bodyText confidence="0.988108">
We experimented with two language pairs: the
closely related Malay and Indonesian and the more
dissimilar Spanish and Portuguese.
</bodyText>
<listItem confidence="0.966172111111111">
Malay and Indonesian are mutually intelligible,
but differ in pronunciation and vocabulary. An ex-
ample follows5:
• Malay: Semua manusia dilahirkan bebas
dan samarata dari segi kemuliaan dan hak-
hak.
• Indonesian: Semua orang dilahirkan
merdeka dan mempunyai martabat dan
hak-hak yang sama.
</listItem>
<bodyText confidence="0.993848428571429">
Spanish and Portuguese also exhibit a notice-
able degree of mutual intelligibility, but differ in
pronunciation, spelling, and vocabulary. Unlike
Malay and Indonesian, however, they also differ
syntactically and have a high degree of spelling
differences as demonstrated by the following ex-
amples6:
</bodyText>
<listItem confidence="0.975058333333333">
• Spanish: Se˜nora Presidenta, estimados cole-
gas, lo que est´a sucediendo en Oriente Medio
es una tragedia.
• Portuguese: Senhora Presidente, caros cole-
gas, o que est´a a acontecer no Medio Oriente
e´ uma trag´edia.
5In English: All human beings are born free and equal in
dignity and rights. (from Article 1 of the Universal Declara-
tion of Human Rights)
</listItem>
<footnote confidence="0.430338">
6In English: Madam President, ladies and gentlemen, the
events in the Middle East are a real tragedy.
</footnote>
<page confidence="0.964991">
1360
</page>
<subsectionHeader confidence="0.990713">
3.2 Datasets
</subsectionHeader>
<bodyText confidence="0.99965675">
In our experiments, we used the following number
of training sentence pairs (number of words shown
in parentheses) for English (en), Indonesian (in),
Malay (ml), Portuguese(pt), and Spanish (es):
</bodyText>
<listItem confidence="0.971281285714286">
• Indonesian-English (in-en):
– 28,383 pairs (0.8M, 0.9M words);
– monolingual English enin: 5.1M words.
• Malay-English (ml-en):
– 190,503 pairs (5.4M, 5.8M words);
– monoling. English enrol: 27.9M words.
• Spanish-English (es-en):
– 1,240,518 pairs (35.7M, 34.6M words);
– monolingual English enes:pt: 45.3M
words (the same as for pt-en).
• Portuguese-English (pt-en):
– 1,230,038 pairs (35.9M, 34.6M words).
– monolingual English enes:pt: 45.3M
words (the same as for es-en).
</listItem>
<bodyText confidence="0.993718">
All of the above datasets contain sentences with
up to 100 tokens. In addition, for each of the
four language pairs, we have a development and
a testing bi-text, each with 2,000 parallel sentence
pairs. We made sure the development and the test-
ing bi-texts shared no sentences with the training
bi-texts; we further excluded from the monolin-
gual English data all sentences from the English
sides of the training and the development bi-texts.
The training bi-text datasets for es-en and pt-en
were built from release v.3 of the Europarl corpus,
excluding the Q4/2000 portion out of which we
created our testing and development datasets.
We built the in-en bi-texts from texts that we
downloaded from the Web. We translated the In-
donesian texts to English using Google Translate,
and we matched7 them against the English texts
using a cosine similarity measure and heuristic
constraints based on document length in words
and in sentences, overlap of numbers, words in
uppercase, and words in the title. Next, we ex-
tracted pairs of sentences from the matched doc-
ument pairs using competitive linking (Melamed,
2000), and we retained the ones whose similarity
was above a pre-specified threshold. The ml-en
was built in a similar manner.
7Note that the automatic translations were used for match-
ing only; the final bi-text contained no automatic translations.
</bodyText>
<subsectionHeader confidence="0.999479">
3.3 Baseline SMT System
</subsectionHeader>
<bodyText confidence="0.999994925925926">
In the baseline, we used the following setup: We
first tokenized and lowercased both sides of the
training bi-text. We then built separate directed
word alignments for English→X and X→English
(XE{Indonesian, Spanish}) using IBM model 4
(Brown et al., 1993), combined them using the in-
tersect+grow heuristic (Och and Ney, 2003), and
extracted phrase-level translation pairs of maxi-
mum length seven using the alignment template
approach (Och and Ney, 2004). We thus obtained
a phrase table where each pair is associated with
five parameters: forward and reverse phrase trans-
lation probabilities, forward and reverse lexical
translation probabilities, and phrase penalty.
We then trained a log-linear model using stan-
dard SMT feature functions: trigram language
model probability, word penalty, distance-based8
distortion cost, and the parameters from the phrase
table. We set all weights by optimizing Bleu (Pap-
ineni et al., 2002) using minimum error rate train-
ing (MERT) (Och, 2003) on a separate develop-
ment set of 2,000 sentences (Indonesian or Span-
ish), and we used them in a beam search decoder
(Koehn et al., 2007) to translate 2,000 test sen-
tences (Indonesian or Spanish) into English. Fi-
nally, we detokenized the output, and we evaluated
it against a lowercased gold standard using Bleu9.
</bodyText>
<subsectionHeader confidence="0.884702">
3.4 Transliteration
</subsectionHeader>
<bodyText confidence="0.999932">
As was mentioned in Section 2, transliteration can
be helpful for languages with regular spelling dif-
ferences. Thus, we built a system for translitera-
tion from Portuguese into Spanish that was trained
on a list of automatically extracted likely cognates.
The system was applied on the Portuguese side of
the pt-en training bi-text.
Classic approaches to automatic cognate extrac-
tion look for non-stopwords with similar spelling
that appear in parallel sentences in a bi-text (Kon-
drak et al., 2003). In our case, however, we need to
extract cognates between Spanish and Portuguese
given pt-en and es-en bi-texts only, i.e., without
having a pt-es bi-text. Although it is easy to con-
struct a pt-es bi-text from the Europarl corpus,
we chose not to do so since, in general, synthe-
</bodyText>
<footnote confidence="0.826395833333333">
8We also tried lexicalized reordering (Koehn et al., 2005).
While it yielded higher absolute Bleu scores, the relative im-
provement for a sample of our experiments was very similar
to that achieved with distance-based re-ordering.
9We used version 11b of the NIST scoring tool:
http://www.nist.gov/speech/tools/
</footnote>
<page confidence="0.990428">
1361
</page>
<bodyText confidence="0.999954176470588">
sizing a bi-text for X1-X2 would be impossible:
e.g., it cannot be done for ml-in given our training
datasets for in-en and ml-en since the English sides
of these bi-texts have no sentences in common.
Thus, we extracted the list of likely cognates be-
tween Portuguese and Spanish from the training
pt-en and es-en bi-texts using English as a pivot as
follows: We started with IBM model 4 word align-
ments, from which we extracted four conditional
lexical translation probabilities: Pr(pj|ei) and
Pr(ei|pj) for Portuguese-English, and Pr(sk|ei)
and Pr(ei|sk) for Spanish-English, where pj, ei
and sk stand for a Portuguese, an English and
a Spanish word respectively. Following Wu and
Wang (2007), we then induced conditional lexical
translation probabilities Pr(pj|sk) and Pr(sk|pj)
for Portuguese-Spanish as follows:
</bodyText>
<equation confidence="0.923681166666667">
Pr(pj|sk) = Ei Pr(pj|ei, sk)Pr(ei|sk)
Assuming pj is conditionally independent of sk
given ei, we can simplify the above (expression:
Pr(pj  |sk) = Ei Pr(pj |ei)Pr(ei|sk)
Similarly, for Pr(sk|pj), we obtain
Pr(sk|pj) = Ei Pr(sk|ei)Pr(ei|pj)
</equation>
<bodyText confidence="0.999659363636364">
We excluded all stopwords, words of length less
than three, and those containing digits. We further
calculated Prod(pj,sk) = Pr(pj|sk)Pr(sk|pj),
and we excluded all Portuguese-Spanish word
pairs (pj, sk) for which Prod(pj, sk) &lt; 0.01.
From the remaining pairs, we extracted likely cog-
nates based on Prod(pj, sk) and on the ortho-
graphic similarity between pj and sk.
Following Melamed (1995), we measured the
orthographic similarity using the longest common
subsequence ratio (LCSR), defined as follows:
</bodyText>
<equation confidence="0.99628">
LCSR(s1, s2) = |LCS(s1,s2)|
max(|s1|,|s2|)
</equation>
<bodyText confidence="0.997224507692308">
where LCS(s1, s2) is the longest common subse-
quence of s1 and s2, and |s |is the length of s.
We retained as likely cognates all pairs for
which LCSR was 0.58 or higher; that value was
found by Kondrak et al. (2003) to be optimal for a
number of language pairs in the Europarl corpus.
Finally, we performed competitive linking
(Melamed, 2000), assuming that each Portuguese
wordform had at most one Spanish best cognate
match. Thus, using the values of Prod(pj, sk),
we induced a fully-connected weighted bipartite
graph. Then, we performed a greedy approxima-
tion to the maximum weighted bipartite match-
ing in that graph (i.e., competitive linking) as fol-
lows: First, we accepted as cognates the cross-
lingual pair (pj, sk) with the highest Prod(pj, sk)
in the graph, and we discarded pj and sk from fur-
ther consideration. Then, we accepted the next
highest-scored pair, and we discarded the involved
wordforms and so forth. The process was repeated
until there were no matchable pairs left.
As a result of the above procedure, we ended
up with 28,725 Portuguese-Spanish cognate pairs,
9,201 (or 32%) of which had spelling differences.
For each pair in the list of cognate pairs, we added
spaces between any two adjacent letters for both
wordforms, and we further appended the start and
the end characters ˆ and $. For example, the cog-
nate pair evoluc¸˜ao – evoluci´on became
ˆ e v o l u c¸ a˜o $ — ˆ e v o l u c i o´ n $
We randomly split the resulting list into a train-
ing (26,725 pairs) and a development dataset
(2,000 pairs), and trained and tuned a character-
level phrase-based monotone SMT system similar
to (Finch and Sumita, 2008) to transliterate a Por-
tuguese wordform into a Spanish wordform. We
used a Spanish language model trained on 14M
word tokens (obtained from the above-mentioned
45.3M-token monolingual English corpus after ex-
cluding punctuation, stopwords, words of length
less than three, and those containing digits): one
per line and character-separated with added start
and end characters as in the above example. We set
both the maximum phrase length and the language
model order to ten; this value was found by tun-
ing on the development dataset. The system was
tuned using MERT, and the feature weights were
saved. The tuning Bleu was 95.22%, while the
baseline Bleu, for leaving the Portuguese words
intact, was 87.63%. Finally, the training and the
tuning datasets were merged, and a new training
round was performed. The resulting system was
used with the saved feature weights to transliterate
the Portuguese side of the training pt-en bi-text,
which yielded a new ptes-en training bi-text.
We did the same for Malay into Indonesian. We
extracted 5,847 cognate pairs, 844 (or 14.4%) of
which had spelling differences, and we trained a
transliteration system. The highest tuning Bleu
was 95.18% (for maximum phrase size and LM
order of 10), but the baseline was 93.15%. We
then re-trained the system on the combination of
the training and the development datasets, and we
transliterated the Malay side of the training ml-en
bi-text, obtaining a new mlin-en training bi-text.
</bodyText>
<page confidence="0.989677">
1362
</page>
<figure confidence="0.964666833333333">
# Train LM Dev Test
1 ml-en enml ml-en ml-en
2 mlin-en enml ml-en ml-en
3 ml-en enml ml-en in-en
4 ml-en enml in-en in-en
5 ml-en enin in-en in-en
6 mlin-en enin in-en in-en
7 pt-en enes:pt pt-en pt-en
8 ptes-en enes:pt pt-en pt-en
9 pt-en enes:pt pt-en es-en
10 pt-en enes:pt es-en es-en
11 ptes-en enes:pt es-en es-en
</figure>
<table confidence="0.993639166666667">
10K 20K 40K 80K 160K 320K 640K 1230K
44.93 46.98 47.15 48.04 49.01 – – –
38.99 40.96 41.02 41.88 42.81 – – –
13.69 14.58 14.76 15.12 15.84 – – –
13.98 14.75 14.91 15.51 16.27 – – –
15.56 16.38 16.52 17.04 17.90 – – –
16.44 17.36 17.62 18.14 19.15 – – –
21.28 23.11 24.43 25.72 26.43 27.10 27.78 27.96
10.91 11.56 12.16 12.50 12.83 13.27 13.48 13.71
4.40 4.77 4.57 5.02 4.99 5.32 5.08 5.34
4.91 5.12 5.64 5.82 6.35 6.87 6.44 7.10
8.18 9.03 9.97 10.66 11.35 12.26 12.69 13.79
</table>
<tableCaption confidence="0.749857">
Table 1: Cross-lingual SMT experiments (shown in bold). Columns 2-5 present the bi-texts used for
training, development and testing, and the monolingual data used to train the English language model.
The following columns show the resulting Bleu (in %s) for different numbers of training sentence pairs.
</tableCaption>
<subsectionHeader confidence="0.65174">
3.5 Cross-lingual Translation
</subsectionHeader>
<bodyText confidence="0.990905444444445">
In this subsection, we study the similarity between
the original and the additional source languages.
First, we measured the vocabulary overlap be-
tween Spanish and Portuguese, which was fea-
sible since our training pt-en and es-en bi-texts
are from the same time span in the Europarl cor-
pus and their English sides largely overlap. We
found 110,053 Portuguese and 121,444 Spanish
word types, and 44,461 (or 36.6%) of them were
identical. Unfortunately, we could not do the same
for Malay and Indonesian since the English sides
of the in-en and ml-en bi-texts do not overlap.
Second, following the setup of the baseline sys-
tem, we performed cross-lingual experiments. The
results are shown in Table 1. As we can see, this
yielded a huge decrease in Bleu compared to the
baseline – three to five times – even for very large
training datasets, and even when a proper English
LM and development dataset were used: compare
line 1 to lines 3-6, and line 7 to lines 9-11.
Third, we tried transliteration. Bleu doubled for
Spanish (see lines 10-11), but improved far less for
Indonesian (lines 5-6). Training on the translit-
erated data and testing on Malay and Portuguese
yielded about 10-12% relative decrease for Malay
(lines 1-2) but 50% for Portuguese (lines 7-
8).10 Thus, unlike Spanish and Portuguese, there
were far less systematic spelling variations be-
tween Malay and Indonesian. A closer inspec-
tion confirmed this: many extracted likely Malay-
Indonesian cognate pairs with spelling differences
were in fact forms of a word existing in both lan-
guages, e.g., kata and berkata (‘to say’).
10However, as lines 8 and 11 show, a system trained on
1.23M ptes-en sentence pairs, performs equally well when
translating Portuguese and Spanish text: 13.71% vs. 13.79%.
</bodyText>
<subsectionHeader confidence="0.998267">
3.6 Using an Additional Language
</subsectionHeader>
<bodyText confidence="0.999642777777778">
We performed various experiments combining the
original and an additional training bi-text:
Two-tables: We built two separate phrase tables
for the two bi-texts, and we used them in the alter-
native decoding path model of Birch et al. (2007).
Interpolation: We built two separate phrase
tables for the original and for the additional bi-
text, and we used linear interpolation to com-
bine the corresponding conditional probabilities:
</bodyText>
<equation confidence="0.743886">
Pr(e|s) = αProrig(e|s) + (1 − α)Prextra(e|s).
</equation>
<bodyText confidence="0.999919653846154">
We optimized the value of α on the development
dataset, trying .5, .6, .7, .8 and .9; we used the
same α for all four conditional probabilities.
Merge: We built separate phrase tables, Torig
and Textra, for the original and for the additional
training bi-text. We then concatenated them giv-
ing priority to Torig: We kept all phrase pairs from
Torig, adding to them those ones from Textra that
were not present in Torig. For each phrase pair
added, we retained its associated conditional prob-
abilities and the phrase penalty. We further added
three additional features to each entry in the new
table: F1, F2 and F3. The value of F1 was 1 if
the phrase pair came from Torig, and 0.5 other-
wise. Similarly, F2=1 if the phrase pair came from
Textra, and F2=0.5 otherwise. The value of F3
was 1 if the pair came from both Torig and Textra,
and 0.5 otherwise. We experimented using (1)
F1 only, (2) F1 and F2, (3) F1, F2, and F3. We set
all feature weights using MERT, and we optimized
the number of features on the development set.11
11In theory, we should have also re-normalized the proba-
bilities since they may not sum to one. In practice, this was
not that important since the log-linear SMT model does not
require that the features be probabilities at all (e.g., the phrase
penalty), and we had extra features whose impact was bigger.
</bodyText>
<page confidence="0.939272">
1363
</page>
<bodyText confidence="0.999160421052632">
Concatxk: We concatenated k copies of the
original and one copy of the additional training bi-
text; we then trained and tuned an SMT system as
for the baseline. The value for k was optimized on
the development dataset.
Concatxk:align: We concatenated k copies of
the original and one copy of the additional train-
ing bi-text. We then generated IBM model 4 word
alignments, and we truncated them, only keeping
them for one copy of the original training bi-text.
Next, we extracted phrase pairs, thus buildng a
phrase table, and we tuned an SMT system as for
the baseline.
Our Method: Our method was described in
Section 2. We used merge to combine the phrase
tables for concatxk:align and concatx1, consid-
ering the former as Torig and the latter as Textra.
We had two parameters to tune: k and the number
of extra features in the merged phrase table.
</bodyText>
<figureCaption confidence="0.660389333333333">
Figure 1: Impact of k on Bleu for concatxk for
different number of extra ml-en sentence pairs
in Indonesian—*English SMT.
</figureCaption>
<sectionHeader confidence="0.998818" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999922323076923">
First, we studied the impact of k on concatxk
for Indonesian—*English SMT using Malay as an
additional language. We tried all values of k
such that 1&lt;k&lt;16 with 10000n extra ml-en sen-
tence pairs, nE{1,2,4,8,16}. As we can see in
Figure 1, the highest Bleu scores are achieved
for (n; k)E{(1;2),(2;2),(4;4),(8;7),(16;16)}, i.e.,
when k Pz� n. In order to limit the search space,
we used this relationship between k and n in our
experiments (also for Portuguese and Spanish).
Table 2 shows the results for experiments on
improving Indonesian—*English SMT using 10K,
20K, ..., 160K additional ml-en pairs of paral-
lel sentences. Several observations can be made.
First, using more additional sentences yields bet-
ter results. Second, with one exception, all ex-
periments yield improvements over the baseline.
Third, the improvements are always statistically
significant for our method, according to (Collins
et al., 2005)’s sign test. Overall, among the dif-
ferent bi-text combination strategies, our method
performs best, followed by concatxk, merge, and
interpolate, which are very close in performance;
these three strategies are the only ones to consis-
tently yield higher Bleu as the number of addi-
tional ml-en sentence pairs grows. Methods like
concatx1, concatxk:align and two-tables are
somewhat inconsistent in that respect. The latter
method performs worst and is the only one to go
below the baseline (for 10K ml-en pairs).
Table 3 shows the results when using pt-en data
to improve Spanish—*English SMT. Overall, the
results and the conclusions that can be made are
consistent with those for Table 2. We can further
observe that, as the size of the original bi-text in-
creases, the gain in Bleu decreases, which is to be
expected. Note also that here transliteration is very
important: it doubles the absolute gain in Bleu.
Finally, Table 4 shows a comparison to the piv-
oting technique of Callison-Burch et al. (2006).
for English—*Spanish SMT. Despite using just
Portuguese, we achieve an improvement that is, in
five out of six cases, much better than what they
achieve with eight pivot languages (which include
not only Portuguese, but also two other Romance
languages, French and Italian, which are closely
related to Spanish). Moreover, our method yields
improvements for very large original datasets –
1.2M pairs, while theirs stops improving at 160K.
However, our improvements are only statistically
significant for 160K original pairs or less. Finally,
note that our translation direction is reversed.
Based on the experimental results, we can make
several conclusions. First, we have shown that us-
ing bi-text data from related languages improves
SMT: we achieved up to 1.35 and 3.37 improve-
ment in Bleu for in-en (+ml-en) and es-en (+pt-
en) respectively. Second, while simple concate-
nation can help, it is problematic when the ad-
ditional sentences out-number the ones from the
original bi-text. Third, concatenation can work
very well if the original bi-text is repeated enough
times so that the additional bi-text does not dom-
inate. Fourth, merging phrase tables giving prior-
ity to the original bi-text and using additional fea-
</bodyText>
<page confidence="0.976049">
1364
</page>
<table confidence="0.999097666666667">
in-en ml-en Baseline Two tables Interpol. Merge concatx1 concatxk concatxk:align Our method
28.4K 10K 23.80&lt; ≥23.79&lt; 23.89&lt;(.9) 23.97&lt;(3) 24.29&lt; 24.29&lt; 24.01&lt; &lt;24.51(2;1) (+0.72)
28.4K 20K 23.80&lt; 24.24&lt; 24.22&lt; ≤24.46&lt; (3) 24.37&lt; (1) &lt;24.35&lt; &lt;24.70(2;2) (+0.90)
28.4K 40K 23.80&lt; 24.27&lt; (.8) 24.43≤(3) 24.38≤ ≤24.48(2) &lt;24.39&lt; &lt;24.73(4;2) (+0.93)
28.4K 80K 23.80&lt; 24.11&lt; 24.27&lt;(.8) &lt;24.67(3) 24.17&lt; ≤24.54(4) 24.18&lt; &lt;24.97(8;3) (+1.17)
28.4K 160K 23.80&lt; &lt;24.58&lt; ≤24.46&lt; &lt;24.79≤ ≤24.43&lt; ≤24.65&lt; (8) &lt;25.15(16;3) (+1.35)
(.8) (8) ≤24.27&lt;
&lt;24.58&lt; &lt;25.00(16) (16)
(.8)
</table>
<tableCaption confidence="0.979164">
Table 2: Improving Indonesian→English SMT using ml-en data. Shown are the Bleu scores (in %s)
for different methods. A subscript shows the best parameter value(s) found on the development set and
used on the test set to produce the given result. Bleu scores that are statistically significantly better than
the baseline/our method are marked on the left/right side by &lt; (for p &lt; 0.01) or &lt; (for p &lt; 0.05).
</tableCaption>
<table confidence="0.99965525">
es-en pt-en Transl. Baseline Two tables Interpol. Merge concatx1 concatxk concatxk:align Our method
10K 160K no 22.87&lt; &lt;23.81 &lt;23.73(.5) &lt;23.60(2) &lt;23.54&lt; &lt;23.83&lt;(16) 22.93&lt; &lt;23.98(16;3) (+1.11)
yes 22.87&lt; &lt;25.29≤ &lt;25.22&lt; &lt;25.16&lt; &lt;25.26 &lt;25.42(16) (16) &lt;25.73(16;3) (+2.86)
(.5) &lt;23.31&lt;
(16)
20K 160K no 24.71&lt; &lt;25.22 ≤25.02&lt; &lt;25.32≤ &lt;25.19&lt; &lt;25.29&lt; 24.91&lt; &lt;25.65(8;2) (+0.94)
yes 24.71&lt; &lt;26.07≤ (.5) &lt;26.04&lt; &lt;26.16≤ (8) (8) &lt;26.36(8;3) (+1.65)
&lt;26.07(.7) (3) &lt;26.18≤ 24.88&lt;
(8) (8)
40K 160K no 25.80&lt; 25.96&lt; 26.15&lt;(.6) 25.99&lt;(3) 26.24&lt; 25.92&lt; 25.99&lt; &lt;26.49(4;2) (+0.69)
yes 25.80&lt; &lt;26.68 &lt;26.43(.7) &lt;26.64(3) &lt;26.78 &lt;26.93(4) (4) &lt;26.95(4;3) (+1.15)
25.88&lt;
(4)
80K 160K no 27.08≤ ≥26.89&lt; 27.04&lt;(.8) 27.02&lt;(3) 27.23 27.09&lt; 27.01&lt; ≤27.30(2;2) (+0.22)
yes 27.08&lt; 27.20&lt; 27.42(.5) 27.29≤(3) 27.26&lt; (2) (2) &lt;27.49(2;3) (+0.41)
≤27.53(2) 27.09&lt;
(2)
160K 160K no 27.90 27.99 27.72(.5) 27.95(2) 27.83&lt; 27.83&lt; 27.94(1) 28.05(1;3) (+0.15)
yes 27.90 28.11 ≤28.13(.6) ≤28.17(2) ≤28.14 (1) 28.06(1) 28.16(1;2) (+0.26)
≤28.14(1)
</table>
<tableCaption confidence="0.995359">
Table 3: Improving Spanish→English SMT using 160K additional pt-en sentence pairs. Column
</tableCaption>
<bodyText confidence="0.851488538461538">
three shows whether transliteration was used; the following columns list the Bleu scores (in %s) for
different methods. A small subscript shows the best parameter value(s) found on the development set
and used on the test set to produce the given result. Bleu scores that are statistically significantly better
than the baseline/our method are marked on the left/right side by &lt; (for p &lt; 0.01) or &lt; (for p &lt; 0.05).
tures is a good strategy. Fifth, part of the improve-
ment when combining bi-texts is due to increased
vocabulary coverage because of cognates, but an-
other part comes from improved word alignments.
Sixth, the best results are achieved when the latter
two sources are first isolated and then combined
(our method). Finally, transliteration can help a lot
in case of systematic spelling variations between
the original and the additional source languages.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99995525">
In this section, we describe two general lines of
related previous research: using cognates between
the source and the target language, and source-
language side paraphrasing with a pivot language.
</bodyText>
<subsectionHeader confidence="0.896145">
5.1 Cognates
</subsectionHeader>
<bodyText confidence="0.993116814814815">
Many researchers have used likely cognates to
obtain improved word alignments and thus build
better SMT systems. Al-Onaizan et al. (1999)
extracted such likely cognates for Czech-English
using one of the variations of LCSR (Melamed,
1995) described in (Tiedemann, 1999) as a simi-
larity measure. They used these cognates to im-
prove word alignments with IBM models 1-4 in
three different ways: (1) by seeding the parameters
of IBM model 1, (2) by constraining the word co-
occurrences when training IBM models 1-4, and
(3) by adding the cognate pairs to the bi-text as
additional “sentence pairs”. The last approach per-
formed best and was later used by Kondrak et al.
(2003) who demonstrated improved SMT for nine
European languages.
Unlike these approaches, which extract cog-
nates between the source and the target language,
we use cognates between the source and some
other related language that is different from the
target. Moreover, we only implicitly rely on the
existence of such cognates; we do not try to ex-
tract them at all, and we leave them in their origi-
nal sentence contexts.12
12However, in some of our experiments, we extract cog-
nates for training a transliteration system from the resource-
rich source language X2 into the resource-poor one X1.
</bodyText>
<page confidence="0.958412">
1365
</page>
<table confidence="0.988344142857143">
10K 20K 40K 80K 160K 320K 1,230K
22.6 25.0 26.5 26.5 28.7 30.0 –
23.3 26.0 27.2 28.0 29.0 30.0 –
+0.7 +1.0 +0.7 +1.5 +0.3 +0.0 –
22.87 24.71 25.80 27.08 27.90 28.46 29.90
23.98* 25.65* 26.49* 27.30° 28.05 28.52 29.87
+1.11* +0.94* +0.69* +0.22° +0.15 +0.06 -0.03
25.73* 26.36* 26.95* 27.49* 28.16 28.43 29.94
+2.86* +1.65* +1.15* +0.41* +0.26 -0.03 +0.04
24.23* 25.70* 26.78* 27.49 28.22° 28.58 29.84
+1.36* +0.99* +0.98* +0.41 +0.32° +0.12 -0.06
26.24* 26.82* 27.47* 27.85* 28.50* 28.70 29.99
+3.37* +2.11* +1.67* +0.77* +0.60* +0.24 +0.09
Direction System
</table>
<figure confidence="0.561433333333333">
en--+es baseline
pivoting (+8 languages x -1.3M pairs)
improvement
es--+en baseline
our method (+1 language x 160K pairs)
improvement
our method (translit., +1 lang. x 160K)
improvement
our method (+1 language x 1.23M pairs)
improvement
our method (translit., +1 lang. x 1.23M)
improvement
</figure>
<tableCaption confidence="0.904785">
Table 4: Comparison to the pivoting technique of Callison-Burch et al. (2006) for English→Spanish.
</tableCaption>
<bodyText confidence="0.8491126">
Shown are Bleu scores (in %s) and absolute improvement over the baseline for training bi-texts with
different numbers of parallel sentences (10K, 20K, ..., 1230K) and fixed amount of additional data:
(1) about 1.3M sentence pairs for each of eight additional languages in Callison-Burch et al. (2006)’s
pivoting, and (2) 160K and 1,230K pairs for one language (Portuguese) for our method. Statistically
significant improvements over the baseline are marked with a ∗ (for p &lt; 0.01) and with a ° (for p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.999756">
5.2 Paraphrasing with a Pivot-Language
</subsectionHeader>
<bodyText confidence="0.999983311111111">
Another relevant line of research is on using multi-
lingual parallel corpora to improve SMT using ad-
ditional languages as pivots.
Callison-Burch et al. (2006) improved
English→Spanish and English→French SMT
using source-language paraphrases extracted with
the pivoting technique of Bannard and Callison-
Burch (2005) and eight additional languages from
the Europarl corpus (Koehn, 2005). For example,
using German as a pivot, they extracted English
paraphrases from a parallel English-German
corpus by looking for English phrases that were
aligned to the same German phrase: e.g., if under
control and in check were aligned to unter con-
trolle, they were hypothesized to be paraphrases
with some probability. Such (English) paraphrases
were added as additional entries in the phrase
table of an English→Spanish/English→French
phrase-based SMT system and paired with the
foreign (Spanish/French) translation of the origi-
nal (English) phrase. The system was then tuned
with MERT using an extra feature penalizing
low-probability paraphrases; this yielded up to
1.8% absolute improvement in Bleu.
Other important publications about pivoting ap-
proaches for machine translation include (Wu and
Wang, 2007), (Utiyama and Isahara, 2007), (Hajiˇc
et al., 2000) and (Habash and Hu, 2009).
Unlike pivoting, which can only improve
source-language lexical coverage, we augment
both the source- and the target-language sides.
Second, while pivoting ignores context when ex-
tracting paraphrases, we take it into account.
Third, by using as an additional language one that
is related to the source, we are able to get increase
in Bleu that is comparable and even better than
what pivoting achieves with eight pivot languages.
On the negative side, our approach is limited in
that it requires that X2 be related to X1, while the
pivoting language Z does not need to be related to
X1 nor to Y . However, we only need one addi-
tional parallel corpus (for X2-Y ), while pivoting
needs two: one for X1-Z and one for Z-Y . Fi-
nally, note that our approach is orthogonal to piv-
oting, and thus the two can be combined.
</bodyText>
<sectionHeader confidence="0.996435" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999990714285714">
We have proposed a novel method for improving
SMT for resource-poor languages by exploiting
their similarity to resource-rich ones.
In future work, we would like to extend that ap-
proach in several interesting directions. First, we
want to make better use of multi-lingual parallel
corpora, e.g., while we had access to a Spanish-
Portuguese-English corpus, we used it as two
separate bi-texts Spanish-English and Portuguese-
English. Second, we would like to exploit multi-
ple auxiliary resource-rich languages the resource-
poor source language is related to. Third, we could
also experiment with using auxiliary languages
that are related to the target language.
</bodyText>
<sectionHeader confidence="0.998747" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.6926725">
This research was supported by research grant
POD0713875.
</bodyText>
<page confidence="0.991379">
1366
</page>
<sectionHeader confidence="0.995786" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952882352941">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Joseph
Och, David Purdy, Noah Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, CLSP, Johns Hopkins University,
Baltimore, MD.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL’05, pages 597–604.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of ACL’07, pages 656–663.
Albert Bickford and David Tuggy. 2002.
Electronic glossary of linguistic terms.
http://www.sil.org/mexico/ling/glosario/E005ai-
Glossary.htm.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of WMT’2007, pages 9–
16.
Peter Brown, Vincent Della Pietra, Stephen Della
Pietra, and Robert Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings of HLT-
NAACL’06, pages 17–24.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL’05, pages 531–
540.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proceedings of
WTCAST’08, pages 13–18.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese statistical machine translation using English
as pivot language. In Proceedings of the WMT’09,
pages 173–181.
Jan Hajiˇc, Jan Hric, and Vladislav Kuboˇn. 2000. Ma-
chine translation of very close languages. In Pro-
ceedings of ANLP’00, pages 7–12.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proceedings of IWSLT’05.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings ofACL’07. Demonstration ses-
sion, pages 177–180.
Philipp Koehn. 2005. Europarl: A parallel corpus for
evaluation of machine translation. In Proceedings of
MT Summit, pages 79–86.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proceedings of NAACL’03, pages 46–48.
Gideon Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages.
In Proceedings of NAACL’01, pages 1–8.
Dan Melamed. 1995. Automatic evaluation and uni-
form filter cascades for inducing N-best translation
lexicons. In Proceedings of WVLC’95, pages 184–
198.
Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107–130.
Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221–249.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL’03, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
ofACL’02, pages 311–318.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of LREC’2006, pages 2142–2147.
Jorg Tiedemann. 1999. Automatic construction of
weighted string similarity measures. In Proceedings
of EMNLP-VLC’99, pages 213–219.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statisti-
cal machine translation. In Proceedings of NAACL-
HLT’07, pages 484–491.
Hua Wu and Haifeng Wang. 2007. Pivot language ap-
proach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165–181.
</reference>
<page confidence="0.993938">
1367
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.567500">
<title confidence="0.997835">Improved Statistical Machine Translation for Resource-Poor Using Related Resource-Rich Languages</title>
<author confidence="0.85509">Preslav</author>
<affiliation confidence="0.911345333333333">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.939539">Singapore</address>
<email confidence="0.991061">nakov@comp.nus.edu.sg</email>
<abstract confidence="0.99824415">We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resourcesource language a resourcelanguage a bi-text containing a limited number of parallel sentences a larger bi-text for some resource-rich language closely related to The evaluation (using Malay) (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz Joseph Och</author>
<author>David Purdy</author>
<author>Noah Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Technical report, CLSP,</tech>
<institution>Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<contexts>
<context position="34209" citStr="Al-Onaizan et al. (1999)" startWordPosition="5491" endWordPosition="5494">ments. Sixth, the best results are achieved when the latter two sources are first isolated and then combined (our method). Finally, transliteration can help a lot in case of systematic spelling variations between the original and the additional source languages. 5 Related Work In this section, we describe two general lines of related previous research: using cognates between the source and the target language, and sourcelanguage side paraphrasing with a pivot language. 5.1 Cognates Many researchers have used likely cognates to obtain improved word alignments and thus build better SMT systems. Al-Onaizan et al. (1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed, 1995) described in (Tiedemann, 1999) as a similarity measure. They used these cognates to improve word alignments with IBM models 1-4 in three different ways: (1) by seeding the parameters of IBM model 1, (2) by constraining the word cooccurrences when training IBM models 1-4, and (3) by adding the cognate pairs to the bi-text as additional “sentence pairs”. The last approach performed best and was later used by Kondrak et al. (2003) who demonstrated improved SMT for nine European languages. Unlike </context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz Joseph Och, David Purdy, Noah Smith, and David Yarowsky. 1999. Statistical machine translation. Technical report, CLSP, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>597--604</pages>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL’05, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Alignment-based discriminative string similarity.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07,</booktitle>
<pages>656--663</pages>
<contexts>
<context position="10150" citStr="Bergsma and Kondrak, 2007" startWordPosition="1606" endWordPosition="1610">ese word alignments to extract phrases, and build a phrase table Trep trunc. 3. Produce a phrase table Tmerged by combining Tcat and Trep trunc, giving priority to the latter, and use it in an X1 → Y SMT system. 2.4 Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸˜ao corresponds to the Spanish suffix -ci´on, e.g., evoluc¸˜ao vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4. Even 3E.g., Latin tu, Old English thou, Spanish t´u, Greek s´u and Germa</context>
</contexts>
<marker>Bergsma, Kondrak, 2007</marker>
<rawString>Shane Bergsma and Grzegorz Kondrak. 2007. Alignment-based discriminative string similarity. In Proceedings of ACL’07, pages 656–663.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bickford</author>
<author>David Tuggy</author>
</authors>
<title>Electronic glossary of linguistic terms.</title>
<date>2002</date>
<note>http://www.sil.org/mexico/ling/glosario/E005aiGlossary.htm.</note>
<contexts>
<context position="9967" citStr="Bickford and Tuggy, 2002" startWordPosition="1579" endWordPosition="1582">repeated k times followed by one copy of the X2-Y bi-text. Generate word alignments for Brep, then truncate them, only keeping word alignments for one copy of the X1-Y bi-text. Use these word alignments to extract phrases, and build a phrase table Trep trunc. 3. Produce a phrase table Tmerged by combining Tcat and Trep trunc, giving priority to the latter, and use it in an X1 → Y SMT system. 2.4 Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸˜ao corresponds to the Spanish suffix -ci´on, e.g.</context>
</contexts>
<marker>Bickford, Tuggy, 2002</marker>
<rawString>Albert Bickford and David Tuggy. 2002. Electronic glossary of linguistic terms. http://www.sil.org/mexico/ling/glosario/E005aiGlossary.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of WMT’2007,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="25193" citStr="Birch et al. (2007)" startWordPosition="4048" endWordPosition="4051">is: many extracted likely MalayIndonesian cognate pairs with spelling differences were in fact forms of a word existing in both languages, e.g., kata and berkata (‘to say’). 10However, as lines 8 and 11 show, a system trained on 1.23M ptes-en sentence pairs, performs equally well when translating Portuguese and Spanish text: 13.71% vs. 13.79%. 3.6 Using an Additional Language We performed various experiments combining the original and an additional training bi-text: Two-tables: We built two separate phrase tables for the two bi-texts, and we used them in the alternative decoding path model of Birch et al. (2007). Interpolation: We built two separate phrase tables for the original and for the additional bitext, and we used linear interpolation to combine the corresponding conditional probabilities: Pr(e|s) = αProrig(e|s) + (1 − α)Prextra(e|s). We optimized the value of α on the development dataset, trying .5, .6, .7, .8 and .9; we used the same α for all four conditional probabilities. Merge: We built separate phrase tables, Torig and Textra, for the original and for the additional training bi-text. We then concatenated them giving priority to Torig: We kept all phrase pairs from Torig, adding to them</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. CCG supertags in factored statistical machine translation. In Proceedings of WMT’2007, pages 9– 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Vincent Della Pietra</author>
<author>Stephen Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="15171" citStr="Brown et al., 1993" startWordPosition="2399" endWordPosition="2402">ed pairs of sentences from the matched document pairs using competitive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. 3.3 Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (XE{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phr</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Vincent Della Pietra, Stephen Della Pietra, and Robert Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL’06,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="29682" citStr="Callison-Burch et al. (2006)" startWordPosition="4805" endWordPosition="4808">hat inconsistent in that respect. The latter method performs worst and is the only one to go below the baseline (for 10K ml-en pairs). Table 3 shows the results when using pt-en data to improve Spanish—*English SMT. Overall, the results and the conclusions that can be made are consistent with those for Table 2. We can further observe that, as the size of the original bi-text increases, the gain in Bleu decreases, which is to be expected. Note also that here transliteration is very important: it doubles the absolute gain in Bleu. Finally, Table 4 shows a comparison to the pivoting technique of Callison-Burch et al. (2006). for English—*Spanish SMT. Despite using just Portuguese, we achieve an improvement that is, in five out of six cases, much better than what they achieve with eight pivot languages (which include not only Portuguese, but also two other Romance languages, French and Italian, which are closely related to Spanish). Moreover, our method yields improvements for very large original datasets – 1.2M pairs, while theirs stops improving at 160K. However, our improvements are only statistically significant for 160K original pairs or less. Finally, note that our translation direction is reversed. Based o</context>
<context position="36259" citStr="Callison-Burch et al. (2006)" startWordPosition="5828" endWordPosition="5831">28.43 29.94 +2.86* +1.65* +1.15* +0.41* +0.26 -0.03 +0.04 24.23* 25.70* 26.78* 27.49 28.22° 28.58 29.84 +1.36* +0.99* +0.98* +0.41 +0.32° +0.12 -0.06 26.24* 26.82* 27.47* 27.85* 28.50* 28.70 29.99 +3.37* +2.11* +1.67* +0.77* +0.60* +0.24 +0.09 Direction System en--+es baseline pivoting (+8 languages x -1.3M pairs) improvement es--+en baseline our method (+1 language x 160K pairs) improvement our method (translit., +1 lang. x 160K) improvement our method (+1 language x 1.23M pairs) improvement our method (translit., +1 lang. x 1.23M) improvement Table 4: Comparison to the pivoting technique of Callison-Burch et al. (2006) for English→Spanish. Shown are Bleu scores (in %s) and absolute improvement over the baseline for training bi-texts with different numbers of parallel sentences (10K, 20K, ..., 1230K) and fixed amount of additional data: (1) about 1.3M sentence pairs for each of eight additional languages in Callison-Burch et al. (2006)’s pivoting, and (2) 160K and 1,230K pairs for one language (Portuguese) for our method. Statistically significant improvements over the baseline are marked with a ∗ (for p &lt; 0.01) and with a ° (for p &lt; 0.05). 5.2 Paraphrasing with a Pivot-Language Another relevant line of rese</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of HLTNAACL’06, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL’05, pages 531– 540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Phrasebased machine transliteration.</title>
<date>2008</date>
<booktitle>In Proceedings of WTCAST’08,</booktitle>
<pages>13--18</pages>
<contexts>
<context position="20562" citStr="Finch and Sumita, 2008" startWordPosition="3271" endWordPosition="3274">e procedure, we ended up with 28,725 Portuguese-Spanish cognate pairs, 9,201 (or 32%) of which had spelling differences. For each pair in the list of cognate pairs, we added spaces between any two adjacent letters for both wordforms, and we further appended the start and the end characters ˆ and $. For example, the cognate pair evoluc¸˜ao – evoluci´on became ˆ e v o l u c¸ a˜o $ — ˆ e v o l u c i o´ n $ We randomly split the resulting list into a training (26,725 pairs) and a development dataset (2,000 pairs), and trained and tuned a characterlevel phrase-based monotone SMT system similar to (Finch and Sumita, 2008) to transliterate a Portuguese wordform into a Spanish wordform. We used a Spanish language model trained on 14M word tokens (obtained from the above-mentioned 45.3M-token monolingual English corpus after excluding punctuation, stopwords, words of length less than three, and those containing digits): one per line and character-separated with added start and end characters as in the above example. We set both the maximum phrase length and the language model order to ten; this value was found by tuning on the development dataset. The system was tuned using MERT, and the feature weights were save</context>
</contexts>
<marker>Finch, Sumita, 2008</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2008. Phrasebased machine transliteration. In Proceedings of WTCAST’08, pages 13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Jun Hu</author>
</authors>
<title>Improving ArabicChinese statistical machine translation using English as pivot language.</title>
<date>2009</date>
<booktitle>In Proceedings of the WMT’09,</booktitle>
<pages>173--181</pages>
<contexts>
<context position="38104" citStr="Habash and Hu, 2009" startWordPosition="6106" endWordPosition="6109">othesized to be paraphrases with some probability. Such (English) paraphrases were added as additional entries in the phrase table of an English→Spanish/English→French phrase-based SMT system and paired with the foreign (Spanish/French) translation of the original (English) phrase. The system was then tuned with MERT using an extra feature penalizing low-probability paraphrases; this yielded up to 1.8% absolute improvement in Bleu. Other important publications about pivoting approaches for machine translation include (Wu and Wang, 2007), (Utiyama and Isahara, 2007), (Hajiˇc et al., 2000) and (Habash and Hu, 2009). Unlike pivoting, which can only improve source-language lexical coverage, we augment both the source- and the target-language sides. Second, while pivoting ignores context when extracting paraphrases, we take it into account. Third, by using as an additional language one that is related to the source, we are able to get increase in Bleu that is comparable and even better than what pivoting achieves with eight pivot languages. On the negative side, our approach is limited in that it requires that X2 be related to X1, while the pivoting language Z does not need to be related to X1 nor to Y . H</context>
</contexts>
<marker>Habash, Hu, 2009</marker>
<rawString>Nizar Habash and Jun Hu. 2009. Improving ArabicChinese statistical machine translation using English as pivot language. In Proceedings of the WMT’09, pages 173–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jan Hric</author>
<author>Vladislav Kuboˇn</author>
</authors>
<title>Machine translation of very close languages.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP’00,</booktitle>
<pages>7--12</pages>
<marker>Hajiˇc, Hric, Kuboˇn, 2000</marker>
<rawString>Jan Hajiˇc, Jan Hric, and Vladislav Kuboˇn. 2000. Machine translation of very close languages. In Proceedings of ANLP’00, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of IWSLT’05.</booktitle>
<contexts>
<context position="17065" citStr="Koehn et al., 2005" startWordPosition="2701" endWordPosition="2704">tomatically extracted likely cognates. The system was applied on the Portuguese side of the pt-en training bi-text. Classic approaches to automatic cognate extraction look for non-stopwords with similar spelling that appear in parallel sentences in a bi-text (Kondrak et al., 2003). In our case, however, we need to extract cognates between Spanish and Portuguese given pt-en and es-en bi-texts only, i.e., without having a pt-es bi-text. Although it is easy to construct a pt-es bi-text from the Europarl corpus, we chose not to do so since, in general, synthe8We also tried lexicalized reordering (Koehn et al., 2005). While it yielded higher absolute Bleu scores, the relative improvement for a sample of our experiments was very similar to that achieved with distance-based re-ordering. 9We used version 11b of the NIST scoring tool: http://www.nist.gov/speech/tools/ 1361 sizing a bi-text for X1-X2 would be impossible: e.g., it cannot be done for ml-in given our training datasets for in-en and ml-en since the English sides of these bi-texts have no sentences in common. Thus, we extracted the list of likely cognates between Portuguese and Spanish from the training pt-en and es-en bi-texts using English as a p</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of IWSLT’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL’07. Demonstration session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="1205" citStr="Koehn et al., 2007" startWordPosition="165" endWordPosition="168">of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian→English (using Malay) and Spanish→English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data. 1 Introduction Recent developments in statistical machine translation (SMT), e.g., the availability of efficient implementations of integrated open-source toolkits like Moses (Koehn et al., 2007), have made it possible to build a prototype system with decent translation quality for any language pair in a few days or even hours. In theory. In practice, doing so requires having a large set of parallel sentencealigned bi-lingual texts (a bi-text) for that language pair, which is often unavailable. Large highquality bi-texts are rare; except for Arabic, Chinese, and some official languages of the European Union (EU), most of the 6,500+ world languages remain resource-poor from an SMT viewpoint. While manually creating a small bi-text could be relatively easy, building a large one is hard,</context>
<context position="16033" citStr="Koehn et al., 2007" startWordPosition="2534" endWordPosition="2537">pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish), and we used them in a beam search decoder (Koehn et al., 2007) to translate 2,000 test sentences (Indonesian or Spanish) into English. Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9. 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of automatically extracted likely cognates. The system was applied on the Portuguese side of the pt-en training bi-text. Classic approaches to automatic cognate extraction look for non-stopwor</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL’07. Demonstration session, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="2379" citStr="Koehn, 2005" startWordPosition="351" endWordPosition="352">ly easy, building a large one is hard, e.g., because of copyright. Most bi-texts for SMT come from parliament debates and legislation of Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nght@comp.nus.edu.sg multi-lingual countries (e.g., French-English from Canada, and Chinese-English from Hong Kong), or from international organizations like the United Nations and the European Union. For example, the Europarl corpus of parliament proceedings consists of about 1.3M parallel sentences (up to 44M words) per language for 11 languages (Koehn, 2005), and the JRC-Acquis corpus provides a comparable amount of European legislation in 22 languages (Steinberger et al., 2006). The official languages of the EU are especially lucky in that respect; while this includes such “classic SMT languages” like English and French, and some important international ones like Spanish and Portuguese, most of the rest have a limited number of speakers and were resource-poor until recently; this is changing quickly because of the increasing volume of EU parliament debates and the ever-growing European legislation. Thus, becoming an official language of the EU h</context>
<context position="37213" citStr="Koehn, 2005" startWordPosition="5975" endWordPosition="5976"> 160K and 1,230K pairs for one language (Portuguese) for our method. Statistically significant improvements over the baseline are marked with a ∗ (for p &lt; 0.01) and with a ° (for p &lt; 0.05). 5.2 Paraphrasing with a Pivot-Language Another relevant line of research is on using multilingual parallel corpora to improve SMT using additional languages as pivots. Callison-Burch et al. (2006) improved English→Spanish and English→French SMT using source-language paraphrases extracted with the pivoting technique of Bannard and CallisonBurch (2005) and eight additional languages from the Europarl corpus (Koehn, 2005). For example, using German as a pivot, they extracted English paraphrases from a parallel English-German corpus by looking for English phrases that were aligned to the same German phrase: e.g., if under control and in check were aligned to unter controlle, they were hypothesized to be paraphrases with some probability. Such (English) paraphrases were added as additional entries in the phrase table of an English→Spanish/English→French phrase-based SMT system and paired with the foreign (Spanish/French) translation of the original (English) phrase. The system was then tuned with MERT using an e</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for evaluation of machine translation. In Proceedings of MT Summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<title>Cognates can improve statistical translation models.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL’03,</booktitle>
<pages>46--48</pages>
<contexts>
<context position="16727" citStr="Kondrak et al., 2003" startWordPosition="2642" endWordPosition="2646">Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9. 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of automatically extracted likely cognates. The system was applied on the Portuguese side of the pt-en training bi-text. Classic approaches to automatic cognate extraction look for non-stopwords with similar spelling that appear in parallel sentences in a bi-text (Kondrak et al., 2003). In our case, however, we need to extract cognates between Spanish and Portuguese given pt-en and es-en bi-texts only, i.e., without having a pt-es bi-text. Although it is easy to construct a pt-es bi-text from the Europarl corpus, we chose not to do so since, in general, synthe8We also tried lexicalized reordering (Koehn et al., 2005). While it yielded higher absolute Bleu scores, the relative improvement for a sample of our experiments was very similar to that achieved with distance-based re-ordering. 9We used version 11b of the NIST scoring tool: http://www.nist.gov/speech/tools/ 1361 sizi</context>
<context position="19140" citStr="Kondrak et al. (2003)" startWordPosition="3023" endWordPosition="3026">pj), and we excluded all Portuguese-Spanish word pairs (pj, sk) for which Prod(pj, sk) &lt; 0.01. From the remaining pairs, we extracted likely cognates based on Prod(pj, sk) and on the orthographic similarity between pj and sk. Following Melamed (1995), we measured the orthographic similarity using the longest common subsequence ratio (LCSR), defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and |s |is the length of s. We retained as likely cognates all pairs for which LCSR was 0.58 or higher; that value was found by Kondrak et al. (2003) to be optimal for a number of language pairs in the Europarl corpus. Finally, we performed competitive linking (Melamed, 2000), assuming that each Portuguese wordform had at most one Spanish best cognate match. Thus, using the values of Prod(pj, sk), we induced a fully-connected weighted bipartite graph. Then, we performed a greedy approximation to the maximum weighted bipartite matching in that graph (i.e., competitive linking) as follows: First, we accepted as cognates the crosslingual pair (pj, sk) with the highest Prod(pj, sk) in the graph, and we discarded pj and sk from further consider</context>
<context position="34742" citStr="Kondrak et al. (2003)" startWordPosition="5582" endWordPosition="5585">ain improved word alignments and thus build better SMT systems. Al-Onaizan et al. (1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed, 1995) described in (Tiedemann, 1999) as a similarity measure. They used these cognates to improve word alignments with IBM models 1-4 in three different ways: (1) by seeding the parameters of IBM model 1, (2) by constraining the word cooccurrences when training IBM models 1-4, and (3) by adding the cognate pairs to the bi-text as additional “sentence pairs”. The last approach performed best and was later used by Kondrak et al. (2003) who demonstrated improved SMT for nine European languages. Unlike these approaches, which extract cognates between the source and the target language, we use cognates between the source and some other related language that is different from the target. Moreover, we only implicitly rely on the existence of such cognates; we do not try to extract them at all, and we leave them in their original sentence contexts.12 12However, in some of our experiments, we extract cognates for training a transliteration system from the resourcerich source language X2 into the resource-poor one X1. 1365 10K 20K </context>
</contexts>
<marker>Kondrak, Marcu, Knight, 2003</marker>
<rawString>Grzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003. Cognates can improve statistical translation models. In Proceedings of NAACL’03, pages 46–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL’01,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="10175" citStr="Mann and Yarowsky, 2001" startWordPosition="1611" endWordPosition="1614">act phrases, and build a phrase table Trep trunc. 3. Produce a phrase table Tmerged by combining Tcat and Trep trunc, giving priority to the latter, and use it in an X1 → Y SMT system. 2.4 Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸˜ao corresponds to the Spanish suffix -ci´on, e.g., evoluc¸˜ao vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4. Even 3E.g., Latin tu, Old English thou, Spanish t´u, Greek s´u and German du are all cognates mea</context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>Gideon Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In Proceedings of NAACL’01, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Automatic evaluation and uniform filter cascades for inducing N-best translation lexicons.</title>
<date>1995</date>
<booktitle>In Proceedings of WVLC’95,</booktitle>
<pages>184--198</pages>
<contexts>
<context position="18769" citStr="Melamed (1995)" startWordPosition="2962" endWordPosition="2963">r(pj|ei, sk)Pr(ei|sk) Assuming pj is conditionally independent of sk given ei, we can simplify the above (expression: Pr(pj |sk) = Ei Pr(pj |ei)Pr(ei|sk) Similarly, for Pr(sk|pj), we obtain Pr(sk|pj) = Ei Pr(sk|ei)Pr(ei|pj) We excluded all stopwords, words of length less than three, and those containing digits. We further calculated Prod(pj,sk) = Pr(pj|sk)Pr(sk|pj), and we excluded all Portuguese-Spanish word pairs (pj, sk) for which Prod(pj, sk) &lt; 0.01. From the remaining pairs, we extracted likely cognates based on Prod(pj, sk) and on the orthographic similarity between pj and sk. Following Melamed (1995), we measured the orthographic similarity using the longest common subsequence ratio (LCSR), defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and |s |is the length of s. We retained as likely cognates all pairs for which LCSR was 0.58 or higher; that value was found by Kondrak et al. (2003) to be optimal for a number of language pairs in the Europarl corpus. Finally, we performed competitive linking (Melamed, 2000), assuming that each Portuguese wordform had at most one Spanish best cognate match. Thus, using the v</context>
<context position="34310" citStr="Melamed, 1995" startWordPosition="5508" endWordPosition="5509">ur method). Finally, transliteration can help a lot in case of systematic spelling variations between the original and the additional source languages. 5 Related Work In this section, we describe two general lines of related previous research: using cognates between the source and the target language, and sourcelanguage side paraphrasing with a pivot language. 5.1 Cognates Many researchers have used likely cognates to obtain improved word alignments and thus build better SMT systems. Al-Onaizan et al. (1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed, 1995) described in (Tiedemann, 1999) as a similarity measure. They used these cognates to improve word alignments with IBM models 1-4 in three different ways: (1) by seeding the parameters of IBM model 1, (2) by constraining the word cooccurrences when training IBM models 1-4, and (3) by adding the cognate pairs to the bi-text as additional “sentence pairs”. The last approach performed best and was later used by Kondrak et al. (2003) who demonstrated improved SMT for nine European languages. Unlike these approaches, which extract cognates between the source and the target language, we use cognates </context>
</contexts>
<marker>Melamed, 1995</marker>
<rawString>Dan Melamed. 1995. Automatic evaluation and uniform filter cascades for inducing N-best translation lexicons. In Proceedings of WVLC’95, pages 184– 198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="10191" citStr="Melamed, 1999" startWordPosition="1615" endWordPosition="1616">phrase table Trep trunc. 3. Produce a phrase table Tmerged by combining Tcat and Trep trunc, giving priority to the latter, and use it in an X1 → Y SMT system. 2.4 Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸˜ao corresponds to the Spanish suffix -ci´on, e.g., evoluc¸˜ao vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4. Even 3E.g., Latin tu, Old English thou, Spanish t´u, Greek s´u and German du are all cognates meaning ‘2nd person</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>Dan Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25(1):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="14647" citStr="Melamed, 2000" startWordPosition="2319" endWordPosition="2320">uilt from release v.3 of the Europarl corpus, excluding the Q4/2000 portion out of which we created our testing and development datasets. We built the in-en bi-texts from texts that we downloaded from the Web. We translated the Indonesian texts to English using Google Translate, and we matched7 them against the English texts using a cosine similarity measure and heuristic constraints based on document length in words and in sentences, overlap of numbers, words in uppercase, and words in the title. Next, we extracted pairs of sentences from the matched document pairs using competitive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. 3.3 Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (XE{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and </context>
<context position="19267" citStr="Melamed, 2000" startWordPosition="3045" endWordPosition="3046">likely cognates based on Prod(pj, sk) and on the orthographic similarity between pj and sk. Following Melamed (1995), we measured the orthographic similarity using the longest common subsequence ratio (LCSR), defined as follows: LCSR(s1, s2) = |LCS(s1,s2)| max(|s1|,|s2|) where LCS(s1, s2) is the longest common subsequence of s1 and s2, and |s |is the length of s. We retained as likely cognates all pairs for which LCSR was 0.58 or higher; that value was found by Kondrak et al. (2003) to be optimal for a number of language pairs in the Europarl corpus. Finally, we performed competitive linking (Melamed, 2000), assuming that each Portuguese wordform had at most one Spanish best cognate match. Thus, using the values of Prod(pj, sk), we induced a fully-connected weighted bipartite graph. Then, we performed a greedy approximation to the maximum weighted bipartite matching in that graph (i.e., competitive linking) as follows: First, we accepted as cognates the crosslingual pair (pj, sk) with the highest Prod(pj, sk) in the graph, and we discarded pj and sk from further consideration. Then, we accepted the next highest-scored pair, and we discarded the involved wordforms and so forth. The process was re</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>Dan Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="15241" citStr="Och and Ney, 2003" startWordPosition="2410" endWordPosition="2413"> linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. 3.3 Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (XE{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 200</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="15369" citStr="Och and Ney, 2004" startWordPosition="2429" endWordPosition="2432">a similar manner. 7Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. 3.3 Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (XE{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish)</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="15896" citStr="Och, 2003" startWordPosition="2510" endWordPosition="2511">s of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish), and we used them in a beam search decoder (Koehn et al., 2007) to translate 2,000 test sentences (Indonesian or Spanish) into English. Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9. 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of automatically extracted likely cognates. The system </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL’03, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="15843" citStr="Papineni et al., 2002" startWordPosition="2498" endWordPosition="2502"> (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish), and we used them in a beam search decoder (Koehn et al., 2007) to translate 2,000 test sentences (Indonesian or Spanish) into English. Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9. 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings ofACL’02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
</authors>
<title>Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis,</title>
<date>2006</date>
<booktitle>In Proceedings of LREC’2006,</booktitle>
<pages>2142--2147</pages>
<location>and</location>
<marker>Steinberger, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Daniel Varga. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of LREC’2006, pages 2142–2147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorg Tiedemann</author>
</authors>
<title>Automatic construction of weighted string similarity measures.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP-VLC’99,</booktitle>
<pages>213--219</pages>
<contexts>
<context position="34341" citStr="Tiedemann, 1999" startWordPosition="5512" endWordPosition="5513">eration can help a lot in case of systematic spelling variations between the original and the additional source languages. 5 Related Work In this section, we describe two general lines of related previous research: using cognates between the source and the target language, and sourcelanguage side paraphrasing with a pivot language. 5.1 Cognates Many researchers have used likely cognates to obtain improved word alignments and thus build better SMT systems. Al-Onaizan et al. (1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed, 1995) described in (Tiedemann, 1999) as a similarity measure. They used these cognates to improve word alignments with IBM models 1-4 in three different ways: (1) by seeding the parameters of IBM model 1, (2) by constraining the word cooccurrences when training IBM models 1-4, and (3) by adding the cognate pairs to the bi-text as additional “sentence pairs”. The last approach performed best and was later used by Kondrak et al. (2003) who demonstrated improved SMT for nine European languages. Unlike these approaches, which extract cognates between the source and the target language, we use cognates between the source and some oth</context>
</contexts>
<marker>Tiedemann, 1999</marker>
<rawString>Jorg Tiedemann. 1999. Automatic construction of weighted string similarity measures. In Proceedings of EMNLP-VLC’99, pages 213–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A comparison of pivot methods for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACLHLT’07,</booktitle>
<pages>484--491</pages>
<contexts>
<context position="38055" citStr="Utiyama and Isahara, 2007" startWordPosition="6097" endWordPosition="6100">in check were aligned to unter controlle, they were hypothesized to be paraphrases with some probability. Such (English) paraphrases were added as additional entries in the phrase table of an English→Spanish/English→French phrase-based SMT system and paired with the foreign (Spanish/French) translation of the original (English) phrase. The system was then tuned with MERT using an extra feature penalizing low-probability paraphrases; this yielded up to 1.8% absolute improvement in Bleu. Other important publications about pivoting approaches for machine translation include (Wu and Wang, 2007), (Utiyama and Isahara, 2007), (Hajiˇc et al., 2000) and (Habash and Hu, 2009). Unlike pivoting, which can only improve source-language lexical coverage, we augment both the source- and the target-language sides. Second, while pivoting ignores context when extracting paraphrases, we take it into account. Third, by using as an additional language one that is related to the source, we are able to get increase in Bleu that is comparable and even better than what pivoting achieves with eight pivot languages. On the negative side, our approach is limited in that it requires that X2 be related to X1, while the pivoting language</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A comparison of pivot methods for phrase-based statistical machine translation. In Proceedings of NAACLHLT’07, pages 484–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Pivot language approach for phrase-based statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="18016" citStr="Wu and Wang (2007)" startWordPosition="2850" endWordPosition="2853"> ml-in given our training datasets for in-en and ml-en since the English sides of these bi-texts have no sentences in common. Thus, we extracted the list of likely cognates between Portuguese and Spanish from the training pt-en and es-en bi-texts using English as a pivot as follows: We started with IBM model 4 word alignments, from which we extracted four conditional lexical translation probabilities: Pr(pj|ei) and Pr(ei|pj) for Portuguese-English, and Pr(sk|ei) and Pr(ei|sk) for Spanish-English, where pj, ei and sk stand for a Portuguese, an English and a Spanish word respectively. Following Wu and Wang (2007), we then induced conditional lexical translation probabilities Pr(pj|sk) and Pr(sk|pj) for Portuguese-Spanish as follows: Pr(pj|sk) = Ei Pr(pj|ei, sk)Pr(ei|sk) Assuming pj is conditionally independent of sk given ei, we can simplify the above (expression: Pr(pj |sk) = Ei Pr(pj |ei)Pr(ei|sk) Similarly, for Pr(sk|pj), we obtain Pr(sk|pj) = Ei Pr(sk|ei)Pr(ei|pj) We excluded all stopwords, words of length less than three, and those containing digits. We further calculated Prod(pj,sk) = Pr(pj|sk)Pr(sk|pj), and we excluded all Portuguese-Spanish word pairs (pj, sk) for which Prod(pj, sk) &lt; 0.01. Fr</context>
<context position="38026" citStr="Wu and Wang, 2007" startWordPosition="6093" endWordPosition="6096">if under control and in check were aligned to unter controlle, they were hypothesized to be paraphrases with some probability. Such (English) paraphrases were added as additional entries in the phrase table of an English→Spanish/English→French phrase-based SMT system and paired with the foreign (Spanish/French) translation of the original (English) phrase. The system was then tuned with MERT using an extra feature penalizing low-probability paraphrases; this yielded up to 1.8% absolute improvement in Bleu. Other important publications about pivoting approaches for machine translation include (Wu and Wang, 2007), (Utiyama and Isahara, 2007), (Hajiˇc et al., 2000) and (Habash and Hu, 2009). Unlike pivoting, which can only improve source-language lexical coverage, we augment both the source- and the target-language sides. Second, while pivoting ignores context when extracting paraphrases, we take it into account. Third, by using as an additional language one that is related to the source, we are able to get increase in Bleu that is comparable and even better than what pivoting achieves with eight pivot languages. On the negative side, our approach is limited in that it requires that X2 be related to X1</context>
</contexts>
<marker>Wu, Wang, 2007</marker>
<rawString>Hua Wu and Haifeng Wang. 2007. Pivot language approach for phrase-based statistical machine translation. Machine Translation, 21(3):165–181.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>