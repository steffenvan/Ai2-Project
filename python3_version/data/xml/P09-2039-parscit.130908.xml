<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000338">
<title confidence="0.9992565">
Extracting Comparative Sentences from Korean Text Documents Us-
ing Comparative Lexical Patterns and Machine Learning Techniques
</title>
<author confidence="0.998118">
Seon Yang
</author>
<affiliation confidence="0.9044885">
Department of Computer Engineering,
Dong-A University,
</affiliation>
<address confidence="0.986078">
840 Hadan 2-dong, Saha-gu,
Busan 604-714 Korea
</address>
<email confidence="0.999417">
syang@donga.ac.kr
</email>
<author confidence="0.998351">
Youngjoong Ko
</author>
<affiliation confidence="0.90456">
Department of Computer Engineering,
Dong-A University,
</affiliation>
<address confidence="0.974859">
840 Hadan 2-dong, Saha-gu,
Busan 604-714 Korea
</address>
<email confidence="0.99891">
yjko@dau.ac.kr
</email>
<sectionHeader confidence="0.99389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999088461538462">
This paper proposes how to automatically
identify Korean comparative sentences from
text documents. This paper first investigates
many comparative sentences referring to pre-
vious studies and then defines a set of compar-
ative keywords from them. A sentence which
contains one or more elements of the keyword
set is called a comparative-sentence candidate.
Finally, we use machine learning techniques to
eliminate non-comparative sentences from the
candidates. As a result, we achieved signifi-
cant performance, an F1-score of 88.54%, in
our experiments using various web documents.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.972185589285714">
Comparing one entity with other entities is one
of the most convincing ways of evaluation (Jin-
dal and Liu, 2006). A comparative sentence for-
mulates an ordering relation between two entities
and that relation is very useful for many applica-
tion areas. One key area is for the customers. For
example, a customer can make a decision on
his/her final choice about a digital camera after
reading other customers&apos; product reviews, e.g.,
“Digital Camera X is much cheaper than Y
though it functions as good as Y!” Another one
is for manufacturers. All the manufacturers have
an interest in the articles saying how their prod-
ucts are compared with competitors’ ones.
Comparative sentences often contain some
comparative keywords. A sentence may express
some comparison if it contains any comparative
keywords such as ‘�� ([bo-da]: than)’, ‘4of
([ga-jang]: most)’, ‘�a ([da-reu]: different)’,
‘&amp; ([gat]: same)’. But many sentences also ex-
press comparison without those keywords. Simi-
larly, although some sentences contain some
keywords, they cannot be comparative sentences.
By these reasons, extracting comparative sen-
tences is not a simple or easy problem. It needs
more complicated and challenging processes
than only searching out some keywords for ex-
tracting comparative sentences.
Jindal and Liu (2006) previously studied to
identify English comparative sentences. But the
mechanism of Korean as an agglutinative lan-
guage and that of English as an inflecting lan-
guage have seriously different aspects. One of
the greatest differences related to our work is that
there are Part-of-Speech (POS) Tags for compar-
ative and superlative in English1, whereas, unfor-
tunately, the POS tagger of Korean does not pro-
vide any comparative and superlative tags be-
cause the analysis of Korean comparative is
much more difficult than that of English. The
major challenge of our work is therefore to iden-
tify comparative sentences without comparative
and superlative POS Tags.
We first survey previous studies about the Ko-
rean comparative syntax and collect the corpus
of Korean comparative sentences from the Web.
As we refer to previous studies and investigate
real comparative sentences form the collected
corpus, we can construct the set of comparative
keywords and extract comparative-sentence can-
didates; the sentences which contain one or more
element of the keyword set are called compara-
tive-sentence candidates. Then we use some ma-
chine learning techniques to eliminate non-
comparative sentences from those candidates.
The final experimental results in 5-fold cross
</bodyText>
<footnote confidence="0.830163">
1 JJR: adjective and comparative, JJS: adjective and superla-
tive, RBR: adverb and comparative, and RBS: adverb and
superlative
</footnote>
<page confidence="0.96097">
153
</page>
<note confidence="0.925964">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 153–156,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999937090909091">
validation show the overall precision of 88.68%
and the overall recall of 88.40%.
The remainder of the paper is organized as fol-
lows. Section 2 describes the related work. In
section 3, we explain comparative keywords and
comparative-sentence candidates. In section 4,
we describe how to eliminate non-comparative
sentences from the candidates extracted in pre-
ceding section. Section 5 presents the experimen-
tal results. Finally, we discuss conclusions and
future work in section 6
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999931580645161">
We have not found any direct work on automati-
cally extracting Korean comparative sentences.
There is only one study by Jindal and Liu (2006)
that is related to English. They used comparative
and superlative POS tags and additional some
keywords to search English comparative sen-
tences. Then they used Class Sequential Rules
and Naïve Bayesian learning method. Their ex-
periment showed a precision of 79% and recall
of 81%.
Our research is closely related to linguistics.
Ha (1999) described Korean comparative con-
structions with a linguistic view. Oh (2003) dis-
cussed the gradability of comparatives. Jeong
(2000) classified the adjective superlative by the
type of measures.
Opinion mining is also related to our work.
Many comparative sentences also contain the
speaker’s opinions and especially comparison is
one of the most powerful tools for evaluation.
We have surveyed many studies about opinion
mining (Lee et al., 2008; Kim and Hovy, 2006;
Wilson and Wiebe, 2003; Riloff and Wiebe,
2003; Esuli and Sebastiani, 2006).
Maximum Entropy Model is used in our tech-
nique. Berger et al. (1996) described Maximum
Entropy approach to National Language
Processing. In our experiments, we used Zhang’s
Maximum Entropy Model Toolkit (2004). Naïve
Bayesian classifier is used to prove the perfor-
mance of MEM (McCallum and Nigam (1998)).
</bodyText>
<sectionHeader confidence="0.9865765" genericHeader="method">
3 Extracting Comparative-sentence
Candidates
</sectionHeader>
<bodyText confidence="0.997359">
In this section, we define comparative keywords
and extract comparative-sentence candidates by
using those keywords.
</bodyText>
<subsectionHeader confidence="0.997634">
3.1 Comparative keyword
</subsectionHeader>
<bodyText confidence="0.998798">
First of all, we classify comparative sentences
into six types and then we extract single compar-
ative keywords from each type as follows:
</bodyText>
<tableCaption confidence="0.999909">
Table 1. The six types of comparative sentences
</tableCaption>
<table confidence="0.96977075">
Type Single-keyword Examples
1 Equality ‘&amp; ([gat]: same)’
2 Similarity ‘H1 o/ ([bi-seut-ha]: similar)’
3 Difference ‘c/a ([da-reu]: different)’
4 Greater or ‘nc/ ([bo-da]: than)’
lesser
5 Superlative ‘4g ([ga-jang]: most)’
6 Predicative No single-keywords
</table>
<bodyText confidence="0.7593615">
We can easily find such keywords from the vari-
ous sentences in first five types, while we cannot
find any single keyword in the sentences of type
6.
</bodyText>
<equation confidence="0.675542">
Ex1) “X 00—/ `-Y-TIff` �LFH1zT�/EILI,, Y &apos;�&apos;°
,�Ya1A1 0/c/.” ([X-gum-eui won-jae-ryo-neun
cho-san-vi-nil-su-ji-in-de, Y-gum-eun cheon-
</equation>
<bodyText confidence="0.692559416666667">
yeon-chi-kl-i-da]: Raw material of gum X is po-
lyvinyl acetate, but that of Y is natural chicle.)2
And we can find many non-comparative sen-
tences which contain some keywords. The fol-
lowing example (Ex2) shows non-comparative
though it contains ‘&amp; ([gat]: It means &apos;same&apos;, but
it sometimes means &apos;think’)’.
Ex2) “Lll A27,E011 Lllz H14 -_ :�( 같Vo R.” ([Nae
sang-gak-en nae-il bi-ga ol geot gat-a-yo]: I
think it will rain tomorrow.)
Thus all the sentences can be divided into four
categories as follows:
</bodyText>
<tableCaption confidence="0.999724">
Table 2. The four categories of the sentences
</tableCaption>
<table confidence="0.9704062">
Single-keyword Contain Not contain
Comparative S1 S2
Sentences
Non-comparative S3 S4 (unconcerned
Sentences group)
</table>
<bodyText confidence="0.7859464">
2 In fact, type 6 can be sorted as non-comparative from lin-
guistic view. But the speaker is probably saying that Y is
better than X. This is very important comparative data as an
opinion. Therefore, we also regard the sentences containing
implicit comparison as comparative sentences
</bodyText>
<page confidence="0.999573">
154
</page>
<bodyText confidence="0.952363411764706">
Our final goal is to find an effective method to
extract S1 and S2, but single-keyword searching
just outputs S1 and S3. In order to capture S2, we
added long-distance-words sequences to the set
of single-keywords. For example, we could ex-
tract ‘&lt;` [neun], EIL1/ [in-de], ° [eun], 0/EI [i-
da]&gt;’ as a long-distance-words sequence from
Ex1-sentence. It means that the sentence is
formed as &lt; S V but S V&gt; in English (S: subject
phrase, V: verb phrase). Thus we defined com-
parative keyword in this paper as follows:
Definition (comparative keyword): A compara-
tive keyword is formed as a word or a phrase or
a long-distance-words sequence. When a com-
parative keyword is contained in any sentence,
the sentence is most likely to be a comparative
sentence. (We will use an abbreviation ‘CK’.)
</bodyText>
<subsectionHeader confidence="0.986386">
3.2 Comparative-sentence Candidates
</subsectionHeader>
<bodyText confidence="0.9998702">
We finally set up a total of 177 CKs by human
efforts. In the previous work, Jindal and Liu
(2006) defined 83 keywords and key phrases in-
cluding comparative or superlative POS tags in
English; they did not use any long-distance-
words sequence.
Keyword searching process can detect most of
comparative sentences (S1, S2 and S3)3 from
original text documents. That is, the recall is high
but the precision is low. We here defined a com-
parative-sentence candidate as a sentence which
contains one or more elements of the set of CKs.
Now we need to eliminate the incorrect sen-
tences (S3) from those captured sentences. First,
we divided the set of CKs into two subsets de-
noted by CKL1 and CKL2 according to the pre-
cision of each keyword; we used 90% of the pre-
cision as a threshold value. The average preci-
sion of comparative-sentence candidates with a
CKL1 keyword is 97.44% and they do not re-
quire any additional process. But that of compar-
ative-sentence candidates with a CKL2 keyword
is 29.34% and we decide to eliminate non-
comparative sentences only from comparative
sentence candidates with a CKL2 keyword.
</bodyText>
<sectionHeader confidence="0.7177715" genericHeader="method">
4 Eliminating Non-comparative Sen-
tences from the Candidates
</sectionHeader>
<bodyText confidence="0.964470153846154">
3 As you can see in the experiment section, keyword search-
ing captures 95.96% comparative sentences.
To effectively eliminate non-comparative sen-
tences from comparative sentence candidates
with a CKL2 keyword, we employ machine
learning techniques (MEM and Naïve Bayes).
For feature extraction from each comparative-
sentence candidate, we use continuous words
sequence within the radius of 3 (the window size
of 7) of each keyword in the sentence; we expe-
rimented with radius options of 2, 3, and 4 and
we achieved the best performance in the radius
of 3. After determining the radius, we replace
each word with its POS tag; in order to reflect
various expressions of each sentence, POS tags
are more proper than lexical information of ac-
tual words. However, since CKs play the most
important role to discriminate comparative sen-
tences, they are represented as a combination of
their actual keyword and POS tag. Thus our fea-
ture is formed as “X 4 y”. (‘X’ means a se-
quence and ‘y’ means a class; y1 denotes com-
parative and y2 denotes non-comparative). For
instance, ‘&lt;pv etm nbn 같/pa ep ef sf &gt;4 4 y2’ is
one of the features from the sentence of Ex2 in
section 3.1.
</bodyText>
<sectionHeader confidence="0.989401" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999616">
Three trained human annotators compiled a cor-
pus of 277 online documents from various do-
mains. They discussed their disagreements and
they finally annotated 7,384 sentences. Table 3
shows the number of comparative sentences and
non-comparative sentences in our corpus.
</bodyText>
<tableCaption confidence="0.997275">
Table 3. The numbers of annotated sentences
</tableCaption>
<table confidence="0.8711665">
Total Comparative Non-comparative
7,384 2,383 (32%) 5,001 (68%)
</table>
<bodyText confidence="0.999041833333333">
Before evaluating our proposed method, we
conducted some experiments by machine learn-
ing techniques with all the unigrams of total ac-
tual words as baseline systems; they do not use
any CKs. The precision, recall and F1-score of
the baseline systems are shown at Table 4.
</bodyText>
<tableCaption confidence="0.997795">
Table 4. The results of baseline systems (%
</tableCaption>
<table confidence="0.98730275">
Baseline Precision Recall F1-score
System
NB 35.98 91.62 51.66
MEM 78.17 63.34 69.94
</table>
<bodyText confidence="0.9408905">
The final overall results using the 5-fold cross
validation are shown in Table 5 and Figure 1.
</bodyText>
<footnote confidence="0.4864745">
4 The labels such as ‘pv’, ‘etm’, ‘nbn’, etc. are Korean POS
Tags
</footnote>
<page confidence="0.998878">
155
</page>
<tableCaption confidence="0.999588">
Table 5. The results of our proposed method (%
</tableCaption>
<table confidence="0.9942866">
Method Preci- Recall F1-score
sion
CKs only 68.39 95.96 79.87
CKs + NB 85.42 88.59 86.67
CKs + MEM 88.68 88.40 88.54
</table>
<figureCaption confidence="0.940828">
Fig. 1 The results of our proposed method (%)
</figureCaption>
<bodyText confidence="0.999932166666667">
As shown in Table 5 and Figure 1, both of MEM
and NB is shown good performance but the F1-
score of MEM is little higher than that of NB. By
applying machine learning technique to our me-
thod, we can achieve high precision while we
can preserve high recall.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999973923076923">
In this paper, we have presented how to extract
comparative sentences from Korean text docu-
ments by keyword searching process and ma-
chine learning techniques. Our experimental re-
sults showed that our proposed method can be
effectively used to identify comparative sen-
tences. Since the research of comparison mining
is currently in the beginning step in the world,
our proposed techniques can contribute much to
text mining and opinion mining research.
In our future work, we plan to classify com-
parative types and to extract comparative rela-
tions from identified comparative sentences.
</bodyText>
<sectionHeader confidence="0.978466" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.613528333333333">
This paper was supported by the Korean Re-
search Foundation Grant funded by the Korean
Government (KRF-2008-331-D00553)
</bodyText>
<sectionHeader confidence="0.944636" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996491978723404">
Adam L. Berger et al. 1996. A Maximum Entropy
Approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39-71.
Andrea Esuli and Fabrizio Sebastiani. 2006. Deter-
mining Term Subjectivity and Term Orientation for
Opinion Mining. European Chapter of the Associa-
tion for Computational Linguistics, 193-200.
Andrew McCallum and Kamal Nigam. 1998. A
Comparison of Event Models for Naïve Bayes Text
Classification. Association for Advancement of Ar-
tificial Intelligence, 41-48.
Dong-joo Lee et al. 2008. Opinion Mining of Cus-
tomer Feedback Data on the Web. International
Conference on Ubiquitous Information Manage-
ment and Community, 247-252.
Ellen Riloff and Janyce Wiebe. 2003. Learning Ex-
traction Patterns for Subjective Expressions. Em-
pirical Methods in Natural Language Processing.
Gil-jong Ha. 1999. Korean Modern Comparative Syn-
tax, Pijbook Press, Seoul, Korea.
Gil-jong Ha. 1999. Research on Korean Equality
Comparative Syntax, Association for Korean Lin-
guistics, 5:229-265.
In-su Jeong. 2000. Research on Korean Adjective
Superlative Comparative Syntax. Korean Han-min-
jok Eo-mun-hak, 36:61-86.
Kyeong-sook Oh. 2004. The Difference between
‘Man-kum’ Comparative and ‘Cheo-rum’ Compar-
ative. Society of Korean Semantics, 14:197-221.
Nitin Jindal and Bing Liu. 2006. Identifying Com-
parative Sentences in Text Documents, Association
for Computing Machinery/Special Interest Group
on Information Retrieval, 244-251.
Nitin Jindal and Bing Liu. 2006. Mining Comparative
Sentences and Relations, Association for Ad-
vancement of Artificial Intelligence, 1331-1336.
Soomin Kim and Eduard Hovy. 2006. Automatic De-
tection of Opinion Bearing Words and Sentences.
Computational Linguistics/Association for Compu-
tational Linguistics.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
Opinions in the World Press. Special Interest
Group in Discourse and Dialoque/Association for
Computational Linguistics.
Zhang Le. 2004. Maximum Entropy Modeling Toolkit
for Python and C++. http://homepages.inf.ed.ac.
uk/s0450736/maxent_toolkit.html.
</reference>
<page confidence="0.998785">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851333">
<title confidence="0.9976855">Extracting Comparative Sentences from Korean Text Documents Using Comparative Lexical Patterns and Machine Learning Techniques</title>
<author confidence="0.999462">Seon Yang</author>
<affiliation confidence="0.9987725">Department of Computer Engineering, Dong-A University,</affiliation>
<address confidence="0.97301">840 Hadan 2-dong, Saha-gu, Busan 604-714 Korea</address>
<email confidence="0.985254">syang@donga.ac.kr</email>
<author confidence="0.998429">Youngjoong Ko</author>
<affiliation confidence="0.9987775">Department of Computer Engineering, Dong-A University,</affiliation>
<address confidence="0.9689685">840 Hadan 2-dong, Saha-gu, Busan 604-714 Korea</address>
<email confidence="0.986936">yjko@dau.ac.kr</email>
<abstract confidence="0.999269785714286">This paper proposes how to automatically identify Korean comparative sentences from text documents. This paper first investigates many comparative sentences referring to previous studies and then defines a set of comparative keywords from them. A sentence which contains one or more elements of the keyword set is called a comparative-sentence candidate. Finally, we use machine learning techniques to eliminate non-comparative sentences from the candidates. As a result, we achieved significant performance, an F1-score of 88.54%, in our experiments using various web documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<marker>Berger, 1996</marker>
<rawString>Adam L. Berger et al. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining Term Subjectivity and Term Orientation for Opinion Mining.</title>
<date>2006</date>
<booktitle>European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="5327" citStr="Esuli and Sebastiani, 2006" startWordPosition="807" endWordPosition="810">of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sentences also contain the speaker’s opinions and especially comparison is one of the most powerful tools for evaluation. We have surveyed many studies about opinion mining (Lee et al., 2008; Kim and Hovy, 2006; Wilson and Wiebe, 2003; Riloff and Wiebe, 2003; Esuli and Sebastiani, 2006). Maximum Entropy Model is used in our technique. Berger et al. (1996) described Maximum Entropy approach to National Language Processing. In our experiments, we used Zhang’s Maximum Entropy Model Toolkit (2004). Naïve Bayesian classifier is used to prove the performance of MEM (McCallum and Nigam (1998)). 3 Extracting Comparative-sentence Candidates In this section, we define comparative keywords and extract comparative-sentence candidates by using those keywords. 3.1 Comparative keyword First of all, we classify comparative sentences into six types and then we extract single comparative keyw</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Determining Term Subjectivity and Term Orientation for Opinion Mining. European Chapter of the Association for Computational Linguistics, 193-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A Comparison of Event Models for Naïve Bayes Text Classification. Association for Advancement of</title>
<date>1998</date>
<journal>Artificial Intelligence,</journal>
<pages>41--48</pages>
<contexts>
<context position="5632" citStr="McCallum and Nigam (1998)" startWordPosition="855" endWordPosition="858">ed to our work. Many comparative sentences also contain the speaker’s opinions and especially comparison is one of the most powerful tools for evaluation. We have surveyed many studies about opinion mining (Lee et al., 2008; Kim and Hovy, 2006; Wilson and Wiebe, 2003; Riloff and Wiebe, 2003; Esuli and Sebastiani, 2006). Maximum Entropy Model is used in our technique. Berger et al. (1996) described Maximum Entropy approach to National Language Processing. In our experiments, we used Zhang’s Maximum Entropy Model Toolkit (2004). Naïve Bayesian classifier is used to prove the performance of MEM (McCallum and Nigam (1998)). 3 Extracting Comparative-sentence Candidates In this section, we define comparative keywords and extract comparative-sentence candidates by using those keywords. 3.1 Comparative keyword First of all, we classify comparative sentences into six types and then we extract single comparative keywords from each type as follows: Table 1. The six types of comparative sentences Type Single-keyword Examples 1 Equality ‘&amp; ([gat]: same)’ 2 Similarity ‘H1 o/ ([bi-seut-ha]: similar)’ 3 Difference ‘c/a ([da-reu]: different)’ 4 Greater or ‘nc/ ([bo-da]: than)’ lesser 5 Superlative ‘4g ([ga-jang]: most)’ 6 </context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A Comparison of Event Models for Naïve Bayes Text Classification. Association for Advancement of Artificial Intelligence, 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong-joo Lee</author>
</authors>
<date>2008</date>
<booktitle>Opinion Mining of Customer Feedback Data on the Web. International Conference on Ubiquitous Information Management and Community,</booktitle>
<pages>247--252</pages>
<marker>Lee, 2008</marker>
<rawString>Dong-joo Lee et al. 2008. Opinion Mining of Customer Feedback Data on the Web. International Conference on Ubiquitous Information Management and Community, 247-252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning Extraction Patterns for Subjective Expressions. Empirical Methods in Natural Language Processing.</title>
<date>2003</date>
<contexts>
<context position="5298" citStr="Riloff and Wiebe, 2003" startWordPosition="803" endWordPosition="806">ment showed a precision of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sentences also contain the speaker’s opinions and especially comparison is one of the most powerful tools for evaluation. We have surveyed many studies about opinion mining (Lee et al., 2008; Kim and Hovy, 2006; Wilson and Wiebe, 2003; Riloff and Wiebe, 2003; Esuli and Sebastiani, 2006). Maximum Entropy Model is used in our technique. Berger et al. (1996) described Maximum Entropy approach to National Language Processing. In our experiments, we used Zhang’s Maximum Entropy Model Toolkit (2004). Naïve Bayesian classifier is used to prove the performance of MEM (McCallum and Nigam (1998)). 3 Extracting Comparative-sentence Candidates In this section, we define comparative keywords and extract comparative-sentence candidates by using those keywords. 3.1 Comparative keyword First of all, we classify comparative sentences into six types and then we ex</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning Extraction Patterns for Subjective Expressions. Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gil-jong Ha</author>
</authors>
<title>Korean Modern Comparative Syntax,</title>
<date>1999</date>
<publisher>Pijbook Press,</publisher>
<location>Seoul,</location>
<contexts>
<context position="4783" citStr="Ha (1999)" startWordPosition="725" endWordPosition="726">section. Section 5 presents the experimental results. Finally, we discuss conclusions and future work in section 6 2 Related Work We have not found any direct work on automatically extracting Korean comparative sentences. There is only one study by Jindal and Liu (2006) that is related to English. They used comparative and superlative POS tags and additional some keywords to search English comparative sentences. Then they used Class Sequential Rules and Naïve Bayesian learning method. Their experiment showed a precision of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sentences also contain the speaker’s opinions and especially comparison is one of the most powerful tools for evaluation. We have surveyed many studies about opinion mining (Lee et al., 2008; Kim and Hovy, 2006; Wilson and Wiebe, 2003; Riloff and Wiebe, 2003; Esuli and Sebastiani, 2006). Maximum Entropy Model is used in our technique. Berger</context>
</contexts>
<marker>Ha, 1999</marker>
<rawString>Gil-jong Ha. 1999. Korean Modern Comparative Syntax, Pijbook Press, Seoul, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gil-jong Ha</author>
</authors>
<date>1999</date>
<booktitle>Research on Korean Equality Comparative Syntax, Association for Korean Linguistics,</booktitle>
<pages>5--229</pages>
<contexts>
<context position="4783" citStr="Ha (1999)" startWordPosition="725" endWordPosition="726">section. Section 5 presents the experimental results. Finally, we discuss conclusions and future work in section 6 2 Related Work We have not found any direct work on automatically extracting Korean comparative sentences. There is only one study by Jindal and Liu (2006) that is related to English. They used comparative and superlative POS tags and additional some keywords to search English comparative sentences. Then they used Class Sequential Rules and Naïve Bayesian learning method. Their experiment showed a precision of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sentences also contain the speaker’s opinions and especially comparison is one of the most powerful tools for evaluation. We have surveyed many studies about opinion mining (Lee et al., 2008; Kim and Hovy, 2006; Wilson and Wiebe, 2003; Riloff and Wiebe, 2003; Esuli and Sebastiani, 2006). Maximum Entropy Model is used in our technique. Berger</context>
</contexts>
<marker>Ha, 1999</marker>
<rawString>Gil-jong Ha. 1999. Research on Korean Equality Comparative Syntax, Association for Korean Linguistics, 5:229-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>In-su Jeong</author>
</authors>
<title>Research on Korean Adjective Superlative Comparative Syntax. Korean Han-minjok Eo-mun-hak,</title>
<date>2000</date>
<pages>36--61</pages>
<contexts>
<context position="4916" citStr="Jeong (2000)" startWordPosition="744" endWordPosition="745">We have not found any direct work on automatically extracting Korean comparative sentences. There is only one study by Jindal and Liu (2006) that is related to English. They used comparative and superlative POS tags and additional some keywords to search English comparative sentences. Then they used Class Sequential Rules and Naïve Bayesian learning method. Their experiment showed a precision of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sentences also contain the speaker’s opinions and especially comparison is one of the most powerful tools for evaluation. We have surveyed many studies about opinion mining (Lee et al., 2008; Kim and Hovy, 2006; Wilson and Wiebe, 2003; Riloff and Wiebe, 2003; Esuli and Sebastiani, 2006). Maximum Entropy Model is used in our technique. Berger et al. (1996) described Maximum Entropy approach to National Language Processing. In our experiments, we used Zhang’s Maximum Entrop</context>
</contexts>
<marker>Jeong, 2000</marker>
<rawString>In-su Jeong. 2000. Research on Korean Adjective Superlative Comparative Syntax. Korean Han-minjok Eo-mun-hak, 36:61-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyeong-sook Oh</author>
</authors>
<title>The Difference between ‘Man-kum’ Comparative and ‘Cheo-rum’ Comparative.</title>
<date>2004</date>
<journal>Society of Korean Semantics,</journal>
<pages>14--197</pages>
<marker>Oh, 2004</marker>
<rawString>Kyeong-sook Oh. 2004. The Difference between ‘Man-kum’ Comparative and ‘Cheo-rum’ Comparative. Society of Korean Semantics, 14:197-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Identifying Comparative Sentences in Text Documents,</title>
<date>2006</date>
<journal>Association for Computing Machinery/Special Interest Group on Information Retrieval,</journal>
<pages>244--251</pages>
<contexts>
<context position="1104" citStr="Jindal and Liu, 2006" startWordPosition="151" endWordPosition="155">xt documents. This paper first investigates many comparative sentences referring to previous studies and then defines a set of comparative keywords from them. A sentence which contains one or more elements of the keyword set is called a comparative-sentence candidate. Finally, we use machine learning techniques to eliminate non-comparative sentences from the candidates. As a result, we achieved significant performance, an F1-score of 88.54%, in our experiments using various web documents. 1 Introduction Comparing one entity with other entities is one of the most convincing ways of evaluation (Jindal and Liu, 2006). A comparative sentence formulates an ordering relation between two entities and that relation is very useful for many application areas. One key area is for the customers. For example, a customer can make a decision on his/her final choice about a digital camera after reading other customers&apos; product reviews, e.g., “Digital Camera X is much cheaper than Y though it functions as good as Y!” Another one is for manufacturers. All the manufacturers have an interest in the articles saying how their products are compared with competitors’ ones. Comparative sentences often contain some comparative </context>
<context position="4444" citStr="Jindal and Liu (2006)" startWordPosition="669" endWordPosition="672">the overall precision of 88.68% and the overall recall of 88.40%. The remainder of the paper is organized as follows. Section 2 describes the related work. In section 3, we explain comparative keywords and comparative-sentence candidates. In section 4, we describe how to eliminate non-comparative sentences from the candidates extracted in preceding section. Section 5 presents the experimental results. Finally, we discuss conclusions and future work in section 6 2 Related Work We have not found any direct work on automatically extracting Korean comparative sentences. There is only one study by Jindal and Liu (2006) that is related to English. They used comparative and superlative POS tags and additional some keywords to search English comparative sentences. Then they used Class Sequential Rules and Naïve Bayesian learning method. Their experiment showed a precision of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sent</context>
<context position="8415" citStr="Jindal and Liu (2006)" startWordPosition="1296" endWordPosition="1299">long-distance-words sequence from Ex1-sentence. It means that the sentence is formed as &lt; S V but S V&gt; in English (S: subject phrase, V: verb phrase). Thus we defined comparative keyword in this paper as follows: Definition (comparative keyword): A comparative keyword is formed as a word or a phrase or a long-distance-words sequence. When a comparative keyword is contained in any sentence, the sentence is most likely to be a comparative sentence. (We will use an abbreviation ‘CK’.) 3.2 Comparative-sentence Candidates We finally set up a total of 177 CKs by human efforts. In the previous work, Jindal and Liu (2006) defined 83 keywords and key phrases including comparative or superlative POS tags in English; they did not use any long-distancewords sequence. Keyword searching process can detect most of comparative sentences (S1, S2 and S3)3 from original text documents. That is, the recall is high but the precision is low. We here defined a comparative-sentence candidate as a sentence which contains one or more elements of the set of CKs. Now we need to eliminate the incorrect sentences (S3) from those captured sentences. First, we divided the set of CKs into two subsets denoted by CKL1 and CKL2 according</context>
</contexts>
<marker>Jindal, Liu, 2006</marker>
<rawString>Nitin Jindal and Bing Liu. 2006. Identifying Comparative Sentences in Text Documents, Association for Computing Machinery/Special Interest Group on Information Retrieval, 244-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Mining Comparative Sentences and Relations,</title>
<date>2006</date>
<journal>Association for Advancement of Artificial Intelligence,</journal>
<pages>1331--1336</pages>
<contexts>
<context position="1104" citStr="Jindal and Liu, 2006" startWordPosition="151" endWordPosition="155">xt documents. This paper first investigates many comparative sentences referring to previous studies and then defines a set of comparative keywords from them. A sentence which contains one or more elements of the keyword set is called a comparative-sentence candidate. Finally, we use machine learning techniques to eliminate non-comparative sentences from the candidates. As a result, we achieved significant performance, an F1-score of 88.54%, in our experiments using various web documents. 1 Introduction Comparing one entity with other entities is one of the most convincing ways of evaluation (Jindal and Liu, 2006). A comparative sentence formulates an ordering relation between two entities and that relation is very useful for many application areas. One key area is for the customers. For example, a customer can make a decision on his/her final choice about a digital camera after reading other customers&apos; product reviews, e.g., “Digital Camera X is much cheaper than Y though it functions as good as Y!” Another one is for manufacturers. All the manufacturers have an interest in the articles saying how their products are compared with competitors’ ones. Comparative sentences often contain some comparative </context>
<context position="4444" citStr="Jindal and Liu (2006)" startWordPosition="669" endWordPosition="672">the overall precision of 88.68% and the overall recall of 88.40%. The remainder of the paper is organized as follows. Section 2 describes the related work. In section 3, we explain comparative keywords and comparative-sentence candidates. In section 4, we describe how to eliminate non-comparative sentences from the candidates extracted in preceding section. Section 5 presents the experimental results. Finally, we discuss conclusions and future work in section 6 2 Related Work We have not found any direct work on automatically extracting Korean comparative sentences. There is only one study by Jindal and Liu (2006) that is related to English. They used comparative and superlative POS tags and additional some keywords to search English comparative sentences. Then they used Class Sequential Rules and Naïve Bayesian learning method. Their experiment showed a precision of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sent</context>
<context position="8415" citStr="Jindal and Liu (2006)" startWordPosition="1296" endWordPosition="1299">long-distance-words sequence from Ex1-sentence. It means that the sentence is formed as &lt; S V but S V&gt; in English (S: subject phrase, V: verb phrase). Thus we defined comparative keyword in this paper as follows: Definition (comparative keyword): A comparative keyword is formed as a word or a phrase or a long-distance-words sequence. When a comparative keyword is contained in any sentence, the sentence is most likely to be a comparative sentence. (We will use an abbreviation ‘CK’.) 3.2 Comparative-sentence Candidates We finally set up a total of 177 CKs by human efforts. In the previous work, Jindal and Liu (2006) defined 83 keywords and key phrases including comparative or superlative POS tags in English; they did not use any long-distancewords sequence. Keyword searching process can detect most of comparative sentences (S1, S2 and S3)3 from original text documents. That is, the recall is high but the precision is low. We here defined a comparative-sentence candidate as a sentence which contains one or more elements of the set of CKs. Now we need to eliminate the incorrect sentences (S3) from those captured sentences. First, we divided the set of CKs into two subsets denoted by CKL1 and CKL2 according</context>
</contexts>
<marker>Jindal, Liu, 2006</marker>
<rawString>Nitin Jindal and Bing Liu. 2006. Mining Comparative Sentences and Relations, Association for Advancement of Artificial Intelligence, 1331-1336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soomin Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic Detection of Opinion Bearing Words and Sentences. Computational Linguistics/Association for Computational Linguistics.</title>
<date>2006</date>
<contexts>
<context position="5250" citStr="Kim and Hovy, 2006" startWordPosition="795" endWordPosition="798">Naïve Bayesian learning method. Their experiment showed a precision of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sentences also contain the speaker’s opinions and especially comparison is one of the most powerful tools for evaluation. We have surveyed many studies about opinion mining (Lee et al., 2008; Kim and Hovy, 2006; Wilson and Wiebe, 2003; Riloff and Wiebe, 2003; Esuli and Sebastiani, 2006). Maximum Entropy Model is used in our technique. Berger et al. (1996) described Maximum Entropy approach to National Language Processing. In our experiments, we used Zhang’s Maximum Entropy Model Toolkit (2004). Naïve Bayesian classifier is used to prove the performance of MEM (McCallum and Nigam (1998)). 3 Extracting Comparative-sentence Candidates In this section, we define comparative keywords and extract comparative-sentence candidates by using those keywords. 3.1 Comparative keyword First of all, we classify com</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soomin Kim and Eduard Hovy. 2006. Automatic Detection of Opinion Bearing Words and Sentences. Computational Linguistics/Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<date>2003</date>
<booktitle>Annotating Opinions in the World Press. Special Interest Group in Discourse and Dialoque/Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5274" citStr="Wilson and Wiebe, 2003" startWordPosition="799" endWordPosition="802">ing method. Their experiment showed a precision of 79% and recall of 81%. Our research is closely related to linguistics. Ha (1999) described Korean comparative constructions with a linguistic view. Oh (2003) discussed the gradability of comparatives. Jeong (2000) classified the adjective superlative by the type of measures. Opinion mining is also related to our work. Many comparative sentences also contain the speaker’s opinions and especially comparison is one of the most powerful tools for evaluation. We have surveyed many studies about opinion mining (Lee et al., 2008; Kim and Hovy, 2006; Wilson and Wiebe, 2003; Riloff and Wiebe, 2003; Esuli and Sebastiani, 2006). Maximum Entropy Model is used in our technique. Berger et al. (1996) described Maximum Entropy approach to National Language Processing. In our experiments, we used Zhang’s Maximum Entropy Model Toolkit (2004). Naïve Bayesian classifier is used to prove the performance of MEM (McCallum and Nigam (1998)). 3 Extracting Comparative-sentence Candidates In this section, we define comparative keywords and extract comparative-sentence candidates by using those keywords. 3.1 Comparative keyword First of all, we classify comparative sentences into </context>
</contexts>
<marker>Wilson, Wiebe, 2003</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2003. Annotating Opinions in the World Press. Special Interest Group in Discourse and Dialoque/Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Le</author>
</authors>
<date>2004</date>
<booktitle>Maximum Entropy Modeling Toolkit for Python and C++. http://homepages.inf.ed.ac. uk/s0450736/maxent_toolkit.html.</booktitle>
<marker>Le, 2004</marker>
<rawString>Zhang Le. 2004. Maximum Entropy Modeling Toolkit for Python and C++. http://homepages.inf.ed.ac. uk/s0450736/maxent_toolkit.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>