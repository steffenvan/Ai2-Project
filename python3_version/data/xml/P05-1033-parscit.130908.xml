<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.957479">
A Hierarchical Phrase-Based Model for Statistical Machine Translation
</title>
<author confidence="0.99846">
David Chiang
</author>
<affiliation confidence="0.999324">
Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742, USA
</affiliation>
<email confidence="0.998466">
dchiang@umiacs.umd.edu
</email>
<sectionHeader confidence="0.994791" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999229307692308">
We present a statistical phrase-based transla-
tion model that uses hierarchical phrases—
phrases that contain subphrases. The model
is formally a synchronous context-free gram-
mar but is learned from a bitext without any
syntactic information. Thus it can be seen as
a shift to the formal machinery of syntax-
based translation systems without any lin-
guistic commitment. In our experiments us-
ing BLEU as a metric, the hierarchical phrase-
based model achieves a relative improve-
ment of 7.5% over Pharaoh, a state-of-the-art
phrase-based system.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919461538461">
The alignment template translation model (Och and
Ney, 2004) and related phrase-based models ad-
vanced the previous state of the art by moving
from words to phrases as the basic unit of transla-
tion. Phrases, which can be any substring and not
necessarily phrases in any syntactic theory, allow
these models to learn local reorderings, translation
of short idioms, or insertions and deletions that are
sensitive to local context. They are thus a simple and
powerful mechanism for machine translation.
The basic phrase-based model is an instance of
the noisy-channel approach (Brown et al., 1993),1 in
which the translation of a French sentence f into an
</bodyText>
<footnote confidence="0.87313">
1Throughout this paper, we follow the convention of Brown
et al. of designating the source and target languages as “French”
and “English,” respectively. The variables f and e stand for
source and target sentences; fij stands for the substring of f
</footnote>
<bodyText confidence="0.718392">
from position i to position j inclusive, and similarly for eji .
English sentence e is modeled as:
</bodyText>
<equation confidence="0.852561">
(1) arg max P(e  |f) = arg max P(e, f)
e e
(2) = arg max(P(e) x P(f  |e))
e
</equation>
<bodyText confidence="0.9918995">
The translation model P(f  |e) “encodes” e into f by
the following steps:
</bodyText>
<listItem confidence="0.971341571428572">
1. segment e into phrases ¯e1 ¯eI, typically with
a uniform distribution over segmentations;
2. reorder the ¯ei according to some distortion
model;
3. translate each of the ¯ei into French phrases ac-
cording to a model P(f¯  |¯e) estimated from the
training data.
</listItem>
<bodyText confidence="0.984882578947369">
Other phrase-based models model the joint distribu-
tion P(e, f) (Marcu and Wong, 2002) or made P(e)
and P(f  |e) into features of a log-linear model (Och
and Ney, 2002). But the basic architecture of phrase
segmentation (or generation), phrase reordering, and
phrase translation remains the same.
Phrase-based models can robustly perform trans-
lations that are localized to substrings that are com-
mon enough to have been observed in training. But
Koehn et al. (2003) find that phrases longer than
three words improve performance little, suggesting
that data sparseness takes over for longer phrases.
Above the phrase level, these models typically have
a simple distortion model that reorders phrases in-
dependently of their content (Och and Ney, 2004;
Koehn et al., 2003), or not at all (Zens and Ney,
2004; Kumar et al., 2005).
But it is often desirable to capture translations
whose scope is larger than a few consecutive words.
</bodyText>
<page confidence="0.978443">
263
</page>
<note confidence="0.9938845">
Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<figure confidence="0.606255333333333">
Consider the following Mandarin example and its
English translation:
(3) j,5#H
Aozhou
Australia
� .9 ®* 2-
</figure>
<bodyText confidence="0.958344530864197">
de shaoshu guojia zhiyi
that few countries one of
‘Australia is one of the few countries that have
diplomatic relations with North Korea’
If we count zhiyi, lit. ‘of-one,’ as a single token, then
translating this sentence correctly into English re-
quires reversing a sequence of five elements. When
we run a phrase-based system, Pharaoh (Koehn et
al., 2003; Koehn, 2004a), on this sentence (using the
experimental setup described below), we get the fol-
lowing phrases with translations:
(4) [Aozhou] [shi] [yu] [Bei Han] [you]
[bangjiao]1 [de shaoshu guojia zhiyi]
[Australia] [is] [dipl. rels.]1 [with] [North
Korea] [is] [one of the few countries]
where we have used subscripts to indicate the re-
ordering of phrases. The phrase-based model is
able to order “diplomatic...Korea” correctly (using
phrase reordering) and “one...countries” correctly
(using a phrase translation), but does not accom-
plish the necessary inversion of those two groups.
A lexicalized phrase-reordering model like that in
use in ISI’s system (Och et al., 2004) might be able
to learn a better reordering, but simpler distortion
models will probably not.
We propose a solution to these problems that
does not interfere with the strengths of the phrase-
based approach, but rather capitalizes on them: since
phrases are good for learning reorderings of words,
we can use them to learn reorderings of phrases
as well. In order to do this we need hierarchical
phrases that consist of both words and subphrases.
For example, a hierarchical phrase pair that might
help with the above example is:
(5) (yu 1 you 2 , have 2 with 1 )
where 1 and 2 are placeholders for subphrases. This
would capture the fact that Chinese PPs almost al-
ways modify VP on the left, whereas English PPs
usually modify VP on the right. Because it gener-
alizes over possible prepositional objects and direct
objects, it acts both as a discontinuous phrase pair
and as a phrase-reordering rule. Thus it is consider-
ably more powerful than a conventional phrase pair.
Similarly,
(6) ( 1 de 2 , the 2 that 1 )
would capture the fact that Chinese relative clauses
modify NPs on the left, whereas English relative
clauses modify on the right; and
(7) ( 1 zhiyi, one of 1 )
would render the construction zhiyi in English word
order. These three rules, along with some conven-
tional phrase pairs, suffice to translate the sentence
correctly:
(8) [Aozhou] [shi] [[[yu [Bei Han]1 you
[bangjiao]2] de [shaoshu guojia]3] zhiyi]
[Australia] [is] [one of [the [few countries]3
that [have [dipl. rels.]2 with [North Korea]1]]]
The system we describe below uses rules like this,
and in fact is able to learn them automatically from
a bitext without syntactic annotation. It translates the
above example almost exactly as we have shown, the
only error being that it omits the word ‘that’ from (6)
and therefore (8).
These hierarchical phrase pairs are formally pro-
ductions of a synchronous context-free grammar
(defined below). A move to synchronous CFG can
be seen as a move towards syntax-based MT; how-
ever, we make a distinction here between formally
syntax-based and linguistically syntax-based MT. A
system like that of Yamada and Knight (2001) is
both formally and linguistically syntax-based: for-
mally because it uses synchronous CFG, linguisti-
cally because the structures it is defined over are (on
the English side) informed by syntactic theory (via
the Penn Treebank). Our system is formally syntax-
based in that it uses synchronous CFG, but not nec-
essarily linguistically syntax-based, because it in-
duces a grammar from a parallel text without relying
on any linguistic annotations or assumptions; the re-
sult sometimes resembles a syntactician’s grammar
but often does not. In this respect it resembles Wu’s
</bodyText>
<figure confidence="0.999043055555556">
IL
Bei
North
0
Han
Korea
9x
bangjiao
dipl. rels.
�
you
have
A
shi
is
4
yu
with
</figure>
<page confidence="0.993439">
264
</page>
<bodyText confidence="0.999907909090909">
bilingual bracketer (Wu, 1997), but ours uses a dif-
ferent extraction method that allows more than one
lexical item in a rule, in keeping with the phrase-
based philosophy. Our extraction method is basi-
cally the same as that of Block (2000), except we
allow more than one nonterminal symbol in a rule,
and use a more sophisticated probability model.
In this paper we describe the design and imple-
mentation of our hierarchical phrase-based model,
and report on experiments that demonstrate that hi-
erarchical phrases indeed improve translation.
</bodyText>
<sectionHeader confidence="0.955466" genericHeader="method">
2 The model
</sectionHeader>
<bodyText confidence="0.9998315">
Our model is based on a weighted synchronous CFG
(Aho and Ullman, 1969). In a synchronous CFG the
elementary structures are rewrite rules with aligned
pairs of right-hand sides:
</bodyText>
<equation confidence="0.822709">
(9) X —� (γ, α, —)
</equation>
<bodyText confidence="0.999183965517241">
where X is a nonterminal, γ and α are both strings
of terminals and nonterminals, and — is a one-to-one
correspondence between nonterminal occurrences
in γ and nonterminal occurrences in α. Rewriting
begins with a pair of linked start symbols. At each
step, two coindexed nonterminals are rewritten us-
ing the two components of a single rule, such that
none of the newly introduced symbols is linked to
any symbols already present.
Thus the hierarchical phrase pairs from our above
example could be formalized in a synchronous CFG
as:
where we have used boxed indices to indicate which
occurrences of X are linked by —.
Note that we have used only a single nonterminal
symbol X instead of assigning syntactic categories
to phrases. In the grammar we extract from a bitext
(described below), all of our rules use only X, ex-
cept for two special “glue” rules, which combine a
sequence of Xs to form an S:
These give the model the option to build only par-
tial translations using hierarchical phrases, and then
combine them serially as in a standard phrase-based
model. For a partial example of a synchronous CFG
derivation, see Figure 1.
Following Och and Ney (2002), we depart from
the traditional noisy-channel approach and use a
more general log-linear model. The weight of each
rule is:
</bodyText>
<equation confidence="0.981515">
(15) w(X � (γ, α)) = � φi(X —� (γ, α))λi
i
</equation>
<bodyText confidence="0.999731666666667">
where the φi are features defined on rules. For our
experiments we used the following features, analo-
gous to Pharaoh’s default feature set:
</bodyText>
<listItem confidence="0.998726625">
• P(γ  |α) and P(α  |γ), the latter of which is not
found in the noisy-channel model, but has been
previously found to be a helpful feature (Och
and Ney, 2002);
• the lexical weights Pw(γ  |α) and Pw(α  |γ)
(Koehn et al., 2003), which estimate how well
the words in α translate the words in γ;2
• a phrase penalty exp(1), which allows the
</listItem>
<bodyText confidence="0.8970148125">
model to learn a preference for longer or
shorter derivations, analogous to Koehn’s
phrase penalty (Koehn, 2003).
The exceptions to the above are the two glue rules,
(13), which has weight one, and (14), which has
weight
(16) w(S _� (S 1 X 2 , S 1 X 2 )) = exp(—λg)
the idea being that λg controls the model’s prefer-
ence for hierarchical phrases over serial combination
of phrases.
Let D be a derivation of the grammar, and let f(D)
and e(D) be the French and English strings gener-
ated by D. Let us represent D as a set of triples
(r, i, j), each of which stands for an application of
a grammar rule r to rewrite a nonterminal that spans
f(D)ji on the French side.3 Then the weight of D
</bodyText>
<footnote confidence="0.906765166666667">
2This feature uses word alignment information, which is dis-
carded in the final grammar. If a rule occurs in training with
more than one possible word alignment, Koehn et al. take the
maximum lexical weight; we take a weighted average.
3This representation is not completely unambiguous, but is
sufficient for defining the model.
</footnote>
<equation confidence="0.999808166666667">
X —� (X 1 de X 2 , the X 2 that X 1 )
(11)
(10) X _� (yu X 1 you X 2 , have X 2 with X 1 )
(12) X —� (X 1 zhiyi, one of X 1 )
(13) S_�(S1X2,S1X2)
S —� (X 1 ,X 1 )
</equation>
<page confidence="0.928268">
265
</page>
<bodyText confidence="0.9747485">
hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i
⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i
⇒ hX 6 X 5 X 3 ,X 6 X 5 X 3 i
⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i
⇒ hAozhou shi X 3 , Australia is X 3 i
⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i
⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i
⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i
</bodyText>
<figureCaption confidence="0.998904">
Figure 1: Example partial derivation of a synchronous CFG.
</figureCaption>
<bodyText confidence="0.988724">
is the product of the weights of the rules used in the
translation, multiplied by the following extra factors:
</bodyText>
<equation confidence="0.993587">
(17) w(D) = � w(r) × plm(e)λlm × exp(−λwp|e|)
hr,i,ji∈D
</equation>
<bodyText confidence="0.999985666666667">
where plm is the language model, and exp(−λwp|e|),
the word penalty, gives some control over the length
of the English output.
We have separated these factors out from the rule
weights for notational convenience, but it is concep-
tually cleaner (and necessary for polynomial-time
decoding) to integrate them into the rule weights,
so that the whole model is a weighted synchronous
CFG. The word penalty is easy; the language model
is integrated by intersecting the English-side CFG
with the language model, which is a weighted finite-
state automaton.
</bodyText>
<sectionHeader confidence="0.996953" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.969743409090909">
The training process begins with a word-aligned cor-
pus: a set of triples hf, e, ∼i, where f is a French
sentence, e is an English sentence, and ∼ is a (many-
to-many) binary relation between positions of f and
positions of e. We obtain the word alignments using
the method of Koehn et al. (2003), which is based
on that of Och and Ney (2004). This involves run-
ning GIZA++ (Och and Ney, 2000) on the corpus in
both directions, and applying refinement rules (the
variant they designate “final-and”) to obtain a single
many-to-many word alignment for each sentence.
Then, following Och and others, we use heuris-
tics to hypothesize a distribution of possible deriva-
tions of each training example, and then estimate
the phrase translation parameters from the hypoth-
esized distribution. To do this, we first identify ini-
tial phrase pairs using the same criterion as previous
systems (Och and Ney, 2004; Koehn et al., 2003):
Definition 1. Given a word-aligned sentence pair
hf, e, ∼i, a rule hfij, ej0
i0 i is an initial phrase pair of
hf, e, ∼i iff:
</bodyText>
<listItem confidence="0.997883333333333">
1. fk ∼ ek0 for some k ∈ [i, j] and k0 ∈ [i0, j0];
2. fk -P ek0 for all k ∈ [i, j] and k0 0 [i0, j0];
3. fk -P ek0 for all k 0 [i, j] and k0 ∈ [i0, j0].
</listItem>
<bodyText confidence="0.9760565">
Next, we form all possible differences of phrase
pairs:
Definition 2. The set of rules of hf, e, ∼i is the
smallest set satisfying the following:
</bodyText>
<listItem confidence="0.745731">
1. If hfij, ej0
</listItem>
<equation confidence="0.996660333333333">
i0 i is an initial phrase pair, then
X → hf i j ,ej0
i0 i
</equation>
<listItem confidence="0.8262228">
is a rule.
2. If r = X → hγ, αi is a rule and hfij, ej0
i0 i is an
initial phrase pair such that γ = γ1 fij γ2 and α =
j0
</listItem>
<equation confidence="0.9955635">
α1ei0 α2, then
X → hγ1X k γ2,α1X k α2i
</equation>
<bodyText confidence="0.9211405">
is a rule, where k is an index not used in r.
The above scheme generates a very large num-
ber of rules, which is undesirable not only because
it makes training and decoding very slow, but also
</bodyText>
<page confidence="0.990342">
266
</page>
<bodyText confidence="0.9998678">
because it creates spurious ambiguity—a situation
where the decoder produces many derivations that
are distinct yet have the same model feature vectors
and give the same translation. This can result in n-
best lists with very few different translations or fea-
ture vectors, which is problematic for the algorithm
we use to tune the feature weights. Therefore we
filter our grammar according to the following prin-
ciples, chosen to balance grammar size and perfor-
mance on our development set:
</bodyText>
<listItem confidence="0.997904727272727">
1. If there are multiple initial phrase pairs contain-
ing the same set of alignment points, we keep
only the smallest.
2. Initial phrases are limited to a length of 10 on
the French side, and rule to five (nonterminals
plus terminals) on the French right-hand side.
3. In the subtraction step, fi/ must have length
greater than one. The rationale is that little
would be gained by creating a new rule that is
no shorter than the original.
4. Rules can have at most two nonterminals,
</listItem>
<bodyText confidence="0.860362125">
which simplifies the decoder implementation.
Moreover, we prohibit nonterminals that are
adjacent on the French side, a major cause of
spurious ambiguity.
5. A rule must have at least one pair of aligned
words, making translation decisions always
based on some lexical evidence.
Now we must hypothesize weights for all the deriva-
tions. Och’s method gives equal weight to all the
extracted phrase occurences. However, our method
may extract many rules from a single initial phrase
pair; therefore we distribute weight equally among
initial phrase pairs, but distribute that weight equally
among the rules extracted from each. Treating this
distribution as our observed data, we use relative-
frequency estimation to obtain P(y  |α) and P(α  |y).
</bodyText>
<sectionHeader confidence="0.99713" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.994576571428571">
Our decoder is a CKY parser with beam search
together with a postprocessor for mapping French
derivations to English derivations. Given a French
sentence f, it finds the best derivation (or n best
derivations, with little overhead) that generates (f, e)
for some e. Note that we find the English yield of the
highest-probability single derivation
</bodyText>
<equation confidence="0.599515333333333">
����� �����
(18) e �arg max w(D) �
D s.t. f(D) = f
</equation>
<bodyText confidence="0.999982976190476">
and not necessarily the highest-probability e, which
would require a more expensive summation over
derivations.
We prune the search space in several ways. First,
an item that has a score worse than /3 times the best
score in the same cell is discarded; second, an item
that is worse than the bth best item in the same cell is
discarded. Each cell contains all the items standing
for X spanning fi/. We choose b and /3 to balance
speed and performance on our development set. For
our experiments, we set b = 40,/3 = 10−1 for X cells,
and b = 15,/3 = 10−1 for S cells. We also prune rules
that have the same French side (b = 100).
The parser only operates on the French-side gram-
mar; the English-side grammar affects parsing only
by increasing the effective grammar size, because
there may be multiple rules with the same French
side but different English sides, and also because in-
tersecting the language model with the English-side
grammar introduces many states into the nontermi-
nal alphabet, which are projected over to the French
side. Thus, our decoder’s search space is many times
larger than a monolingual parser’s would be. To re-
duce this effect, we apply the following heuristic
when filling a cell: if an item falls outside the beam,
then any item that would be generated using a lower-
scoring rule or a lower-scoring antecedent item is
also assumed to fall outside the beam. This heuristic
greatly increases decoding speed, at the cost of some
search errors.
Finally, the decoder has a constraint that pro-
hibits any X from spanning a substring longer than
10 on the French side, corresponding to the maxi-
mum length constraint on initial rules during train-
ing. This makes the decoding algorithm asymptoti-
cally linear-time.
The decoder is implemented in Python, an inter-
preted language, with C++ code from the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002). Using the
settings described above, on a 2.4 GHz Pentium IV,
it takes about 20 seconds to translate each sentence
(average length about 30). This is faster than our
</bodyText>
<page confidence="0.990699">
267
</page>
<bodyText confidence="0.99979075">
Python implementation of a standard phrase-based
decoder, so we expect that a future optimized imple-
mentation of the hierarchical decoder will run at a
speed competitive with other phrase-based systems.
</bodyText>
<sectionHeader confidence="0.996838" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99997">
Our experiments were on Mandarin-to-English
translation. We compared a baseline system,
the state-of-the-art phrase-based system Pharaoh
(Koehn et al., 2003; Koehn, 2004a), against our sys-
tem. For all three systems we trained the transla-
tion model on the FBIS corpus (7.2M+9.2M words);
for the language model, we used the SRI Language
Modeling Toolkit to train a trigram model with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998) on 155M words of English newswire text,
mostly from the Xinhua portion of the Gigaword
corpus. We used the 2002 NIST MT evaluation test
set as our development set, and the 2003 test set as
our test set. Our evaluation metric was BLEU (Pap-
ineni et al., 2002), as calculated by the NIST script
(version 11a) with its default settings, which is to
perform case-insensitive matching of n-grams up to
n = 4, and to use the shortest (as opposed to nearest)
reference sentence for the brevity penalty. The re-
sults of the experiments are summarized in Table 1.
</bodyText>
<subsectionHeader confidence="0.97037">
5.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999732473684211">
The baseline system we used for comparison was
Pharaoh (Koehn et al., 2003; Koehn, 2004a), as pub-
licly distributed. We used the default feature set: lan-
guage model (same as above), p(f¯  |¯e), p(¯e  |f¯), lex-
ical weighting (both directions), distortion model,
word penalty, and phrase penalty. We ran the trainer
with its default settings (maximum phrase length 7),
and then used Koehn’s implementation of minimum-
error-rate training (Och, 2003) to tune the feature
weights to maximize the system’s BLEU score on
our development set, yielding the values shown in
Table 2. Finally, we ran the decoder on the test set,
pruning the phrase table with b = 100, pruning the
chart with b = 100,/3 = 10−5, and limiting distor-
tions to 4. These are the default settings, except for
the phrase table’s b, which was raised from 20, and
the distortion limit. Both of these changes, made by
Koehn’s minimum-error-rate trainer by default, im-
prove performance on the development set.
</bodyText>
<note confidence="0.388425">
Rank Chinese English
</note>
<equation confidence="0.980834466666667">
1 .
3  the
14 ;rE in
23  ’s
577 X 1  X 2 the X 2 of X 1
735 X 1  X 2 the X 2 X 1
763 X 1 2— one of X 1
1201 X 1,,It- president X 1
1240 X 1 517=G $ X 1
2091 �f_ X 1 X 1 this year
3253 ~3�2 X 1 X 1 percent
10508 ;rt X 1 T under X 1
28426 ;rE X 1 i�U before X 1
47015 X 1  X 2 the X 2 that X 1
1752457 4 X 1 - 4X 2 have X 2 with X 1
</equation>
<figureCaption confidence="0.744592333333333">
Figure 2: A selection of extracted rules, with ranks
after filtering for the development set. All have X for
their left-hand sides.
</figureCaption>
<subsectionHeader confidence="0.997519">
5.2 Hierarchical model
</subsectionHeader>
<bodyText confidence="0.952185238095238">
We ran the training process of Section 3 on the same
data, obtaining a grammar of 24M rules. When fil-
tered for the development set, the grammar has 2.2M
rules (see Figure 2 for examples). We then ran the
minimum-error rate trainer with our decoder to tune
the feature weights, yielding the values shown in Ta-
ble 2. Note that Ag penalizes the glue rule much less
than App does ordinary rules. This suggests that the
model will prefer serial combination of phrases, un-
less some other factor supports the use of hierarchi-
cal phrases (e.g., a better language model score).
We then tested our system, using the settings de-
scribed above.4 Our system achieves an absolute im-
provement of 0.02 over the baseline (7.5% relative),
without using any additional training data. This dif-
ference is statistically significant (p &lt; 0.01).5 See
Table 1, which also shows that the relative gain is
higher for higher n-grams.
4Note that we gave Pharaoh wider beam settings than we
used on our own decoder; on the other hand, since our decoder’s
chart has more cells, its b limits do not need to be as high.
</bodyText>
<footnote confidence="0.99288175">
5We used Zhang’s significance tester (Zhang et al., 2004),
which uses bootstrap resampling (Koehn, 2004b); it was mod-
ified to conform to NIST’s current definition of the BLEU
brevity penalty.
</footnote>
<page confidence="0.97955">
268
</page>
<table confidence="0.996239">
BLEU-n n-gram precisions
System 4 1 2 3 4 5 6 7 8
Pharaoh 0.2676 0.72 0.37 0.19 0.10 0.052 0.027 0.014 0.0075
hierarchical 0.2877 0.74 0.39 0.21 0.11 0.060 0.032 0.017 0.0084
+constituent 0.2881 0.73 0.39 0.21 0.11 0.062 0.032 0.017 0.0088
</table>
<tableCaption confidence="0.998996">
Table 1: Results on baseline system and hierarchical system, with and without constituent feature.
</tableCaption>
<table confidence="0.9997186">
Features
System Plm(e) P(y|α) P(α|y) P.(y|α) P.(α|y) Word Phr Ad Ag Ac
Pharaoh 0.19 0.095 0.030 0.14 0.029 −0.20 0.22 0.11 — —
hierarchical 0.15 0.036 0.074 0.037 0.076 −0.32 0.22 — 0.09 —
+constituent 0.11 0.026 0.062 0.025 0.029 −0.23 0.21 — 0.11 0.20
</table>
<tableCaption confidence="0.688778666666667">
Table 2: Feature weights obtained by minimum-error-rate training (normalized so that absolute values sum
to one). Word = word penalty; Phr = phrase penalty. Note that we have inverted the sense of Pharaoh’s
phrase penalty so that a positive weight indicates a penalty.
</tableCaption>
<subsectionHeader confidence="0.999451">
5.3 Adding a constituent feature
</subsectionHeader>
<bodyText confidence="0.999859">
The use of hierarchical structures opens the pos-
sibility of making the model sensitive to syntac-
tic structure. Koehn et al. (2003) mention German
(es gibt, there is) as an example of a good phrase
pair which is not a syntactic phrase pair, and report
that favoring syntactic phrases does not improve ac-
curacy. But in our model, the rule
</bodyText>
<equation confidence="0.635193">
(19) X -� (es gibt X 1 , there is X 1 )
</equation>
<bodyText confidence="0.995285333333333">
would indeed respect syntactic phrases, because it
builds a pair of Ss out of a pair of NPs. Thus, favor-
ing subtrees in our model that are syntactic phrases
might provide a fairer way of testing the hypothesis
that syntactic phrases are better phrases.
This feature adds a factor to (17),
{ 1 if fij is a constituent
0 otherwise
as determined by a statistical tree-substitution-
grammar parser (Bikel and Chiang, 2000), trained
on the Penn Chinese Treebank, version 3 (250k
words). Note that the parser was run only on the
test data and not the (much larger) training data. Re-
running the minimum-error-rate trainer with the new
feature yielded the feature weights shown in Table 2.
Although the feature improved accuracy on the de-
velopment set (from 0.314 to 0.322), it gave no sta-
tistically significant improvement on the test set.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999971107142857">
Hierarchical phrase pairs, which can be learned
without any syntactically-annotated training data,
improve translation accuracy significantly compared
with a state-of-the-art phrase-based system. They
also facilitate the incorporation of syntactic informa-
tion, which, however, did not provide a statistically
significant gain.
Our primary goal for the future is to move towards
a more syntactically-motivated grammar, whether
by automatic methods to induce syntactic categories,
or by better integration of parsers trained on an-
notated data. This would potentially improve both
accuracy and efficiency. Moreover, reducing the
grammar size would allow more ambitious train-
ing settings. The maximum initial phrase length
is currently 10; preliminary experiments show that
increasing this limit to as high as 15 does im-
prove accuracy, but requires more memory. On the
other hand, we have successfully trained on almost
30M+30M words by tightening the initial phrase
length limit for part of the data. Streamlining the
grammar would allow further experimentation in
these directions.
In any case, future improvements to this system
will maintain the design philosophy proven here,
that ideas from syntax should be incorporated into
statistical translation, but not in exchange for the
strengths of the phrase-based approach.
</bodyText>
<equation confidence="0.975638">
(20) c(i, j) =
</equation>
<page confidence="0.998676">
269
</page>
<sectionHeader confidence="0.996537" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.989813714285714">
I would like to thank Philipp Koehn for the use of the
Pharaoh software; and Adam Lopez, Michael Sub-
otin, Nitin Madnani, Christof Monz, Liang Huang,
and Philip Resnik. This work was partially sup-
ported by ONR MURI contract FCPO.810548265
and Department of Defense contract RD-02-5700.
S. D. G.
</bodyText>
<sectionHeader confidence="0.996546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99656434939759">
A. V. Aho and J. D. Ullman. 1969. Syntax directed trans-
lations and the pushdown assembler. Journal of Com-
puter and System Sciences, 3:37–56.
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the Chinese Treebank.
In Proceedings of the Second Chinese Language Pro-
cessing Workshop, pages 1–6.
Hans Ulrich Block. 2000. Example-based incremen-
tal synchronous interpretation. In Wolfgang Wahlster,
editor, Verbmobil: Foundations of Speech-to-Speech
Translation, pages 411–417. Springer-Verlag, Berlin.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19:263–311.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings ofHLT-NAACL 2003, pages 127–133.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D.
thesis, University of Southern California.
Philipp Koehn. 2004a. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115–124.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 388–395.
Shankar Kumar, Yonggang Deng, and William Byrne.
2005. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering. To appear.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 133–139.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the ACL, pages 440–447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295–302.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417–449.
Franz Josef Och, Ignacio Thayer, Daniel Marcu, Kevin
Knight, Dragos Stefan Munteanu, Quamrul Tipu,
Michel Galley, and Mark Hopkins. 2004. Arabic and
Chinese MT at USC/ISI. Presentation given at NIST
Machine Translation Evaluation Workshop.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 311–318.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901–904.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the ACL, pages 523–530.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings ofHLT-NAACL 2004, pages 257–264.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of the Fourth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 2051–
2054.
</reference>
<page confidence="0.996815">
270
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.783227">
<title confidence="0.999981">A Hierarchical Phrase-Based Model for Statistical Machine Translation</title>
<author confidence="0.999984">David Chiang</author>
<affiliation confidence="0.999518">Institute for Advanced Computer Studies (UMIACS)</affiliation>
<address confidence="0.801879">University of Maryland, College Park, MD 20742, USA</address>
<email confidence="0.999825">dchiang@umiacs.umd.edu</email>
<abstract confidence="0.998360285714286">We present a statistical phrase-based translamodel that uses phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as shift to the of syntaxtranslation systems without any lin- In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>3--37</pages>
<contexts>
<context position="7798" citStr="Aho and Ullman, 1969" startWordPosition="1277" endWordPosition="1280">lingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrasebased philosophy. Our extraction method is basically the same as that of Block (2000), except we allow more than one nonterminal symbol in a rule, and use a more sophisticated probability model. In this paper we describe the design and implementation of our hierarchical phrase-based model, and report on experiments that demonstrate that hierarchical phrases indeed improve translation. 2 The model Our model is based on a weighted synchronous CFG (Aho and Ullman, 1969). In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: (9) X —� (γ, α, —) where X is a nonterminal, γ and α are both strings of terminals and nonterminals, and — is a one-to-one correspondence between nonterminal occurrences in γ and nonterminal occurrences in α. Rewriting begins with a pair of linked start symbols. At each step, two coindexed nonterminals are rewritten using the two components of a single rule, such that none of the newly introduced symbols is linked to any symbols already present. Thus the hierarchical phrase pairs from our</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>A. V. Aho and J. D. Ullman. 1969. Syntax directed translations and the pushdown assembler. Journal of Computer and System Sciences, 3:37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two statistical parsing models applied to the Chinese Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Chinese Language Processing Workshop,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="23683" citStr="Bikel and Chiang, 2000" startWordPosition="4202" endWordPosition="4205">n example of a good phrase pair which is not a syntactic phrase pair, and report that favoring syntactic phrases does not improve accuracy. But in our model, the rule (19) X -� (es gibt X 1 , there is X 1 ) would indeed respect syntactic phrases, because it builds a pair of Ss out of a pair of NPs. Thus, favoring subtrees in our model that are syntactic phrases might provide a fairer way of testing the hypothesis that syntactic phrases are better phrases. This feature adds a factor to (17), { 1 if fij is a constituent 0 otherwise as determined by a statistical tree-substitutiongrammar parser (Bikel and Chiang, 2000), trained on the Penn Chinese Treebank, version 3 (250k words). Note that the parser was run only on the test data and not the (much larger) training data. Rerunning the minimum-error-rate trainer with the new feature yielded the feature weights shown in Table 2. Although the feature improved accuracy on the development set (from 0.314 to 0.322), it gave no statistically significant improvement on the test set. 6 Conclusion Hierarchical phrase pairs, which can be learned without any syntactically-annotated training data, improve translation accuracy significantly compared with a state-of-the-a</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied to the Chinese Treebank. In Proceedings of the Second Chinese Language Processing Workshop, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Ulrich Block</author>
</authors>
<title>Example-based incremental synchronous interpretation.</title>
<date>2000</date>
<booktitle>Verbmobil: Foundations of Speech-to-Speech Translation,</booktitle>
<pages>411--417</pages>
<editor>In Wolfgang Wahlster, editor,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="7412" citStr="Block (2000)" startWordPosition="1217" endWordPosition="1218">ses synchronous CFG, but not necessarily linguistically syntax-based, because it induces a grammar from a parallel text without relying on any linguistic annotations or assumptions; the result sometimes resembles a syntactician’s grammar but often does not. In this respect it resembles Wu’s IL Bei North 0 Han Korea 9x bangjiao dipl. rels. � you have A shi is 4 yu with 264 bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrasebased philosophy. Our extraction method is basically the same as that of Block (2000), except we allow more than one nonterminal symbol in a rule, and use a more sophisticated probability model. In this paper we describe the design and implementation of our hierarchical phrase-based model, and report on experiments that demonstrate that hierarchical phrases indeed improve translation. 2 The model Our model is based on a weighted synchronous CFG (Aho and Ullman, 1969). In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: (9) X —� (γ, α, —) where X is a nonterminal, γ and α are both strings of terminals and nonterminals, and — </context>
</contexts>
<marker>Block, 2000</marker>
<rawString>Hans Ulrich Block. 2000. Example-based incremental synchronous interpretation. In Wolfgang Wahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation, pages 411–417. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1363" citStr="Brown et al., 1993" startWordPosition="203" endWordPosition="206">ed system. 1 Introduction The alignment template translation model (Och and Ney, 2004) and related phrase-based models advanced the previous state of the art by moving from words to phrases as the basic unit of translation. Phrases, which can be any substring and not necessarily phrases in any syntactic theory, allow these models to learn local reorderings, translation of short idioms, or insertions and deletions that are sensitive to local context. They are thus a simple and powerful mechanism for machine translation. The basic phrase-based model is an instance of the noisy-channel approach (Brown et al., 1993),1 in which the translation of a French sentence f into an 1Throughout this paper, we follow the convention of Brown et al. of designating the source and target languages as “French” and “English,” respectively. The variables f and e stand for source and target sentences; fij stands for the substring of f from position i to position j inclusive, and similarly for eji . English sentence e is modeled as: (1) arg max P(e |f) = arg max P(e, f) e e (2) = arg max(P(e) x P(f |e)) e The translation model P(f |e) “encodes” e into f by the following steps: 1. segment e into phrases ¯e1 ¯eI, typically wi</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="18669" citStr="Chen and Goodman, 1998" startWordPosition="3282" endWordPosition="3285">ndard phrase-based decoder, so we expect that a future optimized implementation of the hierarchical decoder will run at a speed competitive with other phrase-based systems. 5 Experiments Our experiments were on Mandarin-to-English translation. We compared a baseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system. For all three systems we trained the translation model on the FBIS corpus (7.2M+9.2M words); for the language model, we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as our development set, and the 2003 test set as our test set. Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its default settings, which is to perform case-insensitive matching of n-grams up to n = 4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty. The results of the experiments are summarized in Table 1. 5.1 Baseline The baseline system we used for c</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="2641" citStr="Koehn et al. (2003)" startWordPosition="425" endWordPosition="428">e ¯ei according to some distortion model; 3. translate each of the ¯ei into French phrases according to a model P(f¯ |¯e) estimated from the training data. Other phrase-based models model the joint distribution P(e, f) (Marcu and Wong, 2002) or made P(e) and P(f |e) into features of a log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005). But it is often desirable to capture translations whose scope is larger than a few consecutive words. 263 Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270, Ann Arbor, June 2005. c�2005 Association for Computational Linguis</context>
<context position="9616" citStr="Koehn et al., 2003" startWordPosition="1598" endWordPosition="1601">tial example of a synchronous CFG derivation, see Figure 1. Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model. The weight of each rule is: (15) w(X � (γ, α)) = � φi(X —� (γ, α))λi i where the φi are features defined on rules. For our experiments we used the following features, analogous to Pharaoh’s default feature set: • P(γ |α) and P(α |γ), the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature (Och and Ney, 2002); • the lexical weights Pw(γ |α) and Pw(α |γ) (Koehn et al., 2003), which estimate how well the words in α translate the words in γ;2 • a phrase penalty exp(1), which allows the model to learn a preference for longer or shorter derivations, analogous to Koehn’s phrase penalty (Koehn, 2003). The exceptions to the above are the two glue rules, (13), which has weight one, and (14), which has weight (16) w(S _� (S 1 X 2 , S 1 X 2 )) = exp(—λg) the idea being that λg controls the model’s preference for hierarchical phrases over serial combination of phrases. Let D be a derivation of the grammar, and let f(D) and e(D) be the French and English strings generated by</context>
<context position="12384" citStr="Koehn et al. (2003)" startWordPosition="2174" endWordPosition="2177">y cleaner (and necessary for polynomial-time decoding) to integrate them into the rule weights, so that the whole model is a weighted synchronous CFG. The word penalty is easy; the language model is integrated by intersecting the English-side CFG with the language model, which is a weighted finitestate automaton. 3 Training The training process begins with a word-aligned corpus: a set of triples hf, e, ∼i, where f is a French sentence, e is an English sentence, and ∼ is a (manyto-many) binary relation between positions of f and positions of e. We obtain the word alignments using the method of Koehn et al. (2003), which is based on that of Och and Ney (2004). This involves running GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applying refinement rules (the variant they designate “final-and”) to obtain a single many-to-many word alignment for each sentence. Then, following Och and others, we use heuristics to hypothesize a distribution of possible derivations of each training example, and then estimate the phrase translation parameters from the hypothesized distribution. To do this, we first identify initial phrase pairs using the same criterion as previous systems (Och and Ney, 2004</context>
<context position="18389" citStr="Koehn et al., 2003" startWordPosition="3235" endWordPosition="3238">uage, with C++ code from the SRI Language Modeling Toolkit (Stolcke, 2002). Using the settings described above, on a 2.4 GHz Pentium IV, it takes about 20 seconds to translate each sentence (average length about 30). This is faster than our 267 Python implementation of a standard phrase-based decoder, so we expect that a future optimized implementation of the hierarchical decoder will run at a speed competitive with other phrase-based systems. 5 Experiments Our experiments were on Mandarin-to-English translation. We compared a baseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system. For all three systems we trained the translation model on the FBIS corpus (7.2M+9.2M words); for the language model, we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as our development set, and the 2003 test set as our test set. Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its defa</context>
<context position="23020" citStr="Koehn et al. (2003)" startWordPosition="4079" endWordPosition="4082"> Ac Pharaoh 0.19 0.095 0.030 0.14 0.029 −0.20 0.22 0.11 — — hierarchical 0.15 0.036 0.074 0.037 0.076 −0.32 0.22 — 0.09 — +constituent 0.11 0.026 0.062 0.025 0.029 −0.23 0.21 — 0.11 0.20 Table 2: Feature weights obtained by minimum-error-rate training (normalized so that absolute values sum to one). Word = word penalty; Phr = phrase penalty. Note that we have inverted the sense of Pharaoh’s phrase penalty so that a positive weight indicates a penalty. 5.3 Adding a constituent feature The use of hierarchical structures opens the possibility of making the model sensitive to syntactic structure. Koehn et al. (2003) mention German (es gibt, there is) as an example of a good phrase pair which is not a syntactic phrase pair, and report that favoring syntactic phrases does not improve accuracy. But in our model, the rule (19) X -� (es gibt X 1 , there is X 1 ) would indeed respect syntactic phrases, because it builds a pair of Ss out of a pair of NPs. Thus, favoring subtrees in our model that are syntactic phrases might provide a fairer way of testing the hypothesis that syntactic phrases are better phrases. This feature adds a factor to (17), { 1 if fij is a constituent 0 otherwise as determined by a stati</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT-NAACL 2003, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Noun Phrase Translation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<contexts>
<context position="9840" citStr="Koehn, 2003" startWordPosition="1638" endWordPosition="1639"> = � φi(X —� (γ, α))λi i where the φi are features defined on rules. For our experiments we used the following features, analogous to Pharaoh’s default feature set: • P(γ |α) and P(α |γ), the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature (Och and Ney, 2002); • the lexical weights Pw(γ |α) and Pw(α |γ) (Koehn et al., 2003), which estimate how well the words in α translate the words in γ;2 • a phrase penalty exp(1), which allows the model to learn a preference for longer or shorter derivations, analogous to Koehn’s phrase penalty (Koehn, 2003). The exceptions to the above are the two glue rules, (13), which has weight one, and (14), which has weight (16) w(S _� (S 1 X 2 , S 1 X 2 )) = exp(—λg) the idea being that λg controls the model’s preference for hierarchical phrases over serial combination of phrases. Let D be a derivation of the grammar, and let f(D) and e(D) be the French and English strings generated by D. Let us represent D as a set of triples (r, i, j), each of which stands for an application of a grammar rule r to rewrite a nonterminal that spans f(D)ji on the French side.3 Then the weight of D 2This feature uses word a</context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>Philipp Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="3721" citStr="Koehn, 2004" startWordPosition="602" endWordPosition="603">roceedings of the 43rd Annual Meeting of the ACL, pages 263–270, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) j,5#H Aozhou Australia � .9 ®* 2- de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit. ‘of-one,’ as a single token, then translating this sentence correctly into English requires reversing a sequence of five elements. When we run a phrase-based system, Pharaoh (Koehn et al., 2003; Koehn, 2004a), on this sentence (using the experimental setup described below), we get the following phrases with translations: (4) [Aozhou] [shi] [yu] [Bei Han] [you] [bangjiao]1 [de shaoshu guojia zhiyi] [Australia] [is] [dipl. rels.]1 [with] [North Korea] [is] [one of the few countries] where we have used subscripts to indicate the reordering of phrases. The phrase-based model is able to order “diplomatic...Korea” correctly (using phrase reordering) and “one...countries” correctly (using a phrase translation), but does not accomplish the necessary inversion of those two groups. A lexicalized phrase-re</context>
<context position="18402" citStr="Koehn, 2004" startWordPosition="3239" endWordPosition="3240">from the SRI Language Modeling Toolkit (Stolcke, 2002). Using the settings described above, on a 2.4 GHz Pentium IV, it takes about 20 seconds to translate each sentence (average length about 30). This is faster than our 267 Python implementation of a standard phrase-based decoder, so we expect that a future optimized implementation of the hierarchical decoder will run at a speed competitive with other phrase-based systems. 5 Experiments Our experiments were on Mandarin-to-English translation. We compared a baseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system. For all three systems we trained the translation model on the FBIS corpus (7.2M+9.2M words); for the language model, we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as our development set, and the 2003 test set as our test set. Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its default settings,</context>
<context position="21902" citStr="Koehn, 2004" startWordPosition="3893" endWordPosition="3894">ested our system, using the settings described above.4 Our system achieves an absolute improvement of 0.02 over the baseline (7.5% relative), without using any additional training data. This difference is statistically significant (p &lt; 0.01).5 See Table 1, which also shows that the relative gain is higher for higher n-grams. 4Note that we gave Pharaoh wider beam settings than we used on our own decoder; on the other hand, since our decoder’s chart has more cells, its b limits do not need to be as high. 5We used Zhang’s significance tester (Zhang et al., 2004), which uses bootstrap resampling (Koehn, 2004b); it was modified to conform to NIST’s current definition of the BLEU brevity penalty. 268 BLEU-n n-gram precisions System 4 1 2 3 4 5 6 7 8 Pharaoh 0.2676 0.72 0.37 0.19 0.10 0.052 0.027 0.014 0.0075 hierarchical 0.2877 0.74 0.39 0.21 0.11 0.060 0.032 0.017 0.0084 +constituent 0.2881 0.73 0.39 0.21 0.11 0.062 0.032 0.017 0.0088 Table 1: Results on baseline system and hierarchical system, with and without constituent feature. Features System Plm(e) P(y|α) P(α|y) P.(y|α) P.(α|y) Word Phr Ad Ag Ac Pharaoh 0.19 0.095 0.030 0.14 0.029 −0.20 0.22 0.11 — — hierarchical 0.15 0.036 0.074 0.037 0.076</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004a. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<contexts>
<context position="3721" citStr="Koehn, 2004" startWordPosition="602" endWordPosition="603">roceedings of the 43rd Annual Meeting of the ACL, pages 263–270, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) j,5#H Aozhou Australia � .9 ®* 2- de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit. ‘of-one,’ as a single token, then translating this sentence correctly into English requires reversing a sequence of five elements. When we run a phrase-based system, Pharaoh (Koehn et al., 2003; Koehn, 2004a), on this sentence (using the experimental setup described below), we get the following phrases with translations: (4) [Aozhou] [shi] [yu] [Bei Han] [you] [bangjiao]1 [de shaoshu guojia zhiyi] [Australia] [is] [dipl. rels.]1 [with] [North Korea] [is] [one of the few countries] where we have used subscripts to indicate the reordering of phrases. The phrase-based model is able to order “diplomatic...Korea” correctly (using phrase reordering) and “one...countries” correctly (using a phrase translation), but does not accomplish the necessary inversion of those two groups. A lexicalized phrase-re</context>
<context position="18402" citStr="Koehn, 2004" startWordPosition="3239" endWordPosition="3240">from the SRI Language Modeling Toolkit (Stolcke, 2002). Using the settings described above, on a 2.4 GHz Pentium IV, it takes about 20 seconds to translate each sentence (average length about 30). This is faster than our 267 Python implementation of a standard phrase-based decoder, so we expect that a future optimized implementation of the hierarchical decoder will run at a speed competitive with other phrase-based systems. 5 Experiments Our experiments were on Mandarin-to-English translation. We compared a baseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system. For all three systems we trained the translation model on the FBIS corpus (7.2M+9.2M words); for the language model, we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as our development set, and the 2003 test set as our test set. Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its default settings,</context>
<context position="21902" citStr="Koehn, 2004" startWordPosition="3893" endWordPosition="3894">ested our system, using the settings described above.4 Our system achieves an absolute improvement of 0.02 over the baseline (7.5% relative), without using any additional training data. This difference is statistically significant (p &lt; 0.01).5 See Table 1, which also shows that the relative gain is higher for higher n-grams. 4Note that we gave Pharaoh wider beam settings than we used on our own decoder; on the other hand, since our decoder’s chart has more cells, its b limits do not need to be as high. 5We used Zhang’s significance tester (Zhang et al., 2004), which uses bootstrap resampling (Koehn, 2004b); it was modified to conform to NIST’s current definition of the BLEU brevity penalty. 268 BLEU-n n-gram precisions System 4 1 2 3 4 5 6 7 8 Pharaoh 0.2676 0.72 0.37 0.19 0.10 0.052 0.027 0.014 0.0075 hierarchical 0.2877 0.74 0.39 0.21 0.11 0.060 0.032 0.017 0.0084 +constituent 0.2881 0.73 0.39 0.21 0.11 0.062 0.032 0.017 0.0088 Table 1: Results on baseline system and hierarchical system, with and without constituent feature. Features System Plm(e) P(y|α) P(α|y) P.(y|α) P.(α|y) Word Phr Ad Ag Ac Pharaoh 0.19 0.095 0.030 0.14 0.029 −0.20 0.22 0.11 — — hierarchical 0.15 0.036 0.074 0.037 0.076</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004b. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>A weighted finite state transducer translation template model for statistical machine translation. Natural Language Engineering.</title>
<date>2005</date>
<note>To appear.</note>
<contexts>
<context position="3001" citStr="Kumar et al., 2005" startWordPosition="484" endWordPosition="487"> segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005). But it is often desirable to capture translations whose scope is larger than a few consecutive words. 263 Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) j,5#H Aozhou Australia � .9 ®* 2- de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit. ‘of-one,’ as a single token, then translating this sentence correctly into English req</context>
</contexts>
<marker>Kumar, Deng, Byrne, 2005</marker>
<rawString>Shankar Kumar, Yonggang Deng, and William Byrne. 2005. A weighted finite state transducer translation template model for statistical machine translation. Natural Language Engineering. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrasebased, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>133--139</pages>
<contexts>
<context position="2263" citStr="Marcu and Wong, 2002" startWordPosition="364" endWordPosition="367">ands for the substring of f from position i to position j inclusive, and similarly for eji . English sentence e is modeled as: (1) arg max P(e |f) = arg max P(e, f) e e (2) = arg max(P(e) x P(f |e)) e The translation model P(f |e) “encodes” e into f by the following steps: 1. segment e into phrases ¯e1 ¯eI, typically with a uniform distribution over segmentations; 2. reorder the ¯ei according to some distortion model; 3. translate each of the ¯ei into French phrases according to a model P(f¯ |¯e) estimated from the training data. Other phrase-based models model the joint distribution P(e, f) (Marcu and Wong, 2002) or made P(e) and P(f |e) into features of a log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reord</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrasebased, joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="12480" citStr="Och and Ney, 2000" startWordPosition="2193" endWordPosition="2196">o that the whole model is a weighted synchronous CFG. The word penalty is easy; the language model is integrated by intersecting the English-side CFG with the language model, which is a weighted finitestate automaton. 3 Training The training process begins with a word-aligned corpus: a set of triples hf, e, ∼i, where f is a French sentence, e is an English sentence, and ∼ is a (manyto-many) binary relation between positions of f and positions of e. We obtain the word alignments using the method of Koehn et al. (2003), which is based on that of Och and Ney (2004). This involves running GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applying refinement rules (the variant they designate “final-and”) to obtain a single many-to-many word alignment for each sentence. Then, following Och and others, we use heuristics to hypothesize a distribution of possible derivations of each training example, and then estimate the phrase translation parameters from the hypothesized distribution. To do this, we first identify initial phrase pairs using the same criterion as previous systems (Och and Ney, 2004; Koehn et al., 2003): Definition 1. Given a word-aligned sentence pair hf, e, ∼i, a rule hfij, </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the ACL, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="2344" citStr="Och and Ney, 2002" startWordPosition="380" endWordPosition="383">or eji . English sentence e is modeled as: (1) arg max P(e |f) = arg max P(e, f) e e (2) = arg max(P(e) x P(f |e)) e The translation model P(f |e) “encodes” e into f by the following steps: 1. segment e into phrases ¯e1 ¯eI, typically with a uniform distribution over segmentations; 2. reorder the ¯ei according to some distortion model; 3. translate each of the ¯ei into French phrases according to a model P(f¯ |¯e) estimated from the training data. Other phrase-based models model the joint distribution P(e, f) (Marcu and Wong, 2002) or made P(e) and P(f |e) into features of a log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003</context>
<context position="9085" citStr="Och and Ney (2002)" startWordPosition="1499" endWordPosition="1502">have used boxed indices to indicate which occurrences of X are linked by —. Note that we have used only a single nonterminal symbol X instead of assigning syntactic categories to phrases. In the grammar we extract from a bitext (described below), all of our rules use only X, except for two special “glue” rules, which combine a sequence of Xs to form an S: These give the model the option to build only partial translations using hierarchical phrases, and then combine them serially as in a standard phrase-based model. For a partial example of a synchronous CFG derivation, see Figure 1. Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model. The weight of each rule is: (15) w(X � (γ, α)) = � φi(X —� (γ, α))λi i where the φi are features defined on rules. For our experiments we used the following features, analogous to Pharaoh’s default feature set: • P(γ |α) and P(α |γ), the latter of which is not found in the noisy-channel model, but has been previously found to be a helpful feature (Och and Ney, 2002); • the lexical weights Pw(γ |α) and Pw(α |γ) (Koehn et al., 2003), which estimate how well the words in α translate the words in γ;2 •</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="830" citStr="Och and Ney, 2004" startWordPosition="117" endWordPosition="120">act We present a statistical phrase-based translation model that uses hierarchical phrases— phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntaxbased translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. 1 Introduction The alignment template translation model (Och and Ney, 2004) and related phrase-based models advanced the previous state of the art by moving from words to phrases as the basic unit of translation. Phrases, which can be any substring and not necessarily phrases in any syntactic theory, allow these models to learn local reorderings, translation of short idioms, or insertions and deletions that are sensitive to local context. They are thus a simple and powerful mechanism for machine translation. The basic phrase-based model is an instance of the noisy-channel approach (Brown et al., 1993),1 in which the translation of a French sentence f into an 1Through</context>
<context position="2924" citStr="Och and Ney, 2004" startWordPosition="468" endWordPosition="471"> log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005). But it is often desirable to capture translations whose scope is larger than a few consecutive words. 263 Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) j,5#H Aozhou Australia � .9 ®* 2- de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit. ‘of-one,’</context>
<context position="12430" citStr="Och and Ney (2004)" startWordPosition="2184" endWordPosition="2187">coding) to integrate them into the rule weights, so that the whole model is a weighted synchronous CFG. The word penalty is easy; the language model is integrated by intersecting the English-side CFG with the language model, which is a weighted finitestate automaton. 3 Training The training process begins with a word-aligned corpus: a set of triples hf, e, ∼i, where f is a French sentence, e is an English sentence, and ∼ is a (manyto-many) binary relation between positions of f and positions of e. We obtain the word alignments using the method of Koehn et al. (2003), which is based on that of Och and Ney (2004). This involves running GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applying refinement rules (the variant they designate “final-and”) to obtain a single many-to-many word alignment for each sentence. Then, following Och and others, we use heuristics to hypothesize a distribution of possible derivations of each training example, and then estimate the phrase translation parameters from the hypothesized distribution. To do this, we first identify initial phrase pairs using the same criterion as previous systems (Och and Ney, 2004; Koehn et al., 2003): Definition 1. Given a w</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Ignacio Thayer</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
<author>Dragos Stefan Munteanu</author>
<author>Quamrul Tipu</author>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
</authors>
<date>2004</date>
<booktitle>Arabic and Chinese MT at USC/ISI. Presentation given at NIST Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="4387" citStr="Och et al., 2004" startWordPosition="700" endWordPosition="703">p described below), we get the following phrases with translations: (4) [Aozhou] [shi] [yu] [Bei Han] [you] [bangjiao]1 [de shaoshu guojia zhiyi] [Australia] [is] [dipl. rels.]1 [with] [North Korea] [is] [one of the few countries] where we have used subscripts to indicate the reordering of phrases. The phrase-based model is able to order “diplomatic...Korea” correctly (using phrase reordering) and “one...countries” correctly (using a phrase translation), but does not accomplish the necessary inversion of those two groups. A lexicalized phrase-reordering model like that in use in ISI’s system (Och et al., 2004) might be able to learn a better reordering, but simpler distortion models will probably not. We propose a solution to these problems that does not interfere with the strengths of the phrasebased approach, but rather capitalizes on them: since phrases are good for learning reorderings of words, we can use them to learn reorderings of phrases as well. In order to do this we need hierarchical phrases that consist of both words and subphrases. For example, a hierarchical phrase pair that might help with the above example is: (5) (yu 1 you 2 , have 2 with 1 ) where 1 and 2 are placeholders for sub</context>
</contexts>
<marker>Och, Thayer, Marcu, Knight, Munteanu, Tipu, Galley, Hopkins, 2004</marker>
<rawString>Franz Josef Och, Ignacio Thayer, Daniel Marcu, Kevin Knight, Dragos Stefan Munteanu, Quamrul Tipu, Michel Galley, and Mark Hopkins. 2004. Arabic and Chinese MT at USC/ISI. Presentation given at NIST Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="19678" citStr="Och, 2003" startWordPosition="3455" endWordPosition="3456">= 4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty. The results of the experiments are summarized in Table 1. 5.1 Baseline The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004a), as publicly distributed. We used the default feature set: language model (same as above), p(f¯ |¯e), p(¯e |f¯), lexical weighting (both directions), distortion model, word penalty, and phrase penalty. We ran the trainer with its default settings (maximum phrase length 7), and then used Koehn’s implementation of minimumerror-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on our development set, yielding the values shown in Table 2. Finally, we ran the decoder on the test set, pruning the phrase table with b = 100, pruning the chart with b = 100,/3 = 10−5, and limiting distortions to 4. These are the default settings, except for the phrase table’s b, which was raised from 20, and the distortion limit. Both of these changes, made by Koehn’s minimum-error-rate trainer by default, improve performance on the development set. Rank Chinese English 1 . 3  the 14 ;rE in 23  ’s 577 X 1  X 2 the X 2 of X</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="18927" citStr="Papineni et al., 2002" startWordPosition="3329" endWordPosition="3333">aseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system. For all three systems we trained the translation model on the FBIS corpus (7.2M+9.2M words); for the language model, we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as our development set, and the 2003 test set as our test set. Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its default settings, which is to perform case-insensitive matching of n-grams up to n = 4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty. The results of the experiments are summarized in Table 1. 5.1 Baseline The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004a), as publicly distributed. We used the default feature set: language model (same as above), p(f¯ |¯e), p(¯e |f¯), lexical weighting (both directions), distortion model, word penalty, and phrase penalty.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="17845" citStr="Stolcke, 2002" startWordPosition="3155" endWordPosition="3156">any item that would be generated using a lowerscoring rule or a lower-scoring antecedent item is also assumed to fall outside the beam. This heuristic greatly increases decoding speed, at the cost of some search errors. Finally, the decoder has a constraint that prohibits any X from spanning a substring longer than 10 on the French side, corresponding to the maximum length constraint on initial rules during training. This makes the decoding algorithm asymptotically linear-time. The decoder is implemented in Python, an interpreted language, with C++ code from the SRI Language Modeling Toolkit (Stolcke, 2002). Using the settings described above, on a 2.4 GHz Pentium IV, it takes about 20 seconds to translate each sentence (average length about 30). This is faster than our 267 Python implementation of a standard phrase-based decoder, so we expect that a future optimized implementation of the hierarchical decoder will run at a speed competitive with other phrase-based systems. 5 Experiments Our experiments were on Mandarin-to-English translation. We compared a baseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system. For all three syste</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="7205" citStr="Wu, 1997" startWordPosition="1179" endWordPosition="1180">es synchronous CFG, linguistically because the structures it is defined over are (on the English side) informed by syntactic theory (via the Penn Treebank). Our system is formally syntaxbased in that it uses synchronous CFG, but not necessarily linguistically syntax-based, because it induces a grammar from a parallel text without relying on any linguistic annotations or assumptions; the result sometimes resembles a syntactician’s grammar but often does not. In this respect it resembles Wu’s IL Bei North 0 Han Korea 9x bangjiao dipl. rels. � you have A shi is 4 yu with 264 bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrasebased philosophy. Our extraction method is basically the same as that of Block (2000), except we allow more than one nonterminal symbol in a rule, and use a more sophisticated probability model. In this paper we describe the design and implementation of our hierarchical phrase-based model, and report on experiments that demonstrate that hierarchical phrases indeed improve translation. 2 The model Our model is based on a weighted synchronous CFG (Aho and Ullman, 1969). In a </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="6523" citStr="Yamada and Knight (2001)" startWordPosition="1064" endWordPosition="1067">]]] The system we describe below uses rules like this, and in fact is able to learn them automatically from a bitext without syntactic annotation. It translates the above example almost exactly as we have shown, the only error being that it omits the word ‘that’ from (6) and therefore (8). These hierarchical phrase pairs are formally productions of a synchronous context-free grammar (defined below). A move to synchronous CFG can be seen as a move towards syntax-based MT; however, we make a distinction here between formally syntax-based and linguistically syntax-based MT. A system like that of Yamada and Knight (2001) is both formally and linguistically syntax-based: formally because it uses synchronous CFG, linguistically because the structures it is defined over are (on the English side) informed by syntactic theory (via the Penn Treebank). Our system is formally syntaxbased in that it uses synchronous CFG, but not necessarily linguistically syntax-based, because it induces a grammar from a parallel text without relying on any linguistic annotations or assumptions; the result sometimes resembles a syntactician’s grammar but often does not. In this respect it resembles Wu’s IL Bei North 0 Han Korea 9x ban</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the ACL, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL 2004,</booktitle>
<pages>257--264</pages>
<contexts>
<context position="2980" citStr="Zens and Ney, 2004" startWordPosition="480" endWordPosition="483">chitecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005). But it is often desirable to capture translations whose scope is larger than a few consecutive words. 263 Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) j,5#H Aozhou Australia � .9 ®* 2- de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit. ‘of-one,’ as a single token, then translating this sentence corre</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In Proceedings ofHLT-NAACL 2004, pages 257–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>pages</pages>
<contexts>
<context position="21856" citStr="Zhang et al., 2004" startWordPosition="3885" endWordPosition="3888">rases (e.g., a better language model score). We then tested our system, using the settings described above.4 Our system achieves an absolute improvement of 0.02 over the baseline (7.5% relative), without using any additional training data. This difference is statistically significant (p &lt; 0.01).5 See Table 1, which also shows that the relative gain is higher for higher n-grams. 4Note that we gave Pharaoh wider beam settings than we used on our own decoder; on the other hand, since our decoder’s chart has more cells, its b limits do not need to be as high. 5We used Zhang’s significance tester (Zhang et al., 2004), which uses bootstrap resampling (Koehn, 2004b); it was modified to conform to NIST’s current definition of the BLEU brevity penalty. 268 BLEU-n n-gram precisions System 4 1 2 3 4 5 6 7 8 Pharaoh 0.2676 0.72 0.37 0.19 0.10 0.052 0.027 0.014 0.0075 hierarchical 0.2877 0.74 0.39 0.21 0.11 0.060 0.032 0.017 0.0084 +constituent 0.2881 0.73 0.39 0.21 0.11 0.062 0.032 0.017 0.0088 Table 1: Results on baseline system and hierarchical system, with and without constituent feature. Features System Plm(e) P(y|α) P(α|y) P.(y|α) P.(α|y) Word Phr Ad Ag Ac Pharaoh 0.19 0.095 0.030 0.14 0.029 −0.20 0.22 0.11</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), pages 2051– 2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>