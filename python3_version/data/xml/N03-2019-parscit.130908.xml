<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003319">
<title confidence="0.985135">
Inferring Temporal Ordering of Events in News
</title>
<author confidence="0.672083">
Inderjeet Mani
</author>
<note confidence="0.742529333333333">
The MITRE Corporation
7515 Colshire Drive
McLean, VA 22102
</note>
<email confidence="0.983821">
imani@mitre.org
</email>
<author confidence="0.996248">
Barry Schiffman
</author>
<affiliation confidence="0.997663">
Columbia University
</affiliation>
<address confidence="0.9218105">
1214 Amsterdam Avenue
New York, NY 10027
</address>
<email confidence="0.986841">
bschiff@cs.columbia.
</email>
<note confidence="0.6906044">
edu
Jianping Zhang
The MITRE Corporation
7515 Colshire Drive
McLean, VA 22102
</note>
<email confidence="0.990607">
jzhang@mitre.org
</email>
<sectionHeader confidence="0.995493" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901833333333">
This paper describes a domain-independent,
machine-learning based approach to tempo-
rally anchoring and ordering events in news.
The approach achieves 84.6% accuracy in
temporally anchoring events and 75.4% accu-
racy in partially ordering them.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999202785714285">
Practical NLP applications such as text summariza-
tion and question-answering place increasing demands
on the processing of temporal information. In multi-
document summarization of news, it is important to
know the relative order of events so as to correctly
merge and present information. In question-answering,
one would like to be able to ask when an event occurs,
or what events occurred prior to a particular event. Such
capabilities presuppose an ability to infer the temporal
order of events in discourse.
A number of different knowledge sources appear to
be involved in inferring event ordering (Lascarides and
Asher 1993), including tense and aspect (1), temporal
adverbials (2), and world knowledge (3).
</bodyText>
<listItem confidence="0.988208714285714">
(1) Max entered the room. He had drunk/was drink-
ing the wine.
(2) A drunken man died in the central Phillipines
when he put a firecracker under his armpit.
(3) U. N. Secretary- General Boutros Boutros-Ghali
Sunday opened a meeting of ....Boutros-Ghali ar-
rived in Nairobi from South Africa, ...
</listItem>
<bodyText confidence="0.99979352631579">
As (Bell 1999) has pointed out, the temporal struc-
ture of news is dictated by perceived news value rather
than chronology. Thus, the latest news is often pre-
sented first, instead of events being described in order of
occurrence (the latter ordering is called the narrative
convention).
This paper describes a domain-independent ap-
proach to temporally anchoring and ordering events in
news. The approach is motivated by a pilot experiment
with 8 subjects providing news event-ordering judg-
ments which revealed that the narrative convention ap-
plied only 47% of the time in ordering the events in
successive past-tense clauses. Our approach involves
mixed-initiative corpus annotation, with automatic tag-
ging to identify clause structure, tense, aspect, and tem-
poral adverbials, as well as tagging of reference times
and anchoring of events with respect to reference times.
We report on machine learning results from event-time
anchoring judgments.
</bodyText>
<sectionHeader confidence="0.980501" genericHeader="method">
2 Linguistic Processing
</sectionHeader>
<bodyText confidence="0.99912853125">
The time expression tagger TempEx (Mani and Wil-
son 2000) tags and assigns values to temporal expres-
sions, both “absolute” expressions like “June 1, 2001”
and relative expressions like “Monday”. It was cited in
(Mani and Wilson 2000) as achieving a .83 F-measure
against hand-annotated data. Inter-annotator reliability
across 5 annotators on 193 TDT2-documents was .79F
for extent and .86F for time values, with TempEx scor-
ing .76F (extent) and .82F (value) on these documents.
The clause tagger (CLAUSE-IT) identifies top-level
clauses (C), top-level clauses with gapped subjects
(GC), e.g., “&lt;C&gt;He returned the book&lt;/C&gt; &lt;GC&gt;and
went home&lt;/GC&gt;”, relative clauses (RC), and comple-
ment clauses (CO), which include all non-finite clauses.
Our pilot experiment also revealed that the propor-
tion of clauses with explicit time expressions (TIMEX2)
is approximately 25%, suggesting that anchoring the
events to just the explicit times wouldn’t be sufficient.
The system accordingly computes a reference time
(Reichenbach 1947) value (tval) for each clause, de-
fined to be either the time value of an explicit temporal
expression mentioned in the clause, or, when the ex-
plicit time expression is absent, an implicit time value
inferred from context.
To generate this tval feature, the simple algorithm
in Figure 1 was used. The system also anchors the
event’s time with respect to the tval (at, before, or after)
when the tval is an explicit reference time. This feature
is called anchor-explicit. All in all, the features shown
in Table 1 were computed for each clause.
Set initial tval to document-creation-date.
For each clause:
</bodyText>
<listItem confidence="0.9925025">
1. If clause has explicit time, then set its tval to it.
2. If clause-type is relative clause, assume its tval
is inaccessible to later discourse.
3. If clause verb is of type reporting verb, set tval
to document-creation-date.
4. If clause is inside quotes inherit tval from em-
bedding clause.
5. Otherwise, pick most recent tval.
</listItem>
<figureCaption confidence="0.726105">
Figure 1: Algorithm for Computing
Reference Time (tval)
</figureCaption>
<table confidence="0.999604214285714">
CTYPE: clause is a regular clause,
complement clause, or relative clause
CINDEX: subclause index
PARA: paragraph number
SENT: sentence number
SCONJ: subordinating conjunction
(e.g., while, since, before)
TPREP: preposition in a TIMEX2 PP
TIMEX2: string in the TIMEX2 tag
TMOD: temporal modifier not at-
tached to a TIMEX2, (e.g., after [an
altercation])
QUOTE: number of words in quotes
REPVERB-P: reporting verb in clause
STATIVE-P: stative verb in clause
ACCOMP-P: accomplishment verb
ASPECTSHIFT: shift in aspect from
previous clause
G-ASPECT: grammatical aspect
{progessive, perfect,nil}
TENSE: tense of clause {past, pre-
sent, future, nil}
TENSESHIFT: shift in tense from
previous clause
ANCHOR_EXPLICIT: {&lt;, &gt;, =, un-
def}
TVAL: reference time for clause, i.e.,
a time value
</table>
<tableCaption confidence="0.99973">
Table 1: Linguistic Features for each Clause1,2
</tableCaption>
<footnote confidence="0.7098185">
1 The statives and accomplishments were computed from
UMaryland’s LCS lexicon, based on (Dorr and Olsen 1997)
</footnote>
<sectionHeader confidence="0.828742" genericHeader="method">
3 Learning Anchoring Rules
</sectionHeader>
<bodyText confidence="0.99991765">
A human unconnected with our project corrected the
tval, based on a set of annotation guidelines, on a sam-
ple of 2069 clauses extracted at random from the North
American News Corpus. She also anchored the event’s
time with respect to the tval (AT, BEF, AFT, or unde-
fined). This feature (not a machine feature) is called
anchors.
The corrections showed that the algorithm in Figure
1 was right on tval for 1231 out of 2069, giving an accu-
racy of 59%. Tracking the sequence of corrected tvals
revealed that the tval of the previous clause was kept
65.75% of the time, that it reverted to some other previ-
ous tval 22.99% of the time, and that it shifted to a new
tval 11.26% of the times. Most of the errors in com-
puted tvals had to do with the tval being assigned erro-
neously the document date rather than reverting to a
non-immediately previous tval. Finally, the anchor-
explicit relation is correct 83.8% of the time; however,
just guessing “at” for the explicit anchor will get an
accuracy of 90.2%.
</bodyText>
<table confidence="0.9840294">
ANCHORS TVAL-
MOVES
MAJORITY (AT) 76.9 (KEEP)
65.75
C5.0 Rules 80.2 (±1.8) 71.8 (±0.5)
</table>
<tableCaption confidence="0.999364">
Table 2: Accuracy of Anchoring Rules
</tableCaption>
<bodyText confidence="0.840521962962963">
We then used this training data to train a statistical
classifier, C5.0 Rules (Quinlan 1997), to learn (1) an-
chors relation rules and (2) rules for tracking the tval
moves (keep, revert, shift) across successive clauses.
The accuracy of anchors rules as well as tval change
rules are shown in Table 2. It can be seen that accuracy
of machine learning here is significantly better than the
majority class. The tval, tense, and tense shift play a
useful role in anchoring, revealing that the tval is a use-
ful abstraction. Here are some of the rules learnt (here te
is the clause index, assumed to stand for the event time
of the clause):
If no sconj and no tmod and no tprep and tval-class
=day then anchors(AT, te,, tval) 80.4% accurate
(156 examples).
If tense is present and no sconj and tval-
class=month then anchors(AT, te,, tval) 77.8 (7).
If tense is present perfect and no sconj, then
See www.umiacs.umd.edu/ ~bonnie/ LCS_ Data-
base_Documentation.html.
2 Since the TIMEX2 and tval values form an open class, they
were automatically grouped into classes based on the granular-
ity of the time expression, namely, {time-of-day, day, week,
month, year, or non-specific}.
anchors(BEF, te,, tval) 83 (4).
If tense shift is present2past and no explicit time and
no sconj, then anchors(AT, te,, tval) 90 (30)
</bodyText>
<sectionHeader confidence="0.919512" genericHeader="method">
4 Partially Ordering Links
</sectionHeader>
<bodyText confidence="0.999830055555555">
Based on the best machine-learned rules for the
anchors relation, anchors tuples are generated for each
document. The tvals in the document’s anchor tuples are
also partially ordered, yielding tuples consisting of or-
dered pairs of tvals. The two sets of tuples are then used
to provide a partial ordering of events in the document,
in the form of links tuples: links(R, ei, ej), where ei and
ej are the events corresponding to clauses i and j, and R
is in {at, bef, aft, or undefined}. One of the authors
evaluated the partial ordering for accuracy, on seven
documents3. The results of this evaluation are shown in
Table 3. #Correct-anchor is the number of the anchors
tuples correctly classified and #total is the total number
of anchors tuples classified. Link Recall is the percent-
age of human generated links tuples (723 in all) that are
correctly identified by machine learned rules. Link Pre-
cision is the percentage of the machine generated links
tuples that are correct.
</bodyText>
<table confidence="0.9957711">
#Cl #Wo #correct- Link Link
aus rds anchor / Recall Precision
es #total-
anchor
40 525 15/18 44/65 53/63
(83.3%) (67.7%) (84.1%)
18 335 12/13 59/59 59/62
(92.3%) (100%) (95.2%)
27 509 17/22 23/40 23/58
(77.2%) (57.5%) (39.7%)
38 617 21/27 94/172 94/190
(77.8%) (54.7%) (49.5%)
22 296 11/12 39/42 39/49
(91.7%) (92.9%) (79.6%)
14 242 6/7 6/6 6/7
(85.7%) (100%) (85.7%)
35 447 28/31 297/339 289/335
(90.3%) (87.6%) (86.3%)
194 2971 110/130 562/723 563/764
(84.6%) (77.7%) (73.7%)
</table>
<tableCaption confidence="0.909787">
Table 3: Document-Level Accuracy
of Learnt Rules
</tableCaption>
<sectionHeader confidence="0.978244" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<footnote confidence="0.561815">
3 Note that the naïve algorithm for tval is only 59% correct.
While improvements to the naïve algorithm are clearly possi-
ble based on the corrected tval, to adequately test the machine
learnt rules we use the corrected tval.
</footnote>
<bodyText confidence="0.9998875625">
Overall, our approach achieves 84.6% accuracy in
anchoring events and 75.4% F-measure in partially or-
dering them. These numbers compare favorably with the
previous literature: (Filatova and Hovy 2001) obtained
82% accuracy on anchoring for a single type of
event/topic on 172 clauses, while (Mani and Wilson
2000) obtained accuracy of 59.4% on anchoring over
663 verb contexts. Our approach is also distinct in its
use of human experimentation, machine learning and
the variety of linguistically motivated features (includ-
ing temporal adverbials) that are brought to bear.
Future work will examine the role of aspectual fea-
tures, learning from skewed distributions dominated by
AT (an overwhelming majority of news events occur at
the reference times), and the incorporation of unsuper-
vised learning methods.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997156">
A. Bell. News Stories as Narratives. In A. Jaworski
and N. Coupland, The Discourse Reader, Routledge,
1999, 236-251.
E. Filatova, and E. Hovy. Assigning Time-Stamps to
Event-Clauses. Workshop on Temporal and Spatial In-
formation Processing, ACL’2001, Toulouse, 88-95.
A. Lascarides and N. Asher. Temporal Relations,
Discourse Structure, and Commonsense Entailment.
1993. Linguistics and Philosophy 16, 437-494.
I. Mani and G. Wilson. Robust Temporal Processing
of News. ACL&apos;2000, 69-76.
R. Quinlan. 1997. C5.0. www.rulequest.com.
H. Reichenbach. The tenses of verbs. In H. Reichen-
bach, Elements of Symbolic Logic. Macmillan, 1947,
Section 51, 287-298.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.029101">
<title confidence="0.998493">Inferring Temporal Ordering of Events in News</title>
<author confidence="0.465499">Inderjeet</author>
<affiliation confidence="0.649581">The MITRE</affiliation>
<address confidence="0.944284">7515 Colshire McLean, VA 22102</address>
<email confidence="0.975043">imani@mitre.org</email>
<author confidence="0.718361">Barry</author>
<affiliation confidence="0.775398">Columbia</affiliation>
<address confidence="0.999766">1214 Amsterdam New York, NY 10027</address>
<email confidence="0.737548">bschiff@cs.columbia.edu</email>
<affiliation confidence="0.599926">Jianping The MITRE</affiliation>
<address confidence="0.9608155">7515 Colshire McLean, VA 22102</address>
<email confidence="0.994365">jzhang@mitre.org</email>
<abstract confidence="0.939294833333333">This paper describes a domain-independent, machine-learning based approach to temporally anchoring and ordering events in news. The approach achieves 84.6% accuracy in temporally anchoring events and 75.4% accu-</abstract>
<intro confidence="0.607966">in partially ordering</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bell</author>
</authors>
<title>News Stories as Narratives. In</title>
<date>1999</date>
<pages>236--251</pages>
<contexts>
<context position="1616" citStr="Bell 1999" startWordPosition="242" endWordPosition="243">r event. Such capabilities presuppose an ability to infer the temporal order of events in discourse. A number of different knowledge sources appear to be involved in inferring event ordering (Lascarides and Asher 1993), including tense and aspect (1), temporal adverbials (2), and world knowledge (3). (1) Max entered the room. He had drunk/was drinking the wine. (2) A drunken man died in the central Phillipines when he put a firecracker under his armpit. (3) U. N. Secretary- General Boutros Boutros-Ghali Sunday opened a meeting of ....Boutros-Ghali arrived in Nairobi from South Africa, ... As (Bell 1999) has pointed out, the temporal structure of news is dictated by perceived news value rather than chronology. Thus, the latest news is often presented first, instead of events being described in order of occurrence (the latter ordering is called the narrative convention). This paper describes a domain-independent approach to temporally anchoring and ordering events in news. The approach is motivated by a pilot experiment with 8 subjects providing news event-ordering judgments which revealed that the narrative convention applied only 47% of the time in ordering the events in successive past-tens</context>
</contexts>
<marker>Bell, 1999</marker>
<rawString>A. Bell. News Stories as Narratives. In A. Jaworski and N. Coupland, The Discourse Reader, Routledge, 1999, 236-251.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Filatova</author>
<author>E Hovy</author>
</authors>
<title>Assigning Time-Stamps to Event-Clauses.</title>
<booktitle>Workshop on Temporal and Spatial Information Processing, ACL’2001,</booktitle>
<pages>88--95</pages>
<location>Toulouse,</location>
<marker>Filatova, Hovy, </marker>
<rawString>E. Filatova, and E. Hovy. Assigning Time-Stamps to Event-Clauses. Workshop on Temporal and Spatial Information Processing, ACL’2001, Toulouse, 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lascarides</author>
<author>N Asher</author>
</authors>
<title>Temporal Relations, Discourse Structure, and Commonsense Entailment.</title>
<date>1993</date>
<journal>Linguistics and Philosophy</journal>
<volume>16</volume>
<pages>437--494</pages>
<contexts>
<context position="1224" citStr="Lascarides and Asher 1993" startWordPosition="175" endWordPosition="178">actical NLP applications such as text summarization and question-answering place increasing demands on the processing of temporal information. In multidocument summarization of news, it is important to know the relative order of events so as to correctly merge and present information. In question-answering, one would like to be able to ask when an event occurs, or what events occurred prior to a particular event. Such capabilities presuppose an ability to infer the temporal order of events in discourse. A number of different knowledge sources appear to be involved in inferring event ordering (Lascarides and Asher 1993), including tense and aspect (1), temporal adverbials (2), and world knowledge (3). (1) Max entered the room. He had drunk/was drinking the wine. (2) A drunken man died in the central Phillipines when he put a firecracker under his armpit. (3) U. N. Secretary- General Boutros Boutros-Ghali Sunday opened a meeting of ....Boutros-Ghali arrived in Nairobi from South Africa, ... As (Bell 1999) has pointed out, the temporal structure of news is dictated by perceived news value rather than chronology. Thus, the latest news is often presented first, instead of events being described in order of occur</context>
</contexts>
<marker>Lascarides, Asher, 1993</marker>
<rawString>A. Lascarides and N. Asher. Temporal Relations, Discourse Structure, and Commonsense Entailment. 1993. Linguistics and Philosophy 16, 437-494.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Mani</author>
<author>G Wilson</author>
</authors>
<title>Robust Temporal Processing of News.</title>
<volume>2000</volume>
<pages>69--76</pages>
<marker>Mani, Wilson, </marker>
<rawString>I. Mani and G. Wilson. Robust Temporal Processing of News. ACL&apos;2000, 69-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>The tenses of verbs.</title>
<date>1997</date>
<journal>Section</journal>
<booktitle>In H. Reichenbach, Elements of Symbolic Logic. Macmillan,</booktitle>
<volume>51</volume>
<pages>287--298</pages>
<contexts>
<context position="6758" citStr="Quinlan 1997" startWordPosition="1063" endWordPosition="1064">ious tval 22.99% of the time, and that it shifted to a new tval 11.26% of the times. Most of the errors in computed tvals had to do with the tval being assigned erroneously the document date rather than reverting to a non-immediately previous tval. Finally, the anchorexplicit relation is correct 83.8% of the time; however, just guessing “at” for the explicit anchor will get an accuracy of 90.2%. ANCHORS TVALMOVES MAJORITY (AT) 76.9 (KEEP) 65.75 C5.0 Rules 80.2 (±1.8) 71.8 (±0.5) Table 2: Accuracy of Anchoring Rules We then used this training data to train a statistical classifier, C5.0 Rules (Quinlan 1997), to learn (1) anchors relation rules and (2) rules for tracking the tval moves (keep, revert, shift) across successive clauses. The accuracy of anchors rules as well as tval change rules are shown in Table 2. It can be seen that accuracy of machine learning here is significantly better than the majority class. The tval, tense, and tense shift play a useful role in anchoring, revealing that the tval is a useful abstraction. Here are some of the rules learnt (here te is the clause index, assumed to stand for the event time of the clause): If no sconj and no tmod and no tprep and tval-class =day</context>
</contexts>
<marker>Quinlan, 1997</marker>
<rawString>R. Quinlan. 1997. C5.0. www.rulequest.com. H. Reichenbach. The tenses of verbs. In H. Reichenbach, Elements of Symbolic Logic. Macmillan, 1947, Section 51, 287-298.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>