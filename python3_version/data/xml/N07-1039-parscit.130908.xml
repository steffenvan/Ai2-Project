<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000170">
<title confidence="0.907032">
Extracting Appraisal Expressions
</title>
<author confidence="0.990858">
Kenneth Bloom and Navendu Garg and Shlomo Argamon
</author>
<affiliation confidence="0.997199">
Computer Science Department
Illinois Institute of Technology
</affiliation>
<address confidence="0.95845">
10 W. 31st St.
Chicago, IL 60616
</address>
<email confidence="0.999447">
{kbloom1,gargnav,argamon}@iit.edu
</email>
<sectionHeader confidence="0.99481" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901176470588">
Sentiment analysis seeks to characterize
opinionated or evaluative aspects of nat-
ural language text. We suggest here that
appraisal expression extraction should be
viewed as a fundamental task in sentiment
analysis. An appraisal expression is a tex-
tual unit expressing an evaluative stance
towards some target. The task is to find
and characterize the evaluative attributes
of such elements. This paper describes a
system for effectively extracting and dis-
ambiguating adjectival appraisal expres-
sions in English outputting a generic rep-
resentation in terms of their evaluative
function in the text. Data mining on ap-
praisal expressions gives meaningful and
non-obvious insights.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999881333333334">
Sentiment analysis, which seeks to analyze opin-
ion in natural language text, has grown in interest
in recent years. Sentiment analysis includes a vari-
ety of different problems, including: sentiment clas-
sification techniques to classify reviews as positive
or negative, based on bag of words (Pang et al.,
2002) or positive and negative words (Turney, 2002;
Mullen and Collier, 2004); classifying sentences in
a document as either subjective or objective (Riloff
and Wiebe, 2003; Pang and Lee, 2004); identifying
or classifying appraisal targets (Nigam and Hurst,
2004); identifying the source of an opinion in a text
(Choi et al., 2005), whether the author is expressing
the opinion, or whether he is attributing the opinion
to someone else; and developing interactive and vi-
sual opinion mining methods (Gamon et al., 2005;
Popescu and Etzioni, 2005). Much of this work has
utilized the fundamental concept of ‘semantic orien-
tation’, (Turney, 2002); however, sentiment analysis
still lacks a ‘unified field theory’.
We propose in this paper that a fundamental task
underlying many of these formulations is the extrac-
tion and analysis of appraisal expressions, defined
as those structured textual units which express an
evaluation of some object. An appraisal expression
has three main components: an attitude (which takes
an evaluative stance about an object), a target (the
object of the stance), and a source (the person tak-
ing the stance) which may be implied.
The idea of appraisal extraction is a generaliza-
tion of problem formulations developed in earlier
works. Mullen and Collier’s (2004) notion of classi-
fying appraisal terms using a multidimensional set
of attributes is closely tied to the definition of an
appraisal expression, which is classified along sev-
eral dimensions. In previous work (Whitelaw et
al., 2005), we presented a related technique of find-
ing opinion phrases, using a multidimensional set
of attributes and modeling the semantics of mod-
ifiers in these phrases. The use of multiple text
classifiers by Wiebe and colleagues (Wilson et al.,
2005; Wiebe et al., 2004) for various kinds of senti-
ment classification can also be viewed as a sentence-
level technique for analyzing appraisal expressions.
Nigam and Hurst’s (2004) work on detecting opin-
ions about a certain topic presages our notion of
connecting attitudes to targets, while Popescu and
Etzioni’s (2005) opinion mining technique also fits
well into our framework.
In this paper we describe a system for extracting
adjectival appraisal expressions, based on a hand-
built lexicon, a combination of heuristic shallow
parsing and dependency parsing, and expectation-
maximization word sense disambiguation. Each ex-
</bodyText>
<page confidence="0.98167">
308
</page>
<note confidence="0.798113">
Proceedings of NAACL HLT 2007, pages 308–315,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999540666666667">
tracted appraisal expression is represented as a set of
feature values in terms of its evaluative function in
the text. We have applied this system to two domains
of texts: product reviews, and movie reviews. Man-
ual evaluation of the extraction shows our system to
work well, as well as giving some directions for im-
provement. We also show how straightforward data
mining can give users very useful information about
public opinion.
</bodyText>
<sectionHeader confidence="0.99009" genericHeader="method">
2 Appraisal Expressions
</sectionHeader>
<bodyText confidence="0.999978227272727">
We define an appraisal expression to be an elemen-
tary linguistic unit that conveys an attitude of some
kind towards some target. An appraisal expression
is defined to comprise a source, an attitude, and a
target, each represented by various attributes. For
example, in ‘I found the movie quite monotonous’,
the speaker (the Source) expresses a negative Atti-
tude (‘quite monotonous’) towards ‘the movie’ (the
Target). Note that attitudes come in different types;
for example, ‘monotonous’ describes an inherent
quality of the Target, while ‘loathed’ would describe
the emotional reaction of the Source.
Attitude may be expressed through nouns, verbs,
adjectives and metaphors. Extracting all of this in-
formation accurately for all of these types of ap-
praisal expressions is a very difficult problem. We
therefore restrict ourselves for now to adjectival ap-
praisal expressions that are each contained in a sin-
gle sentence. Additionally, we focus here only on
extracting and analyzing the attitude and the target,
but not the source. Even with these restrictions, we
obtain interesting results (Sec. 7).
</bodyText>
<subsectionHeader confidence="0.995369">
2.1 Appraisal attributes
</subsectionHeader>
<bodyText confidence="0.995773333333334">
Our method is grounded in Appraisal Theory, devel-
oped by Martin and White (2005), which analyzes
the way opinion is expressed. Following Martin and
White, we define:
Attitude type is type of appraisal being
expressed—one of affect, appreciation, or
judgment (Figure 1). Affect refers to an
emotional state (e.g., ‘happy’, ‘angry’), and
is the most explicitly subjective type of ap-
praisal. The other two types express evaluation
of external entities, differentiating between
intrinsic appreciation of object properties (e.g.,
‘slender’, ‘ugly’) and social judgment (e.g.,
‘heroic’, ‘idiotic’).
Orientation is whether the attitude is positive
</bodyText>
<table confidence="0.468789166666667">
Attitude Type
Appreciation
Composition
Balance: consistent, discordant,...
Complexity: elaborate, convoluted, ...
Reaction
</table>
<construct confidence="0.986213">
Impact: amazing, compelling, dull,...
Quality: beautiful, elegant, hideous,...
Valuation: innovative, profound, inferior, ...
Affect: happy, joyful, furious, ...
</construct>
<figure confidence="0.37115225">
Judgment
Social Esteem
Capacity: clever, competent, immature,...
Tenacity: brave, hard-working, foolhardy, ...
Normality: famous, lucky, obscure, ...
Social Sanction
Propriety: generous, virtuous, corrupt, ...
Veracity: honest, sincere, sneaky, ...
</figure>
<figureCaption confidence="0.984472">
Figure 1: The Attitude Type taxonomy, with exam-
ples of adjectives from the lexicon.
</figureCaption>
<bodyText confidence="0.992497294117647">
(‘good’) or negative (‘bad’).
Force describes the intensity of the appraisal. Force
is largely expressed via modifiers such as
‘very’ (increased force), or ‘slightly’ (de-
creased force), but may also be expressed lex-
ically, for example ‘greatest’ vs. ‘great’ vs.
‘good’.
Polarity of an appraisal is marked if it is scoped in
a polarity marker (such as ‘not’), or unmarked
otherwise. Other attributes of appraisal are af-
fected by negation; e.g., ‘not good’ also has the
opposite orientation from ‘good’.
Target type is a domain-dependent semantic type
for the target. This attribute takes on values
from a domain-dependent taxonomy, represent-
ing important (and easily extractable) distinc-
tions between targets in the domain.
</bodyText>
<subsectionHeader confidence="0.999295">
2.2 Target taxonomies
</subsectionHeader>
<bodyText confidence="0.999966230769231">
Two domain-dependent target type taxonomies are
shown in Figure 2. In both, the primary distinction
is between a direct naming of a kind of “Thing” or a
deictic/pronominal reference (e.g., “those” or “it”),
since the system does not currently rely on corefer-
ence resolution. References are further divided into
references to the writer/reader (‘interactants’) and to
other people or objects.
The Thing subtrees for the two domains dif-
fer somewhat. In the movie domain, Things such
as ‘this movie’, ‘Nicholas Cage’, or ‘cinematogra-
phy’, are classified into six main categories: movies
(the one being reviewed, or another one), people
</bodyText>
<page confidence="0.999268">
309
</page>
<figureCaption confidence="0.964876">
Figure 2: Target taxonomies for movie and product
reviews.
</figureCaption>
<bodyText confidence="0.999889866666667">
(whether characters, or real people involved in mak-
ing the film), aspects of the movie itself (its plot,
special effects, etc.), the companies involved in mak-
ing it, or aspects of marketing the movie (such
as trailers). For target Things in product reviews,
we replace ‘Movie Person’ and ‘Movie Aspect’ by
‘Product Part’ with two subcategories: ‘Integral’, for
parts of the product itself (e.g., wheels or lenses),
and ‘Replaceable’, for parts or supplies meant to
be periodically replaced (e.g., batteries or ink car-
tridges). The categories of ‘Support’, for references
to aspects of customer support, and ‘Experience’ for
things associated with the experience of using the
product (such as ‘pictures’ or ‘resolution’, were also
added.
</bodyText>
<sectionHeader confidence="0.972873" genericHeader="method">
3 Appraisal Extraction
</sectionHeader>
<bodyText confidence="0.999982">
In our system, appraisal extraction runs in several in-
dependent stages. First, the appraisal extractor finds
appraisal expressions by finding the chunks of text
that express attitudes and targets. Then, it links each
attitude group found to a target in the text. Finally, it
uses a probabilistic model to determine which atti-
tude type should be assigned when attitude chunks
were ambiguous.
</bodyText>
<subsectionHeader confidence="0.998868">
3.1 Chunking
</subsectionHeader>
<bodyText confidence="0.99998">
The chunker is based on our earlier work (Whitelaw
et al., 2005), which finds attitude groups and tar-
gets using a hand-built lexicon (Sec. 4). This lexi-
con contains head adjectives (which specify values
for the attributes attitude type, force, polarity, and
orientation), and appraisal modifiers (which specify
transformations to the four attributes). Some head
adjectives are ambiguous, having multiple entries in
the lexicon with different attribute values. In all
cases, different entries for a given word have dif-
ferent attitude types. If the head adjective is am-
biguous, multiple groups are created, to be disam-
biguated later. See our previous work (Whitelaw et
al., 2005) for a discussion of the technique.
Target groups are found by matching phrases in
the lexicon with corresponding phrases in the text
and assigning the target type listed in the lexicon.
</bodyText>
<subsectionHeader confidence="0.999213">
3.2 Linking
</subsectionHeader>
<bodyText confidence="0.999960454545455">
After finding attitude groups and candidate targets,
the system links each attitude to a target. Each
sentence is parsed to a dependency representation,
and a ranked list of linkage specifications is used
to look for paths in the dependency tree connecting
some word in the source to some word in the target.
Such linkage specifications are hand-constructed,
and manually assigned priorities, so that when two
linkage specifications match, only the highest prior-
ity specification is used. For example, the two high-
est priority linkage specifications are:
</bodyText>
<subsubsectionHeader confidence="0.396068">
nsubj dobj amod
</subsubsectionHeader>
<listItem confidence="0.9865145">
1. target −−−→ x ←−− y ←−−− attitude
2. attitude amod −−−→ target
</listItem>
<bodyText confidence="0.999881882352941">
The first specification selects the subject of a sen-
tence where the appraisal modifies a noun in the
predicate, for example ‘The Matrix’ in ‘The Matrix
is a good movie’. The second selects the noun mod-
ified by an adjective group, for example ‘movie’ in
‘The Matrix is a good movie’.
If no linkage is found connecting an attitude to a
candidate target, the system goes through the link-
age specifications again, trying to find any word in
the sentence connected to the appraisal group by a
known linkage. The selected word is assigned the
generic category of movie thing or product thing (de-
pending on the domain of the text). If no linkage is
found at all, the system assigns the default category
movie thing or product thing, assuming that there is
an appraised thing that couldn’t be found using the
given linkage specifications.
</bodyText>
<figure confidence="0.999160675675676">
Movie Target Type
Movie Thing
Any Movie
This Movie
Other Movie
Movie Person
Real Person...
Character
Movie Aspect...
Company
Marketing
Reference
Interactant
First Person
Second Person
Other
Third Person
Deictic
Product Target Type
Product Thing
Any Product
This Product
Other Product
Product Part
Integral
Replaceable
Experience
Company
Marketing
Support
Reference
Interactant
First Person
Second Person
Other
Third Person
Deictic
</figure>
<page confidence="0.953516">
310
</page>
<subsectionHeader confidence="0.983327">
3.3 Disambiguation
</subsectionHeader>
<bodyText confidence="0.9991044">
After linkages are made, this information is used to
disambiguate multiple senses that may be present
in a given appraisal expression. Most cases are
unambiguous, but in some cases two, or occasion-
ally even three, senses are possible. We bootstrap
from the unambiguous cases, using a probabilistic
model, to resolve the ambiguities. The attitude
type places some grammatical/semantic constraints
on the clause. Two key constraints are the syntactic
relation with the target (which can differentiate af-
fect from the other types of appraisal), and whether
the target type has consciousness (which helps dif-
ferentiate judgment and affect from appreciation).
To capture these constraints, we model the proba-
bility of a given attitude type being correct, given
the target type and the linkage specification used to
connect the attitude to the target, as follows.
The correct attitude type of an appraisal expres-
sion is modeled by a random variable A, the set of
all attitude types in the system is denoted by A, and
a specific attitude type is denoted by a. As described
above, other attributes besides attitude type may
also vary between word senses, but attitude type
always changes between word senses, so when the
system assigns a probability to an attitude type, it
is assigning that probability to the whole word sense.
We denote the linkage type used in a given ap-
praisal expression by L, the set of all possible link-
ages as L, and a specific linkage type by l. Note that
the first attempt with a linkage specification (to find
a chunked target) is considered to be different from
the second attempt with the same linkage specifica-
tion (which attempts to find any word). Failure to
find an applicable linkage rule is considered as yet
another ‘linkage’ for the probability model. Since
our system uses 29 different linkage specifications,
there are a total of 59 different possible linkages
types.
The target type of a given appraisal expression is
denoted by T, the set of all target types by T, and a
specific target type by t. We consider an expression
to have a given target type T = t only if that is its
specific target type; if its target type is a descendant
of t, then its target type is not t in the model. £
denotes the set of all extracted appraisal expressions.
The term exp denotes a specific expression.
Our goal is to estimate, for each appraisal expres-
sion exp in the corpus, the probability of its attitude
type being a, given the expression’s target type t
and linkage type l
</bodyText>
<equation confidence="0.981799">
P(A = a|exp) = P(A = a|T = t, L = l)
</equation>
<bodyText confidence="0.9897668">
To do this, we define a model M of this probability,
and then estimate the maximum likelihood model
using Expectation-Maximization.
We model PM(A = a|T = t, L = l) by first
applying Bayes’ theorem:
</bodyText>
<equation confidence="0.999984">
PM(A = a|T = t,L = l) =
PM(T = t,L = l|A = a)PM(A = a)
PM(T = t,L = l)
</equation>
<bodyText confidence="0.99491">
Assuming conditional independence of target type
and linkage, this becomes:
</bodyText>
<equation confidence="0.999971">
PM(T = t|A = a)PM(L = l|A = a)PM(A = a)
PM(T = t)PM(L = l)
</equation>
<bodyText confidence="0.999651">
M’s parameters thus represent the conditional and
marginal probabilities on this right-hand-side.
Given a set of (possibly ambiguous) appraisal ex-
pressions £ identified by chunking and linkage de-
tection, we seek the maximum likelihood model
</bodyText>
<equation confidence="0.956095">
M* = arg max
M
ri
expE£
</equation>
<bodyText confidence="0.999430666666667">
M* will be our best estimate of P, given the pro-
cessed data in a given corpus. The system esti-
mates M* using an implementation of Expectation-
Maximization over the entire corpus. The highest-
probability attitude type (hence sense) according to
M is then chosen for each appraisal expression.
</bodyText>
<sectionHeader confidence="0.988849" genericHeader="method">
4 The Lexicon
</sectionHeader>
<bodyText confidence="0.999805818181818">
As noted above, attitude groups were identified via a
domain-independent lexicon of appraisal adjectives,
adverbs, and adverb modifiers. 1 For the movie
domain, appraised things were identified based on
a manually constructed lexicon containing generic
movie words, as well as automatically constructed
lexicons of proper names specific to each movie be-
ing reviewed. For each product type considered, we
manually constructed a lexicon containing generic
product words; we did not find it necessary to con-
struct product-specific lexicons.
</bodyText>
<footnote confidence="0.951316666666667">
1All of the lexicons used in the paper can be
found at http://lingcog.iit.edu/arc/
appraisal lexicon 2007a.tar.gz
</footnote>
<equation confidence="0.884563">
ri M(A = a|exp)
aEA
</equation>
<page confidence="0.992474">
311
</page>
<bodyText confidence="0.999945181818182">
For adjectival attitudes, we used the lexicon
developed we developed in our previous work
(Whitelaw et al., 2005) on appraisal. We reviewed
the entire lexicon to determine its accuracy and
made numerous improvements.
Generic target lexicons were constructed by start-
ing with a small sample of the kind of reviews
that the lexicon would apply to. We examined
these manually to find generic words referring to ap-
praised things to serve as seed terms for the lexicon
and used WordNet (Miller, 1995) to suggest addi-
tional terms to add to the lexicon.
Since movie reviews often refer to the specific
contents of the movie under review by proper names
(of actors, the director, etc.), we also automatically
constructed a specific target lexicon for each movie
in the corpus, based on lists of actors, characters,
writers, directors, and companies listed for the film
at imdb.com. Each such specific lexicon was only
used for processing reviews of the movie it was gen-
erated for, so the system had no specific knowledge
of terms related to other movies during processing.
</bodyText>
<sectionHeader confidence="0.996575" genericHeader="method">
5 Corpora
</sectionHeader>
<bodyText confidence="0.975339294117647">
We evaluated our appraisal extraction system on two
corpora. The first is the standard publicly available
collection of movie reviews constructed by Pang and
Lee (2004). This standard testbed consists of 1000
positive and 1000 negative reviews, taken from the
IMDb movie review archives2. Reviews with ‘neu-
tral’ scores (such as three stars out of five) were re-
moved by Pang and Lee, giving a data set with only
clearly positive and negative reviews. The average
document length in this corpus is 764 words, and
1107 different movies are reviewed.
The second corpus is a collection of user prod-
uct reviews taken from epinions.com supplied
in 2004 for research purposes by Amir Ashkenazi
of Shopping.Com. The base collection contains re-
views for three types of products: baby strollers, dig-
ital cameras, and printers. Each review has a numer-
ical rating (1–5); based on this, we labeled positive
and negative reviews in the same way as Pang and
Lee did for the movie reviews corpus. The prod-
ucts corpus has 15162 documents, averaging 442
words long. This comprises 11769 positive docu-
ments, 1420 neutral documents, and 1973 negative
documents. There are 905 reviews of strollers, 5778
2Seehttp://www.cs.cornell.edu/people/pabo
/movie-review-data/
reviews of ink-jet printers and 8479 reviews of digi-
tal cameras, covering 516 individual products.
Each document in each corpus was preprocessed
into individual sentences, lower-cased, and tok-
enized. We used an implementation of Brill’s (1992)
part-of-speech tagger to find adjectives and modi-
fiers; for parsing, we used the Stanford dependency
parser (Klein and Manning, 2003).
</bodyText>
<sectionHeader confidence="0.974734" genericHeader="method">
6 Evaluating Extraction
</sectionHeader>
<bodyText confidence="0.9996748">
We performed two manual evaluations on the sys-
tem. The first was to evaluate the overall accuracy
of the entire system. The second was to specifi-
cally evaluate the accuracy of the probabilistic dis-
ambiguator.
</bodyText>
<subsectionHeader confidence="0.998446">
6.1 Evaluating Accuracy
</subsectionHeader>
<bodyText confidence="0.9729603">
We evaluated randomly selected appraisal expres-
sions for extraction accuracy on a number of binary
measures. This manual evaluation was performed
by the first author.We evaluated interrater reliability
between this rater and another author on 200 ran-
domly selected appraisal expressions (100 on each
corpus). The first rater rated an additional 120 ex-
pressions (60 for each corpus), and combined these
with his ratings for interrater reliability to compute
system accuracy, for a total of 320 expressions (160
for each corpus). The (binary) rating criteria were as
follows. Relating to the appraisal group:
APP Does the expression express appraisal at all?
ARM If so, does the appraisal group have all rele-
vant modifiers?
HEM Does the appraisal group include extra mod-
ifiers? (Results are shown negated, so that
higher numbers are better.)
Relating to the target:
HT If there is appraisal, is there an identifiable tar-
get (even if the system missed it)?
FT If there is appraisal, did the system identify
some target? (Determined automatically.)
RT If so, is it the correct one?
Relating to the expression’s attribute values (if it ex-
presses appraisal):
Att Is the attitude type assigned correct?
Ori Is the orientation assigned correct?
Pol Is the polarity assigned correct?
Tar Is the target type assigned correct?
</bodyText>
<listItem confidence="0.5007835">
Pre Is the target type the most precise value in the
taxonomy for this target?
</listItem>
<page confidence="0.997651">
312
</page>
<tableCaption confidence="0.986985">
Table 1: System accuracy at evaluated tasks. 95%
confidence one-proportion z-intervals are reported.
</tableCaption>
<table confidence="0.962260461538461">
Measure Movies Products Combined
APP 86% f 3% 81% f 3% 83% f 2%
ARM 94% f 2% 95% f 2% 95% f 1%
� HEM 99% f 1% 100% 99.6% f 0.4%
HT 91% f 2% 97% f 2% 94% f 1%
FT 96% f 2% 94% f 2% 95% f 1%
RT 77% f 4% 73% f 4% 75% f 3%
Att 78% f 4% 80% f 4% 79% f 2%
Ori 95% f 2% 95% f 2% 94% f 1%
Pol 97% f 1% 96% f 2% 97% f 1%
Tar 84% f 3% 86% f 3% 85% f 2%
Pre 70% f 4% 77% f 4% 73% f 3%
Table 2: Interrater reliability of manual evaluation.
95% confidence intervals are reported.
Measure Movies Products Combined
APP 71% f 9% 87% f 7% 79% f 6%
ARM 95% f 5% 91% f 6% 93% f 4%
� HEM 98% f 3% 100% 99% f 1%
HT 97% f 4% 99% f 3% 98% f 3%
FT N/A N/A N/A
RT 94% f 6% 97% f 4% 96% f 4%
Att 79% f 10% 86% f 8% 83% f 6%
Ori 93% f 6% 94% f 5% 93% f 4%
Pol 96% f 4% 94% f 5% 95% f 4%
Tar 94% f 6% 90% f 7% 91% f 5%
Pre 86% f 10% 90% f 8% 88% f 6%
</table>
<bodyText confidence="0.996229214285714">
Results are given in Table 1, and interrater relia-
bility is given in Table 2. In nearly all cases agree-
ment percentages are above 80%, indicating good
inter-rater consensus. Regarding precision, we note
that most aspects of extraction seem to work quite
well. The area of most concern in the system is
precision of target classification. This may be im-
proved with further development of the target lex-
icons to classify more terms to specific leaves in
the target type hierarchy. The other area of con-
cern is the APP test, which encountered difficulties
when a word could be used as appraisal in some
contexts, but not in others, particularly when an ap-
praisal word appeared as a nominal classifier.
</bodyText>
<subsectionHeader confidence="0.999554">
6.2 Evaluating Disambiguation
</subsectionHeader>
<bodyText confidence="0.999987766666667">
The second experiment evaluated the accuracy of
EM in disambiguating the attitude type of appraisal
expressions. We evaluated the same number of ex-
pressions as used for the overall accuracy experi-
ment (100 used for interrater reliability and accu-
racy, plus 60 used only for accuracy on each corpus),
each having two or more word senses, presenting all
of the attitude types possible for each appraisal ex-
pression, as well as a ‘none of the above’ and a ‘not
appraisal’ option, asking the rater to select which
one applied to the selected expression in context.
Baseline disambiguator accuracy, if the computer
were to simply pick randomly from the choices
specified in the lexicon is 48% for both corpora. In-
terrater agreement was 80% for movies and 73% for
products (taken over 100 expressions from each cor-
pus.)
Considering just those appraisal expressions
which the raters decided were appraisal, the dis-
ambiguator achieved 58% accuracy on appraisal ex-
pressions from the movies corpus and 56% accuracy
on the products corpus. Further analysis of the re-
sults of the disambiguator shows that most of the er-
rors occur when the target type is the generic cate-
gory thing which occurs when the target is not in the
target lexicon. Performance on words recognized as
having more specific target types is better: 68% for
movies, and 59% for products. This indicates that
specific target type is an important indicator of at-
titude type.
</bodyText>
<sectionHeader confidence="0.943406" genericHeader="method">
7 Opinion Mining
</sectionHeader>
<bodyText confidence="0.999966962962963">
We (briefly) demonstrate the usefulness of appraisal
expression extraction by using it for opinion mining.
In opinion mining, we find large numbers of reviews
and perform data mining to determine which aspects
of a product people like or dislike, and in which
ways. To do this, we search for association rules de-
scribing the appraisal features that can be found in
a single appraisal expression. We generally look for
rules that contain attitude type, orientation, thing
type, and a product name, when these rules occur
more frequently than expected.
The idea is similar to Agrawal and
Srikant’s (1995) notion of generalized associa-
tion rules. We treat each appraisal expression as
a transaction, with the attributes of attitude type,
orientation, polarity, force, and thing type, as well
as the document attributes product name, product
type, and document classification (based on the
number of stars the reviewer gave the product).
We use CLOSET+ (Wang et al., 2003) to find all
of the frequent closed itemsets in the data, with a
support greater than or equal to 20 occurrences.
Let (b, a1, a2,... an) or (b, A) denote the contents
of an itemset, and c ((b, A)) denote the support for
this itemset. For a given item b, 7r(b) denotes its
immediate parent its value taxonomy, or ‘root’ for
flat sets.
</bodyText>
<page confidence="0.999517">
313
</page>
<tableCaption confidence="0.999898">
Table 3: The most interesting specific rules for products.
</tableCaption>
<table confidence="0.991225583333334">
Int. b A Target Type Orientation Polarity Doc.
Product Name Attitude class
45.7 Peg Perego Pliko Matic (1) quality this-product positive unmarked
42.8 Lexmark Color JetPrinter 1100 reaction this-product negative unmarked neg
41.9 Peg Perego Milano XL reaction this-product positive unmarked pos
41.1 Peg Perego Pliko Matic reaction this-product positive unmarked
40.8 Peg Perego Milano XL quality this-product positive unmarked pos
37.5 Peg Perego Milano XL reaction this-product positive unmarked
37.1 Peg Perego Milano XL quality this-product positive unmarked
36.3 Agfa ePhoto Smile (2) reaction experience negative unmarked neg
36.0 Agfa ePhoto Smile (2) reaction experience negative neg
33.9 KB Gear KG-JC3S Jamcam quality experience negative neg
</table>
<tableCaption confidence="0.999451">
Table 4: The most interesting oppositional rules for products.
</tableCaption>
<table confidence="0.995798285714286">
Int. b A Target Orient. Polarity Doc.
Product Name Attitude class
31.6 Lexmark Color JetPrinter 1100 (3) reaction this-product positive neg
31.5 Lexmark Color JetPrinter 1100 quality this-product positive neg
29.5 Lexmark Color JetPrinter 1100 reaction this-product positive unmarked neg
29.2 Lexmark Color JetPrinter 1100 quality this-product positive unmarked neg
28.9 Lexmark Color JetPrinter 1100 appreciation this-product positive neg
</table>
<bodyText confidence="0.831913">
For each item set, we collect rules (b, A) and
compute their interestingness relative to the itemset
(π(b), A). Interestingness is defined as follows:
</bodyText>
<equation confidence="0.950759333333333">
c((b, A)) x c((π(b)))
=
c((π(b), A)) x c((b))
</equation>
<bodyText confidence="0.999987195121951">
Int is the relative probability of finding the child
itemset in an appraisal expression, compared to find-
ing it in a parent itemset. Values greater than 1 in-
dicate that the child itemset appears more frequently
than we would expect.
We applied two simple filters to the output, to help
find more meaningful results. Specificity requires
that b be a product name, and that attitude type and
thing type be sufficiently deep nodes in the hier-
archy to describe something specific. (For exam-
ple, ‘product thing’ gives no real information about
what part of the product is being appraised.) Oppo-
sition chooses rules with a different rating than the
review as a whole, that is, document classification
is the opposite of appraisal orientation. The filter
also ensures that thing type is sufficiently specific,
as with specificity, and requires that b be a product
name.
We present the ten most ‘interesting’ rules from
each filter, for the products corpus. Rules from the
specificity filter are shown in Table 3 and rules from
the opposition filter are shown in Table 4. We con-
sider the meaning of some of these rules.
The first specificity rule (1) describes a typical ex-
ample of users who like the product very well over-
all. An example sentence that created this rule says
‘Not only is it an excellent stroller, because of it’s
[sic] size it even doubled for us as a portable crib.’
The specificity rules for the Agfa ePhoto Smile
Digital Camera (2) are an example of the kind of
rule we expect to see when bad user experience con-
tributes to bad reviews. The text of the reviews that
gave these rules quite clearly convey that users were
not happy specifically with the photo quality.
In the oppositional rules for the Lexmark Color
JetPrinter 1100 (3), we see that users made positive
comments about the product overall, while neverthe-
less giving the product a negative review. Drilling
down into the text, we can see some examples of re-
views like ‘On the surface it looks like a good printer
but it has many flaws that cause it to be frustrating.’
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999697583333333">
We have presented a new task, appraisal expres-
sion extraction, which, we suggest, is a fundamental
tasks for sentiment analysis. Shallow parsing based
on a set of appraisal lexicons, together with sparse
use of syntactic dependencies, can be used to ef-
fectively address the subtask of extracting adjectival
appraisal expressions. Indeed, straightforward data
mining applied to appraisal expressions can yield in-
sights into public opinion as expressed in patterns of
evaluative language in a corpus of product reviews.
Immediate future work includes extending the ap-
proach to include other types of appraisal expres-
</bodyText>
<equation confidence="0.917609">
P (A�b)
Int = P(A7r(b))
</equation>
<page confidence="0.992984">
314
</page>
<bodyText confidence="0.9999899">
sions, such as where an attitude is expressed via a
noun or a verb. In this regard, we will be examin-
ing extension of existing methods for automatically
building lexicons of positive/negative words (Tur-
ney, 2002; Esuli and Sebastiani, 2005) to the more
complex task of estimating also attitude type and
force. As well, a key problem is the fact that eval-
uative language is often context-dependent, and so
proper interpretation must consider interactions be-
tween a given phrase and its larger textual context.
</bodyText>
<sectionHeader confidence="0.998902" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999739755813954">
Rakesh Agrawal and Ramakrishnan Srikant. 1995. Min-
ing generalized association rules. In Umeshwar Dayal,
Peter M. D. Gray, and Shojiro Nishio, editors, Proc.
21st Int. Conf. Very Large Data Bases, VLDB, pages
407–419. Morgan Kaufmann, 11–15 September.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proc. of ACL Conference on Applied Natural
Language Processing.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
Human Language Technology Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT-EMNLP 2005), October.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms through gloss
analysis. In Proceedings of CIKM-05, the ACM SIGIR
Conference on Information and Knowledge Manage-
ment, Bremen, DE.
Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric Ringger. 2005. Pulse: Mining customer
opinions from free text. In Proceedings of IDA-05, the
6th International Symposium on Intelligent Data Anal-
ysis, Lecture Notes in Computer Science, Madrid, ES.
Springer-Verlag.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics.
J. R. Martin and P. R. R. White. 2005. The Language of
Evaluation: Appraisal in English. Palgrave, London.
(http://grammatics.com/appraisal/).
George A. Miller. 1995. Wordnet: A lexical database for
English. Commun. ACM, 38(11):39–41.
Tony Mullen and Nigel Collier. 2004. Sentiment analysis
using support vector machines with diverse informa-
tion sources. In Proceedings of EMNLP-04, 9th Con-
ference on Empirical Methods in Natural Language
Processing, Barcelon, ES.
Kamal Nigam and Matthew Hurst. 2004. Towards a ro-
bust metric of opinion. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications, Standford, US.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. 42nd ACL, pages
271–278, Barcelona, Spain, July.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of HLT-EMNLP-05, the Human Lan-
guage Technology Conference/Conference on Empiri-
cal Methods in Natural Language Processing, Vancou-
ver, CA.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP.
Peter D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings 40th Annual Meeting
of the ACL (ACL’02), pages 417–424, Philadelphia,
Pennsylvania.
Jianyong Wang, Jiawei Han, and Jian Pei. 2003.
CLOSET+: searching for the best strategies for min-
ing frequent closed itemsets. In Pedro Domingos,
Christos Faloutsos, Ted Senator, Hillol Kargupta,
and Lise Getoor, editors, Proceedings of the ninth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD-03), pages
236–245, New York, August 24–27. ACM Press.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal taxonomies for sentiment anal-
ysis. In Proceedings of CIKM-05, the ACM SIGIR
Conference on Information and Knowledge Manage-
ment, Bremen, DE.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30(3).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP 2005), Vancouver, CA.
</reference>
<page confidence="0.99929">
315
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.613941">
<title confidence="0.999715">Extracting Appraisal Expressions</title>
<author confidence="0.999802">Bloom Garg</author>
<affiliation confidence="0.9903015">Computer Science Illinois Institute of</affiliation>
<address confidence="0.7327325">10 W. 31st Chicago, IL</address>
<abstract confidence="0.998181555555555">Sentiment analysis seeks to characterize opinionated or evaluative aspects of natural language text. We suggest here that expression extraction be viewed as a fundamental task in sentiment An expression a textual unit expressing an evaluative stance towards some target. The task is to find and characterize the evaluative attributes of such elements. This paper describes a system for effectively extracting and disambiguating adjectival appraisal expressions in English outputting a generic representation in terms of their evaluative function in the text. Data mining on appraisal expressions gives meaningful and non-obvious insights.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Ramakrishnan Srikant</author>
</authors>
<title>Mining generalized association rules.</title>
<date>1995</date>
<booktitle>Proc. 21st Int. Conf. Very Large Data Bases, VLDB,</booktitle>
<pages>407--419</pages>
<editor>In Umeshwar Dayal, Peter M. D. Gray, and Shojiro Nishio, editors,</editor>
<publisher>Morgan Kaufmann,</publisher>
<marker>Agrawal, Srikant, 1995</marker>
<rawString>Rakesh Agrawal and Ramakrishnan Srikant. 1995. Mining generalized association rules. In Umeshwar Dayal, Peter M. D. Gray, and Shojiro Nishio, editors, Proc. 21st Int. Conf. Very Large Data Bases, VLDB, pages 407–419. Morgan Kaufmann, 11–15 September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proc. of ACL Conference on Applied Natural Language Processing.</booktitle>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A simple rule-based part of speech tagger. In Proc. of ACL Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP</booktitle>
<contexts>
<context position="1550" citStr="Choi et al., 2005" startWordPosition="226" endWordPosition="229">, which seeks to analyze opinion in natural language text, has grown in interest in recent years. Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of ‘semantic orientation’, (Turney, 2002); however, sentiment analysis still lacks a ‘unified field theory’. We propose in this paper that a fundamental task underlying many of these formulations is the extraction and analysis of appraisal expressions, defined as those structured textual units which express an evaluation of som</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP 2005), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining the semantic orientation of terms through gloss analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM-05, the ACM SIGIR Conference on Information and Knowledge Management,</booktitle>
<location>Bremen, DE.</location>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss analysis. In Proceedings of CIKM-05, the ACM SIGIR Conference on Information and Knowledge Management, Bremen, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Simon Corston-Oliver</author>
<author>Eric Ringger</author>
</authors>
<title>Pulse: Mining customer opinions from free text.</title>
<date>2005</date>
<booktitle>In Proceedings of IDA-05, the 6th International Symposium on Intelligent Data Analysis, Lecture Notes in Computer Science,</booktitle>
<publisher>Springer-Verlag.</publisher>
<location>Madrid, ES.</location>
<contexts>
<context position="1736" citStr="Gamon et al., 2005" startWordPosition="256" endWordPosition="259">fication techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of ‘semantic orientation’, (Turney, 2002); however, sentiment analysis still lacks a ‘unified field theory’. We propose in this paper that a fundamental task underlying many of these formulations is the extraction and analysis of appraisal expressions, defined as those structured textual units which express an evaluation of some object. An appraisal expression has three main components: an attitude (which takes an evaluative stance about an object), a target (the object of the stance), and a source (the person</context>
</contexts>
<marker>Gamon, Aue, Corston-Oliver, Ringger, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, Simon Corston-Oliver, and Eric Ringger. 2005. Pulse: Mining customer opinions from free text. In Proceedings of IDA-05, the 6th International Symposium on Intelligent Data Analysis, Lecture Notes in Computer Science, Madrid, ES. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18856" citStr="Klein and Manning, 2003" startWordPosition="2984" endWordPosition="2987">us has 15162 documents, averaging 442 words long. This comprises 11769 positive documents, 1420 neutral documents, and 1973 negative documents. There are 905 reviews of strollers, 5778 2Seehttp://www.cs.cornell.edu/people/pabo /movie-review-data/ reviews of ink-jet printers and 8479 reviews of digital cameras, covering 516 individual products. Each document in each corpus was preprocessed into individual sentences, lower-cased, and tokenized. We used an implementation of Brill’s (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003). 6 Evaluating Extraction We performed two manual evaluations on the system. The first was to evaluate the overall accuracy of the entire system. The second was to specifically evaluate the accuracy of the probabilistic disambiguator. 6.1 Evaluating Accuracy We evaluated randomly selected appraisal expressions for extraction accuracy on a number of binary measures. This manual evaluation was performed by the first author.We evaluated interrater reliability between this rater and another author on 200 randomly selected appraisal expressions (100 on each corpus). The first rater rated an additio</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Martin</author>
<author>P R R White</author>
</authors>
<title>The Language of Evaluation: Appraisal in English. Palgrave,</title>
<date>2005</date>
<location>London. (http://grammatics.com/appraisal/).</location>
<contexts>
<context position="5395" citStr="Martin and White (2005)" startWordPosition="828" endWordPosition="831">ion of the Source. Attitude may be expressed through nouns, verbs, adjectives and metaphors. Extracting all of this information accurately for all of these types of appraisal expressions is a very difficult problem. We therefore restrict ourselves for now to adjectival appraisal expressions that are each contained in a single sentence. Additionally, we focus here only on extracting and analyzing the attitude and the target, but not the source. Even with these restrictions, we obtain interesting results (Sec. 7). 2.1 Appraisal attributes Our method is grounded in Appraisal Theory, developed by Martin and White (2005), which analyzes the way opinion is expressed. Following Martin and White, we define: Attitude type is type of appraisal being expressed—one of affect, appreciation, or judgment (Figure 1). Affect refers to an emotional state (e.g., ‘happy’, ‘angry’), and is the most explicitly subjective type of appraisal. The other two types express evaluation of external entities, differentiating between intrinsic appreciation of object properties (e.g., ‘slender’, ‘ugly’) and social judgment (e.g., ‘heroic’, ‘idiotic’). Orientation is whether the attitude is positive Attitude Type Appreciation Composition </context>
</contexts>
<marker>Martin, White, 2005</marker>
<rawString>J. R. Martin and P. R. R. White. 2005. The Language of Evaluation: Appraisal in English. Palgrave, London. (http://grammatics.com/appraisal/).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for English.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="16655" citStr="Miller, 1995" startWordPosition="2636" endWordPosition="2637"> used in the paper can be found at http://lingcog.iit.edu/arc/ appraisal lexicon 2007a.tar.gz ri M(A = a|exp) aEA 311 For adjectival attitudes, we used the lexicon developed we developed in our previous work (Whitelaw et al., 2005) on appraisal. We reviewed the entire lexicon to determine its accuracy and made numerous improvements. Generic target lexicons were constructed by starting with a small sample of the kind of reviews that the lexicon would apply to. We examined these manually to find generic words referring to appraised things to serve as seed terms for the lexicon and used WordNet (Miller, 1995) to suggest additional terms to add to the lexicon. Since movie reviews often refer to the specific contents of the movie under review by proper names (of actors, the director, etc.), we also automatically constructed a specific target lexicon for each movie in the corpus, based on lists of actors, characters, writers, directors, and companies listed for the film at imdb.com. Each such specific lexicon was only used for processing reviews of the movie it was generated for, so the system had no specific knowledge of terms related to other movies during processing. 5 Corpora We evaluated our app</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for English. Commun. ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Sentiment analysis using support vector machines with diverse information sources.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-04, 9th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Barcelon, ES.</location>
<contexts>
<context position="1296" citStr="Mullen and Collier, 2004" startWordPosition="186" endWordPosition="189">d disambiguating adjectival appraisal expressions in English outputting a generic representation in terms of their evaluative function in the text. Data mining on appraisal expressions gives meaningful and non-obvious insights. 1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years. Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of ‘semantic orientation’, (Turney, 2002); however, sentiment analysis stil</context>
</contexts>
<marker>Mullen, Collier, 2004</marker>
<rawString>Tony Mullen and Nigel Collier. 2004. Sentiment analysis using support vector machines with diverse information sources. In Proceedings of EMNLP-04, 9th Conference on Empirical Methods in Natural Language Processing, Barcelon, ES.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Matthew Hurst</author>
</authors>
<title>Towards a robust metric of opinion.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications,</booktitle>
<location>Standford, US.</location>
<contexts>
<context position="1482" citStr="Nigam and Hurst, 2004" startWordPosition="213" endWordPosition="216">s meaningful and non-obvious insights. 1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years. Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of ‘semantic orientation’, (Turney, 2002); however, sentiment analysis still lacks a ‘unified field theory’. We propose in this paper that a fundamental task underlying many of these formulations is the extraction and analysis of appraisal expressions, defined </context>
</contexts>
<marker>Nigam, Hurst, 2004</marker>
<rawString>Kamal Nigam and Matthew Hurst. 2004. Towards a robust metric of opinion. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications, Standford, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proc. 42nd ACL,</booktitle>
<pages>271--278</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1412" citStr="Pang and Lee, 2004" startWordPosition="204" endWordPosition="207">ive function in the text. Data mining on appraisal expressions gives meaningful and non-obvious insights. 1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years. Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of ‘semantic orientation’, (Turney, 2002); however, sentiment analysis still lacks a ‘unified field theory’. We propose in this paper that a fundamental task underlying many of these formulat</context>
<context position="17403" citStr="Pang and Lee (2004)" startWordPosition="2757" endWordPosition="2760">eview by proper names (of actors, the director, etc.), we also automatically constructed a specific target lexicon for each movie in the corpus, based on lists of actors, characters, writers, directors, and companies listed for the film at imdb.com. Each such specific lexicon was only used for processing reviews of the movie it was generated for, so the system had no specific knowledge of terms related to other movies during processing. 5 Corpora We evaluated our appraisal extraction system on two corpora. The first is the standard publicly available collection of movie reviews constructed by Pang and Lee (2004). This standard testbed consists of 1000 positive and 1000 negative reviews, taken from the IMDb movie review archives2. Reviews with ‘neutral’ scores (such as three stars out of five) were removed by Pang and Lee, giving a data set with only clearly positive and negative reviews. The average document length in this corpus is 764 words, and 1107 different movies are reviewed. The second corpus is a collection of user product reviews taken from epinions.com supplied in 2004 for research purposes by Amir Ashkenazi of Shopping.Com. The base collection contains reviews for three types of products:</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proc. 42nd ACL, pages 271–278, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1224" citStr="Pang et al., 2002" startWordPosition="175" endWordPosition="178">ents. This paper describes a system for effectively extracting and disambiguating adjectival appraisal expressions in English outputting a generic representation in terms of their evaluative function in the text. Data mining on appraisal expressions gives meaningful and non-obvious insights. 1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years. Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP-05, the Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, CA.</location>
<contexts>
<context position="1764" citStr="Popescu and Etzioni, 2005" startWordPosition="260" endWordPosition="263">to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of ‘semantic orientation’, (Turney, 2002); however, sentiment analysis still lacks a ‘unified field theory’. We propose in this paper that a fundamental task underlying many of these formulations is the extraction and analysis of appraisal expressions, defined as those structured textual units which express an evaluation of some object. An appraisal expression has three main components: an attitude (which takes an evaluative stance about an object), a target (the object of the stance), and a source (the person taking the stance) which ma</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT-EMNLP-05, the Human Language Technology Conference/Conference on Empirical Methods in Natural Language Processing, Vancouver, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1391" citStr="Riloff and Wiebe, 2003" startWordPosition="200" endWordPosition="203">n terms of their evaluative function in the text. Data mining on appraisal expressions gives meaningful and non-obvious insights. 1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years. Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of ‘semantic orientation’, (Turney, 2002); however, sentiment analysis still lacks a ‘unified field theory’. We propose in this paper that a fundamental task underlying m</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings 40th Annual Meeting of the ACL (ACL’02),</booktitle>
<pages>417--424</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1269" citStr="Turney, 2002" startWordPosition="184" endWordPosition="185"> extracting and disambiguating adjectival appraisal expressions in English outputting a generic representation in terms of their evaluative function in the text. Data mining on appraisal expressions gives meaningful and non-obvious insights. 1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years. Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al., 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of ‘semantic orientation’, (Turney, 2002); howev</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings 40th Annual Meeting of the ACL (ACL’02), pages 417–424, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianyong Wang</author>
<author>Jiawei Han</author>
<author>Jian Pei</author>
</authors>
<title>CLOSET+: searching for the best strategies for mining frequent closed itemsets.</title>
<date>2003</date>
<booktitle>Proceedings of the ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-03),</booktitle>
<pages>236--245</pages>
<editor>In Pedro Domingos, Christos Faloutsos, Ted Senator, Hillol Kargupta, and Lise Getoor, editors,</editor>
<publisher>ACM Press.</publisher>
<location>New York,</location>
<contexts>
<context position="24582" citStr="Wang et al., 2003" startWordPosition="4018" endWordPosition="4021">an be found in a single appraisal expression. We generally look for rules that contain attitude type, orientation, thing type, and a product name, when these rules occur more frequently than expected. The idea is similar to Agrawal and Srikant’s (1995) notion of generalized association rules. We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product). We use CLOSET+ (Wang et al., 2003) to find all of the frequent closed itemsets in the data, with a support greater than or equal to 20 occurrences. Let (b, a1, a2,... an) or (b, A) denote the contents of an itemset, and c ((b, A)) denote the support for this itemset. For a given item b, 7r(b) denotes its immediate parent its value taxonomy, or ‘root’ for flat sets. 313 Table 3: The most interesting specific rules for products. Int. b A Target Type Orientation Polarity Doc. Product Name Attitude class 45.7 Peg Perego Pliko Matic (1) quality this-product positive unmarked 42.8 Lexmark Color JetPrinter 1100 reaction this-product </context>
</contexts>
<marker>Wang, Han, Pei, 2003</marker>
<rawString>Jianyong Wang, Jiawei Han, and Jian Pei. 2003. CLOSET+: searching for the best strategies for mining frequent closed itemsets. In Pedro Domingos, Christos Faloutsos, Ted Senator, Hillol Kargupta, and Lise Getoor, editors, Proceedings of the ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-03), pages 236–245, New York, August 24–27. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal taxonomies for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM-05, the ACM SIGIR Conference on Information and Knowledge Management,</booktitle>
<location>Bremen, DE.</location>
<contexts>
<context position="2740" citStr="Whitelaw et al., 2005" startWordPosition="413" endWordPosition="416">ich express an evaluation of some object. An appraisal expression has three main components: an attitude (which takes an evaluative stance about an object), a target (the object of the stance), and a source (the person taking the stance) which may be implied. The idea of appraisal extraction is a generalization of problem formulations developed in earlier works. Mullen and Collier’s (2004) notion of classifying appraisal terms using a multidimensional set of attributes is closely tied to the definition of an appraisal expression, which is classified along several dimensions. In previous work (Whitelaw et al., 2005), we presented a related technique of finding opinion phrases, using a multidimensional set of attributes and modeling the semantics of modifiers in these phrases. The use of multiple text classifiers by Wiebe and colleagues (Wilson et al., 2005; Wiebe et al., 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions. Nigam and Hurst’s (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzioni’s (2005) opinion mining technique also fits we</context>
<context position="9243" citStr="Whitelaw et al., 2005" startWordPosition="1398" endWordPosition="1401"> ‘Experience’ for things associated with the experience of using the product (such as ‘pictures’ or ‘resolution’, were also added. 3 Appraisal Extraction In our system, appraisal extraction runs in several independent stages. First, the appraisal extractor finds appraisal expressions by finding the chunks of text that express attitudes and targets. Then, it links each attitude group found to a target in the text. Finally, it uses a probabilistic model to determine which attitude type should be assigned when attitude chunks were ambiguous. 3.1 Chunking The chunker is based on our earlier work (Whitelaw et al., 2005), which finds attitude groups and targets using a hand-built lexicon (Sec. 4). This lexicon contains head adjectives (which specify values for the attributes attitude type, force, polarity, and orientation), and appraisal modifiers (which specify transformations to the four attributes). Some head adjectives are ambiguous, having multiple entries in the lexicon with different attribute values. In all cases, different entries for a given word have different attitude types. If the head adjective is ambiguous, multiple groups are created, to be disambiguated later. See our previous work (Whitelaw </context>
<context position="16273" citStr="Whitelaw et al., 2005" startWordPosition="2570" endWordPosition="2573">sed things were identified based on a manually constructed lexicon containing generic movie words, as well as automatically constructed lexicons of proper names specific to each movie being reviewed. For each product type considered, we manually constructed a lexicon containing generic product words; we did not find it necessary to construct product-specific lexicons. 1All of the lexicons used in the paper can be found at http://lingcog.iit.edu/arc/ appraisal lexicon 2007a.tar.gz ri M(A = a|exp) aEA 311 For adjectival attitudes, we used the lexicon developed we developed in our previous work (Whitelaw et al., 2005) on appraisal. We reviewed the entire lexicon to determine its accuracy and made numerous improvements. Generic target lexicons were constructed by starting with a small sample of the kind of reviews that the lexicon would apply to. We examined these manually to find generic words referring to appraised things to serve as seed terms for the lexicon and used WordNet (Miller, 1995) to suggest additional terms to add to the lexicon. Since movie reviews often refer to the specific contents of the movie under review by proper names (of actors, the director, etc.), we also automatically constructed </context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal taxonomies for sentiment analysis. In Proceedings of CIKM-05, the ACM SIGIR Conference on Information and Knowledge Management, Bremen, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="3006" citStr="Wiebe et al., 2004" startWordPosition="457" endWordPosition="460">of appraisal extraction is a generalization of problem formulations developed in earlier works. Mullen and Collier’s (2004) notion of classifying appraisal terms using a multidimensional set of attributes is closely tied to the definition of an appraisal expression, which is classified along several dimensions. In previous work (Whitelaw et al., 2005), we presented a related technique of finding opinion phrases, using a multidimensional set of attributes and modeling the semantics of modifiers in these phrases. The use of multiple text classifiers by Wiebe and colleagues (Wilson et al., 2005; Wiebe et al., 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions. Nigam and Hurst’s (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzioni’s (2005) opinion mining technique also fits well into our framework. In this paper we describe a system for extracting adjectival appraisal expressions, based on a handbuilt lexicon, a combination of heuristic shallow parsing and dependency parsing, and expectationmaximization word sense disambiguation. Each ex</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational Linguistics, 30(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<location>Vancouver, CA.</location>
<contexts>
<context position="2985" citStr="Wilson et al., 2005" startWordPosition="453" endWordPosition="456">be implied. The idea of appraisal extraction is a generalization of problem formulations developed in earlier works. Mullen and Collier’s (2004) notion of classifying appraisal terms using a multidimensional set of attributes is closely tied to the definition of an appraisal expression, which is classified along several dimensions. In previous work (Whitelaw et al., 2005), we presented a related technique of finding opinion phrases, using a multidimensional set of attributes and modeling the semantics of modifiers in these phrases. The use of multiple text classifiers by Wiebe and colleagues (Wilson et al., 2005; Wiebe et al., 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions. Nigam and Hurst’s (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzioni’s (2005) opinion mining technique also fits well into our framework. In this paper we describe a system for extracting adjectival appraisal expressions, based on a handbuilt lexicon, a combination of heuristic shallow parsing and dependency parsing, and expectationmaximization word sense di</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), Vancouver, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>