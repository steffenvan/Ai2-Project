<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000362">
<title confidence="0.997982">
Dialogue Segmentation with Large Numbers of Volunteer Internet
Annotators
</title>
<author confidence="0.992855">
T. Daniel Midgley
</author>
<affiliation confidence="0.940769">
Discipline of Linguistics, School of Computer Science and Software Engineering
University of Western Australia
Perth, Australia
</affiliation>
<email confidence="0.988458">
Daniel.Midgley@uwa.edu.au
</email>
<sectionHeader confidence="0.997266" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873066666667">
This paper shows the results of an
experiment in dialogue segmentation. In this
experiment, segmentation was done on a
level of analysis similar to adjacency pairs.
The method of annotation was somewhat
novel: volunteers were invited to participate
over the Web, and their responses were
aggregated using a simple voting method.
Though volunteers received a minimum of
training, the aggregated responses of the
group showed very high agreement with
expert opinion. The group, as a unit,
performed at the top of the list of
annotators, and in many cases performed as
well as or better than the best annotator.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923909090909">
Aggregated human behaviour is a valuable
source of information. The Internet shows us
many examples of collaboration as a means of
resource creation. Wikipedia, Amazon.com
reviews, and Yahoo! Answers are just some
examples of large repositories of information
powered by individuals who voluntarily
contribute their time and talents. Some NLP
projects are now using this idea, notably the
`ESP Game&apos; (von Ahn 2004), a data collection
effort presented as a game in which players label
images from the Web. This paper presents an
extension of this collaborative volunteer ethic in
the area of dialogue annotation.
For dialogue researchers, the prospect of
using volunteer annotators from the Web can be
an attractive option. The task of training
annotators can be time-consuming, expensive,
and (if inter-annotator agreement turns out to be
poor) risky.
Getting Internet volunteers for annotation has
its own pitfalls. Dialogue annotation is often not
very interesting, so it can be difficult to attract
willing participants. Experimenters will have
little control over the conditions of the
annotation and the skill of the annotators.
Training will be minimal, limited to whatever an
average Web surfer is willing to read. There may
also be perverse or uncomprehending users
whose answers may skew the data.
This project began as an exploratory study
about the intuitions of language users with
regard to dialogue segmentation. We wanted
information about how language users perceive
dialogue segments, and we wanted to be able to
use this information as a kind of gold standard
against which we could compare the
performance of an automatic dialogue
segmenter. For our experiment, the advantages
of Internet annotation were compelling. We
could get free data from as many language users
as we could attract, instead of just two or three
well-trained experts. Having more respondents
meant that our results could be more readily
generalised to language users as a whole.
We expected that multiple users would
converge upon some kind of uniform result.
What we found (for this task at least) was that
large numbers of volunteers show very strong
tendencies that correspond well to expert
opinion, and that these patterns of agreement are
surprisingly resilient in the face of noisy input
from some users. We also gained some insights
into the way that people perceived dialogue
segments.
</bodyText>
<sectionHeader confidence="0.994903" genericHeader="introduction">
2 Segmentation
</sectionHeader>
<bodyText confidence="0.999897666666667">
While much work in dialogue segmentation
centers around topic (e.g. Galley et al. 2003,
Hsueh et al. 2006, Purver et al. 2006), we
decided to examine dialogue at a more fine-
grained level. The level of analysis that we have
chosen corresponds most closely to adjacency
pairs (after Sacks, Schegloff and Jefferson
1974), where a segment is made of matched sets
of utterances from different speakers (e.g.
question/answer or suggest/accept). We chose to
segment dialogues this way in order to improve
dialogue act tagging, and we think that
</bodyText>
<page confidence="0.995638">
897
</page>
<bodyText confidence="0.999791611111111">
examining the back-and-forth detail of the
mechanics of dialogue will be the most helpful
level of analysis for this task.
The back-and-forth nature of dialogue also
appears in Clark and Schaefer&apos;s (1989)
influential work on contributions in dialogue. In
this view, two-party dialogue is seen as a set of
cooperative acts used to add information to the
common ground for the purpose of
accomplishing some joint action. Clark and
Schaefer map these speech acts onto
contribution trees. Each utterance within a
contribution tree serves either to present some
proposition or to acknowledge a previous one.
Accordingly, each contribution tree has a
presentation phase and an acceptance phase.
Participants in dialogue assume that items they
present will be added to the common ground
unless there is evidence to the contrary.
However, participants do not always show
acceptance of these items explicitly. Speaker B
may repeat Speaker&apos;s A&apos;s information verbatim
to show understanding (as one does with a
phone number), but for other kinds of
information a simple `uh-huh&apos; will constitute
adequate evidence of understanding. In general,
less and less evidence will be required the
farther on in the segment one goes.
In practice, then, segments have a tailing-off
quality that we can see in many dialogues. Table
1 shows one example from Verbmobil-2, a
corpus of appointment scheduling dialogues. (A
description of this corpus appears in
Alexandersson 1997.)
A segment begins when WJH brings a
question to the table (utterances 1 and 2 in our
</bodyText>
<table confidence="0.874759076923077">
WJH &lt;uhm&gt; basically we have to be
in Hanover for a day and a half
WJH correct
AHS right
WJH okay
WJH &lt;uh&gt; I am looking through my
schedule for the next three
months
WJH and I just noticed I am working
all of Christmas week
WJH so I am going to do it in
Germany if at all possible
AHS okay
</table>
<tableCaption confidence="0.9946605">
Table 1. A sample of the corpus. Two segments are
represented here.
</tableCaption>
<bodyText confidence="0.999934771428572">
example), AHS answers it (utterance 3), and
WJH acknowledges the response (utterance 4).
At this point, the question is considered to be
resolved, and a new contribution can be issued.
WJH starts a new segment in utterance 5, and
this utterance shows features that will be
familiar to dialogue researchers: the number of
words increases, as does the incidence of new
words. By the end of this segment (utterance 8),
AHS only needs to offer a simple `okay&apos; to show
acceptance of the foregoing.
Our work is not intended to be a strict
implementation of Clark and Schaefer&apos;s
contribution trees. The segments represented by
these units is what we were asking our volunteer
annotators to find. Other researchers have also
used a level of analysis similar to our own.
J6nsson&apos;s (1991) initiative-response units is one
example.
Taking a cue from Mann (1987), we decided
to describe the behaviour in these segments
using an atomic metaphor: dialogue segments
have nuclei, where someone says something,
and someone says something back (roughly
corresponding to adjacency pairs), and satellites,
usually shorter utterances that give feedback on
whatever the nucleus is about.
For our annotators, the process was simply to
find the nuclei, with both speakers taking part,
and then attach any nearby satellites that
pertained to the segment.
We did not attempt to distinguish nested
adjacency pairs. These would be placed within
the same segment. Eventually we plan to modify
our system to recognise these nested pairs.
</bodyText>
<sectionHeader confidence="0.996714" genericHeader="method">
3 Experimental Design
</sectionHeader>
<subsectionHeader confidence="0.996281">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9999475">
In the pilot phase of the experiment, volunteers
could choose to segment up to four randomly-
chosen dialogues from the Verbmobil-2 corpus.
(One longer dialogue was separated into two.)
We later ran a replication of the experiment with
eleven dialogues. For this latter phase, each
volunteer started on a randomly chosen dialogue
to ensure evenness of responses.
The dialogues contained between 44 and 109
utterances. The average segment was 3.59
utterances in length, by our annotation.
Two dialogues have not been examined
because they will be used as held-out data for
the next phase of our research. Results from the
</bodyText>
<figure confidence="0.693848">
1
2
3
4
5
6
</figure>
<page confidence="0.943423333333333">
7
8
898
</page>
<bodyText confidence="0.9956145">
other thirteen dialogues appear in part 4 of this
paper.
</bodyText>
<subsectionHeader confidence="0.99936">
3.2 Annotators
</subsectionHeader>
<bodyText confidence="0.997356945945946">
Volunteers were recruited via postings on
various email lists and websites. This included a
posting on the university events mailing list,
sent to people associated with the university, but
with no particular linguistic training. Linguistics
first-year students and Computer Science
students and staff were also informed of the
project. We sent advertisements to a variety of
international mailing lists pertaining to
language, computation, and cognition, since
these lists were most likely to have a readership
that was interested in language. These included
Linguist List, Corpora, CogLing-L, and
HCSNet. An invitation also appeared on the
personal blog of the first author.
At the experimental website, volunteers were
asked to read a brief description of how to
annotate, including the descriptions of nuclei
and satellites. The instruction page showed some
examples of segments. Volunteers were
requested not to return to the instruction page
once they had started the experiment.
The annotator guide with examples can be
seen at the following URL:
http://tinyurl.com/ynwmx9
A scheme that relies on volunteer annotation
will need to address the issue of motivation.
People have a desire to be entertained, but
dialogue annotation can often be tedious and
difficult. We attempted humor as a way of
keeping annotators amused and annotating for as
long as possible. After submitting a dialogue,
annotators would see an encouraging page,
sometimes with pretend `badges&apos; like the one
pictured in Figure 1. This was intended as a way
of keeping annotators interested to see what
comments would come next. Figure 2 shows
</bodyText>
<figureCaption confidence="0.9222355">
Figure 1. One of the screens that appears after an
annotator submits a marked form.
</figureCaption>
<bodyText confidence="0.99983525">
statistics on how many dialogues were marked
by any one IP address. While over half of the
volunteers marked only one dialogue, many
volunteers marked all four (or in the replication,
all eleven) dialogues. Sometimes more than
eleven dialogues were submitted from the same
location, most likely due to multiple users
sharing a computer.
</bodyText>
<figure confidence="0.9997025">
0
25 10 32 17
3 4 3 1 2 0 2
0
1 2 3 4 5 6 7 8 9 10 11 &gt;11
Number of dialogues completed
</figure>
<figureCaption confidence="0.901428">
Figure 2. Number of dialogues annotated by single
IP addresses
</figureCaption>
<bodyText confidence="0.820986">
numbers). We collected between 32 an
d 73
responses for each of the 15 dialogues.
</bodyText>
<subsectionHeader confidence="0.999898">
3.3 Method of Evaluation
</subsectionHeader>
<bodyText confidence="0.99162075">
In all, we received 626 responses from about
231 volunteers (though this is difficult to
determine from only the volunteers&apos; IP
We used the WindowDiff (WD) metric (Pevzner
and Hearst 2002) to evaluate the responses of
our volunteers against expert opinion (our
responses). The WD algorithm calculates
agreement between a reference copy of the
corpus and a volunteer&apos;s hypothesis by moving a
window over the utterances in the two corpora.
The window has a size equal to half the average
segment length. Within the window, the
algorithm examines the number of segment
boundaries in the reference and in the
hypothesis, and a counter is augmented by one if
they disagree. The WD score between the
reference and the hypothesis is equal to the
number of discrepancies divided by the number
of measurements taken. A score of 0 would be
given to two annotators who agree perfectly, and
1 would signify perfect disagreement.
Figure 3 shows the WD scores for the
volunteers. Most volunteers achieved a WD
score between .15 and .2, with an average of
.245.
Cohen&apos;s Kappa (x) (Carletta 1996) is another
method of compari
ng inter-annotator agreement
</bodyText>
<figure confidence="0.981120352941176">
Number of annotators
150
1
120
90
60
30
899
150
125
100
75
50
25
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
WindowDiff range
</figure>
<figureCaption confidence="0.9880165">
Figure 3. WD scores for individual responses. A
score of 0 indicates perfect agreement.
</figureCaption>
<bodyText confidence="0.999916">
in segmentation that is widely used in
computational language tasks. It measures the
observed agreement (AO) against the agreement
we should expect by chance (AE), as follows:
</bodyText>
<equation confidence="0.979304">
AO - AE
K =
1 - AE
</equation>
<bodyText confidence="0.999867352941177">
For segmentation tasks, x is a more stringent
method than WindowDiff, as it does not
consider near-misses. Even so, x scores are
reported in Section 4.
About a third of the data came from
volunteers who chose to complete all eleven of
the dialogues. Since they contributed so much of
the data, we wanted to find out whether they
were performing better than the other
volunteers. This group had an average WD score
of .199, better than the rest of the group at .268.
However, skill does not appear to increase
smoothly as more dialogues are completed. The
highest performance came from the group that
completed 5 dialogues (average WD = .187), the
lowest from those that completed 8 dialogues (.
299).
</bodyText>
<subsectionHeader confidence="0.991151">
3.4 Aggregation
</subsectionHeader>
<bodyText confidence="0.999993103448276">
We wanted to determine, insofar as was
possible, whether there was a group consensus
as to where the segment boundaries should go.
We decided to try overlaying the results from all
respondents on top of each other, so that each
click from each respondent acted as a sort of
vote. Figure 4 shows the result of aggregating
annotator responses from one dialogue in this
way. There are broad patterns of agreement;
high `peaks&apos; where many annotators agreed that
an utterance was a segment boundary, areas of
uncertainty where opinion was split between
two adjacent utterances, and some background
noise from near-random respondents.
Group opinion is manifested in these peaks.
Figure 5 shows a hypothetical example to
illustrate how we defined this notion. A peak is
any local maximum (any utterance u where u - 1
&lt; u &gt; u + 1) above background noise, which we
define as any utterance with a number of votes
below the arithmetic mean. Utterance 5, being a
local maximum, is a peak. Utterance 2, though a
local maximum, is not a peak as it is below the
mean. Utterance 4 has a comparatively large
number of votes, but it is not considered a peak
because its neighbour, utterance 5, is higher.
Defining peaks this way allows us to focus on
the points of highest agreement, while ignoring
not only the relatively low-scoring utterances,
</bodyText>
<figure confidence="0.985565816326531">
Number of responses
e059 n = 42
45
40
25
20
35
30
15
10
5
0
22
after_e059ach1_000_ANV_00
after_e059ach1_000_ANV_01
after_e059ach2_001_CNK_02
after_e059ach2_001_CNK_03
after_e059ach1_002_ANV_04
after_e059ach1_002_ANV_05
after_e059ach1_002_ANV_06
after_e059ach2_003_CNK_07
after_e059ach1_004_ANV_08
after_e059ach2_005_CNK_09
after_e059ach1_006_ANV_10
after_e059ach1_006_ANV_11
after_e059ach1_006_ANV_12
after_e059ach2_007_CNK_13
after_e059ach2_007_CNK_14
after_e059ach2_007_CNK_15
after_e059ach1_008_ANV_16
after_e059ach1_008_ANV_17
after_e059ach1_008_ANV_8
after_e059ach1_008_ANV_19
after_e059ach2_009_CNK_20
after_e059ach1_010_ANV_21
after_e059ach1_010_ANV_22
after_e059ach1_010_ANV_23
after_e059ach1_010_ANV_24
after_e059ach2_011_CNK_25
after_e059ach1_012_ANV_26
after_e059ach1_012_ANV_27
after_e059ach2_013_CNK_28
after_e059ach2_014_CNK_29
after_e059ach1_015_ANV_30
after_e059ach1_016_ANV_31
after_e059ach1_016_ANV_32
after_e059ach1_016_ANV_33
after_e059ach2_017_CNK_34
after_e059ach1_018_ANV_35
after_e059ach1_018_ANV_36
after_e059ach1_018_ANV_37
after_e059ach2_019_CNK_38
after_e059ach2_019_CNK_39
after_e059ach1_020_ANV_40
after_e059ach2_021_CNK_41
after_e059ach2_021_CNK_42
after_e059ach1_022_ANV_43
after_e059ach2_023_CNK_44
after_e059ach1_024_ANV_45
after_e059ach2_025_CNK_46
after_e059ach1_026_ANV_47
after_e059ach2_027_CNK_48
after_e059ach1_028_ANV_49
after_e059ach2_029_CNK_50
after_e059ach2_030_CNK_51
after_e059ach2_030_CNK_52
after_e059ach2_030_CNK_53
after_e059ach2_030_CNK_54
after_e059ach2_030_CNK_55
after_e059ach2_030_CNK_56
after_e059ach2_030_CNK_57
after_e059ach1_031_ANV_58
after_e059ach2_032_CNK_59
after_e059ach1_033_ANV_60
after_e059ach2_034_CNK_61
after_e059ach1_035_ANV_62
after_e059ach1_035_ANV_63
after_e059ach2_036_CNK_64
after_e059ach1_037_ANV_65
after_e059ach2_038_CNK_66
after_e059ach2_039_CNK_67
after_e059ach1_040_ANV_68
after_e059ach1_040_ANV_69
after_e059ach1_040_ANV_70
after_e059ach2_041_CNK_71
after_e059ach1_042_ANV_72
after_e059ach1_042_ANV_73
after_e059ach2_043_CNK_74
after_e059ach1_044_ANV_75
37
3
41
11
24
6 6 6
2 3 43
890123456 3 0
30
37
3 2
24
12
3
4
5
0 23
38
44
9
1
86
32
38
12
23
8
2
4
17
1
42
221
37
322
9
0
1
2
3
0 0
4 1 32
15
2
35
44
25
10
21
26
16
1
4
7
28
43
36
mean = 9.89
</figure>
<figureCaption confidence="0.977269666666667">
Figure 4. The results for one dialogue. Each utterance in the dialogue is represented in sequence along the x axis.
Numbers in dots represent the number of respondents that `voted&apos; for that utterance as a segment boundary. Peaks
appear where agreement is strongest. A circle around a data point indicates our choices for segment boundary.
</figureCaption>
<page confidence="0.973336">
900
</page>
<figureCaption confidence="0.934106666666667">
Figure 5. Defining the notion of `peak&apos;. Numbers in
circles indicate number of `votes&apos; for that utterance
as a boundary.
</figureCaption>
<bodyText confidence="0.999451375">
but also the potentially misleading utterances
near a peak.
There are three disagreements in the dialogue
presented in Figure 4. For the first, annotators
saw a break where we saw a continuation. The
other two disagreements show the reverse:
annotators saw a continuation of topic as a
continuation of segment.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.99998003030303">
Table 2 shows the agreement of the aggregated
group votes with regard to expert opinion. The
aggregated responses from the volunteer
annotators agree extremely well with expert
opinion. Acting as a unit, the group&apos;s
WindowDiff scores always perform better than
the individual annotators on average. While the
individual annotators attained an average WD
score of .245, the annotators-as-group scored
WD = .108.
On five of the thirteen dialogues, the group
performed as well as or better than the best
individual annotator. On the other eight
dialogues, the group performance was toward
the top of the group, bested by one annotator
(three times), two annotators (once), four
annotators (three times), or six annotators
(once), out of a field of 32Ð73 individuals. This
suggests that aggregating the scores in this way
causes a `majority rule&apos; effect that brings out the
best answers of the group.
One drawback of the WD statistic (as
opposed to x) is that there is no clear consensus
for what constitutes `good agreement&apos;. For
computational linguistics, x &gt; .67 is generally
considered strong agreement. We found that x
for the aggregated group ranged from .71 to .94.
Over all the dialogues, x = .84. This is
surprisingly high agreement for a dialogue-level
task, especially considering the stringency of the
K statistic, and that the data comes from
untrained volunteers, none of whom were
dropped from the sample.
</bodyText>
<sectionHeader confidence="0.984891" genericHeader="method">
5 Comparison to Trivial Baselines
</sectionHeader>
<bodyText confidence="0.966764571428572">
We used a number of trivial baselines to see if
our results could be bested by simple means.
These were random placement of boundaries,
majority class, marking the last utterance in each
turn as a boundary, and a set of hand-built rules
we called `the Trigger&apos;. The results of these
trials can be seen in Figure 6.
</bodyText>
<figure confidence="0.914452">
27
A1 A2 A3 A4 A5 A6
2
7
3
11
5
mean = 9.5
</figure>
<table confidence="0.960980105263158">
Dialogue name WD average as WD single WD single WD for group How many Number of
marked by annotator best annotator opinion annotator did annotators
volunteers worst better.
e041a 0.210 0.094 0.766 0.094 0 39
e041b 0.276 0.127 0.794 0.095 0 39
e059 0.236 0.080 0.920 0.107 1 42
e081a 0.244 0.037 0.611 0.148 4 36
e081b 0.267 0.093 0.537 0.148 4 32
e096a 0.219 0.083 0.604 - - 32
e096b 0.160 0.000 0.689 0.044 1 36
e115 0.214 0.079 0.750 0.079 0 34
e119 0.241 0.102 0.610 - - 32
e123a 0.259 0.043 1.000 0.174 6 34
e123b 0.193 0.093 0.581 0.047 0 33
e030 0.298 0.110 0.807 0.147 2 55
e066 0.288 0.063 0.921 0.063 0 69
e076a 0.235 0.026 0.868 0.053 1 73
e076b 0.270 0.125 0.700 0.175 4 40
ALL 0.245 0.000 1.000 0.108 60 626
</table>
<tableCaption confidence="0.998038">
Table 2. Summary of WD results for dialogues. Data has not been aggregated for two dialogues because they
are being held out for future work.
</tableCaption>
<page confidence="0.993611">
901
</page>
<subsectionHeader confidence="0.985446">
5.1 Majority Class
</subsectionHeader>
<bodyText confidence="0.999852714285714">
This baseline consisted of marking every
utterance with the most common classification,
which was `not a boundary&apos;. (About one in four
utterances was marked as the end of a segment
in the reference dialogues.) This was one of the
worst case baselines, and gave WD = .551 over
all dialogues.
</bodyText>
<subsectionHeader confidence="0.999209">
5.2 Random Boundary Placement
</subsectionHeader>
<bodyText confidence="0.9999792">
We used a random number generator to
randomly place as many boundaries in each
dialogue as we had in our reference dialogues.
This method gave about the same accuracy as
the `majority class&apos; method with WD = .544.
</bodyText>
<subsectionHeader confidence="0.999462">
5.3 Last Utterance in Turn
</subsectionHeader>
<bodyText confidence="0.999857333333333">
In these dialogues, a speaker&apos;s turn could consist
of more than one utterance. For this baseline,
every final utterance in a turn was marked as the
beginning of a segment, except when lone
utterances would have created a segment with
only one speaker.
This method was suggested by work from
Sacks, Schegloff, and Jefferson (1974) who
observed that the last utterance in a turn tends to
be the first pair part for another adjacency pair.
Wright, Poesio, and Isard (1999) used a variant
of this idea in a dialogue act tagger, including
not only the previous utterance as a feature, but
also the previous speaker&apos;s last speech act type.
This method gave a WD score of .392.
</bodyText>
<subsectionHeader confidence="0.999183">
5.4 The Trigger
</subsectionHeader>
<bodyText confidence="0.9903775">
This method of segmentation was a set of hand-
built rules created by the author. In this method,
two conditions have to exist in order to start a
new segment.
</bodyText>
<listItem confidence="0.976188333333333">
• Both speakers have to have spoken.
• One utterance must contain four words or
less.
</listItem>
<bodyText confidence="0.999403166666667">
The `four words&apos; requirement was determined
empirically during the feature selection phase of
an earlier experiment.
Once both these conditions have been met,
the `trigger&apos; is set. The next utterance to have
more than four words is the start of a new
segment.
This method performed comparatively well,
with WD = .210, very close to the average
individual annotator score of .245.
As mentioned, the aggregated annotator score
was WD = .108.
</bodyText>
<figure confidence="0.679873">
0.6 0.551 0.544
0.5
0.4
WD scores 0.392
0.3
0.210
0.2
0.108
0.1
0
Majority Random Last utterance Trigger Group
</figure>
<figureCaption confidence="0.995882">
Figure 6. Comparison of the group&apos;s aggregated
responses to trivial baselines.
</figureCaption>
<subsectionHeader confidence="0.996635">
5.5 Comparison to Other Work
</subsectionHeader>
<bodyText confidence="0.999986941176471">
Comparing these results to other work is
difficult because very little research focuses on
dialogue segmentation at this level of analysis.
Jonsson (1991) uses initiative-response pairs as
a part of a dialogue manager, but does not
attempt to recognise these segments explicitly.
Comparable statistics exist for a different
task, that of multiparty topic segmentation. WD
scores for this task fall consistently into the .25
range, with Galley et al. (2003) at .254, Hsueh et
al. (2006) at .283, and Purver et al. (2006)
at .284. We can only draw tenuous conclusions
between this task and our own, however this
does show the kind of scores we should be
expecting to see for a dialogue-level task. A
more similar project would help us to make a
more valid comparison.
</bodyText>
<sectionHeader confidence="0.999062" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99899925">
The discussion of results will follow the two
foci of the project: first, some comments about
the aggregation of the volunteer data, and then
some comments about the segmentation itself.
</bodyText>
<subsectionHeader confidence="0.998801">
6.1 Discussion of Aggregation
</subsectionHeader>
<bodyText confidence="0.99987875">
A combination of factors appear to have
contributed to the success of this method, some
involving the nature of the task itself, and some
involving the nature of aggregated group
opinion, which has been called `the wisdom of
crowds&apos; (for an informal introduction, see
Surowiecki 2004).
The fact that annotator responses were
aggregated means that no one annotator had to
perform particularly well. We noticed a range of
styles among our annotators. Some annotators
agreed very well with the expert opinion. A few
</bodyText>
<page confidence="0.991546">
902
</page>
<bodyText confidence="0.999895047619048">
annotators seemed to mark utterances in near-
random ways. Some `casual annotators&apos; seemed
to drop in, click only a few of the most obvious
boundaries in the dialogue, and then submit the
form. This kind of behaviour would give that
annotator a disastrous individual score, but
when aggregated, the work of the casual
annotator actually contributes to the overall
picture provided by the group. As long as the
wrong responses are randomly wrong, they do
not detract from the overall pattern and no
volunteers need to be dropped from the sample.
It may not be surprising that people with
language experience tend to arrive at more or
less the same judgments on this kind of task, or
that the aggregation of the group data would
normalise out the individual errors. What is
surprising is that the judgments of the group,
aggregated in this way, correspond more closely
to expert opinion than (in many cases) the best
individual annotators.
</bodyText>
<subsectionHeader confidence="0.999449">
6.2 Discussion of Segmentation
</subsectionHeader>
<bodyText confidence="0.845824392857143">
The concept of segmentation as described here,
including the description of nuclei and satellites,
appears to be one that annotators can grasp even
with minimal training.
The task of segmentation here is somewhat
different from other classification tasks.
Annotators were asked to find segment
boundaries, making this essentially a two-class
classification task where each utterance was
marked as either a boundary or not a boundary.
It may be easier for volunteers to cope with
fewer labels than with many, as is more common
in dialogue tasks. The comparatively low
perplexity would also help to ensure that
volunteers would see the annotation through.
One of the outcomes of seeing annotator
opinion was that we could examine and learn
from cases where the annotators voted
overwhelmingly contrary to expert opinion. This
MGT so what time should we meet
ADB &lt;uh&gt; well it doesn&apos;t matter
as long as we both checked
in I mean whenever we meet
is kind of irrelevant
ADB so maybe about try to
ADB you want to get some lunch
at the airport before we go
MGT that is a good idea
</bodyText>
<tableCaption confidence="0.990993">
Table 3. Example from a dialogue.
</tableCaption>
<bodyText confidence="0.9999205">
gave us a chance to learn from what the human
annotators thought about language. Even though
these results do not literally come from one
person, it is still interesting to look at the general
patterns suggested by these results.
`let&apos;s see&apos;: This utterance usually appears
near boundaries, but does it mark the end of a
segment, or the beginning of a new one? We
tended to place it at the end of the previous
segment, but human annotators showed a very
strong tendency to group it with the next
segment. This was despite an example on the
training page that suggested joining these
utterances with the previous segment.
Topic: The segments under study here are
different from topic. The segments tend to be
smaller, and they focus on the mechanics of the
exchanges rather than centering around one
topic to its conclusion. Even though the
annotators were asked to mark for adjacency
pairs, there was a distinct tendency to mark
longer units more closely pertaining to topic.
Table 3 shows one example. We had marked the
space between utterances 2 and 3 as a boundary;
volunteers ignored it. It was slightly more
common for annotators to omit our boundaries
than to suggest new ones. The average segment
length was 3.64 utterances for our volunteers,
compared with 3.59 utterances for experts.
Areas of uncertainty: At certain points on
the chart, opinion seemed to be split as one or
more potential boundaries presented themselves.
This seemed to happen most often when two or
more of the same speech act appeared
sequentially, e.g. two or more questions,
information-giving statements, or the like.
</bodyText>
<sectionHeader confidence="0.998007" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999222">
We drew a number of conclusions from this
study, both about the viability of our method,
and about the outcomes of the study itself.
First, it appears that for this task, aggregating
the responses from a large number of
anonymous volunteers is a valid method of
annotation. We would like to see if this pattern
holds for other kinds of classification tasks. If it
does, it could have tremendous implications for
dialogue-level annotation. Reliable results could
be obtained quickly and cheaply from large
numbers of volunteers over the Internet, without
the time, the expense, and the logistical
complexity of training. At present, however, it is
unclear whether this volunteer annotation
</bodyText>
<page confidence="0.930339833333333">
1
2
3
4
5
903
</page>
<bodyText confidence="0.99998596875">
technique could be extended to other
classification tasks. It is possible that the strong
agreement seen here would also be seen on any
two-class annotation problem. A retest is
underway with annotation for a different two-
class annotation set and for a multi-class task.
Second, it appears that the concept of
segmentation on the adjacency pair level, with
this description of nuclei and satellites, is one
that annotators can grasp even with minimal
training. We found very strong agreement
between the aggregated group answers and the
expert opinion.
We now have a sizable amount of
information from language users as to how they
perceive dialogue segmentation. Our next step is
to use these results as the corpus for a machine
learning task that can duplicate human
performance. We are considering the
Transformation-Based Learning algorithm,
which has been used successfully in NLP tasks
such as part of speech tagging (Brill 1995) and
dialogue act classification (Samuel 1998). TBL
is attractive because it allows one to start from a
marked up corpus (perhaps the Trigger, as the
best-performing trivial baseline), and improves
performance from there.
We also plan to use the information from the
segmentation to examine the structure of
segments, especially the sequences of dialogue
acts within them, with a view to improving a
dialogue act tagger.
</bodyText>
<sectionHeader confidence="0.998409" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9993638">
Thanks to Alan Dench and to T. Mark Ellison
for reviewing an early draft of this paper. We
especially wish to thank the individual
volunteers who contributed the data for this
research.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994156446153846">
Luis von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of
the SIGCHI Conference on Human Factors in
Computing Systems. pp. 319Ð326.
Jan Alexandersson, Bianka Buschbeck-Wolf,
Tsutomu Fujinami, Elisabeth Maier, Norbert
Reithinger, Birte Schmitz, and Melanie Siegel.
1997. Dialogue acts in VERBMOBIL-2.
Verbmobil Report 204, DFKI, University of
Saarbruecken.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21(4): 543Ð565.
Jean C. Carletta. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2): 249Ð254.
Herbert H. Clark and Edward F. Schaefer. 1989.
Contributing to discourse. Cognitive Science,
13:259Ð294.
Michael Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse
segmentation of multi-party conversation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pp.
562Ð569.
Pei-Yun Hsueh, Johanna Moore, and Steve Renals.
2006. Automatic segmentation of multiparty
dialogue. In Proceedings of the EACL 2006, pp.
273Ð280.
Arne Jonsson. 1991. A dialogue manager using
initiative-response units and distributed control. In
Proceedings of the Fifth Conference of the
European Association for Computational
Linguistics, pp. 233Ð238.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: A framework for the
analysis of texts. In IPRA Papers in Pragmatics 1:
1-21.
Lev Pevzner and Marti A. Hearst. 2002. A critique
and improvement of an evaluation metric for text
segmentation. Computational Linguistics, 28(1):
19Ð36.
Matthew Purver, Konrad P. Kording, Thomas L.
Griffiths, and Joshua B. Tenenbaum. 2006.
Unsupervised topic modelling for multi-party
spoken discourse. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the ACL,
pp. 17Ð24.
Harvey Sacks, Emanuel A. Schegloff, and Gail
Jefferson. 1974. A simplest systematics for the
organization of turn-taking for conversation.
Language, 50:696Ð735.
Ken Samuel, Sandra Carberry, and K. Vijay-Shanker.
1998. Dialogue act tagging with transformation-
based learning. In Proceedings of COLING/
ACL�98, pp. 1150Ð1156.
James Surowiecki. 2004. The wisdom of crowds:
Why the many are smarter than the few. Abacus:
London, UK.
Helen Wright, Massimo Poesio, and Stephen Isard.
1999. Using high level dialogue information for
dialogue act recognition using prosodic features.
In DIAPRO-1999, pp. 139Ð143.
</reference>
<page confidence="0.99857">
904
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.876496">
<title confidence="0.998961">Dialogue Segmentation with Large Numbers of Volunteer Internet Annotators</title>
<author confidence="0.999744">T Daniel Midgley</author>
<affiliation confidence="0.992467">Discipline of Linguistics, School of Computer Science and Software Engineering University of Western Australia</affiliation>
<address confidence="0.983691">Perth, Australia</address>
<email confidence="0.966327">Daniel.Midgley@uwa.edu.au</email>
<abstract confidence="0.995333125">This paper shows the results of an experiment in dialogue segmentation. In this experiment, segmentation was done on a of analysis similar to The method of annotation was somewhat novel: volunteers were invited to participate over the Web, and their responses were aggregated using a simple voting method. Though volunteers received a minimum of training, the aggregated responses of the group showed very high agreement with expert opinion. The group, as a unit, performed at the top of the list of annotators, and in many cases performed as well as or better than the best annotator.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.</booktitle>
<pages>319--326</pages>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. pp. 319Ð326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Alexandersson</author>
<author>Bianka Buschbeck-Wolf</author>
<author>Tsutomu Fujinami</author>
<author>Elisabeth Maier</author>
<author>Norbert Reithinger</author>
<author>Birte Schmitz</author>
<author>Melanie Siegel</author>
</authors>
<title>Dialogue acts in VERBMOBIL-2.</title>
<date>1997</date>
<tech>Verbmobil Report 204,</tech>
<institution>DFKI, University of Saarbruecken.</institution>
<marker>Alexandersson, Buschbeck-Wolf, Fujinami, Maier, Reithinger, Schmitz, Siegel, 1997</marker>
<rawString>Jan Alexandersson, Bianka Buschbeck-Wolf, Tsutomu Fujinami, Elisabeth Maier, Norbert Reithinger, Birte Schmitz, and Melanie Siegel. 1997. Dialogue acts in VERBMOBIL-2. Verbmobil Report 204, DFKI, University of Saarbruecken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>543--565</pages>
<contexts>
<context position="28575" citStr="Brill 1995" startWordPosition="4558" endWordPosition="4559">n the adjacency pair level, with this description of nuclei and satellites, is one that annotators can grasp even with minimal training. We found very strong agreement between the aggregated group answers and the expert opinion. We now have a sizable amount of information from language users as to how they perceive dialogue segmentation. Our next step is to use these results as the corpus for a machine learning task that can duplicate human performance. We are considering the Transformation-Based Learning algorithm, which has been used successfully in NLP tasks such as part of speech tagging (Brill 1995) and dialogue act classification (Samuel 1998). TBL is attractive because it allows one to start from a marked up corpus (perhaps the Trigger, as the best-performing trivial baseline), and improves performance from there. We also plan to use the information from the segmentation to examine the structure of segments, especially the sequences of dialogue acts within them, with a view to improving a dialogue act tagger. Acknowledgements Thanks to Alan Dench and to T. Mark Ellison for reviewing an early draft of this paper. We especially wish to thank the individual volunteers who contributed the </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4): 543Ð565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean C Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>249--254</pages>
<contexts>
<context position="11346" citStr="Carletta 1996" startWordPosition="1840" endWordPosition="1841">to half the average segment length. Within the window, the algorithm examines the number of segment boundaries in the reference and in the hypothesis, and a counter is augmented by one if they disagree. The WD score between the reference and the hypothesis is equal to the number of discrepancies divided by the number of measurements taken. A score of 0 would be given to two annotators who agree perfectly, and 1 would signify perfect disagreement. Figure 3 shows the WD scores for the volunteers. Most volunteers achieved a WD score between .15 and .2, with an average of .245. Cohen&apos;s Kappa (x) (Carletta 1996) is another method of compari ng inter-annotator agreement Number of annotators 150 1 120 90 60 30 899 150 125 100 75 50 25 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 WindowDiff range Figure 3. WD scores for individual responses. A score of 0 indicates perfect agreement. in segmentation that is widely used in computational language tasks. It measures the observed agreement (AO) against the agreement we should expect by chance (AE), as follows: AO - AE K = 1 - AE For segmentation tasks, x is a more stringent method than WindowDiff, as it does not consider near-misses. Even so, x scores are repor</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean C. Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2): 249Ð254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Edward F Schaefer</author>
</authors>
<title>Contributing to discourse. Cognitive Science,</title>
<date>1989</date>
<marker>Clark, Schaefer, 1989</marker>
<rawString>Herbert H. Clark and Edward F. Schaefer. 1989. Contributing to discourse. Cognitive Science, 13:259Ð294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Galley</author>
<author>Kathleen McKeown</author>
<author>Eric FoslerLussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>562--569</pages>
<contexts>
<context position="3366" citStr="Galley et al. 2003" startWordPosition="519" endWordPosition="522">e respondents meant that our results could be more readily generalised to language users as a whole. We expected that multiple users would converge upon some kind of uniform result. What we found (for this task at least) was that large numbers of volunteers show very strong tendencies that correspond well to expert opinion, and that these patterns of agreement are surprisingly resilient in the face of noisy input from some users. We also gained some insights into the way that people perceived dialogue segments. 2 Segmentation While much work in dialogue segmentation centers around topic (e.g. Galley et al. 2003, Hsueh et al. 2006, Purver et al. 2006), we decided to examine dialogue at a more finegrained level. The level of analysis that we have chosen corresponds most closely to adjacency pairs (after Sacks, Schegloff and Jefferson 1974), where a segment is made of matched sets of utterances from different speakers (e.g. question/answer or suggest/accept). We chose to segment dialogues this way in order to improve dialogue act tagging, and we think that 897 examining the back-and-forth detail of the mechanics of dialogue will be the most helpful level of analysis for this task. The back-and-forth na</context>
<context position="22175" citStr="Galley et al. (2003)" startWordPosition="3487" endWordPosition="3490">.108 0.1 0 Majority Random Last utterance Trigger Group Figure 6. Comparison of the group&apos;s aggregated responses to trivial baselines. 5.5 Comparison to Other Work Comparing these results to other work is difficult because very little research focuses on dialogue segmentation at this level of analysis. Jonsson (1991) uses initiative-response pairs as a part of a dialogue manager, but does not attempt to recognise these segments explicitly. Comparable statistics exist for a different task, that of multiparty topic segmentation. WD scores for this task fall consistently into the .25 range, with Galley et al. (2003) at .254, Hsueh et al. (2006) at .283, and Purver et al. (2006) at .284. We can only draw tenuous conclusions between this task and our own, however this does show the kind of scores we should be expecting to see for a dialogue-level task. A more similar project would help us to make a more valid comparison. 6 Discussion The discussion of results will follow the two foci of the project: first, some comments about the aggregation of the volunteer data, and then some comments about the segmentation itself. 6.1 Discussion of Aggregation A combination of factors appear to have contributed to the s</context>
</contexts>
<marker>Galley, McKeown, FoslerLussier, Jing, 2003</marker>
<rawString>Michael Galley, Kathleen McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 562Ð569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei-Yun Hsueh</author>
<author>Johanna Moore</author>
<author>Steve Renals</author>
</authors>
<title>Automatic segmentation of multiparty dialogue.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL</booktitle>
<pages>273--280</pages>
<contexts>
<context position="3385" citStr="Hsueh et al. 2006" startWordPosition="523" endWordPosition="526">that our results could be more readily generalised to language users as a whole. We expected that multiple users would converge upon some kind of uniform result. What we found (for this task at least) was that large numbers of volunteers show very strong tendencies that correspond well to expert opinion, and that these patterns of agreement are surprisingly resilient in the face of noisy input from some users. We also gained some insights into the way that people perceived dialogue segments. 2 Segmentation While much work in dialogue segmentation centers around topic (e.g. Galley et al. 2003, Hsueh et al. 2006, Purver et al. 2006), we decided to examine dialogue at a more finegrained level. The level of analysis that we have chosen corresponds most closely to adjacency pairs (after Sacks, Schegloff and Jefferson 1974), where a segment is made of matched sets of utterances from different speakers (e.g. question/answer or suggest/accept). We chose to segment dialogues this way in order to improve dialogue act tagging, and we think that 897 examining the back-and-forth detail of the mechanics of dialogue will be the most helpful level of analysis for this task. The back-and-forth nature of dialogue al</context>
<context position="22204" citStr="Hsueh et al. (2006)" startWordPosition="3493" endWordPosition="3496">t utterance Trigger Group Figure 6. Comparison of the group&apos;s aggregated responses to trivial baselines. 5.5 Comparison to Other Work Comparing these results to other work is difficult because very little research focuses on dialogue segmentation at this level of analysis. Jonsson (1991) uses initiative-response pairs as a part of a dialogue manager, but does not attempt to recognise these segments explicitly. Comparable statistics exist for a different task, that of multiparty topic segmentation. WD scores for this task fall consistently into the .25 range, with Galley et al. (2003) at .254, Hsueh et al. (2006) at .283, and Purver et al. (2006) at .284. We can only draw tenuous conclusions between this task and our own, however this does show the kind of scores we should be expecting to see for a dialogue-level task. A more similar project would help us to make a more valid comparison. 6 Discussion The discussion of results will follow the two foci of the project: first, some comments about the aggregation of the volunteer data, and then some comments about the segmentation itself. 6.1 Discussion of Aggregation A combination of factors appear to have contributed to the success of this method, some i</context>
</contexts>
<marker>Hsueh, Moore, Renals, 2006</marker>
<rawString>Pei-Yun Hsueh, Johanna Moore, and Steve Renals. 2006. Automatic segmentation of multiparty dialogue. In Proceedings of the EACL 2006, pp. 273Ð280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Jonsson</author>
</authors>
<title>A dialogue manager using initiative-response units and distributed control.</title>
<date>1991</date>
<booktitle>In Proceedings of the Fifth Conference of the European Association for Computational Linguistics,</booktitle>
<pages>233--238</pages>
<contexts>
<context position="21873" citStr="Jonsson (1991)" startWordPosition="3442" endWordPosition="3443"> to have more than four words is the start of a new segment. This method performed comparatively well, with WD = .210, very close to the average individual annotator score of .245. As mentioned, the aggregated annotator score was WD = .108. 0.6 0.551 0.544 0.5 0.4 WD scores 0.392 0.3 0.210 0.2 0.108 0.1 0 Majority Random Last utterance Trigger Group Figure 6. Comparison of the group&apos;s aggregated responses to trivial baselines. 5.5 Comparison to Other Work Comparing these results to other work is difficult because very little research focuses on dialogue segmentation at this level of analysis. Jonsson (1991) uses initiative-response pairs as a part of a dialogue manager, but does not attempt to recognise these segments explicitly. Comparable statistics exist for a different task, that of multiparty topic segmentation. WD scores for this task fall consistently into the .25 range, with Galley et al. (2003) at .254, Hsueh et al. (2006) at .283, and Purver et al. (2006) at .284. We can only draw tenuous conclusions between this task and our own, however this does show the kind of scores we should be expecting to see for a dialogue-level task. A more similar project would help us to make a more valid </context>
</contexts>
<marker>Jonsson, 1991</marker>
<rawString>Arne Jonsson. 1991. A dialogue manager using initiative-response units and distributed control. In Proceedings of the Fifth Conference of the European Association for Computational Linguistics, pp. 233Ð238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: A framework for the analysis of texts.</title>
<date>1987</date>
<booktitle>In IPRA Papers in Pragmatics</booktitle>
<volume>1</volume>
<pages>1--21</pages>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical structure theory: A framework for the analysis of texts. In IPRA Papers in Pragmatics 1: 1-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>19--36</pages>
<contexts>
<context position="10454" citStr="Pevzner and Hearst 2002" startWordPosition="1688" endWordPosition="1691"> (or in the replication, all eleven) dialogues. Sometimes more than eleven dialogues were submitted from the same location, most likely due to multiple users sharing a computer. 0 25 10 32 17 3 4 3 1 2 0 2 0 1 2 3 4 5 6 7 8 9 10 11 &gt;11 Number of dialogues completed Figure 2. Number of dialogues annotated by single IP addresses numbers). We collected between 32 an d 73 responses for each of the 15 dialogues. 3.3 Method of Evaluation In all, we received 626 responses from about 231 volunteers (though this is difficult to determine from only the volunteers&apos; IP We used the WindowDiff (WD) metric (Pevzner and Hearst 2002) to evaluate the responses of our volunteers against expert opinion (our responses). The WD algorithm calculates agreement between a reference copy of the corpus and a volunteer&apos;s hypothesis by moving a window over the utterances in the two corpora. The window has a size equal to half the average segment length. Within the window, the algorithm examines the number of segment boundaries in the reference and in the hypothesis, and a counter is augmented by one if they disagree. The WD score between the reference and the hypothesis is equal to the number of discrepancies divided by the number of </context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1): 19Ð36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Konrad P Kording</author>
<author>Thomas L Griffiths</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multi-party spoken discourse.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="3406" citStr="Purver et al. 2006" startWordPosition="527" endWordPosition="530">uld be more readily generalised to language users as a whole. We expected that multiple users would converge upon some kind of uniform result. What we found (for this task at least) was that large numbers of volunteers show very strong tendencies that correspond well to expert opinion, and that these patterns of agreement are surprisingly resilient in the face of noisy input from some users. We also gained some insights into the way that people perceived dialogue segments. 2 Segmentation While much work in dialogue segmentation centers around topic (e.g. Galley et al. 2003, Hsueh et al. 2006, Purver et al. 2006), we decided to examine dialogue at a more finegrained level. The level of analysis that we have chosen corresponds most closely to adjacency pairs (after Sacks, Schegloff and Jefferson 1974), where a segment is made of matched sets of utterances from different speakers (e.g. question/answer or suggest/accept). We chose to segment dialogues this way in order to improve dialogue act tagging, and we think that 897 examining the back-and-forth detail of the mechanics of dialogue will be the most helpful level of analysis for this task. The back-and-forth nature of dialogue also appears in Clark a</context>
<context position="22238" citStr="Purver et al. (2006)" startWordPosition="3500" endWordPosition="3503">6. Comparison of the group&apos;s aggregated responses to trivial baselines. 5.5 Comparison to Other Work Comparing these results to other work is difficult because very little research focuses on dialogue segmentation at this level of analysis. Jonsson (1991) uses initiative-response pairs as a part of a dialogue manager, but does not attempt to recognise these segments explicitly. Comparable statistics exist for a different task, that of multiparty topic segmentation. WD scores for this task fall consistently into the .25 range, with Galley et al. (2003) at .254, Hsueh et al. (2006) at .283, and Purver et al. (2006) at .284. We can only draw tenuous conclusions between this task and our own, however this does show the kind of scores we should be expecting to see for a dialogue-level task. A more similar project would help us to make a more valid comparison. 6 Discussion The discussion of results will follow the two foci of the project: first, some comments about the aggregation of the volunteer data, and then some comments about the segmentation itself. 6.1 Discussion of Aggregation A combination of factors appear to have contributed to the success of this method, some involving the nature of the task it</context>
</contexts>
<marker>Purver, Kording, Griffiths, Tenenbaum, 2006</marker>
<rawString>Matthew Purver, Konrad P. Kording, Thomas L. Griffiths, and Joshua B. Tenenbaum. 2006. Unsupervised topic modelling for multi-party spoken discourse. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp. 17Ð24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schegloff</author>
<author>Gail Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turn-taking for conversation.</title>
<date>1974</date>
<tech>Language, 50:696Ð735.</tech>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson. 1974. A simplest systematics for the organization of turn-taking for conversation. Language, 50:696Ð735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Samuel</author>
<author>Sandra Carberry</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Dialogue act tagging with transformationbased learning.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ ACL�98,</booktitle>
<pages>1150--1156</pages>
<marker>Samuel, Carberry, Vijay-Shanker, 1998</marker>
<rawString>Ken Samuel, Sandra Carberry, and K. Vijay-Shanker. 1998. Dialogue act tagging with transformationbased learning. In Proceedings of COLING/ ACL�98, pp. 1150Ð1156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Surowiecki</author>
</authors>
<title>The wisdom of crowds: Why the many are smarter than the few.</title>
<date>2004</date>
<publisher>Abacus: London, UK.</publisher>
<contexts>
<context position="22999" citStr="Surowiecki 2004" startWordPosition="3629" endWordPosition="3630"> see for a dialogue-level task. A more similar project would help us to make a more valid comparison. 6 Discussion The discussion of results will follow the two foci of the project: first, some comments about the aggregation of the volunteer data, and then some comments about the segmentation itself. 6.1 Discussion of Aggregation A combination of factors appear to have contributed to the success of this method, some involving the nature of the task itself, and some involving the nature of aggregated group opinion, which has been called `the wisdom of crowds&apos; (for an informal introduction, see Surowiecki 2004). The fact that annotator responses were aggregated means that no one annotator had to perform particularly well. We noticed a range of styles among our annotators. Some annotators agreed very well with the expert opinion. A few 902 annotators seemed to mark utterances in nearrandom ways. Some `casual annotators&apos; seemed to drop in, click only a few of the most obvious boundaries in the dialogue, and then submit the form. This kind of behaviour would give that annotator a disastrous individual score, but when aggregated, the work of the casual annotator actually contributes to the overall pictu</context>
</contexts>
<marker>Surowiecki, 2004</marker>
<rawString>James Surowiecki. 2004. The wisdom of crowds: Why the many are smarter than the few. Abacus: London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Wright</author>
<author>Massimo Poesio</author>
<author>Stephen Isard</author>
</authors>
<title>Using high level dialogue information for dialogue act recognition using prosodic features.</title>
<date>1999</date>
<booktitle>In DIAPRO-1999,</booktitle>
<pages>139--143</pages>
<marker>Wright, Poesio, Isard, 1999</marker>
<rawString>Helen Wright, Massimo Poesio, and Stephen Isard. 1999. Using high level dialogue information for dialogue act recognition using prosodic features. In DIAPRO-1999, pp. 139Ð143.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>