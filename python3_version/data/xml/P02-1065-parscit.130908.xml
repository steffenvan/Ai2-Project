<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000326">
<note confidence="0.95341">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 513-520.
</note>
<title confidence="0.92037">
Memory-Based Learning of Morphology with Stochastic Transducers
</title>
<author confidence="0.605008">
Alexander Clark
</author>
<affiliation confidence="0.7043285">
ISSCO / TIM
University of Geneva
</affiliation>
<address confidence="0.867674333333333">
UNI-MAIL, Boulevard du Pont-d’Arve,
CH-1211 Geneve 4,
Switzerland
</address>
<email confidence="0.997473">
Alex.Clark@issco.unige.ch
</email>
<sectionHeader confidence="0.995609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998874230769231">
This paper discusses the supervised learn-
ing of morphology using stochastic trans-
ducers, trained using the Expectation-
Maximization (EM) algorithm. Two ap-
proaches are presented: first, using the
transducers directly to model the process,
and secondly using them to define a sim-
ilarity measure, related to the Fisher ker-
nel method (Jaakkola and Haussler, 1998),
and then using a Memory-Based Learn-
ing (MBL) technique. These are evaluated
and compared on data sets from English,
German, Slovene and Arabic.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978052631579">
Finite-state methods are in large part adequate to
model morphological processes in many languages.
A standard methodology is that of two-level mor-
phology (Koskenniemi, 1983) which is capable of
handling the complexity of Finnish, though it needs
substantial extensions to handle non-concatenative
languages such as Arabic (Kiraz, 1994). These mod-
els are primarily concerned with the mapping from
deep lexical strings to surface strings, and within
this framework learning is in general difficult (Itai,
1994). In this paper I present algorithms for learn-
ing the finite-state transduction between pairs of un-
inflected and inflected words. – supervised learning
of morphology. The techniques presented here are,
however, applicable to learning other types of string
transductions.
Memory-based techniques, based on principles of
non-parametric density estimation, are a powerful
form of machine learning well-suited to natural lan-
guage tasks. A particular strength is their ability to
model both general rules and specific exceptions in
a single framework (van den Bosch and Daelemans,
1999).
However they have generally only been used in
supervised learning techniques where a class label or
tag has been associated to each feature vector. Given
these manual or semi-automatic class labels, a set of
features and a pre-defined distance function new in-
stances are classified according to the class label of
the closest instance. However these approaches are
not a complete solution to the problem of learning
morphology, since they do not directly produce the
transduction. The problem must first be converted
into an appropriate feature-based representation and
classified in some way. The techniques presented
here operate directly on sequences of atomic sym-
bols, using a much less articulated representation,
and much less input information.
</bodyText>
<sectionHeader confidence="0.986961" genericHeader="introduction">
2 Stochastic Transducers
</sectionHeader>
<bodyText confidence="0.991341086021506">
It is possible to apply the EM algorithm to learn the
parameters of stochastic transducers, (Ristad, 1997;
Casacuberta, 1995; Clark, 2001a). (Clark, 2001a)
showed how this approach could be used to learn
morphology by starting with a randomly initialized
model and using the EM algorithm to find a local
maximum of the joint probabilities over the pairs of
inflected and uninflected words. In addition rather
than using the EM algorithm to optimize the joint
probability it would be possible to use a gradient de-
scent algorithm to maximize the conditional proba-
bility.
The models used here are Stochastic Non-
Deterministic Finite-State Transducers (FST), or
Pair Hidden Markov Models (Durbin et al., 1998),
a name that emphasizes the similarity of the train-
ing algorithm to the well-known Forward-Backward
training algorithm for Hidden Markov Models.
Instead of outputting symbols in a single stream,
however, as in normal Hidden Markov Models they
output them on two separate streams, the left and
right streams. In general we could have different
left and right alphabets; here we assume they are the
same. At each transition the FST may output the
same symbol on both streams, a symbol on the left
stream only, or a symbol on the right stream only. I
call these , and outputs respectively. For
each state the sum of all these output parameters
over the alphabet must be one.
Since we are concerned with finite strings rather
than indefinite streams of symbols, we have in ad-
dition to the normal initial state , an explicit end
state , such that the FST terminates when it enters
this state. The FST then defines a joint probabil-
ity distribution on pairs of strings from the alphabet.
Though we are more interested in stochastic trans-
ductions, which are best represented by the condi-
tional probability of one string given the other, it is
more convenient to operate with models of the joint
probability, and then to derive the conditional prob-
ability as needed later on.
It is possible to modify the normal dynamic-
programming training algorithm for HMMs, the
Baum-Welch algorithm (Baum and Petrie, 1966) to
work with FSTs as well. This algorithm will maxi-
mize the joint probability of the training data.
We define the forward and backward proba-
bilities as follows. Given two strings
and we define the forward probabilities
as the probability that it will start from
and output on the left stream, and
on the right stream and be in state , and
the backward probabilities as the probability
that starting from state it will output
on the right and on the left and then
terminate, ie end in state .
We can calculate these using the following recur-
rence relations:
where, in these models, is zero un-
less is equal to . Instead of the normal two-
dimensional trellis discussed in standard works on
HMMs, which has one dimension corresponding to
the current state and one corresponding to the posi-
tion, we have a three-dimensional trellis, with a di-
mension for the position in each string. With these
modifications, we can use all of the standard HMM
algorithms. In particular, we can use this as the ba-
sis of a parameter estimation algorithm using the
expectation-maximization theorem. We use the for-
ward and backward probabilities to calculate the ex-
pected number of times each transition will be taken;
at each iteration we set the new values of the parame-
ters to be the appropriately normalized sums of these
expectations.
Given a FST, and a string , we often need to find
the string that maximizes . This is equiv-
alent to the task of finding the most likely string
generated by a HMM, which is NP-hard (Casacu-
berta and de la Higuera, 2000), but it is possible
to sample from the conditional distribution ,
which allows an efficient stochastic computation. If
we consider only what is output on the left stream,
the FST is equivalent to a HMM with null transitions
corresponding to the transitions of the FST. We
can remove these using standard techniques and then
use this to calculate the left backward probabilities
,
for a particular string : defined as the prob-
ability that starting from state the FST generates
on the left and terminates. Then if one
samples from the FST, but weights each transition by
the appropriate left backward probability, it will be
equivalent to sampling from the conditional distri-
</bodyText>
<listItem confidence="0.906633">
bution of . We can then find the string that
is most likely given , by generating randomly from
. After we have generated a number of strings,
</listItem>
<bodyText confidence="0.99328041025641">
we can sum for all the observed strings; if the
difference between this sum and 1 is less than the
maximum value of we know we have found
the most likely . In practice, the distributions we
are interested in often have a with ; in
this case we immediately know that we have found
the maximum.
We then model the morphological process as a
transduction from the lemma form to the inflected
form, and assume that the model outputs for each
input, the output with highest conditional or joint
probability with respect to the model. There are a
number of reasons why this simple approach will
not work: first, for many languages the inflected
form is lexically not phonologically specified and
thus the model will not be able to identify the cor-
rect form; secondly, modelling all of the irregular
exceptions in a single transduction is computation-
ally intractable at the moment. One way to improve
the efficiency is to use a mixture of models as dis-
cussed in (Clark, 2001a), each corresponding to a
morphological paradigm. The productivity of each
paradigm can be directly modelled, and the class of
each lexical item can again be memorized.
There are a number of criticisms that can be made
of this approach.
Many of the models produced merely memo-
rize a pair of strings – this is extremely ineffi-
cient.
Though the model correctly models the produc-
tivity of some morphological classes, it mod-
els this directly. A more satisfactory approach
would be to have this arise naturally as an emer-
gent property of other aspects of the model.
These models may not be able to account for
some psycho-linguistic evidence that appears
to require some form ofproximity or similarity.
In the next section I shall present a technique that
addresses these problems.
</bodyText>
<sectionHeader confidence="0.9720045" genericHeader="method">
3 Fisher Kernels and Information
Geometry
</sectionHeader>
<bodyText confidence="0.99995795">
The method used is a simple application of the infor-
mation geometry approach introduced by (Jaakkola
and Haussler, 1998) in the field of bio-informatics.
The central idea is to use a generative model to ex-
tract finite-dimensional features from a symbol se-
quence. Given a generative model for a string, one
can use the sufficient statistics of those generative
models as features. The vector of sufficient statis-
tics can be thought of as a finite-dimensional rep-
resentation of the sequence in terms of the model.
This transformation from an unbounded sequence of
atomic symbols to a finite-dimensional real vector is
very powerful and allows the use of Support Vec-
tor Machine techniques for classification. (Jaakkola
and Haussler, 1998) recommend that instead of us-
ing the sufficient statistics, that the Fisher scores are
used, together with an inner product derived from
the Fisher information matrix of the model. The
Fisher scores are defined for a data point and a
particular model as
</bodyText>
<equation confidence="0.915717">
(1)
</equation>
<bodyText confidence="0.96900315">
The partial derivative of the log likelihood is easy
to calculate as a byproduct of the E-step of the EM
algorithm, and has the value for HMMs (Jaakkola et
al., 2000) of
(2)
where is the indicator variable for the parameter
, and is the indicator value for the state where
leaves state ; the last term reflects the constraint
that the sum of the parameters must be one.
The kernel function is defined as
(3)
where is the Fisher information matrix.
This kernel function thus defines a distance be-
tween elements,
(4)
This distance in the feature space then defines a
pseudo-distance in the example space.
The name information geometry which is some-
times used to describe this approach derives from
a geometrical interpretation of this kernel. For a
parametric model with free parameters, the set of
all these models will form a smooth -dimensional
manifold in the space of all distributions. The curva-
ture of this manifold can be described by a Rieman-
nian tensor – this tensor is just the expected Fisher
information for that model. It is a tensor because
it transforms properly when the parametrization is
changed.
In spite of this compelling geometric explanation,
there are difficulties with using this approach di-
rectly. First, the Fisher information matrix cannot
be calculated directly, and secondly in natural lan-
guage applications, unlike in bio-informatic applica-
tions we have the perennial problem of data sparsity,
which means that unlikely events occur frequently.
This means that the scaling in the Fisher scores gives
extremely high weights to these rare events, which
can skew the results. Accordingly this work uses the
unscaled sufficient statistics. This is demonstrated
below.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="method">
4 Details
</sectionHeader>
<bodyText confidence="0.999928736842105">
Given a transducer that models the transduction
from uninflected to inflected words, we can ex-
tract the sufficient statistics from the model in two
ways. We can consider the statistics of the joint
model or the statistics of the conditional
model . Here we have used the condi-
tional model, since we are interested primarily in the
change of the stem, and not the parts of the stem that
remain unchanged. It is thus possible to use either
the features of the joint model or of the conditional
model, and it is also possible to either scale the fea-
tures or not, by dividing by the parameter value as
in Equation 2. The second term in Equation 2 cor-
responding to the normalization can be neglected.
We thus have four possible features that are com-
pared on one of the data sets in Table 4. Based on
the performance here we have chosen the unscaled
conditional sufficient statistics for the rest of the ex-
periments presented here, which are calculated thus:
</bodyText>
<page confidence="0.491912">
(5)
</page>
<tableCaption confidence="0.931001">
Table 1: Example of the MBL technique for the past
</tableCaption>
<bodyText confidence="0.995233864864865">
tense of apply (6pl3). This example shows that the
most likely transduction is the suffix Id, which is in-
correct, but the MBL approach gives the correct re-
sult in line 2.
Given an input string we want to find the string
such that the pair is very close to some ele-
ment of the training data. We can do this in a num-
ber of different ways. Clearly if is already in the
training set then the distance will be minimized by
choosing to be one of the outputs that is stored for
input ; the distance in this case will be zero. Other-
wise we sample repeatedly (here we have taken 100
samples) from the conditional distribution of each of
the submodels. This in practice seems to give good
results, though there are more principled criteria that
could be applied.
We give a concrete example using the LING En-
glish past tense data set described below. Given an
unseen verb in its base form, for example apply, in
phonetic transcription 6pl3, we generate 100 sam-
ples from the conditional distribution. The five most
likely of these are shown in Table 1, together with
the conditional probability, the distance to the clos-
est example and the closest example.
We are using a -nearest-neighbor rule with
, since there are irregular words that have com-
pletely idionsyncratic inflected forms. It would be
possible to use a larger value of , which might help
with robustness, particularly if the token frequency
was also used, since irregular words tend to be more
common.
In summary the algorithm proceeds as follows:
We train a small Stochastic Transducer on the
pairs of strings using the EM algorithm.
We derive from this model a distance function
between two pairs of strings that is sensitive to
the properties of this transduction.
</bodyText>
<figure confidence="0.821950454545455">
Closest
6pl3Id 0.313 1.46
6pl3d 0.223 0.678
6pld 0.0907 1.36
6pl3It 0.0884 1.67
6pl3t 0.0632 1.33
pl3 pl3d
s6pl3 s6pl3d
s6pl3 s6pl3d
p6f p6ft
p6f p6ft
</figure>
<bodyText confidence="0.970281833333333">
We store all of the observed pairs of strings.
Given a new word, we sample repeatedly from
the conditional distribution to get a set of pos-
sible outputs.
We select the output such that the input/output
pair is closest to one of the oberved pairs.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.986698">
5.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999617363636364">
The data sets used in the experiments are summa-
rized in Table 2. A few additional comments follow.
LING These are in UNIBET phonetic transcription.
EPT In SAMPA transcription. The training data
consists of all of the verbs with a non-zero
lemma spoken frequency in the 1.3 million
word CO-BUILD corpus. The test data consists
of all the remaining verbs. This is intended to
more accurately reflect the situation of an infant
learner.
GP This is a data set of pairs of German nouns
in singular and plural form prepared from the
CELEX lexical database.
NAKISA This is a data set prepared for (Plunkett
and Nakisa, 1997). Its consists of pairs of sin-
gular and plural nouns, in Modern Standard
Arabic, randomly selected from the standard
Wehr dictionary in a fully vocalized ASCII
transcription. It has a mixture of broken and
sound plurals, and has been simplified in the
sense that rare forms of the broken plural have
been removed.
</bodyText>
<subsectionHeader confidence="0.983588">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9995262">
Table 4 shows a comparison of the four possible fea-
ture sets on the Ling data. We used 10-fold cross
validation on all of these data sets apart from the
EPT data set, and the SLOVENE data set; in these
cases we averaged over 10 runs with different ran-
dom seeds. We compared the performance of the
models evaluated using them directly to model the
transduction using the conditional likelihood (CL)
and using the MBL approach with the unscaled con-
ditional features. Based on these results, we used
</bodyText>
<table confidence="0.994517333333333">
Unscaled Scaled
Joint 75.3 (3.5) 78.2 (3.6)
Conditional 85.8 (2.4) 23.8 (3.6)
</table>
<tableCaption confidence="0.730977">
Table 4: Comparison of different metrics on the
LING data set with 10 fold cross validation, 1 10-
state model trained with 10 iterations. Mean in %
with standard deviation in brackets.
</tableCaption>
<bodyText confidence="0.999940763157895">
the unscaled conditional features; subsequent exper-
iments confirmed that these performed best.
The results are summarized in Table 3. Run-times
for these experiments were from about 1 hour to 1
week on a current workstation. There are a few re-
sults to which these can be directly compared; on
the LING data set, (Mooney and Califf, 1995) re-
port figures of approximately 90% using a logic pro-
gram that learns decision lists for suffixes. For the
Arabic data sets, (Plunkett and Nakisa, 1997) do
not present results on modelling the transduction
on words not in the training set; however they re-
port scores of 63.8% (0.64%) using a neural network
classifier. The data is classified according to the type
of the plural, and is mapped onto a syllabic skele-
ton, with each phoneme represented as a bundle of
phonological features. for the data set SLOVENE,
(Manandhar et al., 1998) report scores of 97.4% for
FOIDL and 96.2% for CLOG. This uses a logic pro-
gramming methodology that specifically codes for
suffixation and prefixation alone. On the very large
and complex German data set, we score 70.6%; note
however that there is substantial disagreement be-
tween native speakers about the correct plural of
nonce words (K¨opcke, 1988). We observe that the
MBL approach significantly outperforms the condi-
tional likelihood method over a wide range of ex-
periments; the performance on the training data is a
further difference, the MBL approach scoring close
to 100%, whereas the CL approach scores only a lit-
tle better than it does on the test data. It is certainly
possible to make the conditional likelihood method
work rather better than it does in this paper by pay-
ing careful attention to convergence criteria of the
models to avoid overfitting, and by smoothing the
models carefully. In addition some sort of model
size selection must be used. A major advantage of
the MBL approach is that it works well without re-
</bodyText>
<table confidence="0.977745">
Label Language Source Description Total Size Train Test
LING English (Ling, 1994) Past tense 1394 1251 140
EPT English CELEX Past tense 5324 1957 3367
GP German CELEX noun plural 16970 15282 1706
NAKISA Arabic (Plunkett and Nakisa, 1997) plural 859 773 86
MCCARTHY Arabic (McCarthy and Prince, 1990) broken plural 3261 2633 293
SLOVENE Slovene (Manandhar et al., 1998) genitive nouns 921 608 313
</table>
<tableCaption confidence="0.977326">
Table 2: Summary of the data sets.
</tableCaption>
<table confidence="0.9999632">
Data Set CV Models States Iterations CL MBLSS
LING 10 1 10 10 61.3 (4.0) 85.8 (2.4)
10 2 10 10 72.1 (2.0) 79.3 (3.3)
EPT No 1 10 10 59.5 (9.4) 93.1 (2.1)
NAKISA 10 1 10 10 0.6 (0.8) 15.4 (3.8)
10 5 10 10 9.2 (2.9) 31.0 (6.1)
10 5 10 50 11.3 (3.3) 35.0 (5.3)
GP1 10 1 10 10 42.5 (0.8) 70.6 (0.8)
MCCARTHY 10 5 10 10 1.6 (0.6) 16.7 (1.8)
SLOVENE No 1 10 10 63.6 (28.6) 98.9 (0.8)
</table>
<tableCaption confidence="0.999367">
Table 3: Results. CV is the degree of cross-validation, Models determines how many components there
</tableCaption>
<bodyText confidence="0.959086904761905">
are in the mixture, CL gives the percentage correct using the conditional likelihood evaluation and MBLSS,
using the Memory-based learning with sufficient statistics, with the standard deviation in brackets.
quiring extensive tuning of the parameters.
In terms of the absolute quality of the results, this
depends to a great extent on how phonologically
predictable the process is. When it is completely
predictable, as in SLOVENE the performance ap-
proaches 100%; similarly a large majority of the
less frequent words in English are completely regu-
lar, and accordingly the performance on EPT is very
good. However in other cases, where the morphol-
ogy is very irregular the performance will be poor.
In particular with the Arabic data sets, the NAKISA
data set is very small compared to the complexity
of the process being learned, and the MCCARTHY
data set is rather noisy, with a large number of er-
roneous transcriptions. With the German data set,
though it is quite irregular, and the data set is not
frequency-weighted, so the frequent irregular words
are not more likely to be in the training data, there is
a lot of data, so the algorithm performs quite well.
</bodyText>
<subsectionHeader confidence="0.993646">
5.3 Cognitive Modelling
</subsectionHeader>
<bodyText confidence="0.99998552631579">
In addition to these formal evaluations we exam-
ined the extent to which this approach can account
for some psycho-linguistic data, in particular the
data collected by (Prasada and Pinker, 1993) on
the mild productivity of irregular forms in the En-
glish past tense. Space does not permit more than a
rather crude summary. They prepared six data sets
of 10 pairs of nonce words together with regular
and irregular plurals of them: a sequence of three
data sets that were similar to, but progressively fur-
ther away from sets of irregular verbs (prototypical-
intermediate- and distant- pseudoirregular – PPI IPI
and DPI), and another set that were similar to sets
of regular verbs (prototypical-, intermediate- and
distant- pseudoregular PPR, IPR and DPR). Thus
the first data sets contained words like spling which
would have a vowel change form of splung and a
regular suffixed form of splinged, and the second
data sets contained words like smeeb with regular
smeebed and irregular smeb. They asked subjects
for their opinions on the acceptabilities of the stems,
and of the regular (suffixed) and irregular (vowel
change) forms. A surprising result of this was that
subtracting the rating of the past tense form from
the rating of the stem form (in order to control for
the varying acceptability of the stem) gave differ-
ent results for the two data sets. With the pseudo-
irregular forms the irregular form got less acceptable
as the stems became less like the most similar irreg-
ular stems, but with the pseudo-regulars the regular
form got more acceptable. This was taken as evi-
dence for the presence of two qualitatively distinct
modules in human morphological processing.
In an attempt to see whether the models presented
here could account for these effects, we transcribed
the data into UNIBET transcription and tested it
with the models prepared for the LING data set. We
calculated the average negative log probability for
each of the six data sets in 3 ways: first we cal-
culated the probability of the stem alone to model
the acceptability of the stem; secondly we calcu-
lated the conditional probability of the regular (suf-
fixed form), and thirdly we calculated the condi-
tional probability of the irregular (vowel change)
form of the word. Then we calculated the differ-
ence between the figures for the appropriate past
tense form from the stem form. This is unjustifiable
in terms of probabilities but seems the most natu-
ral way of modelling the effects reported in (Prasada
and Pinker, 1993). These results are presented in Ta-
ble 5. Interestingly we observed the same effect: a
decrease in “acceptability” for irregulars, as they be-
came more distant, and the opposite effect for regu-
lars. In our case though it is clear why this happens
– the probability of the stem decreases rapidly, and
this overwhelms the mild decrease in the conditional
probability.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9996572">
The productivity of the regular forms is an emergent
property of the system. This is an advantage over
previous work using the EM algorithm with SFST,
which directly specified the productivity as a param-
eter.
</bodyText>
<sectionHeader confidence="0.928071" genericHeader="method">
6.1 Related work
</sectionHeader>
<bodyText confidence="0.999894714285714">
Using the EM algorithm to learn stochastic transduc-
ers has been known for a while in the biocomputing
field as a generalization of edit distance (Allison et
al., 1992). The Fisher kernel method has not been
used in NLP to our knowledge before though we
have noted two recent papers that have some points
of similarity. First, (Kazama et al., 2001) derive a
Maximum Entropy tagger, by training a HMM and
using the most likely state sequence of the HMM as
features for the Maximum Entropy tagging model.
Secondly, (van den Bosch, 2000) presents an ap-
proach that is again similar since it uses rules, in-
duced using a symbolic learning approach as fea-
tures in a nearest-neighbour approach.
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978615384615">
We have presented some algorithms for the super-
vised learning of morphology using the EM algo-
rithm applied to non-deterministic finite-state trans-
ducers.
We have shown that a novel Memory-based learn-
ing technique inspired by the Fisher kernel method
produces high performance in a wide range of lan-
guages without the need for fine-tuning of parame-
ters or language specific representations, and that it
can account for some psycho-linguistic data. These
techniques can also be applied to the unsupervised
learning of morphology, as described in (Clark,
2001b).
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999899285714286">
I am grateful to Prof. McCarthy, Ramin Nakisa and
Tomaz Erjavec for providing me with the data sets
used. Part of this work was done as part of the
TMR network Learning Computational Grammars.
Thanks also to Bill Keller, Gerald Gazdar, Chris
Manning, and the anonymous reviewers for helpful
comments.
</bodyText>
<sectionHeader confidence="0.998578" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.954196846153846">
L. Allison, C. S. Wallace, and C. N. Yee. 1992. Finite-
state models in the alignment of macro-molecules.
Journal ofMolecular Evolution, 35:77–89.
L. E. Baum and T. Petrie. 1966. Statistical inference for
probabilistic functions of finite state markov chains.
Annals ofMathematical Statistics, 37:1559–1663.
Francisco Casacuberta and Colin de la Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In Arlindo L. Oliveira,
editor, Grammatical Inference: Algorithms and Appli-
cations, pages 15–24. Springer Verlag.
F. Casacuberta. 1995. Probabilistic estimation of
stochastic regular syntax-directed translation schemes.
</reference>
<table confidence="0.999109857142857">
Data set Stem Suffix Vowel Change Past Tense - Stem
PPI 14.8 (0.08) 1.34 (0.04) 8.70 (0.30) -6.1
IPI 13.9 (0.12) 1.50 (0.13) 10.4 (0.31) -3.5
DPI 14.2 (0.34) 1.40 (0.07) 17.9 (2.12) 3.7
PPR 13.4 (0.34) 0.58 (0.08) 16.5 (2.18) -12.8
IPR 19.0 (0.22) 1.02 (0.13) 19.5 (2.22) -18.0
DPR 21.3 (0.14) 1.14 (0.17) 19.3 (0.94) -20.2
</table>
<tableCaption confidence="0.975441">
Table 5: Average negative log-likelihood in nats for the six data sets in (Prasada and Pinker, 1993). Larger
figures mean less likely. Standard deviations in brackets.
</tableCaption>
<reference confidence="0.999005126760564">
In Proceedings of the VIth Spanish Symposium on Pat-
tern Recognition and Image Analysis, pages 201–207.
Alexander Clark. 2001a. Learning morphology with
Pair Hidden Markov Models. In Proc. of the Stu-
dent Workshop at the 39th Annual Meeting of the As-
sociation for Computational Linguistics, pages 55–60,
Toulouse, France, July.
Alexander Clark. 2001b. Partially supervised learning of
morphology with stochastic transducers. In Proc. of
Natural Language Processing Pacific Rim Symposium,
NLPRS 2001, pages 341–348, Tokyo, Japan, Novem-
ber.
R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 1998.
Biological Sequence Analysis: Probabilistic Models
ofproteins and nucleic acids. Cambridge University
Press.
Alon Itai. 1994. Learning morphology – practice makes
good. In R. C. Carrasco and J. Oncina, editors, Gram-
matical Inference and Applications: ICGI-94, pages
5–15. Springer-Verlag.
T. S. Jaakkola and D. Haussler. 1998. Exploiting gener-
ative models in discriminative classifiers. In Proc. of
Tenth Conference on Advances in Neural Information
Processing Systems.
T. S. Jaakkola, M. Diekhans, and D. Haussler. 2000.
A discriminative framework for detecting remote pro-
tein homologies. Journal of Computational Biology,
7(1,2):95–114.
Jun’ichi Kazama, Yusuke Miyao, and Jun’ichi Tsujii.
2001. A maximum entropy tagger with unsupervised
hidden markov models. In Proc. ofNatural Language
Processing Pacific Rim Symposium (NLPRS 2001),
pages 333–340, Tokyo, Japan.
George Kiraz. 1994. Multi-tape two-level morphology.
In COLING-94, pages 180–186.
Klaus-Michael K¨opcke. 1988. Schemas in German plu-
ral formation. Lingua, 74:303–335.
Kimmo Koskenniemi. 1983. A Two-level Morphological
Processor. Ph.D. thesis, University of Helsinki.
Charles X. Ling. 1994. Learning the past tense of En-
glish verbs: The symbolic pattern associator vs. con-
nectionist models. Journal ofArtifical Intelligence Re-
search, 1:209–229.
S. Manandhar, S. Dzeroski, and T. Erjavec. 1998. Learn-
ing multi-lingual morphology with CLOG. In C. D.
Page, editor, Proc. of the 8th International Workshop
on Inductive Logic Programming (ILP-98). Springer
Verlag.
J. McCarthy and A. Prince. 1990. Foot and word in
prosodic morphology: The Arabic broken plural. Nat-
ural Language and Linguistic Theory, 8:209–284.
Raymond J. Mooney and Mary Elaine Califf. 1995. In-
duction of first-order decision lists: Results on learn-
ing the past tense of English verbs. Journal of Artifi-
cial Intelligence Research, 3:1–24.
Kim Plunkett and Ramin Charles Nakisa. 1997. A con-
nectionist model of the Arabic plural system. Lan-
guage and Cognitive Processes, 12(5/6):807–836.
Sandeep Prasada and Steven Pinker. 1993. Generalisa-
tion of regular and irregular morphological patterns.
Language and Cognitive Processes, 8(1):1–56.
Eric Sven Ristad. 1997. Finite growth models. Tech-
nical Report CS-TR-533-96, Department of Computer
Science, Princeton University. revised in 1997.
Antal van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, pages 285–292.
Antal van den Bosch. 2000. Using induced rules as com-
plex features in memory-based language learning. In
Proceedings of CoNLL 2000, pages 73–78.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.157804">
<note confidence="0.997525">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 513-520.</note>
<title confidence="0.998107">Memory-Based Learning of Morphology with Stochastic Transducers</title>
<author confidence="0.999004">Alexander Clark</author>
<affiliation confidence="0.980023">ISSCO / TIM University of Geneva UNI-MAIL, Boulevard du Pont-d’Arve,</affiliation>
<address confidence="0.714923">CH-1211 Geneve 4, Switzerland</address>
<email confidence="0.655931">Alex.Clark@issco.unige.ch</email>
<abstract confidence="0.984607153846154">This paper discusses the supervised learning of morphology using stochastic transducers, trained using the Expectation- Maximization (EM) algorithm. Two approaches are presented: first, using the transducers directly to model the process, and secondly using them to define a similarity measure, related to the Fisher kernel method (Jaakkola and Haussler, 1998), and then using a Memory-Based Learning (MBL) technique. These are evaluated and compared on data sets from English,</abstract>
<address confidence="0.527156">German, Slovene and Arabic.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Allison</author>
<author>C S Wallace</author>
<author>C N Yee</author>
</authors>
<title>Finitestate models in the alignment of macro-molecules.</title>
<date>1992</date>
<journal>Journal ofMolecular Evolution,</journal>
<pages>35--77</pages>
<contexts>
<context position="23811" citStr="Allison et al., 1992" startWordPosition="4010" endWordPosition="4013"> distant, and the opposite effect for regulars. In our case though it is clear why this happens – the probability of the stem decreases rapidly, and this overwhelms the mild decrease in the conditional probability. 6 Discussion The productivity of the regular forms is an emergent property of the system. This is an advantage over previous work using the EM algorithm with SFST, which directly specified the productivity as a parameter. 6.1 Related work Using the EM algorithm to learn stochastic transducers has been known for a while in the biocomputing field as a generalization of edit distance (Allison et al., 1992). The Fisher kernel method has not been used in NLP to our knowledge before though we have noted two recent papers that have some points of similarity. First, (Kazama et al., 2001) derive a Maximum Entropy tagger, by training a HMM and using the most likely state sequence of the HMM as features for the Maximum Entropy tagging model. Secondly, (van den Bosch, 2000) presents an approach that is again similar since it uses rules, induced using a symbolic learning approach as features in a nearest-neighbour approach. 7 Conclusion We have presented some algorithms for the supervised learning of mor</context>
</contexts>
<marker>Allison, Wallace, Yee, 1992</marker>
<rawString>L. Allison, C. S. Wallace, and C. N. Yee. 1992. Finitestate models in the alignment of macro-molecules. Journal ofMolecular Evolution, 35:77–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
<author>T Petrie</author>
</authors>
<title>Statistical inference for probabilistic functions of finite state markov chains. Annals ofMathematical Statistics,</title>
<date>1966</date>
<pages>37--1559</pages>
<contexts>
<context position="4839" citStr="Baum and Petrie, 1966" startWordPosition="748" endWordPosition="751"> to the normal initial state , an explicit end state , such that the FST terminates when it enters this state. The FST then defines a joint probability distribution on pairs of strings from the alphabet. Though we are more interested in stochastic transductions, which are best represented by the conditional probability of one string given the other, it is more convenient to operate with models of the joint probability, and then to derive the conditional probability as needed later on. It is possible to modify the normal dynamicprogramming training algorithm for HMMs, the Baum-Welch algorithm (Baum and Petrie, 1966) to work with FSTs as well. This algorithm will maximize the joint probability of the training data. We define the forward and backward probabilities as follows. Given two strings and we define the forward probabilities as the probability that it will start from and output on the left stream, and on the right stream and be in state , and the backward probabilities as the probability that starting from state it will output on the right and on the left and then terminate, ie end in state . We can calculate these using the following recurrence relations: where, in these models, is zero unless is </context>
</contexts>
<marker>Baum, Petrie, 1966</marker>
<rawString>L. E. Baum and T. Petrie. 1966. Statistical inference for probabilistic functions of finite state markov chains. Annals ofMathematical Statistics, 37:1559–1663.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Colin de la Higuera</author>
</authors>
<title>Computational complexity of problems on probabilistic grammars and transducers.</title>
<date>2000</date>
<booktitle>Grammatical Inference: Algorithms and Applications,</booktitle>
<pages>15--24</pages>
<editor>In Arlindo L. Oliveira, editor,</editor>
<publisher>Springer Verlag.</publisher>
<marker>Casacuberta, Higuera, 2000</marker>
<rawString>Francisco Casacuberta and Colin de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In Arlindo L. Oliveira, editor, Grammatical Inference: Algorithms and Applications, pages 15–24. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
</authors>
<title>Probabilistic estimation of stochastic regular syntax-directed translation schemes.</title>
<date>1995</date>
<booktitle>In Proceedings of the VIth Spanish Symposium on Pattern Recognition and Image Analysis,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="2859" citStr="Casacuberta, 1995" startWordPosition="418" endWordPosition="419">cording to the class label of the closest instance. However these approaches are not a complete solution to the problem of learning morphology, since they do not directly produce the transduction. The problem must first be converted into an appropriate feature-based representation and classified in some way. The techniques presented here operate directly on sequences of atomic symbols, using a much less articulated representation, and much less input information. 2 Stochastic Transducers It is possible to apply the EM algorithm to learn the parameters of stochastic transducers, (Ristad, 1997; Casacuberta, 1995; Clark, 2001a). (Clark, 2001a) showed how this approach could be used to learn morphology by starting with a randomly initialized model and using the EM algorithm to find a local maximum of the joint probabilities over the pairs of inflected and uninflected words. In addition rather than using the EM algorithm to optimize the joint probability it would be possible to use a gradient descent algorithm to maximize the conditional probability. The models used here are Stochastic NonDeterministic Finite-State Transducers (FST), or Pair Hidden Markov Models (Durbin et al., 1998), a name that emphas</context>
</contexts>
<marker>Casacuberta, 1995</marker>
<rawString>F. Casacuberta. 1995. Probabilistic estimation of stochastic regular syntax-directed translation schemes. In Proceedings of the VIth Spanish Symposium on Pattern Recognition and Image Analysis, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Learning morphology with Pair Hidden Markov Models.</title>
<date>2001</date>
<booktitle>In Proc. of the Student Workshop at the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>55--60</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="2872" citStr="Clark, 2001" startWordPosition="420" endWordPosition="421">s label of the closest instance. However these approaches are not a complete solution to the problem of learning morphology, since they do not directly produce the transduction. The problem must first be converted into an appropriate feature-based representation and classified in some way. The techniques presented here operate directly on sequences of atomic symbols, using a much less articulated representation, and much less input information. 2 Stochastic Transducers It is possible to apply the EM algorithm to learn the parameters of stochastic transducers, (Ristad, 1997; Casacuberta, 1995; Clark, 2001a). (Clark, 2001a) showed how this approach could be used to learn morphology by starting with a randomly initialized model and using the EM algorithm to find a local maximum of the joint probabilities over the pairs of inflected and uninflected words. In addition rather than using the EM algorithm to optimize the joint probability it would be possible to use a gradient descent algorithm to maximize the conditional probability. The models used here are Stochastic NonDeterministic Finite-State Transducers (FST), or Pair Hidden Markov Models (Durbin et al., 1998), a name that emphasizes the simi</context>
<context position="8154" citStr="Clark, 2001" startWordPosition="1335" endWordPosition="1336"> the lemma form to the inflected form, and assume that the model outputs for each input, the output with highest conditional or joint probability with respect to the model. There are a number of reasons why this simple approach will not work: first, for many languages the inflected form is lexically not phonologically specified and thus the model will not be able to identify the correct form; secondly, modelling all of the irregular exceptions in a single transduction is computationally intractable at the moment. One way to improve the efficiency is to use a mixture of models as discussed in (Clark, 2001a), each corresponding to a morphological paradigm. The productivity of each paradigm can be directly modelled, and the class of each lexical item can again be memorized. There are a number of criticisms that can be made of this approach. Many of the models produced merely memorize a pair of strings – this is extremely inefficient. Though the model correctly models the productivity of some morphological classes, it models this directly. A more satisfactory approach would be to have this arise naturally as an emergent property of other aspects of the model. These models may not be able to accou</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001a. Learning morphology with Pair Hidden Markov Models. In Proc. of the Student Workshop at the 39th Annual Meeting of the Association for Computational Linguistics, pages 55–60, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Partially supervised learning of morphology with stochastic transducers.</title>
<date>2001</date>
<booktitle>In Proc. of Natural Language Processing Pacific Rim Symposium, NLPRS</booktitle>
<pages>341--348</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="2872" citStr="Clark, 2001" startWordPosition="420" endWordPosition="421">s label of the closest instance. However these approaches are not a complete solution to the problem of learning morphology, since they do not directly produce the transduction. The problem must first be converted into an appropriate feature-based representation and classified in some way. The techniques presented here operate directly on sequences of atomic symbols, using a much less articulated representation, and much less input information. 2 Stochastic Transducers It is possible to apply the EM algorithm to learn the parameters of stochastic transducers, (Ristad, 1997; Casacuberta, 1995; Clark, 2001a). (Clark, 2001a) showed how this approach could be used to learn morphology by starting with a randomly initialized model and using the EM algorithm to find a local maximum of the joint probabilities over the pairs of inflected and uninflected words. In addition rather than using the EM algorithm to optimize the joint probability it would be possible to use a gradient descent algorithm to maximize the conditional probability. The models used here are Stochastic NonDeterministic Finite-State Transducers (FST), or Pair Hidden Markov Models (Durbin et al., 1998), a name that emphasizes the simi</context>
<context position="8154" citStr="Clark, 2001" startWordPosition="1335" endWordPosition="1336"> the lemma form to the inflected form, and assume that the model outputs for each input, the output with highest conditional or joint probability with respect to the model. There are a number of reasons why this simple approach will not work: first, for many languages the inflected form is lexically not phonologically specified and thus the model will not be able to identify the correct form; secondly, modelling all of the irregular exceptions in a single transduction is computationally intractable at the moment. One way to improve the efficiency is to use a mixture of models as discussed in (Clark, 2001a), each corresponding to a morphological paradigm. The productivity of each paradigm can be directly modelled, and the class of each lexical item can again be memorized. There are a number of criticisms that can be made of this approach. Many of the models produced merely memorize a pair of strings – this is extremely inefficient. Though the model correctly models the productivity of some morphological classes, it models this directly. A more satisfactory approach would be to have this arise naturally as an emergent property of other aspects of the model. These models may not be able to accou</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001b. Partially supervised learning of morphology with stochastic transducers. In Proc. of Natural Language Processing Pacific Rim Symposium, NLPRS 2001, pages 341–348, Tokyo, Japan, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Durbin</author>
<author>S Eddy</author>
<author>A Krogh</author>
<author>G Mitchison</author>
</authors>
<title>Biological Sequence Analysis: Probabilistic Models ofproteins and nucleic acids.</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3439" citStr="Durbin et al., 1998" startWordPosition="509" endWordPosition="512">ducers, (Ristad, 1997; Casacuberta, 1995; Clark, 2001a). (Clark, 2001a) showed how this approach could be used to learn morphology by starting with a randomly initialized model and using the EM algorithm to find a local maximum of the joint probabilities over the pairs of inflected and uninflected words. In addition rather than using the EM algorithm to optimize the joint probability it would be possible to use a gradient descent algorithm to maximize the conditional probability. The models used here are Stochastic NonDeterministic Finite-State Transducers (FST), or Pair Hidden Markov Models (Durbin et al., 1998), a name that emphasizes the similarity of the training algorithm to the well-known Forward-Backward training algorithm for Hidden Markov Models. Instead of outputting symbols in a single stream, however, as in normal Hidden Markov Models they output them on two separate streams, the left and right streams. In general we could have different left and right alphabets; here we assume they are the same. At each transition the FST may output the same symbol on both streams, a symbol on the left stream only, or a symbol on the right stream only. I call these , and outputs respectively. For each sta</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 1998. Biological Sequence Analysis: Probabilistic Models ofproteins and nucleic acids. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Itai</author>
</authors>
<title>Learning morphology – practice makes good. In</title>
<date>1994</date>
<booktitle>Grammatical Inference and Applications: ICGI-94,</booktitle>
<pages>5--15</pages>
<editor>R. C. Carrasco and J. Oncina, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1375" citStr="Itai, 1994" startWordPosition="196" endWordPosition="197">hese are evaluated and compared on data sets from English, German, Slovene and Arabic. 1 Introduction Finite-state methods are in large part adequate to model morphological processes in many languages. A standard methodology is that of two-level morphology (Koskenniemi, 1983) which is capable of handling the complexity of Finnish, though it needs substantial extensions to handle non-concatenative languages such as Arabic (Kiraz, 1994). These models are primarily concerned with the mapping from deep lexical strings to surface strings, and within this framework learning is in general difficult (Itai, 1994). In this paper I present algorithms for learning the finite-state transduction between pairs of uninflected and inflected words. – supervised learning of morphology. The techniques presented here are, however, applicable to learning other types of string transductions. Memory-based techniques, based on principles of non-parametric density estimation, are a powerful form of machine learning well-suited to natural language tasks. A particular strength is their ability to model both general rules and specific exceptions in a single framework (van den Bosch and Daelemans, 1999). However they have</context>
</contexts>
<marker>Itai, 1994</marker>
<rawString>Alon Itai. 1994. Learning morphology – practice makes good. In R. C. Carrasco and J. Oncina, editors, Grammatical Inference and Applications: ICGI-94, pages 5–15. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T S Jaakkola</author>
<author>D Haussler</author>
</authors>
<title>Exploiting generative models in discriminative classifiers.</title>
<date>1998</date>
<booktitle>In Proc. of Tenth Conference on Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="705" citStr="Jaakkola and Haussler, 1998" startWordPosition="94" endWordPosition="97">inguistics (ACL), Philadelphia, July 2002, pp. 513-520. Memory-Based Learning of Morphology with Stochastic Transducers Alexander Clark ISSCO / TIM University of Geneva UNI-MAIL, Boulevard du Pont-d’Arve, CH-1211 Geneve 4, Switzerland Alex.Clark@issco.unige.ch Abstract This paper discusses the supervised learning of morphology using stochastic transducers, trained using the ExpectationMaximization (EM) algorithm. Two approaches are presented: first, using the transducers directly to model the process, and secondly using them to define a similarity measure, related to the Fisher kernel method (Jaakkola and Haussler, 1998), and then using a Memory-Based Learning (MBL) technique. These are evaluated and compared on data sets from English, German, Slovene and Arabic. 1 Introduction Finite-state methods are in large part adequate to model morphological processes in many languages. A standard methodology is that of two-level morphology (Koskenniemi, 1983) which is capable of handling the complexity of Finnish, though it needs substantial extensions to handle non-concatenative languages such as Arabic (Kiraz, 1994). These models are primarily concerned with the mapping from deep lexical strings to surface strings, a</context>
<context position="9095" citStr="Jaakkola and Haussler, 1998" startWordPosition="1488" endWordPosition="1491">tremely inefficient. Though the model correctly models the productivity of some morphological classes, it models this directly. A more satisfactory approach would be to have this arise naturally as an emergent property of other aspects of the model. These models may not be able to account for some psycho-linguistic evidence that appears to require some form ofproximity or similarity. In the next section I shall present a technique that addresses these problems. 3 Fisher Kernels and Information Geometry The method used is a simple application of the information geometry approach introduced by (Jaakkola and Haussler, 1998) in the field of bio-informatics. The central idea is to use a generative model to extract finite-dimensional features from a symbol sequence. Given a generative model for a string, one can use the sufficient statistics of those generative models as features. The vector of sufficient statistics can be thought of as a finite-dimensional representation of the sequence in terms of the model. This transformation from an unbounded sequence of atomic symbols to a finite-dimensional real vector is very powerful and allows the use of Support Vector Machine techniques for classification. (Jaakkola and </context>
</contexts>
<marker>Jaakkola, Haussler, 1998</marker>
<rawString>T. S. Jaakkola and D. Haussler. 1998. Exploiting generative models in discriminative classifiers. In Proc. of Tenth Conference on Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T S Jaakkola</author>
<author>M Diekhans</author>
<author>D Haussler</author>
</authors>
<title>A discriminative framework for detecting remote protein homologies.</title>
<date>2000</date>
<journal>Journal of Computational Biology,</journal>
<pages>7--1</pages>
<contexts>
<context position="10134" citStr="Jaakkola et al., 2000" startWordPosition="1662" endWordPosition="1665">an unbounded sequence of atomic symbols to a finite-dimensional real vector is very powerful and allows the use of Support Vector Machine techniques for classification. (Jaakkola and Haussler, 1998) recommend that instead of using the sufficient statistics, that the Fisher scores are used, together with an inner product derived from the Fisher information matrix of the model. The Fisher scores are defined for a data point and a particular model as (1) The partial derivative of the log likelihood is easy to calculate as a byproduct of the E-step of the EM algorithm, and has the value for HMMs (Jaakkola et al., 2000) of (2) where is the indicator variable for the parameter , and is the indicator value for the state where leaves state ; the last term reflects the constraint that the sum of the parameters must be one. The kernel function is defined as (3) where is the Fisher information matrix. This kernel function thus defines a distance between elements, (4) This distance in the feature space then defines a pseudo-distance in the example space. The name information geometry which is sometimes used to describe this approach derives from a geometrical interpretation of this kernel. For a parametric model wi</context>
</contexts>
<marker>Jaakkola, Diekhans, Haussler, 2000</marker>
<rawString>T. S. Jaakkola, M. Diekhans, and D. Haussler. 2000. A discriminative framework for detecting remote protein homologies. Journal of Computational Biology, 7(1,2):95–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A maximum entropy tagger with unsupervised hidden markov models.</title>
<date>2001</date>
<booktitle>In Proc. ofNatural Language Processing Pacific Rim Symposium (NLPRS</booktitle>
<pages>333--340</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="23991" citStr="Kazama et al., 2001" startWordPosition="4042" endWordPosition="4045">e in the conditional probability. 6 Discussion The productivity of the regular forms is an emergent property of the system. This is an advantage over previous work using the EM algorithm with SFST, which directly specified the productivity as a parameter. 6.1 Related work Using the EM algorithm to learn stochastic transducers has been known for a while in the biocomputing field as a generalization of edit distance (Allison et al., 1992). The Fisher kernel method has not been used in NLP to our knowledge before though we have noted two recent papers that have some points of similarity. First, (Kazama et al., 2001) derive a Maximum Entropy tagger, by training a HMM and using the most likely state sequence of the HMM as features for the Maximum Entropy tagging model. Secondly, (van den Bosch, 2000) presents an approach that is again similar since it uses rules, induced using a symbolic learning approach as features in a nearest-neighbour approach. 7 Conclusion We have presented some algorithms for the supervised learning of morphology using the EM algorithm applied to non-deterministic finite-state transducers. We have shown that a novel Memory-based learning technique inspired by the Fisher kernel metho</context>
</contexts>
<marker>Kazama, Miyao, Tsujii, 2001</marker>
<rawString>Jun’ichi Kazama, Yusuke Miyao, and Jun’ichi Tsujii. 2001. A maximum entropy tagger with unsupervised hidden markov models. In Proc. ofNatural Language Processing Pacific Rim Symposium (NLPRS 2001), pages 333–340, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Kiraz</author>
</authors>
<title>Multi-tape two-level morphology.</title>
<date>1994</date>
<booktitle>In COLING-94,</booktitle>
<pages>180--186</pages>
<contexts>
<context position="1202" citStr="Kiraz, 1994" startWordPosition="169" endWordPosition="170">condly using them to define a similarity measure, related to the Fisher kernel method (Jaakkola and Haussler, 1998), and then using a Memory-Based Learning (MBL) technique. These are evaluated and compared on data sets from English, German, Slovene and Arabic. 1 Introduction Finite-state methods are in large part adequate to model morphological processes in many languages. A standard methodology is that of two-level morphology (Koskenniemi, 1983) which is capable of handling the complexity of Finnish, though it needs substantial extensions to handle non-concatenative languages such as Arabic (Kiraz, 1994). These models are primarily concerned with the mapping from deep lexical strings to surface strings, and within this framework learning is in general difficult (Itai, 1994). In this paper I present algorithms for learning the finite-state transduction between pairs of uninflected and inflected words. – supervised learning of morphology. The techniques presented here are, however, applicable to learning other types of string transductions. Memory-based techniques, based on principles of non-parametric density estimation, are a powerful form of machine learning well-suited to natural language t</context>
</contexts>
<marker>Kiraz, 1994</marker>
<rawString>George Kiraz. 1994. Multi-tape two-level morphology. In COLING-94, pages 180–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Michael K¨opcke</author>
</authors>
<date>1988</date>
<booktitle>Schemas in German plural formation. Lingua,</booktitle>
<pages>74--303</pages>
<marker>K¨opcke, 1988</marker>
<rawString>Klaus-Michael K¨opcke. 1988. Schemas in German plural formation. Lingua, 74:303–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>A Two-level Morphological Processor.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<contexts>
<context position="1040" citStr="Koskenniemi, 1983" startWordPosition="146" endWordPosition="147"> transducers, trained using the ExpectationMaximization (EM) algorithm. Two approaches are presented: first, using the transducers directly to model the process, and secondly using them to define a similarity measure, related to the Fisher kernel method (Jaakkola and Haussler, 1998), and then using a Memory-Based Learning (MBL) technique. These are evaluated and compared on data sets from English, German, Slovene and Arabic. 1 Introduction Finite-state methods are in large part adequate to model morphological processes in many languages. A standard methodology is that of two-level morphology (Koskenniemi, 1983) which is capable of handling the complexity of Finnish, though it needs substantial extensions to handle non-concatenative languages such as Arabic (Kiraz, 1994). These models are primarily concerned with the mapping from deep lexical strings to surface strings, and within this framework learning is in general difficult (Itai, 1994). In this paper I present algorithms for learning the finite-state transduction between pairs of uninflected and inflected words. – supervised learning of morphology. The techniques presented here are, however, applicable to learning other types of string transduct</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. A Two-level Morphological Processor. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles X Ling</author>
</authors>
<title>Learning the past tense of English verbs: The symbolic pattern associator vs. connectionist models.</title>
<date>1994</date>
<journal>Journal ofArtifical Intelligence Research,</journal>
<pages>1--209</pages>
<contexts>
<context position="18526" citStr="Ling, 1994" startWordPosition="3106" endWordPosition="3107">aining data is a further difference, the MBL approach scoring close to 100%, whereas the CL approach scores only a little better than it does on the test data. It is certainly possible to make the conditional likelihood method work rather better than it does in this paper by paying careful attention to convergence criteria of the models to avoid overfitting, and by smoothing the models carefully. In addition some sort of model size selection must be used. A major advantage of the MBL approach is that it works well without reLabel Language Source Description Total Size Train Test LING English (Ling, 1994) Past tense 1394 1251 140 EPT English CELEX Past tense 5324 1957 3367 GP German CELEX noun plural 16970 15282 1706 NAKISA Arabic (Plunkett and Nakisa, 1997) plural 859 773 86 MCCARTHY Arabic (McCarthy and Prince, 1990) broken plural 3261 2633 293 SLOVENE Slovene (Manandhar et al., 1998) genitive nouns 921 608 313 Table 2: Summary of the data sets. Data Set CV Models States Iterations CL MBLSS LING 10 1 10 10 61.3 (4.0) 85.8 (2.4) 10 2 10 10 72.1 (2.0) 79.3 (3.3) EPT No 1 10 10 59.5 (9.4) 93.1 (2.1) NAKISA 10 1 10 10 0.6 (0.8) 15.4 (3.8) 10 5 10 10 9.2 (2.9) 31.0 (6.1) 10 5 10 50 11.3 (3.3) 35.</context>
</contexts>
<marker>Ling, 1994</marker>
<rawString>Charles X. Ling. 1994. Learning the past tense of English verbs: The symbolic pattern associator vs. connectionist models. Journal ofArtifical Intelligence Research, 1:209–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Manandhar</author>
<author>S Dzeroski</author>
<author>T Erjavec</author>
</authors>
<title>Learning multi-lingual morphology with CLOG.</title>
<date>1998</date>
<booktitle>Proc. of the 8th International Workshop on Inductive Logic Programming (ILP-98).</booktitle>
<editor>In C. D. Page, editor,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="17408" citStr="Manandhar et al., 1998" startWordPosition="2917" endWordPosition="2920">which these can be directly compared; on the LING data set, (Mooney and Califf, 1995) report figures of approximately 90% using a logic program that learns decision lists for suffixes. For the Arabic data sets, (Plunkett and Nakisa, 1997) do not present results on modelling the transduction on words not in the training set; however they report scores of 63.8% (0.64%) using a neural network classifier. The data is classified according to the type of the plural, and is mapped onto a syllabic skeleton, with each phoneme represented as a bundle of phonological features. for the data set SLOVENE, (Manandhar et al., 1998) report scores of 97.4% for FOIDL and 96.2% for CLOG. This uses a logic programming methodology that specifically codes for suffixation and prefixation alone. On the very large and complex German data set, we score 70.6%; note however that there is substantial disagreement between native speakers about the correct plural of nonce words (K¨opcke, 1988). We observe that the MBL approach significantly outperforms the conditional likelihood method over a wide range of experiments; the performance on the training data is a further difference, the MBL approach scoring close to 100%, whereas the CL a</context>
<context position="18813" citStr="Manandhar et al., 1998" startWordPosition="3152" endWordPosition="3155"> paying careful attention to convergence criteria of the models to avoid overfitting, and by smoothing the models carefully. In addition some sort of model size selection must be used. A major advantage of the MBL approach is that it works well without reLabel Language Source Description Total Size Train Test LING English (Ling, 1994) Past tense 1394 1251 140 EPT English CELEX Past tense 5324 1957 3367 GP German CELEX noun plural 16970 15282 1706 NAKISA Arabic (Plunkett and Nakisa, 1997) plural 859 773 86 MCCARTHY Arabic (McCarthy and Prince, 1990) broken plural 3261 2633 293 SLOVENE Slovene (Manandhar et al., 1998) genitive nouns 921 608 313 Table 2: Summary of the data sets. Data Set CV Models States Iterations CL MBLSS LING 10 1 10 10 61.3 (4.0) 85.8 (2.4) 10 2 10 10 72.1 (2.0) 79.3 (3.3) EPT No 1 10 10 59.5 (9.4) 93.1 (2.1) NAKISA 10 1 10 10 0.6 (0.8) 15.4 (3.8) 10 5 10 10 9.2 (2.9) 31.0 (6.1) 10 5 10 50 11.3 (3.3) 35.0 (5.3) GP1 10 1 10 10 42.5 (0.8) 70.6 (0.8) MCCARTHY 10 5 10 10 1.6 (0.6) 16.7 (1.8) SLOVENE No 1 10 10 63.6 (28.6) 98.9 (0.8) Table 3: Results. CV is the degree of cross-validation, Models determines how many components there are in the mixture, CL gives the percentage correct using t</context>
</contexts>
<marker>Manandhar, Dzeroski, Erjavec, 1998</marker>
<rawString>S. Manandhar, S. Dzeroski, and T. Erjavec. 1998. Learning multi-lingual morphology with CLOG. In C. D. Page, editor, Proc. of the 8th International Workshop on Inductive Logic Programming (ILP-98). Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>A Prince</author>
</authors>
<title>Foot and word in prosodic morphology: The Arabic broken plural. Natural Language and Linguistic Theory,</title>
<date>1990</date>
<pages>8--209</pages>
<contexts>
<context position="18744" citStr="McCarthy and Prince, 1990" startWordPosition="3141" endWordPosition="3144">ional likelihood method work rather better than it does in this paper by paying careful attention to convergence criteria of the models to avoid overfitting, and by smoothing the models carefully. In addition some sort of model size selection must be used. A major advantage of the MBL approach is that it works well without reLabel Language Source Description Total Size Train Test LING English (Ling, 1994) Past tense 1394 1251 140 EPT English CELEX Past tense 5324 1957 3367 GP German CELEX noun plural 16970 15282 1706 NAKISA Arabic (Plunkett and Nakisa, 1997) plural 859 773 86 MCCARTHY Arabic (McCarthy and Prince, 1990) broken plural 3261 2633 293 SLOVENE Slovene (Manandhar et al., 1998) genitive nouns 921 608 313 Table 2: Summary of the data sets. Data Set CV Models States Iterations CL MBLSS LING 10 1 10 10 61.3 (4.0) 85.8 (2.4) 10 2 10 10 72.1 (2.0) 79.3 (3.3) EPT No 1 10 10 59.5 (9.4) 93.1 (2.1) NAKISA 10 1 10 10 0.6 (0.8) 15.4 (3.8) 10 5 10 10 9.2 (2.9) 31.0 (6.1) 10 5 10 50 11.3 (3.3) 35.0 (5.3) GP1 10 1 10 10 42.5 (0.8) 70.6 (0.8) MCCARTHY 10 5 10 10 1.6 (0.6) 16.7 (1.8) SLOVENE No 1 10 10 63.6 (28.6) 98.9 (0.8) Table 3: Results. CV is the degree of cross-validation, Models determines how many compone</context>
</contexts>
<marker>McCarthy, Prince, 1990</marker>
<rawString>J. McCarthy and A. Prince. 1990. Foot and word in prosodic morphology: The Arabic broken plural. Natural Language and Linguistic Theory, 8:209–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
<author>Mary Elaine Califf</author>
</authors>
<title>Induction of first-order decision lists: Results on learning the past tense of English verbs.</title>
<date>1995</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>3--1</pages>
<contexts>
<context position="16870" citStr="Mooney and Califf, 1995" startWordPosition="2825" endWordPosition="2828">these results, we used Unscaled Scaled Joint 75.3 (3.5) 78.2 (3.6) Conditional 85.8 (2.4) 23.8 (3.6) Table 4: Comparison of different metrics on the LING data set with 10 fold cross validation, 1 10- state model trained with 10 iterations. Mean in % with standard deviation in brackets. the unscaled conditional features; subsequent experiments confirmed that these performed best. The results are summarized in Table 3. Run-times for these experiments were from about 1 hour to 1 week on a current workstation. There are a few results to which these can be directly compared; on the LING data set, (Mooney and Califf, 1995) report figures of approximately 90% using a logic program that learns decision lists for suffixes. For the Arabic data sets, (Plunkett and Nakisa, 1997) do not present results on modelling the transduction on words not in the training set; however they report scores of 63.8% (0.64%) using a neural network classifier. The data is classified according to the type of the plural, and is mapped onto a syllabic skeleton, with each phoneme represented as a bundle of phonological features. for the data set SLOVENE, (Manandhar et al., 1998) report scores of 97.4% for FOIDL and 96.2% for CLOG. This use</context>
</contexts>
<marker>Mooney, Califf, 1995</marker>
<rawString>Raymond J. Mooney and Mary Elaine Califf. 1995. Induction of first-order decision lists: Results on learning the past tense of English verbs. Journal of Artificial Intelligence Research, 3:1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Plunkett</author>
<author>Ramin Charles Nakisa</author>
</authors>
<title>A connectionist model of the Arabic plural system.</title>
<date>1997</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>12--5</pages>
<contexts>
<context position="15443" citStr="Plunkett and Nakisa, 1997" startWordPosition="2581" endWordPosition="2584">ata Sets The data sets used in the experiments are summarized in Table 2. A few additional comments follow. LING These are in UNIBET phonetic transcription. EPT In SAMPA transcription. The training data consists of all of the verbs with a non-zero lemma spoken frequency in the 1.3 million word CO-BUILD corpus. The test data consists of all the remaining verbs. This is intended to more accurately reflect the situation of an infant learner. GP This is a data set of pairs of German nouns in singular and plural form prepared from the CELEX lexical database. NAKISA This is a data set prepared for (Plunkett and Nakisa, 1997). Its consists of pairs of singular and plural nouns, in Modern Standard Arabic, randomly selected from the standard Wehr dictionary in a fully vocalized ASCII transcription. It has a mixture of broken and sound plurals, and has been simplified in the sense that rare forms of the broken plural have been removed. 5.2 Evaluation Table 4 shows a comparison of the four possible feature sets on the Ling data. We used 10-fold cross validation on all of these data sets apart from the EPT data set, and the SLOVENE data set; in these cases we averaged over 10 runs with different random seeds. We compar</context>
<context position="17023" citStr="Plunkett and Nakisa, 1997" startWordPosition="2851" endWordPosition="2854">NG data set with 10 fold cross validation, 1 10- state model trained with 10 iterations. Mean in % with standard deviation in brackets. the unscaled conditional features; subsequent experiments confirmed that these performed best. The results are summarized in Table 3. Run-times for these experiments were from about 1 hour to 1 week on a current workstation. There are a few results to which these can be directly compared; on the LING data set, (Mooney and Califf, 1995) report figures of approximately 90% using a logic program that learns decision lists for suffixes. For the Arabic data sets, (Plunkett and Nakisa, 1997) do not present results on modelling the transduction on words not in the training set; however they report scores of 63.8% (0.64%) using a neural network classifier. The data is classified according to the type of the plural, and is mapped onto a syllabic skeleton, with each phoneme represented as a bundle of phonological features. for the data set SLOVENE, (Manandhar et al., 1998) report scores of 97.4% for FOIDL and 96.2% for CLOG. This uses a logic programming methodology that specifically codes for suffixation and prefixation alone. On the very large and complex German data set, we score </context>
<context position="18682" citStr="Plunkett and Nakisa, 1997" startWordPosition="3131" endWordPosition="3134"> on the test data. It is certainly possible to make the conditional likelihood method work rather better than it does in this paper by paying careful attention to convergence criteria of the models to avoid overfitting, and by smoothing the models carefully. In addition some sort of model size selection must be used. A major advantage of the MBL approach is that it works well without reLabel Language Source Description Total Size Train Test LING English (Ling, 1994) Past tense 1394 1251 140 EPT English CELEX Past tense 5324 1957 3367 GP German CELEX noun plural 16970 15282 1706 NAKISA Arabic (Plunkett and Nakisa, 1997) plural 859 773 86 MCCARTHY Arabic (McCarthy and Prince, 1990) broken plural 3261 2633 293 SLOVENE Slovene (Manandhar et al., 1998) genitive nouns 921 608 313 Table 2: Summary of the data sets. Data Set CV Models States Iterations CL MBLSS LING 10 1 10 10 61.3 (4.0) 85.8 (2.4) 10 2 10 10 72.1 (2.0) 79.3 (3.3) EPT No 1 10 10 59.5 (9.4) 93.1 (2.1) NAKISA 10 1 10 10 0.6 (0.8) 15.4 (3.8) 10 5 10 10 9.2 (2.9) 31.0 (6.1) 10 5 10 50 11.3 (3.3) 35.0 (5.3) GP1 10 1 10 10 42.5 (0.8) 70.6 (0.8) MCCARTHY 10 5 10 10 1.6 (0.6) 16.7 (1.8) SLOVENE No 1 10 10 63.6 (28.6) 98.9 (0.8) Table 3: Results. CV is the </context>
</contexts>
<marker>Plunkett, Nakisa, 1997</marker>
<rawString>Kim Plunkett and Ramin Charles Nakisa. 1997. A connectionist model of the Arabic plural system. Language and Cognitive Processes, 12(5/6):807–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandeep Prasada</author>
<author>Steven Pinker</author>
</authors>
<title>Generalisation of regular and irregular morphological patterns.</title>
<date>1993</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="20736" citStr="Prasada and Pinker, 1993" startWordPosition="3492" endWordPosition="3495">et is very small compared to the complexity of the process being learned, and the MCCARTHY data set is rather noisy, with a large number of erroneous transcriptions. With the German data set, though it is quite irregular, and the data set is not frequency-weighted, so the frequent irregular words are not more likely to be in the training data, there is a lot of data, so the algorithm performs quite well. 5.3 Cognitive Modelling In addition to these formal evaluations we examined the extent to which this approach can account for some psycho-linguistic data, in particular the data collected by (Prasada and Pinker, 1993) on the mild productivity of irregular forms in the English past tense. Space does not permit more than a rather crude summary. They prepared six data sets of 10 pairs of nonce words together with regular and irregular plurals of them: a sequence of three data sets that were similar to, but progressively further away from sets of irregular verbs (prototypicalintermediate- and distant- pseudoirregular – PPI IPI and DPI), and another set that were similar to sets of regular verbs (prototypical-, intermediate- and distant- pseudoregular PPR, IPR and DPR). Thus the first data sets contained words </context>
<context position="23040" citStr="Prasada and Pinker, 1993" startWordPosition="3880" endWordPosition="3883">We calculated the average negative log probability for each of the six data sets in 3 ways: first we calculated the probability of the stem alone to model the acceptability of the stem; secondly we calculated the conditional probability of the regular (suffixed form), and thirdly we calculated the conditional probability of the irregular (vowel change) form of the word. Then we calculated the difference between the figures for the appropriate past tense form from the stem form. This is unjustifiable in terms of probabilities but seems the most natural way of modelling the effects reported in (Prasada and Pinker, 1993). These results are presented in Table 5. Interestingly we observed the same effect: a decrease in “acceptability” for irregulars, as they became more distant, and the opposite effect for regulars. In our case though it is clear why this happens – the probability of the stem decreases rapidly, and this overwhelms the mild decrease in the conditional probability. 6 Discussion The productivity of the regular forms is an emergent property of the system. This is an advantage over previous work using the EM algorithm with SFST, which directly specified the productivity as a parameter. 6.1 Related w</context>
</contexts>
<marker>Prasada, Pinker, 1993</marker>
<rawString>Sandeep Prasada and Steven Pinker. 1993. Generalisation of regular and irregular morphological patterns. Language and Cognitive Processes, 8(1):1–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
</authors>
<title>Finite growth models.</title>
<date>1997</date>
<tech>Technical Report CS-TR-533-96,</tech>
<institution>Department of Computer Science, Princeton University.</institution>
<note>revised in</note>
<contexts>
<context position="2840" citStr="Ristad, 1997" startWordPosition="416" endWordPosition="417"> classified according to the class label of the closest instance. However these approaches are not a complete solution to the problem of learning morphology, since they do not directly produce the transduction. The problem must first be converted into an appropriate feature-based representation and classified in some way. The techniques presented here operate directly on sequences of atomic symbols, using a much less articulated representation, and much less input information. 2 Stochastic Transducers It is possible to apply the EM algorithm to learn the parameters of stochastic transducers, (Ristad, 1997; Casacuberta, 1995; Clark, 2001a). (Clark, 2001a) showed how this approach could be used to learn morphology by starting with a randomly initialized model and using the EM algorithm to find a local maximum of the joint probabilities over the pairs of inflected and uninflected words. In addition rather than using the EM algorithm to optimize the joint probability it would be possible to use a gradient descent algorithm to maximize the conditional probability. The models used here are Stochastic NonDeterministic Finite-State Transducers (FST), or Pair Hidden Markov Models (Durbin et al., 1998),</context>
</contexts>
<marker>Ristad, 1997</marker>
<rawString>Eric Sven Ristad. 1997. Finite growth models. Technical Report CS-TR-533-96, Department of Computer Science, Princeton University. revised in 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based morphological analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>285--292</pages>
<marker>van den Bosch, Daelemans, 1999</marker>
<rawString>Antal van den Bosch and Walter Daelemans. 1999. Memory-based morphological analysis. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 285–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
</authors>
<title>Using induced rules as complex features in memory-based language learning.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>73--78</pages>
<marker>van den Bosch, 2000</marker>
<rawString>Antal van den Bosch. 2000. Using induced rules as complex features in memory-based language learning. In Proceedings of CoNLL 2000, pages 73–78.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>