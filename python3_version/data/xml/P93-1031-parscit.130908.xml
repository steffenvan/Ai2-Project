<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.829346">
TAILORING LEXICAL CHOICE TO THE USER&apos;S VOCABULARY
IN MULTIMEDIA EXPLANATION GENERATION
</title>
<author confidence="0.894336666666667">
Kathleen McKeown
Jacques Robin
Michael Tanenblatt
</author>
<affiliation confidence="0.931134333333333">
Department of Computer Science
450 Computer Science Building
Columbia University
</affiliation>
<address confidence="0.981922">
New York, N.Y. 10027
</address>
<email confidence="0.987593">
kathy,robin,tanenbla} @cs.columbia.edu
</email>
<sectionHeader confidence="0.998029" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.997751">
In this paper, we discuss the different strategies used in COMET
(COordinated Multimedia Explanation Testbed) for selecting
words with which the user is familiar. When pictures cannot be
used to disambiguate a word or phrase, COMET has four
strategies for avoiding unknown words. We give examples for
each of these strategies and show how they are implemented in
COMET.
</bodyText>
<sectionHeader confidence="0.998766" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.98606237037037">
A language generation system should select words
that its user knows. While this would seem to involve
simply selecting a known word instead of an un-
known word (as is done, for example, in [1]), in many
cases it requires entirely rephrasing the rest of the
sentence. For example, in our domain of equipment
maintenance and repair, if the user does not know the
word &amp;quot;polarity,&amp;quot; a sentence like &amp;quot;Check the
polarity.&apos; will be rephrased as &amp;quot;Make sure the plus
on the battery lines up with the plus on the battery
compartment.&amp;quot; Even when alternative words can be
used instead of an unknown word (e.g., a descriptive
expression can be used instead of an object name),
the alternative phrase may interact with other parts of
the sentence which then need to be reworded as well.
In this paper, we discuss the different strategies used
in COMET for selecting words with which the user is
familiar. Since COMET integrates text and pictures
in a single explanation 1, unknown words are fre-
quently disambiguated through accompanying pic-
tures. For example, when the accompanying picture
clearly shows the object and its location, COMET
will use the most common object name even if the
user is unfamiliar with the name2. When pictures can-
not be used to disambiguate a word or phrase,
COMET has four strategies for avoiding unknown
words:
</bodyText>
<listItem confidence="0.987984944444444">
1. Selecting an alternative word or phrase
(e.g., generating &amp;quot;some number&amp;quot; in-
stead of &amp;quot;arbitrary number&amp;quot;)
2. Rephrasing by providing conceptual
definitions (e.g., generating &amp;quot;Make sure
the plus on the battery lines up with the
plus on the battery compartment.&amp;quot; in-
stead of &amp;quot;Check the polarity&amp;quot;)
3. Rephrasing by generating descriptive
referring expressions (e.g., generating
&amp;quot;the cable that runs to the KY57&amp;quot; in-
stead of &amp;quot;the COMSEC cable&amp;quot;)
4. Using past discourse to construct a
referring expression (e.g., generating
&amp;quot;Test the cable you just removed.&amp;quot; in-
stead of &amp;quot;Test the COMSEC cable.&amp;quot; if
the user had previously been instructed
to remove this cable.)
</listItem>
<bodyText confidence="0.948951222222222">
In the following sections, we first provide an over-
view of lexical choice in COMET, showing how and
where it occurs in the overall system. Each of the
strategies is then described in turn, prefaced by a
brief discussion of disambiguation of unknown terms
through pictures. Finally, we compare our work with
previous work in the area.
I See [2] for a system overview and [3, 4] for details on media 2This is similar to Appelt&apos;s [5] integration of language and
coordination in COMET. physical actions for generating referring expressions.
</bodyText>
<page confidence="0.996418">
226
</page>
<figure confidence="0.998521481481481">
(Knowledge
Base
Content
Planner
Logical Form A
Media
Coordinator
I
Annotated
Logical Form
Discourse
History
User
Model
Lexical
CLexicon Chooser
Gamma-i&gt; FUF
Text Generator
Text
Graphics
Generator
Illustration
Layout
Manager
if
Multimedia
Explanation
</figure>
<figureCaption confidence="0.999949">
Figure 1: COMET System Architecture
</figureCaption>
<sectionHeader confidence="0.636405" genericHeader="method">
2. Lexical Choice and Architecture
</sectionHeader>
<bodyText confidence="0.9991998">
COMET&apos;s architecture is shown in Figure 1. On
receiving a request for an explanation via a menu in-
terface, the content planner uses schemas [6] to
determine which information should be included in
the explanation from the underlying knowledge
sources. The explanation content, represented as a
hierarchy of logical forms (LFs) [7] is passed to the
media coordinator [3, 8], which adds annotations in-
dicating which portions are to be produced by the
text generator and which by the graphics generator
[9].
The Lexical Chooser is part of the text generator [7].
Typically, it selects a word or phrase for each seman-
tic concept in the input LF (i.e., the semantic con-
straints on word choice). In terms of coverage, the
implementation can select words for 148 different
semantic concepts using 253 mapping rules, thus
yielding on average slightly less than two alternative
word choices per concept (there are many concepts
which are mapped to a single word, while others have
more than two alternatives). The lexicon contains 159
open class words.
In this paper, we show how the user model and past
discourse (pragmatic constraints) also influence word
choice. But these are not the only constraints on
word choice. Syntactic form of the sentence and lex-
ical constraints are other demonstrated
[10, 11] influences on lexical choice. For example,
once the verb has been chosen, syntactic constraints
on its arguments (e.g., whether the object is a clause,
</bodyText>
<page confidence="0.998068">
227
</page>
<figureCaption confidence="0.999518">
Figure 2: Accompanying Picture Clarifies Referent
</figureCaption>
<bodyText confidence="0.993423440677966">
Load the frequency in channel one. Step 3 of 4
an adj, or np) will influence what words are chosen to
realize the semantic concept that fill these arguments.
Conversely, if one of the verb roles can only be real-
ized as a noun phrase, for example, and not as other
syntactic categories, this restricts which verb is
selected. Lexical constraints on word choice arise
from the use of collocations [12]. For example, a verb
like &amp;quot;stand&amp;quot; takes the preposition &amp;quot;on&amp;quot; for its loca-
tion role, while the verb &amp;quot;turn&amp;quot; takes the preposition
&amp;quot;onto.&amp;quot; Lexical choice is thus influenced by a wide
variety of constraints which interact in many ways.
Since syntactic and lexical constraints are only avail-
able within the text generator, lexical choice is
delayed until this point. Thus COMET waits until a
variety of semantic, pragmatic, syntactic and lexical
constraints are accumulated before selecting words.
This means that COMET can use syntactic and lex-
ical constraints on word choice in conjunction with
semantic and graphical constraints provided as input,
plus the new pragmatic constraints we present. Pre-
vious work addressing pragmatic constraints on word
usage folded lexical choice into the content planner
(e.g., [13], [1]). This was possible since the work
focused primarily on lexical side effects of content
determination (e.g., what property to include in a ref-
erence as opposed to what linguistic form to use for a
property). Such approaches do not allow a system to
take syntactic and lexical constraints on word choice
into account.
On receiving the hierarchy of logical forms, the Lex-
ical Chooser determines the overall grammatical form
of each sentence based on the semantic structure of
the LFs (e.g., conditional sentences are generated for
precondition-action structures) and selects the words
and phrases realizing semantic concepts of the LF. It
passes a specification of the sentence&apos;s grammatical
form and open-class words to the general purpose
surface sentence generator FUF [14, 15, 16]. The
Lexical Chooser uses a rewriting system itself im-
plemented on top of FUF. Its lexicon consists of a
base of rules, where each rule rewrites a given set of
semantic features into a corresponding set of lexical
and syntactic features. Thus, each lexicon entry as-
sociates a semantic concept with words that can be
used to realize it. Additional constraints from the user
model, past discourse, and the underlying knowledge
base determine which of the alternative words or
phrases should be selected.3 The user model indicates
both the reading level of the current user4, any in-
dividual words that COMET knows the user does not
understand, and any wording preferences (e.g., the
user knows abbreviations, the user is familiar with
military terminology). We make no claims about
which of these forms of user models is easier to ac-
quire, but simply show how to use them when avail-
able.
If none of the alternative wordings for a given seman-
tic concept of the LF are known to the user and the
</bodyText>
<footnote confidence="0.99603">
3When these constraints come from knowledge sources exter-
nal to FUF, the Lexical Chooser uses FUF extensions to access
such knowledge through the use of coroutines [17].
4We currently use two levels for a poor and good reader. At the
beginning of the session, the reading level is either preset or
COMET can ask the user.
</footnote>
<page confidence="0.992687">
228
</page>
<figureCaption confidence="0.999553">
Figure 3: Use of Cross References: Remove the holding battery, shown in the cutaway view
</figureCaption>
<bodyText confidence="0.999504555555556">
Install the new holding battery. Step 2 of 6
Remove the old holding battery, shown in the cutaway view.
accompanying illustration cannot disambiguate these
words, COMET reinvokes the content planner to
replan portions of the sentence content or to include
additional semantic information. Thus, COMET&apos; s ar-
chitecture interleaves lexical choice and content plan-
ning in order to account for a wide variety of inter-
acting constraints on word choice.
</bodyText>
<sectionHeader confidence="0.970167" genericHeader="method">
3. Multimedia Disambiguation
</sectionHeader>
<bodyText confidence="0.99984225">
An accompanying picture often makes clear what the
referent of a referring expression is. If the user is
unfamiliar with a term, the accompanying picture
might define it. For example, Figure 2 shows one
step of an explanation generated by COMET for
loading frequency into the radio. The text refers to a
&amp;quot;FCTN knob&amp;quot; and the accompanying picture clearly
singles out the knob on the front panel of the radio
[4]. COMET can also generate an explicit reference
to the illustration itself (called a cross reference). For
example, the cross reference shown in Figure 3 is
generated if the user does not understand the term
&amp;quot;holding battery&amp;quot;. In this case, the Lexical Chooser,
on determining that &amp;quot;holding battery&amp;quot; is an un-
familiar term, reinvokes the content planner which
finds that no accompanying illustration is currently
planned and invokes graphics to generate an accom-
panying illustration that depicts the holding battery
and its location. For full details on cross referencing
in COMET see [18].
</bodyText>
<sectionHeader confidence="0.588498" genericHeader="method">
4. Selecting a Familiar Word/phrase
</sectionHeader>
<bodyText confidence="0.78207425">
Whenever possible, COMET simply selects a
familiar word over an unknown word from the list of
alternatives in the lexicon. Figure 4 shows some
paired sentences that COMET generates which il-
lustrate alternative wordings. The first italicized
phrase is generated if the user&apos;s vocabulary level is
above a certain reading level or if a word is not ex-
plicitly listed in the user model as unknown. Since
the lexicon maintains a simple association between
the semantic concept and alternative phrasings,
COMET selects the first alternative which the user
model indicates is familiar to the user. For example,
</bodyText>
<figureCaption confidence="0.936425272727273">
Figure 5 shows that for any concept under the con-
cept c-disconnect in the knowledge base taxonomy,
COMET will use the word &amp;quot;disconnect&amp;quot; if the user&apos;s
vocabulary level is high and the word &amp;quot;remove&amp;quot;
otherwise. COMET also checks whether the user
knows abbreviations and if so, will use a referring
expression such as &amp;quot;FCTN knob&amp;quot; as shown in
Figure 2. If not, COMET uses the full name (&amp;quot;func-
tion knob&amp;quot;). If COMET has no information about the
user, it generates the abbreviation and relies on the
accompanying illustration to clarify the referent.
</figureCaption>
<listItem confidence="0.995889714285714">
1. Screw the new manpack antenna onto the RT
and tighten until the manpack antenna is
snug/tight.
2. Disconnect/Remove the COMSEC cable
from the KY57 audio connector.
3. This will cause the display to show an
arbitrary/some number.
</listItem>
<figureCaption confidence="0.998238">
Figure 4: COMET-Generated Word Substitutions
</figureCaption>
<page confidence="0.918443">
229
</page>
<figure confidence="0.835547454545455">
(; semantic key
((concept #(under c-disconnect)))
; realization
((process
((cat verb-group) ; will be a verb
(alt
; if level high select &amp;quot;disconnect&amp;quot;
((CONTROL (OK-Lex-UM &apos;c-disconnect high))
(lex &amp;quot;disconnect&amp;quot;))
; else select &amp;quot;remove&amp;quot;
((lex &amp;quot;remove&amp;quot;))))))))
</figure>
<figureCaption confidence="0.99556">
Figure 5: Lexicon Entry for Disconnect Concept
</figureCaption>
<sectionHeader confidence="0.9555" genericHeader="method">
5. Rephrasing through Replanning
</sectionHeader>
<bodyText confidence="0.999926375">
Selecting an alternative wording for a semantic con-
cept is not always possible since none of the alter-
natives may be known by the user. Instead, COMET
can describe concepts at a more detailed semantic
level of abstraction by retrieving additional defini-
tional information from the knowledge base and it
can create referring descriptions when object names
are not known, by retrieving object attributes.
</bodyText>
<subsectionHeader confidence="0.99628">
5.1. Retrieving alternative concept definitions
</subsectionHeader>
<bodyText confidence="0.935844704918033">
Sometimes the original text uses a word or phrase
that abstracts the details of a concept to allow genera-
tion of a very concise expression. If unfamiliar with
the word or phrase, the user will be unable to infer
the specifics needed to perform the task. Alternative
wordings require choosing a less abstract level of
semantic decomposition at which to describe the con-
cept. In these cases, COMET&apos;s lexical chooser rein-
yokes the content planner to retrieve a finer grained
definition of the concept from the knowledge base.
For example, this strategy is used for rephrasing the
request &amp;quot;Check the polarity&amp;quot; which COMET issues
when providing instructions for installing a new hold-
ing battery. More detailed semantics of checking the
polarity are stored as different tokens of the concept
c-polarity in the knowledge base.5 For example, in
Figure 6 polarity is represented as the equivalence be-
tween the two plusses on two batteries°. Now, if the
plan calls for checking polarity, it can be represented
in terms of a checking action on the equivalence of
these two plusses (i.e., that they line up). If the user
5The more detailed definition is stored with c-polarity and not
with c-check since in our domain checking is carried out on
many different objects, while few actions are carried out on
polarity.
6The equative relations has two roles, identified and identifier.
Since they are included here, the equative relation (i.e., that the
two plusses &amp;quot;lineup&amp;quot;) is inferred to hold.
is unfamiliar with the word &amp;quot;polarity,&amp;quot; an alternate
decomposition will be retrieved and replace the
phenomenon role filler in the original LF (Figure 7).
Figure 8 shows the alternative LF with a new
phenomenon role (the remainder of the LF is un-
changed). The resulting rephrased sentence is
&amp;quot;Make sure that the plus on the battery lines up with
the plus on the battery compartment.&amp;quot;. &amp;quot;Lines up&amp;quot;
is selected in the lexicon for the equivalence relation
based on the semantics of its roles (i.e., that they are
both plusses on the batteries). Here semantic selec-
tional restrictions on the _roles control lexical choice
of the verb.
Since the object of the new sentence is an embedded
sentence, COMET can use either the verb &amp;quot;check&amp;quot;
or the collocation &amp;quot;make sure&amp;quot; as the verb realizing
the mental process concept c-check. Note that, while
these two verbs are listed as alternatives in the lex-
icon for c-check, &amp;quot;make sure&amp;quot; cannot be used in the
original sentence due to a syntactic constraint: its ob-
ject cannot be an NP as one cannot say &amp;quot;Make sure
the polarity.&amp;quot;. This is an example of interaction be-
tween syntactic and pragmatic constraints. Since syn-
tax does not constrain the choice of verb in the
modified sentence, COMET arbitrarily selects &amp;quot;make
sure&amp;quot;.
The lexicon entry containing these two verbs is
shown below in Figure 9. Note that the entry is in-
dexed by the semantic concept c-check. There are
two alternative verbs, only one of which is com-
patible with a clause as phenomenon role (ultimately
the object). When the phenomenon is an NP, both
verbs are valid and one is randomly selected.
</bodyText>
<table confidence="0.882936">
; Instance definitions for polarity
(tellm (polarity polarity-1)
(polarity polarity-2))
; More detail for one instance: polarity is
; represented as two plusses which should
be equivalent. The roles of the equative
relation are identified and identifier
</table>
<equation confidence="0.903832285714286">
(:about polarity-2
(identified plus-1)
(identifier plus-2))
; one is located on the battery
(:about plus-1 (on-lc battery-1))
; one is located on the battery compartment
(:about plus-2 (on-lc bc-1))))
</equation>
<figureCaption confidence="0.978772">
Figure 6: Knowledge base tokens for polarity
</figureCaption>
<page confidence="0.847585">
230
</page>
<figure confidence="0.9911708">
((Concept C-Check) ; &amp;quot;check&amp;quot;
(Process-Type Mental)
(Roles
((Phenomenon
((Concept C-Polarity)))))) ; &amp;quot;the polarity&amp;quot;
</figure>
<figureCaption confidence="0.990036">
Figure 7: Logical Form for Original Sentence
</figureCaption>
<figure confidence="0.999641611111111">
((Concept C-Check) ; &amp;quot;make sure that
(Process-Type Mental)
(Roles
((Phenomenon
((Concept C-Polarity)
(Process-Type Equative) ; &amp;quot;lines up with&amp;quot;
(Roles
((Identified
((Concept C-Plus) ; &amp;quot;the plus&amp;quot;
(Roles
((On-Loc ; &amp;quot;on the battery&amp;quot;
((Concept C-Battery)))))))
(Identifier
((Concept C-Plus) ; &amp;quot;the plus&amp;quot;
(Roles
((On-Loc
; &amp;quot;on the battery compartment&amp;quot;
((Concept C-BC))))))))))))))
</figure>
<figureCaption confidence="0.994434">
Figure 8: Logical Form of Rephrased Sentence
</figureCaption>
<equation confidence="0.232302">
( ; semantic key
((concept #(under c-check))
; realization
(cat verb-group) ; will be a verb
(alt
( ; if phenomenon realized by NP
((roles
((phenomenon ((cat #((under np)))))))
</equation>
<bodyText confidence="0.55210225">
; then always choose &amp;quot;to check&amp;quot;
(lex &amp;quot;check&amp;quot;))
; if phenomenon realized by clause
((roles
((phenomenon ((cat #((under clause)))))))
; then randomly pick &amp;quot;to check&amp;quot; or
; &amp;quot;to make sure&amp;quot;
(lex ((Ralt (&amp;quot;check&amp;quot; &amp;quot;make sure&amp;quot;)))))))))
</bodyText>
<figureCaption confidence="0.995361">
Figure 9: Lexicon Entry for Check Concept
</figureCaption>
<subsectionHeader confidence="0.97954">
5.2. Generating New Referential Descriptions
</subsectionHeader>
<bodyText confidence="0.997847775">
If the user does not know an object name, the content
planner is reinvoked to generate object attributes to
build a referential description. Although our selec-
tion algorithm is not as sophisticated as others
[19, 5, 13] because we do not use a detailed model of
user beliefs, we address a new issue: the interaction
between the new description and other parts of the
original sentence which may require rephrasin. Two
types of object attributes are used in a referring ex-
pression in COMET: object subpart relations and
spatial relations to other objects in the accompanying
illustration. COMET selects the relations that
uniquely identify the object.
For example, suppose COMET&apos; s Lexical Chooser is
provided with the LF for sentence 1, Figure 10, but
the user does not know the term &amp;quot;COMSEC.&amp;quot; In-
stead of generating sentence 1, COMET generates
sentence 2. To do this, COMET first selects a unique
relation between the cable and a known object. In this
case, it selects the connects spatial relation between
the Radio Transmitter (RT) and the KY57, since this
cable is the only one that connects the radio and the
KY57. Selecting this relation for the description and
substituting it for &amp;quot;the COMSEC cable&amp;quot; would
result in sentence 3, Fig. 10. However, COMET notes
the redundant references to the audio connector and
removes one from the cable modifier by selecting the
verb &amp;quot;runs to&amp;quot; instead which only requires one role
in the generated sentence. This would result in the
sentence 4, Fig. 10. In this sentence, the attachment
of the prepositional phrase &amp;quot;from the KY57 audio
connector&amp;quot; is ambiguous. COMET detects this am-
biguity when it removes the first from-location; since
the two from-locations would have occurred side by
side and both previous verbs of the sentence take it as
a modifier, the generator must clarify that it is the
from-location of the earlier verb &amp;quot;disconnect&amp;quot; and
not &amp;quot;run to.&amp;quot; To remove ambiguity, COMET sur-
rounds the modifier of the cable by commas in sen-
tence 2, Fig. 107.
</bodyText>
<subsubsectionHeader confidence="0.772201">
Descriptions Generated by COMET:
</subsubsectionHeader>
<listItem confidence="0.95596875">
1. &amp;quot;Disconnect the COMSEC cable from the
KY57 audio connector.&amp;quot;
2. &amp;quot;Disconnect the cable, which runs to the RT,
from the KY57 audio connector.&amp;quot;
</listItem>
<subsubsectionHeader confidence="0.597253">
Descriptions Avoided by COMET:
</subsubsectionHeader>
<listItem confidence="0.9689288">
3. &amp;quot;Disconnect the cable that connects the RT
to the KY57 audio connector from the KY57
audio connector.&amp;quot;
4. &amp;quot;Disconnect the cable that runs to the RT
from the KY57 audio connector.&amp;quot;
</listItem>
<figureCaption confidence="0.896949">
Figure 10: Generated Object Description
</figureCaption>
<footnote confidence="0.99807675">
7Another possible way to avoid ambiguity would be to
generate two sentences such as &amp;quot;Find the cable that runs from the
RT to the KY57 audio connector. Disconnect the cable from the
audio connector.&amp;quot;
</footnote>
<page confidence="0.99651">
231
</page>
<sectionHeader confidence="0.929354" genericHeader="method">
6. Using Past Discourse
</sectionHeader>
<bodyText confidence="0.998400777777778">
For subsequent reference, the presence of a discursive
context allows for a wider variety of strategies to get
around gaps in the user&apos;s vocabulary. COMET takes
advantage of this fact by maintaining a discourse his-
tory. The content planner records all descriptions
into the discourse history, creating one record for the
description as a whole and a separate record for each
of its roles. The entry for the description has four
fields:
</bodyText>
<listItem confidence="0.99050525">
• The name of the concept.
• The description used in the reference.
• The action in which the referring
description plays a role.
• The list of roles that the description fills
in that action (e.g., &amp;quot;COMSEC cable&amp;quot; is
the medium of the action &amp;quot;discon-
nect&amp;quot;).
</listItem>
<bodyText confidence="0.999921714285714">
For each subsequent reference, the concept name is
used as the access key and the three other fields are
updated; they thus always contain the information on
the last reference. By looking up information in the
discourse history, the content planner is able to con-
struct object descriptions in terms of the last action it
was involved in.
</bodyText>
<subsubsectionHeader confidence="0.508624">
Sentences generated if the user knows &amp;quot;COMSEC&amp;quot;
</subsubsectionHeader>
<listItem confidence="0.9988906">
1. &amp;quot;Disconnect the COMSEC cable from the
KY57 audio-connector.&amp;quot;
2. &amp;quot;Plug in the handset to the KY57 audio-
connector.&amp;quot;
3. &amp;quot;Test the COMSEC cable.&amp;quot;
</listItem>
<subsubsectionHeader confidence="0.444771">
Sentences generated if not:
</subsubsectionHeader>
<listItem confidence="0.9956144">
4. &amp;quot;Disconnect the cable, which runs to
the RT, from the KY57 audio connector.&amp;quot;
5. &amp;quot;Plug in the handset to the KY57 audio
connector.&amp;quot;
6. &amp;quot;Test the cable that you just disconnected.&amp;quot;
</listItem>
<figureCaption confidence="0.660318">
Figure 11: Use of Previous Discourse
</figureCaption>
<bodyText confidence="0.999507714285715">
As an example, consider the explanations COMET
generates when instructing the user how to diagnose
loss of side tone. When the user has no vocabulary
gaps, COMET generates sentences 1-3, Figure 11.
When the user is unfamiliar with the term &amp;quot;COM-
SEC,&amp;quot; sentences 4-6 are generated instead. Here
COMET uses past discourse to produce a descriptive
reference for the second reference to the COMSEC
cable.
As in the previous examples, the gap is detected
when the Lexical Chooser checks the user model.
Since there is no alternative phrase for &amp;quot;COMSEC&amp;quot;
in the lexicon, COMET calls the content planner to
replan the reference. Since it is not the first reference
to the cable, COMET uses the discourse history to
plan a modifying description. A reference to the cable
is discovered in the history (its entry is shown in
Figure 12) and the action in this entry is selected as
the modifier to build a referring expression.8 The role
of the cable was medium and thus, COMET can
generate the modifier as a relative clause. The LF for
this referring expression is shown in Figure 13. This
LF is sent back to the lexical chooser, which selects
the words for the concepts within it, and continues
with generation where it left off. On third and fourth
reference to the same concept, COMET uses its
anaphoric reference facility to generate either a bare
head (e.g., &amp;quot;cable&amp;quot;) or a pronoun (e.g., &amp;quot;it&amp;quot;).
</bodyText>
<table confidence="0.9691844">
(; The concept name:
((Concept C-Comsec-Cable))
; The initial generated description:
; included where connected to and from.
((Concept C-Cable)
(Roles ((To-Loc ((Concept C-RT)))
(From-Loc ((Concept C-KY57))))))
; The role it plays in the action:
((Roles Medium))
; The action itself: &amp;quot;disconnect the cable&amp;quot;.
((Process-Type Material)
(Concept C-Disconnect)
; Rest of action description
; in discourse history
)) ; but not shown here
</table>
<figureCaption confidence="0.8884915">
Figure 12: Entry for COMSEC Cable
in the Discourse History
</figureCaption>
<sectionHeader confidence="0.934993" genericHeader="conclusions">
7. Conclusions and Related Work
</sectionHeader>
<bodyText confidence="0.999338">
COMET performs several lexical choice tasks. It can
choose between alternative words or phrases for any
part of speech. When generating a request to perform
an action, it chooses a level of detail in the concept
description appropriate to the user. When generating
both initial and subsequent referring expressions, it
selects a set of distinguishing properties of the
referent and chooses words to express the selected
</bodyText>
<footnote confidence="0.996358">
8There is a limit to how far back COMET looks in the dis-
course to construct a new referring expression: the discourse
history is cleared after each menu request for a new explanation.
</footnote>
<page confidence="0.979055">
232
</page>
<figure confidence="0.998570555555556">
((Concept C-Cable)
(Roles
((Latest-Participation
((Process-Type Material)
(Concept C-Disconnect)
(Roles
((Agent ((Concept C-User)))
(Medium
((Concept (^5 Concept)))))))))))
</figure>
<figureCaption confidence="0.999774">
Figure 13: &amp;quot;the cable you just disconnected&amp;quot;
</figureCaption>
<bodyText confidence="0.998360333333333">
properties. Finally, for subsequent references,
COMET can use previous discourse to avoid un-
known words.
COMET is thus using constraints from the user
model, the accompanying illustration, and past dis-
course in addition to traditional constraints from
semantics, syntax, and other word choices. Although
other generation systems take into account some of
these constraints, COMET is the first attempt to in-
tegrate such a variety of constraints and lexical
choice strategies in a single system. In addition, be-
cause COMET is a multimedia system, it can use the
accompanying illustrations advantageously for dis-
ambiguation.
WIP [20] can also generate cross references but does
not rely on a user model for either cross reference
generation or lexical choice. EPICURE [19] , KAMP
[5], and FN [13] tailor references based on situation,
but they do not constrain this choice based on the
user&apos;s lexical knowledge. EPICURE uses the user&apos;s
domain knowledge, KAMP mutual beliefs about the
domain, and FN the user&apos;s domain knowledge in con-
junction with rules on implicatures. They focus on
the selection of appropriate properties to distinguish
an object in generating references but do not choose
between alternative wordings for the selected
properties. None of these systems reword action
descriptions or use past discourse to avoid terms the
user does not know. While Bateman and Paris&apos; sys-
tem [21] uses different dialects depending on which
class of users it is addressing through register map-
pings, in COMET different terms can be mixed and
matched depending on the individual user model.
</bodyText>
<sectionHeader confidence="0.994247" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997122">
Research on language generation in COMET has
been supported in part by Defense Advanced
Research Projects Agency Contract N00039-84-
C-0165, National Science Foundation Grants
IRT-84-51438 and GER-90-2406, New York State
Center for Advanced Technology Contracts
NYSSTF-CAT(90)-053, (91)-053, and (92)-053, and
Office of Naval Research Contracts N00014-82-
K-0256 and N00014-894-1782. COMET&apos; s develop-
ment is an ongoing group effort and has benefited
from the contributions of Michael Elhadad (FUF),
Doree Seligmann (graphics generator), Andrea
Danyluk (diagnostic rule base), Yumiko Fukumoto
(media coordinator), Jong Lim (static knowledge base
and content planner), Christine Lombardi (media
coordinator), Jacques Robin (lexical chooser), James
Shaw (anaphoric reference facility), Michael
Tanenblatt (knowledge base, content planner),
Michelle Baker, Cliff Beshers, David Fox, Laura
Gabbe, Frank Smadja, and Tony Weida.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999885857142857">
1. Swartout, W.R., &amp;quot;XPLAIN: a system for creating
and explaining expert consulting systems&amp;quot;,
Artificial Intelligence, Vol. 21, No. 3, 1983, pp.
285-325.
2. Feiner, S. and K.R. McKeown, &amp;quot;Generating Coor-
dinated Multimedia Explanations&amp;quot;, Proceedings of
the IEEE Conference on Al Applications, Santa
Barbara, CA., March 1990.
3. Feiner, S. and K.R. McKeown, &amp;quot;Coordinating Text
and Graphics in Explanation Generation&amp;quot;,
Proceedings of the National Conference on Artifi-
cial Intelligence, Boston, Mass., August 1990.
4. Feiner, S. and McKeown, K.R., &amp;quot;Automating the
Generation of Coordinated Multimedia Explana-
tions&amp;quot;, IEEE Computer, Vol. 24, No.
10, October 1991, pp. 33-41.
5. Appelt, D.E., Planning English Sentences,
Cambridge University Press, Cambridge, England,
1985.
6. McKeown, K.R., Text Generation: Using Dis-
course Strategies and Focus Constraints to
Generate Natural Language Text, Cambridge
University Press, Cambridge, England, 1985.
7. McKeown, K.R., Elhadad, M., Fukumoto, Y., Lim,
J., Lombardi, C., Robin, J., and Smadja, F., &amp;quot;Lan-
guage Generation in COMET&amp;quot;, in Current
Research in Language Generation, Mellish, C.,
Dale, R., and Zock, M., eds., Academic Press, Lon-
don, 1990.
8. Elhadad, M., Seligmann, D., Feiner, S., and
McKeown, K., &amp;quot;A Common Intention Description
Language for Interactive Multi-media Systems&amp;quot;, A
New Generation of Intelligent Interfaces: Proceed-
ings of IJCAI89 Workshop on Intelligent Interfaces,
Detroit, MI, August 22 1989, pp. 46-52.
9. Seligmann, D.D., and Feiner, S., &amp;quot;Specifying
Composite Illustrations with Communicative
Goals&amp;quot;, Proc. ACM Symposium on User Interface
Software and Technology, Williamsburg, VA,
November 13-15 1989, pp. 1-9.
10. McDonald, D.D, &amp;quot;On the place of words in the
generation process&amp;quot;, in Natural Language Genera-
</reference>
<page confidence="0.981659">
233
</page>
<reference confidence="0.999899326086957">
tion in Artificial Intelligence and Computational
Linguistics, Paris, C., Swartout, W. and Mann.
W.C., eds., Kluwer Academic Publishers, 1991.
11. Danlos, L., The Linguistic Basis of Text
Generation, Cambridge University Press,
Cambridge, England, 1987.
12. Smadja, F. and K.R. McKeown, &amp;quot;Automatically
Extracting and Representing Collocations for Lan-
guage Generation&amp;quot;, Proceedings of the 28th An-
nual Meeting of the Association for Computational
Linguistics, Pittsburgh, Pa., June 1990, pp. 252-9.
13. Reiter, E.B., Generating appropriate natural lan-
guage object description, PhD dissertation, Center
for research in computing technology, Harvard
University, 1990.
14. Elhadad, M., &amp;quot;The FUF Functional Unifier: User&apos;s
Manual&amp;quot;, Tech. report, Columbia University, 1988.
15. Elhadad, M., &amp;quot;Types in Functional Unification
Grammars&amp;quot;, Proceedings of the 28th meeting of
the Association for Computational Linguistics,
Pittsburgh, Pa, June 1990.
16. Elhadad, M., Using argumentation to control lex-
ical choice: a unification-based implementation,
PhD dissertation, Computer Science Department,
Columbia University, 1993.
17. Elhadad, M. and Robin, J., &amp;quot;Controlling Content
Realization with Functional Unification Gram-
mars&amp;quot;, in Aspects of Automated Natural Language
Generation, Dale, R. and Hovy, H. and Roesner,
D. and Stock, 0., ed., Springler Verlag, 1992, pp.
89-104.
18. McKeown, K. R., Feiner, S.K., Robin, J., Selig-
mann, D., and Tanenblatt, M., &amp;quot;Generating Cross
References for Multimedia Explanations&amp;quot;,
Proceedings of AAAI-92, AAAI, July 1992.
19. Dale, R., Generating Referring Expressions, ACL-
MIT Press Series in Natural Language Processing,
Cambridge, Ma., 1992.
20. Wahlster, W., Andre, E., Hecicing, M., and T. Rist,
&amp;quot;WIP: Knowledge-based Presentation of Infor-
mation&amp;quot;, Tech. report WIP-1, German Research
Center for Artificial Intelligence, May 1989.
21. Bateman, J.A. and Paris, C.L., &amp;quot;Phrasing a text in
terms the user can understand&amp;quot;, Proceedings of the
1 1 th International Joint Conference on Artificial
Intelligence, Detroit, MI, 1989, pp. 1511-1517.
</reference>
<page confidence="0.998378">
234
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448346">
<title confidence="0.986218">TAILORING LEXICAL CHOICE TO THE USER&apos;S VOCABULARY IN MULTIMEDIA EXPLANATION GENERATION</title>
<author confidence="0.998670333333333">Kathleen McKeown Jacques Robin Michael Tanenblatt</author>
<affiliation confidence="0.999843">Department of Computer Science</affiliation>
<address confidence="0.878753">450 Computer Science Building</address>
<affiliation confidence="0.996428">Columbia University</affiliation>
<address confidence="0.999779">New York, N.Y. 10027</address>
<email confidence="0.991125">kathy,robin,tanenbla}@cs.columbia.edu</email>
<abstract confidence="0.94018575">In this paper, we discuss the different strategies used in COMET (COordinated Multimedia Explanation Testbed) for selecting words with which the user is familiar. When pictures cannot be used to disambiguate a word or phrase, COMET has four strategies for avoiding unknown words. We give examples for each of these strategies and show how they are implemented in COMET.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W R Swartout</author>
</authors>
<title>XPLAIN: a system for creating and explaining expert consulting systems&amp;quot;,</title>
<date>1983</date>
<journal>Artificial Intelligence,</journal>
<volume>21</volume>
<pages>285--325</pages>
<contexts>
<context position="865" citStr="[1]" startWordPosition="129" endWordPosition="129">cs.columbia.edu ABSTRACT In this paper, we discuss the different strategies used in COMET (COordinated Multimedia Explanation Testbed) for selecting words with which the user is familiar. When pictures cannot be used to disambiguate a word or phrase, COMET has four strategies for avoiding unknown words. We give examples for each of these strategies and show how they are implemented in COMET. 1. Introduction A language generation system should select words that its user knows. While this would seem to involve simply selecting a known word instead of an unknown word (as is done, for example, in [1]), in many cases it requires entirely rephrasing the rest of the sentence. For example, in our domain of equipment maintenance and repair, if the user does not know the word &amp;quot;polarity,&amp;quot; a sentence like &amp;quot;Check the polarity.&apos; will be rephrased as &amp;quot;Make sure the plus on the battery lines up with the plus on the battery compartment.&amp;quot; Even when alternative words can be used instead of an unknown word (e.g., a descriptive expression can be used instead of an object name), the alternative phrase may interact with other parts of the sentence which then need to be reworded as well. In this paper, we di</context>
<context position="6236" citStr="[1]" startWordPosition="1000" endWordPosition="1000">act in many ways. Since syntactic and lexical constraints are only available within the text generator, lexical choice is delayed until this point. Thus COMET waits until a variety of semantic, pragmatic, syntactic and lexical constraints are accumulated before selecting words. This means that COMET can use syntactic and lexical constraints on word choice in conjunction with semantic and graphical constraints provided as input, plus the new pragmatic constraints we present. Previous work addressing pragmatic constraints on word usage folded lexical choice into the content planner (e.g., [13], [1]). This was possible since the work focused primarily on lexical side effects of content determination (e.g., what property to include in a reference as opposed to what linguistic form to use for a property). Such approaches do not allow a system to take syntactic and lexical constraints on word choice into account. On receiving the hierarchy of logical forms, the Lexical Chooser determines the overall grammatical form of each sentence based on the semantic structure of the LFs (e.g., conditional sentences are generated for precondition-action structures) and selects the words and phrases real</context>
</contexts>
<marker>1.</marker>
<rawString>Swartout, W.R., &amp;quot;XPLAIN: a system for creating and explaining expert consulting systems&amp;quot;, Artificial Intelligence, Vol. 21, No. 3, 1983, pp. 285-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feiner</author>
<author>K R McKeown</author>
</authors>
<title>Generating Coordinated Multimedia Explanations&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of the IEEE Conference on Al Applications,</booktitle>
<location>Santa Barbara, CA.,</location>
<contexts>
<context position="2995" citStr="[2]" startWordPosition="483" endWordPosition="483"> to the KY57&amp;quot; instead of &amp;quot;the COMSEC cable&amp;quot;) 4. Using past discourse to construct a referring expression (e.g., generating &amp;quot;Test the cable you just removed.&amp;quot; instead of &amp;quot;Test the COMSEC cable.&amp;quot; if the user had previously been instructed to remove this cable.) In the following sections, we first provide an overview of lexical choice in COMET, showing how and where it occurs in the overall system. Each of the strategies is then described in turn, prefaced by a brief discussion of disambiguation of unknown terms through pictures. Finally, we compare our work with previous work in the area. I See [2] for a system overview and [3, 4] for details on media 2This is similar to Appelt&apos;s [5] integration of language and coordination in COMET. physical actions for generating referring expressions. 226 (Knowledge Base Content Planner Logical Form A Media Coordinator I Annotated Logical Form Discourse History User Model Lexical CLexicon Chooser Gamma-i&gt; FUF Text Generator Text Graphics Generator Illustration Layout Manager if Multimedia Explanation Figure 1: COMET System Architecture 2. Lexical Choice and Architecture COMET&apos;s architecture is shown in Figure 1. On receiving a request for an explanat</context>
</contexts>
<marker>2.</marker>
<rawString>Feiner, S. and K.R. McKeown, &amp;quot;Generating Coordinated Multimedia Explanations&amp;quot;, Proceedings of the IEEE Conference on Al Applications, Santa Barbara, CA., March 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feiner</author>
<author>K R McKeown</author>
</authors>
<title>Coordinating Text and Graphics in Explanation Generation&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<location>Boston, Mass.,</location>
<contexts>
<context position="3028" citStr="[3, 4]" startWordPosition="489" endWordPosition="490">COMSEC cable&amp;quot;) 4. Using past discourse to construct a referring expression (e.g., generating &amp;quot;Test the cable you just removed.&amp;quot; instead of &amp;quot;Test the COMSEC cable.&amp;quot; if the user had previously been instructed to remove this cable.) In the following sections, we first provide an overview of lexical choice in COMET, showing how and where it occurs in the overall system. Each of the strategies is then described in turn, prefaced by a brief discussion of disambiguation of unknown terms through pictures. Finally, we compare our work with previous work in the area. I See [2] for a system overview and [3, 4] for details on media 2This is similar to Appelt&apos;s [5] integration of language and coordination in COMET. physical actions for generating referring expressions. 226 (Knowledge Base Content Planner Logical Form A Media Coordinator I Annotated Logical Form Discourse History User Model Lexical CLexicon Chooser Gamma-i&gt; FUF Text Generator Text Graphics Generator Illustration Layout Manager if Multimedia Explanation Figure 1: COMET System Architecture 2. Lexical Choice and Architecture COMET&apos;s architecture is shown in Figure 1. On receiving a request for an explanation via a menu interface, the con</context>
</contexts>
<marker>3.</marker>
<rawString>Feiner, S. and K.R. McKeown, &amp;quot;Coordinating Text and Graphics in Explanation Generation&amp;quot;, Proceedings of the National Conference on Artificial Intelligence, Boston, Mass., August 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feiner</author>
<author>K R McKeown</author>
</authors>
<title>Automating the Generation of Coordinated Multimedia Explanations&amp;quot;,</title>
<date>1991</date>
<journal>IEEE Computer,</journal>
<volume>24</volume>
<pages>33--41</pages>
<contexts>
<context position="3028" citStr="[3, 4]" startWordPosition="489" endWordPosition="490">COMSEC cable&amp;quot;) 4. Using past discourse to construct a referring expression (e.g., generating &amp;quot;Test the cable you just removed.&amp;quot; instead of &amp;quot;Test the COMSEC cable.&amp;quot; if the user had previously been instructed to remove this cable.) In the following sections, we first provide an overview of lexical choice in COMET, showing how and where it occurs in the overall system. Each of the strategies is then described in turn, prefaced by a brief discussion of disambiguation of unknown terms through pictures. Finally, we compare our work with previous work in the area. I See [2] for a system overview and [3, 4] for details on media 2This is similar to Appelt&apos;s [5] integration of language and coordination in COMET. physical actions for generating referring expressions. 226 (Knowledge Base Content Planner Logical Form A Media Coordinator I Annotated Logical Form Discourse History User Model Lexical CLexicon Chooser Gamma-i&gt; FUF Text Generator Text Graphics Generator Illustration Layout Manager if Multimedia Explanation Figure 1: COMET System Architecture 2. Lexical Choice and Architecture COMET&apos;s architecture is shown in Figure 1. On receiving a request for an explanation via a menu interface, the con</context>
<context position="9310" citStr="[4]" startWordPosition="1511" endWordPosition="1511">ation. Thus, COMET&apos; s architecture interleaves lexical choice and content planning in order to account for a wide variety of interacting constraints on word choice. 3. Multimedia Disambiguation An accompanying picture often makes clear what the referent of a referring expression is. If the user is unfamiliar with a term, the accompanying picture might define it. For example, Figure 2 shows one step of an explanation generated by COMET for loading frequency into the radio. The text refers to a &amp;quot;FCTN knob&amp;quot; and the accompanying picture clearly singles out the knob on the front panel of the radio [4]. COMET can also generate an explicit reference to the illustration itself (called a cross reference). For example, the cross reference shown in Figure 3 is generated if the user does not understand the term &amp;quot;holding battery&amp;quot;. In this case, the Lexical Chooser, on determining that &amp;quot;holding battery&amp;quot; is an unfamiliar term, reinvokes the content planner which finds that no accompanying illustration is currently planned and invokes graphics to generate an accompanying illustration that depicts the holding battery and its location. For full details on cross referencing in COMET see [18]. 4. Selecti</context>
</contexts>
<marker>4.</marker>
<rawString>Feiner, S. and McKeown, K.R., &amp;quot;Automating the Generation of Coordinated Multimedia Explanations&amp;quot;, IEEE Computer, Vol. 24, No. 10, October 1991, pp. 33-41.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D E Appelt</author>
</authors>
<title>Planning English Sentences,</title>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England,</location>
<contexts>
<context position="3082" citStr="[5]" startWordPosition="500" endWordPosition="500">erring expression (e.g., generating &amp;quot;Test the cable you just removed.&amp;quot; instead of &amp;quot;Test the COMSEC cable.&amp;quot; if the user had previously been instructed to remove this cable.) In the following sections, we first provide an overview of lexical choice in COMET, showing how and where it occurs in the overall system. Each of the strategies is then described in turn, prefaced by a brief discussion of disambiguation of unknown terms through pictures. Finally, we compare our work with previous work in the area. I See [2] for a system overview and [3, 4] for details on media 2This is similar to Appelt&apos;s [5] integration of language and coordination in COMET. physical actions for generating referring expressions. 226 (Knowledge Base Content Planner Logical Form A Media Coordinator I Annotated Logical Form Discourse History User Model Lexical CLexicon Chooser Gamma-i&gt; FUF Text Generator Text Graphics Generator Illustration Layout Manager if Multimedia Explanation Figure 1: COMET System Architecture 2. Lexical Choice and Architecture COMET&apos;s architecture is shown in Figure 1. On receiving a request for an explanation via a menu interface, the content planner uses schemas [6] to determine which infor</context>
<context position="17024" citStr="[19, 5, 13]" startWordPosition="2736" endWordPosition="2738">( ; if phenomenon realized by NP ((roles ((phenomenon ((cat #((under np))))))) ; then always choose &amp;quot;to check&amp;quot; (lex &amp;quot;check&amp;quot;)) ; if phenomenon realized by clause ((roles ((phenomenon ((cat #((under clause))))))) ; then randomly pick &amp;quot;to check&amp;quot; or ; &amp;quot;to make sure&amp;quot; (lex ((Ralt (&amp;quot;check&amp;quot; &amp;quot;make sure&amp;quot;))))))))) Figure 9: Lexicon Entry for Check Concept 5.2. Generating New Referential Descriptions If the user does not know an object name, the content planner is reinvoked to generate object attributes to build a referential description. Although our selection algorithm is not as sophisticated as others [19, 5, 13] because we do not use a detailed model of user beliefs, we address a new issue: the interaction between the new description and other parts of the original sentence which may require rephrasin. Two types of object attributes are used in a referring expression in COMET: object subpart relations and spatial relations to other objects in the accompanying illustration. COMET selects the relations that uniquely identify the object. For example, suppose COMET&apos; s Lexical Chooser is provided with the LF for sentence 1, Figure 10, but the user does not know the term &amp;quot;COMSEC.&amp;quot; Instead of generating sen</context>
<context position="24450" citStr="[5]" startWordPosition="3957" endWordPosition="3957">lustration, and past discourse in addition to traditional constraints from semantics, syntax, and other word choices. Although other generation systems take into account some of these constraints, COMET is the first attempt to integrate such a variety of constraints and lexical choice strategies in a single system. In addition, because COMET is a multimedia system, it can use the accompanying illustrations advantageously for disambiguation. WIP [20] can also generate cross references but does not rely on a user model for either cross reference generation or lexical choice. EPICURE [19] , KAMP [5], and FN [13] tailor references based on situation, but they do not constrain this choice based on the user&apos;s lexical knowledge. EPICURE uses the user&apos;s domain knowledge, KAMP mutual beliefs about the domain, and FN the user&apos;s domain knowledge in conjunction with rules on implicatures. They focus on the selection of appropriate properties to distinguish an object in generating references but do not choose between alternative wordings for the selected properties. None of these systems reword action descriptions or use past discourse to avoid terms the user does not know. While Bateman and Paris</context>
</contexts>
<marker>5.</marker>
<rawString>Appelt, D.E., Planning English Sentences, Cambridge University Press, Cambridge, England,</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text,</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England,</location>
<contexts>
<context position="3657" citStr="[6]" startWordPosition="582" endWordPosition="582">s is similar to Appelt&apos;s [5] integration of language and coordination in COMET. physical actions for generating referring expressions. 226 (Knowledge Base Content Planner Logical Form A Media Coordinator I Annotated Logical Form Discourse History User Model Lexical CLexicon Chooser Gamma-i&gt; FUF Text Generator Text Graphics Generator Illustration Layout Manager if Multimedia Explanation Figure 1: COMET System Architecture 2. Lexical Choice and Architecture COMET&apos;s architecture is shown in Figure 1. On receiving a request for an explanation via a menu interface, the content planner uses schemas [6] to determine which information should be included in the explanation from the underlying knowledge sources. The explanation content, represented as a hierarchy of logical forms (LFs) [7] is passed to the media coordinator [3, 8], which adds annotations indicating which portions are to be produced by the text generator and which by the graphics generator [9]. The Lexical Chooser is part of the text generator [7]. Typically, it selects a word or phrase for each semantic concept in the input LF (i.e., the semantic constraints on word choice). In terms of coverage, the implementation can select w</context>
</contexts>
<marker>6.</marker>
<rawString>McKeown, K.R., Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text, Cambridge University Press, Cambridge, England, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>M Elhadad</author>
<author>Y Fukumoto</author>
<author>J Lim</author>
<author>C Lombardi</author>
<author>J Robin</author>
<author>F Smadja</author>
</authors>
<title>Language Generation in COMET&amp;quot;,</title>
<date>1990</date>
<booktitle>in Current Research in Language Generation,</booktitle>
<editor>Mellish, C., Dale, R., and Zock, M., eds.,</editor>
<publisher>Academic Press,</publisher>
<location>London,</location>
<contexts>
<context position="3844" citStr="[7]" startWordPosition="609" endWordPosition="609">edia Coordinator I Annotated Logical Form Discourse History User Model Lexical CLexicon Chooser Gamma-i&gt; FUF Text Generator Text Graphics Generator Illustration Layout Manager if Multimedia Explanation Figure 1: COMET System Architecture 2. Lexical Choice and Architecture COMET&apos;s architecture is shown in Figure 1. On receiving a request for an explanation via a menu interface, the content planner uses schemas [6] to determine which information should be included in the explanation from the underlying knowledge sources. The explanation content, represented as a hierarchy of logical forms (LFs) [7] is passed to the media coordinator [3, 8], which adds annotations indicating which portions are to be produced by the text generator and which by the graphics generator [9]. The Lexical Chooser is part of the text generator [7]. Typically, it selects a word or phrase for each semantic concept in the input LF (i.e., the semantic constraints on word choice). In terms of coverage, the implementation can select words for 148 different semantic concepts using 253 mapping rules, thus yielding on average slightly less than two alternative word choices per concept (there are many concepts which are m</context>
</contexts>
<marker>7.</marker>
<rawString>McKeown, K.R., Elhadad, M., Fukumoto, Y., Lim, J., Lombardi, C., Robin, J., and Smadja, F., &amp;quot;Language Generation in COMET&amp;quot;, in Current Research in Language Generation, Mellish, C., Dale, R., and Zock, M., eds., Academic Press, London, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>D Seligmann</author>
<author>S Feiner</author>
<author>K McKeown</author>
</authors>
<title>A Common Intention Description Language for Interactive Multi-media Systems&amp;quot;, A New Generation of Intelligent Interfaces:</title>
<date></date>
<booktitle>Proceedings of IJCAI89 Workshop on Intelligent Interfaces,</booktitle>
<volume>22</volume>
<pages>46--52</pages>
<location>Detroit, MI,</location>
<contexts>
<context position="3886" citStr="[3, 8]" startWordPosition="616" endWordPosition="617">rm Discourse History User Model Lexical CLexicon Chooser Gamma-i&gt; FUF Text Generator Text Graphics Generator Illustration Layout Manager if Multimedia Explanation Figure 1: COMET System Architecture 2. Lexical Choice and Architecture COMET&apos;s architecture is shown in Figure 1. On receiving a request for an explanation via a menu interface, the content planner uses schemas [6] to determine which information should be included in the explanation from the underlying knowledge sources. The explanation content, represented as a hierarchy of logical forms (LFs) [7] is passed to the media coordinator [3, 8], which adds annotations indicating which portions are to be produced by the text generator and which by the graphics generator [9]. The Lexical Chooser is part of the text generator [7]. Typically, it selects a word or phrase for each semantic concept in the input LF (i.e., the semantic constraints on word choice). In terms of coverage, the implementation can select words for 148 different semantic concepts using 253 mapping rules, thus yielding on average slightly less than two alternative word choices per concept (there are many concepts which are mapped to a single word, while others have </context>
</contexts>
<marker>8.</marker>
<rawString>Elhadad, M., Seligmann, D., Feiner, S., and McKeown, K., &amp;quot;A Common Intention Description Language for Interactive Multi-media Systems&amp;quot;, A New Generation of Intelligent Interfaces: Proceedings of IJCAI89 Workshop on Intelligent Interfaces, Detroit, MI, August 22 1989, pp. 46-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Seligmann</author>
<author>S Feiner</author>
</authors>
<title>Specifying Composite Illustrations with Communicative Goals&amp;quot;,</title>
<date>1989</date>
<booktitle>Proc. ACM Symposium on User Interface Software and Technology,</booktitle>
<pages>1--9</pages>
<location>Williamsburg, VA,</location>
<contexts>
<context position="4017" citStr="[9]" startWordPosition="639" endWordPosition="639">er if Multimedia Explanation Figure 1: COMET System Architecture 2. Lexical Choice and Architecture COMET&apos;s architecture is shown in Figure 1. On receiving a request for an explanation via a menu interface, the content planner uses schemas [6] to determine which information should be included in the explanation from the underlying knowledge sources. The explanation content, represented as a hierarchy of logical forms (LFs) [7] is passed to the media coordinator [3, 8], which adds annotations indicating which portions are to be produced by the text generator and which by the graphics generator [9]. The Lexical Chooser is part of the text generator [7]. Typically, it selects a word or phrase for each semantic concept in the input LF (i.e., the semantic constraints on word choice). In terms of coverage, the implementation can select words for 148 different semantic concepts using 253 mapping rules, thus yielding on average slightly less than two alternative word choices per concept (there are many concepts which are mapped to a single word, while others have more than two alternatives). The lexicon contains 159 open class words. In this paper, we show how the user model and past discours</context>
</contexts>
<marker>9.</marker>
<rawString>Seligmann, D.D., and Feiner, S., &amp;quot;Specifying Composite Illustrations with Communicative Goals&amp;quot;, Proc. ACM Symposium on User Interface Software and Technology, Williamsburg, VA, November 13-15 1989, pp. 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D McDonald</author>
</authors>
<title>On the place of words in the generation process&amp;quot;,</title>
<date>1991</date>
<booktitle>in Natural Language Generation in Artificial Intelligence and Computational Linguistics,</booktitle>
<editor>Paris, C., Swartout, W. and Mann. W.C., eds.,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="4812" citStr="[10, 11]" startWordPosition="770" endWordPosition="771">In terms of coverage, the implementation can select words for 148 different semantic concepts using 253 mapping rules, thus yielding on average slightly less than two alternative word choices per concept (there are many concepts which are mapped to a single word, while others have more than two alternatives). The lexicon contains 159 open class words. In this paper, we show how the user model and past discourse (pragmatic constraints) also influence word choice. But these are not the only constraints on word choice. Syntactic form of the sentence and lexical constraints are other demonstrated [10, 11] influences on lexical choice. For example, once the verb has been chosen, syntactic constraints on its arguments (e.g., whether the object is a clause, 227 Figure 2: Accompanying Picture Clarifies Referent Load the frequency in channel one. Step 3 of 4 an adj, or np) will influence what words are chosen to realize the semantic concept that fill these arguments. Conversely, if one of the verb roles can only be realized as a noun phrase, for example, and not as other syntactic categories, this restricts which verb is selected. Lexical constraints on word choice arise from the use of collocation</context>
</contexts>
<marker>10.</marker>
<rawString>McDonald, D.D, &amp;quot;On the place of words in the generation process&amp;quot;, in Natural Language Generation in Artificial Intelligence and Computational Linguistics, Paris, C., Swartout, W. and Mann. W.C., eds., Kluwer Academic Publishers, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Danlos</author>
</authors>
<title>The Linguistic Basis of Text Generation,</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England,</location>
<contexts>
<context position="4812" citStr="[10, 11]" startWordPosition="770" endWordPosition="771">In terms of coverage, the implementation can select words for 148 different semantic concepts using 253 mapping rules, thus yielding on average slightly less than two alternative word choices per concept (there are many concepts which are mapped to a single word, while others have more than two alternatives). The lexicon contains 159 open class words. In this paper, we show how the user model and past discourse (pragmatic constraints) also influence word choice. But these are not the only constraints on word choice. Syntactic form of the sentence and lexical constraints are other demonstrated [10, 11] influences on lexical choice. For example, once the verb has been chosen, syntactic constraints on its arguments (e.g., whether the object is a clause, 227 Figure 2: Accompanying Picture Clarifies Referent Load the frequency in channel one. Step 3 of 4 an adj, or np) will influence what words are chosen to realize the semantic concept that fill these arguments. Conversely, if one of the verb roles can only be realized as a noun phrase, for example, and not as other syntactic categories, this restricts which verb is selected. Lexical constraints on word choice arise from the use of collocation</context>
</contexts>
<marker>11.</marker>
<rawString>Danlos, L., The Linguistic Basis of Text Generation, Cambridge University Press, Cambridge, England, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
<author>K R McKeown</author>
</authors>
<title>Automatically Extracting and Representing Collocations for Language Generation&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>252--9</pages>
<location>Pittsburgh, Pa.,</location>
<contexts>
<context position="5418" citStr="[12]" startWordPosition="873" endWordPosition="873">nfluences on lexical choice. For example, once the verb has been chosen, syntactic constraints on its arguments (e.g., whether the object is a clause, 227 Figure 2: Accompanying Picture Clarifies Referent Load the frequency in channel one. Step 3 of 4 an adj, or np) will influence what words are chosen to realize the semantic concept that fill these arguments. Conversely, if one of the verb roles can only be realized as a noun phrase, for example, and not as other syntactic categories, this restricts which verb is selected. Lexical constraints on word choice arise from the use of collocations [12]. For example, a verb like &amp;quot;stand&amp;quot; takes the preposition &amp;quot;on&amp;quot; for its location role, while the verb &amp;quot;turn&amp;quot; takes the preposition &amp;quot;onto.&amp;quot; Lexical choice is thus influenced by a wide variety of constraints which interact in many ways. Since syntactic and lexical constraints are only available within the text generator, lexical choice is delayed until this point. Thus COMET waits until a variety of semantic, pragmatic, syntactic and lexical constraints are accumulated before selecting words. This means that COMET can use syntactic and lexical constraints on word choice in conjunction with semanti</context>
</contexts>
<marker>12.</marker>
<rawString>Smadja, F. and K.R. McKeown, &amp;quot;Automatically Extracting and Representing Collocations for Language Generation&amp;quot;, Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, Pa., June 1990, pp. 252-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E B Reiter</author>
</authors>
<title>Generating appropriate natural language object description, PhD dissertation, Center for research in computing technology,</title>
<date>1990</date>
<institution>Harvard University,</institution>
<contexts>
<context position="6231" citStr="[13]" startWordPosition="999" endWordPosition="999"> interact in many ways. Since syntactic and lexical constraints are only available within the text generator, lexical choice is delayed until this point. Thus COMET waits until a variety of semantic, pragmatic, syntactic and lexical constraints are accumulated before selecting words. This means that COMET can use syntactic and lexical constraints on word choice in conjunction with semantic and graphical constraints provided as input, plus the new pragmatic constraints we present. Previous work addressing pragmatic constraints on word usage folded lexical choice into the content planner (e.g., [13], [1]). This was possible since the work focused primarily on lexical side effects of content determination (e.g., what property to include in a reference as opposed to what linguistic form to use for a property). Such approaches do not allow a system to take syntactic and lexical constraints on word choice into account. On receiving the hierarchy of logical forms, the Lexical Chooser determines the overall grammatical form of each sentence based on the semantic structure of the LFs (e.g., conditional sentences are generated for precondition-action structures) and selects the words and phrases</context>
<context position="17024" citStr="[19, 5, 13]" startWordPosition="2736" endWordPosition="2738">( ; if phenomenon realized by NP ((roles ((phenomenon ((cat #((under np))))))) ; then always choose &amp;quot;to check&amp;quot; (lex &amp;quot;check&amp;quot;)) ; if phenomenon realized by clause ((roles ((phenomenon ((cat #((under clause))))))) ; then randomly pick &amp;quot;to check&amp;quot; or ; &amp;quot;to make sure&amp;quot; (lex ((Ralt (&amp;quot;check&amp;quot; &amp;quot;make sure&amp;quot;))))))))) Figure 9: Lexicon Entry for Check Concept 5.2. Generating New Referential Descriptions If the user does not know an object name, the content planner is reinvoked to generate object attributes to build a referential description. Although our selection algorithm is not as sophisticated as others [19, 5, 13] because we do not use a detailed model of user beliefs, we address a new issue: the interaction between the new description and other parts of the original sentence which may require rephrasin. Two types of object attributes are used in a referring expression in COMET: object subpart relations and spatial relations to other objects in the accompanying illustration. COMET selects the relations that uniquely identify the object. For example, suppose COMET&apos; s Lexical Chooser is provided with the LF for sentence 1, Figure 10, but the user does not know the term &amp;quot;COMSEC.&amp;quot; Instead of generating sen</context>
<context position="24463" citStr="[13]" startWordPosition="3960" endWordPosition="3960">and past discourse in addition to traditional constraints from semantics, syntax, and other word choices. Although other generation systems take into account some of these constraints, COMET is the first attempt to integrate such a variety of constraints and lexical choice strategies in a single system. In addition, because COMET is a multimedia system, it can use the accompanying illustrations advantageously for disambiguation. WIP [20] can also generate cross references but does not rely on a user model for either cross reference generation or lexical choice. EPICURE [19] , KAMP [5], and FN [13] tailor references based on situation, but they do not constrain this choice based on the user&apos;s lexical knowledge. EPICURE uses the user&apos;s domain knowledge, KAMP mutual beliefs about the domain, and FN the user&apos;s domain knowledge in conjunction with rules on implicatures. They focus on the selection of appropriate properties to distinguish an object in generating references but do not choose between alternative wordings for the selected properties. None of these systems reword action descriptions or use past discourse to avoid terms the user does not know. While Bateman and Paris&apos; system [21]</context>
</contexts>
<marker>13.</marker>
<rawString>Reiter, E.B., Generating appropriate natural language object description, PhD dissertation, Center for research in computing technology, Harvard University, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>The FUF Functional Unifier: User&apos;s Manual&amp;quot;,</title>
<date>1988</date>
<tech>Tech. report,</tech>
<institution>Columbia University,</institution>
<contexts>
<context position="7019" citStr="[14, 15, 16]" startWordPosition="1121" endWordPosition="1123">nguistic form to use for a property). Such approaches do not allow a system to take syntactic and lexical constraints on word choice into account. On receiving the hierarchy of logical forms, the Lexical Chooser determines the overall grammatical form of each sentence based on the semantic structure of the LFs (e.g., conditional sentences are generated for precondition-action structures) and selects the words and phrases realizing semantic concepts of the LF. It passes a specification of the sentence&apos;s grammatical form and open-class words to the general purpose surface sentence generator FUF [14, 15, 16]. The Lexical Chooser uses a rewriting system itself implemented on top of FUF. Its lexicon consists of a base of rules, where each rule rewrites a given set of semantic features into a corresponding set of lexical and syntactic features. Thus, each lexicon entry associates a semantic concept with words that can be used to realize it. Additional constraints from the user model, past discourse, and the underlying knowledge base determine which of the alternative words or phrases should be selected.3 The user model indicates both the reading level of the current user4, any individual words that </context>
</contexts>
<marker>14.</marker>
<rawString>Elhadad, M., &amp;quot;The FUF Functional Unifier: User&apos;s Manual&amp;quot;, Tech. report, Columbia University, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>Types in Functional Unification Grammars&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of the 28th meeting of the Association for Computational Linguistics,</booktitle>
<location>Pittsburgh, Pa,</location>
<contexts>
<context position="7019" citStr="[14, 15, 16]" startWordPosition="1121" endWordPosition="1123">nguistic form to use for a property). Such approaches do not allow a system to take syntactic and lexical constraints on word choice into account. On receiving the hierarchy of logical forms, the Lexical Chooser determines the overall grammatical form of each sentence based on the semantic structure of the LFs (e.g., conditional sentences are generated for precondition-action structures) and selects the words and phrases realizing semantic concepts of the LF. It passes a specification of the sentence&apos;s grammatical form and open-class words to the general purpose surface sentence generator FUF [14, 15, 16]. The Lexical Chooser uses a rewriting system itself implemented on top of FUF. Its lexicon consists of a base of rules, where each rule rewrites a given set of semantic features into a corresponding set of lexical and syntactic features. Thus, each lexicon entry associates a semantic concept with words that can be used to realize it. Additional constraints from the user model, past discourse, and the underlying knowledge base determine which of the alternative words or phrases should be selected.3 The user model indicates both the reading level of the current user4, any individual words that </context>
</contexts>
<marker>15.</marker>
<rawString>Elhadad, M., &amp;quot;Types in Functional Unification Grammars&amp;quot;, Proceedings of the 28th meeting of the Association for Computational Linguistics, Pittsburgh, Pa, June 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>Using argumentation to control lexical choice: a unification-based implementation,</title>
<date>1993</date>
<institution>Computer Science Department, Columbia University,</institution>
<note>PhD dissertation,</note>
<contexts>
<context position="7019" citStr="[14, 15, 16]" startWordPosition="1121" endWordPosition="1123">nguistic form to use for a property). Such approaches do not allow a system to take syntactic and lexical constraints on word choice into account. On receiving the hierarchy of logical forms, the Lexical Chooser determines the overall grammatical form of each sentence based on the semantic structure of the LFs (e.g., conditional sentences are generated for precondition-action structures) and selects the words and phrases realizing semantic concepts of the LF. It passes a specification of the sentence&apos;s grammatical form and open-class words to the general purpose surface sentence generator FUF [14, 15, 16]. The Lexical Chooser uses a rewriting system itself implemented on top of FUF. Its lexicon consists of a base of rules, where each rule rewrites a given set of semantic features into a corresponding set of lexical and syntactic features. Thus, each lexicon entry associates a semantic concept with words that can be used to realize it. Additional constraints from the user model, past discourse, and the underlying knowledge base determine which of the alternative words or phrases should be selected.3 The user model indicates both the reading level of the current user4, any individual words that </context>
</contexts>
<marker>16.</marker>
<rawString>Elhadad, M., Using argumentation to control lexical choice: a unification-based implementation, PhD dissertation, Computer Science Department, Columbia University, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>J Robin</author>
</authors>
<title>Controlling Content Realization with Functional Unification Grammars&amp;quot;,</title>
<date>1992</date>
<booktitle>in Aspects of Automated Natural Language Generation,</booktitle>
<pages>89--104</pages>
<editor>Dale, R. and Hovy, H. and Roesner, D. and Stock, 0., ed.,</editor>
<publisher>Springler Verlag,</publisher>
<contexts>
<context position="8178" citStr="[17]" startWordPosition="1322" endWordPosition="1322">the current user4, any individual words that COMET knows the user does not understand, and any wording preferences (e.g., the user knows abbreviations, the user is familiar with military terminology). We make no claims about which of these forms of user models is easier to acquire, but simply show how to use them when available. If none of the alternative wordings for a given semantic concept of the LF are known to the user and the 3When these constraints come from knowledge sources external to FUF, the Lexical Chooser uses FUF extensions to access such knowledge through the use of coroutines [17]. 4We currently use two levels for a poor and good reader. At the beginning of the session, the reading level is either preset or COMET can ask the user. 228 Figure 3: Use of Cross References: Remove the holding battery, shown in the cutaway view Install the new holding battery. Step 2 of 6 Remove the old holding battery, shown in the cutaway view. accompanying illustration cannot disambiguate these words, COMET reinvokes the content planner to replan portions of the sentence content or to include additional semantic information. Thus, COMET&apos; s architecture interleaves lexical choice and conte</context>
</contexts>
<marker>17.</marker>
<rawString>Elhadad, M. and Robin, J., &amp;quot;Controlling Content Realization with Functional Unification Grammars&amp;quot;, in Aspects of Automated Natural Language Generation, Dale, R. and Hovy, H. and Roesner, D. and Stock, 0., ed., Springler Verlag, 1992, pp. 89-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>S K Feiner</author>
<author>J Robin</author>
<author>D Seligmann</author>
<author>M Tanenblatt</author>
</authors>
<title>Generating Cross References for Multimedia Explanations&amp;quot;,</title>
<date>1992</date>
<booktitle>Proceedings of AAAI-92, AAAI,</booktitle>
<contexts>
<context position="9898" citStr="[18]" startWordPosition="1603" endWordPosition="1603">of the radio [4]. COMET can also generate an explicit reference to the illustration itself (called a cross reference). For example, the cross reference shown in Figure 3 is generated if the user does not understand the term &amp;quot;holding battery&amp;quot;. In this case, the Lexical Chooser, on determining that &amp;quot;holding battery&amp;quot; is an unfamiliar term, reinvokes the content planner which finds that no accompanying illustration is currently planned and invokes graphics to generate an accompanying illustration that depicts the holding battery and its location. For full details on cross referencing in COMET see [18]. 4. Selecting a Familiar Word/phrase Whenever possible, COMET simply selects a familiar word over an unknown word from the list of alternatives in the lexicon. Figure 4 shows some paired sentences that COMET generates which illustrate alternative wordings. The first italicized phrase is generated if the user&apos;s vocabulary level is above a certain reading level or if a word is not explicitly listed in the user model as unknown. Since the lexicon maintains a simple association between the semantic concept and alternative phrasings, COMET selects the first alternative which the user model indicat</context>
</contexts>
<marker>18.</marker>
<rawString>McKeown, K. R., Feiner, S.K., Robin, J., Seligmann, D., and Tanenblatt, M., &amp;quot;Generating Cross References for Multimedia Explanations&amp;quot;, Proceedings of AAAI-92, AAAI, July 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
</authors>
<title>Generating Referring Expressions,</title>
<date>1992</date>
<booktitle>ACLMIT Press Series in Natural Language Processing,</booktitle>
<location>Cambridge, Ma.,</location>
<contexts>
<context position="17024" citStr="[19, 5, 13]" startWordPosition="2736" endWordPosition="2738">( ; if phenomenon realized by NP ((roles ((phenomenon ((cat #((under np))))))) ; then always choose &amp;quot;to check&amp;quot; (lex &amp;quot;check&amp;quot;)) ; if phenomenon realized by clause ((roles ((phenomenon ((cat #((under clause))))))) ; then randomly pick &amp;quot;to check&amp;quot; or ; &amp;quot;to make sure&amp;quot; (lex ((Ralt (&amp;quot;check&amp;quot; &amp;quot;make sure&amp;quot;))))))))) Figure 9: Lexicon Entry for Check Concept 5.2. Generating New Referential Descriptions If the user does not know an object name, the content planner is reinvoked to generate object attributes to build a referential description. Although our selection algorithm is not as sophisticated as others [19, 5, 13] because we do not use a detailed model of user beliefs, we address a new issue: the interaction between the new description and other parts of the original sentence which may require rephrasin. Two types of object attributes are used in a referring expression in COMET: object subpart relations and spatial relations to other objects in the accompanying illustration. COMET selects the relations that uniquely identify the object. For example, suppose COMET&apos; s Lexical Chooser is provided with the LF for sentence 1, Figure 10, but the user does not know the term &amp;quot;COMSEC.&amp;quot; Instead of generating sen</context>
<context position="24439" citStr="[19]" startWordPosition="3954" endWordPosition="3954">ompanying illustration, and past discourse in addition to traditional constraints from semantics, syntax, and other word choices. Although other generation systems take into account some of these constraints, COMET is the first attempt to integrate such a variety of constraints and lexical choice strategies in a single system. In addition, because COMET is a multimedia system, it can use the accompanying illustrations advantageously for disambiguation. WIP [20] can also generate cross references but does not rely on a user model for either cross reference generation or lexical choice. EPICURE [19] , KAMP [5], and FN [13] tailor references based on situation, but they do not constrain this choice based on the user&apos;s lexical knowledge. EPICURE uses the user&apos;s domain knowledge, KAMP mutual beliefs about the domain, and FN the user&apos;s domain knowledge in conjunction with rules on implicatures. They focus on the selection of appropriate properties to distinguish an object in generating references but do not choose between alternative wordings for the selected properties. None of these systems reword action descriptions or use past discourse to avoid terms the user does not know. While Batema</context>
</contexts>
<marker>19.</marker>
<rawString>Dale, R., Generating Referring Expressions, ACLMIT Press Series in Natural Language Processing, Cambridge, Ma., 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
<author>E Andre</author>
<author>M Hecicing</author>
<author>T Rist</author>
</authors>
<title>WIP: Knowledge-based Presentation of Information&amp;quot;,</title>
<date>1989</date>
<tech>Tech. report WIP-1,</tech>
<institution>German Research Center for Artificial Intelligence,</institution>
<contexts>
<context position="24300" citStr="[20]" startWordPosition="3931" endWordPosition="3931">ubsequent references, COMET can use previous discourse to avoid unknown words. COMET is thus using constraints from the user model, the accompanying illustration, and past discourse in addition to traditional constraints from semantics, syntax, and other word choices. Although other generation systems take into account some of these constraints, COMET is the first attempt to integrate such a variety of constraints and lexical choice strategies in a single system. In addition, because COMET is a multimedia system, it can use the accompanying illustrations advantageously for disambiguation. WIP [20] can also generate cross references but does not rely on a user model for either cross reference generation or lexical choice. EPICURE [19] , KAMP [5], and FN [13] tailor references based on situation, but they do not constrain this choice based on the user&apos;s lexical knowledge. EPICURE uses the user&apos;s domain knowledge, KAMP mutual beliefs about the domain, and FN the user&apos;s domain knowledge in conjunction with rules on implicatures. They focus on the selection of appropriate properties to distinguish an object in generating references but do not choose between alternative wordings for the sele</context>
</contexts>
<marker>20.</marker>
<rawString>Wahlster, W., Andre, E., Hecicing, M., and T. Rist, &amp;quot;WIP: Knowledge-based Presentation of Information&amp;quot;, Tech. report WIP-1, German Research Center for Artificial Intelligence, May 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bateman</author>
<author>C L Paris</author>
</authors>
<title>Phrasing a text in terms the user can understand&amp;quot;,</title>
<date>1989</date>
<booktitle>Proceedings of the 1 1 th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1511--1517</pages>
<location>Detroit, MI,</location>
<contexts>
<context position="25063" citStr="[21]" startWordPosition="4055" endWordPosition="4055">[13] tailor references based on situation, but they do not constrain this choice based on the user&apos;s lexical knowledge. EPICURE uses the user&apos;s domain knowledge, KAMP mutual beliefs about the domain, and FN the user&apos;s domain knowledge in conjunction with rules on implicatures. They focus on the selection of appropriate properties to distinguish an object in generating references but do not choose between alternative wordings for the selected properties. None of these systems reword action descriptions or use past discourse to avoid terms the user does not know. While Bateman and Paris&apos; system [21] uses different dialects depending on which class of users it is addressing through register mappings, in COMET different terms can be mixed and matched depending on the individual user model. Acknowledgements Research on language generation in COMET has been supported in part by Defense Advanced Research Projects Agency Contract N00039-84- C-0165, National Science Foundation Grants IRT-84-51438 and GER-90-2406, New York State Center for Advanced Technology Contracts NYSSTF-CAT(90)-053, (91)-053, and (92)-053, and Office of Naval Research Contracts N00014-82- K-0256 and N00014-894-1782. COMET&apos;</context>
</contexts>
<marker>21.</marker>
<rawString>Bateman, J.A. and Paris, C.L., &amp;quot;Phrasing a text in terms the user can understand&amp;quot;, Proceedings of the 1 1 th International Joint Conference on Artificial Intelligence, Detroit, MI, 1989, pp. 1511-1517.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>