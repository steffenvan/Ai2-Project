<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.9863595">
Lateen EM: Unsupervised Training with Multiple Objectives,
Applied to Dependency Grammar Induction
</title>
<author confidence="0.995691">
Valentin I. Spitkovsky Hiyan Alshawi
</author>
<affiliation confidence="0.993718">
Computer Science Department Google Inc.
Stanford University and Google Inc. Mountain View, CA, 94043, USA
</affiliation>
<email confidence="0.998477">
valentin@cs.stanford.edu hiyan@google.com
</email>
<author confidence="0.997511">
Daniel Jurafsky
</author>
<affiliation confidence="0.9040455">
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
</affiliation>
<email confidence="0.998715">
jurafsky@stanford.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945363636364">
We present new training methods that aim to
mitigate local optima and slow convergence in
unsupervised training by using additional im-
perfect objectives. In its simplest form, lateen
EM alternates between the two objectives of
ordinary “soft” and “hard” expectation max-
imization (EM) algorithms. Switching objec-
tives when stuck can help escape local optima.
We find that applying a single such alternation
already yields state-of-the-art results for En-
glish dependency grammar induction. More
elaborate lateen strategies track both objec-
tives, with each validating the moves proposed
by the other. Disagreements can signal earlier
opportunities to switch or terminate, saving it-
erations. De-emphasizing fixed points in these
ways eliminates some guesswork from tuning
EM. An evaluation against a suite of unsu-
pervised dependency parsing tasks, for a vari-
ety of languages, showed that lateen strategies
significantly speed up training of both EM al-
gorithms, and improve accuracy for hard EM.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999835565217392">
Expectation maximization (EM) algorithms (Demp-
ster et al., 1977) play important roles in learning
latent linguistic structure. Unsupervised techniques
from this family excel at core natural language pro-
cessing (NLP) tasks, including segmentation, align-
ment, tagging and parsing. Typical implementations
specify a probabilistic framework, pick an initial
model instance, and iteratively improve parameters
using EM. A key guarantee is that subsequent model
instances are no worse than the previous, according
to training data likelihood in the given framework.
Another attractive feature that helped make EM
instrumental (Meng, 2007) is its initial efficiency:
Training tends to begin with large steps in a param-
eter space, sometimes bypassing many local optima
at once. After a modest number of such iterations,
however, EM lands close to an attractor. Next, its
convergence rate necessarily suffers: Disproportion-
ately many (and ever-smaller) steps are needed to
finally approach this fixed point, which is almost in-
variably a local optimum. Deciding when to termi-
nate EM often involves guesswork; and finding ways
out of local optima requires trial and error. We pro-
pose several strategies that address both limitations.
Unsupervised objectives are, at best, loosely cor-
related with extrinsic performance (Pereira and Sch-
abes, 1992; Merialdo, 1994; Liang and Klein, 2008,
inter alia). This fact justifies (occasionally) devi-
ating from a prescribed training course. For exam-
ple, since multiple equi-plausible objectives are usu-
ally available, a learner could cycle through them,
optimizing alternatives when the primary objective
function gets stuck; or, instead of trying to escape, it
could aim to avoid local optima in the first place, by
halting search early if an improvement to one objec-
tive would come at the expense of harming another.
We test these general ideas by focusing on non-
convex likelihood optimization using EM. This set-
ting is standard and has natural and well-understood
objectives: the classic, “soft” EM; and Viterbi, or
“hard” EM (Kearns et al., 1997). The name “la-
teen” comes from the sea — triangular lateen sails
can take wind on either side, enabling sailing ves-
sels to tack (see Figure 1). As a captain can’t count
on favorable winds, so an unsupervised learner can’t
rely on co-operative gradients: soft EM maximizes
</bodyText>
<page confidence="0.957669">
1269
</page>
<note confidence="0.9815895">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1269–1280,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999787">
Figure 1: A triangular sail atop a traditional Arab sail-
</figureCaption>
<bodyText confidence="0.920563">
ing vessel, the dhow (right). Older square sails permitted
sailing only before the wind. But the efficient lateen sail
worked like a wing (with high pressure on one side and
low pressure on the other), allowing a ship to go almost
directly into a headwind. By tacking, in a zig-zag pattern,
it became possible to sail in any direction, provided there
was some wind at all (left). For centuries seafarers ex-
pertly combined both sails to traverse extensive distances,
greatly increasing the reach of medieval navigation.1
likelihoods of observed data across assignments to
hidden variables, whereas hard EM focuses on most
likely completions.2 These objectives are plausible,
yet both can be provably “wrong” (Spitkovsky et al.,
2010a, §7.3). Thus, it is permissible for lateen EM
to maneuver between their gradients, for example by
tacking around local attractors, in a zig-zag fashion.
</bodyText>
<sectionHeader confidence="0.770931" genericHeader="introduction">
2 The Lateen Family of Algorithms
</sectionHeader>
<bodyText confidence="0.9998935">
We propose several strategies that use a secondary
objective to improve over standard EM training. For
hard EM, the secondary objective is that of soft EM;
and vice versa if soft EM is the primary algorithm.
</bodyText>
<subsectionHeader confidence="0.974487">
2.1 Algorithm #1: Simple Lateen EM
</subsectionHeader>
<bodyText confidence="0.999958625">
Simple lateen EM begins by running standard EM
to convergence, using a user-supplied initial model,
primary objective and definition of convergence.
Next, the algorithm alternates. A single lateen al-
ternation involves two phases: (i) retraining using
the secondary objective, starting from the previ-
ous converged solution (once again iterating until
convergence, but now of the secondary objective);
</bodyText>
<footnote confidence="0.995806">
1Partially adapted from http://www.britannica.com/
EBchecked/topic/331395, http://allitera.tive.org/
archives/004922.html and http://landscapedvd.com/
desktops/images/ship1280x1024.jpg.
2See Brown et al.’s (1993, §6.2) definition of Viterbi train-
ing for a succinct justification of hard EM; in our case, the cor-
responding objective is Spitkovsky et al.’s (2010a, §7.1) ˆθVIT.
</footnote>
<bodyText confidence="0.999454875">
and (ii) retraining using the primary objective again,
starting from the latest converged solution (once
more to convergence of the primary objective). The
algorithm stops upon failing to sufficiently improve
the primary objective across alternations (applying
the standard convergence criterion end-to-end) and
returns the best of all models re-estimated during
training (as judged by the primary objective).
</bodyText>
<subsectionHeader confidence="0.994794">
2.2 Algorithm #2: Shallow Lateen EM
</subsectionHeader>
<bodyText confidence="0.999986333333333">
Same as algorithm #1, but switches back to optimiz-
ing the primary objective after a single step with the
secondary, during phase (i) of all lateen alternations.
Thus, the algorithm alternates between optimizing
a primary objective to convergence, then stepping
away, using one iteration of the secondary optimizer.
</bodyText>
<subsectionHeader confidence="0.996699">
2.3 Algorithm #3: Early-Stopping Lateen EM
</subsectionHeader>
<bodyText confidence="0.999830857142857">
This variant runs standard EM but quits early if
the secondary objective suffers. We redefine con-
vergence by “or”-ing the user-supplied termination
criterion (i.e., a “small-enough” change in the pri-
mary objective) with any adverse change of the sec-
ondary (i.e., an increase in its cross-entropy). Early-
stopping lateen EM does not alternate objectives.
</bodyText>
<subsectionHeader confidence="0.994176">
2.4 Algorithm #4: Early-Switching Lateen EM
</subsectionHeader>
<bodyText confidence="0.999989857142857">
Same as algorithm #1, but with the new definition
of convergence, as in algorithm #3. Early-switching
lateen EM halts primary optimizers as soon as they
hurt the secondary objective and stops secondary op-
timizers once they harm the primary objective. This
algorithm terminates when it fails to sufficiently im-
prove the primary objective across a full alternation.
</bodyText>
<subsectionHeader confidence="0.994248">
2.5 Algorithm #5: Partly-Switching Lateen EM
</subsectionHeader>
<bodyText confidence="0.999923333333333">
Same as algorithm #4, but again iterating primary
objectives to convergence, as in algorithm #1; sec-
ondary optimizers still continue to terminate early.
</bodyText>
<sectionHeader confidence="0.891109" genericHeader="method">
3 The Task and Study #1
</sectionHeader>
<bodyText confidence="0.994071285714286">
We chose to test the impact of these five lateen al-
gorithms on unsupervised dependency parsing — a
task in which EM plays an important role (Paskin,
2001; Klein and Manning, 2004; Gillenwater et al.,
2010, inter alia). This entailed two sets of exper-
iments: In study #1, we tested whether single al-
ternations of simple lateen EM (as defined in §2.1,
</bodyText>
<page confidence="0.997782">
1270
</page>
<tableCaption confidence="0.953458">
Table 1: Directed dependency accuracies (DDA) on Sec-
tion 23 of WSJ (all sentences) for recent state-of-the-art
</tableCaption>
<bodyText confidence="0.966950064516129">
systems and our two experiments (one unlexicalized and
one lexicalized) with a single alternation of lateen EM.
Algorithm #1) improve our recent publicly-available
system for English dependency grammar induction.
In study #2, we introduced a more sophisticated
methodology that uses factorial designs and regres-
sions to evaluate lateen strategies with unsupervised
dependency parsing in many languages, after also
controlling for other important sources of variation.
For study #1, our base system (Spitkovsky et al.,
2010b) is an instance of the popular (unlexicalized)
Dependency Model with Valence (Klein and Man-
ning, 2004). This model was trained using hard EM
on WSJ45 (WSJ sentences up to length 45) until suc-
cessive changes in per-token cross-entropy fell be-
low 2−20 bits (Spitkovsky et al., 2010b; 2010a, §4).3
We confirmed that the base model had indeed con-
verged, by running 10 steps of hard EM on WSJ45
and verifying that its objective did not change much.
Next, we applied a single alternation of simple la-
teen EM: first running soft EM (this took 101 steps,
using the same termination criterion), followed by
hard EM (again to convergence — another 23 it-
erations). The result was a decrease in hard EM’s
cross-entropy, from 3.69 to 3.59 bits per token (bpt),
accompanied by a 2.4% jump in accuracy, from 50.4
to 52.8%, on Section 23 of WSJ (see Table 1).4
Our first experiment showed that lateen EM holds
promise for simple models. Next, we tested it in
a more realistic setting, by re-estimating lexicalized
models,5 starting from the unlexicalized model’s
</bodyText>
<footnote confidence="0.997296777777778">
3http://nlp.stanford.edu/pubs/
markup-data.tar.bz2:dp.model.dmv
4It is standard practice to convert gold labeled constituents
from Penn English Treebank’s Wall Street Journal (WSJ) por-
tion (Marcus et al., 1993) into unlabeled reference dependency
parses using deterministic “head-percolation” rules (Collins,
1999); sentence root symbols (but not punctuation) arcs count
towards accuracies (Paskin, 2001; Klein and Manning, 2004).
5We used Headden et al.’s (2009) method (also the approach
</footnote>
<bodyText confidence="0.999819">
parses; this took 24 steps with hard EM. We then
applied another single lateen alternation: This time,
soft EM ran for 37 steps, hard EM took another 14,
and the new model again improved, by 1.3%, from
54.3 to 55.6% (see Table 1); the corresponding drop
in (lexicalized) cross-entropy was from 6.10 to 6.09
bpt. This last model is competitive with the state-of-
the-art; moreover, gains from single applications of
simple lateen alternations (2.4 and 1.3%) are on par
with the increase due to lexicalization alone (1.5%).
</bodyText>
<sectionHeader confidence="0.99937" genericHeader="method">
4 Methodology for Study #2
</sectionHeader>
<bodyText confidence="0.999986947368421">
Study #1 suggests that lateen EM can improve gram-
mar induction in English. To establish statistical sig-
nificance, however, it is important to test a hypothe-
sis in many settings (Ioannidis, 2005). We therefore
use a factorial experimental design and regression
analyses with a variety of lateen strategies. Two re-
gressions — one predicting accuracy, the other, the
number of iterations — capture the effects that la-
teen algorithms have on performance and efficiency,
relative to standard EM training. We controlled for
important dimensions of variation, such as the un-
derlying language: to make sure that our results are
not English-specific, we induced grammars in 19
languages. We also explored the impact from the
quality of an initial model (using both uniform and
ad hoc initializers), the choice of a primary objective
(i.e., soft or hard EM), and the quantity and com-
plexity of training data (shorter versus both short and
long sentences). Appendix A gives the full details.
</bodyText>
<subsectionHeader confidence="0.998211">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.979496">
We use all 23 train/test splits from the 2006/7
CoNLL shared tasks (Buchholz and Marsi, 2006;
Nivre et al., 2007),6 which cover 19 different lan-
guages.7 We splice out all punctuation labeled in the
data, as is standard practice (Paskin, 2001; Klein and
Manning, 2004), introducing new arcs from grand-
mothers to grand-daughters where necessary, both in
train- and test-sets. Evaluation is always against the
taken by the two stronger state-of-the-art systems): for words
seen at least 100 times in the training corpus, gold part-of-
speech tags are augmented with lexical items.
</bodyText>
<footnote confidence="0.97293825">
6These disjoint splits require smoothing; in the WSJ setting,
training and test sets overlapped (Klein and Manning, 2004).
7We down-weigh languages appearing in both years — Ara-
bic, Chinese, Czech and Turkish — by 50% in all our analyses.
</footnote>
<figure confidence="0.858492647058823">
System
DDA (%)
(Blunsom and Cohn, 2010)
(Gillenwater et al., 2010)
(Spitkovsky et al., 2010b)
55.7
53.3
50.4
+ soft EM + hard EM
lexicalized, using hard EM
+ soft EM + hard EM
52.8 (+2.4)
54.3 (+1.5)
55.6 (+1.3)
1271
entire resulting test sets (i.e., all sentence lengths).8
4.2 Grammar Models
</figure>
<bodyText confidence="0.995368428571428">
and hard EM.9 Three such baselines begin with hard
EM (marked with the subscript h); and three more
start with soft EM (marked with the subscript s).
In all remaining experiments we model grammars
via the original DMV, which ignores punctuation; all
models are unlexicalized, with gold part-of-speech
tags for word classes (Klein and Manning, 2004).
</bodyText>
<subsectionHeader confidence="0.998045">
5.2 Lateen Models
</subsectionHeader>
<bodyText confidence="0.999873">
Ten models, A{1, 2, 3, 4, 51{h,3}, correspond to our la-
teen algorithms #1–5 (§2), starting with either hard
or soft EM’s objective, to be used as the primary.
</bodyText>
<subsectionHeader confidence="0.999843">
4.3 Smoothing Mechanism
</subsectionHeader>
<bodyText confidence="0.9996115">
All unsmoothed models are smoothed immediately
prior to evaluation; some of the baseline models are
also smoothed during training. In both cases, we use
the “add-one” (a.k.a. Laplace) smoothing algorithm.
</bodyText>
<subsectionHeader confidence="0.997793">
4.4 Standard Convergence
</subsectionHeader>
<bodyText confidence="0.99792">
We always halt an optimizer once a change in its ob-
jective’s consecutive cross-entropy values falls be-
low 2−20 bpt (at which point we consider it “stuck”).
</bodyText>
<figure confidence="0.978246333333333">
Soft EM
Hard EM
Aa Ai Aa Ai
-2.7 x0.2 -2.0 x0.3
+0.6 x0.7 +0.6 x1.2
0.0 x2.0 +0.8 x3.7
0.0 x1.3 +5.5 x6.5
-0.0 x1.3 +1.5 x3.6
0.0 x0.7 -0.1 x0.7
0.0 x0.8 +3.0 x2.1
0.0 x1.2 +2.9 x3.8
Model
Baselines B3
B2
B1
Algorithms A1
A2
A3
A4
A5
6 Results
</figure>
<subsectionHeader confidence="0.99751">
4.5 Scoring Function
</subsectionHeader>
<bodyText confidence="0.9999545">
We report directed accuracies — fractions of cor-
rectly guessed (unlabeled) dependency arcs, includ-
ing arcs from sentence root symbols, as is standard
practice (Paskin, 2001; Klein and Manning, 2004).
Punctuation does not affect scoring, as it had been
removed from all parse trees in our data (see §4.1).
</bodyText>
<sectionHeader confidence="0.998921" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999944">
We now summarize our baseline models and briefly
review the proposed lateen algorithms. For details of
the default systems (standard soft and hard EM), all
control variables and both regressions (against final
accuracies and iteration counts) see Appendix A.
</bodyText>
<subsectionHeader confidence="0.97578">
5.1 Baseline Models
</subsectionHeader>
<bodyText confidence="0.999882888888889">
We tested a total of six baseline models, experiment-
ing with two types of alternatives: (i) strategies that
perturb stuck models directly, by smoothing, ignor-
ing secondary objectives; and (ii) shallow applica-
tions of a single EM step, ignoring convergence.
Baseline B1 alternates running standard EM to
convergence and smoothing. A second baseline, B2,
smooths after every step of EM instead. Another
shallow baseline, B3, alternates single steps of soft
</bodyText>
<tableCaption confidence="0.627790666666667">
8With the exception of Arabic ’07, from which we discarded
a single sentence containing 145 non-punctuation tokens.
Table 2: Estimated additive changes in directed depen-
</tableCaption>
<bodyText confidence="0.940728733333333">
dency accuracy (Da) and multiplicative changes in the
number of iterations before terminating (Di) for all base-
line models and lateen algorithms, relative to standard
training: soft EM (left) and hard EM (right). Bold en-
tries are statistically different (p &lt; 0.01) from zero, for
Da, and one, for Di (details in Table 4 and Appendix A).
Not one baseline attained a statistically significant
performance improvement. Shallow models B3{h,3},
in fact, significantly lowered accuracy: by 2.0%, on
average (p pz� 7.8 x 10−4), for B3h, which began with
hard EM; and down 2.7% on average (p Pz� 6.4x10−7),
for B33, started with soft EM. They were, however,
3–5x faster than standard training, on average (see
Table 4 for all estimates and associated p-values;
above, Table 2 shows a preview of the full results).
</bodyText>
<subsectionHeader confidence="0.982873">
6.1 A1{h,s} — Simple Lateen EM
</subsectionHeader>
<bodyText confidence="0.942817">
A1h runs 6.5x slower, but scores 5.5% higher, on av-
erage, compared to standard Viterbi training; A13 is
only 30% slower than standard soft EM, but does not
impact its accuracy at all, on average.
Figure 2 depicts a sample training run: Italian ’07
with A1h. Viterbi EM converges after 47 iterations,
9It approximates a mixture (the average of soft and hard
objectives) — a natural comparison, computable via gradients
and standard optimization algorithms, such as L-BFGS (Liu and
Nocedal, 1989). We did not explore exact interpolations, how-
ever, because replacing EM is itself a significant confounder,
even with unchanged objectives (Berg-Kirkpatrick et al., 2010).
</bodyText>
<page confidence="0.952301">
1272
</page>
<figure confidence="0.551139">
iteration 50 100 150 200 250 300
</figure>
<figureCaption confidence="0.999504571428571">
Figure 2: Cross-entropies for Italian ’07, initialized uni-
formly and trained on sentences up to length 45. The two
curves are primary and secondary objectives (soft EM’s
lies below, as sentence yields are at least as likely as parse
trees): shaded regions indicate iterations of hard EM (pri-
mary); and annotated values are measurements upon each
optimizer’s convergence (soft EM’s are parenthesized).
</figureCaption>
<bodyText confidence="0.9999805">
reducing the primary objective to 3.39 bpt (the sec-
ondary is then at 3.26); accuracy on the held-out set
is 41.8%. Three alternations of lateen EM (totaling
265 iterations) further decrease the primary objec-
tive to 3.29 bpt (the secondary also declines, to 3.22)
and accuracy increases to 56.2% (14.4% higher).
</bodyText>
<subsectionHeader confidence="0.983942">
6.2 A2{h,s} — Shallow Lateen EM
</subsectionHeader>
<bodyText confidence="0.99998675">
A2h runs 3.6x slower, but scores only 1.5% higher,
on average, compared to standard Viterbi training;
A23 is again 30% slower than standard soft EM and
also has no measurable impact on parsing accuracy.
</bodyText>
<subsectionHeader confidence="0.971513">
6.3 A3{h,s} — Early-Stopping Lateen EM
</subsectionHeader>
<bodyText confidence="0.999937125">
Both A3h and A33 run 30% faster, on average, than
standard training with hard or soft EM; and neither
heuristic causes a statistical change to accuracy.
Table 3 shows accuracies and iteration counts for
10 (of 23) train/test splits that terminate early with
A3s (in one particular, example setting). These runs
are nearly twice as fast, and only two score (slightly)
lower, compared to standard training using soft EM.
</bodyText>
<subsectionHeader confidence="0.968426">
6.4 A4{h,s} — Early-Switching Lateen EM
</subsectionHeader>
<bodyText confidence="0.99986925">
A4h runs only 2.1x slower, but scores only 3.0%
higher, on average, compared to standard Viterbi
training; A43 is, in fact, 20% faster than standard soft
EM, but still has no measurable impact on accuracy.
</bodyText>
<subsectionHeader confidence="0.835031">
6.5 A5{h,s} — Partly-Switching Lateen EM
</subsectionHeader>
<bodyText confidence="0.90441475">
A5h runs 3.8x slower, scoring 2.9% higher, on av-
erage, compared to standard Viterbi training; A5s is
20% slower than soft EM, but, again, no more accu-
rate. Indeed, A4 strictly dominates both A5 variants.
</bodyText>
<table confidence="0.997797615384615">
CoNLL Year Soft EM A33 iters
&amp; Language DDA iters DDA
Arabic 2006 28.4 180 28.4 118
Bulgarian ’06 39.1 253 39.6 131
Chinese ’06 49.4 268 49.4 204
Dutch ’06 21.3 246 27.8 35
Hungarian ’07 17.1 366 17.4 213
Italian ’07 39.6 194 39.6 164
Japanese ’06 56.6 113 56.6 93
Portuguese ’06 37.9 180 37.5 102
Slovenian ’06 30.8 234 31.1 118
Spanish ’06 33.3 125 33.1 73
Average: 35.4 216 36.1 125
</table>
<tableCaption confidence="0.99205775">
Table 3: Directed dependency accuracies (DDA) and iter-
ation counts for the 10 (of 23) train/test splits affected by
early termination (setting: soft EM’s primary objective,
trained using shorter sentences and ad-hoc initialization).
</tableCaption>
<sectionHeader confidence="0.997954" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999989">
Lateen strategies improve dependency grammar in-
duction in several ways. Early stopping offers a
clear benefit: 30% higher efficiency yet same perfor-
mance as standard training. This technique could be
used to (more) fairly compare learners with radically
different objectives (e.g., lexicalized and unlexical-
ized), requiring quite different numbers of steps (or
magnitude changes in cross-entropy) to converge.
The second benefit is improved performance, but
only starting with hard EM. Initial local optima dis-
covered by soft EM are such that the impact on ac-
curacy of all subsequent heuristics is indistinguish-
able from noise (it’s not even negative). But for hard
EM, lateen strategies consistently improve accuracy
— by 1.5, 3.0 or 5.5% — as an algorithm follows the
secondary objective longer (a single step, until the
primary objective gets worse, or to convergence).
Our results suggest that soft EM should use early
termination to improve efficiency. Hard EM, by con-
trast, could use any lateen strategy to improve either
efficiency or performance, or to strike a balance.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="method">
8 Related Work
</sectionHeader>
<subsectionHeader confidence="0.99985">
8.1 Avoiding and/or Escaping Local Attractors
</subsectionHeader>
<bodyText confidence="0.999671">
Simple lateen EM is similar to Dhillon et al.’s (2002)
refinement algorithm for text clustering with spher-
ical k-means. Their “ping-pong” strategy alternates
batch and incremental EM, exploits the strong points
of each, and improves a shared objective at every
</bodyText>
<figure confidence="0.997488631578947">
cross-entropies (in bits per token)
3.39
3.26
bpt
4.5
4.0
3.5
3.0
(3.19) 3.23
3.33
3.29
3.29
(3.42)
(3.39)
(3.39)
3.21
3.22
(3.18)
(3.18)
</figure>
<page confidence="0.960904">
1273
</page>
<bodyText confidence="0.999964222222222">
step. Unlike generalized (GEM) variants (Neal and
Hinton, 1999), lateen EM uses multiple objectives:
it sacrifices the primary in the short run, to escape
local optima; in the long run, it also does no harm,
by construction (as it returns the best model seen).
Of the meta-heuristics that use more than a stan-
dard, scalar objective, deterministic annealing (DA)
(Rose, 1998) is closest to lateen EM. DA perturbs
objective functions, instead of manipulating solu-
tions directly. As other continuation methods (All-
gower and Georg, 1990), it optimizes an easy (e.g.,
convex) function first, then “rides” that optimum by
gradually morphing functions towards the difficult
objective; each step reoptimizes from the previous
approximate solution. Smith and Eisner (2004) em-
ployed DA to improve part-of-speech disambigua-
tion, but found that objectives had to be further
“skewed,” using domain knowledge, before it helped
(constituent) grammar induction. (For this reason,
we did not experiment with DA, despite its strong
similarities to lateen EM.) Smith and Eisner (2004)
used a “temperature” 0 to anneal a flat uniform dis-
tribution (0 = 0) into soft EM’s non-convex objec-
tive (0 = 1). In their framework, hard EM corre-
sponds to 0 −→ oo, so the algorithms differ only in
their 0-schedule: DA’s is continuous, from 0 to 1; la-
teen EM’s is a discrete alternation, of 1 and +oo.10
</bodyText>
<subsectionHeader confidence="0.985713">
8.2 Terminating Early, Before Convergence
</subsectionHeader>
<bodyText confidence="0.9999101875">
EM is rarely run to (even numerical) convergence.
Fixing a modest number of iterations a priori (Klein,
2005, §5.3.4), running until successive likelihood ra-
tios become small (Spitkovsky et al., 2009, §4.1) or
using a combination of the two (Ravi and Knight,
2009, §4, Footnote 5) is standard practice in NLP.
Elworthy’s (1994, §5, Figure 1) analysis of part-of-
speech tagging showed that, in most cases, a small
number of iterations is actually preferable to conver-
gence, in terms of final accuracies: “regularization
by early termination” had been suggested for image
deblurring algorithms in statistical astronomy (Lucy,
1974, §2); and validation against held-out data — a
strategy proposed much earlier, in psychology (Lar-
son, 1931), has also been used as a halting crite-
rion in NLP (Yessenalina et al., 2010, §4.2, 5.2).
</bodyText>
<footnote confidence="0.9581545">
10One can think of this as a kind of “beam search” (Lowerre,
1976), with soft EM expanding and hard EM pruning a frontier.
</footnote>
<bodyText confidence="0.9972896">
Early-stopping lateen EM tethers termination to a
sign change in the direction of a secondary objective,
similarly to (cross-)validation (Stone, 1974; Geisser,
1975; Arlot and Celisse, 2010), but without splitting
data — it trains using all examples, at all times.11,12
</bodyText>
<subsectionHeader confidence="0.99727">
8.3 Training with Multiple Views
</subsectionHeader>
<bodyText confidence="0.998024823529412">
Lateen strategies may seem conceptually related to
co-training (Blum and Mitchell, 1998). However,
bootstrapping methods generally begin with some
labeled data and gradually label the rest (discrimina-
tively) as they grow more confident, but do not opti-
mize an explicit objective function; EM, on the other
hand, can be fully unsupervised, relabels all exam-
ples on each iteration (generatively), and guarantees
not to hurt a well-defined objective, at every step.13
Co-training classically relies on two views of the
data — redundant feature sets that allow different al-
gorithms to label examples for each other, yielding
“probably approximately correct” (PAC)-style guar-
antees under certain (strong) assumptions. In con-
trast, lateen EM uses the same data, features, model
and essentially the same algorithms, changing only
their objective functions: it makes no assumptions,
but guarantees not to harm the primary objective.
Some of these distinctions have become blurred
with time: Collins and Singer (1999) introduced
an objective function (also based on agreement)
into co-training; Goldman and Zhou (2000), Ng
and Cardie (2003) and Chan et al. (2004) made do
without redundant views; Balcan et al. (2004) re-
laxed other strong assumptions; and Zhou and Gold-
man (2004) generalized co-training to accommodate
three and more algorithms. Several such methods
have been applied to dependency parsing (Søgaard
and Rishøj, 2010), constituent parsing (Sarkar,
11We see in it a milder contrastive estimation (Smith and Eis-
ner, 2005a; 2005b), agnostic to implicit negative evidence, but
caring whence learners push probability mass towards training
examples: when most likely parse trees begin to benefit at the
expense of their sentence yields (or vice versa), optimizers halt.
</bodyText>
<footnote confidence="0.904736444444444">
12For a recently proposed instance of EM that uses cross-
validation (CV) to optimize smoothed data likelihoods (in learn-
ing synchronous PCFGs, for phrase-based machine translation),
see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm.
13Some authors (Nigam and Ghani, 2000; Ng and Cardie,
2003; Smith and Eisner, 2005a, §5.2, 7; §2; §6) draw a hard line
between bootstrapping algorithms, such as self- and co-training,
and probabilistic modeling using EM; others (Dasgupta et al.,
2001; Chang et al., 2007, §1; §5) tend to lump them together.
</footnote>
<page confidence="0.995915">
1274
</page>
<bodyText confidence="0.999868666666667">
2001) and parser reranking (Crim, 2002). Funda-
mentally, co-training exploits redundancies in unla-
beled data and/or learning algorithms. Lateen strate-
gies also exploit redundancies: in noisy objectives.
Both approaches use a second vantage point to im-
prove their perception of difficult training terrains.
</bodyText>
<sectionHeader confidence="0.978286" genericHeader="evaluation">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999623">
Lateen strategies can improve performance and effi-
ciency for dependency grammar induction with the
DMV. Early-stopping lateen EM is 30% faster than
standard training, without affecting accuracy — it
reduces guesswork in terminating EM. At the other
extreme, simple lateen EM is slower, but signifi-
cantly improves accuracy — by 5.5%, on average
— for hard EM, escaping some of its local optima.
It would be interesting to apply lateen algorithms
to advanced parsing models (Blunsom and Cohn,
2010; Headden et al., 2009, inter alia) and learn-
ing algorithms (Gillenwater et al., 2010; Cohen and
Smith, 2009, inter alia). Future work could explore
other NLP tasks — such as clustering, sequence la-
beling, segmentation and alignment — that often
employ EM. Our meta-heuristics are multi-faceted,
featuring aspects of iterated local search, determin-
istic annealing, cross-validation, contrastive estima-
tion and co-training. They may be generally useful
in machine learning and non-convex optimization.
</bodyText>
<sectionHeader confidence="0.946008" genericHeader="conclusions">
Appendix A. Experimental Design
</sectionHeader>
<bodyText confidence="0.9910688">
Statistical techniques are vital to many aspects of
computational linguistics (Johnson, 2009; Charniak,
1997; Abney, 1996, inter alia). We used factorial
designs,14 which are standard throughout the natu-
ral and social sciences, to assist with experimental
design and statistical analyses. Combined with or-
dinary regressions, these methods provide succinct
and interpretable summaries that explain which set-
tings meaningfully contribute to changes in depen-
dent variables, such as running time and accuracy.
14We used full factorial designs for clarity of exposition. But
many fewer experiments would suffice, especially in regression
models without interaction terms: for the more efficient frac-
tional factorial designs, as well as for randomized block designs
and full factorial designs, see Montgomery (2005, Ch. 4–9).
</bodyText>
<subsectionHeader confidence="0.99188">
9.1 Dependent Variables
</subsectionHeader>
<bodyText confidence="0.9999890625">
We constructed two regressions, for two types of de-
pendent variables: to summarize performance, we
predict accuracies; and to summarize efficiency, we
predict (logarithms of) iterations before termination.
In the performance regression, we used four dif-
ferent scores for the dependent variable. These in-
clude both directed accuracies and undirected accu-
racies, each computed in two ways: (i) using a best
parse tree; and (ii) using all parse trees. These four
types of scores provide different kinds of informa-
tion. Undirected scores ignore polarity of parent-
child relations (Paskin, 2001; Klein and Manning,
2004; Schwartz et al., 2011), partially correcting for
some effects of alternate analyses (e.g., systematic
choices between modals and main verbs for heads
of sentences, determiners for noun phrases, etc.).
And integrated scoring, using the inside-outside al-
gorithm (Baker, 1979) to compute expected accu-
racy across all — not just best — parse trees, has the
advantage of incorporating probabilities assigned to
individual arcs: This metric is more sensitive to the
margins that separate best from next-best parse trees,
and is not affected by tie-breaking. We tag scores
using two binary predictors in a simple (first order,
multi-linear) regression, where having multiple rel-
evant quality assessments improves goodness-of-fit.
In the efficiency regression, dependent variables
are logarithms of the numbers of iterations. Wrap-
ping EM in an inner loop of a heuristic has a mul-
tiplicative effect on the total number of models re-
estimated prior to termination. Consequently, loga-
rithms of the final counts better fit the observed data.
</bodyText>
<subsectionHeader confidence="0.980761">
9.2 Independent Predictors
</subsectionHeader>
<bodyText confidence="0.999967">
All of our predictors are binary indicators (a.k.a.
“dummy” variables). The undirected and integrated
factors only affect the regression for accuracies (see
Table 4, left); remaining factors participate also in
the running times regression (see Table 4, right). In a
default run, all factors are zero, corresponding to the
intercept estimated by a regression; other estimates
reflect changes in the dependent variable associated
with having that factor “on” instead of “off.”
</bodyText>
<listItem confidence="0.78577">
• adhoc — This setting controls initialization.
By default, we use the uninformed uniform ini-
tializer (Spitkovsky et al., 2010a); when it is
</listItem>
<page confidence="0.965699">
1275
</page>
<figure confidence="0.918366783783784">
Regression for Accuracies Regression for ln(Iterations)
Goodness-of-Fit: (R2adj ≈ 76.2%) (R2adj ≈ 82.4%)
βˆ adj. p-value βˆ mult. eˆo adj. p-value
coeff. coeff.
18.1 &lt; 2.0 × 10 −16
-0.9 ≈ 7.0 × 10 −7
30.9 &lt; 2.0 × 10 −16 5.5 255.8 &lt; 2.0 × 10 −16
1.2 ≈ 3.1 × 10 −13 -0.0 1.0 ≈ 1.0
1.0 ≈ 3.1 × 10 −9 -0.2 0.8 &lt; 2.0 × 10 −16
-2.7 ≈ 6.4 × 10 −7 -1.5 0.2 &lt; 2.0 × 10 −16
-2.0 ≈ 7.8 × 10 −4 -1.2 0.3 &lt; 2.0 × 10 −16
0.6 ≈ 1.0 -0.4 0.7 ≈ 1.4 × 10 −12
0.0 ≈ 1.0 0.7 2.0 &lt; 2.0 × 10 −16
0.0 ≈ 1.0 0.2 1.3 ≈ 4.1 × 10 −4
-0.0 ≈ 1.0 0.2 1.3 ≈ 5.8 × 10 −4
0.0 ≈ 1.0 -0.3 0.7 ≈ 2.6 × 10 −7
0.0 ≈ 1.0 -0.3 0.8 ≈ 2.6 × 10 −7
0.0 ≈ 1.0 0.2 1.2 ≈ 4.2 × 10 −3
-4.0 ≈ 5.7 × 10 −16 -1.7 0.2 &lt; 2.0 × 10 −16
0.6 ≈ 1.0 0.2 1.2 ≈ 5.6 × 10 −2
0.8 ≈ 1.0 1.3 3.7 &lt; 2.0 × 10 −16
5.5 &lt; 2.0 × 10 −16 1.9 6.5 &lt; 2.0 × 10 −16
1.5 ≈ 5.0 × 10 −2 1.3 3.6 &lt; 2.0 × 10 −16
-0.1 ≈ 1.0 -0.4 0.7 ≈ 1.7 × 10 −11
3.0 ≈ 1.0 × 10 −8 0.7 2.1 &lt; 2.0 × 10 −16
2.9 ≈ 7.6 × 10 −8 1.3 3.8 &lt; 2.0 × 10 −16
Indicator Factors
undirected
integrated
(intercept)
adhoc
sweet
shallow (soft-first)
shallow (hard-first)
Model
B33
B3h
</figure>
<table confidence="0.969646666666667">
B23 shallow smooth
B13 smooth
A13 simple lateen
A23 shallow lateen
A33 early-stopping lateen
A43 early-switching lateen
A53 partly-switching lateen
viterbi
B2h shallow smooth
B1h smooth
A1h simple lateen
A2h shallow lateen
A3h early-stopping lateen
A4h early-switching lateen
A5h partly-switching lateen
</table>
<tableCaption confidence="0.910051">
Table 4: Regressions for accuracies and natural-log-iterations, using 86 binary predictors (all p-values jointly adjusted
for simultaneous hypothesis testing; {langyearl indicators not shown). Accuracies’ estimated coefficients βˆ that are
statistically different from 0 — and iteration counts’ multipliers eˆo significantly different from 1 — are shown in bold.
</tableCaption>
<bodyText confidence="0.997362666666667">
on, we use Klein and Manning’s (2004) “ad-
hoc” harmonic heuristic, bootstrapped using
sentences up to length 10, from the training set.
</bodyText>
<listItem confidence="0.997399444444444">
• sweet — This setting controls the length cut-
off. By default, we train with all sentences con-
taining up to 45 tokens; when it is on, we use
Spitkovsky et al.’s (2009) “sweet spot” cutoff
of 15 tokens (recommended for English, WSJ).
• viterbi — This setting controls the primary ob-
jective of the learning algorithm. By default,
we run soft EM; when it is on, we use hard EM.
• {langyeari}22
</listItem>
<bodyText confidence="0.96165325">
i=1 — This is a set of 22 mutually-
exclusive selectors for the language/year of a
train/test split; default (all zeros) is English ’07.
Due to space limitations, we exclude langyear pre-
dictors from Table 4. Further, we do not explore
(even two-way) interactions between predictors.15
15This approach may miss some interesting facts, e.g., that
the adhoc initializer is exceptionally good for English, with soft
</bodyText>
<subsectionHeader confidence="0.999139">
9.3 Statistical Significance
</subsectionHeader>
<bodyText confidence="0.9999816">
Our statistical analyses relied on the R package (R
Development Core Team, 2011), which does not,
by default, adjust statistical significance (p-values)
for multiple hypotheses testing.16 We corrected
this using the Holm-Bonferroni method (Holm,
1979), which is uniformly more powerful than the
older (Dunn-)Bonferroni procedure; since we tested
many fewer hypotheses (44 + 42 — one per inter-
cept/coefficient ˆβ) than settings combinations, its ad-
justments to the p-values are small (see Table 4).17
</bodyText>
<footnote confidence="0.84204225">
EM. Instead it yields coarse summaries of regularities supported
by overwhelming evidence across data and training regimes.
16Since we would expect p% of randomly chosen hypotheses
to appear significant at the p% level simply by chance, we must
take precautions against these and other “data-snooping” biases.
17We adjusted the p-values for all 86 hypotheses jointly, us-
ing http://rss.acs.unt.edu/Rdoc/library/multtest/
html/mt.rawp2adjp.html.
</footnote>
<page confidence="0.930323">
1276
</page>
<table confidence="0.999950423076923">
CoNLL Year A3s iters Soft EM A3h iters Hard EM A1h iters
&amp; Language DDA DDA iters DDA DDA iters DDA
Arabic 2006 28.4 118 28.4 162 21.6 19 21.6 21 32.1 200
’7 26.9 171 24.7 17 24.8 24 22.0 239
Basque ’7 39.9 180 32.0 16 32.2 20 43.6 128
Bulgarian ’6 39.6 131 39.1 253 41.6 22 41.5 25 44.3 140
Catalan ’7 58.5 135 50.1 48 50.1 54 63.8 279
Chinese ’6 49.4 204 49.4 268 31.3 24 31.6 55 37.9 378
’7 46.0 262 30.0 25 30.2 64 34.5 307
Czech ’6 50.5 294 27.8 27 27.7 33 35.2 445
’7 49.8 263 29.0 37 29.0 41 31.4 307
Danish ’6 – 43.5 116 43.8 31 43.9 45 44.0 289
Dutch ’6 27.8 35 21.3 246 24.9 44 24.9 49 32.5 241
English ’7 38.1 180 34.0 32 33.9 42 34.9 186
German ’6 – 33.3 136 25.4 20 25.4 39 33.5 155
Greek ’7 17.5 230 18.3 18 18.3 21 21.4 117
Hungarian ’7 17.4 213 17.1 366 12.3 26 12.4 36 23.0 246
Italian ’7 39.6 164 39.6 194 32.6 25 32.6 27 37.6 273
Japanese ’6 56.6 93 56.6 113 49.6 20 49.7 23 53.5 91
Portuguese ’6 37.5 102 37.9 180 28.6 27 28.9 41 34.4 134
Slovenian ’6 31.1 118 30.8 234 23.4 22 33.6 255
Spanish ’6 33.1 73 33.3 125 18.2 29 18.4 36 33.3 235
Swedish ’6 – 41.8 242 36.0 24 36.1 29 42.5 296
Turkish ’6 29.8 303 17.8 19 22.2 38 31.9 134
’7 28.3 227 14.0 9 10.7 31 33.4 242
Average: 37.4 162 37.0 206 30.0 26 30.0 35 37.1 221
</table>
<tableCaption confidence="0.966689333333333">
Table 5: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and
efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple
lateen EM with hard EM’s primary objective (A1h), for all 23 train/test splits, with adhoc and sweet settings on.
</tableCaption>
<subsectionHeader confidence="0.940663">
9.4 Interpretation
</subsectionHeader>
<bodyText confidence="0.999977333333333">
Table 4 shows the estimated coefficients and their
(adjusted) p-values for both intercepts and most pre-
dictors (excluding the language/year of the data sets)
for all 1,840 experiments. The default (English) sys-
tem uses soft EM, trains with both short and long
sentences, and starts from an uninformed uniform
initializer. It is estimated to score 30.9%, converging
after approximately 256 iterations (both intercepts
are statistically different from zero: p &lt; 2.0 × 10−16).
As had to be the case, we detect a gain from undi-
rected scoring; integrated scoring is slightly (but
significantly: p ≈ 7.0 × 10−7) negative, which is re-
assuring: best parses are scoring higher than the rest
and may be standing out by large margins. The ad-
hoc initializer boosts accuracy by 1.2%, overall (also
significant: p ≈ 3.1 × 10−13), without a measurable
impact on running time (p ≈ 1.0). Training with
fewer, shorter sentences, at the sweet spot gradation,
adds 1.0% and shaves 20% off the total number of it-
erations, on average (both estimates are significant).
We find the viterbi objective harmful — by 4.0%,
on average (p ≈ 5.7 × 10−16) — for the CoNLL sets.
Spitkovsky et al. (2010a) reported that it helps on
WSJ, at least with long sentences and uniform ini-
tializers. Half of our experiments are with shorter
sentences, and half use ad hoc initializers (i.e., three
quarters of settings are not ideal for Viterbi EM),
which may have contributed to this negative result;
still, our estimates do confirm that hard EM is sig-
nificantly (80%, p &lt; 2.0 × 10−16) faster than soft EM.
</bodyText>
<subsectionHeader confidence="0.993887">
9.5 More on Viterbi Training
</subsectionHeader>
<bodyText confidence="0.9423144">
The overall negative impact of Viterbi objectives is
a cause for concern: On average, A1h’s estimated
gain of 5.5% should more than offset the expected
4.0% loss from starting with hard EM. But it is, nev-
ertheless, important to make sure that simple lateen
EM with hard EM’s primary objective is in fact an
improvement over both standard EM algorithms.
Table 5 shows performance and efficiency num-
bers for A1h, A3{h,s}, as well as standard soft and
hard EM, using settings that are least favorable for
</bodyText>
<page confidence="0.915434">
1277
</page>
<table confidence="0.999938384615385">
CoNLL Year A3s iters Soft EM A3h iters Hard EM A1h iters
&amp; Language DDA DDA iters DDA DDA iters DDA
Arabic 2006 – 33.4 317 20.8 8 20.2 32 16.6 269
’7 18.6 60 8.7 252 26.5 9 26.4 14 49.5 171
Basque ’7 – 18.3 245 23.2 16 23.0 23 24.0 162
Bulgarian ’6 27.0 242 27.1 293 40.6 33 40.5 34 43.9 276
Catalan ’7 15.0 74 13.8 159 53.2 30 53.1 31 59.8 176
Chinese ’6 63.5 131 63.6 261 36.8 45 36.8 47 44.5 213
’7 58.5 130 58.5 258 35.2 20 35.0 48 43.2 372
Czech ’6 29.5 125 29.7 224 23.6 18 23.8 41 27.7 179
’7 – 25.9 215 27.1 37 27.2 64 28.4 767
Danish ’6 – 16.6 155 28.7 30 28.7 30 38.3 241
Dutch ’6 20.4 51 21.2 174 25.5 30 25.6 38 27.8 243
English ’7 18.0 162 38.7 35 45.2 366
German ’6 – 24.4 148 30.1 39 30.1 44 30.4 185
Greek ’7 25.5 133 25.3 156 13.2 27 13.2 252
Hungarian ’7 – 18.9 310 28.9 34 28.9 44 34.7 414
Italian ’7 25.4 127 25.3 165 52.3 36 52.3 81
Japanese ’6 – 39.3 143 42.2 38 42.4 48 50.2 199
Portuguese ’6 35.2 48 35.6 224 34.5 21 36.7 143
Slovenian ’6 24.8 182 25.3 397 28.8 17 28.8 20 32.2 121
Spanish ’6 27.7 252 28.3 31 50.6 130
Swedish ’6 27.9 49 32.6 287 45.2 22 45.6 52 50.0 314
Turkish ’6 30.5 239 30.2 16 30.6 24 29.0 138
’7 48.8 254 34.3 24 33.1 34 35.9 269
Average: 27.3 161 27.3 225 33.2 28 33.2 35 38.2 236
</table>
<tableCaption confidence="0.952367666666667">
Table 6: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and
efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple
lateen EM with hard EM’s primary objective (A1h), for all 23 train/test splits, with setting adhoc off and sweet on.
</tableCaption>
<bodyText confidence="0.998974266666667">
Viterbi training: adhoc and sweet on. Although A1h
scores 7.1% higher than hard EM, on average, it is
only slightly better than soft EM — up 0.1% (and
worse than A1s). Without adhoc (i.e., using uniform
initializers — see Table 6), however, hard EM still
improves, by 3.2%, on average, whereas soft EM
drops nearly 10%; here, A1h further improves over
hard EM, scoring 38.2% (up 5.0), higher than soft
EM’s accuracies from both settings (27.3 and 37.0).
This suggests that A1h is indeed better than both
standard EM algorithms. We suspect that our exper-
imental set-up may be disadvantageous for Viterbi
training, since half the settings use ad hoc initializ-
ers, and because CoNLL sets are small. (Viterbi EM
works best with more data and longer sentences.)
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995188125">
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Angel X. Chang, Spence Green,
David McClosky, Fernando Pereira, Slav Petrov and the anony-
mous reviewers, for many helpful comments on draft versions
of this paper, and Andrew Y. Ng, for a stimulating discussion.
First author is grateful to Lynda K. Dunnigan for first introduc-
ing him to lateen sails, among other connections, in Humanities.
</bodyText>
<page confidence="0.989675">
1278
</page>
<sectionHeader confidence="0.979908" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99924659223301">
S. Abney. 1996. Statistical methods and linguistics. In
J. L. Klavans and P. Resnik, editors, The Balancing
Act: Combining Symbolic and Statistical Approaches
to Language. MIT Press.
E. L. Allgower and K. Georg. 1990. Numerical Contin-
uation Methods: An Introduction. Springer-Verlag.
S. Arlot and A. Celisse. 2010. A survey of cross-
validation procedures for model selection. Statistics
Surveys, 4.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society ofAmerica.
M.-F. Balcan, A. Blum, and K. Yang. 2004. Co-training
and expansion: Towards bridging theory and practice.
In NIPS.
T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In NAACL-HLT.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In EMNLP.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
J. Chan, I. Koprinska, and J. Poon. 2004. Co-training
with a single natural feature set applied to email clas-
sification. In WI.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
E. Charniak. 1997. Statistical techniques for natural lan-
guage parsing. AI Magazine, 18.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In NAACL-HLT.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Crim. 2002. Co-training re-rankers for improved
parser accuracy.
S. Dasgupta, M. L. Littman, and D. McAllester. 2001.
PAC generalization bounds for co-training. In NIPS.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B, 39.
I. S. Dhillon, Y. Guan, and J. Kogan. 2002. Iterative
clustering of high dimensional text data augmented by
local search. In ICDM.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In ANLP.
S. Geisser. 1975. The predictive sample reuse method
with applications. Journal of the American Statistical
Association, 70.
J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, University of
Pennsylvania.
S. Goldman and Y. Zhou. 2000. Enhancing supervised
learning with unlabeled data. In ICML.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
S. Holm. 1979. A simple sequentially rejective multiple
test procedure. Scandinavian Journal of Statistics, 6.
J. P. A. Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2.
M. Johnson. 2009. How the statistical revolution
changes (computational) linguistics. In EACL: In-
teraction between Linguistics and Computational Lin-
guistics: Virtuous, Vicious or Vacuous?
M. Kearns, Y. Mansour, and A. Y. Ng. 1997. An
information-theoretic analysis of hard and soft assign-
ment methods for clustering. In UAI.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
S. C. Larson. 1931. The shrinkage of the coefficient of
multiple correlation. Journal of Educational Psychol-
ogy, 22.
P. Liang and D. Klein. 2008. Analyzing the errors of
unsupervised learning. In HLT-ACL.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming. Series B, 45.
B. T. Lowerre. 1976. The HARPY Speech Recognition
System. Ph.D. thesis, CMU.
L. B. Lucy. 1974. An iterative technique for the recti-
fication of observed distributions. The Astronomical
Journal, 79.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
</reference>
<page confidence="0.865283">
1279
</page>
<reference confidence="0.999824529411765">
X.-L. Meng. 2007. EM and MCMC: Workhorses for sci-
entific computing (thirty years of EM and much more).
Statistica Sinica, 17.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20.
D. C. Montgomery. 2005. Design and Analysis ofExper-
iments. John Wiley &amp; Sons, 6th edition.
M. Mylonakis and K. Sima’an. 2010. Learning prob-
abilistic synchronous CFGs for phrase-based transla-
tion. In CoNLL.
R. M. Neal and G. E. Hinton. 1999. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In M. I. Jordan, editor, Learning in Graphical
Models. MIT Press.
V. Ng and C. Cardie. 2003. Weakly supervised natural
language learning without redundant views. In HLT-
NAACL.
K. Nigam and R. Ghani. 2000. Analyzing the effective-
ness and applicability of co-training. In CIKM.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In ACL.
R Development Core Team, 2011. R: A Language and
Environmentfor Statistical Computing. R Foundation
for Statistical Computing.
S. Ravi and K. Knight. 2009. Minimized models for un-
supervised part-of-speech tagging. In ACL-IJCNLP.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression and related opt-
mization problems. Proceedings of the IEEE, 86.
A. Sarkar. 2001. Applying co-training methods to statis-
tical parsing. In NAACL.
R. Schwartz, O. Abend, R. Reichart, and A. Rappoport.
2011. Neutralizing linguistically problematic annota-
tions in unsupervised dependency parsing evaluation.
In ACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In IJ-
CAI: Grammatical Inference Applications.
A. Søgaard and C. Rishøj. 2010. Semi-supervised de-
pendency parsing using generalized tri-training. In
COLING.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009.
Baby Steps: How “Less is More” in unsupervised de-
pendency parsing. In NIPS: Grammar Induction, Rep-
resentation ofLanguage and Language Learning.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010a. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In ACL.
M. Stone. 1974. Cross-validatory choice and assessment
of statistical predictions. Journal of the Royal Statisti-
cal Society. Series B, 36.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In EMNLP.
Y. Zhou and S. Goldman. 2004. Democratic co-learning.
In ICTAI.
</reference>
<page confidence="0.988057">
1280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325194">
<title confidence="0.9976575">Lateen EM: Unsupervised Training with Multiple Applied to Dependency Grammar Induction</title>
<author confidence="0.999697">Valentin I Spitkovsky Hiyan Alshawi</author>
<affiliation confidence="0.99997">Computer Science Department Google Inc.</affiliation>
<address confidence="0.886819">Stanford University and Google Inc. Mountain View, CA, 94043, USA</address>
<email confidence="0.996793">valentin@cs.stanford.eduhiyan@google.com</email>
<author confidence="0.936736">Daniel</author>
<affiliation confidence="0.81325">Departments of Linguistics and Computer Stanford University, Stanford, CA, 94305,</affiliation>
<email confidence="0.999644">jurafsky@stanford.edu</email>
<abstract confidence="0.983124130434783">We present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imobjectives. In its simplest form, between the two objectives of ordinary “soft” and “hard” expectation maximization (EM) algorithms. Switching objectives when stuck can help escape local optima. We find that applying a single such alternation already yields state-of-the-art results for English dependency grammar induction. More lateen strategies track objectives, with each validating the moves proposed by the other. Disagreements can signal earlier opportunities to switch or terminate, saving iterations. De-emphasizing fixed points in these ways eliminates some guesswork from tuning EM. An evaluation against a suite of unsupervised dependency parsing tasks, for a variety of languages, showed that lateen strategies significantly speed up training of both EM algorithms, and improve accuracy for hard EM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Statistical methods and linguistics.</title>
<date>1996</date>
<booktitle>The Balancing Act: Combining Symbolic and Statistical Approaches to Language.</booktitle>
<editor>In J. L. Klavans and P. Resnik, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="27451" citStr="Abney, 1996" startWordPosition="4300" endWordPosition="4301">rning algorithms (Gillenwater et al., 2010; Cohen and Smith, 2009, inter alia). Future work could explore other NLP tasks — such as clustering, sequence labeling, segmentation and alignment — that often employ EM. Our meta-heuristics are multi-faceted, featuring aspects of iterated local search, deterministic annealing, cross-validation, contrastive estimation and co-training. They may be generally useful in machine learning and non-convex optimization. Appendix A. Experimental Design Statistical techniques are vital to many aspects of computational linguistics (Johnson, 2009; Charniak, 1997; Abney, 1996, inter alia). We used factorial designs,14 which are standard throughout the natural and social sciences, to assist with experimental design and statistical analyses. Combined with ordinary regressions, these methods provide succinct and interpretable summaries that explain which settings meaningfully contribute to changes in dependent variables, such as running time and accuracy. 14We used full factorial designs for clarity of exposition. But many fewer experiments would suffice, especially in regression models without interaction terms: for the more efficient fractional factorial designs, a</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>S. Abney. 1996. Statistical methods and linguistics. In J. L. Klavans and P. Resnik, editors, The Balancing Act: Combining Symbolic and Statistical Approaches to Language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Allgower</author>
<author>K Georg</author>
</authors>
<title>Numerical Continuation Methods: An Introduction.</title>
<date>1990</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="21512" citStr="Allgower and Georg, 1990" startWordPosition="3382" endWordPosition="3386">.5 4.0 3.5 3.0 (3.19) 3.23 3.33 3.29 3.29 (3.42) (3.39) (3.39) 3.21 3.22 (3.18) (3.18) 1273 step. Unlike generalized (GEM) variants (Neal and Hinton, 1999), lateen EM uses multiple objectives: it sacrifices the primary in the short run, to escape local optima; in the long run, it also does no harm, by construction (as it returns the best model seen). Of the meta-heuristics that use more than a standard, scalar objective, deterministic annealing (DA) (Rose, 1998) is closest to lateen EM. DA perturbs objective functions, instead of manipulating solutions directly. As other continuation methods (Allgower and Georg, 1990), it optimizes an easy (e.g., convex) function first, then “rides” that optimum by gradually morphing functions towards the difficult objective; each step reoptimizes from the previous approximate solution. Smith and Eisner (2004) employed DA to improve part-of-speech disambiguation, but found that objectives had to be further “skewed,” using domain knowledge, before it helped (constituent) grammar induction. (For this reason, we did not experiment with DA, despite its strong similarities to lateen EM.) Smith and Eisner (2004) used a “temperature” 0 to anneal a flat uniform distribution (0 = 0</context>
</contexts>
<marker>Allgower, Georg, 1990</marker>
<rawString>E. L. Allgower and K. Georg. 1990. Numerical Continuation Methods: An Introduction. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Arlot</author>
<author>A Celisse</author>
</authors>
<title>A survey of crossvalidation procedures for model selection.</title>
<date>2010</date>
<journal>Statistics Surveys,</journal>
<volume>4</volume>
<contexts>
<context position="23530" citStr="Arlot and Celisse, 2010" startWordPosition="3708" endWordPosition="3711">ion by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Training with Multiple Views Lateen strategies may seem conceptually related to co-training (Blum and Mitchell, 1998). However, bootstrapping methods generally begin with some labeled data and gradually label the rest (discriminatively) as they grow more confident, but do not optimize an explicit objective function; EM, on the other hand, can be fully unsupervised, relabels all examples on each iteration (generatively), and guarantees not to hurt a well-defined objective, at every step.13 Co-training classicall</context>
</contexts>
<marker>Arlot, Celisse, 2010</marker>
<rawString>S. Arlot and A. Celisse. 2010. A survey of crossvalidation procedures for model selection. Statistics Surveys, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</booktitle>
<contexts>
<context position="29062" citStr="Baker, 1979" startWordPosition="4537" endWordPosition="4538">ependent variable. These include both directed accuracies and undirected accuracies, each computed in two ways: (i) using a best parse tree; and (ii) using all parse trees. These four types of scores provide different kinds of information. Undirected scores ignore polarity of parentchild relations (Paskin, 2001; Klein and Manning, 2004; Schwartz et al., 2011), partially correcting for some effects of alternate analyses (e.g., systematic choices between modals and main verbs for heads of sentences, determiners for noun phrases, etc.). And integrated scoring, using the inside-outside algorithm (Baker, 1979) to compute expected accuracy across all — not just best — parse trees, has the advantage of incorporating probabilities assigned to individual arcs: This metric is more sensitive to the margins that separate best from next-best parse trees, and is not affected by tie-breaking. We tag scores using two binary predictors in a simple (first order, multi-linear) regression, where having multiple relevant quality assessments improves goodness-of-fit. In the efficiency regression, dependent variables are logarithms of the numbers of iterations. Wrapping EM in an inner loop of a heuristic has a multi</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-F Balcan</author>
<author>A Blum</author>
<author>K Yang</author>
</authors>
<title>Co-training and expansion: Towards bridging theory and practice.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="24851" citStr="Balcan et al. (2004)" startWordPosition="3909" endWordPosition="3912">xamples for each other, yielding “probably approximately correct” (PAC)-style guarantees under certain (strong) assumptions. In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12For a recently proposed instance o</context>
</contexts>
<marker>Balcan, Blum, Yang, 2004</marker>
<rawString>M.-F. Balcan, A. Blum, and K. Yang. 2004. Co-training and expansion: Towards bridging theory and practice. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>A Bouchard-Cˆot´e</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In NAACL-HLT.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and D. Klein. 2010. Painless unsupervised learning with features. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="23731" citStr="Blum and Mitchell, 1998" startWordPosition="3738" endWordPosition="3741">ology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Training with Multiple Views Lateen strategies may seem conceptually related to co-training (Blum and Mitchell, 1998). However, bootstrapping methods generally begin with some labeled data and gradually label the rest (discriminatively) as they grow more confident, but do not optimize an explicit objective function; EM, on the other hand, can be fully unsupervised, relabels all examples on each iteration (generatively), and guarantees not to hurt a well-defined objective, at every step.13 Co-training classically relies on two views of the data — redundant feature sets that allow different algorithms to label examples for each other, yielding “probably approximately correct” (PAC)-style guarantees under certa</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
</authors>
<title>Unsupervised induction of tree substitution grammars for dependency parsing.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="12686" citStr="Blunsom and Cohn, 2010" startWordPosition="1942" endWordPosition="1945">skin, 2001; Klein and Manning, 2004), introducing new arcs from grandmothers to grand-daughters where necessary, both in train- and test-sets. Evaluation is always against the taken by the two stronger state-of-the-art systems): for words seen at least 100 times in the training corpus, gold part-ofspeech tags are augmented with lexical items. 6These disjoint splits require smoothing; in the WSJ setting, training and test sets overlapped (Klein and Manning, 2004). 7We down-weigh languages appearing in both years — Arabic, Chinese, Czech and Turkish — by 50% in all our analyses. System DDA (%) (Blunsom and Cohn, 2010) (Gillenwater et al., 2010) (Spitkovsky et al., 2010b) 55.7 53.3 50.4 + soft EM + hard EM lexicalized, using hard EM + soft EM + hard EM 52.8 (+2.4) 54.3 (+1.5) 55.6 (+1.3) 1271 entire resulting test sets (i.e., all sentence lengths).8 4.2 Grammar Models and hard EM.9 Three such baselines begin with hard EM (marked with the subscript h); and three more start with soft EM (marked with the subscript s). In all remaining experiments we model grammars via the original DMV, which ignores punctuation; all models are unlexicalized, with gold part-of-speech tags for word classes (Klein and Manning, 20</context>
<context position="26797" citStr="Blunsom and Cohn, 2010" startWordPosition="4206" endWordPosition="4209">pproaches use a second vantage point to improve their perception of difficult training terrains. 9 Conclusions and Future Work Lateen strategies can improve performance and efficiency for dependency grammar induction with the DMV. Early-stopping lateen EM is 30% faster than standard training, without affecting accuracy — it reduces guesswork in terminating EM. At the other extreme, simple lateen EM is slower, but significantly improves accuracy — by 5.5%, on average — for hard EM, escaping some of its local optima. It would be interesting to apply lateen algorithms to advanced parsing models (Blunsom and Cohn, 2010; Headden et al., 2009, inter alia) and learning algorithms (Gillenwater et al., 2010; Cohen and Smith, 2009, inter alia). Future work could explore other NLP tasks — such as clustering, sequence labeling, segmentation and alignment — that often employ EM. Our meta-heuristics are multi-faceted, featuring aspects of iterated local search, deterministic annealing, cross-validation, contrastive estimation and co-training. They may be generally useful in machine learning and non-convex optimization. Appendix A. Experimental Design Statistical techniques are vital to many aspects of computational l</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="11924" citStr="Buchholz and Marsi, 2006" startWordPosition="1820" endWordPosition="1823">y, relative to standard EM training. We controlled for important dimensions of variation, such as the underlying language: to make sure that our results are not English-specific, we induced grammars in 19 languages. We also explored the impact from the quality of an initial model (using both uniform and ad hoc initializers), the choice of a primary objective (i.e., soft or hard EM), and the quantity and complexity of training data (shorter versus both short and long sentences). Appendix A gives the full details. 4.1 Data Sets We use all 23 train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007),6 which cover 19 different languages.7 We splice out all punctuation labeled in the data, as is standard practice (Paskin, 2001; Klein and Manning, 2004), introducing new arcs from grandmothers to grand-daughters where necessary, both in train- and test-sets. Evaluation is always against the taken by the two stronger state-of-the-art systems): for words seen at least 100 times in the training corpus, gold part-ofspeech tags are augmented with lexical items. 6These disjoint splits require smoothing; in the WSJ setting, training and test sets overlapped (Klein and Manning, </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chan</author>
<author>I Koprinska</author>
<author>J Poon</author>
</authors>
<title>Co-training with a single natural feature set applied to email classification.</title>
<date>2004</date>
<booktitle>In WI.</booktitle>
<contexts>
<context position="24797" citStr="Chan et al. (2004)" startWordPosition="3900" endWordPosition="3903">ture sets that allow different algorithms to label examples for each other, yielding “probably approximately correct” (PAC)-style guarantees under certain (strong) assumptions. In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa),</context>
</contexts>
<marker>Chan, Koprinska, Poon, 2004</marker>
<rawString>J. Chan, I. Koprinska, and J. Poon. 2004. Co-training with a single natural feature set applied to email classification. In WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-W Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Guiding semi-supervision with constraint-driven learning.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="25924" citStr="Chang et al., 2007" startWordPosition="4071" endWordPosition="4074">ikely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13Some authors (Nigam and Ghani, 2000; Ng and Cardie, 2003; Smith and Eisner, 2005a, §5.2, 7; §2; §6) draw a hard line between bootstrapping algorithms, such as self- and co-training, and probabilistic modeling using EM; others (Dasgupta et al., 2001; Chang et al., 2007, §1; §5) tend to lump them together. 1274 2001) and parser reranking (Crim, 2002). Fundamentally, co-training exploits redundancies in unlabeled data and/or learning algorithms. Lateen strategies also exploit redundancies: in noisy objectives. Both approaches use a second vantage point to improve their perception of difficult training terrains. 9 Conclusions and Future Work Lateen strategies can improve performance and efficiency for dependency grammar induction with the DMV. Early-stopping lateen EM is 30% faster than standard training, without affecting accuracy — it reduces guesswork in te</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-supervision with constraint-driven learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical techniques for natural language parsing.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<contexts>
<context position="27438" citStr="Charniak, 1997" startWordPosition="4298" endWordPosition="4299">er alia) and learning algorithms (Gillenwater et al., 2010; Cohen and Smith, 2009, inter alia). Future work could explore other NLP tasks — such as clustering, sequence labeling, segmentation and alignment — that often employ EM. Our meta-heuristics are multi-faceted, featuring aspects of iterated local search, deterministic annealing, cross-validation, contrastive estimation and co-training. They may be generally useful in machine learning and non-convex optimization. Appendix A. Experimental Design Statistical techniques are vital to many aspects of computational linguistics (Johnson, 2009; Charniak, 1997; Abney, 1996, inter alia). We used factorial designs,14 which are standard throughout the natural and social sciences, to assist with experimental design and statistical analyses. Combined with ordinary regressions, these methods provide succinct and interpretable summaries that explain which settings meaningfully contribute to changes in dependent variables, such as running time and accuracy. 14We used full factorial designs for clarity of exposition. But many fewer experiments would suffice, especially in regression models without interaction terms: for the more efficient fractional factori</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical techniques for natural language parsing. AI Magazine, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="26905" citStr="Cohen and Smith, 2009" startWordPosition="4224" endWordPosition="4227">s and Future Work Lateen strategies can improve performance and efficiency for dependency grammar induction with the DMV. Early-stopping lateen EM is 30% faster than standard training, without affecting accuracy — it reduces guesswork in terminating EM. At the other extreme, simple lateen EM is slower, but significantly improves accuracy — by 5.5%, on average — for hard EM, escaping some of its local optima. It would be interesting to apply lateen algorithms to advanced parsing models (Blunsom and Cohn, 2010; Headden et al., 2009, inter alia) and learning algorithms (Gillenwater et al., 2010; Cohen and Smith, 2009, inter alia). Future work could explore other NLP tasks — such as clustering, sequence labeling, segmentation and alignment — that often employ EM. Our meta-heuristics are multi-faceted, featuring aspects of iterated local search, deterministic annealing, cross-validation, contrastive estimation and co-training. They may be generally useful in machine learning and non-convex optimization. Appendix A. Experimental Design Statistical techniques are vital to many aspects of computational linguistics (Johnson, 2009; Charniak, 1997; Abney, 1996, inter alia). We used factorial designs,14 which are </context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="24651" citStr="Collins and Singer (1999)" startWordPosition="3877" endWordPosition="3880">eratively), and guarantees not to hurt a well-defined objective, at every step.13 Co-training classically relies on two views of the data — redundant feature sets that allow different algorithms to label examples for each other, yielding “probably approximately correct” (PAC)-style guarantees under certain (strong) assumptions. In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push </context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10114" citStr="Collins, 1999" startWordPosition="1526" endWordPosition="1527"> accompanied by a 2.4% jump in accuracy, from 50.4 to 52.8%, on Section 23 of WSJ (see Table 1).4 Our first experiment showed that lateen EM holds promise for simple models. Next, we tested it in a more realistic setting, by re-estimating lexicalized models,5 starting from the unlexicalized model’s 3http://nlp.stanford.edu/pubs/ markup-data.tar.bz2:dp.model.dmv 4It is standard practice to convert gold labeled constituents from Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) into unlabeled reference dependency parses using deterministic “head-percolation” rules (Collins, 1999); sentence root symbols (but not punctuation) arcs count towards accuracies (Paskin, 2001; Klein and Manning, 2004). 5We used Headden et al.’s (2009) method (also the approach parses; this took 24 steps with hard EM. We then applied another single lateen alternation: This time, soft EM ran for 37 steps, hard EM took another 14, and the new model again improved, by 1.3%, from 54.3 to 55.6% (see Table 1); the corresponding drop in (lexicalized) cross-entropy was from 6.10 to 6.09 bpt. This last model is competitive with the state-ofthe-art; moreover, gains from single applications of simple late</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Crim</author>
</authors>
<title>Co-training re-rankers for improved parser accuracy.</title>
<date>2002</date>
<contexts>
<context position="26006" citStr="Crim, 2002" startWordPosition="4087" endWordPosition="4088">), optimizers halt. 12For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13Some authors (Nigam and Ghani, 2000; Ng and Cardie, 2003; Smith and Eisner, 2005a, §5.2, 7; §2; §6) draw a hard line between bootstrapping algorithms, such as self- and co-training, and probabilistic modeling using EM; others (Dasgupta et al., 2001; Chang et al., 2007, §1; §5) tend to lump them together. 1274 2001) and parser reranking (Crim, 2002). Fundamentally, co-training exploits redundancies in unlabeled data and/or learning algorithms. Lateen strategies also exploit redundancies: in noisy objectives. Both approaches use a second vantage point to improve their perception of difficult training terrains. 9 Conclusions and Future Work Lateen strategies can improve performance and efficiency for dependency grammar induction with the DMV. Early-stopping lateen EM is 30% faster than standard training, without affecting accuracy — it reduces guesswork in terminating EM. At the other extreme, simple lateen EM is slower, but significantly </context>
</contexts>
<marker>Crim, 2002</marker>
<rawString>J. Crim. 2002. Co-training re-rankers for improved parser accuracy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
<author>M L Littman</author>
<author>D McAllester</author>
</authors>
<title>PAC generalization bounds for co-training.</title>
<date>2001</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="25904" citStr="Dasgupta et al., 2001" startWordPosition="4067" endWordPosition="4070">g examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13Some authors (Nigam and Ghani, 2000; Ng and Cardie, 2003; Smith and Eisner, 2005a, §5.2, 7; §2; §6) draw a hard line between bootstrapping algorithms, such as self- and co-training, and probabilistic modeling using EM; others (Dasgupta et al., 2001; Chang et al., 2007, §1; §5) tend to lump them together. 1274 2001) and parser reranking (Crim, 2002). Fundamentally, co-training exploits redundancies in unlabeled data and/or learning algorithms. Lateen strategies also exploit redundancies: in noisy objectives. Both approaches use a second vantage point to improve their perception of difficult training terrains. 9 Conclusions and Future Work Lateen strategies can improve performance and efficiency for dependency grammar induction with the DMV. Early-stopping lateen EM is 30% faster than standard training, without affecting accuracy — it red</context>
</contexts>
<marker>Dasgupta, Littman, McAllester, 2001</marker>
<rawString>S. Dasgupta, M. L. Littman, and D. McAllester. 2001. PAC generalization bounds for co-training. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B,</journal>
<volume>39</volume>
<contexts>
<context position="1495" citStr="Dempster et al., 1977" startWordPosition="205" endWordPosition="209">r English dependency grammar induction. More elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. Disagreements can signal earlier opportunities to switch or terminate, saving iterations. De-emphasizing fixed points in these ways eliminates some guesswork from tuning EM. An evaluation against a suite of unsupervised dependency parsing tasks, for a variety of languages, showed that lateen strategies significantly speed up training of both EM algorithms, and improve accuracy for hard EM. 1 Introduction Expectation maximization (EM) algorithms (Dempster et al., 1977) play important roles in learning latent linguistic structure. Unsupervised techniques from this family excel at core natural language processing (NLP) tasks, including segmentation, alignment, tagging and parsing. Typical implementations specify a probabilistic framework, pick an initial model instance, and iteratively improve parameters using EM. A key guarantee is that subsequent model instances are no worse than the previous, according to training data likelihood in the given framework. Another attractive feature that helped make EM instrumental (Meng, 2007) is its initial efficiency: Trai</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Dhillon</author>
<author>Y Guan</author>
<author>J Kogan</author>
</authors>
<title>Iterative clustering of high dimensional text data augmented by local search.</title>
<date>2002</date>
<booktitle>In ICDM.</booktitle>
<marker>Dhillon, Guan, Kogan, 2002</marker>
<rawString>I. S. Dhillon, Y. Guan, and J. Kogan. 2002. Iterative clustering of high dimensional text data augmented by local search. In ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In ANLP.</booktitle>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geisser</author>
</authors>
<title>The predictive sample reuse method with applications.</title>
<date>1975</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>70</volume>
<contexts>
<context position="23504" citStr="Geisser, 1975" startWordPosition="3706" endWordPosition="3707">s: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Training with Multiple Views Lateen strategies may seem conceptually related to co-training (Blum and Mitchell, 1998). However, bootstrapping methods generally begin with some labeled data and gradually label the rest (discriminatively) as they grow more confident, but do not optimize an explicit objective function; EM, on the other hand, can be fully unsupervised, relabels all examples on each iteration (generatively), and guarantees not to hurt a well-defined objective, at every step</context>
</contexts>
<marker>Geisser, 1975</marker>
<rawString>S. Geisser. 1975. The predictive sample reuse method with applications. Journal of the American Statistical Association, 70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>F Pereira</author>
<author>B Taskar</author>
</authors>
<title>Posterior sparsity in unsupervised dependency parsing.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, Taskar, 2010</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and B. Taskar. 2010. Posterior sparsity in unsupervised dependency parsing. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldman</author>
<author>Y Zhou</author>
</authors>
<title>Enhancing supervised learning with unlabeled data.</title>
<date>2000</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="24752" citStr="Goldman and Zhou (2000)" startWordPosition="3891" endWordPosition="3894">ly relies on two views of the data — redundant feature sets that allow different algorithms to label examples for each other, yielding “probably approximately correct” (PAC)-style guarantees under certain (strong) assumptions. In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expe</context>
</contexts>
<marker>Goldman, Zhou, 2000</marker>
<rawString>S. Goldman and Y. Zhou. 2000. Enhancing supervised learning with unlabeled data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="26819" citStr="Headden et al., 2009" startWordPosition="4210" endWordPosition="4213">antage point to improve their perception of difficult training terrains. 9 Conclusions and Future Work Lateen strategies can improve performance and efficiency for dependency grammar induction with the DMV. Early-stopping lateen EM is 30% faster than standard training, without affecting accuracy — it reduces guesswork in terminating EM. At the other extreme, simple lateen EM is slower, but significantly improves accuracy — by 5.5%, on average — for hard EM, escaping some of its local optima. It would be interesting to apply lateen algorithms to advanced parsing models (Blunsom and Cohn, 2010; Headden et al., 2009, inter alia) and learning algorithms (Gillenwater et al., 2010; Cohen and Smith, 2009, inter alia). Future work could explore other NLP tasks — such as clustering, sequence labeling, segmentation and alignment — that often employ EM. Our meta-heuristics are multi-faceted, featuring aspects of iterated local search, deterministic annealing, cross-validation, contrastive estimation and co-training. They may be generally useful in machine learning and non-convex optimization. Appendix A. Experimental Design Statistical techniques are vital to many aspects of computational linguistics (Johnson, 2</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Holm</author>
</authors>
<title>A simple sequentially rejective multiple test procedure.</title>
<date>1979</date>
<journal>Scandinavian Journal of Statistics,</journal>
<volume>6</volume>
<contexts>
<context position="33416" citStr="Holm, 1979" startWordPosition="5317" endWordPosition="5318">a train/test split; default (all zeros) is English ’07. Due to space limitations, we exclude langyear predictors from Table 4. Further, we do not explore (even two-way) interactions between predictors.15 15This approach may miss some interesting facts, e.g., that the adhoc initializer is exceptionally good for English, with soft 9.3 Statistical Significance Our statistical analyses relied on the R package (R Development Core Team, 2011), which does not, by default, adjust statistical significance (p-values) for multiple hypotheses testing.16 We corrected this using the Holm-Bonferroni method (Holm, 1979), which is uniformly more powerful than the older (Dunn-)Bonferroni procedure; since we tested many fewer hypotheses (44 + 42 — one per intercept/coefficient ˆβ) than settings combinations, its adjustments to the p-values are small (see Table 4).17 EM. Instead it yields coarse summaries of regularities supported by overwhelming evidence across data and training regimes. 16Since we would expect p% of randomly chosen hypotheses to appear significant at the p% level simply by chance, we must take precautions against these and other “data-snooping” biases. 17We adjusted the p-values for all 86 hyp</context>
</contexts>
<marker>Holm, 1979</marker>
<rawString>S. Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P A Ioannidis</author>
</authors>
<title>Why most published research findings are false.</title>
<date>2005</date>
<journal>PLoS Medicine,</journal>
<volume>2</volume>
<contexts>
<context position="11031" citStr="Ioannidis, 2005" startWordPosition="1676" endWordPosition="1677">ps, hard EM took another 14, and the new model again improved, by 1.3%, from 54.3 to 55.6% (see Table 1); the corresponding drop in (lexicalized) cross-entropy was from 6.10 to 6.09 bpt. This last model is competitive with the state-ofthe-art; moreover, gains from single applications of simple lateen alternations (2.4 and 1.3%) are on par with the increase due to lexicalization alone (1.5%). 4 Methodology for Study #2 Study #1 suggests that lateen EM can improve grammar induction in English. To establish statistical significance, however, it is important to test a hypothesis in many settings (Ioannidis, 2005). We therefore use a factorial experimental design and regression analyses with a variety of lateen strategies. Two regressions — one predicting accuracy, the other, the number of iterations — capture the effects that lateen algorithms have on performance and efficiency, relative to standard EM training. We controlled for important dimensions of variation, such as the underlying language: to make sure that our results are not English-specific, we induced grammars in 19 languages. We also explored the impact from the quality of an initial model (using both uniform and ad hoc initializers), the </context>
</contexts>
<marker>Ioannidis, 2005</marker>
<rawString>J. P. A. Ioannidis. 2005. Why most published research findings are false. PLoS Medicine, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>How the statistical revolution changes (computational) linguistics.</title>
<date>2009</date>
<booktitle>In EACL: Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?</booktitle>
<contexts>
<context position="27422" citStr="Johnson, 2009" startWordPosition="4296" endWordPosition="4297"> al., 2009, inter alia) and learning algorithms (Gillenwater et al., 2010; Cohen and Smith, 2009, inter alia). Future work could explore other NLP tasks — such as clustering, sequence labeling, segmentation and alignment — that often employ EM. Our meta-heuristics are multi-faceted, featuring aspects of iterated local search, deterministic annealing, cross-validation, contrastive estimation and co-training. They may be generally useful in machine learning and non-convex optimization. Appendix A. Experimental Design Statistical techniques are vital to many aspects of computational linguistics (Johnson, 2009; Charniak, 1997; Abney, 1996, inter alia). We used factorial designs,14 which are standard throughout the natural and social sciences, to assist with experimental design and statistical analyses. Combined with ordinary regressions, these methods provide succinct and interpretable summaries that explain which settings meaningfully contribute to changes in dependent variables, such as running time and accuracy. 14We used full factorial designs for clarity of exposition. But many fewer experiments would suffice, especially in regression models without interaction terms: for the more efficient fr</context>
</contexts>
<marker>Johnson, 2009</marker>
<rawString>M. Johnson. 2009. How the statistical revolution changes (computational) linguistics. In EACL: Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kearns</author>
<author>Y Mansour</author>
<author>A Y Ng</author>
</authors>
<title>An information-theoretic analysis of hard and soft assignment methods for clustering.</title>
<date>1997</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="3507" citStr="Kearns et al., 1997" startWordPosition="514" endWordPosition="517">ng course. For example, since multiple equi-plausible objectives are usually available, a learner could cycle through them, optimizing alternatives when the primary objective function gets stuck; or, instead of trying to escape, it could aim to avoid local optima in the first place, by halting search early if an improvement to one objective would come at the expense of harming another. We test these general ideas by focusing on nonconvex likelihood optimization using EM. This setting is standard and has natural and well-understood objectives: the classic, “soft” EM; and Viterbi, or “hard” EM (Kearns et al., 1997). The name “lateen” comes from the sea — triangular lateen sails can take wind on either side, enabling sailing vessels to tack (see Figure 1). As a captain can’t count on favorable winds, so an unsupervised learner can’t rely on co-operative gradients: soft EM maximizes 1269 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1269–1280, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Figure 1: A triangular sail atop a traditional Arab sailing vessel, the dhow (right). Older square sails permitted sailing onl</context>
</contexts>
<marker>Kearns, Mansour, Ng, 1997</marker>
<rawString>M. Kearns, Y. Mansour, and A. Y. Ng. 1997. An information-theoretic analysis of hard and soft assignment methods for clustering. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7941" citStr="Klein and Manning, 2004" startWordPosition="1186" endWordPosition="1189">oon as they hurt the secondary objective and stops secondary optimizers once they harm the primary objective. This algorithm terminates when it fails to sufficiently improve the primary objective across a full alternation. 2.5 Algorithm #5: Partly-Switching Lateen EM Same as algorithm #4, but again iterating primary objectives to convergence, as in algorithm #1; secondary optimizers still continue to terminate early. 3 The Task and Study #1 We chose to test the impact of these five lateen algorithms on unsupervised dependency parsing — a task in which EM plays an important role (Paskin, 2001; Klein and Manning, 2004; Gillenwater et al., 2010, inter alia). This entailed two sets of experiments: In study #1, we tested whether single alternations of simple lateen EM (as defined in §2.1, 1270 Table 1: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences) for recent state-of-the-art systems and our two experiments (one unlexicalized and one lexicalized) with a single alternation of lateen EM. Algorithm #1) improve our recent publicly-available system for English dependency grammar induction. In study #2, we introduced a more sophisticated methodology that uses factorial designs and regress</context>
<context position="10229" citStr="Klein and Manning, 2004" startWordPosition="1540" endWordPosition="1543">st experiment showed that lateen EM holds promise for simple models. Next, we tested it in a more realistic setting, by re-estimating lexicalized models,5 starting from the unlexicalized model’s 3http://nlp.stanford.edu/pubs/ markup-data.tar.bz2:dp.model.dmv 4It is standard practice to convert gold labeled constituents from Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) into unlabeled reference dependency parses using deterministic “head-percolation” rules (Collins, 1999); sentence root symbols (but not punctuation) arcs count towards accuracies (Paskin, 2001; Klein and Manning, 2004). 5We used Headden et al.’s (2009) method (also the approach parses; this took 24 steps with hard EM. We then applied another single lateen alternation: This time, soft EM ran for 37 steps, hard EM took another 14, and the new model again improved, by 1.3%, from 54.3 to 55.6% (see Table 1); the corresponding drop in (lexicalized) cross-entropy was from 6.10 to 6.09 bpt. This last model is competitive with the state-ofthe-art; moreover, gains from single applications of simple lateen alternations (2.4 and 1.3%) are on par with the increase due to lexicalization alone (1.5%). 4 Methodology for S</context>
<context position="12099" citStr="Klein and Manning, 2004" startWordPosition="1849" endWordPosition="1852">ic, we induced grammars in 19 languages. We also explored the impact from the quality of an initial model (using both uniform and ad hoc initializers), the choice of a primary objective (i.e., soft or hard EM), and the quantity and complexity of training data (shorter versus both short and long sentences). Appendix A gives the full details. 4.1 Data Sets We use all 23 train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007),6 which cover 19 different languages.7 We splice out all punctuation labeled in the data, as is standard practice (Paskin, 2001; Klein and Manning, 2004), introducing new arcs from grandmothers to grand-daughters where necessary, both in train- and test-sets. Evaluation is always against the taken by the two stronger state-of-the-art systems): for words seen at least 100 times in the training corpus, gold part-ofspeech tags are augmented with lexical items. 6These disjoint splits require smoothing; in the WSJ setting, training and test sets overlapped (Klein and Manning, 2004). 7We down-weigh languages appearing in both years — Arabic, Chinese, Czech and Turkish — by 50% in all our analyses. System DDA (%) (Blunsom and Cohn, 2010) (Gillenwater</context>
<context position="14341" citStr="Klein and Manning, 2004" startWordPosition="2224" endWordPosition="2227">ence We always halt an optimizer once a change in its objective’s consecutive cross-entropy values falls below 2−20 bpt (at which point we consider it “stuck”). Soft EM Hard EM Aa Ai Aa Ai -2.7 x0.2 -2.0 x0.3 +0.6 x0.7 +0.6 x1.2 0.0 x2.0 +0.8 x3.7 0.0 x1.3 +5.5 x6.5 -0.0 x1.3 +1.5 x3.6 0.0 x0.7 -0.1 x0.7 0.0 x0.8 +3.0 x2.1 0.0 x1.2 +2.9 x3.8 Model Baselines B3 B2 B1 Algorithms A1 A2 A3 A4 A5 6 Results 4.5 Scoring Function We report directed accuracies — fractions of correctly guessed (unlabeled) dependency arcs, including arcs from sentence root symbols, as is standard practice (Paskin, 2001; Klein and Manning, 2004). Punctuation does not affect scoring, as it had been removed from all parse trees in our data (see §4.1). 5 Experiments We now summarize our baseline models and briefly review the proposed lateen algorithms. For details of the default systems (standard soft and hard EM), all control variables and both regressions (against final accuracies and iteration counts) see Appendix A. 5.1 Baseline Models We tested a total of six baseline models, experimenting with two types of alternatives: (i) strategies that perturb stuck models directly, by smoothing, ignoring secondary objectives; and (ii) shallow</context>
<context position="28787" citStr="Klein and Manning, 2004" startWordPosition="4496" endWordPosition="4499">Variables We constructed two regressions, for two types of dependent variables: to summarize performance, we predict accuracies; and to summarize efficiency, we predict (logarithms of) iterations before termination. In the performance regression, we used four different scores for the dependent variable. These include both directed accuracies and undirected accuracies, each computed in two ways: (i) using a best parse tree; and (ii) using all parse trees. These four types of scores provide different kinds of information. Undirected scores ignore polarity of parentchild relations (Paskin, 2001; Klein and Manning, 2004; Schwartz et al., 2011), partially correcting for some effects of alternate analyses (e.g., systematic choices between modals and main verbs for heads of sentences, determiners for noun phrases, etc.). And integrated scoring, using the inside-outside algorithm (Baker, 1979) to compute expected accuracy across all — not just best — parse trees, has the advantage of incorporating probabilities assigned to individual arcs: This metric is more sensitive to the margins that separate best from next-best parse trees, and is not affected by tie-breaking. We tag scores using two binary predictors in a</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
</authors>
<title>The Unsupervised Learning of Natural Language Structure.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="22500" citStr="Klein, 2005" startWordPosition="3546" endWordPosition="3547">ed (constituent) grammar induction. (For this reason, we did not experiment with DA, despite its strong similarities to lateen EM.) Smith and Eisner (2004) used a “temperature” 0 to anneal a flat uniform distribution (0 = 0) into soft EM’s non-convex objective (0 = 1). In their framework, hard EM corresponds to 0 −→ oo, so the algorithms differ only in their 0-schedule: DA’s is continuous, from 0 to 1; lateen EM’s is a discrete alternation, of 1 and +oo.10 8.2 Terminating Early, Before Convergence EM is rarely run to (even numerical) convergence. Fixing a modest number of iterations a priori (Klein, 2005, §5.3.4), running until successive likelihood ratios become small (Spitkovsky et al., 2009, §4.1) or using a combination of the two (Ravi and Knight, 2009, §4, Footnote 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, i</context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>D. Klein. 2005. The Unsupervised Learning of Natural Language Structure. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Larson</author>
</authors>
<title>The shrinkage of the coefficient of multiple correlation.</title>
<date>1931</date>
<journal>Journal of Educational Psychology,</journal>
<volume>22</volume>
<contexts>
<context position="23127" citStr="Larson, 1931" startWordPosition="3642" endWordPosition="3644">ning until successive likelihood ratios become small (Spitkovsky et al., 2009, §4.1) or using a combination of the two (Ravi and Knight, 2009, §4, Footnote 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Training with Multiple Views Lateen strategies may seem conceptually related to co-training (Blum and Mitchell, 1</context>
</contexts>
<marker>Larson, 1931</marker>
<rawString>S. C. Larson. 1931. The shrinkage of the coefficient of multiple correlation. Journal of Educational Psychology, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Analyzing the errors of unsupervised learning.</title>
<date>2008</date>
<booktitle>In HLT-ACL.</booktitle>
<contexts>
<context position="2803" citStr="Liang and Klein, 2008" startWordPosition="401" endWordPosition="404">al optima at once. After a modest number of such iterations, however, EM lands close to an attractor. Next, its convergence rate necessarily suffers: Disproportionately many (and ever-smaller) steps are needed to finally approach this fixed point, which is almost invariably a local optimum. Deciding when to terminate EM often involves guesswork; and finding ways out of local optima requires trial and error. We propose several strategies that address both limitations. Unsupervised objectives are, at best, loosely correlated with extrinsic performance (Pereira and Schabes, 1992; Merialdo, 1994; Liang and Klein, 2008, inter alia). This fact justifies (occasionally) deviating from a prescribed training course. For example, since multiple equi-plausible objectives are usually available, a learner could cycle through them, optimizing alternatives when the primary objective function gets stuck; or, instead of trying to escape, it could aim to avoid local optima in the first place, by halting search early if an improvement to one objective would come at the expense of harming another. We test these general ideas by focusing on nonconvex likelihood optimization using EM. This setting is standard and has natural</context>
</contexts>
<marker>Liang, Klein, 2008</marker>
<rawString>P. Liang and D. Klein. 2008. Analyzing the errors of unsupervised learning. In HLT-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming. Series B,</booktitle>
<pages>45</pages>
<contexts>
<context position="16696" citStr="Liu and Nocedal, 1989" startWordPosition="2602" endWordPosition="2605"> for all estimates and associated p-values; above, Table 2 shows a preview of the full results). 6.1 A1{h,s} — Simple Lateen EM A1h runs 6.5x slower, but scores 5.5% higher, on average, compared to standard Viterbi training; A13 is only 30% slower than standard soft EM, but does not impact its accuracy at all, on average. Figure 2 depicts a sample training run: Italian ’07 with A1h. Viterbi EM converges after 47 iterations, 9It approximates a mixture (the average of soft and hard objectives) — a natural comparison, computable via gradients and standard optimization algorithms, such as L-BFGS (Liu and Nocedal, 1989). We did not explore exact interpolations, however, because replacing EM is itself a significant confounder, even with unchanged objectives (Berg-Kirkpatrick et al., 2010). 1272 iteration 50 100 150 200 250 300 Figure 2: Cross-entropies for Italian ’07, initialized uniformly and trained on sentences up to length 45. The two curves are primary and secondary objectives (soft EM’s lies below, as sentence yields are at least as likely as parse trees): shaded regions indicate iterations of hard EM (primary); and annotated values are measurements upon each optimizer’s convergence (soft EM’s are pare</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming. Series B, 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T Lowerre</author>
</authors>
<title>The HARPY Speech Recognition System.</title>
<date>1976</date>
<tech>Ph.D. thesis, CMU.</tech>
<contexts>
<context position="23283" citStr="Lowerre, 1976" startWordPosition="3673" endWordPosition="3674"> 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Training with Multiple Views Lateen strategies may seem conceptually related to co-training (Blum and Mitchell, 1998). However, bootstrapping methods generally begin with some labeled data and gradually label the rest (discriminatively) as they grow more confident, but</context>
</contexts>
<marker>Lowerre, 1976</marker>
<rawString>B. T. Lowerre. 1976. The HARPY Speech Recognition System. Ph.D. thesis, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L B Lucy</author>
</authors>
<title>An iterative technique for the rectification of observed distributions.</title>
<date>1974</date>
<journal>The Astronomical Journal,</journal>
<volume>79</volume>
<contexts>
<context position="23019" citStr="Lucy, 1974" startWordPosition="3626" endWordPosition="3627">to (even numerical) convergence. Fixing a modest number of iterations a priori (Klein, 2005, §5.3.4), running until successive likelihood ratios become small (Spitkovsky et al., 2009, §4.1) or using a combination of the two (Ravi and Knight, 2009, §4, Footnote 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Train</context>
</contexts>
<marker>Lucy, 1974</marker>
<rawString>L. B. Lucy. 1974. An iterative technique for the rectification of observed distributions. The Astronomical Journal, 79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="10010" citStr="Marcus et al., 1993" startWordPosition="1513" endWordPosition="1516"> 23 iterations). The result was a decrease in hard EM’s cross-entropy, from 3.69 to 3.59 bits per token (bpt), accompanied by a 2.4% jump in accuracy, from 50.4 to 52.8%, on Section 23 of WSJ (see Table 1).4 Our first experiment showed that lateen EM holds promise for simple models. Next, we tested it in a more realistic setting, by re-estimating lexicalized models,5 starting from the unlexicalized model’s 3http://nlp.stanford.edu/pubs/ markup-data.tar.bz2:dp.model.dmv 4It is standard practice to convert gold labeled constituents from Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) into unlabeled reference dependency parses using deterministic “head-percolation” rules (Collins, 1999); sentence root symbols (but not punctuation) arcs count towards accuracies (Paskin, 2001; Klein and Manning, 2004). 5We used Headden et al.’s (2009) method (also the approach parses; this took 24 steps with hard EM. We then applied another single lateen alternation: This time, soft EM ran for 37 steps, hard EM took another 14, and the new model again improved, by 1.3%, from 54.3 to 55.6% (see Table 1); the corresponding drop in (lexicalized) cross-entropy was from 6.10 to 6.09 bpt. This las</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X-L Meng</author>
</authors>
<title>EM and MCMC: Workhorses for scientific computing (thirty years of EM and much more).</title>
<date>2007</date>
<journal>Statistica Sinica,</journal>
<volume>17</volume>
<contexts>
<context position="2063" citStr="Meng, 2007" startWordPosition="287" endWordPosition="288">ion (EM) algorithms (Dempster et al., 1977) play important roles in learning latent linguistic structure. Unsupervised techniques from this family excel at core natural language processing (NLP) tasks, including segmentation, alignment, tagging and parsing. Typical implementations specify a probabilistic framework, pick an initial model instance, and iteratively improve parameters using EM. A key guarantee is that subsequent model instances are no worse than the previous, according to training data likelihood in the given framework. Another attractive feature that helped make EM instrumental (Meng, 2007) is its initial efficiency: Training tends to begin with large steps in a parameter space, sometimes bypassing many local optima at once. After a modest number of such iterations, however, EM lands close to an attractor. Next, its convergence rate necessarily suffers: Disproportionately many (and ever-smaller) steps are needed to finally approach this fixed point, which is almost invariably a local optimum. Deciding when to terminate EM often involves guesswork; and finding ways out of local optima requires trial and error. We propose several strategies that address both limitations. Unsupervi</context>
</contexts>
<marker>Meng, 2007</marker>
<rawString>X.-L. Meng. 2007. EM and MCMC: Workhorses for scientific computing (thirty years of EM and much more). Statistica Sinica, 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<contexts>
<context position="2780" citStr="Merialdo, 1994" startWordPosition="399" endWordPosition="400">passing many local optima at once. After a modest number of such iterations, however, EM lands close to an attractor. Next, its convergence rate necessarily suffers: Disproportionately many (and ever-smaller) steps are needed to finally approach this fixed point, which is almost invariably a local optimum. Deciding when to terminate EM often involves guesswork; and finding ways out of local optima requires trial and error. We propose several strategies that address both limitations. Unsupervised objectives are, at best, loosely correlated with extrinsic performance (Pereira and Schabes, 1992; Merialdo, 1994; Liang and Klein, 2008, inter alia). This fact justifies (occasionally) deviating from a prescribed training course. For example, since multiple equi-plausible objectives are usually available, a learner could cycle through them, optimizing alternatives when the primary objective function gets stuck; or, instead of trying to escape, it could aim to avoid local optima in the first place, by halting search early if an improvement to one objective would come at the expense of harming another. We test these general ideas by focusing on nonconvex likelihood optimization using EM. This setting is s</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Montgomery</author>
</authors>
<title>Design and Analysis ofExperiments.</title>
<date>2005</date>
<publisher>John Wiley &amp; Sons,</publisher>
<note>6th edition.</note>
<contexts>
<context position="28138" citStr="Montgomery (2005" startWordPosition="4399" endWordPosition="4400">ut the natural and social sciences, to assist with experimental design and statistical analyses. Combined with ordinary regressions, these methods provide succinct and interpretable summaries that explain which settings meaningfully contribute to changes in dependent variables, such as running time and accuracy. 14We used full factorial designs for clarity of exposition. But many fewer experiments would suffice, especially in regression models without interaction terms: for the more efficient fractional factorial designs, as well as for randomized block designs and full factorial designs, see Montgomery (2005, Ch. 4–9). 9.1 Dependent Variables We constructed two regressions, for two types of dependent variables: to summarize performance, we predict accuracies; and to summarize efficiency, we predict (logarithms of) iterations before termination. In the performance regression, we used four different scores for the dependent variable. These include both directed accuracies and undirected accuracies, each computed in two ways: (i) using a best parse tree; and (ii) using all parse trees. These four types of scores provide different kinds of information. Undirected scores ignore polarity of parentchild</context>
</contexts>
<marker>Montgomery, 2005</marker>
<rawString>D. C. Montgomery. 2005. Design and Analysis ofExperiments. John Wiley &amp; Sons, 6th edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mylonakis</author>
<author>K Sima’an</author>
</authors>
<title>Learning probabilistic synchronous CFGs for phrase-based translation. In CoNLL.</title>
<date>2010</date>
<marker>Mylonakis, Sima’an, 2010</marker>
<rawString>M. Mylonakis and K. Sima’an. 2010. Learning probabilistic synchronous CFGs for phrase-based translation. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
<author>G E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1999</date>
<booktitle>Learning in Graphical Models.</booktitle>
<editor>In M. I. Jordan, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="21042" citStr="Neal and Hinton, 1999" startWordPosition="3307" endWordPosition="3310"> use any lateen strategy to improve either efficiency or performance, or to strike a balance. 8 Related Work 8.1 Avoiding and/or Escaping Local Attractors Simple lateen EM is similar to Dhillon et al.’s (2002) refinement algorithm for text clustering with spherical k-means. Their “ping-pong” strategy alternates batch and incremental EM, exploits the strong points of each, and improves a shared objective at every cross-entropies (in bits per token) 3.39 3.26 bpt 4.5 4.0 3.5 3.0 (3.19) 3.23 3.33 3.29 3.29 (3.42) (3.39) (3.39) 3.21 3.22 (3.18) (3.18) 1273 step. Unlike generalized (GEM) variants (Neal and Hinton, 1999), lateen EM uses multiple objectives: it sacrifices the primary in the short run, to escape local optima; in the long run, it also does no harm, by construction (as it returns the best model seen). Of the meta-heuristics that use more than a standard, scalar objective, deterministic annealing (DA) (Rose, 1998) is closest to lateen EM. DA perturbs objective functions, instead of manipulating solutions directly. As other continuation methods (Allgower and Georg, 1990), it optimizes an easy (e.g., convex) function first, then “rides” that optimum by gradually morphing functions towards the diffic</context>
</contexts>
<marker>Neal, Hinton, 1999</marker>
<rawString>R. M. Neal and G. E. Hinton. 1999. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="24774" citStr="Ng and Cardie (2003)" startWordPosition="3895" endWordPosition="3898"> the data — redundant feature sets that allow different algorithms to label examples for each other, yielding “probably approximately correct” (PAC)-style guarantees under certain (strong) assumptions. In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence </context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>V. Ng and C. Cardie. 2003. Weakly supervised natural language learning without redundant views. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>R Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="25691" citStr="Nigam and Ghani, 2000" startWordPosition="4033" endWordPosition="4036">onstituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13Some authors (Nigam and Ghani, 2000; Ng and Cardie, 2003; Smith and Eisner, 2005a, §5.2, 7; §2; §6) draw a hard line between bootstrapping algorithms, such as self- and co-training, and probabilistic modeling using EM; others (Dasgupta et al., 2001; Chang et al., 2007, §1; §5) tend to lump them together. 1274 2001) and parser reranking (Crim, 2002). Fundamentally, co-training exploits redundancies in unlabeled data and/or learning algorithms. Lateen strategies also exploit redundancies: in noisy objectives. Both approaches use a second vantage point to improve their perception of difficult training terrains. 9 Conclusions and F</context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>K. Nigam and R. Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Paskin</author>
</authors>
<title>Grammatical bigrams. In</title>
<date>2001</date>
<booktitle>In ACL. R Development Core Team,</booktitle>
<contexts>
<context position="7916" citStr="Paskin, 2001" startWordPosition="1184" endWordPosition="1185">ptimizers as soon as they hurt the secondary objective and stops secondary optimizers once they harm the primary objective. This algorithm terminates when it fails to sufficiently improve the primary objective across a full alternation. 2.5 Algorithm #5: Partly-Switching Lateen EM Same as algorithm #4, but again iterating primary objectives to convergence, as in algorithm #1; secondary optimizers still continue to terminate early. 3 The Task and Study #1 We chose to test the impact of these five lateen algorithms on unsupervised dependency parsing — a task in which EM plays an important role (Paskin, 2001; Klein and Manning, 2004; Gillenwater et al., 2010, inter alia). This entailed two sets of experiments: In study #1, we tested whether single alternations of simple lateen EM (as defined in §2.1, 1270 Table 1: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences) for recent state-of-the-art systems and our two experiments (one unlexicalized and one lexicalized) with a single alternation of lateen EM. Algorithm #1) improve our recent publicly-available system for English dependency grammar induction. In study #2, we introduced a more sophisticated methodology that uses fact</context>
<context position="10203" citStr="Paskin, 2001" startWordPosition="1538" endWordPosition="1539">e 1).4 Our first experiment showed that lateen EM holds promise for simple models. Next, we tested it in a more realistic setting, by re-estimating lexicalized models,5 starting from the unlexicalized model’s 3http://nlp.stanford.edu/pubs/ markup-data.tar.bz2:dp.model.dmv 4It is standard practice to convert gold labeled constituents from Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) into unlabeled reference dependency parses using deterministic “head-percolation” rules (Collins, 1999); sentence root symbols (but not punctuation) arcs count towards accuracies (Paskin, 2001; Klein and Manning, 2004). 5We used Headden et al.’s (2009) method (also the approach parses; this took 24 steps with hard EM. We then applied another single lateen alternation: This time, soft EM ran for 37 steps, hard EM took another 14, and the new model again improved, by 1.3%, from 54.3 to 55.6% (see Table 1); the corresponding drop in (lexicalized) cross-entropy was from 6.10 to 6.09 bpt. This last model is competitive with the state-ofthe-art; moreover, gains from single applications of simple lateen alternations (2.4 and 1.3%) are on par with the increase due to lexicalization alone (</context>
<context position="12073" citStr="Paskin, 2001" startWordPosition="1847" endWordPosition="1848">English-specific, we induced grammars in 19 languages. We also explored the impact from the quality of an initial model (using both uniform and ad hoc initializers), the choice of a primary objective (i.e., soft or hard EM), and the quantity and complexity of training data (shorter versus both short and long sentences). Appendix A gives the full details. 4.1 Data Sets We use all 23 train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007),6 which cover 19 different languages.7 We splice out all punctuation labeled in the data, as is standard practice (Paskin, 2001; Klein and Manning, 2004), introducing new arcs from grandmothers to grand-daughters where necessary, both in train- and test-sets. Evaluation is always against the taken by the two stronger state-of-the-art systems): for words seen at least 100 times in the training corpus, gold part-ofspeech tags are augmented with lexical items. 6These disjoint splits require smoothing; in the WSJ setting, training and test sets overlapped (Klein and Manning, 2004). 7We down-weigh languages appearing in both years — Arabic, Chinese, Czech and Turkish — by 50% in all our analyses. System DDA (%) (Blunsom an</context>
<context position="14315" citStr="Paskin, 2001" startWordPosition="2222" endWordPosition="2223">andard Convergence We always halt an optimizer once a change in its objective’s consecutive cross-entropy values falls below 2−20 bpt (at which point we consider it “stuck”). Soft EM Hard EM Aa Ai Aa Ai -2.7 x0.2 -2.0 x0.3 +0.6 x0.7 +0.6 x1.2 0.0 x2.0 +0.8 x3.7 0.0 x1.3 +5.5 x6.5 -0.0 x1.3 +1.5 x3.6 0.0 x0.7 -0.1 x0.7 0.0 x0.8 +3.0 x2.1 0.0 x1.2 +2.9 x3.8 Model Baselines B3 B2 B1 Algorithms A1 A2 A3 A4 A5 6 Results 4.5 Scoring Function We report directed accuracies — fractions of correctly guessed (unlabeled) dependency arcs, including arcs from sentence root symbols, as is standard practice (Paskin, 2001; Klein and Manning, 2004). Punctuation does not affect scoring, as it had been removed from all parse trees in our data (see §4.1). 5 Experiments We now summarize our baseline models and briefly review the proposed lateen algorithms. For details of the default systems (standard soft and hard EM), all control variables and both regressions (against final accuracies and iteration counts) see Appendix A. 5.1 Baseline Models We tested a total of six baseline models, experimenting with two types of alternatives: (i) strategies that perturb stuck models directly, by smoothing, ignoring secondary ob</context>
<context position="28762" citStr="Paskin, 2001" startWordPosition="4494" endWordPosition="4495">9.1 Dependent Variables We constructed two regressions, for two types of dependent variables: to summarize performance, we predict accuracies; and to summarize efficiency, we predict (logarithms of) iterations before termination. In the performance regression, we used four different scores for the dependent variable. These include both directed accuracies and undirected accuracies, each computed in two ways: (i) using a best parse tree; and (ii) using all parse trees. These four types of scores provide different kinds of information. Undirected scores ignore polarity of parentchild relations (Paskin, 2001; Klein and Manning, 2004; Schwartz et al., 2011), partially correcting for some effects of alternate analyses (e.g., systematic choices between modals and main verbs for heads of sentences, determiners for noun phrases, etc.). And integrated scoring, using the inside-outside algorithm (Baker, 1979) to compute expected accuracy across all — not just best — parse trees, has the advantage of incorporating probabilities assigned to individual arcs: This metric is more sensitive to the margins that separate best from next-best parse trees, and is not affected by tie-breaking. We tag scores using t</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>M. A. Paskin. 2001. Grammatical bigrams. In NIPS. F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In ACL. R Development Core Team, 2011. R: A Language and Environmentfor Statistical Computing. R Foundation for Statistical Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>K Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="22655" citStr="Ravi and Knight, 2009" startWordPosition="3569" endWordPosition="3572">ner (2004) used a “temperature” 0 to anneal a flat uniform distribution (0 = 0) into soft EM’s non-convex objective (0 = 1). In their framework, hard EM corresponds to 0 −→ oo, so the algorithms differ only in their 0-schedule: DA’s is continuous, from 0 to 1; lateen EM’s is a discrete alternation, of 1 and +oo.10 8.2 Terminating Early, Before Convergence EM is rarely run to (even numerical) convergence. Fixing a modest number of iterations a priori (Klein, 2005, §5.3.4), running until successive likelihood ratios become small (Spitkovsky et al., 2009, §4.1) or using a combination of the two (Ravi and Knight, 2009, §4, Footnote 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>S. Ravi and K. Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rose</author>
</authors>
<title>Deterministic annealing for clustering, compression, classification, regression and related optmization problems.</title>
<date>1998</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>86</pages>
<contexts>
<context position="21353" citStr="Rose, 1998" startWordPosition="3361" endWordPosition="3362"> incremental EM, exploits the strong points of each, and improves a shared objective at every cross-entropies (in bits per token) 3.39 3.26 bpt 4.5 4.0 3.5 3.0 (3.19) 3.23 3.33 3.29 3.29 (3.42) (3.39) (3.39) 3.21 3.22 (3.18) (3.18) 1273 step. Unlike generalized (GEM) variants (Neal and Hinton, 1999), lateen EM uses multiple objectives: it sacrifices the primary in the short run, to escape local optima; in the long run, it also does no harm, by construction (as it returns the best model seen). Of the meta-heuristics that use more than a standard, scalar objective, deterministic annealing (DA) (Rose, 1998) is closest to lateen EM. DA perturbs objective functions, instead of manipulating solutions directly. As other continuation methods (Allgower and Georg, 1990), it optimizes an easy (e.g., convex) function first, then “rides” that optimum by gradually morphing functions towards the difficult objective; each step reoptimizes from the previous approximate solution. Smith and Eisner (2004) employed DA to improve part-of-speech disambiguation, but found that objectives had to be further “skewed,” using domain knowledge, before it helped (constituent) grammar induction. (For this reason, we did not</context>
</contexts>
<marker>Rose, 1998</marker>
<rawString>K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression and related optmization problems. Proceedings of the IEEE, 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In NAACL.</booktitle>
<marker>Sarkar, 2001</marker>
<rawString>A. Sarkar. 2001. Applying co-training methods to statistical parsing. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
<author>O Abend</author>
<author>R Reichart</author>
<author>A Rappoport</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28811" citStr="Schwartz et al., 2011" startWordPosition="4500" endWordPosition="4503">two regressions, for two types of dependent variables: to summarize performance, we predict accuracies; and to summarize efficiency, we predict (logarithms of) iterations before termination. In the performance regression, we used four different scores for the dependent variable. These include both directed accuracies and undirected accuracies, each computed in two ways: (i) using a best parse tree; and (ii) using all parse trees. These four types of scores provide different kinds of information. Undirected scores ignore polarity of parentchild relations (Paskin, 2001; Klein and Manning, 2004; Schwartz et al., 2011), partially correcting for some effects of alternate analyses (e.g., systematic choices between modals and main verbs for heads of sentences, determiners for noun phrases, etc.). And integrated scoring, using the inside-outside algorithm (Baker, 1979) to compute expected accuracy across all — not just best — parse trees, has the advantage of incorporating probabilities assigned to individual arcs: This metric is more sensitive to the margins that separate best from next-best parse trees, and is not affected by tie-breaking. We tag scores using two binary predictors in a simple (first order, mu</context>
</contexts>
<marker>Schwartz, Abend, Reichart, Rappoport, 2011</marker>
<rawString>R. Schwartz, O. Abend, R. Reichart, and A. Rappoport. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing techniques for unsupervised statistical language learning.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="21742" citStr="Smith and Eisner (2004)" startWordPosition="3415" endWordPosition="3418">run, to escape local optima; in the long run, it also does no harm, by construction (as it returns the best model seen). Of the meta-heuristics that use more than a standard, scalar objective, deterministic annealing (DA) (Rose, 1998) is closest to lateen EM. DA perturbs objective functions, instead of manipulating solutions directly. As other continuation methods (Allgower and Georg, 1990), it optimizes an easy (e.g., convex) function first, then “rides” that optimum by gradually morphing functions towards the difficult objective; each step reoptimizes from the previous approximate solution. Smith and Eisner (2004) employed DA to improve part-of-speech disambiguation, but found that objectives had to be further “skewed,” using domain knowledge, before it helped (constituent) grammar induction. (For this reason, we did not experiment with DA, despite its strong similarities to lateen EM.) Smith and Eisner (2004) used a “temperature” 0 to anneal a flat uniform distribution (0 = 0) into soft EM’s non-convex objective (0 = 1). In their framework, hard EM corresponds to 0 −→ oo, so the algorithms differ only in their 0-schedule: DA’s is continuous, from 0 to 1; lateen EM’s is a discrete alternation, of 1 and</context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>N. A. Smith and J. Eisner. 2004. Annealing techniques for unsupervised statistical language learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="25168" citStr="Smith and Eisner, 2005" startWordPosition="3956" endWordPosition="3960">he primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13Some authors (Nigam and Ghani, 2000; Ng and Cardie, 2003; Smith and Eisner, 2005a, §5.2, 7; §2; §6) draw a hard </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005a. Contrastive estimation: Training log-linear models on unlabeled data. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation. In IJCAI: Grammatical Inference Applications.</title>
<date>2005</date>
<contexts>
<context position="25168" citStr="Smith and Eisner, 2005" startWordPosition="3956" endWordPosition="3960">he primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13Some authors (Nigam and Ghani, 2000; Ng and Cardie, 2003; Smith and Eisner, 2005a, §5.2, 7; §2; §6) draw a hard </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005b. Guiding unsupervised grammar induction using contrastive estimation. In IJCAI: Grammatical Inference Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Søgaard</author>
<author>C Rishøj</author>
</authors>
<title>Semi-supervised dependency parsing using generalized tri-training.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="25067" citStr="Søgaard and Rishøj, 2010" startWordPosition="3941" endWordPosition="3944">gorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data likelihoods (in learning synchronous PCFGs, for phrase-based machine translation), see Mylonakis and Sima’an’s (2010, §3.1) CV-EM algorithm. 13Some author</context>
</contexts>
<marker>Søgaard, Rishøj, 2010</marker>
<rawString>A. Søgaard and C. Rishøj. 2010. Semi-supervised dependency parsing using generalized tri-training. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Baby Steps: How “Less is More” in unsupervised dependency parsing. In NIPS: Grammar Induction, Representation ofLanguage and Language Learning.</title>
<date>2009</date>
<contexts>
<context position="22591" citStr="Spitkovsky et al., 2009" startWordPosition="3557" endWordPosition="3560">h DA, despite its strong similarities to lateen EM.) Smith and Eisner (2004) used a “temperature” 0 to anneal a flat uniform distribution (0 = 0) into soft EM’s non-convex objective (0 = 1). In their framework, hard EM corresponds to 0 −→ oo, so the algorithms differ only in their 0-schedule: DA’s is continuous, from 0 to 1; lateen EM’s is a discrete alternation, of 1 and +oo.10 8.2 Terminating Early, Before Convergence EM is rarely run to (even numerical) convergence. Fixing a modest number of iterations a priori (Klein, 2005, §5.3.4), running until successive likelihood ratios become small (Spitkovsky et al., 2009, §4.1) or using a combination of the two (Ravi and Knight, 2009, §4, Footnote 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2009</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby Steps: How “Less is More” in unsupervised dependency parsing. In NIPS: Grammar Induction, Representation ofLanguage and Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="4768" citStr="Spitkovsky et al., 2010" startWordPosition="714" endWordPosition="717">ateen sail worked like a wing (with high pressure on one side and low pressure on the other), allowing a ship to go almost directly into a headwind. By tacking, in a zig-zag pattern, it became possible to sail in any direction, provided there was some wind at all (left). For centuries seafarers expertly combined both sails to traverse extensive distances, greatly increasing the reach of medieval navigation.1 likelihoods of observed data across assignments to hidden variables, whereas hard EM focuses on most likely completions.2 These objectives are plausible, yet both can be provably “wrong” (Spitkovsky et al., 2010a, §7.3). Thus, it is permissible for lateen EM to maneuver between their gradients, for example by tacking around local attractors, in a zig-zag fashion. 2 The Lateen Family of Algorithms We propose several strategies that use a secondary objective to improve over standard EM training. For hard EM, the secondary objective is that of soft EM; and vice versa if soft EM is the primary algorithm. 2.1 Algorithm #1: Simple Lateen EM Simple lateen EM begins by running standard EM to convergence, using a user-supplied initial model, primary objective and definition of convergence. Next, the algorithm</context>
<context position="8751" citStr="Spitkovsky et al., 2010" startWordPosition="1309" endWordPosition="1312">irected dependency accuracies (DDA) on Section 23 of WSJ (all sentences) for recent state-of-the-art systems and our two experiments (one unlexicalized and one lexicalized) with a single alternation of lateen EM. Algorithm #1) improve our recent publicly-available system for English dependency grammar induction. In study #2, we introduced a more sophisticated methodology that uses factorial designs and regressions to evaluate lateen strategies with unsupervised dependency parsing in many languages, after also controlling for other important sources of variation. For study #1, our base system (Spitkovsky et al., 2010b) is an instance of the popular (unlexicalized) Dependency Model with Valence (Klein and Manning, 2004). This model was trained using hard EM on WSJ45 (WSJ sentences up to length 45) until successive changes in per-token cross-entropy fell below 2−20 bits (Spitkovsky et al., 2010b; 2010a, §4).3 We confirmed that the base model had indeed converged, by running 10 steps of hard EM on WSJ45 and verifying that its objective did not change much. Next, we applied a single alternation of simple lateen EM: first running soft EM (this took 101 steps, using the same termination criterion), followed by </context>
<context position="12738" citStr="Spitkovsky et al., 2010" startWordPosition="1950" endWordPosition="1953">w arcs from grandmothers to grand-daughters where necessary, both in train- and test-sets. Evaluation is always against the taken by the two stronger state-of-the-art systems): for words seen at least 100 times in the training corpus, gold part-ofspeech tags are augmented with lexical items. 6These disjoint splits require smoothing; in the WSJ setting, training and test sets overlapped (Klein and Manning, 2004). 7We down-weigh languages appearing in both years — Arabic, Chinese, Czech and Turkish — by 50% in all our analyses. System DDA (%) (Blunsom and Cohn, 2010) (Gillenwater et al., 2010) (Spitkovsky et al., 2010b) 55.7 53.3 50.4 + soft EM + hard EM lexicalized, using hard EM + soft EM + hard EM 52.8 (+2.4) 54.3 (+1.5) 55.6 (+1.3) 1271 entire resulting test sets (i.e., all sentence lengths).8 4.2 Grammar Models and hard EM.9 Three such baselines begin with hard EM (marked with the subscript h); and three more start with soft EM (marked with the subscript s). In all remaining experiments we model grammars via the original DMV, which ignores punctuation; all models are unlexicalized, with gold part-of-speech tags for word classes (Klein and Manning, 2004). 5.2 Lateen Models Ten models, A{1, 2, 3, 4, 51{</context>
<context position="30447" citStr="Spitkovsky et al., 2010" startWordPosition="4748" endWordPosition="4751">dependent Predictors All of our predictors are binary indicators (a.k.a. “dummy” variables). The undirected and integrated factors only affect the regression for accuracies (see Table 4, left); remaining factors participate also in the running times regression (see Table 4, right). In a default run, all factors are zero, corresponding to the intercept estimated by a regression; other estimates reflect changes in the dependent variable associated with having that factor “on” instead of “off.” • adhoc — This setting controls initialization. By default, we use the uninformed uniform initializer (Spitkovsky et al., 2010a); when it is 1275 Regression for Accuracies Regression for ln(Iterations) Goodness-of-Fit: (R2adj ≈ 76.2%) (R2adj ≈ 82.4%) βˆ adj. p-value βˆ mult. eˆo adj. p-value coeff. coeff. 18.1 &lt; 2.0 × 10 −16 -0.9 ≈ 7.0 × 10 −7 30.9 &lt; 2.0 × 10 −16 5.5 255.8 &lt; 2.0 × 10 −16 1.2 ≈ 3.1 × 10 −13 -0.0 1.0 ≈ 1.0 1.0 ≈ 3.1 × 10 −9 -0.2 0.8 &lt; 2.0 × 10 −16 -2.7 ≈ 6.4 × 10 −7 -1.5 0.2 &lt; 2.0 × 10 −16 -2.0 ≈ 7.8 × 10 −4 -1.2 0.3 &lt; 2.0 × 10 −16 0.6 ≈ 1.0 -0.4 0.7 ≈ 1.4 × 10 −12 0.0 ≈ 1.0 0.7 2.0 &lt; 2.0 × 10 −16 0.0 ≈ 1.0 0.2 1.3 ≈ 4.1 × 10 −4 -0.0 ≈ 1.0 0.2 1.3 ≈ 5.8 × 10 −4 0.0 ≈ 1.0 -0.3 0.7 ≈ 2.6 × 10 −7 0.0 ≈ 1.</context>
<context position="36886" citStr="Spitkovsky et al. (2010" startWordPosition="5944" endWordPosition="5947">ted scoring is slightly (but significantly: p ≈ 7.0 × 10−7) negative, which is reassuring: best parses are scoring higher than the rest and may be standing out by large margins. The adhoc initializer boosts accuracy by 1.2%, overall (also significant: p ≈ 3.1 × 10−13), without a measurable impact on running time (p ≈ 1.0). Training with fewer, shorter sentences, at the sweet spot gradation, adds 1.0% and shaves 20% off the total number of iterations, on average (both estimates are significant). We find the viterbi objective harmful — by 4.0%, on average (p ≈ 5.7 × 10−16) — for the CoNLL sets. Spitkovsky et al. (2010a) reported that it helps on WSJ, at least with long sentences and uniform initializers. Half of our experiments are with shorter sentences, and half use ad hoc initializers (i.e., three quarters of settings are not ideal for Viterbi EM), which may have contributed to this negative result; still, our estimates do confirm that hard EM is significantly (80%, p &lt; 2.0 × 10−16) faster than soft EM. 9.5 More on Viterbi Training The overall negative impact of Viterbi objectives is a cause for concern: On average, A1h’s estimated gain of 5.5% should more than offset the expected 4.0% loss from startin</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010a. Viterbi training improves unsupervised dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>D Jurafsky</author>
<author>H Alshawi</author>
</authors>
<title>Profiting from mark-up: Hyper-text annotations for guided parsing.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4768" citStr="Spitkovsky et al., 2010" startWordPosition="714" endWordPosition="717">ateen sail worked like a wing (with high pressure on one side and low pressure on the other), allowing a ship to go almost directly into a headwind. By tacking, in a zig-zag pattern, it became possible to sail in any direction, provided there was some wind at all (left). For centuries seafarers expertly combined both sails to traverse extensive distances, greatly increasing the reach of medieval navigation.1 likelihoods of observed data across assignments to hidden variables, whereas hard EM focuses on most likely completions.2 These objectives are plausible, yet both can be provably “wrong” (Spitkovsky et al., 2010a, §7.3). Thus, it is permissible for lateen EM to maneuver between their gradients, for example by tacking around local attractors, in a zig-zag fashion. 2 The Lateen Family of Algorithms We propose several strategies that use a secondary objective to improve over standard EM training. For hard EM, the secondary objective is that of soft EM; and vice versa if soft EM is the primary algorithm. 2.1 Algorithm #1: Simple Lateen EM Simple lateen EM begins by running standard EM to convergence, using a user-supplied initial model, primary objective and definition of convergence. Next, the algorithm</context>
<context position="8751" citStr="Spitkovsky et al., 2010" startWordPosition="1309" endWordPosition="1312">irected dependency accuracies (DDA) on Section 23 of WSJ (all sentences) for recent state-of-the-art systems and our two experiments (one unlexicalized and one lexicalized) with a single alternation of lateen EM. Algorithm #1) improve our recent publicly-available system for English dependency grammar induction. In study #2, we introduced a more sophisticated methodology that uses factorial designs and regressions to evaluate lateen strategies with unsupervised dependency parsing in many languages, after also controlling for other important sources of variation. For study #1, our base system (Spitkovsky et al., 2010b) is an instance of the popular (unlexicalized) Dependency Model with Valence (Klein and Manning, 2004). This model was trained using hard EM on WSJ45 (WSJ sentences up to length 45) until successive changes in per-token cross-entropy fell below 2−20 bits (Spitkovsky et al., 2010b; 2010a, §4).3 We confirmed that the base model had indeed converged, by running 10 steps of hard EM on WSJ45 and verifying that its objective did not change much. Next, we applied a single alternation of simple lateen EM: first running soft EM (this took 101 steps, using the same termination criterion), followed by </context>
<context position="12738" citStr="Spitkovsky et al., 2010" startWordPosition="1950" endWordPosition="1953">w arcs from grandmothers to grand-daughters where necessary, both in train- and test-sets. Evaluation is always against the taken by the two stronger state-of-the-art systems): for words seen at least 100 times in the training corpus, gold part-ofspeech tags are augmented with lexical items. 6These disjoint splits require smoothing; in the WSJ setting, training and test sets overlapped (Klein and Manning, 2004). 7We down-weigh languages appearing in both years — Arabic, Chinese, Czech and Turkish — by 50% in all our analyses. System DDA (%) (Blunsom and Cohn, 2010) (Gillenwater et al., 2010) (Spitkovsky et al., 2010b) 55.7 53.3 50.4 + soft EM + hard EM lexicalized, using hard EM + soft EM + hard EM 52.8 (+2.4) 54.3 (+1.5) 55.6 (+1.3) 1271 entire resulting test sets (i.e., all sentence lengths).8 4.2 Grammar Models and hard EM.9 Three such baselines begin with hard EM (marked with the subscript h); and three more start with soft EM (marked with the subscript s). In all remaining experiments we model grammars via the original DMV, which ignores punctuation; all models are unlexicalized, with gold part-of-speech tags for word classes (Klein and Manning, 2004). 5.2 Lateen Models Ten models, A{1, 2, 3, 4, 51{</context>
<context position="30447" citStr="Spitkovsky et al., 2010" startWordPosition="4748" endWordPosition="4751">dependent Predictors All of our predictors are binary indicators (a.k.a. “dummy” variables). The undirected and integrated factors only affect the regression for accuracies (see Table 4, left); remaining factors participate also in the running times regression (see Table 4, right). In a default run, all factors are zero, corresponding to the intercept estimated by a regression; other estimates reflect changes in the dependent variable associated with having that factor “on” instead of “off.” • adhoc — This setting controls initialization. By default, we use the uninformed uniform initializer (Spitkovsky et al., 2010a); when it is 1275 Regression for Accuracies Regression for ln(Iterations) Goodness-of-Fit: (R2adj ≈ 76.2%) (R2adj ≈ 82.4%) βˆ adj. p-value βˆ mult. eˆo adj. p-value coeff. coeff. 18.1 &lt; 2.0 × 10 −16 -0.9 ≈ 7.0 × 10 −7 30.9 &lt; 2.0 × 10 −16 5.5 255.8 &lt; 2.0 × 10 −16 1.2 ≈ 3.1 × 10 −13 -0.0 1.0 ≈ 1.0 1.0 ≈ 3.1 × 10 −9 -0.2 0.8 &lt; 2.0 × 10 −16 -2.7 ≈ 6.4 × 10 −7 -1.5 0.2 &lt; 2.0 × 10 −16 -2.0 ≈ 7.8 × 10 −4 -1.2 0.3 &lt; 2.0 × 10 −16 0.6 ≈ 1.0 -0.4 0.7 ≈ 1.4 × 10 −12 0.0 ≈ 1.0 0.7 2.0 &lt; 2.0 × 10 −16 0.0 ≈ 1.0 0.2 1.3 ≈ 4.1 × 10 −4 -0.0 ≈ 1.0 0.2 1.3 ≈ 5.8 × 10 −4 0.0 ≈ 1.0 -0.3 0.7 ≈ 2.6 × 10 −7 0.0 ≈ 1.</context>
<context position="36886" citStr="Spitkovsky et al. (2010" startWordPosition="5944" endWordPosition="5947">ted scoring is slightly (but significantly: p ≈ 7.0 × 10−7) negative, which is reassuring: best parses are scoring higher than the rest and may be standing out by large margins. The adhoc initializer boosts accuracy by 1.2%, overall (also significant: p ≈ 3.1 × 10−13), without a measurable impact on running time (p ≈ 1.0). Training with fewer, shorter sentences, at the sweet spot gradation, adds 1.0% and shaves 20% off the total number of iterations, on average (both estimates are significant). We find the viterbi objective harmful — by 4.0%, on average (p ≈ 5.7 × 10−16) — for the CoNLL sets. Spitkovsky et al. (2010a) reported that it helps on WSJ, at least with long sentences and uniform initializers. Half of our experiments are with shorter sentences, and half use ad hoc initializers (i.e., three quarters of settings are not ideal for Viterbi EM), which may have contributed to this negative result; still, our estimates do confirm that hard EM is significantly (80%, p &lt; 2.0 × 10−16) faster than soft EM. 9.5 More on Viterbi Training The overall negative impact of Viterbi objectives is a cause for concern: On average, A1h’s estimated gain of 5.5% should more than offset the expected 4.0% loss from startin</context>
</contexts>
<marker>Spitkovsky, Jurafsky, Alshawi, 2010</marker>
<rawString>V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b. Profiting from mark-up: Hyper-text annotations for guided parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
</authors>
<title>Cross-validatory choice and assessment of statistical predictions.</title>
<date>1974</date>
<journal>Journal of the Royal Statistical Society. Series B,</journal>
<volume>36</volume>
<contexts>
<context position="23489" citStr="Stone, 1974" startWordPosition="3704" endWordPosition="3705">nal accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Training with Multiple Views Lateen strategies may seem conceptually related to co-training (Blum and Mitchell, 1998). However, bootstrapping methods generally begin with some labeled data and gradually label the rest (discriminatively) as they grow more confident, but do not optimize an explicit objective function; EM, on the other hand, can be fully unsupervised, relabels all examples on each iteration (generatively), and guarantees not to hurt a well-defined objective</context>
</contexts>
<marker>Stone, 1974</marker>
<rawString>M. Stone. 1974. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal Statistical Society. Series B, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yessenalina</author>
<author>Y Yue</author>
<author>C Cardie</author>
</authors>
<title>Multilevel structured models for document-level sentiment classification.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="23203" citStr="Yessenalina et al., 2010" startWordPosition="3656" endWordPosition="3659">et al., 2009, §4.1) or using a combination of the two (Ravi and Knight, 2009, §4, Footnote 5) is standard practice in NLP. Elworthy’s (1994, §5, Figure 1) analysis of part-ofspeech tagging showed that, in most cases, a small number of iterations is actually preferable to convergence, in terms of final accuracies: “regularization by early termination” had been suggested for image deblurring algorithms in statistical astronomy (Lucy, 1974, §2); and validation against held-out data — a strategy proposed much earlier, in psychology (Larson, 1931), has also been used as a halting criterion in NLP (Yessenalina et al., 2010, §4.2, 5.2). 10One can think of this as a kind of “beam search” (Lowerre, 1976), with soft EM expanding and hard EM pruning a frontier. Early-stopping lateen EM tethers termination to a sign change in the direction of a secondary objective, similarly to (cross-)validation (Stone, 1974; Geisser, 1975; Arlot and Celisse, 2010), but without splitting data — it trains using all examples, at all times.11,12 8.3 Training with Multiple Views Lateen strategies may seem conceptually related to co-training (Blum and Mitchell, 1998). However, bootstrapping methods generally begin with some labeled data </context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multilevel structured models for document-level sentiment classification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhou</author>
<author>S Goldman</author>
</authors>
<title>Democratic co-learning.</title>
<date>2004</date>
<booktitle>In ICTAI.</booktitle>
<contexts>
<context position="24913" citStr="Zhou and Goldman (2004)" startWordPosition="3919" endWordPosition="3923">rrect” (PAC)-style guarantees under certain (strong) assumptions. In contrast, lateen EM uses the same data, features, model and essentially the same algorithms, changing only their objective functions: it makes no assumptions, but guarantees not to harm the primary objective. Some of these distinctions have become blurred with time: Collins and Singer (1999) introduced an objective function (also based on agreement) into co-training; Goldman and Zhou (2000), Ng and Cardie (2003) and Chan et al. (2004) made do without redundant views; Balcan et al. (2004) relaxed other strong assumptions; and Zhou and Goldman (2004) generalized co-training to accommodate three and more algorithms. Several such methods have been applied to dependency parsing (Søgaard and Rishøj, 2010), constituent parsing (Sarkar, 11We see in it a milder contrastive estimation (Smith and Eisner, 2005a; 2005b), agnostic to implicit negative evidence, but caring whence learners push probability mass towards training examples: when most likely parse trees begin to benefit at the expense of their sentence yields (or vice versa), optimizers halt. 12For a recently proposed instance of EM that uses crossvalidation (CV) to optimize smoothed data </context>
</contexts>
<marker>Zhou, Goldman, 2004</marker>
<rawString>Y. Zhou and S. Goldman. 2004. Democratic co-learning. In ICTAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>