<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051435">
<title confidence="0.991983">
Introduction to the CoNLL-2001 Shared Task:
Clause Identification
</title>
<author confidence="0.982786">
Erik F. Tjong Kim Sang
</author>
<affiliation confidence="0.9779715">
CNTS - Language Technology Group
University of Antwerp
</affiliation>
<email confidence="0.991277">
erikt@uia.ua.ac.be
</email>
<author confidence="0.832943">
Herve Dejean
</author>
<affiliation confidence="0.680417">
Seminar fiir Sprachwissenschaft
</affiliation>
<address confidence="0.621104">
Universitat Tubingen
</address>
<email confidence="0.993617">
dejean@sfs.nphil.uni-tuebingen.de
</email>
<sectionHeader confidence="0.97967" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999741666666667">
We describe the CoNLL-2001 shared task: di-
viding text into clauses. We give background
information on the data sets, present a general
overview of the systems that have taken part in
the shared task and briefly discuss their perfor-
mance.
</bodyText>
<sectionHeader confidence="0.995488" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906">
The CoNLL-2001 shared task aims at discov-
ering clause boundaries with machine learning
methods. Why clauses? Clauses are structures
used in applications such as Text-to-Speech con-
version (Ejerhed, 1988), text-alignment (Papa-
georgiou, 1997) and machine translation (Leffa,
1998). Ejerhed (1988) described clauses as a
natural structure above chunks:
It is a hypothesis of the author&apos;s cur-
rent clause-by-clause processing the-
ory, that a unit corresponding to the
basic clause is a stable and easily rec-
ognizable surface unit and that is is
also an important partial result and
building block in the construction od
a richer linguistic representation that
encompasses syntax as well as seman-
tics and discourse structure (Ejerhed,
1988, page 220)
The goal of this shared task is to evaluate
automatic methods, especially machine learn-
ing methods, for finding clause boundaries in
text. We have selected a training and test cor-
pus for performing this evaluation. The task has
been divided in three parts in order to allow ba-
sic machine learning methods to participate in
this task by processing the data in a bottom-up
fashion.
</bodyText>
<sectionHeader confidence="0.73302" genericHeader="method">
2 Task description
</sectionHeader>
<bodyText confidence="0.999510666666667">
Defining clause boundaries is not trivial (Leffa,
1998). In this task, the gold standard clause
segmentation is provided by the Penn Treebank
(Marcus et al., 1993). The guidelines of the
Penn Treebank describe in detail how sentences
are segmented into clauses (Bies et al., 1995).
Here is an example of a sentence and its clauses
obtained from Wall Street Journal section 15 of
the Penn Treebank (Marcus et al., 1993):
</bodyText>
<equation confidence="0.846256333333333">
(S Coach them in
(S—NOM handling complaints)
(SBAR—PRP so that
(S they can resolve problems immediately)
)
)
</equation>
<bodyText confidence="0.999813588235294">
The clauses of this sentence have been enclosed
between brackets. A tag next to the open
bracket denotes the type of the clause.
In the CoNLL-2001 shared task, the goal is to
identify clauses in text. Since clauses can be em-
bedded in each other, this task is considerably
more difficult than last year&apos;s task, recognizing
non-embedded text chunks. For that reason,
we have disregarded type and function informa-
tion of the clauses: every clause has been tagged
with S rather than with an elaborate tag such as
SBAR—PRP. Furthermore, the shared task has
been divided in three parts: identifying clause
starts, recognizing clause ends and finding com-
plete clauses. The results obtained for the first
two parts can be used in the third part of the
task.
</bodyText>
<sectionHeader confidence="0.828553" genericHeader="method">
3 Data and Evaluation
</sectionHeader>
<bodyText confidence="0.998547769230769">
This CoNLL shared task works with roughly
the same sections of the Penn Treebank as the
widely used data set for base noun phrase recog-
nition (Ramshaw and Marcus, 1995): WSJ sec-
tions 15-18 of the Penn Treebank as training
material, section 20 as development material for
tuning the parameter of the learner and sec-
tion 21 as test data&apos;. The data sets contain
tokens (words and punctuation marks), infor-
mation about the location of sentence bound-
aries and information about clause boundaries.
Additionally, a part-of-speech (POS) tag and
a chunk tag was assigned to each token by a
standard POS tagger (Brill, 1994) and a chunk-
ing program (Tjong Kim Sang, 2000). We used
these POS and chunking tags rather than the
Treebank ones in order to make sure that the
performance rates obtained for this data are re-
alistic estimates for data for which no Treebank
tags are available. In the clause segmentation
we have only included clauses in the Treebank
which had a label starting with S thus disre-
garding clauses with label RRC or FRAG. All
clause labels have been converted to S.
Different schemes for encoding phrase infor-
mation have been used in the data:
</bodyText>
<listItem confidence="0.988712076923077">
• B-X, I-X and 0 have been used for mark-
ing the first word in a chunk of type X, a
non-initial word in an X chunk and a word
outside of any chunk, respectively (see also
Tjong Kim Sang and Buchholz (2000)).
• S, E and X mark a clause start, a clause end
and neither a clause start nor a clause end,
respectively. These tags have been used in
the first and second part of the shared task.
• (S*, *S) and * denote a clause start, a
clause end and neither a clause start nor a
clause end, respectively. The first two can
be used in combination with each other.
</listItem>
<bodyText confidence="0.98066525">
For example, (S*S) marks a word where a
clause starts and ends, and *S)S) marks a
word where two clauses end. These tags are
used in the third part of the shared task.
The first two phrase encodings were inspired by
the representation used by Ramshaw and Mar-
cus (1995). Here is an example of the clause
encoding schemes:
</bodyText>
<footnote confidence="0.6428035">
&apos;These clause data sets are available at
http://lcg—www.uia.ac.be/con112001/clauses/
</footnote>
<equation confidence="0.995847384615385">
Coach S X (S*
them X X *
in X X *
handling S X (S*
complaints X E *S)
so S X (S*
that X X *
they S X (S*
can X X *
resolve X X *
problems X X *
immediately X E *S)S)
. X E *S)
</equation>
<bodyText confidence="0.9999838125">
Three tags can be found next to each word, re-
spectively denoting the information for the first,
second and third part of the shared task. The
goal of this task is to predict the test data seg-
mentation as well as possible with a model built
from the training data.
The performance in this task is measured
with three rates. First, the percentage of de-
tected starts, ends or clauses that are correct
(precision). Second, the percentage of starts,
ends or clauses in the data that were found
by the learner (recall). And third, the F0_,
rate which is equal to ([32+1)*precision*recall
/ ([32*precision+recall) with [3=1 (van Rijsber-
gen, 1975). The latter rate has been used as the
target for optimization.
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999576263157895">
Six systems have participated in the shared
task. Two of them used boosting and the
others used techniques which were connection-
ist, memory-based, statistical and symbolic.
Patrick and Goyal (2001) applied the AdaBoost
algorithm for boosting the performance of deci-
sion graphs. The latter are an extension of de-
cision trees: they allow tree nodes to have more
than one parent. The boosting algorithm im-
proves the performance of the decision graphs
by assigning weights to the training data items
based on how accurately they have been clas-
sified. Hammerton (2001) used a feed-forward
neural network architecture, long short-term
memory, for predicting embedded clause struc-
tures. The network processes sentences word-
by-word. Memory cells in its hidden layer en-
able it to remember states with information
about the current clause.
</bodyText>
<table confidence="0.999785428571429">
development 1 precision recall F0_1
Carreras &amp; Mar. 95.77% 92.08% 93.89
Dejean 94.08% 84.59% 89.08
Molina &amp; Pla 89.21% 87.72% 88.46
Tjong Kim Sang 90.26% 85.99% 88.07
Patrick &amp; Goyal 78.34% 36.82% 50.10
baseline 96.32% 38.08% 54.58
test 1 precision recall F0_1
Carreras &amp; Mar. 93.96% 89.59% 91.72
Tjong Kim Sang 90.87% 84.35% 87.49
Dejean 93.76% 81.90% 87.43
Molina &amp; Pla 88.15% 84.88% 86.48
Patrick &amp; Goyal 72.81% 38.47% 50.34
baseline 98.44% 36.58% 53.34
</table>
<tableCaption confidence="0.999728">
Table 1: The performance of five systems while
</tableCaption>
<bodyText confidence="0.991660333333333">
processing the development data and the test
data for part 1 of the shared task: finding clause
starts. The baseline results have been obtained
by a system that assumes that every sentence
consists of one clause which contains the com-
plete sentence.
</bodyText>
<table confidence="0.999811785714286">
development 2 precision recall F0_1
Carreras &amp; Mar. 91.27% 89.00% 90.12
Tjong Kim Sang 83.80% 80.44% 82.09
Molina &amp; Pla 78.81% 78.54% 78.68
Dejean 99.28% 51.73% 68.02
Patrick &amp; Goyal 92.04% 48.57% 63.58
baseline 96.32% 51.86% 67.42
test 2 precision recall F0_1
Carreras &amp; Mar. 90.04% 88.41% 89.22
Tjong Kim Sang 84.72% 79.96% 82.28
Molina &amp; Pla 79.63% 77.17% 78.38
Dejean 99.28% 48.90% 65.47
Patrick &amp; Goyal 89.61% 45.39% 60.26
baseline 98.44% 48.90% 65.34
</table>
<tableCaption confidence="0.997943">
Table 2: The performance of five systems while
</tableCaption>
<bodyText confidence="0.998833410714286">
processing the development data and the test
data for part 2 of the shared task: identifying
clause ends. The baseline results have been ob-
tained by a system that assumes that every sen-
tence consists of one clause which contains the
complete sentence.
Dejean (2001) predicted clause boundaries
with his symbolic learner ALLiS (Architecture
for Learning Linguistic Structure). It is based
on theory refinement, which means that it
adapts grammars. The learner selects a set
of rules based on their prediction accuracy of
classes in a training corpus. Tjong Kim Sang
(2001) evaluated a memory-based learner while
using different combinations of features describ-
ing items which needed to be classified. His
learner was well suited for identifying clause
starts and clause ends but less suited for the
predicting complete clauses. Therefore he used
heuristic rules for converting the part one and
two results of the shared task to results for the
third part.
Molina and Pla (2001) have applied a spe-
cialized Hidden Markov Model (HMM) to the
shared task. They interpreted the three parts of
the shared task as tagging problems and made
the HMM find the most probable sequence of
tags given an input sequence. In the third part
of the task they limited the number of possible
output tags and used rules for fixing bracketing
problems. Carreras and Marquez (2001) con-
verted the clausing task to a set of binary de-
cisions which they modeled with decision trees
which are combined by AdaBoost. The system
uses features which in some cases contain rele-
vant information about a complete sentence. It
produces a list of clauses from which the ones
with the highest confidence scores will be pre-
sented as output.
We have derived baseline scores for the differ-
ent parts of the shared task by evaluating a sys-
tem that assigns one clause to every sentence.
Each of these clauses completely covers a sen-
tence. Five of the six systems perform above the
baseline. The exception is the system of Patrick
and Goyal which seems to have difficulty with
assigning a clause start to the first word of a
sentence. Would it have predicted a clause start
at the each sentence-initial word then the F0_1
rates for the development data for part 1 and
3 of the shared task would have been about 77
and 60 respectively, well above the two baseline
scores (54 and 52).
In the development data for part 1 of the
shared task, at 30 times all five participating
systems (Hammerton&apos;s only did part 3 of the
</bodyText>
<table confidence="0.9995496875">
development 3 precision recall F0_1
Carreras &amp; Mar. 87.18% 82.48% 84.77
Molina &amp; Pla 70.70% 71.35% 71.03
Tjong Kim Sang 74.20% 66.39% 70.08
Dejean 73.93% 62.44% 67.70
Hammerton 59.85% 55.56% 57.62
Patrick &amp; Goyal 30.72% 13.53% 18.79
baseline 96.32% 35.77% 52.17
test 3 precision recall F0_1
Carreras &amp; Mar. 84.82% 73.28% 78.63
Molina &amp; Pla 69.62% 64.17% 66.79
Tjong Kim Sang 75.06% 59.96% 66.67
Dejean 72.56% 54.55% 62.77
Hammerton 55.81% 45.99% 50.42
Patrick &amp; Goyal 29.54% 13.45% 18.49
baseline 98.44% 31.48% 47.71
</table>
<tableCaption confidence="0.99903">
Table 3: The performance of the six systems
</tableCaption>
<bodyText confidence="0.999254511111111">
while processing the development data and the
test data for part 3 of the shared task: recog-
nizing complete clauses. The baseline results
have been obtained by a system that assumes
that every sentence consists of one clause which
contains the complete sentence.
shared task) predicted a clause start at a posi-
tion where there was none. About half of these
were in front of the word to. The situation in
which all five systems missed a clause start oc-
curred 205 times at positions with different suc-
ceeding words. It seems that many of these er-
rors were caused by a missing comma immedi-
ately before the clause start.
In three cases, the five systems unanimously
found an end of a clause where there was none
in the development data of part 2 of the shared
task. All these occurred at the end of &apos;sentences&apos;
which consisted of a single noun phrase or a
single adverbial phrase. In 205 cases all five
systems missed a clause end. These errors often
occurred right before punctuation signs.
It is hard to make a similar overview for
part 3 of the shared task. Therefore we have
only looked at the accuracies of two clause tags:
(S(S* (starting two clauses) and *S)S) (ending
two clauses). Never did more than three of the
six systems correctly predicted the start of two
clauses. The best performing system for this
clause tag was the one of Carreras and Marquez
with about 52% recall. Three of the systems did
not find back any of the double clause starts and
the average recall score of the six was 21%. The
end of two clauses was correctly predicted by all
six systems about 0.5% of the times it occurred.
Again, the system of Carreras and Marquez was
best with 63% recall while the average system
found back 33%.
The six result tables show that the system of
Carreras and Marquez clearly outperforms the
other five systems on all parts of the shared
task. They were the only one to use input
features that contained information of a com-
plete sentence and it seems that this was a good
choice.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999979">
There have been some earlier studies in identi-
fying clauses. Abney (1990) used a clause filter
as a part of his CASS parser. It consists of
two parts: one for recognizing basic clauses and
one for repairing difficult cases (clauses with-
out subjects and clauses with additional VPs).
Ejerhed (1996) showed that a parser can benefit
from automatically identified clause boundaries
in discourse. Papageorgiou (1997) used a set of
hand-crafted rules for identifying clause bound-
aries in one text. Leffa (1998) wrote a set of
clause identification rules and applied them to
a small corpus. The performance was very good,
with recall rates above 90%. Orasan (2000) used
a memory-based learner with post-processing
rules for predicting clause boundaries in Su-
sanne corpus. His system obtained F rates of
about 85 for this particular task.
</bodyText>
<sectionHeader confidence="0.993331" genericHeader="method">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9991555">
We have presented the CoNLL-2001 shared
task: clause identification. The task was split
in three parts: recognizing clause starts, find-
ing clause ends and identifying complete, pos-
sibly embedded, clauses. Six systems have
participated in this shared task. They used
various machine learning techniques, boosting,
connectionist methods, decision trees, memory-
based learning, statistical techniques and sym-
bolic methods. On all three parts of the shared
task the boosted decision tree system of Car-
reras and Marquez (2001) performed best. It
obtained an F0_1 rate of 78.63 for the third part
of the shared task.
</bodyText>
<sectionHeader confidence="0.993766" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999807555555555">
We would like to thank SIGNLL for giving us
the opportunity to organize this shared task
and our colleagues of the Seminar fiir Sprach-
wissenschaft in Tübingen, CNTS - Language
Technology Group in Antwerp, and the ILK
group in Tilburg for valuable discussions and
comments. This research has been funded by
the European TMR network Learning Compu-
tational Grammars2.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899090909091">
Steven Abney. 1990. Rapid Incremental Parsing
with Repair. In Proceedings of the 8th New OED
Conference: Electronic Text Research. University
of Waterloo, Ontario.
Ann Bies, Mark Fergusson, Karen Katz, and RObert
MacIntyre. 1995. Bracketing Guidelines for Tree-
bank II Style Penn Treebank Project. Technical
report, University of Pennsylvania.
Eric Brill. 1994. Some advances in rule-based
part of speech tagging. In Proceedings of the
Twelfth National Conference on Artificial Intel-
ligence (AAAI-94). Seattle, Washington.
Xavier Carreras and Luis Marquez. 2001. Boost-
ing Trees for Clause Splitting. In Proceedings of
CoNLL-2001. Toulouse, France.
Herve Mean. 2001. Using ALLiS for Clausing. In
Proceedings of CoNLL-2001. Toulouse, France.
Eva Ejerhed. 1988. Finding clauses in unrestrected
text by finitary and stochastic methods. In Pro-
ceedings of the second Conference on Applied Nat-
ural Language Processing, pages 219-227.
Eva Ejerhed. 1996. Finite state segmentation of dis-
course into clauses. In Proceedings of the ECAI
&apos;96 Workshop on Extended finite state models of
language. ECAI &apos;96, Budapest, Hungary.
James Hammerton. 2001. Clause identification with
Long Short-Term Memory. In Proceedings of
CoNLL-2001. Toulouse, France.
Vilson J. Leffa. 1998. Clause Processing in Complex
Sentences. In Proceedings of LREC&apos;98. Granada,
Spain.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a large
annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2).
Antonio Molina and Ferran Pla. 2001. Clause De-
tection using HMM. In Proceedings of CoNLL-
2001. Toulouse, France.
Constantin Orasan. 2000. A hybrid method for
clause splitting in unrestricted English texts. In
Proceedings of ACIDC A &apos;2000. Monastir, Tunisia.
H. V. Papageorgiou. 1997. Clause recognition in
the framework of alignment. In R. Mitkov and
N. Nicolov, editors, Recent Advances in Natural
Language Processing. John Benjamins Publishing
Company, Amsterdam/Philadelphia.
Jon D. Patrick and Ishaan Goyal. 2001. Boosted
Decision Graphs for NLP Learning Tasks. In Pro-
ceedings of CoNLL-2001. Toulouse, France.
Lance A. Ramshaw and Mitchell P. Marcus.
1995. Text Chunking Using Transformation-
Based Learning. In Proceedings of the Third ACL
Workshop on Very Large Corpora. Cambridge,
MA, USA.
Erik F. Tjong Kim Sang. 2001. Memory-Based
Clause Identification. In Proceedings of CoNLL-
2001. Toulouse, France.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 Shared Task:
Chunking. In Proceedings of the CoNLL-2000 and
LLL-2000. Lisbon, Portugal.
Erik F. Tjong Kim Sang. 2000. Text Chunking by
System Combination. In Proceedings of CoNLL-
2000 and LLL-2000. Lisbon, Portugal.
C.J. van Rijsbergen. 1975. Information Retrieval.
Buttersworth.
</reference>
<footnote confidence="0.80769">
2http://1cg-www.uia.ac.be/
</footnote>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Rapid Incremental Parsing with Repair.</title>
<date>1990</date>
<booktitle>In Proceedings of the 8th New OED Conference: Electronic</booktitle>
<institution>Text Research. University of Waterloo,</institution>
<location>Ontario.</location>
<contexts>
<context position="13176" citStr="Abney (1990)" startWordPosition="2252" endWordPosition="2253">recall score of the six was 21%. The end of two clauses was correctly predicted by all six systems about 0.5% of the times it occurred. Again, the system of Carreras and Marquez was best with 63% recall while the average system found back 33%. The six result tables show that the system of Carreras and Marquez clearly outperforms the other five systems on all parts of the shared task. They were the only one to use input features that contained information of a complete sentence and it seems that this was a good choice. 5 Related Work There have been some earlier studies in identifying clauses. Abney (1990) used a clause filter as a part of his CASS parser. It consists of two parts: one for recognizing basic clauses and one for repairing difficult cases (clauses without subjects and clauses with additional VPs). Ejerhed (1996) showed that a parser can benefit from automatically identified clause boundaries in discourse. Papageorgiou (1997) used a set of hand-crafted rules for identifying clause boundaries in one text. Leffa (1998) wrote a set of clause identification rules and applied them to a small corpus. The performance was very good, with recall rates above 90%. Orasan (2000) used a memory-</context>
</contexts>
<marker>Abney, 1990</marker>
<rawString>Steven Abney. 1990. Rapid Incremental Parsing with Repair. In Proceedings of the 8th New OED Conference: Electronic Text Research. University of Waterloo, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Fergusson</author>
<author>Karen Katz</author>
<author>RObert MacIntyre</author>
</authors>
<title>Bracketing Guidelines for Treebank II Style Penn Treebank Project.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1947" citStr="Bies et al., 1995" startWordPosition="295" endWordPosition="298">, especially machine learning methods, for finding clause boundaries in text. We have selected a training and test corpus for performing this evaluation. The task has been divided in three parts in order to allow basic machine learning methods to participate in this task by processing the data in a bottom-up fashion. 2 Task description Defining clause boundaries is not trivial (Leffa, 1998). In this task, the gold standard clause segmentation is provided by the Penn Treebank (Marcus et al., 1993). The guidelines of the Penn Treebank describe in detail how sentences are segmented into clauses (Bies et al., 1995). Here is an example of a sentence and its clauses obtained from Wall Street Journal section 15 of the Penn Treebank (Marcus et al., 1993): (S Coach them in (S—NOM handling complaints) (SBAR—PRP so that (S they can resolve problems immediately) ) ) The clauses of this sentence have been enclosed between brackets. A tag next to the open bracket denotes the type of the clause. In the CoNLL-2001 shared task, the goal is to identify clauses in text. Since clauses can be embedded in each other, this task is considerably more difficult than last year&apos;s task, recognizing non-embedded text chunks. For</context>
</contexts>
<marker>Bies, Fergusson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Fergusson, Karen Katz, and RObert MacIntyre. 1995. Bracketing Guidelines for Treebank II Style Penn Treebank Project. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in rule-based part of speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94).</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="3582" citStr="Brill, 1994" startWordPosition="576" endWordPosition="577">is CoNLL shared task works with roughly the same sections of the Penn Treebank as the widely used data set for base noun phrase recognition (Ramshaw and Marcus, 1995): WSJ sections 15-18 of the Penn Treebank as training material, section 20 as development material for tuning the parameter of the learner and section 21 as test data&apos;. The data sets contain tokens (words and punctuation marks), information about the location of sentence boundaries and information about clause boundaries. Additionally, a part-of-speech (POS) tag and a chunk tag was assigned to each token by a standard POS tagger (Brill, 1994) and a chunking program (Tjong Kim Sang, 2000). We used these POS and chunking tags rather than the Treebank ones in order to make sure that the performance rates obtained for this data are realistic estimates for data for which no Treebank tags are available. In the clause segmentation we have only included clauses in the Treebank which had a label starting with S thus disregarding clauses with label RRC or FRAG. All clause labels have been converted to S. Different schemes for encoding phrase information have been used in the data: • B-X, I-X and 0 have been used for marking the first word i</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some advances in rule-based part of speech tagging. In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Luis Marquez</author>
</authors>
<title>Boosting Trees for Clause Splitting.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL-2001.</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="9419" citStr="Carreras and Marquez (2001)" startWordPosition="1584" endWordPosition="1587">or identifying clause starts and clause ends but less suited for the predicting complete clauses. Therefore he used heuristic rules for converting the part one and two results of the shared task to results for the third part. Molina and Pla (2001) have applied a specialized Hidden Markov Model (HMM) to the shared task. They interpreted the three parts of the shared task as tagging problems and made the HMM find the most probable sequence of tags given an input sequence. In the third part of the task they limited the number of possible output tags and used rules for fixing bracketing problems. Carreras and Marquez (2001) converted the clausing task to a set of binary decisions which they modeled with decision trees which are combined by AdaBoost. The system uses features which in some cases contain relevant information about a complete sentence. It produces a list of clauses from which the ones with the highest confidence scores will be presented as output. We have derived baseline scores for the different parts of the shared task by evaluating a system that assigns one clause to every sentence. Each of these clauses completely covers a sentence. Five of the six systems perform above the baseline. The excepti</context>
</contexts>
<marker>Carreras, Marquez, 2001</marker>
<rawString>Xavier Carreras and Luis Marquez. 2001. Boosting Trees for Clause Splitting. In Proceedings of CoNLL-2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herve Mean</author>
</authors>
<title>Using ALLiS for Clausing.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL-2001.</booktitle>
<location>Toulouse, France.</location>
<marker>Mean, 2001</marker>
<rawString>Herve Mean. 2001. Using ALLiS for Clausing. In Proceedings of CoNLL-2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Ejerhed</author>
</authors>
<title>Finding clauses in unrestrected text by finitary and stochastic methods.</title>
<date>1988</date>
<booktitle>In Proceedings of the second Conference on Applied Natural Language Processing,</booktitle>
<pages>219--227</pages>
<contexts>
<context position="727" citStr="Ejerhed, 1988" startWordPosition="100" endWordPosition="101">gy Group University of Antwerp erikt@uia.ua.ac.be Herve Dejean Seminar fiir Sprachwissenschaft Universitat Tubingen dejean@sfs.nphil.uni-tuebingen.de Abstract We describe the CoNLL-2001 shared task: dividing text into clauses. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance. 1 Introduction The CoNLL-2001 shared task aims at discovering clause boundaries with machine learning methods. Why clauses? Clauses are structures used in applications such as Text-to-Speech conversion (Ejerhed, 1988), text-alignment (Papageorgiou, 1997) and machine translation (Leffa, 1998). Ejerhed (1988) described clauses as a natural structure above chunks: It is a hypothesis of the author&apos;s current clause-by-clause processing theory, that a unit corresponding to the basic clause is a stable and easily recognizable surface unit and that is is also an important partial result and building block in the construction od a richer linguistic representation that encompasses syntax as well as semantics and discourse structure (Ejerhed, 1988, page 220) The goal of this shared task is to evaluate automatic metho</context>
</contexts>
<marker>Ejerhed, 1988</marker>
<rawString>Eva Ejerhed. 1988. Finding clauses in unrestrected text by finitary and stochastic methods. In Proceedings of the second Conference on Applied Natural Language Processing, pages 219-227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Ejerhed</author>
</authors>
<title>Finite state segmentation of discourse into clauses.</title>
<date>1996</date>
<booktitle>In Proceedings of the ECAI &apos;96 Workshop on Extended finite state models of language. ECAI &apos;96,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="13400" citStr="Ejerhed (1996)" startWordPosition="2290" endWordPosition="2291">ystem found back 33%. The six result tables show that the system of Carreras and Marquez clearly outperforms the other five systems on all parts of the shared task. They were the only one to use input features that contained information of a complete sentence and it seems that this was a good choice. 5 Related Work There have been some earlier studies in identifying clauses. Abney (1990) used a clause filter as a part of his CASS parser. It consists of two parts: one for recognizing basic clauses and one for repairing difficult cases (clauses without subjects and clauses with additional VPs). Ejerhed (1996) showed that a parser can benefit from automatically identified clause boundaries in discourse. Papageorgiou (1997) used a set of hand-crafted rules for identifying clause boundaries in one text. Leffa (1998) wrote a set of clause identification rules and applied them to a small corpus. The performance was very good, with recall rates above 90%. Orasan (2000) used a memory-based learner with post-processing rules for predicting clause boundaries in Susanne corpus. His system obtained F rates of about 85 for this particular task. 6 Concluding Remarks We have presented the CoNLL-2001 shared task</context>
</contexts>
<marker>Ejerhed, 1996</marker>
<rawString>Eva Ejerhed. 1996. Finite state segmentation of discourse into clauses. In Proceedings of the ECAI &apos;96 Workshop on Extended finite state models of language. ECAI &apos;96, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hammerton</author>
</authors>
<title>Clause identification with Long Short-Term Memory.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL-2001.</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="6538" citStr="Hammerton (2001)" startWordPosition="1118" endWordPosition="1119"> been used as the target for optimization. 4 Results Six systems have participated in the shared task. Two of them used boosting and the others used techniques which were connectionist, memory-based, statistical and symbolic. Patrick and Goyal (2001) applied the AdaBoost algorithm for boosting the performance of decision graphs. The latter are an extension of decision trees: they allow tree nodes to have more than one parent. The boosting algorithm improves the performance of the decision graphs by assigning weights to the training data items based on how accurately they have been classified. Hammerton (2001) used a feed-forward neural network architecture, long short-term memory, for predicting embedded clause structures. The network processes sentences wordby-word. Memory cells in its hidden layer enable it to remember states with information about the current clause. development 1 precision recall F0_1 Carreras &amp; Mar. 95.77% 92.08% 93.89 Dejean 94.08% 84.59% 89.08 Molina &amp; Pla 89.21% 87.72% 88.46 Tjong Kim Sang 90.26% 85.99% 88.07 Patrick &amp; Goyal 78.34% 36.82% 50.10 baseline 96.32% 38.08% 54.58 test 1 precision recall F0_1 Carreras &amp; Mar. 93.96% 89.59% 91.72 Tjong Kim Sang 90.87% 84.35% 87.49 D</context>
</contexts>
<marker>Hammerton, 2001</marker>
<rawString>James Hammerton. 2001. Clause identification with Long Short-Term Memory. In Proceedings of CoNLL-2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vilson J Leffa</author>
</authors>
<title>Clause Processing in Complex Sentences.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC&apos;98.</booktitle>
<location>Granada,</location>
<contexts>
<context position="802" citStr="Leffa, 1998" startWordPosition="109" endWordPosition="110">prachwissenschaft Universitat Tubingen dejean@sfs.nphil.uni-tuebingen.de Abstract We describe the CoNLL-2001 shared task: dividing text into clauses. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance. 1 Introduction The CoNLL-2001 shared task aims at discovering clause boundaries with machine learning methods. Why clauses? Clauses are structures used in applications such as Text-to-Speech conversion (Ejerhed, 1988), text-alignment (Papageorgiou, 1997) and machine translation (Leffa, 1998). Ejerhed (1988) described clauses as a natural structure above chunks: It is a hypothesis of the author&apos;s current clause-by-clause processing theory, that a unit corresponding to the basic clause is a stable and easily recognizable surface unit and that is is also an important partial result and building block in the construction od a richer linguistic representation that encompasses syntax as well as semantics and discourse structure (Ejerhed, 1988, page 220) The goal of this shared task is to evaluate automatic methods, especially machine learning methods, for finding clause boundaries in t</context>
<context position="13608" citStr="Leffa (1998)" startWordPosition="2321" endWordPosition="2322">that contained information of a complete sentence and it seems that this was a good choice. 5 Related Work There have been some earlier studies in identifying clauses. Abney (1990) used a clause filter as a part of his CASS parser. It consists of two parts: one for recognizing basic clauses and one for repairing difficult cases (clauses without subjects and clauses with additional VPs). Ejerhed (1996) showed that a parser can benefit from automatically identified clause boundaries in discourse. Papageorgiou (1997) used a set of hand-crafted rules for identifying clause boundaries in one text. Leffa (1998) wrote a set of clause identification rules and applied them to a small corpus. The performance was very good, with recall rates above 90%. Orasan (2000) used a memory-based learner with post-processing rules for predicting clause boundaries in Susanne corpus. His system obtained F rates of about 85 for this particular task. 6 Concluding Remarks We have presented the CoNLL-2001 shared task: clause identification. The task was split in three parts: recognizing clause starts, finding clause ends and identifying complete, possibly embedded, clauses. Six systems have participated in this shared ta</context>
</contexts>
<marker>Leffa, 1998</marker>
<rawString>Vilson J. Leffa. 1998. Clause Processing in Complex Sentences. In Proceedings of LREC&apos;98. Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1830" citStr="Marcus et al., 1993" startWordPosition="276" endWordPosition="279">mantics and discourse structure (Ejerhed, 1988, page 220) The goal of this shared task is to evaluate automatic methods, especially machine learning methods, for finding clause boundaries in text. We have selected a training and test corpus for performing this evaluation. The task has been divided in three parts in order to allow basic machine learning methods to participate in this task by processing the data in a bottom-up fashion. 2 Task description Defining clause boundaries is not trivial (Leffa, 1998). In this task, the gold standard clause segmentation is provided by the Penn Treebank (Marcus et al., 1993). The guidelines of the Penn Treebank describe in detail how sentences are segmented into clauses (Bies et al., 1995). Here is an example of a sentence and its clauses obtained from Wall Street Journal section 15 of the Penn Treebank (Marcus et al., 1993): (S Coach them in (S—NOM handling complaints) (SBAR—PRP so that (S they can resolve problems immediately) ) ) The clauses of this sentence have been enclosed between brackets. A tag next to the open bracket denotes the type of the clause. In the CoNLL-2001 shared task, the goal is to identify clauses in text. Since clauses can be embedded in </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Molina</author>
<author>Ferran Pla</author>
</authors>
<title>Clause Detection using HMM.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL2001.</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="9039" citStr="Molina and Pla (2001)" startWordPosition="1518" endWordPosition="1521">nguistic Structure). It is based on theory refinement, which means that it adapts grammars. The learner selects a set of rules based on their prediction accuracy of classes in a training corpus. Tjong Kim Sang (2001) evaluated a memory-based learner while using different combinations of features describing items which needed to be classified. His learner was well suited for identifying clause starts and clause ends but less suited for the predicting complete clauses. Therefore he used heuristic rules for converting the part one and two results of the shared task to results for the third part. Molina and Pla (2001) have applied a specialized Hidden Markov Model (HMM) to the shared task. They interpreted the three parts of the shared task as tagging problems and made the HMM find the most probable sequence of tags given an input sequence. In the third part of the task they limited the number of possible output tags and used rules for fixing bracketing problems. Carreras and Marquez (2001) converted the clausing task to a set of binary decisions which they modeled with decision trees which are combined by AdaBoost. The system uses features which in some cases contain relevant information about a complete </context>
</contexts>
<marker>Molina, Pla, 2001</marker>
<rawString>Antonio Molina and Ferran Pla. 2001. Clause Detection using HMM. In Proceedings of CoNLL2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantin Orasan</author>
</authors>
<title>A hybrid method for clause splitting in unrestricted English texts.</title>
<date>2000</date>
<booktitle>In Proceedings of ACIDC A</booktitle>
<location>Monastir, Tunisia.</location>
<contexts>
<context position="13761" citStr="Orasan (2000)" startWordPosition="2347" endWordPosition="2348">ifying clauses. Abney (1990) used a clause filter as a part of his CASS parser. It consists of two parts: one for recognizing basic clauses and one for repairing difficult cases (clauses without subjects and clauses with additional VPs). Ejerhed (1996) showed that a parser can benefit from automatically identified clause boundaries in discourse. Papageorgiou (1997) used a set of hand-crafted rules for identifying clause boundaries in one text. Leffa (1998) wrote a set of clause identification rules and applied them to a small corpus. The performance was very good, with recall rates above 90%. Orasan (2000) used a memory-based learner with post-processing rules for predicting clause boundaries in Susanne corpus. His system obtained F rates of about 85 for this particular task. 6 Concluding Remarks We have presented the CoNLL-2001 shared task: clause identification. The task was split in three parts: recognizing clause starts, finding clause ends and identifying complete, possibly embedded, clauses. Six systems have participated in this shared task. They used various machine learning techniques, boosting, connectionist methods, decision trees, memorybased learning, statistical techniques and symb</context>
</contexts>
<marker>Orasan, 2000</marker>
<rawString>Constantin Orasan. 2000. A hybrid method for clause splitting in unrestricted English texts. In Proceedings of ACIDC A &apos;2000. Monastir, Tunisia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H V Papageorgiou</author>
</authors>
<title>Clause recognition in the framework of alignment.</title>
<date>1997</date>
<booktitle>Recent Advances in Natural Language Processing.</booktitle>
<editor>In R. Mitkov and N. Nicolov, editors,</editor>
<publisher>John Benjamins Publishing Company, Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="764" citStr="Papageorgiou, 1997" startWordPosition="103" endWordPosition="105">rikt@uia.ua.ac.be Herve Dejean Seminar fiir Sprachwissenschaft Universitat Tubingen dejean@sfs.nphil.uni-tuebingen.de Abstract We describe the CoNLL-2001 shared task: dividing text into clauses. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance. 1 Introduction The CoNLL-2001 shared task aims at discovering clause boundaries with machine learning methods. Why clauses? Clauses are structures used in applications such as Text-to-Speech conversion (Ejerhed, 1988), text-alignment (Papageorgiou, 1997) and machine translation (Leffa, 1998). Ejerhed (1988) described clauses as a natural structure above chunks: It is a hypothesis of the author&apos;s current clause-by-clause processing theory, that a unit corresponding to the basic clause is a stable and easily recognizable surface unit and that is is also an important partial result and building block in the construction od a richer linguistic representation that encompasses syntax as well as semantics and discourse structure (Ejerhed, 1988, page 220) The goal of this shared task is to evaluate automatic methods, especially machine learning metho</context>
<context position="13515" citStr="Papageorgiou (1997)" startWordPosition="2305" endWordPosition="2306">he other five systems on all parts of the shared task. They were the only one to use input features that contained information of a complete sentence and it seems that this was a good choice. 5 Related Work There have been some earlier studies in identifying clauses. Abney (1990) used a clause filter as a part of his CASS parser. It consists of two parts: one for recognizing basic clauses and one for repairing difficult cases (clauses without subjects and clauses with additional VPs). Ejerhed (1996) showed that a parser can benefit from automatically identified clause boundaries in discourse. Papageorgiou (1997) used a set of hand-crafted rules for identifying clause boundaries in one text. Leffa (1998) wrote a set of clause identification rules and applied them to a small corpus. The performance was very good, with recall rates above 90%. Orasan (2000) used a memory-based learner with post-processing rules for predicting clause boundaries in Susanne corpus. His system obtained F rates of about 85 for this particular task. 6 Concluding Remarks We have presented the CoNLL-2001 shared task: clause identification. The task was split in three parts: recognizing clause starts, finding clause ends and iden</context>
</contexts>
<marker>Papageorgiou, 1997</marker>
<rawString>H. V. Papageorgiou. 1997. Clause recognition in the framework of alignment. In R. Mitkov and N. Nicolov, editors, Recent Advances in Natural Language Processing. John Benjamins Publishing Company, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon D Patrick</author>
<author>Ishaan Goyal</author>
</authors>
<title>Boosted Decision Graphs for NLP Learning Tasks.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL-2001.</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="6172" citStr="Patrick and Goyal (2001)" startWordPosition="1055" endWordPosition="1058">s task is measured with three rates. First, the percentage of detected starts, ends or clauses that are correct (precision). Second, the percentage of starts, ends or clauses in the data that were found by the learner (recall). And third, the F0_, rate which is equal to ([32+1)*precision*recall / ([32*precision+recall) with [3=1 (van Rijsbergen, 1975). The latter rate has been used as the target for optimization. 4 Results Six systems have participated in the shared task. Two of them used boosting and the others used techniques which were connectionist, memory-based, statistical and symbolic. Patrick and Goyal (2001) applied the AdaBoost algorithm for boosting the performance of decision graphs. The latter are an extension of decision trees: they allow tree nodes to have more than one parent. The boosting algorithm improves the performance of the decision graphs by assigning weights to the training data items based on how accurately they have been classified. Hammerton (2001) used a feed-forward neural network architecture, long short-term memory, for predicting embedded clause structures. The network processes sentences wordby-word. Memory cells in its hidden layer enable it to remember states with infor</context>
</contexts>
<marker>Patrick, Goyal, 2001</marker>
<rawString>Jon D. Patrick and Ishaan Goyal. 2001. Boosted Decision Graphs for NLP Learning Tasks. In Proceedings of CoNLL-2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text Chunking Using TransformationBased Learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora.</booktitle>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="3136" citStr="Ramshaw and Marcus, 1995" startWordPosition="500" endWordPosition="503">nizing non-embedded text chunks. For that reason, we have disregarded type and function information of the clauses: every clause has been tagged with S rather than with an elaborate tag such as SBAR—PRP. Furthermore, the shared task has been divided in three parts: identifying clause starts, recognizing clause ends and finding complete clauses. The results obtained for the first two parts can be used in the third part of the task. 3 Data and Evaluation This CoNLL shared task works with roughly the same sections of the Penn Treebank as the widely used data set for base noun phrase recognition (Ramshaw and Marcus, 1995): WSJ sections 15-18 of the Penn Treebank as training material, section 20 as development material for tuning the parameter of the learner and section 21 as test data&apos;. The data sets contain tokens (words and punctuation marks), information about the location of sentence boundaries and information about clause boundaries. Additionally, a part-of-speech (POS) tag and a chunk tag was assigned to each token by a standard POS tagger (Brill, 1994) and a chunking program (Tjong Kim Sang, 2000). We used these POS and chunking tags rather than the Treebank ones in order to make sure that the performan</context>
<context position="4948" citStr="Ramshaw and Marcus (1995)" startWordPosition="832" endWordPosition="836">00)). • S, E and X mark a clause start, a clause end and neither a clause start nor a clause end, respectively. These tags have been used in the first and second part of the shared task. • (S*, *S) and * denote a clause start, a clause end and neither a clause start nor a clause end, respectively. The first two can be used in combination with each other. For example, (S*S) marks a word where a clause starts and ends, and *S)S) marks a word where two clauses end. These tags are used in the third part of the shared task. The first two phrase encodings were inspired by the representation used by Ramshaw and Marcus (1995). Here is an example of the clause encoding schemes: &apos;These clause data sets are available at http://lcg—www.uia.ac.be/con112001/clauses/ Coach S X (S* them X X * in X X * handling S X (S* complaints X E *S) so S X (S* that X X * they S X (S* can X X * resolve X X * problems X X * immediately X E *S)S) . X E *S) Three tags can be found next to each word, respectively denoting the information for the first, second and third part of the shared task. The goal of this task is to predict the test data segmentation as well as possible with a model built from the training data. The performance in thi</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text Chunking Using TransformationBased Learning. In Proceedings of the Third ACL Workshop on Very Large Corpora. Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Memory-Based Clause Identification.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL2001.</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="8634" citStr="Sang (2001)" startWordPosition="1455" endWordPosition="1456">: The performance of five systems while processing the development data and the test data for part 2 of the shared task: identifying clause ends. The baseline results have been obtained by a system that assumes that every sentence consists of one clause which contains the complete sentence. Dejean (2001) predicted clause boundaries with his symbolic learner ALLiS (Architecture for Learning Linguistic Structure). It is based on theory refinement, which means that it adapts grammars. The learner selects a set of rules based on their prediction accuracy of classes in a training corpus. Tjong Kim Sang (2001) evaluated a memory-based learner while using different combinations of features describing items which needed to be classified. His learner was well suited for identifying clause starts and clause ends but less suited for the predicting complete clauses. Therefore he used heuristic rules for converting the part one and two results of the shared task to results for the third part. Molina and Pla (2001) have applied a specialized Hidden Markov Model (HMM) to the shared task. They interpreted the three parts of the shared task as tagging problems and made the HMM find the most probable sequence </context>
</contexts>
<marker>Sang, 2001</marker>
<rawString>Erik F. Tjong Kim Sang. 2001. Memory-Based Clause Identification. In Proceedings of CoNLL2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 Shared Task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the CoNLL-2000 and LLL-2000.</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="4326" citStr="Sang and Buchholz (2000)" startWordPosition="713" endWordPosition="716">er to make sure that the performance rates obtained for this data are realistic estimates for data for which no Treebank tags are available. In the clause segmentation we have only included clauses in the Treebank which had a label starting with S thus disregarding clauses with label RRC or FRAG. All clause labels have been converted to S. Different schemes for encoding phrase information have been used in the data: • B-X, I-X and 0 have been used for marking the first word in a chunk of type X, a non-initial word in an X chunk and a word outside of any chunk, respectively (see also Tjong Kim Sang and Buchholz (2000)). • S, E and X mark a clause start, a clause end and neither a clause start nor a clause end, respectively. These tags have been used in the first and second part of the shared task. • (S*, *S) and * denote a clause start, a clause end and neither a clause start nor a clause end, respectively. The first two can be used in combination with each other. For example, (S*S) marks a word where a clause starts and ends, and *S)S) marks a word where two clauses end. These tags are used in the third part of the shared task. The first two phrase encodings were inspired by the representation used by Ram</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 Shared Task: Chunking. In Proceedings of the CoNLL-2000 and LLL-2000. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Text Chunking by System Combination.</title>
<date>2000</date>
<journal>Information Retrieval. Buttersworth.</journal>
<booktitle>In Proceedings of CoNLL2000 and LLL-2000.</booktitle>
<location>Lisbon,</location>
<contexts>
<context position="3628" citStr="Sang, 2000" startWordPosition="585" endWordPosition="586">e sections of the Penn Treebank as the widely used data set for base noun phrase recognition (Ramshaw and Marcus, 1995): WSJ sections 15-18 of the Penn Treebank as training material, section 20 as development material for tuning the parameter of the learner and section 21 as test data&apos;. The data sets contain tokens (words and punctuation marks), information about the location of sentence boundaries and information about clause boundaries. Additionally, a part-of-speech (POS) tag and a chunk tag was assigned to each token by a standard POS tagger (Brill, 1994) and a chunking program (Tjong Kim Sang, 2000). We used these POS and chunking tags rather than the Treebank ones in order to make sure that the performance rates obtained for this data are realistic estimates for data for which no Treebank tags are available. In the clause segmentation we have only included clauses in the Treebank which had a label starting with S thus disregarding clauses with label RRC or FRAG. All clause labels have been converted to S. Different schemes for encoding phrase information have been used in the data: • B-X, I-X and 0 have been used for marking the first word in a chunk of type X, a non-initial word in an </context>
</contexts>
<marker>Sang, 2000</marker>
<rawString>Erik F. Tjong Kim Sang. 2000. Text Chunking by System Combination. In Proceedings of CoNLL2000 and LLL-2000. Lisbon, Portugal. C.J. van Rijsbergen. 1975. Information Retrieval. Buttersworth.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>