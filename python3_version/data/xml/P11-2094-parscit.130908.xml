<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9975665">
Nonparametric Bayesian Machine Transliteration with Synchronous
Adaptor Grammars
</title>
<author confidence="0.987027">
Yun Huang1,2 Min Zhang1 Chew Lim Tan2
</author>
<affiliation confidence="0.91281325">
huangyun@comp.nus.edu.sg mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
1Human Language Department 2Department of Computer Science
Institute for Infocomm Research National University of Singapore
1 Fusionopolis Way, Singapore 13 Computing Drive, Singapore
</affiliation>
<sectionHeader confidence="0.98301" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.931665166666667">
Machine transliteration is defined as auto-
matic phonetic translation of names across
languages. In this paper, we propose syn-
chronous adaptor grammar, a novel nonpara-
metric Bayesian learning approach, for ma-
chine transliteration. This model provides
a general framework without heuristic or re-
striction to automatically learn syllable equiv-
alents between languages. The proposed
model outperforms the state-of-the-art EM-
based model in the English to Chinese translit-
eration task.
</bodyText>
<sectionHeader confidence="0.998638" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869113207547">
Proper names are one source of OOV words in many
NLP tasks, such as machine translation and cross-
lingual information retrieval. They are often trans-
lated through transliteration, i.e. translation by pre-
serving how words sound in both languages. In
general, machine transliteration is often modelled
as monotonic machine translation (Rama and Gali,
2009; Finch and Sumita, 2009; Finch and Sumita,
2010), the joint source-channel models (Li et al.,
2004; Yang et al., 2009), or the sequential label-
ing problems (Reddy and Waxmonsky, 2009; Ab-
dul Hamid and Darwish, 2010).
Syllable equivalents acquisition is a critical phase
for all these models. Traditional learning approaches
aim to maximize the likelihood of training data
by the Expectation-Maximization (EM) algorithm.
However, the EM algorithm may over-fit the training
data by memorizing the whole training instances. To
avoid this problem, some approaches restrict that a
single character in one language could be aligned
to many characters of the other, but not vice versa
(Li et al., 2004; Yang et al., 2009). Heuristics are
introduced to obtain many-to-many alignments by
combining two directional one-to-many alignments
(Rama and Gali, 2009). Compared to maximum
likelihood approaches, Bayesian models provide a
systemic way to encode knowledges and infer com-
pact structures. They have been successfully applied
to many machine learning tasks (Liu and Gildea,
2009; Zhang et al., 2008; Blunsom et al., 2009).
Among these models, Adaptor Grammars (AGs)
provide a framework for defining nonparametric
Bayesian models based on PCFGs (Johnson et al.,
2007). They introduce additional stochastic pro-
cesses (named adaptors) allowing the expansion of
an adapted symbol to depend on the expansion his-
tory. Since many existing models could be viewed
as special kinds of PCFG, adaptor grammars give
general Bayesian extension to them. AGs have been
used in various NLP tasks such as topic modeling
(Johnson, 2010), perspective modeling (Hardisty et
al., 2010), morphology analysis and word segmenta-
tion (Johnson and Goldwater, 2009; Johnson, 2008).
In this paper, we extend AGs to Synchronous
Adaptor Grammars (SAGs), and describe the in-
ference algorithm based on the Pitman-Yor process
(Pitman and Yor, 1997). We also describe how
transliteration could be modelled under this formal-
ism. It should be emphasized that the proposed
method is language independent and heuristic-free.
Experiments show the proposed approach outper-
forms the strong EM-based baseline in the English
to Chinese transliteration task.
</bodyText>
<page confidence="0.973869">
534
</page>
<note confidence="0.7982875">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.824441" genericHeader="method">
2 Synchronous Adaptor Grammars
</sectionHeader>
<subsectionHeader confidence="0.977944">
2.1 Model
</subsectionHeader>
<bodyText confidence="0.9989615">
A Pitman-Yor Synchronous Adaptor Grammar
(PYSAG) is a tuple !9 = (!9s, Na, a, b, a), where
!9s = (N, T, T, R, S, O) is a Synchronous
Context-Free Grammar (SCFG) (Chiang, 2007),
N is a set of nonterminal symbols, 7/7 are
source/target terminal symbols, R is a set of rewrite
rules, S E N is the start symbol, O is the distri-
bution of rule probabilities, Na C N is the set of
adapted nonterminals, a E [0, 1], b &gt; 0 are vec-
tors of discount and concentration parameters both
indexed by adapted nonterminals, and a are Dirich-
let prior parameters.
</bodyText>
<listItem confidence="0.9535683125">
Algorithm 1 Generative Process
1: draw BA — Dir(αA) for all A E N
2: for each yield pair (s / t) do
3: SAMPLE(S) &gt; Sample from root
4: return
5: function SAMPLE(A) &gt; For A E N
6: if A E Na then
7: return SAMPLESAG(A)
8: else
9: return SAMPLESCFG(A)
10: function SAMPLESCFG(A) &gt; For A E/ Na
11: draw rule r = (o / γ) — Multi(BA)
12: tree tB +—SAMPLE(B) for nonterminal B E ouγ
13: return BUILDTREE(r, tB1, tB2, . . .)
14: function SAMPLESAG(A) &gt; For A E Na
15: draw cache index zn+1 — P(z|zi&lt;n), where
</listItem>
<equation confidence="0.97719">
�,a+b if zn+1 = m + 1
P(z|zi&lt;n) = n+b
n �a
n+b if zn+1 = k E l1,··· , m}
16: if zn+1 = m + 1 then &gt; New entry
</equation>
<listItem confidence="0.959442">
17: tree t +— SAMPLESCFG(A)
18: m +— m + 1; n,,,, = 1 &gt; Update counts
19: INSERTTOCACHE(CA, t).
20: else &gt; Old entry
21: nk +— nk + 1
22: tree t +— FINDINCACHE(CA, zn+1)
23: return t
</listItem>
<bodyText confidence="0.9839079">
The generative process of a synchronous tree set
T is described in Algorithm 1. First, rule probabil-
ities are sampled for each nonterminal A E N (line
1) according to the Dirichlet distribution. Then syn-
chronous trees are generated in the top-down fashion
from the start symbol S (line 3) for each yield pair.
For nonterminals that are not adapted, the grammar
expands it just as the original synchronous grammar
(function SAMPLESCFG). For each adapted non-
terminal A E Na, the grammar maintains a cache
CA to store previously generated subtrees under A.
Let zi be the subtree index in CA, denoting the syn-
chronous subtree generated at the ith expansion of
A. At some particular time, assuming n subtrees
rooted at A have been generated with m different
types in the cache of A, each of which has been gen-
erated for n1, ... , nm times respectively1. Then the
grammar either generates the (n + 1)th synchronous
subtree as SCFG (line 17) or chooses an existing
subtree (line 22), according to the conditional prob-
ability P(z|zi&lt;n).
The above generative process demonstrates “rich
get richer” dynamics, i.e. previous sampled subtrees
under adapted nonterminals would more likely be
sampled again in following procedures. This is suit-
able for many learning tasks since they prefer sparse
solutions to avoid the over-fitting problems. If we
integrate out the adaptors, the joint probability of a
particular sequence of indexes z with cached counts
(n1, ... , nm) under the Pitman-Yor process is
</bodyText>
<equation confidence="0.9998918">
Hmk=1(a(k — 1) + b) Hn�_1
j=1 (j — a)
Hn_1
i=0 (i + b)
(1)
</equation>
<bodyText confidence="0.987663714285714">
Given synchronous tree set T, the joint probability
under the PYSAG is
B(aA + fA) PY (z(T)  |a, b)
B(aA)
where fA is the vector containing the number of
times that rules r E RA are used in the T, and B
is the Beta function.
</bodyText>
<subsectionHeader confidence="0.839431">
2.2 Inference for PYSAGs
</subsectionHeader>
<bodyText confidence="0.9414085">
Directly drawing samples from Equation (2) is
intractable, so we extend the component-wise
Metropolis-Hastings algorithm (Johnson et al.,
2007) to the synchronous case. In detail, we
draw sample Ti′ from some proposal distribution
Q(Ti|yi, T_i)2, then accept the new sampled syn-
1Obviously, n = E&apos;1 nk.
2T−i means the set of sampled trees except the ith one.
</bodyText>
<equation confidence="0.9816954">
PY (z|a,b) =
.
fl
P(T |�, a, b) =
AEN
</equation>
<page confidence="0.830227">
535
</page>
<bodyText confidence="0.784896">
chronous tree Ti′ with probability
</bodyText>
<equation confidence="0.991394">
� 1, P (T ′|α, a, b)Q(Ti|yi, T−i) �
A(Ti, T ′ i) = min .
P(T |α, a, b)Q(Ti′|yi, T−i)
(3)
</equation>
<bodyText confidence="0.9881228">
In theory, Q could be any distribution if it never
assigns zero probability. For efficiency reason, we
choose the probabilistic SCFG as the proposal dis-
tribution. We pre-parse the training instances3 be-
fore inference and save the structure of synchronous
parsing forests. During the inference, we only
change rule probabilities in parsing forests without
changing the forest structures. The probability of
rule r E RA in Q is estimated by relative frequency
θr =[fr] −i , where RA is the set of rules
</bodyText>
<subsectionHeader confidence="0.377207">
Er′ E RA [fr′] −i
</subsectionHeader>
<bodyText confidence="0.999846222222222">
rooted at A, and [fr]−i is the number of times that
rule r is used in the tree set T−i. We use the sam-
pling algorithm described in (Blunsom and Osborne,
2008) to draw a synchronous tree from the parsing
forest according to the proposal Q.
Following (Johnson and Goldwater, 2009), we put
an uninformative Beta(1,1) prior on a and a “vague”
Gamma(10, 0.1) prior on b to model the uncertainty
of hyperparameters.
</bodyText>
<sectionHeader confidence="0.987784" genericHeader="method">
3 Machine Transliteration
</sectionHeader>
<subsectionHeader confidence="0.988761">
3.1 Grammars
</subsectionHeader>
<bodyText confidence="0.978658">
For machine transliteration, we design the following
grammar to learn syllable mappings4:
</bodyText>
<table confidence="0.8231508">
Name (Syl / Syl)+
Syl (NECs /NECs)
Syl (NECs SECs / NECs SECs)
Syl (NECs TECs / NECs TECs)
NECs (NEC / NEC)+
SECs (SEC / SEC)+
TECs (TEC / TEC)+
NEC (si / tj)
SEC (ε / tj)
TEC (si / ε)
</table>
<footnote confidence="0.9185902">
3We implement the CKY-like bottom up parsing algorithm
described in (Wu, 1997). The complexity is O(|8|3|t|3).
4Similar to (Johnson, 2008), the adapted nonterminal are un-
derlined. Similarly, we also use rules in the regular expression
style X --+ (A / A)+ to denote the following three rules:
</footnote>
<bodyText confidence="0.922117074074074">
X --+ (As / As)
As --+ (A / A)
As --+ (A As / A As)
where the adapted nonterminal Syl is designed to
capture the syllable equivalents between two lan-
guages, and the nonterminal NEC, SEC and TEC cap-
ture the character pairs with no empty character,
empty source and empty target respectively. Note
that this grammar restricts the leftmost characters on
both sides must be aligned one-by-one. Since our
goal is to learn the syllable equivalents, we are not
interested in the subtree tree inside the syllables. We
refer this grammar as syllable grammar.
The above grammar could capture inner-syllable
dependencies. However, the selection of the target
characters also depend on the context. For example,
the following three instances are found in the train-
ing set:
(a a b y e / A[ao] �Ubi])
(a a g a a r d / A[ai] 4[ge] &apos;( [de])
(a a l t o / Fill[a] i [er] +E[tuo])
where the same English syllable (a a) are translit-
erated to (A[ao]), (A[ai]) and (Fill[a]) respec-
tively, depending on the following syllables. To
model these contextual dependencies, we propose
the hierarchical SAG. The two-layer word grammar
is obtained by adding following rules:
</bodyText>
<table confidence="0.4809175">
Name (Word / Word)+
Word (Syl / Syl)+
</table>
<bodyText confidence="0.882658333333333">
We might further add a new adapted nonterminal
Col to learn the word collocations. The following
rules appear in the collocation grammar:
</bodyText>
<figure confidence="0.659151333333333">
Name (Col / Col)+
Col (Word / Word)+
Word (Syl / Syl)+
</figure>
<figureCaption confidence="0.737678333333333">
Figure 1 gives one synchronous parsing trees
under the collocation grammar of the example
(m a x / ð[mai] A[ke] WTT[si]).
</figureCaption>
<subsectionHeader confidence="0.979174">
3.2 Translation Model
</subsectionHeader>
<bodyText confidence="0.992545909090909">
After sampling, we need a translation model to
transliterate new source string to target string.
Following (Li et al., 2004), we use the n-gram
translation model to estimate the joint distribution
P(s, t) = IIKk=1 P(pk|pk−1
1 ), where pk is the kth
syllable pair of the string pair (s / t).
The first step is to construct joint segmentation
lattice for each training instance. We first generate a
merged grammar G′ using collected subtrees under
adapted nonterminals, then use synchronous parsing
</bodyText>
<page confidence="0.992695">
536
</page>
<bodyText confidence="0.631313">
probabilities of corresponding Syl node in the pars-
ing forest.
</bodyText>
<figureCaption confidence="0.996608">
Figure 2: Lattice example.
</figureCaption>
<figure confidence="0.994301521739131">
start
( a/C )
( aa/C )
a/C
( al/i1 )
aa/C
( l/-1 )
aal/CiJ
( lto/_T,,K )
( to/K )
aalto/Ci1K
Name
Cols
Col
Words
Word
Syls
Syls
Syl
Syl
NECs TECs NECs SECs
NEC TEC NEC SEC
m/P a/E x/5A E/AT
</figure>
<figureCaption confidence="0.99997">
Figure 1: An example of parse tree.
</figureCaption>
<bodyText confidence="0.93819625">
to obtain probabilities in the segmentation lattice.
Specifically, we “flatten” the collected subtrees un-
der Syl, i.e. removing internal nodes, to construct
new synchronous rules. For example, we could get
two rules from the tree in Figure 1:
Syl (m a / 2t)
Syl (x / AWT)
If multiple subtrees are flattened to the same syn-
chronous rule, we sum up the counts of these sub-
trees. For rules with non-adapted nonterminal as
parent, we assign the probability as the same of the
sampled rule probability, i.e. let θ′r = θr. For
the adapted nonterminal Syl, there are two kinds
of rules: (1) the rules in the original probabilistic
SCFG, and (2) the rules flattened from subtrees. We
assign the rule probability as
</bodyText>
<equation confidence="0.92749525">
ma+b - θr if r is original SCFG rule
θr = n+b (4)
n,−a if r is flatten from subtree
n+b
</equation>
<bodyText confidence="0.999814933333333">
where a and b are the parameters associated with
Syl, m is the number of types of different rules flat-
ten from subtrees, nr is the count of rule r, and n is
the total number of flatten rules. One may verify that
the rule probabilities are well normalized. Based
on this merged grammar G′, we parse the training
string pairs, then encode the parsed forest into the
lattice. Figure 2 show a lattice example for the string
pair (a a l t o / C[a] i [er] K[tuo]). The
transition probabilities in the lattice are the “inside”
After building the segmentation lattice, we train
3-order language model from the lattice using the
SRILM5. In decoding, given a new source string, we
use the Viterbi algorithm with beam search (Li et al.,
2004) to find the best transliteration candidate.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998306">
4.1 Data and Settings
</subsectionHeader>
<bodyText confidence="0.9987598">
We conduct experiments on the English-Chinese
data in the ACL Named Entities Workshop (NEWS
2009) 6. Table 1 gives some statistics of the data. For
evaluation, we report the word accuracy and mean
F-score metrics defined in (Li et al., 2009).
</bodyText>
<table confidence="0.9991964">
Train Dev Test
# Entry 31,961 2,896 2,896
# En Char 218,073 19,755 19,864
# Ch Char 101,205 9,160 9,246
# Ch Type 370 275 283
</table>
<tableCaption confidence="0.999901">
Table 1: Transliteration data statistics
</tableCaption>
<bodyText confidence="0.999896857142857">
In the inference step, we first run sampler through
the whole training corpus for 10 iterations, then col-
lect adapted subtree statistics for every 10 iterations,
and finally stop after 20 collections. After each it-
eration, we resample each of hyperparameters from
the posterior distribution of hyperparameters using a
slice sampler (Neal, 2003).
</bodyText>
<subsectionHeader confidence="0.581293">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.998075">
We implement the joint source-channel model (Li et
al., 2004) as the baseline system, in which the ortho-
graphic syllable alignment is automatically derived
by the Expectation-Maximization (EM) algorithm.
</bodyText>
<footnote confidence="0.9999635">
5http://www.speech.sri.com/projects/srilm/
6http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
</footnote>
<page confidence="0.995099">
537
</page>
<bodyText confidence="0.999527714285714">
Since EM tends to memorize the training instance
as a whole, Li et al. (2004) restrict the Chinese side
to be single character in syllable equivalents. Our
method can be viewed as the Bayesian extension of
the EM-based baseline. Since PYSAGs could learn
accurate and compact transliteration units, we do not
need the restriction any more.
</bodyText>
<table confidence="0.998852">
Grammar Dev (%) Test (%)
Baseline 67.8/86.9 66.6/85.7
Syl 66.6/87.0 66.6/86.6
Word 67.1/87.2 67.0/86.7
Col 67.2/87.1 66.9/86.7
</table>
<tableCaption confidence="0.868423">
Table 2: Transliteration results, in the format of word ac-
curacy / mean F-score. “Syl”,“Word” and “Col” denote
the syllable, word and collocation grammar respectively.
</tableCaption>
<bodyText confidence="0.8511625">
Table 2 presents the results of all experiments.
From this table, we draw following conclusions:
</bodyText>
<listItem confidence="0.859889142857143">
1. The best results of our model are 67.1%/87.2%
on development set and corresponding
67.0%/86.7% on test set, achieved by word
grammars. The results on test set outperform
the EM-based baseline system on both word
accuracy and mean F-score.
2. Comparing grammars of different layers, we
</listItem>
<bodyText confidence="0.923566272727273">
find that the word grammars perform consis-
tently better than the syllable grammars. These
support the assumption that the context infor-
mation are helpful to identify syllable equiva-
lents. However, the collocation grammars do
not further improve performance. We guess
the reason is that the instances in transliter-
ation are very short, so two-layer grammars
are good enough while the collocations become
very sparse, which results in unreliable proba-
bility estimation.
</bodyText>
<subsectionHeader confidence="0.991734">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99976425">
Table 3 shows some examples of learned syllable
mappings in the final sampled tree of the syllable
grammar. We can see that the PYSAGs could find
good syllable mappings from the raw name pairs
without any heuristic or restriction. In this point of
view, the proposed method is language independent.
Specifically, we are interested in the English to-
ken “x”, which is the only one that has two corre-
</bodyText>
<table confidence="0.999019666666667">
s/A[si]/1669 k/A[ke]/408 ri/A[li]/342
t/ [te]/728 ma/ê[ma]/390 ra/4A1[la]/339
man/ù[man]/703 co/#[ke]/387 ca/-f�[ka]/333
d/*,,[de]/579 ll/i�j.[er]/383 m/ [mu]/323
ck/A[ke]/564 la/4A,[la]/382 li/f q[li]/314
de/*,,[de]/564 tt/ [te]/380 ber/fO[bo]/311
ro/-W[luo]/531 l/�[er]/367 ley/f q[li]/310
son/Ü[sen]/442 ton/î[dun]/360 na/M[na]/302
x/AA[ke si]/40 x/A[ke]/3 x/A[si]/1
</table>
<tableCaption confidence="0.998600333333333">
Table 3: Examples of learned syllable mappings. Chinese
Pinyin are given in the square bracket. The counts of syl-
lable mappings in the final sampled tree are also given.
</tableCaption>
<bodyText confidence="0.999946142857143">
sponding Chinese characters (“AA[ke si]”). Ta-
ble 3 demonstrates that nearly all these correct map-
pings are discovered by PYSAGs. Note that these
kinds of mapping can not be learned if we restrict the
Chinese side to be only one character (the heuristic
used in (Li et al., 2004)). We will conduct experi-
ments on other language pairs in the future.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999728">
This paper proposes synchronous adaptor gram-
mars, a nonparametric Bayesian model, for machine
transliteration. Based on the sampling, the PYSAGs
could automatically discover syllable equivalents
without any heuristic or restriction. In this point
of view, the proposed model is language indepen-
dent. The joint source-channel model is then used
for training and decoding. Experimental results on
the English-Chinese transliteration task show that
the proposed method outperforms the strong EM-
based baseline system. We also compare grammars
in different layers and find that the two-layer gram-
mars are suitable for the transliteration task. We
plan to carry out more transliteration experiments on
other language pairs in the future.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993555">
We would like to thank the anonymous reviewers for
their helpful comments and suggestions. We also
thank Zhixiang Ren, Zhenghua Li, and Jun Sun for
insightful discussions. Special thanks to Professor
Mark Johnson for his open-source codes7.
</bodyText>
<footnote confidence="0.989613">
7Available from http://web.science.mq.edu.
au/~mjohnson/Software.htm
</footnote>
<page confidence="0.992858">
538
</page>
<sectionHeader confidence="0.968947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998320737864078">
Ahmed Abdul Hamid and Kareem Darwish. 2010. Sim-
plified feature set for arabic named entity recognition.
In Proceedings of the 2010 Named Entities Workshop,
pages 110–115, Uppsala, Sweden, July.
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 215–223, Honolulu,
Hawaii, October.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 782–790, Sun-
tec, Singapore, August.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228,
June.
Andrew Finch and Eiichiro Sumita. 2009. Transliter-
ation by bidirectional statistical machine translation.
In Proceedings of the 2009 Named Entities Workshop:
Shared Task on Transliteration (NEWS 2009), pages
52–56, Suntec, Singapore, August.
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Transliteration.
In Proceedings of the 7th International Workshop on
Spoken Language Translation (IWSLT), pages 259–
266, Paris, France, December.
Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 284–
292, Cambridge, MA, October.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317–325, Boulder, Colorado, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for spec-
ifying compositional nonparametric bayesian models.
In B. Schölkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641–648. Cambridge, MA.
Mark Johnson. 2008. Using adaptor grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of ACL-08: HLT,
pages 398–406, Columbus, Ohio, June.
Mark Johnson. 2010. Pcfgs, topic models, adaptor gram-
mars and learning topical collocations and the struc-
ture of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148–1157, Uppsala, Sweden, July.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL’04), Main Vol-
ume, pages 159–166, Barcelona, Spain, July.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min
Zhang. 2009. Report of news 2009 machine transliter-
ation shared task. In Proceedings of the 2009 Named
Entities Workshop: Shared Task on Transliteration
(NEWS 2009), pages 1–18, Suntec, Singapore, August.
Ding Liu and Daniel Gildea. 2009. Bayesian learning
of phrasal tree-to-string templates. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1308–1317, Singapore,
August.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31(3):705–767.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855–900.
Taraka Rama and Karthik Gali. 2009. Modeling ma-
chine transliteration as a phrase based statistical ma-
chine translation problem. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009), pages 124–127, Suntec, Singa-
pore, August.
Sravana Reddy and Sonjia Waxmonsky. 2009.
Substring-based transliteration with conditional ran-
dom fields. In Proceedings of the 2009 Named Enti-
ties Workshop: Shared Task on Transliteration (NEWS
2009), pages 92–95, Suntec, Singapore, August.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403, Septem-
ber.
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-
ishi, Masanobu Nakamura, and Sadaoki Furui. 2009.
Combining a two-step conditional random field model
and a joint source channel model for machine translit-
eration. In Proceedings of the 2009 Named Enti-
ties Workshop: Shared Task on Transliteration (NEWS
2009), pages 72–75, Suntec, Singapore, August.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings ofACL-08: HLT, pages 97–105, Colum-
bus, Ohio, June.
</reference>
<page confidence="0.99863">
539
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.560581">
<title confidence="0.999195">Nonparametric Bayesian Machine Transliteration with Adaptor Grammars</title>
<author confidence="0.991683">Min Chew Lim</author>
<email confidence="0.674338">huangyun@comp.nus.edu.sgmzhang@i2r.a-star.edu.sgtancl@comp.nus.edu.sg</email>
<affiliation confidence="0.9918985">Language Department of Computer Science Institute for Infocomm Research National University of Singapore</affiliation>
<address confidence="0.880799">1 Fusionopolis Way, Singapore 13 Computing Drive, Singapore</address>
<abstract confidence="0.996296384615385">Machine transliteration is defined as automatic phonetic translation of names across languages. In this paper, we propose synchronous adaptor grammar, a novel nonparametric Bayesian learning approach, for machine transliteration. This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abdul Hamid</author>
<author>Kareem Darwish</author>
</authors>
<title>Simplified feature set for arabic named entity recognition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Named Entities Workshop,</booktitle>
<pages>110--115</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1446" citStr="Hamid and Darwish, 2010" startWordPosition="199" endWordPosition="202"> English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-man</context>
</contexts>
<marker>Hamid, Darwish, 2010</marker>
<rawString>Ahmed Abdul Hamid and Kareem Darwish. 2010. Simplified feature set for arabic named entity recognition. In Proceedings of the 2010 Named Entities Workshop, pages 110–115, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Probabilistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>215--223</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="8018" citStr="Blunsom and Osborne, 2008" startWordPosition="1339" endWordPosition="1342">assigns zero probability. For efficiency reason, we choose the probabilistic SCFG as the proposal distribution. We pre-parse the training instances3 before inference and save the structure of synchronous parsing forests. During the inference, we only change rule probabilities in parsing forests without changing the forest structures. The probability of rule r E RA in Q is estimated by relative frequency θr =[fr] −i , where RA is the set of rules Er′ E RA [fr′] −i rooted at A, and [fr]−i is the number of times that rule r is used in the tree set T−i. We use the sampling algorithm described in (Blunsom and Osborne, 2008) to draw a synchronous tree from the parsing forest according to the proposal Q. Following (Johnson and Goldwater, 2009), we put an uninformative Beta(1,1) prior on a and a “vague” Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters. 3 Machine Transliteration 3.1 Grammars For machine transliteration, we design the following grammar to learn syllable mappings4: Name (Syl / Syl)+ Syl (NECs /NECs) Syl (NECs SECs / NECs SECs) Syl (NECs TECs / NECs TECs) NECs (NEC / NEC)+ SECs (SEC / SEC)+ TECs (TEC / TEC)+ NEC (si / tj) SEC (ε / tj) TEC (si / ε) 3We implement the CKY-like bottom </context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>Phil Blunsom and Miles Osborne. 2008. Probabilistic inference for machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215–223, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>782--790</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2346" citStr="Blunsom et al., 2009" startWordPosition="334" endWordPosition="337"> whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and G</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 782–790, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3859" citStr="Chiang, 2007" startWordPosition="564" endWordPosition="565">the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Synchronous Adaptor Grammars 2.1 Model A Pitman-Yor Synchronous Adaptor Grammar (PYSAG) is a tuple !9 = (!9s, Na, a, b, a), where !9s = (N, T, T, R, S, O) is a Synchronous Context-Free Grammar (SCFG) (Chiang, 2007), N is a set of nonterminal symbols, 7/7 are source/target terminal symbols, R is a set of rewrite rules, S E N is the start symbol, O is the distribution of rule probabilities, Na C N is the set of adapted nonterminals, a E [0, 1], b &gt; 0 are vectors of discount and concentration parameters both indexed by adapted nonterminals, and a are Dirichlet prior parameters. Algorithm 1 Generative Process 1: draw BA — Dir(αA) for all A E N 2: for each yield pair (s / t) do 3: SAMPLE(S) &gt; Sample from root 4: return 5: function SAMPLE(A) &gt; For A E N 6: if A E Na then 7: return SAMPLESAG(A) 8: else 9: retu</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Transliteration by bidirectional statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>52--56</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1255" citStr="Finch and Sumita, 2009" startWordPosition="167" endWordPosition="170"> a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligne</context>
</contexts>
<marker>Finch, Sumita, 2009</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2009. Transliteration by bidirectional statistical machine translation. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 52–56, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>A Bayesian Model of Bilingual Segmentation for Transliteration.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>259--266</pages>
<location>Paris, France,</location>
<contexts>
<context position="1280" citStr="Finch and Sumita, 2010" startWordPosition="171" endWordPosition="174">hout heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of t</context>
</contexts>
<marker>Finch, Sumita, 2010</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2010. A Bayesian Model of Bilingual Segmentation for Transliteration. In Proceedings of the 7th International Workshop on Spoken Language Translation (IWSLT), pages 259– 266, Paris, France, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Hardisty</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Modeling perspective using adaptor grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>284--292</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="2888" citStr="Hardisty et al., 2010" startWordPosition="417" endWordPosition="420"> learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the </context>
</contexts>
<marker>Hardisty, Boyd-Graber, Resnik, 2010</marker>
<rawString>Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik. 2010. Modeling perspective using adaptor grammars. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284– 292, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="2960" citStr="Johnson and Goldwater, 2009" startWordPosition="427" endWordPosition="430">et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539, Po</context>
<context position="8138" citStr="Johnson and Goldwater, 2009" startWordPosition="1358" endWordPosition="1361">-parse the training instances3 before inference and save the structure of synchronous parsing forests. During the inference, we only change rule probabilities in parsing forests without changing the forest structures. The probability of rule r E RA in Q is estimated by relative frequency θr =[fr] −i , where RA is the set of rules Er′ E RA [fr′] −i rooted at A, and [fr]−i is the number of times that rule r is used in the tree set T−i. We use the sampling algorithm described in (Blunsom and Osborne, 2008) to draw a synchronous tree from the parsing forest according to the proposal Q. Following (Johnson and Goldwater, 2009), we put an uninformative Beta(1,1) prior on a and a “vague” Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters. 3 Machine Transliteration 3.1 Grammars For machine transliteration, we design the following grammar to learn syllable mappings4: Name (Syl / Syl)+ Syl (NECs /NECs) Syl (NECs SECs / NECs SECs) Syl (NECs TECs / NECs TECs) NECs (NEC / NEC)+ SECs (SEC / SEC)+ TECs (TEC / TEC)+ NEC (si / tj) SEC (ε / tj) TEC (si / ε) 3We implement the CKY-like bottom up parsing algorithm described in (Wu, 1997). The complexity is O(|8|3|t|3). 4Similar to (Johnson, 2008), the adapted no</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 317–325, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Schölkopf, J. Platt, and T. Hoffman, editors,</editor>
<location>Cambridge, MA.</location>
<contexts>
<context position="2491" citStr="Johnson et al., 2007" startWordPosition="355" endWordPosition="358">cters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based o</context>
<context position="6960" citStr="Johnson et al., 2007" startWordPosition="1142" endWordPosition="1145"> problems. If we integrate out the adaptors, the joint probability of a particular sequence of indexes z with cached counts (n1, ... , nm) under the Pitman-Yor process is Hmk=1(a(k — 1) + b) Hn�_1 j=1 (j — a) Hn_1 i=0 (i + b) (1) Given synchronous tree set T, the joint probability under the PYSAG is B(aA + fA) PY (z(T) |a, b) B(aA) where fA is the vector containing the number of times that rules r E RA are used in the T, and B is the Beta function. 2.2 Inference for PYSAGs Directly drawing samples from Equation (2) is intractable, so we extend the component-wise Metropolis-Hastings algorithm (Johnson et al., 2007) to the synchronous case. In detail, we draw sample Ti′ from some proposal distribution Q(Ti|yi, T_i)2, then accept the new sampled syn1Obviously, n = E&apos;1 nk. 2T−i means the set of sampled trees except the ith one. PY (z|a,b) = . fl P(T |�, a, b) = AEN 535 chronous tree Ti′ with probability � 1, P (T ′|α, a, b)Q(Ti|yi, T−i) � A(Ti, T ′ i) = min . P(T |α, a, b)Q(Ti′|yi, T−i) (3) In theory, Q could be any distribution if it never assigns zero probability. For efficiency reason, we choose the probabilistic SCFG as the proposal distribution. We pre-parse the training instances3 before inference an</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor grammars: A framework for specifying compositional nonparametric bayesian models. In B. Schölkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>398--406</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2976" citStr="Johnson, 2008" startWordPosition="431" endWordPosition="432">dels, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539, Portland, Oregon, </context>
<context position="8722" citStr="Johnson, 2008" startWordPosition="1462" endWordPosition="1463"> (Johnson and Goldwater, 2009), we put an uninformative Beta(1,1) prior on a and a “vague” Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters. 3 Machine Transliteration 3.1 Grammars For machine transliteration, we design the following grammar to learn syllable mappings4: Name (Syl / Syl)+ Syl (NECs /NECs) Syl (NECs SECs / NECs SECs) Syl (NECs TECs / NECs TECs) NECs (NEC / NEC)+ SECs (SEC / SEC)+ TECs (TEC / TEC)+ NEC (si / tj) SEC (ε / tj) TEC (si / ε) 3We implement the CKY-like bottom up parsing algorithm described in (Wu, 1997). The complexity is O(|8|3|t|3). 4Similar to (Johnson, 2008), the adapted nonterminal are underlined. Similarly, we also use rules in the regular expression style X --+ (A / A)+ to denote the following three rules: X --+ (As / As) As --+ (A / A) As --+ (A As / A As) where the adapted nonterminal Syl is designed to capture the syllable equivalents between two languages, and the nonterminal NEC, SEC and TEC capture the character pairs with no empty character, empty source and empty target respectively. Note that this grammar restricts the leftmost characters on both sides must be aligned one-by-one. Since our goal is to learn the syllable equivalents, we</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Proceedings of ACL-08: HLT, pages 398–406, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1148--1157</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2842" citStr="Johnson, 2010" startWordPosition="413" endWordPosition="414">n successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 </context>
</contexts>
<marker>Johnson, 2010</marker>
<rawString>Mark Johnson. 2010. Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148–1157, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>159--166</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1330" citStr="Li et al., 2004" startWordPosition="179" endWordPosition="182">ble equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yan</context>
<context position="10520" citStr="Li et al., 2004" startWordPosition="1774" endWordPosition="1777">ual dependencies, we propose the hierarchical SAG. The two-layer word grammar is obtained by adding following rules: Name (Word / Word)+ Word (Syl / Syl)+ We might further add a new adapted nonterminal Col to learn the word collocations. The following rules appear in the collocation grammar: Name (Col / Col)+ Col (Word / Word)+ Word (Syl / Syl)+ Figure 1 gives one synchronous parsing trees under the collocation grammar of the example (m a x / ð[mai] A[ke] WTT[si]). 3.2 Translation Model After sampling, we need a translation model to transliterate new source string to target string. Following (Li et al., 2004), we use the n-gram translation model to estimate the joint distribution P(s, t) = IIKk=1 P(pk|pk−1 1 ), where pk is the kth syllable pair of the string pair (s / t). The first step is to construct joint segmentation lattice for each training instance. We first generate a merged grammar G′ using collected subtrees under adapted nonterminals, then use synchronous parsing 536 probabilities of corresponding Syl node in the parsing forest. Figure 2: Lattice example. start ( a/C ) ( aa/C ) a/C ( al/i1 ) aa/C ( l/-1 ) aal/CiJ ( lto/_T,,K ) ( to/K ) aalto/Ci1K Name Cols Col Words Word Syls Syls Syl S</context>
<context position="12739" citStr="Li et al., 2004" startWordPosition="2179" endWordPosition="2182">rees, nr is the count of rule r, and n is the total number of flatten rules. One may verify that the rule probabilities are well normalized. Based on this merged grammar G′, we parse the training string pairs, then encode the parsed forest into the lattice. Figure 2 show a lattice example for the string pair (a a l t o / C[a] i [er] K[tuo]). The transition probabilities in the lattice are the “inside” After building the segmentation lattice, we train 3-order language model from the lattice using the SRILM5. In decoding, given a new source string, we use the Viterbi algorithm with beam search (Li et al., 2004) to find the best transliteration candidate. 4 Experiments 4.1 Data and Settings We conduct experiments on the English-Chinese data in the ACL Named Entities Workshop (NEWS 2009) 6. Table 1 gives some statistics of the data. For evaluation, we report the word accuracy and mean F-score metrics defined in (Li et al., 2009). Train Dev Test # Entry 31,961 2,896 2,896 # En Char 218,073 19,755 19,864 # Ch Char 101,205 9,160 9,246 # Ch Type 370 275 283 Table 1: Transliteration data statistics In the inference step, we first run sampler through the whole training corpus for 10 iterations, then collect</context>
<context position="13968" citStr="Li et al. (2004)" startWordPosition="2365" endWordPosition="2368">ree statistics for every 10 iterations, and finally stop after 20 collections. After each iteration, we resample each of hyperparameters from the posterior distribution of hyperparameters using a slice sampler (Neal, 2003). 4.2 Results We implement the joint source-channel model (Li et al., 2004) as the baseline system, in which the orthographic syllable alignment is automatically derived by the Expectation-Maximization (EM) algorithm. 5http://www.speech.sri.com/projects/srilm/ 6http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/ 537 Since EM tends to memorize the training instance as a whole, Li et al. (2004) restrict the Chinese side to be single character in syllable equivalents. Our method can be viewed as the Bayesian extension of the EM-based baseline. Since PYSAGs could learn accurate and compact transliteration units, we do not need the restriction any more. Grammar Dev (%) Test (%) Baseline 67.8/86.9 66.6/85.7 Syl 66.6/87.0 66.6/86.6 Word 67.1/87.2 67.0/86.7 Col 67.2/87.1 66.9/86.7 Table 2: Transliteration results, in the format of word accuracy / mean F-score. “Syl”,“Word” and “Col” denote the syllable, word and collocation grammar respectively. Table 2 presents the results of all experim</context>
<context position="16608" citStr="Li et al., 2004" startWordPosition="2762" endWordPosition="2765">314 de/*,,[de]/564 tt/ [te]/380 ber/fO[bo]/311 ro/-W[luo]/531 l/�[er]/367 ley/f q[li]/310 son/Ü[sen]/442 ton/î[dun]/360 na/M[na]/302 x/AA[ke si]/40 x/A[ke]/3 x/A[si]/1 Table 3: Examples of learned syllable mappings. Chinese Pinyin are given in the square bracket. The counts of syllable mappings in the final sampled tree are also given. sponding Chinese characters (“AA[ke si]”). Table 3 demonstrates that nearly all these correct mappings are discovered by PYSAGs. Note that these kinds of mapping can not be learned if we restrict the Chinese side to be only one character (the heuristic used in (Li et al., 2004)). We will conduct experiments on other language pairs in the future. 5 Conclusion This paper proposes synchronous adaptor grammars, a nonparametric Bayesian model, for machine transliteration. Based on the sampling, the PYSAGs could automatically discover syllable equivalents without any heuristic or restriction. In this point of view, the proposed model is language independent. The joint source-channel model is then used for training and decoding. Experimental results on the English-Chinese transliteration task show that the proposed method outperforms the strong EMbased baseline system. We </context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 159–166, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<title>Report of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>1--18</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="13061" citStr="Li et al., 2009" startWordPosition="2232" endWordPosition="2235"> C[a] i [er] K[tuo]). The transition probabilities in the lattice are the “inside” After building the segmentation lattice, we train 3-order language model from the lattice using the SRILM5. In decoding, given a new source string, we use the Viterbi algorithm with beam search (Li et al., 2004) to find the best transliteration candidate. 4 Experiments 4.1 Data and Settings We conduct experiments on the English-Chinese data in the ACL Named Entities Workshop (NEWS 2009) 6. Table 1 gives some statistics of the data. For evaluation, we report the word accuracy and mean F-score metrics defined in (Li et al., 2009). Train Dev Test # Entry 31,961 2,896 2,896 # En Char 218,073 19,755 19,864 # Ch Char 101,205 9,160 9,246 # Ch Type 370 275 283 Table 1: Transliteration data statistics In the inference step, we first run sampler through the whole training corpus for 10 iterations, then collect adapted subtree statistics for every 10 iterations, and finally stop after 20 collections. After each iteration, we resample each of hyperparameters from the posterior distribution of hyperparameters using a slice sampler (Neal, 2003). 4.2 Results We implement the joint source-channel model (Li et al., 2004) as the base</context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min Zhang. 2009. Report of news 2009 machine transliteration shared task. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 1–18, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of phrasal tree-to-string templates.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1308--1317</pages>
<location>Singapore,</location>
<contexts>
<context position="2303" citStr="Liu and Gildea, 2009" startWordPosition="326" endWordPosition="329">er-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology an</context>
</contexts>
<marker>Liu, Gildea, 2009</marker>
<rawString>Ding Liu and Daniel Gildea. 2009. Bayesian learning of phrasal tree-to-string templates. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1308–1317, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="13574" citStr="Neal, 2003" startWordPosition="2318" endWordPosition="2319"> For evaluation, we report the word accuracy and mean F-score metrics defined in (Li et al., 2009). Train Dev Test # Entry 31,961 2,896 2,896 # En Char 218,073 19,755 19,864 # Ch Char 101,205 9,160 9,246 # Ch Type 370 275 283 Table 1: Transliteration data statistics In the inference step, we first run sampler through the whole training corpus for 10 iterations, then collect adapted subtree statistics for every 10 iterations, and finally stop after 20 collections. After each iteration, we resample each of hyperparameters from the posterior distribution of hyperparameters using a slice sampler (Neal, 2003). 4.2 Results We implement the joint source-channel model (Li et al., 2004) as the baseline system, in which the orthographic syllable alignment is automatically derived by the Expectation-Maximization (EM) algorithm. 5http://www.speech.sri.com/projects/srilm/ 6http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/ 537 Since EM tends to memorize the training instance as a whole, Li et al. (2004) restrict the Chinese side to be single character in syllable equivalents. Our method can be viewed as the Bayesian extension of the EM-based baseline. Since PYSAGs could learn accurate and compact translit</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice sampling. Annals of Statistics, 31(3):705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability,</title>
<date>1997</date>
<pages>25--855</pages>
<contexts>
<context position="3138" citStr="Pitman and Yor, 1997" startWordPosition="455" endWordPosition="458">l stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008). In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997). We also describe how transliteration could be modelled under this formalism. It should be emphasized that the proposed method is language independent and heuristic-free. Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task. 534 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Synchronous Adaptor Grammars 2.1 Model A Pitman-Yor Synchronous Adaptor Grammar (PYSAG) is a </context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability, 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taraka Rama</author>
<author>Karthik Gali</author>
</authors>
<title>Modeling machine transliteration as a phrase based statistical machine translation problem.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>124--127</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1231" citStr="Rama and Gali, 2009" startWordPosition="163" endWordPosition="166">. This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one </context>
</contexts>
<marker>Rama, Gali, 2009</marker>
<rawString>Taraka Rama and Karthik Gali. 2009. Modeling machine transliteration as a phrase based statistical machine translation problem. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 124–127, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sravana Reddy</author>
<author>Sonjia Waxmonsky</author>
</authors>
<title>Substring-based transliteration with conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>92--95</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1414" citStr="Reddy and Waxmonsky, 2009" startWordPosition="193" endWordPosition="196">e-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by comb</context>
</contexts>
<marker>Reddy, Waxmonsky, 2009</marker>
<rawString>Sravana Reddy and Sonjia Waxmonsky. 2009. Substring-based transliteration with conditional random fields. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 92–95, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="8662" citStr="Wu, 1997" startWordPosition="1454" endWordPosition="1455">e parsing forest according to the proposal Q. Following (Johnson and Goldwater, 2009), we put an uninformative Beta(1,1) prior on a and a “vague” Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters. 3 Machine Transliteration 3.1 Grammars For machine transliteration, we design the following grammar to learn syllable mappings4: Name (Syl / Syl)+ Syl (NECs /NECs) Syl (NECs SECs / NECs SECs) Syl (NECs TECs / NECs TECs) NECs (NEC / NEC)+ SECs (SEC / SEC)+ TECs (TEC / TEC)+ NEC (si / tj) SEC (ε / tj) TEC (si / ε) 3We implement the CKY-like bottom up parsing algorithm described in (Wu, 1997). The complexity is O(|8|3|t|3). 4Similar to (Johnson, 2008), the adapted nonterminal are underlined. Similarly, we also use rules in the regular expression style X --+ (A / A)+ to denote the following three rules: X --+ (As / As) As --+ (A / A) As --+ (A As / A As) where the adapted nonterminal Syl is designed to capture the syllable equivalents between two languages, and the nonterminal NEC, SEC and TEC capture the character pairs with no empty character, empty source and empty target respectively. Note that this grammar restricts the leftmost characters on both sides must be aligned one-by-</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Yang</author>
<author>Paul Dixon</author>
<author>Yi-Cheng Pan</author>
<author>Tasuku Oonishi</author>
<author>Masanobu Nakamura</author>
<author>Sadaoki Furui</author>
</authors>
<title>Combining a two-step conditional random field model and a joint source channel model for machine transliteration.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>72--75</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1350" citStr="Yang et al., 2009" startWordPosition="183" endWordPosition="186">etween languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task. 1 Introduction Proper names are one source of OOV words in many NLP tasks, such as machine translation and crosslingual information retrieval. They are often translated through transliteration, i.e. translation by preserving how words sound in both languages. In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010). Syllable equivalents acquisition is a critical phase for all these models. Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm. However, the EM algorithm may over-fit the training data by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heu</context>
</contexts>
<marker>Yang, Dixon, Pan, Oonishi, Nakamura, Furui, 2009</marker>
<rawString>Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oonishi, Masanobu Nakamura, and Sadaoki Furui. 2009. Combining a two-step conditional random field model and a joint source channel model for machine transliteration. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 72–75, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2323" citStr="Zhang et al., 2008" startWordPosition="330" endWordPosition="333">ta by memorizing the whole training instances. To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa (Li et al., 2004; Yang et al., 2009). Heuristics are introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009). Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures. They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009). Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007). They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history. Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them. AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segm</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proceedings ofACL-08: HLT, pages 97–105, Columbus, Ohio, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>