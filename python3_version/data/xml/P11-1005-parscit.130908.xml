<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035278">
<title confidence="0.992943">
Evaluating the Impact of Coder Errors on Active Learning
</title>
<author confidence="0.965545">
Ines Rehbein Josef Ruppenhofer
</author>
<affiliation confidence="0.98193">
Computational Linguistics Computational Linguistics
Saarland University Saarland University
</affiliation>
<email confidence="0.996278">
rehbein@coli.uni-sb.de josefr@coli.uni-sb.de
</email>
<sectionHeader confidence="0.995582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992546875">
Active Learning (AL) has been proposed as a
technique to reduce the amount of annotated
data needed in the context of supervised clas-
sification. While various simulation studies
for a number of NLP tasks have shown that
AL works well on goldstandard data, there is
some doubt whether the approach can be suc-
cessful when applied to noisy, real-world data
sets. This paper presents a thorough evalua-
tion of the impact of annotation noise on AL
and shows that systematic noise resulting from
biased coder decisions can seriously harm the
AL process. We present a method to filter out
inconsistent annotations during AL and show
that this makes AL far more robust when ap-
plied to noisy data.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99958594">
Supervised machine learning techniques are still the
mainstay for many NLP tasks. There is, how-
ever, a well-known bottleneck for these approaches:
the amount of high-quality data needed for train-
ing, mostly obtained by human annotation. Active
Learning (AL) has been proposed as a promising ap-
proach to reduce the amount of time and cost for hu-
man annotation. The idea behind active learning is
quite intuitive: instead of annotating a large number
of randomly picked instances we carefully select a
small number of instances that are maximally infor-
mative for the machine learning classifier. Thus a
smaller set of data points is able to boost classifier
performance and to yield an accuracy comparable to
the one obtained when training the same system on
a larger set of randomly chosen data. 43
Active learning has been applied to several NLP
tasks like part-of-speech tagging (Ringger et al.,
2007), chunking (Ngai and Yarowsky, 2000), syn-
tactic parsing (Osborne and Baldridge, 2004; Hwa,
2004), Named Entity Recognition (Shen et al.,
2004; Laws and Sch¨utze, 2008; Tomanek and Hahn,
2009), Word Sense Disambiguation (Chen et al.,
2006; Zhu and Hovy, 2007; Chan and Ng, 2007),
text classification (Tong and Koller, 1998) or statis-
tical machine translation (Haffari and Sarkar, 2009),
and has been shown to reduce the amount of anno-
tated data needed to achieve a certain classifier per-
formance, sometimes by as much as half. Most of
these studies, however, have only simulated the ac-
tive learning process using goldstandard data. This
setting is crucially different from a real world sce-
nario where we have to deal with erroneous data
and inconsistent annotation decisions made by the
human annotators. While simulations are an indis-
pensable instrument to test different parameters and
settings, it has been shown that when applying AL
to highly ambiguous tasks like e.g. Word Sense
Disambiguation (WSD) with fine-grained sense dis-
tinctions, AL can actually harm the learning process
(Dang, 2004; Rehbein et al., 2010). Dang suggests
that the lack of a positive effect of AL might be due
to inconsistencies in the human annotations and that
AL cannot efficiently be applied to tasks which need
double blind annotation with adjudication to insure
a sufficient data quality. Even if we take a more opti-
mistic view and assume that AL might still be useful
even for tasks featuring a high degree of ambiguity,
it remains crucial to address the problem of annota-
tion noise and its impact on AL.
</bodyText>
<note confidence="0.9272165">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43–51,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.998864533333334">
In this paper we present a thorough evaluation of
the impact of annotation noise on AL. We simulate
different types of coder errors and assess the effect
on the learning process. We propose a method to de-
tect inconsistencies and remove them from the train-
ing data, and show that our method does alleviate the
problem of annotation noise in our experiments.
The paper is structured as follows. Section 2 re-
ports on recent research on the impact of annota-
tion noise in the context of supervised classification.
Section 3 describes the experimental setup of our
simulation study and presents results. In Section 4
we present our filtering approach and show its im-
pact on AL performance. Section 5 concludes and
outlines future work.
</bodyText>
<sectionHeader confidence="0.999783" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955214285714">
We are interested in the question whether or not AL
can be successfully applied to a supervised classifi-
cation task where we have to deal with a consider-
able amount of inconsistencies and noise in the data,
which is the case for many NLP tasks (e.g. sen-
timent analysis, the detection of metaphors, WSD
with fine-grained word senses, to name but a few).
Therefore we do not consider part-of-speech tag-
ging or syntactic parsing, where coders are expected
to agree on most annotation decisions. Instead,
we focus on work on AL for WSD, where inter-
coder agreement (at least for fine-grained annotation
schemes) usually is much lower than for the former
tasks.
</bodyText>
<subsectionHeader confidence="0.997919">
2.1 Annotation Noise
</subsectionHeader>
<bodyText confidence="0.999970857142857">
Studies on active learning for WSD have been lim-
ited to running simulations of AL using gold stan-
dard data and a coarse-grained annotation scheme
(Chen et al., 2006; Chan and Ng, 2007; Zhu and
Hovy, 2007). Two exceptions are Dang (2004) and
Rehbein et al. (2010) who both were not able to
replicate the positive findings obtained for AL for
WSD on coarse-grained sense distinctions. A pos-
sible reason for this failure is the amount of annota-
tion noise in the training data which might mislead
the classifier during the AL process. Recent work on
the impact of annotation noise on a machine learning
task (Reidsma and Carletta, 2008) has shown that
random noise can be tolerated in supervised learn-
ing, while systematic errors (as caused by biased an-
notators) can seriously impair the performance of a
supervised classifier even if the observed accuracy
of the classifier on a test set coming from the same
population as the training data is as high as 0.8.
Related work (Beigman Klebanov et al., 2008;
Beigman Klebanov and Beigman, 2009) has been
studying annotation noise in a multi-annotator set-
ting, distinguishing between hard cases (unreliably
annotated due to genuine ambiguity) and easy cases
(reliably annotated data). The authors argue that
even for those data points where the annotators
agreed on one particular class, a proportion of the
agreement might be merely due to chance. Fol-
lowing this assumption, the authors propose a mea-
sure to estimate the amount of annotation noise in
the data after removing all hard cases. Klebanov
et al. (2008; 2009) show that, according to their
model, high inter-annotator agreement (r.) achieved
in an annotation scenario with two annotators is no
guarantee for a high-quality data set. Their model,
however, assumes that a) all instances where anno-
tators disagreed are in fact hard cases, and b) that for
the hard cases the annotators decisions are obtained
by coin-flips. In our experience, some amount of
disagreement can also be observed for easy cases,
caused by attention slips or by a deviant interpre-
tation of some class(es) by one of the annotators,
and the annotation decision of an individual annota-
tor cannot so much be described as random choice
(coin-flip) but as systematically biased selection,
causing the types of errors which have been shown
to be problematic for supervised classification (Rei-
dsma and Carletta, 2008).
Further problems arise in the AL scenario where
the instances to be annotated are selected as a func-
tion of the sampling method and the annotation
judgements made before. Therefore, Beigman and
Klebanov Beigman (2009)’s approach of identify-
ing unreliably annotated instances by disagreement
is not applicable to AL, as most instances are anno-
tated only once.
</bodyText>
<subsectionHeader confidence="0.999954">
2.2 Annotation Noise and Active Learning
</subsectionHeader>
<bodyText confidence="0.99959">
For AL to be succesful, we need to remove system-
atic noise in the training data. The challenge we face
is that we only have a small set of seed data and no
information about the reliability of the annotations
</bodyText>
<page confidence="0.998422">
44
</page>
<bodyText confidence="0.999366444444445">
assigned by the human coders.
Zhu et al. (2008) present a method for detecting
outliers in the pool of unannotated data to prevent
these instances from becoming part of the training
data. This approach is different from ours, where
we focus on detecting annotation noise in the man-
ually labelled training data produced by the human
coders.
Schein and Ungar (2007) provide a systematic in-
vestigation of 8 different sampling methods for AL
and their ability to handle different types of noise
in the data. The types of noise investigated are a)
prediction residual error (the portion of squared er-
ror that is independent of training set size), and b)
different levels of confusion among the categories.
Type a) models the presence of unknown features
that influence the true probabilities of an outcome: a
form of noise that will increase residual error. Type
b) models categories in the data set which are intrin-
sically hard to disambiguate, while others are not.
Therefore, type b) errors are of greater interest to us,
as it is safe to assume that intrinsically ambiguous
categories will lead to biased coder decisions and
result in the systematic annotation noise we are in-
terested in.
Schein and Ungar observe that none of the 8
sampling methods investigated in their experiment
achieved a significant improvement over the random
sampling baseline on type b) errors. In fact, en-
tropy sampling and margin sampling even showed a
decrease in performance compared to random sam-
pling. For AL to work well on noisy data, we need
to identify and remove this type of annotation noise
during the AL process. To the best of our knowl-
edge, there is no work on detecting and removing
annotation noise by human coders during AL.
</bodyText>
<sectionHeader confidence="0.996741" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999953714285714">
To make sure that the data we use in our simula-
tion is as close to real-world data as possible, we do
not create an artificial data set as done in (Schein
and Ungar, 2007; Reidsma and Carletta, 2008) but
use real data from a WSD task for the German verb
drohen (threaten).1 Drohen has three different word
senses which can be disambiguated by humans with
</bodyText>
<footnote confidence="0.993533">
1The data has been provided by the SALSA project:
http://www.coli.uni-saarland.de/projects/salsa 45
</footnote>
<bodyText confidence="0.964885125">
a high accuracy.2 This point is crucial to our setup.
To control the amount of noise in the data, we need
to be sure that the initial data set is noise-free.
For classification we use a maximum entropy
classifier.3 Our sampling method is uncertainty sam-
pling (Lewis and Gale, 1994), a standard sampling
heuristic for AL where new instances are selected
based on the confidence of the classifier for predict-
ing the appropriate label. As a measure of uncer-
tainty we use Shannon entropy (1) (Zhang and Chen,
2002) and the margin metric (2) (Schein and Ungar,
2007). The first measure considers the model’s pre-
dictions q for each class c and selects those instances
from the pool where the Shannon entropy is highest.
1: − qc log qc (1)
c
The second measure looks at the difference be-
tween the largest two values in the prediciton vector
q, namely the two predicted classes c, c′ which are,
according to our model, the most likely ones for in-
stance xn, and selects those instances where the dif-
ference (margin) between the two predicted proba-
bilities is the smallest. We discuss some details of
this metric in Section 4.
</bodyText>
<equation confidence="0.750406">
Mn = |P(c|xn) − P(c′|xn) |(2)
</equation>
<bodyText confidence="0.99999075">
The features we use for WSD are a combination
of context features (word token with window size 11
and POS context with window size 7), syntactic fea-
tures based on the output of a dependency parser4
and semantic features based on GermaNet hyper-
onyms. These settings were tuned to the target verb
by (Rehbein et al., 2009). All results reported below
are averages over a 5-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.999895">
3.1 Simulating Coder Errors in AL
</subsectionHeader>
<bodyText confidence="0.9999438">
Before starting the AL trials we automatically sepa-
rate the 2,500 sentences into test set (498 sentences)
and pool (2,002 sentences),5 retaining the overall
distribution of word senses in the data set. We in-
sert a varying amount of noise into the pool data,
</bodyText>
<footnote confidence="0.9691375">
2In a pilot study where two human coders assigned labels to
a set of 100 sentences, the coders agreed on 99% of the data.
3http://maxent.sourceforge.net
4The MaltParser: http://maltparser.org
5The split has been made automatically, the unusual num-
bers are caused by rounding errors.
</footnote>
<table confidence="0.998615428571429">
test pool
ALrand ALbias
% errors 0% 0% 30% 30%
drohen1-salsa 126 506 524 514
Comittment 129 520 522 327
Run risk 243 976 956 1161
Total 498 2002 2002 2002
</table>
<tableCaption confidence="0.999676">
Table 1: Distribution of word senses in pool and test sets
</tableCaption>
<bodyText confidence="0.999980428571429">
starting from 0% up to 30% of noise, increasing by
2% in each trial.
We assess the impact of annotation noise on ac-
tive learning in three different settings. In the first
setting, we randomly select new instances from the
pool (random sampling; rand). In the second setting,
we randomly replace n percent of all labels (from 0
to 30) in the pool by another label before starting
the active learning trial, but retain the distribution of
the different labels in the pool data (active learning
with random errors); (Table 1, ALrand, 30%). In
the third setting we simulate biased decisions by a
human annotator. For a certain fraction (0 to 30%)
of instances of a particular non-majority class, we
substitute the majority class label for the gold label,
thereby producing a more skewed distribution than
in the original pool (active learning with biased er-
rors); (Table 1, ALbias, 30%).
For all three settings (rand, ALrand, ALbias) and
each degree of noise (0-30%), we run active learning
simulations on the already annotated data, simulat-
ing the annotation process by selecting one new, pre-
labelled instance per trial from the pool and, instead
of handing them over to a human coder, assigning
the known (possibly erroneous) label to the instance
and adding it to the training set. We use the same
split (test, pool) for all three settings and all degrees
of noise, with identical test sets for all trials.
</bodyText>
<subsectionHeader confidence="0.730886">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999892">
Figure 1 shows active learning curves for the differ-
ent settings and varying degrees of noise. The hori-
zontal black line slightly below 0.5 accuracy shows
the majority baseline (the performance obtained
when always assigning the majority class). For all
degrees of randomly inserted noise, active learning
(ALrand) outperforms random sampling (rand) at an
early stage in the learning process. Looking at the
biased errors (ALbias), we see a different picture.
With a low degree of noise, the curves for ALrand
and ALbias are very similar. When inserting more
noise, performance for ALbias decreases, and with
around 20% of biased errors in the pool AL performs
worse than our random sampling baseline. In the
random noise setting (ALrand), even after inserting
30% of errors AL clearly outperforms random sam-
pling. Increasing the size of the seed data reduces
the effect slightly, but does not prevent it (not shown
here due to space limitations). This confirms the
findings that under certain circumstances AL per-
forms worse than random sampling (Dang, 2004;
Schein and Ungar, 2007; Rehbein et al., 2010). We
could also confirm Schein and Ungar (2007)’s obser-
vation that margin sampling is less sensitive to cer-
tain types of noise than entropy sampling (Table 2).
Because of space limitations we only show curves
for margin sampling. For entropy sampling, the gen-
eral trend is the same, with results being slightly
lower than for margin sampling.
</bodyText>
<sectionHeader confidence="0.991448" genericHeader="method">
4 Detecting Annotation Noise
</sectionHeader>
<bodyText confidence="0.999735541666667">
Uncertainty sampling using the margin metric se-
lects instances for which the difference between
classifier predictions for the two most probable
classes c, c′ is very small (Section 3, Equation 2).
When selecting unlabelled instances from the pool,
this metric picks examples which represent regions
of uncertainty between classes which have yet to be
learned by the classifier and thus will advance the
learning process. Our human coder, however, is not
the perfect oracle assumed in most AL simulations,
and might also assign incorrect labels. The filter ap-
proach has two objectives: a) to detect incorrect la-
bels assigned by human coders, and b) to prevent
the hard cases (following the terminology of Kle-
banov et al. (2008)) from becoming part of the train-
ing data.
We proceed as follows. Our approach makes use
of the limited set of seed data 5 and uses heuris-
tics to detect unreliably annotated instances. We
assume that the instances in 5 have been validated
thoroughly. We train an ensemble of classifiers E
on subsets of 5, and use E to decide whether or not
a newly annotated instance should be added to the
seed.
</bodyText>
<page confidence="0.996351">
46
</page>
<figure confidence="0.999627652173913">
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
error=2%
Training size
Training size
error=6%
Training size
error=22%
0 250 600 950
Training size
error=10%
0 250 600 950
Training size
error=26%
0 250 600 950
Training size
error=14%
0 250 600 950
Training size
error=30%
0 250 600 950
Training size
0 250 600 950
rand
al rand
al_ bias
error=18%
0 250 600 950
rand
al rand
al_ bias
0 250 600 950
</figure>
<figureCaption confidence="0.991588333333333">
Figure 1: Active learning curves for varying degrees of noise, starting from 0% up to 30% for a training size up to
1200 instances (solid circle (black): random sampling; filled triangle point-up (red): AL with random errors; cross
(green): AL with biased errors)
</figureCaption>
<page confidence="0.988343">
47
</page>
<table confidence="0.9995305">
filter % error 0 4 8 12 16 20 24 28 30
- rand 0.763 0.752 0.736 0.741 0.726 0.708 0.707 0.677 0.678
entropy - ALrand 0.806 0.786 0.779 0.743 0.752 0.762 0.731 0.724 0.729
entropy y ALrand 0.792 0.786 0.777 0.760 0.771 0.748 0.730 0.729 0.727
margin - ALrand 0.795 0.795 0.782 0.771 0.758 0.755 0.737 0.719 0.708
margin y ALrand 0.800 0.785 0.773 0.777 0.765 0.766 0.734 0.735 0.718
entropy - ALbias 0.806 0.793 0.759 0.748 0.702 0.651 0.625 0.630 0.622
entropy y ALbias 0.802 0.781 0.777 0.735 0.702 0.678 0.687 0.624 0.616
margin - ALbias 0.795 0.789 0.770 0.753 0.706 0.684 0.656 0.634 0.624
margin y ALbias 0.787 0.781 0.787 0.768 0.739 0.700 0.671 0.653 0.651
</table>
<tableCaption confidence="0.944666">
Table 2: Accuracy for the different sampling methods without and with filtering after adding 500 instances to the seed
data
</tableCaption>
<bodyText confidence="0.999986222222222">
There are a number of problems with this ap-
proach. First, there is the risk of overfitting S. Sec-
ond, we know that classifier accuracy in the early
phase of AL is low. Therefore, using classifier pre-
dictions at this stage to accept or reject new in-
stances could result in poor choices that might harm
the learning proceess. To avoid this and to gener-
alise over S to prevent overfitting, we do not directly
train our ensemble on instances from S. Instead, we
create new feature vectors Fgen on the basis of the
feature vectors Fseed in S. For each class in S, we
extract all attribute-value pairs from the feature vec-
tors for this particular class. For each class, we ran-
domly select features (with replacement) from Fseed
and combine them into a new feature vector Fgen,
retaining the distribution of the different classes in
the data. As a result, we obtain a more general set of
feature vectors Fgen with characteristic features be-
ing distributed more evenly over the different feature
vectors.
In the next step we train n = 5 maximum en-
tropy classifiers on subsets of Fgen, excluding the
instances last annotated by the oracle. Each subset
is half the size of the current S. We use the ensemble
to predict the labels for the new instances and, based
on the predictions, accept or reject these, following
the two heuristics below (also see Figure 2).
</bodyText>
<listItem confidence="0.98622725">
1. If all n ensemble classifiers agree on one label
but disagree with the oracle ⇒ reject.
2. If the sum of the margins predicted by the en-
semble classifiers is below a particular theshold
</listItem>
<bodyText confidence="0.941057666666667">
tmargin ⇒ reject. 48
The threshold tmargin was set to 0.01, based on a
qualitative data analysis.
</bodyText>
<sectionHeader confidence="0.922639" genericHeader="method">
AL with Filtering:
</sectionHeader>
<bodyText confidence="0.722379333333333">
Input: annotated seed data S,
unannotated pool P
AL loop:
</bodyText>
<listItem confidence="0.98549005">
• train classifier C on S
• let C predict labels for data in P
• select new instances from P according to
sampling method, hand over to oracle for
annotation
Repeat: after every c new instances
annotated by the oracle
• for each class in S, extract sets of
features Fseed
• create new, more general feature vectors
Fgen from this set (with replacement)
• train an ensemble E of n classifiers on
different subsets of Fgen
Filtering Heuristics:
• if all n classifier in E agree on label
but disagree with oracle:
⇒ remove instance from seed
• if margin is less than threshold tmargin:
⇒ remove instance from seed
Until done
</listItem>
<figureCaption confidence="0.960521">
Figure 2: Heuristics for filtering unreliable data points
(parameters used: initial seed size: 9 sentences, c = 10,
n = 5, tmargi-n, = 0.01)
</figureCaption>
<bodyText confidence="0.9993754375">
In each iteration of the AL process, one new in-
stance is selected using margin sampling. The in-
stance is presented to the oracle who assigns a label.
Then the instance is added to the seed data, thus in-
fluencing the selection of the next data point to be
annotated. After 10 new instances have been added,
we apply the filter technique which finally decides
whether the newly added instances will remain in
the seed data or will be removed.
Figure 3 shows learning curves for the filter ap-
proach. With increasing amount of errors in the
pool, a clear pattern emerges. For both sampling
methods (ALrand, ALbias), the filtering step clearly
improves results. Even for the noisier data sets with
up to 26% of errors, ALbias with filtering performs
at least as well as random sampling.
</bodyText>
<subsectionHeader confidence="0.957501">
4.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99805712">
Next we want to find out what kind of errors the
system could detect. We want to know whether the
approach is able to detect the errors previously in-
serted into the data, and whether it manages to iden-
tify hard cases representing true ambiguities.
To answer these questions we look at one fold of
the ALbias data with 10% of noise. In 1,200 AL it-
erations the system rejected 116 instances (Table 3).
The major part of the rejections was due to the ma-
jority vote of the ensemble classifiers (first heuris-
tic, H1) which rejects all instances where the en-
semble classifiers agree with each other but disagree
with the human judgement. Out of the 105 instances
rejected by H1, 41 were labelled incorrectly. This
means that we were able to detect around half of the
incorrect labels inserted in the pool.
11 instances were filtered out by the margin
threshold (H2). None of these contained an incor-
errors inserted in pool 173
err. instances selected by AL 93
instances rejected by H1+H2 116
instances rejected by H1 105
true errors rejected by H1 41
instances rejected by H2 11
true errors rejected by H2 0
</bodyText>
<tableCaption confidence="0.9794855">
Table 3: Error analysis of the instances rejected by the
filtering approach
</tableCaption>
<bodyText confidence="0.999927842105263">
rect label. On first glance H2 seems to be more le-
nient than H1, considering the number of rejected
sentences. This, however, could also be an effect of
the order in which we apply the filters.
The different word senses are evenly distributed
over the rejected instances (H1: Commitment 30,
drohen1-salsa 38, Run risk 36; H2: Commitment 3,
drohen1-salsa 4, Run risk 4). This shows that there
is less uncertainty about the majority word sense,
Run risk.
It is hard to decide whether the correctly labelled
instances rejected by the filtering method would
have helped or hurt the learning process. Simply
adding them to the seed data after the conclusion
of AL would not answer this question, as it would
merely tell us whether they improve classification
accuracy further, but we still would not know what
impact these instances would have had on the selec-
tion of instances during the AL process.
</bodyText>
<sectionHeader confidence="0.999484" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999953157894737">
This paper shows that certain types of annotation
noise cause serious problems for active learning ap-
proaches. We showed how biased coder decisions
can result in an accuracy for AL approaches which
is below the one for random sampling. In this case,
it is necessary to apply an additional filtering step
to remove the noisy data from the training set. We
presented an approach based on a resampling of the
features in the seed data and guided by an ensemble
of classifiers trained on the resampled feature vec-
tors. We showed that our approach is able to detect
a certain amount of noise in the data.
Future work should focus on finding optimal pa-
rameter settings to make the filtering method more
robust even for noisier data sets. We also plan to im-
prove the filtering heuristics and to explore further
ways of detecting human coder errors. Finally, we
plan to test our method in a real-world annotation
scenario.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99971625">
This work was funded by the German Research
Foundation DFG (grant PI 154/9-3). We would like
to thank the anonymous reviewers for their helpful
comments and suggestions.
</bodyText>
<page confidence="0.996861">
49
</page>
<figure confidence="0.99955542">
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8
error=2%
Training size
error=18%
0 250 600 950
Training size
error=6%
Training size
error=22%
0 250 600 950
Training size
error=10%
0 250 600 950
Training size
error=26%
0 250 600 950
Training size
error=14%
0 250 600 950
Training size
error=30%
0 250 600 950
Training size
0 250 600 950
rand
ALrand
ALrand_f
ALbias
ALbias_f
rand
ALrand
ALrand_f
ALbias
ALbias_f
0 250 600 950
</figure>
<figureCaption confidence="0.955624333333333">
Figure 3: Active learning curves for varying degrees of noise, starting from 0% up to 30% for a training size up to
1200 instances (solid circle (black): random sampling; open circle (red): ALrand; cross (green): ALrand with filtering;
filled triangle point-up (black): ALbias; plus (blue): ALbias with filtering)
</figureCaption>
<page confidence="0.991942">
50
</page>
<sectionHeader confidence="0.989177" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997941443298969">
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Compu-
tational Linguistics, 35:495–503, December.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In Pro-
ceedings of the Workshop on Human Judgements in
Computational Linguistics, HumanJudge ’08, pages
2–7, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings ofACL-2007.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha
Palmer. 2006. An empirical study of the behavior of
active learning for word sense disambiguation. In Pro-
ceedings ofNAACL-2006, New York, NY.
Hoa Trang Dang. 2004. Investigations into the role of
lexical semantics in word sense disambiguation. PhD
dissertation, University of Pennsylvania, Pennsylva-
nia, PA.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1 - Volume 1, pages 181–
189. Association for Computational Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253–276.
Florian Laws and H. Sch¨utze. 2008. Stopping crite-
ria for active learning of named entity recognition.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), Manch-
ester, UK, August.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
ofACM-SIGIR, Dublin, Ireland.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: cost-efficient resource usage for base
noun phrase chunking. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 117–125, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In Proceed-
ings ofHLT-NAACL 2004.
Ines Rehbein, Josef Ruppenhofer, and Jonas Sunde.
2009. Majo - a toolkit for supervised word sense dis-
ambiguation and active learning. In Proceedings of
the 8th Workshop on Treebanks and Linguistic Theo-
ries (TLT-8), Milano, Italy. 51
Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.
2010. Bringing active learning to life. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING 2010), Beijing, China.
Dennis Reidsma and Jean Carletta. 2008. Reliability
measurement without limits. Computational Linguis-
tics, 34:319–326.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop,
Prague.
Andrew I. Schein and Lyle H. Ungar. 2007. Active learn-
ing for logistic regression: an evaluation. Machine
Learning, 68:235–265.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Tomanek and Udo Hahn. 2009. Reducing class
imbalance during active learning for named entity an-
notation. In Proceedings of the 5th International Con-
ference on Knowledge Capture, Redondo Beach, CA.
Simon Tong and Daphne Koller. 1998. Support vector
machine active learning with applications to text clas-
sification. In Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning (ICML-00),
pages 287–295.
Cha Zhang and Tsuhan Chen. 2002. An active learn-
ing framework for content-based information retrieval.
IEEE Transactions on Multimedia, 4(2):260–268.
Jingbo Zhu and Edward Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings ofthe
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, Prague, Czech Republic.
Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Ben-
jamin K. Tsou. 2008. Active learning with sampling
by uncertainty and density for word sense disambigua-
tion and text classification. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), Manchester, UK.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671550">
<title confidence="0.999972">Evaluating the Impact of Coder Errors on Active Learning</title>
<author confidence="0.995205">Ines Rehbein Josef Ruppenhofer</author>
<affiliation confidence="0.997908">Computational Linguistics Computational Linguistics Saarland University Saarland University</affiliation>
<email confidence="0.776951">rehbein@coli.uni-sb.dejosefr@coli.uni-sb.de</email>
<abstract confidence="0.992334117647059">Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when applied to noisy data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>From annotator agreement to noise models.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<pages>35--495</pages>
<contexts>
<context position="6076" citStr="Klebanov and Beigman, 2009" startWordPosition="994" endWordPosition="997">failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learning, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the observed accuracy of the classifier on a test set coming from the same population as the training data is as high as 0.8. Related work (Beigman Klebanov et al., 2008; Beigman Klebanov and Beigman, 2009) has been studying annotation noise in a multi-annotator setting, distinguishing between hard cases (unreliably annotated due to genuine ambiguity) and easy cases (reliably annotated data). The authors argue that even for those data points where the annotators agreed on one particular class, a proportion of the agreement might be merely due to chance. Following this assumption, the authors propose a measure to estimate the amount of annotation noise in the data after removing all hard cases. Klebanov et al. (2008; 2009) show that, according to their model, high inter-annotator agreement (r.) a</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>Beata Beigman Klebanov and Eyal Beigman. 2009. From annotator agreement to noise models. Computational Linguistics, 35:495–503, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
<author>Daniel Diermeier</author>
</authors>
<title>Analyzing disagreements.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Human Judgements in Computational Linguistics, HumanJudge ’08,</booktitle>
<pages>2--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6039" citStr="Klebanov et al., 2008" startWordPosition="989" endWordPosition="992">ns. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learning, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the observed accuracy of the classifier on a test set coming from the same population as the training data is as high as 0.8. Related work (Beigman Klebanov et al., 2008; Beigman Klebanov and Beigman, 2009) has been studying annotation noise in a multi-annotator setting, distinguishing between hard cases (unreliably annotated due to genuine ambiguity) and easy cases (reliably annotated data). The authors argue that even for those data points where the annotators agreed on one particular class, a proportion of the agreement might be merely due to chance. Following this assumption, the authors propose a measure to estimate the amount of annotation noise in the data after removing all hard cases. Klebanov et al. (2008; 2009) show that, according to their model, </context>
<context position="16186" citStr="Klebanov et al. (2008)" startWordPosition="2689" endWordPosition="2693">r predictions for the two most probable classes c, c′ is very small (Section 3, Equation 2). When selecting unlabelled instances from the pool, this metric picks examples which represent regions of uncertainty between classes which have yet to be learned by the classifier and thus will advance the learning process. Our human coder, however, is not the perfect oracle assumed in most AL simulations, and might also assign incorrect labels. The filter approach has two objectives: a) to detect incorrect labels assigned by human coders, and b) to prevent the hard cases (following the terminology of Klebanov et al. (2008)) from becoming part of the training data. We proceed as follows. Our approach makes use of the limited set of seed data 5 and uses heuristics to detect unreliably annotated instances. We assume that the instances in 5 have been validated thoroughly. We train an ensemble of classifiers E on subsets of 5, and use E to decide whether or not a newly annotated instance should be added to the seed. 46 Accuracy 0.4 0.5 0.6 0.7 0.8 Accuracy 0.4 0.5 0.6 0.7 0.8 Accuracy 0.4 0.5 0.6 0.7 0.8 Accuracy 0.4 0.5 0.6 0.7 0.8 Accuracy 0.4 0.5 0.6 0.7 0.8 Accuracy 0.4 0.5 0.6 0.7 0.8 Accuracy 0.4 0.5 0.6 0.7 0</context>
</contexts>
<marker>Klebanov, Beigman, Diermeier, 2008</marker>
<rawString>Beata Beigman Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing disagreements. In Proceedings of the Workshop on Human Judgements in Computational Linguistics, HumanJudge ’08, pages 2–7, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL-2007.</booktitle>
<contexts>
<context position="2116" citStr="Chan and Ng, 2007" startWordPosition="332" endWordPosition="335">the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different param</context>
<context position="5222" citStr="Chan and Ng, 2007" startWordPosition="849" endWordPosition="852"> (e.g. sentiment analysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learning, while systematic errors (as caused by biased annotators) can seriously impair the</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings ofACL-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Andrew Schein</author>
<author>Lyle Ungar</author>
<author>Martha Palmer</author>
</authors>
<title>An empirical study of the behavior of active learning for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings ofNAACL-2006,</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="2076" citStr="Chen et al., 2006" startWordPosition="324" endWordPosition="327">ces that are maximally informative for the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispen</context>
<context position="5203" citStr="Chen et al., 2006" startWordPosition="845" endWordPosition="848"> for many NLP tasks (e.g. sentiment analysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learning, while systematic errors (as caused by biased annotators) can s</context>
</contexts>
<marker>Chen, Schein, Ungar, Palmer, 2006</marker>
<rawString>Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer. 2006. An empirical study of the behavior of active learning for word sense disambiguation. In Proceedings ofNAACL-2006, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Investigations into the role of lexical semantics in word sense disambiguation.</title>
<date>2004</date>
<institution>University of Pennsylvania,</institution>
<location>Pennsylvania, PA.</location>
<note>PhD dissertation,</note>
<contexts>
<context position="2935" citStr="Dang, 2004" startWordPosition="464" endWordPosition="465">formance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different parameters and settings, it has been shown that when applying AL to highly ambiguous tasks like e.g. Word Sense Disambiguation (WSD) with fine-grained sense distinctions, AL can actually harm the learning process (Dang, 2004; Rehbein et al., 2010). Dang suggests that the lack of a positive effect of AL might be due to inconsistencies in the human annotations and that AL cannot efficiently be applied to tasks which need double blind annotation with adjudication to insure a sufficient data quality. Even if we take a more optimistic view and assume that AL might still be useful even for tasks featuring a high degree of ambiguity, it remains crucial to address the problem of annotation noise and its impact on AL. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43–51, Por</context>
<context position="5275" citStr="Dang (2004)" startWordPosition="860" endWordPosition="861">with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learning, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the o</context>
<context position="15040" citStr="Dang, 2004" startWordPosition="2505" endWordPosition="2506">nt picture. With a low degree of noise, the curves for ALrand and ALbias are very similar. When inserting more noise, performance for ALbias decreases, and with around 20% of biased errors in the pool AL performs worse than our random sampling baseline. In the random noise setting (ALrand), even after inserting 30% of errors AL clearly outperforms random sampling. Increasing the size of the seed data reduces the effect slightly, but does not prevent it (not shown here due to space limitations). This confirms the findings that under certain circumstances AL performs worse than random sampling (Dang, 2004; Schein and Ungar, 2007; Rehbein et al., 2010). We could also confirm Schein and Ungar (2007)’s observation that margin sampling is less sensitive to certain types of noise than entropy sampling (Table 2). Because of space limitations we only show curves for margin sampling. For entropy sampling, the general trend is the same, with results being slightly lower than for margin sampling. 4 Detecting Annotation Noise Uncertainty sampling using the margin metric selects instances for which the difference between classifier predictions for the two most probable classes c, c′ is very small (Section</context>
</contexts>
<marker>Dang, 2004</marker>
<rawString>Hoa Trang Dang. 2004. Investigations into the role of lexical semantics in word sense disambiguation. PhD dissertation, University of Pennsylvania, Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Active learning for multilingual statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>181--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2223" citStr="Haffari and Sarkar, 2009" startWordPosition="347" endWordPosition="350">rmance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different parameters and settings, it has been shown that when applying AL to highly ambiguous tasks like e.g. Word Sense </context>
</contexts>
<marker>Haffari, Sarkar, 2009</marker>
<rawString>Gholamreza Haffari and Anoop Sarkar. 2009. Active learning for multilingual statistical machine translation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, pages 181– 189. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="1935" citStr="Hwa, 2004" startWordPosition="304" endWordPosition="305">ing is quite intuitive: instead of annotating a large number of randomly picked instances we carefully select a small number of instances that are maximally informative for the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario wh</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>Rebecca Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Laws</author>
<author>H Sch¨utze</author>
</authors>
<title>Stopping criteria for active learning of named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<location>Manchester, UK,</location>
<marker>Laws, Sch¨utze, 2008</marker>
<rawString>Florian Laws and H. Sch¨utze. 2008. Stopping criteria for active learning of named entity recognition. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings ofACM-SIGIR,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="10515" citStr="Lewis and Gale, 1994" startWordPosition="1735" endWordPosition="1738"> artificial data set as done in (Schein and Ungar, 2007; Reidsma and Carletta, 2008) but use real data from a WSD task for the German verb drohen (threaten).1 Drohen has three different word senses which can be disambiguated by humans with 1The data has been provided by the SALSA project: http://www.coli.uni-saarland.de/projects/salsa 45 a high accuracy.2 This point is crucial to our setup. To control the amount of noise in the data, we need to be sure that the initial data set is noise-free. For classification we use a maximum entropy classifier.3 Our sampling method is uncertainty sampling (Lewis and Gale, 1994), a standard sampling heuristic for AL where new instances are selected based on the confidence of the classifier for predicting the appropriate label. As a measure of uncertainty we use Shannon entropy (1) (Zhang and Chen, 2002) and the margin metric (2) (Schein and Ungar, 2007). The first measure considers the model’s predictions q for each class c and selects those instances from the pool where the Shannon entropy is highest. 1: − qc log qc (1) c The second measure looks at the difference between the largest two values in the prediciton vector q, namely the two predicted classes c, c′ which</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings ofACM-SIGIR, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>117--125</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1875" citStr="Ngai and Yarowsky, 2000" startWordPosition="293" endWordPosition="296">amount of time and cost for human annotation. The idea behind active learning is quite intuitive: instead of annotating a large number of randomly picked instances we carefully select a small number of instances that are maximally informative for the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This </context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: cost-efficient resource usage for base noun phrase chunking. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 117–125, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Jason Baldridge</author>
</authors>
<title>Ensemblebased active learning for parse selection.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<contexts>
<context position="1923" citStr="Osborne and Baldridge, 2004" startWordPosition="300" endWordPosition="303"> The idea behind active learning is quite intuitive: instead of annotating a large number of randomly picked instances we carefully select a small number of instances that are maximally informative for the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world</context>
</contexts>
<marker>Osborne, Baldridge, 2004</marker>
<rawString>Miles Osborne and Jason Baldridge. 2004. Ensemblebased active learning for parse selection. In Proceedings ofHLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef Ruppenhofer</author>
<author>Jonas Sunde</author>
</authors>
<title>Majo - a toolkit for supervised word sense disambiguation and active learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 8th Workshop on Treebanks and Linguistic Theories (TLT-8),</booktitle>
<volume>51</volume>
<location>Milano,</location>
<contexts>
<context position="11701" citStr="Rehbein et al., 2009" startWordPosition="1946" endWordPosition="1949">e two predicted classes c, c′ which are, according to our model, the most likely ones for instance xn, and selects those instances where the difference (margin) between the two predicted probabilities is the smallest. We discuss some details of this metric in Section 4. Mn = |P(c|xn) − P(c′|xn) |(2) The features we use for WSD are a combination of context features (word token with window size 11 and POS context with window size 7), syntactic features based on the output of a dependency parser4 and semantic features based on GermaNet hyperonyms. These settings were tuned to the target verb by (Rehbein et al., 2009). All results reported below are averages over a 5-fold cross validation. 3.1 Simulating Coder Errors in AL Before starting the AL trials we automatically separate the 2,500 sentences into test set (498 sentences) and pool (2,002 sentences),5 retaining the overall distribution of word senses in the data set. We insert a varying amount of noise into the pool data, 2In a pilot study where two human coders assigned labels to a set of 100 sentences, the coders agreed on 99% of the data. 3http://maxent.sourceforge.net 4The MaltParser: http://maltparser.org 5The split has been made automatically, th</context>
</contexts>
<marker>Rehbein, Ruppenhofer, Sunde, 2009</marker>
<rawString>Ines Rehbein, Josef Ruppenhofer, and Jonas Sunde. 2009. Majo - a toolkit for supervised word sense disambiguation and active learning. In Proceedings of the 8th Workshop on Treebanks and Linguistic Theories (TLT-8), Milano, Italy. 51</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef Ruppenhofer</author>
<author>Alexis Palmer</author>
</authors>
<title>Bringing active learning to life.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2958" citStr="Rehbein et al., 2010" startWordPosition="466" endWordPosition="469">metimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different parameters and settings, it has been shown that when applying AL to highly ambiguous tasks like e.g. Word Sense Disambiguation (WSD) with fine-grained sense distinctions, AL can actually harm the learning process (Dang, 2004; Rehbein et al., 2010). Dang suggests that the lack of a positive effect of AL might be due to inconsistencies in the human annotations and that AL cannot efficiently be applied to tasks which need double blind annotation with adjudication to insure a sufficient data quality. Even if we take a more optimistic view and assume that AL might still be useful even for tasks featuring a high degree of ambiguity, it remains crucial to address the problem of annotation noise and its impact on AL. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43–51, Portland, Oregon, June 19-</context>
<context position="5301" citStr="Rehbein et al. (2010)" startWordPosition="863" endWordPosition="866">d word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learning, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the observed accuracy of the cl</context>
<context position="15087" citStr="Rehbein et al., 2010" startWordPosition="2511" endWordPosition="2514">se, the curves for ALrand and ALbias are very similar. When inserting more noise, performance for ALbias decreases, and with around 20% of biased errors in the pool AL performs worse than our random sampling baseline. In the random noise setting (ALrand), even after inserting 30% of errors AL clearly outperforms random sampling. Increasing the size of the seed data reduces the effect slightly, but does not prevent it (not shown here due to space limitations). This confirms the findings that under certain circumstances AL performs worse than random sampling (Dang, 2004; Schein and Ungar, 2007; Rehbein et al., 2010). We could also confirm Schein and Ungar (2007)’s observation that margin sampling is less sensitive to certain types of noise than entropy sampling (Table 2). Because of space limitations we only show curves for margin sampling. For entropy sampling, the general trend is the same, with results being slightly lower than for margin sampling. 4 Detecting Annotation Noise Uncertainty sampling using the margin metric selects instances for which the difference between classifier predictions for the two most probable classes c, c′ is very small (Section 3, Equation 2). When selecting unlabelled inst</context>
</contexts>
<marker>Rehbein, Ruppenhofer, Palmer, 2010</marker>
<rawString>Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer. 2010. Bringing active learning to life. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability measurement without limits.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--319</pages>
<contexts>
<context position="5671" citStr="Reidsma and Carletta, 2008" startWordPosition="926" endWordPosition="929">es on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learning, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the observed accuracy of the classifier on a test set coming from the same population as the training data is as high as 0.8. Related work (Beigman Klebanov et al., 2008; Beigman Klebanov and Beigman, 2009) has been studying annotation noise in a multi-annotator setting, distinguishing between hard cases (unreliably annotated due to genuine ambiguity) and easy cases (reliably annotated data). The a</context>
<context position="7425" citStr="Reidsma and Carletta, 2008" startWordPosition="1209" endWordPosition="1213">ssumes that a) all instances where annotators disagreed are in fact hard cases, and b) that for the hard cases the annotators decisions are obtained by coin-flips. In our experience, some amount of disagreement can also be observed for easy cases, caused by attention slips or by a deviant interpretation of some class(es) by one of the annotators, and the annotation decision of an individual annotator cannot so much be described as random choice (coin-flip) but as systematically biased selection, causing the types of errors which have been shown to be problematic for supervised classification (Reidsma and Carletta, 2008). Further problems arise in the AL scenario where the instances to be annotated are selected as a function of the sampling method and the annotation judgements made before. Therefore, Beigman and Klebanov Beigman (2009)’s approach of identifying unreliably annotated instances by disagreement is not applicable to AL, as most instances are annotated only once. 2.2 Annotation Noise and Active Learning For AL to be succesful, we need to remove systematic noise in the training data. The challenge we face is that we only have a small set of seed data and no information about the reliability of the a</context>
<context position="9978" citStr="Reidsma and Carletta, 2008" startWordPosition="1647" endWordPosition="1650">ment over the random sampling baseline on type b) errors. In fact, entropy sampling and margin sampling even showed a decrease in performance compared to random sampling. For AL to work well on noisy data, we need to identify and remove this type of annotation noise during the AL process. To the best of our knowledge, there is no work on detecting and removing annotation noise by human coders during AL. 3 Experimental Setup To make sure that the data we use in our simulation is as close to real-world data as possible, we do not create an artificial data set as done in (Schein and Ungar, 2007; Reidsma and Carletta, 2008) but use real data from a WSD task for the German verb drohen (threaten).1 Drohen has three different word senses which can be disambiguated by humans with 1The data has been provided by the SALSA project: http://www.coli.uni-saarland.de/projects/salsa 45 a high accuracy.2 This point is crucial to our setup. To control the amount of noise in the data, we need to be sure that the initial data set is noise-free. For classification we use a maximum entropy classifier.3 Our sampling method is uncertainty sampling (Lewis and Gale, 1994), a standard sampling heuristic for AL where new instances are </context>
</contexts>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma and Jean Carletta. 2008. Reliability measurement without limits. Computational Linguistics, 34:319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Ringger</author>
<author>Peter McClanahan</author>
<author>Robbie Haertel</author>
<author>George Busby</author>
<author>Marc Carmen</author>
<author>James Carroll</author>
<author>Kevin Seppi</author>
<author>Deryle Lonsdale</author>
</authors>
<title>Active learning for part-ofspeech tagging: Accelerating corpus annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop,</booktitle>
<location>Prague.</location>
<contexts>
<context position="1839" citStr="Ringger et al., 2007" startWordPosition="288" endWordPosition="291">promising approach to reduce the amount of time and cost for human annotation. The idea behind active learning is quite intuitive: instead of annotating a large number of randomly picked instances we carefully select a small number of instances that are maximally informative for the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning pr</context>
</contexts>
<marker>Ringger, McClanahan, Haertel, Busby, Carmen, Carroll, Seppi, Lonsdale, 2007</marker>
<rawString>Eric Ringger, Peter McClanahan, Robbie Haertel, George Busby, Marc Carmen, James Carroll, Kevin Seppi, and Deryle Lonsdale. 2007. Active learning for part-ofspeech tagging: Accelerating corpus annotation. In Proceedings of the Linguistic Annotation Workshop, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew I Schein</author>
<author>Lyle H Ungar</author>
</authors>
<title>Active learning for logistic regression: an evaluation.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<pages>68--235</pages>
<contexts>
<context position="8402" citStr="Schein and Ungar (2007)" startWordPosition="1375" endWordPosition="1378"> once. 2.2 Annotation Noise and Active Learning For AL to be succesful, we need to remove systematic noise in the training data. The challenge we face is that we only have a small set of seed data and no information about the reliability of the annotations 44 assigned by the human coders. Zhu et al. (2008) present a method for detecting outliers in the pool of unannotated data to prevent these instances from becoming part of the training data. This approach is different from ours, where we focus on detecting annotation noise in the manually labelled training data produced by the human coders. Schein and Ungar (2007) provide a systematic investigation of 8 different sampling methods for AL and their ability to handle different types of noise in the data. The types of noise investigated are a) prediction residual error (the portion of squared error that is independent of training set size), and b) different levels of confusion among the categories. Type a) models the presence of unknown features that influence the true probabilities of an outcome: a form of noise that will increase residual error. Type b) models categories in the data set which are intrinsically hard to disambiguate, while others are not. </context>
<context position="9949" citStr="Schein and Ungar, 2007" startWordPosition="1643" endWordPosition="1646">ed a significant improvement over the random sampling baseline on type b) errors. In fact, entropy sampling and margin sampling even showed a decrease in performance compared to random sampling. For AL to work well on noisy data, we need to identify and remove this type of annotation noise during the AL process. To the best of our knowledge, there is no work on detecting and removing annotation noise by human coders during AL. 3 Experimental Setup To make sure that the data we use in our simulation is as close to real-world data as possible, we do not create an artificial data set as done in (Schein and Ungar, 2007; Reidsma and Carletta, 2008) but use real data from a WSD task for the German verb drohen (threaten).1 Drohen has three different word senses which can be disambiguated by humans with 1The data has been provided by the SALSA project: http://www.coli.uni-saarland.de/projects/salsa 45 a high accuracy.2 This point is crucial to our setup. To control the amount of noise in the data, we need to be sure that the initial data set is noise-free. For classification we use a maximum entropy classifier.3 Our sampling method is uncertainty sampling (Lewis and Gale, 1994), a standard sampling heuristic fo</context>
<context position="15064" citStr="Schein and Ungar, 2007" startWordPosition="2507" endWordPosition="2510">With a low degree of noise, the curves for ALrand and ALbias are very similar. When inserting more noise, performance for ALbias decreases, and with around 20% of biased errors in the pool AL performs worse than our random sampling baseline. In the random noise setting (ALrand), even after inserting 30% of errors AL clearly outperforms random sampling. Increasing the size of the seed data reduces the effect slightly, but does not prevent it (not shown here due to space limitations). This confirms the findings that under certain circumstances AL performs worse than random sampling (Dang, 2004; Schein and Ungar, 2007; Rehbein et al., 2010). We could also confirm Schein and Ungar (2007)’s observation that margin sampling is less sensitive to certain types of noise than entropy sampling (Table 2). Because of space limitations we only show curves for margin sampling. For entropy sampling, the general trend is the same, with results being slightly lower than for margin sampling. 4 Detecting Annotation Noise Uncertainty sampling using the margin metric selects instances for which the difference between classifier predictions for the two most probable classes c, c′ is very small (Section 3, Equation 2). When se</context>
</contexts>
<marker>Schein, Ungar, 2007</marker>
<rawString>Andrew I. Schein and Lyle H. Ungar. 2007. Active learning for logistic regression: an evaluation. Machine Learning, 68:235–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
<author>ChewLim Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1980" citStr="Shen et al., 2004" startWordPosition="309" endWordPosition="312">otating a large number of randomly picked instances we carefully select a small number of instances that are maximally informative for the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and i</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and ChewLim Tan. 2004. Multi-criteria-based active learning for named entity recognition. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
</authors>
<title>Reducing class imbalance during active learning for named entity annotation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 5th International Conference on Knowledge Capture,</booktitle>
<location>Redondo Beach, CA.</location>
<contexts>
<context position="2030" citStr="Tomanek and Hahn, 2009" startWordPosition="317" endWordPosition="320">stances we carefully select a small number of instances that are maximally informative for the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human</context>
</contexts>
<marker>Tomanek, Hahn, 2009</marker>
<rawString>Katrin Tomanek and Udo Hahn. 2009. Reducing class imbalance during active learning for named entity annotation. In Proceedings of the 5th International Conference on Knowledge Capture, Redondo Beach, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning (ICML-00),</booktitle>
<pages>287--295</pages>
<contexts>
<context position="2161" citStr="Tong and Koller, 1998" startWordPosition="338" endWordPosition="341">maller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different parameters and settings, it has been shown that wh</context>
</contexts>
<marker>Tong, Koller, 1998</marker>
<rawString>Simon Tong and Daphne Koller. 1998. Support vector machine active learning with applications to text classification. In Proceedings of the Seventeenth International Conference on Machine Learning (ICML-00), pages 287–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cha Zhang</author>
<author>Tsuhan Chen</author>
</authors>
<title>An active learning framework for content-based information retrieval.</title>
<date>2002</date>
<journal>IEEE Transactions on Multimedia,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="10744" citStr="Zhang and Chen, 2002" startWordPosition="1774" endWordPosition="1777">umans with 1The data has been provided by the SALSA project: http://www.coli.uni-saarland.de/projects/salsa 45 a high accuracy.2 This point is crucial to our setup. To control the amount of noise in the data, we need to be sure that the initial data set is noise-free. For classification we use a maximum entropy classifier.3 Our sampling method is uncertainty sampling (Lewis and Gale, 1994), a standard sampling heuristic for AL where new instances are selected based on the confidence of the classifier for predicting the appropriate label. As a measure of uncertainty we use Shannon entropy (1) (Zhang and Chen, 2002) and the margin metric (2) (Schein and Ungar, 2007). The first measure considers the model’s predictions q for each class c and selects those instances from the pool where the Shannon entropy is highest. 1: − qc log qc (1) c The second measure looks at the difference between the largest two values in the prediciton vector q, namely the two predicted classes c, c′ which are, according to our model, the most likely ones for instance xn, and selects those instances where the difference (margin) between the two predicted probabilities is the smallest. We discuss some details of this metric in Sect</context>
</contexts>
<marker>Zhang, Chen, 2002</marker>
<rawString>Cha Zhang and Tsuhan Chen. 2002. An active learning framework for content-based information retrieval. IEEE Transactions on Multimedia, 4(2):260–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Edward Hovy</author>
</authors>
<title>Active learning for word sense disambiguation with methods for addressing the class imbalance problem.</title>
<date>2007</date>
<booktitle>In Proceedings ofthe 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2096" citStr="Zhu and Hovy, 2007" startWordPosition="328" endWordPosition="331">lly informative for the machine learning classifier. Thus a smaller set of data points is able to boost classifier performance and to yield an accuracy comparable to the one obtained when training the same system on a larger set of randomly chosen data. 43 Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to </context>
<context position="5243" citStr="Zhu and Hovy, 2007" startWordPosition="853" endWordPosition="856">alysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learning, while systematic errors (as caused by biased annotators) can seriously impair the performance of a sup</context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>Jingbo Zhu and Edward Hovy. 2007. Active learning for word sense disambiguation with methods for addressing the class imbalance problem. In Proceedings ofthe 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Huizhen Wang</author>
<author>Tianshun Yao</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Active learning with sampling by uncertainty and density for word sense disambiguation and text classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="8086" citStr="Zhu et al. (2008)" startWordPosition="1323" endWordPosition="1326">where the instances to be annotated are selected as a function of the sampling method and the annotation judgements made before. Therefore, Beigman and Klebanov Beigman (2009)’s approach of identifying unreliably annotated instances by disagreement is not applicable to AL, as most instances are annotated only once. 2.2 Annotation Noise and Active Learning For AL to be succesful, we need to remove systematic noise in the training data. The challenge we face is that we only have a small set of seed data and no information about the reliability of the annotations 44 assigned by the human coders. Zhu et al. (2008) present a method for detecting outliers in the pool of unannotated data to prevent these instances from becoming part of the training data. This approach is different from ours, where we focus on detecting annotation noise in the manually labelled training data produced by the human coders. Schein and Ungar (2007) provide a systematic investigation of 8 different sampling methods for AL and their ability to handle different types of noise in the data. The types of noise investigated are a) prediction residual error (the portion of squared error that is independent of training set size), and b</context>
</contexts>
<marker>Zhu, Wang, Yao, Tsou, 2008</marker>
<rawString>Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin K. Tsou. 2008. Active learning with sampling by uncertainty and density for word sense disambiguation and text classification. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>