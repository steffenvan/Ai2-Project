<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002375">
<title confidence="0.990956">
Distant supervision for relation extraction without labeled data
</title>
<author confidence="0.998267">
Mike Mintz, Steven Bills, Rion Snow, Dan Jurafsky
</author>
<affiliation confidence="0.981783">
Stanford University / Stanford, CA 94305
</affiliation>
<email confidence="0.997406">
{mikemintz,sbills,rion,jurafsky}@cs.stanford.edu
</email>
<sectionHeader confidence="0.99672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.898336727272727">
Modern models of relation extraction for tasks like
ACE are based on supervised learning of relations
from small hand-labeled corpora. We investigate an
alternative paradigm that does not require labeled
corpora, avoiding the domain dependence of ACE-
style algorithms, and allowing the use of corpora
of any size. Our experiments use Freebase, a large
semantic database of several thousand relations, to
provide distant supervision. For each pair of enti-
ties that appears in some Freebase relation, we find
all sentences containing those entities in a large un-
labeled corpus and extract textual features to train
a relation classifier. Our algorithm combines the
advantages of supervised IE (combining 400,000
noisy pattern features in a probabilistic classifier)
and unsupervised IE (extracting large numbers of
relations from large corpora of any domain). Our
model is able to extract 10,000 instances of 102 re-
lations at a precision of 67.6%. We also analyze
feature performance, showing that syntactic parse
features are particularly helpful for relations that are
ambiguous or lexically distant in their expression.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816571428571">
At least three learning paradigms have been ap-
plied to the task of extracting relational facts from
text (for example, learning that a person is em-
ployed by a particular organization, or that a ge-
ographic entity is located in a particular region).
In supervised approaches, sentences in a cor-
pus are first hand-labeled for the presence of en-
tities and the relations between them. The NIST
Automatic Content Extraction (ACE) RDC 2003
and 2004 corpora, for example, include over 1,000
documents in which pairs of entities have been la-
beled with 5 to 7 major relation types and 23 to
24 subrelations, totaling 16,771 relation instances.
ACE systems then extract a wide variety of lexi-
cal, syntactic, and semantic features, and use su-
pervised classifiers to label the relation mention
holding between a given pair of entities in a test
set sentence, optionally combining relation men-
tions (Zhou et al., 2005; Zhou et al., 2007; Sur-
deanu and Ciaramita, 2007).
Supervised relation extraction suffers from a
number of problems, however. Labeled training
data is expensive to produce and thus limited in
quantity. Also, because the relations are labeled
on a particular corpus, the resulting classifiers tend
to be biased toward that text domain.
An alternative approach, purely unsupervised
information extraction, extracts strings of words
between entities in large amounts of text, and
clusters and simplifies these word strings to pro-
duce relation-strings (Shinyama and Sekine, 2006;
Banko et al., 2007). Unsupervised approaches can
use very large amounts of data and extract very
large numbers of relations, but the resulting rela-
tions may not be easy to map to relations needed
for a particular knowledge base.
A third approach has been to use a very small
number of seed instances or patterns to do boot-
strap learning (Brin, 1998; Riloff and Jones, 1999;
Agichtein and Gravano, 2000; Ravichandran and
Hovy, 2002; Etzioni et al., 2005; Pennacchiotti
and Pantel, 2006; Bunescu and Mooney, 2007;
Rozenfeld and Feldman, 2008). These seeds are
used with a large corpus to extract a new set of
patterns, which are used to extract more instances,
which are used to extract more patterns, in an it-
erative fashion. The resulting patterns often suffer
from low precision and semantic drift.
We propose an alternative paradigm, distant su-
pervision, that combines some of the advantages
of each of these approaches. Distant supervision
is an extension of the paradigm used by Snow et
al. (2005) for exploiting WordNet to extract hyper-
nym (is-a) relations between entities, and is simi-
lar to the use of weakly labeled data in bioinfor-
matics (Craven and Kumlien, 1999; Morgan et al.,
</bodyText>
<page confidence="0.857556">
1003
</page>
<note confidence="0.99480875">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1003–1011,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
Relation name New instance
/location/location/contains Paris, Montmartre
/location/location/contains Ontario, Fort Erie
/music/artist/origin Mighty Wagon, Cincinnati
/people/deceased person/place of death Fyodor Kamensky, Clearwater
/people/person/nationality Marianne Yvonne Heemskerk, Netherlands
/people/person/place of birth Wavell Wayne Hinds, Kingston
/book/author/works written Upton Sinclair, Lanny Budd
/business/company/founders WWE, Vince McMahon
/people/person/profession Thomas Mellon, judge
</note>
<tableCaption confidence="0.998529">
Table 1: Ten relation instances extracted by our system that did not appear in Freebase.
</tableCaption>
<bodyText confidence="0.999741745454546">
2004). Our algorithm uses Freebase (Bollacker et
al., 2008), a large semantic database, to provide
distant supervision for relation extraction. Free-
base contains 116 million instances of 7,300 rela-
tions between 9 million entities. The intuition of
distant supervision is that any sentence that con-
tains a pair of entities that participate in a known
Freebase relation is likely to express that relation
in some way. Since there may be many sentences
containing a given entity pair, we can extract very
large numbers of (potentially noisy) features that
are combined in a logistic regression classifier.
Thus whereas the supervised training paradigm
uses a small labeled corpus of only 17,000 rela-
tion instances as training data, our algorithm can
use much larger amounts of data: more text, more
relations, and more instances. We use 1.2 million
Wikipedia articles and 1.8 million instances of 102
relations connecting 940,000 entities. In addition,
combining vast numbers of features in a large clas-
sifier helps obviate problems with bad features.
Because our algorithm is supervised by a
database, rather than by labeled text, it does
not suffer from the problems of overfitting and
domain-dependence that plague supervised sys-
tems. Supervision by a database also means that,
unlike in unsupervised approaches, the output of
our classifier uses canonical names for relations.
Our paradigm offers a natural way of integrating
data from multiple sentences to decide if a relation
holds between two entities. Because our algorithm
can use large amounts of unlabeled data, a pair of
entities may occur multiple times in the test set.
For each pair of entities, we aggregate the features
from the many different sentences in which that
pair appeared into a single feature vector, allowing
us to provide our classifier with more information,
resulting in more accurate labels.
Table 1 shows examples of relation instances
extracted by our system. We also use this system
to investigate the value of syntactic versus lexi-
cal (word sequence) features in relation extraction.
While syntactic features are known to improve the
performance of supervised IE, at least using clean
hand-labeled ACE data (Zhou et al., 2007; Zhou
et al., 2005), we do not know whether syntactic
features can improve the performance of unsuper-
vised or distantly supervised IE. Most previous
research in bootstrapping or unsupervised IE has
used only simple lexical features, thereby avoid-
ing the computational expense of parsing (Brin,
1998; Agichtein and Gravano, 2000; Etzioni et al.,
2005), and the few systems that have used unsu-
pervised IE have not compared the performance
of these two types of feature.
</bodyText>
<sectionHeader confidence="0.997512" genericHeader="introduction">
2 Previous work
</sectionHeader>
<bodyText confidence="0.99997732">
Except for the unsupervised algorithms discussed
above, previous supervised or bootstrapping ap-
proaches to relation extraction have typically re-
lied on relatively small datasets, or on only a small
number of distinct relations. Approaches based on
WordNet have often only looked at the hypernym
(is-a) or meronym (part-of) relation (Girju et al.,
2003; Snow et al., 2005), while those based on the
ACE program (Doddington et al., 2004) have been
restricted in their evaluation to a small number of
relation instances and corpora of less than a mil-
lion words.
Many early algorithms for relation extraction
used little or no syntactic information. For ex-
ample, the DIPRE algorithm by Brin (1998) used
string-based regular expressions in order to rec-
ognize relations such as author-book, while the
SNOWBALL algorithm by Agichtein and Gravano
(2000) learned similar regular expression patterns
over words and named entity tags. Hearst (1992)
used a small number of regular expressions over
words and part-of-speech tags to find examples of
the hypernym relation. The use of these patterns
has been widely replicated in successful systems,
for example by Etzioni et al. (2005). Other work
</bodyText>
<page confidence="0.991445">
1004
</page>
<figureCaption confidence="0.874816583333333">
Relation name Size Example
/people/person/nationality 281,107 John Dugard, South Africa
/location/location/contains 253,223 Belgium, Nijlen
/people/person/profession 208,888 Dusa McDuff, Mathematician
/people/person/place of birth 105,799 Edwin Hubble, Marshfield
/dining/restaurant/cuisine 86,213 MacAyo’s Mexican Kitchen, Mexican
/business/business chain/location 66,529 Apple Inc., Apple Inc., South Park, NC
/biology/organism classification rank 42,806 Scorpaeniformes, Order
/film/film/genre 40,658 Where the Sidewalk Ends, Film noir
/film/film/language 31,103 Enter the Phoenix, Cantonese
/biology/organism higher classification 30,052 Calopteryx, Calopterygidae
/film/film/country 27,217 Turtle Diary, United States
/film/writer/film 23,856 Irving Shulman, Rebel Without a Cause
/film/director/film 23,539 Michael Mann, Collateral
/film/producer/film 22,079 Diane Eskenazi, Aladdin
/people/deceased person/place of death 18,814 John W. Kern, Asheville
/music/artist/origin 18,619 The Octopus Project, Austin
/people/person/religion 17,582 Joseph Chartrand, Catholicism
/book/author/works written 17,278 Paul Auster, Travels in the Scriptorium
/soccer/football position/players 17,244 Midfielder, Chen Tao
/people/deceased person/cause of death 16,709 Richard Daintree, Tuberculosis
/book/book/genre 16,431 Pony Soldiers, Science fiction
/film/film/music 14,070 Stavisky, Stephen Sondheim
/business/company/industry 13,805 ATS Medical, Health care
</figureCaption>
<tableCaption confidence="0.939434">
Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation.
</tableCaption>
<bodyText confidence="0.999898421052632">
such as Ravichandran and Hovy (2002) and Pan-
tel and Pennacchiotti (2006) use the same formal-
ism of learning regular expressions over words and
part-of-speech tags to discover patterns indicating
a variety of relations.
More recent approaches have used deeper syn-
tactic information derived from parses of the input
sentences, including work exploiting syntactic de-
pendencies by Lin and Pantel (2001) and Snow et
al. (2005), and work in the ACE paradigm such
as Zhou et al. (2005) and Zhou et al. (2007).
Perhaps most similar to our distant supervision
algorithm is the effective method of Wu and Weld
(2007) who extract relations from a Wikipedia
page by using supervision from the page’s infobox.
Unlike their corpus-specific method, which is spe-
cific to a (single) Wikipedia page, our algorithm
allows us to extract evidence for a relation from
many different documents, and from any genre.
</bodyText>
<sectionHeader confidence="0.998081" genericHeader="method">
3 Freebase
</sectionHeader>
<bodyText confidence="0.99988824137931">
Following the literature, we use the term ‘rela-
tion’ to refer to an ordered, binary relation be-
tween entities. We refer to individual ordered pairs
in this relation as ‘relation instances’. For ex-
ample, the person-nationality relation holds be-
tween the entities named ‘John Steinbeck’ and
‘United States’, so it has (John Steinbeck,
United States) as an instance.
We use relations and relation instances from
Freebase, a freely available online database of
structured semantic data. Data in Freebase is
collected from a variety of sources. One major
source is text boxes and other tabular data from
Wikipedia. Data is also taken from NNDB (bio-
graphical information), MusicBrainz (music), the
SEC (financial and corporate data), as well as di-
rect, wiki-style user editing. After some basic
processing of the July 2008 link export to con-
vert Freebase’s data representation into binary re-
lations, we have 116 million instances of 7,300
relations between 9 million entities. We next fil-
ter out nameless and uninteresting entities such as
user profiles and music tracks. Freebase also con-
tains the reverses of many of its relations (book-
author v. author-book), and these are merged. Fil-
tering and removing all but the largest relations
leaves us with 1.8 million instances of 102 rela-
tions connecting 940,000 entities. Examples are
shown in Table 2.
</bodyText>
<sectionHeader confidence="0.992172" genericHeader="method">
4 Architecture
</sectionHeader>
<bodyText confidence="0.999898">
The intuition of our distant supervision approach
is to use Freebase to give us a training set of rela-
tions and entity pairs that participate in those rela-
tions. In the training step, all entities are identified
</bodyText>
<page confidence="0.95294">
1005
</page>
<bodyText confidence="0.992297759259259">
in sentences using a named entity tagger that la-
bels persons, organizations and locations. If a sen-
tence contains two entities and those entities are an
instance of one of our Freebase relations, features
are extracted from that sentence and are added to
the feature vector for the relation.
The distant supervision assumption is that if two
entities participate in a relation, any sentence that
contain those two entities might express that rela-
tion. Because any individual sentence may give
an incorrect cue, our algorithm trains a multiclass
logistic regression classifier, learning weights for
each noisy feature. In training, the features for
identical tuples (relation, entity1, entity2) from
different sentences are combined, creating a richer
feature vector.
In the testing step, entities are again identified
using the named entity tagger. This time, every
pair of entities appearing together in a sentence is
considered a potential relation instance, and when-
ever those entities appear together, features are ex-
tracted on the sentence and added to a feature vec-
tor for that entity pair. For example, if a pair of
entities occurs in 10 sentences in the test set, and
each sentence has 3 features extracted from it, the
entity pair will have 30 associated features. Each
entity pair in each sentence in the test corpus is run
through feature extraction, and the regression clas-
sifier predicts a relation name for each entity pair
based on the features from all of the sentences in
which it appeared.
Consider the location-contains relation, imag-
ining that in Freebase we had two instances of
this relation: (Virginia, Richmond) and
(France, Nantes). As we encountered sen-
tences like ‘Richmond, the capital of Virginia’ and
‘Henry’s Edict of Nantes helped the Protestants of
France’ we would extract features from these sen-
tences. Some features would be very useful, such
as the features from the Richmond sentence, and
some would be less useful, like those from the
Nantes sentence. In testing, if we came across
a sentence like ‘Vienna, the capital of Austria’,
one or more of its features would match those of
the Richmond sentence, providing evidence that
(Austria, Vienna) belongs to the location-
contains relation.
Note that one of the main advantages of our
architecture is its ability to combine informa-
tion from many different mentions of the same
relation. Consider the entity pair (Steven
Spielberg, Saving Private Ryan)
from the following two sentences, as evidence for
the film-director relation.
</bodyText>
<figure confidence="0.491953">
[Steven Spielberg]’s film [Saving Private
Ryan] is loosely based on the brothers’ story.
Allison co-produced the Academy Award-
winning [Saving Private Ryan], directed by
[Steven Spielberg]...
</figure>
<bodyText confidence="0.999815">
The first sentence, while providing evidence for
film-director, could instead be evidence for film-
writer orfilm-producer. The second sentence does
not mention that Saving Private Ryan is a film, and
so could instead be evidence for the CEO relation
(consider ‘Robert Mueller directed the FBI’). In
isolation, neither of these features is conclusive,
but in combination, they are.
</bodyText>
<sectionHeader confidence="0.999867" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.9999094">
Our features are based on standard lexical and syn-
tactic features from the literature. Each feature
describes how two entities are related in a sen-
tence, using either syntactic or non-syntactic in-
formation.
</bodyText>
<subsectionHeader confidence="0.987477">
5.1 Lexical features
</subsectionHeader>
<bodyText confidence="0.998003666666667">
Our lexical features describe specific words be-
tween and surrounding the two entities in the sen-
tence in which they appear:
</bodyText>
<listItem confidence="0.997653857142857">
• The sequence of words between the two entities
• The part-of-speech tags of these words
• A flag indicating which entity came first in the sentence
• A window of k words to the left of Entity 1 and their
part-of-speech tags
• A window of k words to the right of Entity 2 and their
part-of-speech tags
</listItem>
<bodyText confidence="0.923634125">
Each lexical feature consists of the conjunction of
all these components. We generate a conjunctive
feature for each k E 10, 1, 21. Thus each lexical
row in Table 3 represents a single lexical feature.
Part-of-speech tags were assigned by a max-
imum entropy tagger trained on the Penn Tree-
bank, and then simplified into seven categories:
nouns, verbs, adverbs, adjectives, numbers, for-
eign words, and everything else.
In an attempt to approximate syntactic features,
we also tested variations on our lexical features:
(1) omitting all words that are not verbs and (2)
omitting all function words. In combination with
the other lexical features, they gave a small boost
to precision, but not large enough to justify the in-
creased demand on our computational resources.
</bodyText>
<page confidence="0.829302">
1006
</page>
<table confidence="0.999894307692308">
Feature type Left window NE1 Middle NE2 Right window
Lexical [] PER [was/VERB born/VERB in/CLOSED] LOC []
Lexical [Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [,]
Lexical [#PAD#, Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [, Missouri]
Syntactic [] PER [fts was 4II,red born 4mod in 4pcomp−n] LOC []
Syntactic [Edwin Hubble 4lex−mod] PER [fts was 4IIpred born 4mod in 4pcomp−n] LOC []
Syntactic [Astronomer 4lex−mod] PER [fts was 4Ipred born 4mod in 4pcomp−n] LOC []
Syntactic [] PER [fts was 4IIpred born 4mod in 4IIpcomp−n] LOC [4lex−mod ,]
Syntactic [Edwin Hubble 4lex−mod] PER [fts was 4Ipred born 4mod in 4IIpcomp−n] LOC [4IIlex−mod ,]
Syntactic [Astronomer 4lex−mod] PER [fts was 4IIpred born 4IImod in 4IIpcomp−n] LOC [41ex−mod ,]
Syntactic [] PER [fts was 4Ipred born 4mod in 4IIpcomp−n] LOC [4IIinside Missouri]
Syntactic [Edwin Hubble 4lex−mod] PER [fts was 4IIpred born 4mod in 4IIpcomp−n] LOC [4inside Missouri]
Syntactic [Astronomer 4lex−mod] PER [fts was 4pred born 4mod in 4pcomp−n] LOC [4inside Missouri]
</table>
<tableCaption confidence="0.85975">
Table 3: Features for ‘Astronomer Edwin Hubble was born in Marshfield, Missouri’.
</tableCaption>
<figure confidence="0.64506">
inside
</figure>
<figureCaption confidence="0.9511125">
Figure 1: Dependency parse with dependency path from ‘Edwin Hubble’ to ‘Marshfield’ highlighted in
boldface.
</figureCaption>
<figure confidence="0.817536714285714">
lex-mod s pred mod pcomp-n lex-mod
Astronomer
Edwin Hubble
Marshfield
Missouri
,
was born in
</figure>
<subsectionHeader confidence="0.9911">
5.2 Syntactic features
</subsectionHeader>
<bodyText confidence="0.957058888888889">
In addition to lexical features we extract a num-
ber of features based on syntax. In order to gener-
ate these features we parse each sentence with the
broad-coverage dependency parser MINIPAR (Lin,
1998).
A dependency parse consists of a set of words
and chunks (e.g. ‘Edwin Hubble’, ‘Missouri’,
‘born’), linked by directional dependencies (e.g.
‘pred’, ‘lex-mod’), as in Figure 1. For each
sentence we extract a dependency path between
each pair of entities. A dependency path con-
sists of a series of dependencies, directions and
words/chunks representing a traversal of the parse.
Part-of-speech tags are not included in the depen-
dency path.
Our syntactic features are similar to those used
in Snow et al. (2005). They consist of the conjunc-
tion of:
</bodyText>
<listItem confidence="0.921091">
• A dependency path between the two entities
• For each entity, one ‘window’ node that is not part of
the dependency path
</listItem>
<bodyText confidence="0.999917285714286">
A window node is a node connected to one of the
two entities and not part of the dependency path.
We generate one conjunctive feature for each pair
of left and right window nodes, as well as features
which omit one or both of them. Thus each syn-
tactic row in Table 3 represents a single syntactic
feature.
</bodyText>
<subsectionHeader confidence="0.991705">
5.3 Named entity tag features
</subsectionHeader>
<bodyText confidence="0.999547285714286">
Every feature contains, in addition to the content
described above, named entity tags for the two en-
tities. We perform named entity tagging using the
Stanford four-class named entity tagger (Finkel et
al., 2005). The tagger provides each word with a
label from {person, location, organization, miscel-
laneous, none}.
</bodyText>
<subsectionHeader confidence="0.991799">
5.4 Feature conjunction
</subsectionHeader>
<bodyText confidence="0.999987571428571">
Rather than use each of the above features in the
classifier independently, we use only conjunctive
features. Each feature consists of the conjunc-
tion of several attributes of the sentence, plus the
named entity tags. For two features to match,
all of their conjuncts must match exactly. This
yields low-recall but high-precision features. With
a small amount of data, this approach would be
problematic, since most features would only be
seen once, rendering them useless to the classifier.
Since we use large amounts of data, even complex
features appear multiple times, allowing our high-
precision features to work as intended. Features
for a sample sentence are shown in Table 3.
</bodyText>
<sectionHeader confidence="0.998195" genericHeader="method">
6 Implementation
</sectionHeader>
<subsectionHeader confidence="0.978005">
6.1 Text
</subsectionHeader>
<bodyText confidence="0.999786">
For unstructured text we use the Freebase
Wikipedia Extraction, a dump of the full text of all
Wikipedia articles (not including discussion and
</bodyText>
<page confidence="0.968359">
1007
</page>
<table confidence="0.999721515151515">
Relation Feature type Left window NE1 Middle NE2 Right window
/architecture/structure/architect LEX-n ORG , the designer of the PER
SYN designed ⇑s ORG ⇑s designed ⇓by−s„bj by ⇓pcn PER ⇑s designed
/book/author/works written LEX PER s novel ORG
SYN PER ⇑pcn by ⇑mod story ⇑pred is ⇓s ORG
/book/book edition/author editor LEX-n ORG s novel PER
SYN PER ⇑nn series ⇓gen PER
/business/company/founders LEX ORG co - founder PER
SYN ORG ⇑nn owner ⇓person PER
/business/company/place founded LEX-n ORG - based LOC
SYN ORG ⇑s founded ⇓mod in ⇓pcn LOC
/film/film/country LEX PER , released in LOC
SYN opened ⇑s ORG ⇑s opened ⇓mod in ⇓pcn LOC ⇑s opened
/geography/river/mouth LEX LOC , which flows into the LOC
SYN the ⇓det LOC ⇑s is ⇓pred tributary ⇓mod of ⇓pcn LOC ⇓det the
/government/political party/country LEX-n ORG politician of the LOC
SYN candidate ⇑nn ORG ⇑nn candidate ⇓mod for ⇓pcn LOC ⇑nn candidate
/influence/influence node/influenced LEX-n PER , a student of PER
SYN of ⇑pcn PER ⇑pcn of ⇑mod student ⇑appo PER ⇑pcn of
/language/human language/region LEX LOC - speaking areas of LOC
SYN LOC ⇑�e.−mod speaking areas ⇓mod of ⇓pcn LOC
/music/artist/origin LEX-n ORG based band LOC
SYN is ⇑s ORG ⇑s is ⇓pred band ⇓mod from ⇓pcn LOC ⇑s is
/people/deceased person/place of death LEX PER died in LOC
SYN hanged ⇑s PER ⇑s hanged ⇓mod in ⇓pcn LOC ⇑s hanged
/people/person/nationality LEX PER is a citizen of LOC
SYN PER ⇓mod from ⇓pcn LOC
/people/person/parents LEX PER , son of PER
SYN father ⇑gen PER ⇑gen father ⇓person PER ⇑gen father
/people/person/place of birth LEX-n PER is the birthplace of PER
SYN PER ⇑s born ⇓mod in ⇓pcn LOC
/people/person/religion LEX PER embraced LOC
SYN convert ⇓appo PER ⇓appo convert ⇓mod to ⇓pcn LOC ⇓appo convert
</table>
<tableCaption confidence="0.9507165">
Table 4: Examples of high-weight features for several relations. Key: SYN = syntactic feature; LEX =
lexical feature; .r-� = reversed; NE# = named entity tag of entity.
</tableCaption>
<bodyText confidence="0.9999541875">
user pages) which has been sentence-tokenized by
Metaweb Technologies, the developers of Free-
base (Metaweb, 2008). This dump consists of
approximately 1.8 million articles, with an av-
erage of 14.3 sentences per article. The total
number of words (counting punctuation marks) is
601,600,703. For our experiments we use about
half of the articles: 800,000 for training and
400,000 for testing.
We use Wikipedia because it is relatively up-
to-date, and because its sentences tend to make
explicit many facts that might be omitted in
newswire. Much of the information in Freebase is
derived from tabular data from Wikipedia, mean-
ing that Freebase relations are more likely to ap-
pear in sentences in Wikipedia.
</bodyText>
<subsectionHeader confidence="0.99984">
6.2 Parsing and chunking
</subsectionHeader>
<bodyText confidence="0.970756583333333">
Each sentence of this unstructured text is depen-
dency parsed by MINIPAR to produce a depen-
dency graph.
In preprocessing, consecutive words with the
same named entity tag are ‘chunked’, so that
Edwin/PERSON Hubble/PERSON becomes
[Edwin Hubble]/PERSON. This chunking is
restricted by the dependency parse of the sentence,
however, in that chunks must be contiguous in
the parse (i.e., no chunks across subtrees). This
ensures that parse tree structure is preserved, since
the parses must be updated to reflect the chunking.
</bodyText>
<subsectionHeader confidence="0.995265">
6.3 Training and testing
</subsectionHeader>
<bodyText confidence="0.999954041666667">
For held-out evaluation experiments (see section
7.1), half of the instances of each relation are not
used in training, and are later used to compare
against newly discovered instances. This means
that 900,000 Freebase relation instances are used
in training, and 900,000 are held out. These ex-
periments used 800,000 Wikipedia articles in the
training phase and 400,000 different articles in the
testing phase.
For human evaluation experiments, all 1.8 mil-
lion relation instances are used in training. Again,
we use 800,000 Wikipedia articles in the training
phase and 400,000 different articles in the testing
phase.
For all our experiments, we only extract relation
instances that do not appear in our training data,
i.e., instances that are not already in Freebase.
Our system needs negative training data for the
purposes of constructing the classifier. Towards
this end, we build a feature vector in the train-
ing phase for an ‘unrelated’ relation by randomly
selecting entity pairs that do not appear in any
Freebase relation and extracting features for them.
While it is possible that some of these entity pairs
</bodyText>
<page confidence="0.972693">
1008
</page>
<figure confidence="0.994229">
1
Precision
0.6
0.4
0.3
0.2
0.9
0.8
0.7
0.5
Both
Syntax
Surface
0.1
0
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
Oracle recall
</figure>
<figureCaption confidence="0.909812">
Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training
</figureCaption>
<bodyText confidence="0.998537153846154">
on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic
features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify
most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and
10% as person-nationality.
are in fact related but are wrongly omitted from
the Freebase data, we expect that on average these
false negatives will have a small effect on the per-
formance of the classifier. For performance rea-
sons, we randomly sample 1% of such entity pairs
for use as negative training examples. By contrast,
in the actual test data, 98.7% of the entity pairs we
extract do not possess any of the top 102 relations
we consider in Freebase.
We use a multi-class logistic classifier opti-
mized using L-BFGS with Gaussian regulariza-
tion. Our classifier takes as input an entity pair
and a feature vector, and returns a relation name
and a confidence score based on the probability of
the entity pair belonging to that relation. Once all
of the entity pairs discovered during testing have
been classified, they can be ranked by confidence
score and used to generate a list of the n most
likely new relation instances.
Table 4 shows some high-weight features
learned by our system. We discuss the results in
the next section.
</bodyText>
<sectionHeader confidence="0.996256" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999971333333333">
We evaluate labels in two ways: automatically,
by holding out part of the Freebase relation data
during training, and comparing newly discovered
relation instances against this held-out data, and
manually, having humans who look at each posi-
tively labeled entity pair and mark whether the re-
lation indeed holds between the participants. Both
evaluations allow us to calculate the precision of
the system for the best N instances.
</bodyText>
<subsectionHeader confidence="0.997837">
7.1 Held-out evaluation
</subsectionHeader>
<bodyText confidence="0.9999148">
Figure 2 shows the performance of our classifier
on held-out Freebase relation data. While held-out
evaluation suffers from false negatives, it gives a
rough measure of precision without requiring ex-
pensive human evaluation, making it useful for pa-
rameter setting.
At most recall levels, the combination of syn-
tactic and lexical features offers a substantial im-
provement in precision over either of these feature
sets on its own.
</bodyText>
<subsectionHeader confidence="0.999728">
7.2 Human evaluation
</subsectionHeader>
<bodyText confidence="0.999985727272727">
Human evaluation was performed by evaluators on
Amazon’s Mechanical Turk service, shown to be
effective for natural language annotation in Snow
et al. (2008). We ran three experiments: one us-
ing only syntactic features; one using only lexical
features; and one using both syntactic and lexical
features. For each of the 10 relations that appeared
most frequently in our test data (according to our
classifier), we took samples from the first 100 and
1000 instances of this relation generated in each
experiment, and sent these to Mechanical Turk for
</bodyText>
<page confidence="0.955763">
1009
</page>
<table confidence="0.999978076923077">
Relation name 100 instances Both 1000 instances Both
Syn Lex Syn Lex
/film/director/film 0.49 0.43 0.44 0.49 0.41 0.46
/film/writer/film 0.70 0.60 0.65 0.71 0.61 0.69
/geography/river/basin countries 0.65 0.64 0.67 0.73 0.71 0.64
/location/country/administrative divisions 0.68 0.59 0.70 0.72 0.68 0.72
/location/location/contains 0.81 0.89 0.84 0.85 0.83 0.84
/location/us county/county seat 0.51 0.51 0.53 0.47 0.57 0.42
/music/artist/origin 0.64 0.66 0.71 0.61 0.63 0.60
/people/deceased person/place of death 0.80 0.79 0.81 0.80 0.81 0.78
/people/person/nationality 0.61 0.70 0.72 0.56 0.61 0.63
/people/person/place of birth 0.78 0.77 0.78 0.88 0.85 0.91
Average 0.67 0.66 0.69 0.68 0.67 0.67
</table>
<tableCaption confidence="0.991733">
Table 5: Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000
</tableCaption>
<bodyText confidence="0.973344157894737">
results per relation, using stratified samples. ‘Average’ gives the mean precision of the 10 relations. Key:
Syn = syntactic features only. Lex = lexical features only. We use stratified samples because of the
overabundance of location-contains instances among our high-confidence results.
human evaluation. Our sample size was 100.
Each predicted relation instance was labeled as
true or false by between 1 and 3 labelers on Me-
chanical Turk. We assigned the truth or falsehood
of each relation according to the majority vote of
the labels; in the case of a tie (one vote each way)
we assigned the relation as true or false with equal
probability. The evaluation of the syntactic, lexi-
cal, and combination of features at a recall of 100
and 1000 instances is presented in Table 5.
At a recall of 100 instances, the combination of
lexical and syntactic features has the best perfor-
mance for a majority of the relations, while at a re-
call level of 1000 instances the results are mixed.
No feature set strongly outperforms any of the oth-
ers across all relations.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.998340340909091">
Our results show that the distant supervision algo-
rithm is able to extract high-precision patterns for
a reasonably large number of relations.
The held-out results in Figure 2 suggest that the
combination of syntactic and lexical features pro-
vides better performance than either feature set on
its own. In order to understand the role of syntactic
features, we examine Table 5, the human evalua-
tion of the most frequent 10 relations. For the top-
ranking 100 instances of each relation, most of the
best results use syntactic features, either alone or
in combination with lexical features. For the top-
ranking 1000 instances of each relation, the results
are more mixed, but syntactic features still helped
in most classifications.
We then examine those relations for which syn-
tactic features seem to help. For example, syn-
tactic features consistently outperform lexical fea-
tures for the director-film and writer-film relations.
As discussed in section 4, these two relations are
particularly ambiguous, suggesting that syntactic
features may help tease apart difficult relations.
Perhaps more telling, we noticed many examples
with a long string of words between the director
and the film:
Back Street is a 1932 film made by Univer-
sal Pictures, directed by John M. Stahl, and
produced by Carl Laemmle Jr.
Sentences like this have very long (and thus rare)
lexical features, but relatively short dependency
paths. Syntactic features can more easily abstract
from the syntactic modifiers that comprise the ex-
traneous parts of these strings.
Our results thus suggest that syntactic features
are indeed useful in distantly supervised informa-
tion extraction, and that the benefit of syntax oc-
curs in cases where the individual patterns are par-
ticularly ambiguous, and where they are nearby in
the dependency structure but distant in terms of
words. It remains for future work to see whether
simpler, chunk-based syntactic features might be
able to capture enough of this gain without the
overhead of full parsing, and whether coreference
resolution could improve performance.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999930571428571">
We would like to acknowledge Sarah Spikes for
her help in developing the relation extraction sys-
tem, Christopher Manning and Mihai Surdeanu
for their invaluable advice, and Fuliang Weng
and Baoshi Yan for their guidance. Our research
was partially funded by the NSF via award IIS-
0811974 and by Robert Bosch LLC.
</bodyText>
<page confidence="0.990955">
1010
</page>
<sectionHeader confidence="0.996211" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999771796460177">
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM Interna-
tional Conference on Digital Libraries.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In
Manuela M Veloso, editor, IJCAI-07, pages 2670–
2676.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD ’08, pages 1247–
1250, New York, NY. ACM.
Sergei Brin. 1998. Extracting patterns and relations
from the World Wide Web. In Proceedings World
Wide Web and Databases International Workshop,
Number 1590 in LNCS, pages 172–183. Springer.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
supervision. In ACL-07, pages 576–583, Prague,
Czech Republic, June.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Thomas Lengauer, Rein-
hard Schneider, Peer Bork, Douglas L. Brutlag, Jan-
ice I. Glasgow, Hans W. Mewes, and Ralf Zimmer,
editors, ISMB, pages 77–86. AAAI.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) Program–Tasks, Data, and Evaluation.
LREC-04, pages 837–840.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91–134.
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL-05, pages 363–370, Ann Arbor,
MI.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In HLT-
NAACL-03, pages 1–8, Edmonton, Canada.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING-92,
Nantes, France.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Natural Lan-
guage Engineering, 7(4):343–360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Workshop on the Evaluation of Parsing
Systems.
Metaweb. 2008. Freebase data dumps. http://
download.freebase.Com/datadumps/.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh, and Jeff B. Colombe.
2004. Gene name identification and normalization
using a model organism database. J. of Biomedical
Informatics, 37(6):396–410.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for auto-
matically harvesting semantic relations. In COL-
ING/ACL 2006, pages 113–120, Sydney, Australia.
Marco Pennacchiotti and Patrick Pantel. 2006. A boot-
strapping algorithm for automatically harvesting se-
mantic relations. In in Proceedings of Inference in
Computational Semantics (ICoS-06), pages 87–96.
Deepak Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. In ACL-02, pages 41–47, Philadelphia,
PA.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI-99, pages 474–479.
Benjamin Rozenfeld and Ronen Feldman. 2008. Self-
supervised relation extraction from the web. Knowl-
edge and Information Systems, 17(1):17–33.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In HLT-NAACL-06, pages 304–311,
New York, NY.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L´eon Bottou, editors, NIPS 17, pages 1297–1304.
MIT Press.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast – but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In EMNLP 2008, pages 254–263, Hon-
olulu, HI.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
Proceedings of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), March.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In CIKM ’07: Proceedings
of the sixteenth ACM conference on Conference on
information and knowledge management, pages 41–
50, Lisbon, Portugal.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In ACL-05, pages 427–434, Ann Arbor,
MI.
Guodong Zhou, Min Zhang, Donghong Ji, and Qiaom-
ing Zhu. 2007. Tree kernel-based relation extrac-
tion with context-sensitive structured parse tree in-
formation. In EMNLP/CoNLL 2007.
</reference>
<page confidence="0.993196">
1011
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.645058">
<title confidence="0.99369">Distant supervision for relation extraction without labeled data</title>
<author confidence="0.998914">Mike Mintz</author>
<author confidence="0.998914">Steven Bills</author>
<author confidence="0.998914">Rion Snow</author>
<author confidence="0.998914">Dan Jurafsky</author>
<affiliation confidence="0.655713">Stanford University / Stanford, CA 94305</affiliation>
<abstract confidence="0.999565869565218">Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th ACM International Conference on Digital Libraries.</booktitle>
<contexts>
<context position="3230" citStr="Agichtein and Gravano, 2000" startWordPosition="499" endWordPosition="502">approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005)</context>
<context position="7312" citStr="Agichtein and Gravano, 2000" startWordPosition="1120" endWordPosition="1123">tances extracted by our system. We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction. While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE. Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations. Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have be</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the 5th ACM International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>IJCAI-07,</booktitle>
<pages>2670--2676</pages>
<editor>In Manuela M Veloso, editor,</editor>
<contexts>
<context position="2848" citStr="Banko et al., 2007" startWordPosition="433" endWordPosition="436">ions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007). Supervised relation extraction suffers from a number of problems, however. Labeled training data is expensive to produce and thus limited in quantity. Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, wh</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Manuela M Veloso, editor, IJCAI-07, pages 2670– 2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In SIGMOD ’08,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="4833" citStr="Bollacker et al., 2008" startWordPosition="729" endWordPosition="732">cation/location/contains Paris, Montmartre /location/location/contains Ontario, Fort Erie /music/artist/origin Mighty Wagon, Cincinnati /people/deceased person/place of death Fyodor Kamensky, Clearwater /people/person/nationality Marianne Yvonne Heemskerk, Netherlands /people/person/place of birth Wavell Wayne Hinds, Kingston /book/author/works written Upton Sinclair, Lanny Budd /business/company/founders WWE, Vince McMahon /people/person/profession Thomas Mellon, judge Table 1: Ten relation instances extracted by our system that did not appear in Freebase. 2004). Our algorithm uses Freebase (Bollacker et al., 2008), a large semantic database, to provide distant supervision for relation extraction. Freebase contains 116 million instances of 7,300 relations between 9 million entities. The intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way. Since there may be many sentences containing a given entity pair, we can extract very large numbers of (potentially noisy) features that are combined in a logistic regression classifier. Thus whereas the supervised training paradigm uses a sma</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD ’08, pages 1247– 1250, New York, NY. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Brin</author>
</authors>
<title>Extracting patterns and relations from the World Wide Web.</title>
<date>1998</date>
<booktitle>In Proceedings World Wide Web and Databases International Workshop, Number 1590 in LNCS,</booktitle>
<pages>172--183</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3177" citStr="Brin, 1998" startWordPosition="493" endWordPosition="494">rd that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an</context>
<context position="7283" citStr="Brin, 1998" startWordPosition="1118" endWordPosition="1119">relation instances extracted by our system. We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction. While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE. Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations. Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Dod</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergei Brin. 1998. Extracting patterns and relations from the World Wide Web. In Proceedings World Wide Web and Databases International Workshop, Number 1590 in LNCS, pages 172–183. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In ACL-07,</booktitle>
<pages>576--583</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3339" citStr="Bunescu and Mooney, 2007" startWordPosition="515" endWordPosition="518">s of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of w</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In ACL-07, pages 576–583, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<pages>77--86</pages>
<editor>In Thomas Lengauer, Reinhard Schneider, Peer Bork, Douglas L. Brutlag, Janice I. Glasgow, Hans W. Mewes, and Ralf Zimmer, editors, ISMB,</editor>
<publisher>AAAI.</publisher>
<contexts>
<context position="4001" citStr="Craven and Kumlien, 1999" startWordPosition="626" endWordPosition="629">seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of weakly labeled data in bioinformatics (Craven and Kumlien, 1999; Morgan et al., 1003 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1003–1011, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP Relation name New instance /location/location/contains Paris, Montmartre /location/location/contains Ontario, Fort Erie /music/artist/origin Mighty Wagon, Cincinnati /people/deceased person/place of death Fyodor Kamensky, Clearwater /people/person/nationality Marianne Yvonne Heemskerk, Netherlands /people/person/place of birth Wavell Wayne Hinds, Kingston /book/author/works written Upton Sinclair, Lanny Budd /business</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Thomas Lengauer, Reinhard Schneider, Peer Bork, Douglas L. Brutlag, Janice I. Glasgow, Hans W. Mewes, and Ralf Zimmer, editors, ISMB, pages 77–86. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<date>2004</date>
<booktitle>The Automatic Content Extraction (ACE) Program–Tasks, Data, and Evaluation. LREC-04,</booktitle>
<pages>837--840</pages>
<contexts>
<context position="7904" citStr="Doddington et al., 2004" startWordPosition="1216" endWordPosition="1219">998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations. Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words. Many early algorithms for relation extraction used little or no syntactic information. For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the SNOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags. Hearst (1992) used a small number of regular expressions over words and part-of-speech tags to find examples of</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content Extraction (ACE) Program–Tasks, Data, and Evaluation. LREC-04, pages 837–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="3281" citStr="Etzioni et al., 2005" startWordPosition="507" endWordPosition="510">acts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) </context>
<context position="7335" citStr="Etzioni et al., 2005" startWordPosition="1124" endWordPosition="1127">m. We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction. While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE. Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations. Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their </context>
<context position="8640" citStr="Etzioni et al. (2005)" startWordPosition="1332" endWordPosition="1335">n words. Many early algorithms for relation extraction used little or no syntactic information. For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the SNOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags. Hearst (1992) used a small number of regular expressions over words and part-of-speech tags to find examples of the hypernym relation. The use of these patterns has been widely replicated in successful systems, for example by Etzioni et al. (2005). Other work 1004 Relation name Size Example /people/person/nationality 281,107 John Dugard, South Africa /location/location/contains 253,223 Belgium, Nijlen /people/person/profession 208,888 Dusa McDuff, Mathematician /people/person/place of birth 105,799 Edwin Hubble, Marshfield /dining/restaurant/cuisine 86,213 MacAyo’s Mexican Kitchen, Mexican /business/business chain/location 66,529 Apple Inc., Apple Inc., South Park, NC /biology/organism classification rank 42,806 Scorpaeniformes, Order /film/film/genre 40,658 Where the Sidewalk Ends, Film noir /film/film/language 31,103 Enter the Phoeni</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL-05,</booktitle>
<pages>363--370</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="19986" citStr="Finkel et al., 2005" startWordPosition="3088" endWordPosition="3091">r each entity, one ‘window’ node that is not part of the dependency path A window node is a node connected to one of the two entities and not part of the dependency path. We generate one conjunctive feature for each pair of left and right window nodes, as well as features which omit one or both of them. Thus each syntactic row in Table 3 represents a single syntactic feature. 5.3 Named entity tag features Every feature contains, in addition to the content described above, named entity tags for the two entities. We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005). The tagger provides each word with a label from {person, location, organization, miscellaneous, none}. 5.4 Feature conjunction Rather than use each of the above features in the classifier independently, we use only conjunctive features. Each feature consists of the conjunction of several attributes of the sentence, plus the named entity tags. For two features to match, all of their conjuncts must match exactly. This yields low-recall but high-precision features. With a small amount of data, this approach would be problematic, since most features would only be seen once, rendering them useles</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny R. Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL-05, pages 363–370, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In HLTNAACL-03,</booktitle>
<pages>1--8</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="7820" citStr="Girju et al., 2003" startWordPosition="1201" endWordPosition="1204">xical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations. Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words. Many early algorithms for relation extraction used little or no syntactic information. For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the SNOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags. Hearst (1992) used a small </context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In HLTNAACL-03, pages 1–8, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING-92,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="8406" citStr="Hearst (1992)" startWordPosition="1296" endWordPosition="1297">elation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words. Many early algorithms for relation extraction used little or no syntactic information. For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the SNOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags. Hearst (1992) used a small number of regular expressions over words and part-of-speech tags to find examples of the hypernym relation. The use of these patterns has been widely replicated in successful systems, for example by Etzioni et al. (2005). Other work 1004 Relation name Size Example /people/person/nationality 281,107 John Dugard, South Africa /location/location/contains 253,223 Belgium, Nijlen /people/person/profession 208,888 Dusa McDuff, Mathematician /people/person/place of birth 105,799 Edwin Hubble, Marshfield /dining/restaurant/cuisine 86,213 MacAyo’s Mexican Kitchen, Mexican /business/busine</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING-92, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question-answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="10611" citStr="Lin and Pantel (2001)" startWordPosition="1566" endWordPosition="1569">nce fiction /film/film/music 14,070 Stavisky, Stephen Sondheim /business/company/industry 13,805 ATS Medical, Health care Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation. such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007). Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision from the page’s infobox. Unlike their corpus-specific method, which is specific to a (single) Wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre. 3 Freebase Following the literature, we use the term ‘relation’ to refer to an ordered, binary relation be</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question-answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="18768" citStr="Lin, 1998" startWordPosition="2881" endWordPosition="2882">ntactic [Astronomer 4lex−mod] PER [fts was 4pred born 4mod in 4pcomp−n] LOC [4inside Missouri] Table 3: Features for ‘Astronomer Edwin Hubble was born in Marshfield, Missouri’. inside Figure 1: Dependency parse with dependency path from ‘Edwin Hubble’ to ‘Marshfield’ highlighted in boldface. lex-mod s pred mod pcomp-n lex-mod Astronomer Edwin Hubble Marshfield Missouri , was born in 5.2 Syntactic features In addition to lexical features we extract a number of features based on syntax. In order to generate these features we parse each sentence with the broad-coverage dependency parser MINIPAR (Lin, 1998). A dependency parse consists of a set of words and chunks (e.g. ‘Edwin Hubble’, ‘Missouri’, ‘born’), linked by directional dependencies (e.g. ‘pred’, ‘lex-mod’), as in Figure 1. For each sentence we extract a dependency path between each pair of entities. A dependency path consists of a series of dependencies, directions and words/chunks representing a traversal of the parse. Part-of-speech tags are not included in the dependency path. Our syntactic features are similar to those used in Snow et al. (2005). They consist of the conjunction of: • A dependency path between the two entities • For </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Metaweb</author>
</authors>
<title>Freebase data dumps.</title>
<date>2008</date>
<note>http:// download.freebase.Com/datadumps/.</note>
<contexts>
<context position="22998" citStr="Metaweb, 2008" startWordPosition="3588" endWordPosition="3589">ER ⇓mod from ⇓pcn LOC /people/person/parents LEX PER , son of PER SYN father ⇑gen PER ⇑gen father ⇓person PER ⇑gen father /people/person/place of birth LEX-n PER is the birthplace of PER SYN PER ⇑s born ⇓mod in ⇓pcn LOC /people/person/religion LEX PER embraced LOC SYN convert ⇓appo PER ⇓appo convert ⇓mod to ⇓pcn LOC ⇓appo convert Table 4: Examples of high-weight features for several relations. Key: SYN = syntactic feature; LEX = lexical feature; .r-� = reversed; NE# = named entity tag of entity. user pages) which has been sentence-tokenized by Metaweb Technologies, the developers of Freebase (Metaweb, 2008). This dump consists of approximately 1.8 million articles, with an average of 14.3 sentences per article. The total number of words (counting punctuation marks) is 601,600,703. For our experiments we use about half of the articles: 800,000 for training and 400,000 for testing. We use Wikipedia because it is relatively upto-date, and because its sentences tend to make explicit many facts that might be omitted in newswire. Much of the information in Freebase is derived from tabular data from Wikipedia, meaning that Freebase relations are more likely to appear in sentences in Wikipedia. 6.2 Pars</context>
</contexts>
<marker>Metaweb, 2008</marker>
<rawString>Metaweb. 2008. Freebase data dumps. http:// download.freebase.Com/datadumps/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander A Morgan</author>
<author>Lynette Hirschman</author>
<author>Marc Colosimo</author>
<author>Alexander S Yeh</author>
<author>Jeff B Colombe</author>
</authors>
<title>Gene name identification and normalization using a model organism database.</title>
<date>2004</date>
<journal>J. of Biomedical Informatics,</journal>
<volume>37</volume>
<issue>6</issue>
<marker>Morgan, Hirschman, Colosimo, Yeh, Colombe, 2004</marker>
<rawString>Alexander A. Morgan, Lynette Hirschman, Marc Colosimo, Alexander S. Yeh, and Jeff B. Colombe. 2004. Gene name identification and normalization using a model organism database. J. of Biomedical Informatics, 37(6):396–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In COLING/ACL 2006,</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="10285" citStr="Pantel and Pennacchiotti (2006)" startWordPosition="1516" endWordPosition="1520">us Project, Austin /people/person/religion 17,582 Joseph Chartrand, Catholicism /book/author/works written 17,278 Paul Auster, Travels in the Scriptorium /soccer/football position/players 17,244 Midfielder, Chen Tao /people/deceased person/cause of death 16,709 Richard Daintree, Tuberculosis /book/book/genre 16,431 Pony Soldiers, Science fiction /film/film/music 14,070 Stavisky, Stephen Sondheim /business/company/industry 13,805 ATS Medical, Health care Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation. such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007). Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision </context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: leveraging generic patterns for automatically harvesting semantic relations. In COLING/ACL 2006, pages 113–120, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>A bootstrapping algorithm for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In in Proceedings of Inference in Computational Semantics (ICoS-06),</booktitle>
<pages>87--96</pages>
<contexts>
<context position="3313" citStr="Pennacchiotti and Pantel, 2006" startWordPosition="511" endWordPosition="514">between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and </context>
</contexts>
<marker>Pennacchiotti, Pantel, 2006</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2006. A bootstrapping algorithm for automatically harvesting semantic relations. In in Proceedings of Inference in Computational Semantics (ICoS-06), pages 87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard H Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In ACL-02,</booktitle>
<pages>41--47</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3259" citStr="Ravichandran and Hovy, 2002" startWordPosition="503" endWordPosition="506"> information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to ex</context>
<context position="10249" citStr="Ravichandran and Hovy (2002)" startWordPosition="1511" endWordPosition="1514">ic/artist/origin 18,619 The Octopus Project, Austin /people/person/religion 17,582 Joseph Chartrand, Catholicism /book/author/works written 17,278 Paul Auster, Travels in the Scriptorium /soccer/football position/players 17,244 Midfielder, Chen Tao /people/deceased person/cause of death 16,709 Richard Daintree, Tuberculosis /book/book/genre 16,431 Pony Soldiers, Science fiction /film/film/music 14,070 Stavisky, Stephen Sondheim /business/company/industry 13,805 ATS Medical, Health care Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation. such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007). Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a </context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard H. Hovy. 2002. Learning surface text patterns for a question answering system. In ACL-02, pages 41–47, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In AAAI-99,</booktitle>
<pages>474--479</pages>
<contexts>
<context position="3201" citStr="Riloff and Jones, 1999" startWordPosition="495" endWordPosition="498"> domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradi</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In AAAI-99, pages 474–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rozenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>Selfsupervised relation extraction from the web.</title>
<date>2008</date>
<journal>Knowledge and Information Systems,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="3369" citStr="Rozenfeld and Feldman, 2008" startWordPosition="519" endWordPosition="522">d simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of weakly labeled data in bioinfor</context>
</contexts>
<marker>Rozenfeld, Feldman, 2008</marker>
<rawString>Benjamin Rozenfeld and Ronen Feldman. 2008. Selfsupervised relation extraction from the web. Knowledge and Information Systems, 17(1):17–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In HLT-NAACL-06,</booktitle>
<pages>304--311</pages>
<location>New York, NY.</location>
<contexts>
<context position="2827" citStr="Shinyama and Sekine, 2006" startWordPosition="429" endWordPosition="432">lly combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007). Supervised relation extraction suffers from a number of problems, however. Labeled training data is expensive to produce and thus limited in quantity. Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base. A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a ne</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In HLT-NAACL-06, pages 304–311, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery. In</title>
<date>2005</date>
<journal>NIPS</journal>
<volume>17</volume>
<pages>1297--1304</pages>
<editor>Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3830" citStr="Snow et al. (2005)" startWordPosition="597" endWordPosition="600"> and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008). These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion. The resulting patterns often suffer from low precision and semantic drift. We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches. Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of weakly labeled data in bioinformatics (Craven and Kumlien, 1999; Morgan et al., 1003 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1003–1011, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP Relation name New instance /location/location/contains Paris, Montmartre /location/location/contains Ontario, Fort Erie /music/artist/origin Mighty Wagon, Cincinnati /people/deceased person/place of death Fyodor Kamensky, Clearwater /people/person/na</context>
<context position="7840" citStr="Snow et al., 2005" startWordPosition="1205" endWordPosition="1208">eby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations. Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words. Many early algorithms for relation extraction used little or no syntactic information. For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the SNOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags. Hearst (1992) used a small number of regular ex</context>
<context position="10634" citStr="Snow et al. (2005)" startWordPosition="1571" endWordPosition="1574">ic 14,070 Stavisky, Stephen Sondheim /business/company/industry 13,805 ATS Medical, Health care Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation. such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007). Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision from the page’s infobox. Unlike their corpus-specific method, which is specific to a (single) Wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre. 3 Freebase Following the literature, we use the term ‘relation’ to refer to an ordered, binary relation between entities. We refe</context>
<context position="19279" citStr="Snow et al. (2005)" startWordPosition="2961" endWordPosition="2964">enerate these features we parse each sentence with the broad-coverage dependency parser MINIPAR (Lin, 1998). A dependency parse consists of a set of words and chunks (e.g. ‘Edwin Hubble’, ‘Missouri’, ‘born’), linked by directional dependencies (e.g. ‘pred’, ‘lex-mod’), as in Figure 1. For each sentence we extract a dependency path between each pair of entities. A dependency path consists of a series of dependencies, directions and words/chunks representing a traversal of the parse. Part-of-speech tags are not included in the dependency path. Our syntactic features are similar to those used in Snow et al. (2005). They consist of the conjunction of: • A dependency path between the two entities • For each entity, one ‘window’ node that is not part of the dependency path A window node is a node connected to one of the two entities and not part of the dependency path. We generate one conjunctive feature for each pair of left and right window nodes, as well as features which omit one or both of them. Thus each syntactic row in Table 3 represents a single syntactic feature. 5.3 Named entity tag features Every feature contains, in addition to the content described above, named entity tags for the two entiti</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, NIPS 17, pages 1297–1304. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<pages>254--263</pages>
<location>Honolulu, HI.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In EMNLP 2008, pages 254–263, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>Robust information extraction with perceptrons.</title>
<date>2007</date>
<booktitle>In Proceedings of the NIST 2007 Automatic Content Extraction Workshop (ACE07),</booktitle>
<contexts>
<context position="2302" citStr="Surdeanu and Ciaramita, 2007" startWordPosition="352" endWordPosition="356">he presence of entities and the relations between them. The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances. ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007). Supervised relation extraction suffers from a number of problems, however. Labeled training data is expensive to produce and thus limited in quantity. Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approaches can use very large amounts o</context>
</contexts>
<marker>Surdeanu, Ciaramita, 2007</marker>
<rawString>Mihai Surdeanu and Massimiliano Ciaramita. 2007. Robust information extraction with perceptrons. In Proceedings of the NIST 2007 Automatic Content Extraction Workshop (ACE07), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>41--50</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="10819" citStr="Wu and Weld (2007)" startWordPosition="1605" endWordPosition="1608">h relation. such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007). Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision from the page’s infobox. Unlike their corpus-specific method, which is specific to a (single) Wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre. 3 Freebase Following the literature, we use the term ‘relation’ to refer to an ordered, binary relation between entities. We refer to individual ordered pairs in this relation as ‘relation instances’. For example, the person-nationality relation holds between the entities named ‘John Steinbeck’ and ‘United States</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying wikipedia. In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 41– 50, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In ACL-05,</booktitle>
<pages>427--434</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2252" citStr="Zhou et al., 2005" startWordPosition="344" endWordPosition="347"> a corpus are first hand-labeled for the presence of entities and the relations between them. The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances. ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007). Supervised relation extraction suffers from a number of problems, however. Labeled training data is expensive to produce and thus limited in quantity. Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Un</context>
<context position="7004" citStr="Zhou et al., 2005" startWordPosition="1074" endWordPosition="1077">es in the test set. For each pair of entities, we aggregate the features from the many different sentences in which that pair appeared into a single feature vector, allowing us to provide our classifier with more information, resulting in more accurate labels. Table 1 shows examples of relation instances extracted by our system. We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction. While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE. Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typ</context>
<context position="10691" citStr="Zhou et al. (2005)" startWordPosition="1583" endWordPosition="1586">dustry 13,805 ATS Medical, Health care Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation. such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007). Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision from the page’s infobox. Unlike their corpus-specific method, which is specific to a (single) Wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre. 3 Freebase Following the literature, we use the term ‘relation’ to refer to an ordered, binary relation between entities. We refer to individual ordered pairs in this relation as ‘relati</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In ACL-05, pages 427–434, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Min Zhang</author>
<author>Donghong Ji</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information.</title>
<date>2007</date>
<booktitle>In EMNLP/CoNLL</booktitle>
<contexts>
<context position="2271" citStr="Zhou et al., 2007" startWordPosition="348" endWordPosition="351"> hand-labeled for the presence of entities and the relations between them. The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances. ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007). Supervised relation extraction suffers from a number of problems, however. Labeled training data is expensive to produce and thus limited in quantity. Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain. An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007). Unsupervised approach</context>
<context position="6984" citStr="Zhou et al., 2007" startWordPosition="1070" endWordPosition="1073"> occur multiple times in the test set. For each pair of entities, we aggregate the features from the many different sentences in which that pair appeared into a single feature vector, allowing us to provide our classifier with more information, resulting in more accurate labels. Table 1 shows examples of relation instances extracted by our system. We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction. While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE. Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature. 2 Previous work Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation</context>
<context position="10714" citStr="Zhou et al. (2007)" startWordPosition="1588" endWordPosition="1591">al, Health care Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation. such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al. (2005) and Zhou et al. (2007). Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision from the page’s infobox. Unlike their corpus-specific method, which is specific to a (single) Wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre. 3 Freebase Following the literature, we use the term ‘relation’ to refer to an ordered, binary relation between entities. We refer to individual ordered pairs in this relation as ‘relation instances’. For exam</context>
</contexts>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>Guodong Zhou, Min Zhang, Donghong Ji, and Qiaoming Zhu. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In EMNLP/CoNLL 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>