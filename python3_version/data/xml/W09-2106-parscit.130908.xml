<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029407">
<title confidence="0.9989365">
An Application of Latent Semantic Analysis to Word Sense Discrimination
for Words with Related and Unrelated Meanings
</title>
<author confidence="0.870043">
Juan Pino and Maxine Eskenazi
</author>
<email confidence="0.765399">
(jmpino, max)@cs.cmu.edu
</email>
<affiliation confidence="0.914313333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<sectionHeader confidence="0.975375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99899325">
We present an application of Latent Semantic
Analysis to word sense discrimination within
a tutor for English vocabulary learning. We
attempt to match the meaning of a word in a
document with the meaning of the same word
in a fill-in-the-blank question. We compare
the performance of the Lesk algorithm to La-
tent Semantic Analysis. We also compare the
performance of Latent Semantic Analysis on a
set of words with several unrelated meanings
and on a set of words having both related and
unrelated meanings.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939333333333">
In this paper, we present an application of Latent
Semantic Analysis (LSA) to word sense discrimi-
nation (WSD) within a tutor for English vocabu-
lary learning for non-native speakers. This tutor re-
trieves documents from the Web that contain tar-
get words a student needs to learn and that are at
an appropriate reading level (Collins-Thompson and
Callan, 2005). It presents a document to the student
and then follows the document reading with practice
questions that measure how the student’s knowledge
has evolved. It is important that the fill-in-the-blank
questions (also known as cloze questions) that we
ask to the students allow us to determine their vocab-
ulary knowledge accurately. An example of cloze
question is shown in Figure 1.
Some words have more than one meaning and so
the cloze question we give could be about a different
meaning than the one that the student learned in the
document. This is something that can lead to confu-
sion and must be avoided. To do this, we need to use
some automatic measure of semantic similarity.
</bodyText>
<page confidence="0.999645">
43
</page>
<figureCaption confidence="0.999937">
Figure 1: Example of cloze question.
</figureCaption>
<bodyText confidence="0.99952636">
To define the problem formally, given a target
word w, a string r (the reading) containing w and
n strings ql, ..., qn (the sentences used for the ques-
tions) each containing w, find the strings qi where
the meaning of w is closest to its meaning in r.
We make the problem simpler by selecting only one
question.
This problem is challenging because the context
defined by cloze questions is short. Furthermore,
a word can have only slight variations in meaning
that even humans find sometimes difficult to distin-
guish. LSA was originally applied to Information
Retrieval (Dumais et al., 1988). It was shown to be
able to match short queries to relevant documents
even when there were no exact matches between the
words. Therefore LSA would seem to be an appro-
priate technique for matching a short context, such
as a question, with a whole document.
So we are looking to first discriminate between
the meanings of words, such as “compound”, that
have several very different meanings (a chemical
compound or a set of buildings) and then to dis-
ambiguate words that have senses that are closely
related such as “comprise” (“be composed of” or
“compose”). In the following sections, we present
</bodyText>
<note confidence="0.6706935">
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 43–46,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999829">
LSA and some of its applications, then we present
some experimental results that compare a baseline to
the use of LSA for both tasks we have just described.
We expect the task to be easier on words with unre-
lated meanings. In addition, we expect that LSA will
perform better when we use context selection on the
documents.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.974093925">
LSA was originally applied to Information Retrieval
(Dumais et al., 1988) and called Latent Semantic In-
dexing (LSI). It is based on the singular value de-
composition (SVD) theorem. A m x n matrix X
with m &gt; n can be written as X = U ·S ·VT where
U is a m x n matrix such that UT · U = I&amp;quot;,; S is
a n x n diagonal matrix whose diagonal coefficients
are in decreasing order; and V is a nxn matrix such
that VT · V = In.
X is typically a term-document matrix that repre-
sents the occurrences of vocabulary words in a set of
documents. LSI uses truncated SVD, that is it con-
siders the first r columns of U (written U,), the r
highest coefficients in S (S,) and the first r columns
of V (V,). Similarity between a query and a docu-
ment represented by vectors d and q is performed by
computing the cosine similarity between S−&apos;
, ·UT, ·d
and S−&apos;
, ·UT, ·q. The motivation for computing sim-
ilarity in a different space is to cope with the sparsity
of the vectors in the original space. The motivation
for truncating SVD is that only the most meaning-
ful semantic components of the document and the
query are represented after this transformation and
that noise is discarded.
LSA was subsequently applied to number of prob-
lems, such as synonym detection (Landauer et al.,
1998), document clustering (Song and Park, 2007),
vocabulary acquisition simulation (Landauer and
Dumais, 1997), etc.
Levin and colleagues (2006) applied LSA to word
sense discrimination. They clustered documents
containing ambiguous words and for a test instance
of a document, they assigned the document to its
closest cluster. Our approach is to assign to a doc-
ument the question that is closest. In addition, we
examine the cases where a word has several unre-
lated meanings and where a word has several closely
related meanings.
</bodyText>
<sectionHeader confidence="0.996197" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999929076923077">
We used a database of 62 manually generated cloze
questions covering 16 target words1. We manually
annotated the senses of the target words in these
questions using WordNet senses (Fellbaum, 1998).
For each word and for each sense, we manually gath-
ered documents from the Web containing the target
word with the corresponding sense. There were 84
documents in total. We added 97 documents ex-
tracted from the tutor database of documents that
contained at least one target word but we did not an-
notate their meaning.
We wanted to evaluate the performances of LSA
for WSD for words with unrelated meanings and for
words with both related and unrelated meanings. For
the first type of evaluation, we retained four target
words. For the second type of evaluation, all 16
words were included. We also wanted to evaluate
the influence of the size of the context of the tar-
get words. We therefore considered two matrices:
a term-document matrix and a term-context matrix
where context designates five sentences around the
target word in the document. In both cases each
cell of the matrix had a tf-idf weight. Finally, we
wanted to investigate the influence of the dimension
reduction on performance. In our experiments, we
explored these three directions.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.813256">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.998695466666667">
We first used a variant of the Lesk algorithm (Lesk,
1986), which is based on word exact match. This al-
gorithm seems well suited for the unsupervised ap-
proach we took here since we were dealing with
discrimination rather than disambiguation. Given
a document d and a question q, we computed the
number of word tokens that were shared between d
and q, excluding the target word. The words were
lower cased and stemmed using the Porter stem-
mer. Stop words and punctuation were discarded;
we used the standard English stopword list. Finally,
we selected a window of nw words around the tar-
get word in the question q and a window of ns
sentences around the target word in the document
d. In order to detect sentence boundaries, we used
</bodyText>
<footnote confidence="0.992825">
1available at: www.cs.cmu.edu/ jmpino/questions.xls
</footnote>
<page confidence="0.998804">
44
</page>
<bodyText confidence="0.9973945">
the OpenNLP toolkit (Baldridge et al., 2002). With
nw = 10 and ns = 2, we obtained an accuracy of
61% for the Lesk algorithm. This can be compared
to a random baseline of 44% accuracy.
</bodyText>
<subsectionHeader confidence="0.893646">
4.2 LSA
</subsectionHeader>
<bodyText confidence="0.99851115">
We indexed the document database using the Lemur
toolkit (Allan et al., 2003). The database contained
both the manually annotated documents and the doc-
uments used by the tutor and containing the target
words. The Colt package (Binko et al., ) was used
to perform singular value decomposition and matrix
operations because it supports sparse matrix oper-
ations. We explored three directions in our analy-
sis. We investigated how LSA performs for words
with related meanings and for words with unrelated
meanings. We also explored the influence of the
truncation parameter r. Finally, we examined if re-
ducing the document to a selected context of the tar-
get word improved performance.
Figures 2 and 3 plot accuracy versus dimension
reduction in different cases. In all cases, LSA out-
performs the baseline for certain values of the trun-
cation parameter and when context selection was
used. This shows that LSA is well suited for measur-
ing semantic similarity between two contexts when
at least one of them is short. In general, using the full
dimension in SVD hurts the performances. Dimen-
sion reduction indeed helps discarding noise and
noise is certainly present in our experiments since
we do not perform stemming and do not use a stop-
word list. One could argue that filling the matrix
cells with tf-idf weights already gives less impor-
tance to noisy words.
Figure 2 shows that selecting context in docu-
ments does not give much improvement in accuracy.
It might be that the amount of context selected de-
pends on each document. Here we had a fixed size
context of five sentences around the target word.
In Figure 3, selecting context gives some im-
provement, although not statistically significant,
over the case with the whole document as context.
The best performance obtained for words with un-
related meanings and context selection is also better
than the performance for words with related and un-
related meanings.
</bodyText>
<figure confidence="0.903901666666667">
Accuracy vs. Dimension Reduction for Related Meanings with Different Contexts
60 80 100 120 140 160 180
Truncation Parameter
</figure>
<figureCaption confidence="0.975873">
Figure 2: Accuracy vs. r, the truncation parameter,
for words with related and unrelated meanings and with
whole document or selected context (95% confidence for
whole document: [0.59; 0.65], 95% confidence for se-
lected context: [0.52; 0.67])
</figureCaption>
<figure confidence="0.876121666666667">
Accuracy vs. Dimension Reduction for Unrelated Meanings with Different Contexts
5 10 15 20 25 30 35
Truncation Parameter
</figure>
<figureCaption confidence="0.9968684">
Figure 3: Accuracy vs. r, the truncation parameter, for
words with unrelated meanings only and with whole doc-
uments or selected context ((95% confidence for whole
document: [0.50; 0.59], 95% confidence for selected con-
text: [0.52; 0.71]))
</figureCaption>
<figure confidence="0.9996366">
Accuracy
0.8
0.6
0.4
0.2
0
1
whole document
selected context
Lesk baseline
Accuracy
0.8
0.6
0.4
0.2
0
1
whole document
selected context
Lesk baseline
</figure>
<page confidence="0.997972">
45
</page>
<sectionHeader confidence="0.997844" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999991214285714">
LSA helps overcome sparsity of short contexts such
as questions and gives an improvement over the ex-
act match baseline. However, reducing the context
of the documents to five sentences around the tar-
get word does not seem to give significant improve-
ment. This might be due to the fact that capturing
the right context for a meaning is a difficult task
and that a fixed size context does not always rep-
resent a relevant context. It is yet unclear how to set
the truncation parameter. Although dimension re-
duction seems to help, better results are sometimes
obtained when the truncation parameter is close to
full dimension or when the truncation parameter is
farther from the full dimension.
</bodyText>
<sectionHeader confidence="0.99749" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999947888888889">
We have shown that LSA, which can be considered
as a second-order representation of the documents
and question vectors, is better suited than the Lesk
algorithm, which is a first-order representation of
vectors, for measuring semantic similarity between
a short context such as a question and a longer con-
text such as a document. Dimension reduction was
shown to play an important role in the performances.
However, LSA is relatively difficult to apply to large
amounts of data because SVD is computationally in-
tensive when the vocabulary size is not limited. In
the context of tutoring systems, LSA could not be
applied on the fly, the documents would need to be
preprocessed and annotated beforehand.
We would like to further apply this promising
technique for WSD. Our tutor is able to provide def-
initions when a student is reading a document. We
currently provide all available definitions. It would
be more beneficial to present only the definitions
that are relevant to the meaning of the word in the
document or at least to order them according to their
semantic similarity with the context. We would also
like to investigate how the size of the selected con-
text in a document can affect performance. Finally,
we would like to compare LSA performance to other
second-order vector representations such as vectors
induced from co-occurrence statistics.
</bodyText>
<sectionHeader confidence="0.995511" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99119475">
Thanks Mehrbod Sharifi, David Klahr and Ken
Koedinger for fruitful discussions. This research is
supported by NSF grant SBE-0354420. Any conclu-
sions expressed here are those of the authors.
</bodyText>
<sectionHeader confidence="0.998315" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999614068181818">
James Allan, Jamie Callan, Kevin Collins-Thompson,
Bruce Croft, Fangfang Feng, David Fisher, John Laf-
ferty, Leah Larkey, Thi N. Truong, Paul Ogilvie, et al.
2003. The lemur toolkit for language modeling and
information retrieval.
Jason Baldridge, Thomas Morton, and Gann Bierner.
2002. The opennlp maximum entropy package. Tech-
nical report, Technical report, SourceForge.
Pavel Binko, Dino Ferrero Merlino, Wolfgang Hoschek,
Tony Johnson, Andreas Pfeiffer, et al. Open source
libraries for high performance scientific and technical
computing in java.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13):1448–1462.
Susane T. Dumais, George W. Furnas, Thomas K.
Landauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve ac-
cess to textual information. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 281–285.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. MIT press.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological review, 104:211–240.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse processes, 25:259–284.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems Docu-
mentation, pages 24–26.
Esther Levin, Mehrbod Sharifi, and Jerry Ball. 2006.
Evaluation of utility of lsa for word sense discrimina-
tion. In Proceedings ofHLT/NAACL, pages 77–80.
Wei Song and Soon Cheol Park. 2007. A novel docu-
ment clustering model based on latent semantic analy-
sis. In Proceedings of the Third International Confer-
ence on Semantics, Knowledge and Grid, pages 539–
542.
</reference>
<page confidence="0.999611">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.355803">
<title confidence="0.99976">An Application of Latent Semantic Analysis to Word Sense Discrimination for Words with Related and Unrelated Meanings</title>
<author confidence="0.966589">Pino</author>
<email confidence="0.74707">(jmpino,</email>
<affiliation confidence="0.7886745">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.991683">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.996898307692308">We present an application of Latent Semantic Analysis to word sense discrimination within a tutor for English vocabulary learning. We attempt to match the meaning of a word in a document with the meaning of the same word in a fill-in-the-blank question. We compare the performance of the Lesk algorithm to Latent Semantic Analysis. We also compare the performance of Latent Semantic Analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jamie Callan</author>
<author>Kevin Collins-Thompson</author>
<author>Bruce Croft</author>
<author>Fangfang Feng</author>
<author>David Fisher</author>
<author>John Lafferty</author>
<author>Leah Larkey</author>
<author>Thi N Truong</author>
<author>Paul Ogilvie</author>
</authors>
<title>The lemur toolkit for language modeling and information retrieval.</title>
<date>2003</date>
<contexts>
<context position="7732" citStr="Allan et al., 2003" startWordPosition="1320" endWordPosition="1323">mer. Stop words and punctuation were discarded; we used the standard English stopword list. Finally, we selected a window of nw words around the target word in the question q and a window of ns sentences around the target word in the document d. In order to detect sentence boundaries, we used 1available at: www.cs.cmu.edu/ jmpino/questions.xls 44 the OpenNLP toolkit (Baldridge et al., 2002). With nw = 10 and ns = 2, we obtained an accuracy of 61% for the Lesk algorithm. This can be compared to a random baseline of 44% accuracy. 4.2 LSA We indexed the document database using the Lemur toolkit (Allan et al., 2003). The database contained both the manually annotated documents and the documents used by the tutor and containing the target words. The Colt package (Binko et al., ) was used to perform singular value decomposition and matrix operations because it supports sparse matrix operations. We explored three directions in our analysis. We investigated how LSA performs for words with related meanings and for words with unrelated meanings. We also explored the influence of the truncation parameter r. Finally, we examined if reducing the document to a selected context of the target word improved performan</context>
</contexts>
<marker>Allan, Callan, Collins-Thompson, Croft, Feng, Fisher, Lafferty, Larkey, Truong, Ogilvie, 2003</marker>
<rawString>James Allan, Jamie Callan, Kevin Collins-Thompson, Bruce Croft, Fangfang Feng, David Fisher, John Lafferty, Leah Larkey, Thi N. Truong, Paul Ogilvie, et al. 2003. The lemur toolkit for language modeling and information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Thomas Morton</author>
<author>Gann Bierner</author>
</authors>
<title>The opennlp maximum entropy package.</title>
<date>2002</date>
<tech>Technical report, Technical report, SourceForge.</tech>
<contexts>
<context position="7506" citStr="Baldridge et al., 2002" startWordPosition="1276" endWordPosition="1279">ation rather than disambiguation. Given a document d and a question q, we computed the number of word tokens that were shared between d and q, excluding the target word. The words were lower cased and stemmed using the Porter stemmer. Stop words and punctuation were discarded; we used the standard English stopword list. Finally, we selected a window of nw words around the target word in the question q and a window of ns sentences around the target word in the document d. In order to detect sentence boundaries, we used 1available at: www.cs.cmu.edu/ jmpino/questions.xls 44 the OpenNLP toolkit (Baldridge et al., 2002). With nw = 10 and ns = 2, we obtained an accuracy of 61% for the Lesk algorithm. This can be compared to a random baseline of 44% accuracy. 4.2 LSA We indexed the document database using the Lemur toolkit (Allan et al., 2003). The database contained both the manually annotated documents and the documents used by the tutor and containing the target words. The Colt package (Binko et al., ) was used to perform singular value decomposition and matrix operations because it supports sparse matrix operations. We explored three directions in our analysis. We investigated how LSA performs for words wi</context>
</contexts>
<marker>Baldridge, Morton, Bierner, 2002</marker>
<rawString>Jason Baldridge, Thomas Morton, and Gann Bierner. 2002. The opennlp maximum entropy package. Technical report, Technical report, SourceForge.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Pavel Binko</author>
<author>Dino Ferrero Merlino</author>
<author>Wolfgang Hoschek</author>
<author>Tony Johnson</author>
<author>Andreas Pfeiffer</author>
</authors>
<title>Open source libraries for high performance scientific and technical computing in java.</title>
<marker>Binko, Merlino, Hoschek, Johnson, Pfeiffer, </marker>
<rawString>Pavel Binko, Dino Ferrero Merlino, Wolfgang Hoschek, Tony Johnson, Andreas Pfeiffer, et al. Open source libraries for high performance scientific and technical computing in java.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>Predicting reading difficulty with statistical language models.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>56</volume>
<issue>13</issue>
<contexts>
<context position="1147" citStr="Collins-Thompson and Callan, 2005" startWordPosition="177" endWordPosition="180">estion. We compare the performance of the Lesk algorithm to Latent Semantic Analysis. We also compare the performance of Latent Semantic Analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings. 1 Introduction In this paper, we present an application of Latent Semantic Analysis (LSA) to word sense discrimination (WSD) within a tutor for English vocabulary learning for non-native speakers. This tutor retrieves documents from the Web that contain target words a student needs to learn and that are at an appropriate reading level (Collins-Thompson and Callan, 2005). It presents a document to the student and then follows the document reading with practice questions that measure how the student’s knowledge has evolved. It is important that the fill-in-the-blank questions (also known as cloze questions) that we ask to the students allow us to determine their vocabulary knowledge accurately. An example of cloze question is shown in Figure 1. Some words have more than one meaning and so the cloze question we give could be about a different meaning than the one that the student learned in the document. This is something that can lead to confusion and must be </context>
</contexts>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>Kevyn Collins-Thompson and Jamie Callan. 2005. Predicting reading difficulty with statistical language models. Journal of the American Society for Information Science and Technology, 56(13):1448–1462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susane T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Scott Deerwester</author>
<author>Richard Harshman</author>
</authors>
<title>Using latent semantic analysis to improve access to textual information.</title>
<date>1988</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>281--285</pages>
<contexts>
<context position="2461" citStr="Dumais et al., 1988" startWordPosition="403" endWordPosition="406">: Example of cloze question. To define the problem formally, given a target word w, a string r (the reading) containing w and n strings ql, ..., qn (the sentences used for the questions) each containing w, find the strings qi where the meaning of w is closest to its meaning in r. We make the problem simpler by selecting only one question. This problem is challenging because the context defined by cloze questions is short. Furthermore, a word can have only slight variations in meaning that even humans find sometimes difficult to distinguish. LSA was originally applied to Information Retrieval (Dumais et al., 1988). It was shown to be able to match short queries to relevant documents even when there were no exact matches between the words. Therefore LSA would seem to be an appropriate technique for matching a short context, such as a question, with a whole document. So we are looking to first discriminate between the meanings of words, such as “compound”, that have several very different meanings (a chemical compound or a set of buildings) and then to disambiguate words that have senses that are closely related such as “comprise” (“be composed of” or “compose”). In the following sections, we present Pro</context>
</contexts>
<marker>Dumais, Furnas, Landauer, Deerwester, Harshman, 1988</marker>
<rawString>Susane T. Dumais, George W. Furnas, Thomas K. Landauer, Scott Deerwester, and Richard Harshman. 1988. Using latent semantic analysis to improve access to textual information. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 281–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="5599" citStr="Fellbaum, 1998" startWordPosition="954" endWordPosition="955">ues (2006) applied LSA to word sense discrimination. They clustered documents containing ambiguous words and for a test instance of a document, they assigned the document to its closest cluster. Our approach is to assign to a document the question that is closest. In addition, we examine the cases where a word has several unrelated meanings and where a word has several closely related meanings. 3 Experimental Setup We used a database of 62 manually generated cloze questions covering 16 target words1. We manually annotated the senses of the target words in these questions using WordNet senses (Fellbaum, 1998). For each word and for each sense, we manually gathered documents from the Web containing the target word with the corresponding sense. There were 84 documents in total. We added 97 documents extracted from the tutor database of documents that contained at least one target word but we did not annotate their meaning. We wanted to evaluate the performances of LSA for WSD for words with unrelated meanings and for words with both related and unrelated meanings. For the first type of evaluation, we retained four target words. For the second type of evaluation, all 16 words were included. We also w</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An electronic lexical database. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological review,</title>
<date>1997</date>
<pages>104--211</pages>
<contexts>
<context position="4960" citStr="Landauer and Dumais, 1997" startWordPosition="848" endWordPosition="851">ctors d and q is performed by computing the cosine similarity between S−&apos; , ·UT, ·d and S−&apos; , ·UT, ·q. The motivation for computing similarity in a different space is to cope with the sparsity of the vectors in the original space. The motivation for truncating SVD is that only the most meaningful semantic components of the document and the query are represented after this transformation and that noise is discarded. LSA was subsequently applied to number of problems, such as synonym detection (Landauer et al., 1998), document clustering (Song and Park, 2007), vocabulary acquisition simulation (Landauer and Dumais, 1997), etc. Levin and colleagues (2006) applied LSA to word sense discrimination. They clustered documents containing ambiguous words and for a test instance of a document, they assigned the document to its closest cluster. Our approach is to assign to a document the question that is closest. In addition, we examine the cases where a word has several unrelated meanings and where a word has several closely related meanings. 3 Experimental Setup We used a database of 62 manually generated cloze questions covering 16 target words1. We manually annotated the senses of the target words in these question</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="4854" citStr="Landauer et al., 1998" startWordPosition="835" endWordPosition="838"> S (S,) and the first r columns of V (V,). Similarity between a query and a document represented by vectors d and q is performed by computing the cosine similarity between S−&apos; , ·UT, ·d and S−&apos; , ·UT, ·q. The motivation for computing similarity in a different space is to cope with the sparsity of the vectors in the original space. The motivation for truncating SVD is that only the most meaningful semantic components of the document and the query are represented after this transformation and that noise is discarded. LSA was subsequently applied to number of problems, such as synonym detection (Landauer et al., 1998), document clustering (Song and Park, 2007), vocabulary acquisition simulation (Landauer and Dumais, 1997), etc. Levin and colleagues (2006) applied LSA to word sense discrimination. They clustered documents containing ambiguous words and for a test instance of a document, they assigned the document to its closest cluster. Our approach is to assign to a document the question that is closest. In addition, we examine the cases where a word has several unrelated meanings and where a word has several closely related meanings. 3 Experimental Setup We used a database of 62 manually generated cloze q</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems Documentation,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="6733" citStr="Lesk, 1986" startWordPosition="1146" endWordPosition="1147">rds. For the second type of evaluation, all 16 words were included. We also wanted to evaluate the influence of the size of the context of the target words. We therefore considered two matrices: a term-document matrix and a term-context matrix where context designates five sentences around the target word in the document. In both cases each cell of the matrix had a tf-idf weight. Finally, we wanted to investigate the influence of the dimension reduction on performance. In our experiments, we explored these three directions. 4 Results 4.1 Baseline We first used a variant of the Lesk algorithm (Lesk, 1986), which is based on word exact match. This algorithm seems well suited for the unsupervised approach we took here since we were dealing with discrimination rather than disambiguation. Given a document d and a question q, we computed the number of word tokens that were shared between d and q, excluding the target word. The words were lower cased and stemmed using the Porter stemmer. Stop words and punctuation were discarded; we used the standard English stopword list. Finally, we selected a window of nw words around the target word in the question q and a window of ns sentences around the targe</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems Documentation, pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Mehrbod Sharifi</author>
<author>Jerry Ball</author>
</authors>
<title>Evaluation of utility of lsa for word sense discrimination.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT/NAACL,</booktitle>
<pages>77--80</pages>
<marker>Levin, Sharifi, Ball, 2006</marker>
<rawString>Esther Levin, Mehrbod Sharifi, and Jerry Ball. 2006. Evaluation of utility of lsa for word sense discrimination. In Proceedings ofHLT/NAACL, pages 77–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Song</author>
<author>Soon Cheol Park</author>
</authors>
<title>A novel document clustering model based on latent semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the Third International Conference on Semantics, Knowledge and Grid,</booktitle>
<pages>539--542</pages>
<contexts>
<context position="4897" citStr="Song and Park, 2007" startWordPosition="841" endWordPosition="844">milarity between a query and a document represented by vectors d and q is performed by computing the cosine similarity between S−&apos; , ·UT, ·d and S−&apos; , ·UT, ·q. The motivation for computing similarity in a different space is to cope with the sparsity of the vectors in the original space. The motivation for truncating SVD is that only the most meaningful semantic components of the document and the query are represented after this transformation and that noise is discarded. LSA was subsequently applied to number of problems, such as synonym detection (Landauer et al., 1998), document clustering (Song and Park, 2007), vocabulary acquisition simulation (Landauer and Dumais, 1997), etc. Levin and colleagues (2006) applied LSA to word sense discrimination. They clustered documents containing ambiguous words and for a test instance of a document, they assigned the document to its closest cluster. Our approach is to assign to a document the question that is closest. In addition, we examine the cases where a word has several unrelated meanings and where a word has several closely related meanings. 3 Experimental Setup We used a database of 62 manually generated cloze questions covering 16 target words1. We manu</context>
</contexts>
<marker>Song, Park, 2007</marker>
<rawString>Wei Song and Soon Cheol Park. 2007. A novel document clustering model based on latent semantic analysis. In Proceedings of the Third International Conference on Semantics, Knowledge and Grid, pages 539– 542.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>