<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000414">
<title confidence="0.9962215">
An Integrated Approach to Measuring Semantic Similarity between Words
Using Information available on the Web
</title>
<author confidence="0.957198">
Danushka Bollegala Yutaka Matsuo Mitsuru Ishizuka
</author>
<affiliation confidence="0.991746">
The University of Tokyo National Institute of Advanced The University of Tokyo
</affiliation>
<address confidence="0.87728075">
7-3-1, Hongo, Tokyo, Industrial Science and 7-3-1, Hongo, Tokyo,
113-8656, Japan Technology 113-8656, Japan
danushka@mi.ci.i.u- 1-18-13, Sotokanda, Tokyo, ishizuka@i.u-
tokyo.ac.jp 101-0021, Japan tokyo.ac.jp
</address>
<email confidence="0.997492">
y.matsuo@aist.go.jp
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999802">
Measuring semantic similarity between
words is vital for various applications
in natural language processing, such as
language modeling, information retrieval,
and document clustering. We propose a
method that utilizes the information avail-
able on the Web to measure semantic sim-
ilarity between a pair of words or entities.
We integrate page counts for each word in
the pair and lexico-syntactic patterns that
occur among the top ranking snippets for
the AND query using support vector ma-
chines. Experimental results on Miller-
Charles’ benchmark data set show that the
proposed measure outperforms all the ex-
isting web based semantic similarity mea-
sures by a wide margin, achieving a cor-
relation coefficient of 0.834. Moreover,
the proposed semantic similarity measure
significantly improves the accuracy (F-
measure of 0.78) in a named entity cluster-
ing task, proving the capability of the pro-
posed measure to capture semantic simi-
larity using web content.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912578947368">
The study of semantic similarity between words has
been an integral part of natural language processing
and information retrieval for many years. Semantic
similarity measures are vital for various applications
in natural language processing such as word sense
disambiguation (Resnik, 1999), language model-
ing (Rosenfield, 1996), synonym extraction (Lin,
1998a) and automatic thesaurus extraction (Curran,
2002).
Pre-compiled taxonomies such as WordNet 1 and
text corpora have been used in previous work on se-
mantic similarity (Lin, 1998a; Resnik, 1995; Jiang
and Conrath, 1998; Lin, 1998b). However, seman-
tic similarity between words change over time as
new senses and associations of words are constantly
created. One major issue behind taxonomies and
corpora oriented approaches is that they might not
necessarily capture similarity between proper names
such as named entities (e.g., personal names, loca-
tion names, product names) and the new uses of ex-
isting words. For example, apple is frequently asso-
ciated with computers on the Web but this sense of
apple is not listed in the WordNet. Maintaining an
up-to-date taxonomy of all the new words and new
usages of existing words is costly if not impossible.
The Web can be regarded as a large-scale, dy-
namic corpus of text. Regarding the Web as a live
corpus has become an active research topic recently.
Simple, unsupervised models have shown to per-
form better when n-gram counts are obtained from
the Web rather than from a large corpus (Keller and
Lapata, 2003; Lapata and Keller, 2005). Resnik and
Smith (2003) extract bilingual sentences from the
Web to create parallel corpora for machine trans-
lation. Turney (2001) defines a point wise mutual
information (PMI-IR) measure using the number of
hits returned by a Web search engine to recognize
synonyms. Matsuo et. al, (2006b) follows a similar
</bodyText>
<footnote confidence="0.984638">
1http://wordnet.princeton.edu/
</footnote>
<page confidence="0.936316">
340
</page>
<note confidence="0.801398">
Proceedings of NAACL HLT 2007, pages 340–347,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99828368">
approach to measure the similarity between words
and apply their method in a graph-based word clus-
tering algorithm.
Due to the huge number of documents and the
high growth rate of the Web, it is difficult to di-
rectly analyze each individual document separately.
Search engines provide an efficient interface to this
vast information. Page counts and snippets are two
useful information sources provided by most Web
search engines. Page count of a query is the number
of pages that contain the query words 2. A snippet is
a brief window of text extracted by a search engine
around the query term in a document. Snippets pro-
vide useful information about the immediate context
of the query term.
This paper proposes a Web-based semantic simi-
larity metric which combines page counts and snip-
pets using support vector machines. We extract
lexico-syntactic patterns from snippets. For exam-
ple, X is a Y indicates there is a high semantic sim-
ilarity between X and Y. Automatically extracted
lexico-syntactic patterns have been successfully em-
ployed in various term extraction tasks (Hearst,
1992).
Our contributions are summarized as follows:
</bodyText>
<listItem confidence="0.999449923076923">
• We propose a lexico-syntactic patterns-based
approach to compute semantic similarity using
snippets obtained from a Web search engine.
• We integrate different Web-based similarity
scores using WordNet synsets and support vec-
tor machines to create a robust semantic sim-
ilarity measure. The integrated measure out-
performs all existing Web-based semantic sim-
ilarity measures in a benchmark dataset and a
named entity clustering task. To the best of
our knowledge, this is the first attempt to com-
bine both WordNet synsets and Web content to
leverage a robust semantic similarity measure.
</listItem>
<sectionHeader confidence="0.982441" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.992397">
Given a taxonomy of concepts, a straightforward
method for calculating similarity between two words
(concepts) is to find the length of the shortest path
</bodyText>
<footnote confidence="0.963296666666667">
2page count may not necessarily be equal to the word fre-
quency because the queried word may appear many times in a
page
</footnote>
<bodyText confidence="0.995985666666667">
connecting the two words in the taxonomy (Rada
et al., 1989). If a word is polysemous (i.e., having
more than one sense) then multiple paths may ex-
ist between the two words. In such cases only the
shortest path between any two senses of the words is
considered for the calculation of similarity. A prob-
lem frequently acknowledged with this approach is
that it relies on the notion that all links in the taxon-
omy represent uniform distances.
Resnik (1995) proposes a similarity measure
based on information content. He defines the sim-
ilarity between two concepts C1 and C2 in the tax-
onomy as the maximum of the information content
of all concepts C that subsume both C1 and C2.
Then the similarity between two words are defined
as the maximum of the similarity between any con-
cepts that the words belong to. He uses WordNet as
the taxonomy and information content is calculated
using the Brown corpus.
Li et al., (2003) combines structural semantic in-
formation from a lexical taxonomy and informa-
tion content from a corpus in a non-linear model.
They propose a similarity measure that uses shortest
path length, depth and local density in a taxonomy.
Their experiments using WordNet and the Brown
corpus reports a Pearson correlation coefficient of
0.8914 on the Miller and Charles’ (1998) bench-
mark dataset. They do not evaluate their method on
similarities between named entities. Recently, some
work has been carried out on measuring semantic
similarity using web content. Matsuo et al., (2006a)
propose the use of Web hits for the extraction of
communities on the Web. They measure the associ-
ation between two personal names using the overlap
coefficient, calculated based on the number of Web
hits for each individual name and their conjunction.
Sahami et al., (2006) measure semantic similarity
between two queries using the snippets returned for
those queries by a search engine. For each query,
they collect snippets from a search engine and rep-
resent each snippet as a TF-IDF weighted term vec-
tor. Each vector is L2 normalized and the centroid
of the set of vectors is computed. Semantic similar-
ity between two queries is then defined as the inner
product between the corresponding centroid vectors.
They do not compare their similarity measure with
taxonomy based similarity measures.
Chen et al., (2006) propose a web-based double-
</bodyText>
<page confidence="0.997525">
341
</page>
<bodyText confidence="0.99995925">
checking model to compute semantic similarity be-
tween words. For two words P and Q, they col-
lect snippets for each word from a web search en-
gine. Then they count the number of occurrences of
word P in the snippets for word Q and the number
of occurrences of word Q in the snippets for word
P. These values are combined non-linearly to com-
pute the similarity between P and Q. This method
heavily depends on the search engine’s ranking al-
gorithm. Although two words P and Q may be very
similar, there is no reason to believe that one can find
Q in the snippets for P, or vice versa. This observa-
tion is confirmed by the experimental results in their
paper which reports 0 similarity scores for many
pairs of words in the Miller and Charles (1998) data
set.
</bodyText>
<sectionHeader confidence="0.990446" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9999282">
In this section we will describe the various similarity
features we use in our model. We utilize page counts
and snippets returned by the Google 3 search engine
for simple text queries to define various similarity
scores.
</bodyText>
<subsectionHeader confidence="0.99951">
3.1 Page Counts-based Similarity Scores
</subsectionHeader>
<bodyText confidence="0.99888505">
For the rest of this paper we use the notation H(P)
to denote the page count for the query P in a search
engine. Terra and Clarke (2003) compare various
similarity scores for measuring similarity between
words in a corpus. We modify the traditional Jac-
card, overlap (Simpson), Dice and PMI measures
for the purpose of measuring similarity using page
counts. WebJaccard coefficient between words (or
phrases) P and Q, WebJaccard(P, Q), is defined
by,
WebJaccard(P, Q) if H(P ∩ Q) ≤ c
� 0 ( 1 )�
= H(PnQ) otherwise
H(P)+H(Q)−H(PnQ)
Here, P ∩ Q denotes the conjunction query P AND
Q. Given the scale and noise in the Web, some words
might occur arbitrarily, i.e. by random chance, on
some pages. Given the scale and noise in web data, it
is a possible that two words man order to reduce the
adverse effect due to random co-occurrences, we set
</bodyText>
<footnote confidence="0.832068">
3http://www.google.com
</footnote>
<bodyText confidence="0.9890485">
the WebJaccard coefficient to zero if the page counts
for the query P ∩ Q is less than a threshold c. 4
Likewise, we define WebOverlap coefficient,
WebOverlap(P, Q), as,
</bodyText>
<equation confidence="0.988214666666667">
WebOverlap(P, Q)
(0 H(Pnif H(P ∩ Q) ≤ c 2
SSII min(H(P),)(Q)) otherwise )
</equation>
<bodyText confidence="0.9066195">
We define WebDice as a variant of Dice coeffi-
cient. WebDice(P, Q) is defined as,
</bodyText>
<equation confidence="0.99282225">
WebDice(P, Q)
(0 if H(P ∩ Q) ≤ c
Sl =(3)
H(P)+H(Q) 2H(PnQ) otherwise
</equation>
<bodyText confidence="0.9887465">
We define WebPMI as a variant form of PMI using
page counts by,
</bodyText>
<equation confidence="0.97850575">
WebPMI(P, Q)
0 if H(P ∩ Q) ≤ c
�(4)
) otherwise
</equation>
<bodyText confidence="0.999749545454546">
Here, N is the number of documents indexed by the
search engine. Probabilities in Formula 4 are esti-
mated according to the maximum likelihood princi-
ple. In order to accurately calculate PMI using For-
mula 4, we must know N, the number of documents
indexed by the search engine. Although estimating
the number of documents indexed by a search en-
gine (Bar-Yossef and Gurevich, 2006) is an interest-
ing task itself, it is beyond the scope of this work. In
this work, we set N = 1010 according to the number
of indexed pages reported by Google.
</bodyText>
<subsectionHeader confidence="0.9978965">
3.2 Snippets-based Synonymous Word
Patterns
</subsectionHeader>
<bodyText confidence="0.9999041">
Page counts-based similarity measures do not con-
sider the relative distance between P and Q in a page
or the length of the page. Although P and Q occur
in a page they might not be related at all. Therefore,
page counts-based similarity measures are prone to
noise and are not reliable when H(P ∩Q) is low. On
the other hand snippets capture the local context of
query words. We propose lexico-syntactic patterns
extracted from snippets as a solution to the problems
with page counts-based similarity measures.
</bodyText>
<equation confidence="0.988650555555556">
4we set c = 5 in our experiments
= I
H(PnQ)
N
H(P)
N
lo92(
H(Q)
N
</equation>
<page confidence="0.983862">
342
</page>
<bodyText confidence="0.99315945652174">
To illustrate our pattern extraction algorithm con-
sider the following snippet from Google for the
query jaguar AND cat.
”The Jaguar is the largest cat in Western Hemi-
sphere and can subdue a larger prey than can the
puma”
Here, the phrase is the largest indicates a hy-
pernymic relationship between Jaguar and the cat.
Phrases such as also known as, is a, part of, is an ex-
ample of all indicate various of semantic relations.
Such indicative phrases have been successfully ap-
plied in various tasks such as synonym extraction,
hyponym extraction (Hearst, 1992) and fact extrac-
tion (Pasca et al., 2006).
We describe our pattern extraction algorithm in
three steps.
Step 1
We replace the two query terms in a snippet by two
wildcards X and Y. We extract all word n-grams that
contain both X and Y. In our experiments we ex-
tracted n-grams for n = 2 to 5. For example, from
the previous snippet we extract the pattern, X is the
largest X. In order to leverage the pattern extraction
process, we randomly select 5000 pairs of synony-
mous nouns from WordNet synsets. We ignore the
nouns which do not have synonyms in the WordNet.
For nouns with more than one sense, we select syn-
onyms from its dominant sense. For each pair of
synonyms (P, Q), we query Google for “P&amp;quot; AND
“Q&amp;quot; and download the snippets. Let us call this col-
lection of snippets as the positive corpus. We apply
the above mentioned n-gram based pattern extrac-
tion procedure and count the frequency of each valid
pattern in the positive corpus.
Step 2
Pattern extraction algorithm described in step 1
yields 4, 562, 471 unique patterns. 80%of these pat-
terns occur less than 10 times in the positive corpus.
It is impossible to learn with such a large number of
sparse patterns. Moreover, some patterns might oc-
cur purely randomly in a snippet and are not good
indicators of semantic similarity. To measure the
reliability of a pattern as an indicator of semantic
similarity we employ the following procedure. We
create a set of non-synonymous word-pairs by ran-
domly shuffling the words in our data set of synony-
</bodyText>
<tableCaption confidence="0.997553">
Table 1: Contingency table
</tableCaption>
<bodyText confidence="0.994440608695652">
v other than v All
Freq. in positive corpus p„ P − p„ P
Freq. in negative corpus n„ N − n„ N
mous word-pairs. We check each pair of words in
this newly created data set against WordNet and con-
firm that they do not belong to any of the synsets
in the WordNet. From this procedure we created
5000 non-synonymous pairs of words. For each
non-synonymous word-pair, we query Google for
the conjunction of its words and download snippets.
Let us call this collection of snippets as the nega-
tive corpus. For each pattern generated in step 1, we
count its frequency in the negative corpus.
Step 3
We create a contingency table as shown in Table 1
for each pattern v extracted in step 1 using its fre-
quency pv in positive corpus and nv in negative cor-
pus. In Table 1, P denotes the total frequency of all
patterns in the positive corpus and N denotes that in
the negative corpus.
Using the information in Table 1, we calculate
x2 (Manning and Sch¨utze, 2002) value for each pat-
tern as,
</bodyText>
<equation confidence="0.83134">
PN(pv + nv)(P + N − pv − nv) �
</equation>
<bodyText confidence="0.9593055">
(5)
We selected the top ranking 200 patterns experimen-
tally as described in section 4.2 according to their x2
values. Some of the selected patterns are shown in
</bodyText>
<tableCaption confidence="0.608325">
Table 2.
</tableCaption>
<subsectionHeader confidence="0.994557">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.999923833333333">
For each pair of synonymous and non-synonymous
words in our datasets, we count the frequency of
occurrence of the patterns selected in Step 3. We
normalize the frequency count of each pattern by
dividing from the total frequency of all patterns.
Moreover, we compute the page counts-based fea-
tures as given by formulae (1-4). Using the 200
pattern features and the 4 page counts-based fea-
tures we create 204 dimensional feature vectors for
each training instance in our synonymous and non-
synonymous datasets. We train a two class support
vector machine (SVM) (Vapnik, 1998), where class
</bodyText>
<equation confidence="0.979461">
2 (P + N)(pv(N − nv) − nv(P − pv))2
x=
</equation>
<page confidence="0.987501">
343
</page>
<bodyText confidence="0.800953">
+1 represents synonymous word-pairs and class
−1 represents non-synonymous word-pairs. Finally,
SVM outputs are converted to posterior probabilities
(Platt, 2000). We consider the posterior probability
of a given pair of words belonging to class +1 as the
semantic similarity between the two words.
</bodyText>
<sectionHeader confidence="0.99958" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999778">
To evaluate the performance of the proposed se-
mantic similarity measure, we conduct two sets of
experiments. Firstly, we compare the similarity
scores produced by the proposed measure against
the Miller-Charles’ benchmark dataset. We analyze
the performance of the proposed measure with the
number of snippets and the size of the training data
set. Secondly, we apply the proposed measure in a
real-world named entity clustering task and measure
its performance.
</bodyText>
<subsectionHeader confidence="0.990271">
4.1 The Benchmark Dataset
</subsectionHeader>
<bodyText confidence="0.999934461538461">
We evaluated the proposed method against Miller-
Charles (1998) dataset, a dataset of 30 5 word-pairs
rated by a group of 38 human subjects. Word-
pairs are rated on a scale from 0 (no similarity) to
4 (perfect synonymy). Miller-Charles’ dataset is
a subset of Rubenstein-Goodenough’s (1965) orig-
inal dataset of 65 word-pairs. Although Miller-
Charles’ experiment was carried out 25 years
later than Rubenstein-Goodenough’s, two sets of
ratings are highly correlated (Pearson correlation
coefficient=0.97). Therefore, Miller-Charles ratings
can be considered as a reliable benchmark for eval-
uating semantic similarity measures.
</bodyText>
<subsectionHeader confidence="0.984426">
4.2 Pattern Selection
</subsectionHeader>
<bodyText confidence="0.9998862">
We trained a linear kernel SVM with top N pattern
features (ranked according to their x2 values) and
calculated the Pearson correlation coefficient against
the Miller-Charles’ benchmark dataset. Experimen-
tal results are shown in Figure 1. From Figure 1
we select N = 200, where correlation maximizes.
Features with the highest linear kernel weights are
shown in Table 2 alongside with their x2 values. The
weight of a feature in the linear kernel can be consid-
ered as a rough estimate of the influence it has on the
</bodyText>
<footnote confidence="0.75767375">
5Due to the omission of two word-pairs in earlier versions
of WordNet most researchers had used only 28 pairs for evalu-
ations
Number of pattern features (N)
</footnote>
<figureCaption confidence="0.997279">
Figure 1: Correlation vs No of pattern features
</figureCaption>
<tableCaption confidence="0.983126">
Table 2: Features with the highest SVM linear ker-
nel weights
</tableCaption>
<table confidence="0.999433727272727">
feature x2 SVM weight
WebDice N/A 8.19
X/Y 33459 7.53
X, Y : 4089 6.00
X or Y 3574 5.83
X Y for 1089 4.49
X . the Y 1784 2.99
with X ( Y 1819 2.85
X=Y 2215 2.74
X and Y are 1343 2.67
X of Y 2472 2.56
</table>
<bodyText confidence="0.9986912">
final SVM output. WebDice has the highest linear
kernel weight followed by a series of patterns-based
features. WebOverlap (rank=18, weight=2.45), We-
bJaccard (rank=66, weight=0.618) and WebPMI
(rank=138, weight=0.0001) are not shown in Table 2
due to space limitations. It is noteworthy that the
pattern features in Table 2 agree with the intuition.
Lexical patterns (e.g., X or Y, X and Y are, X of Y) as
well as syntactic patterns (e.g., bracketing, comma
usage) are extracted by our method.
</bodyText>
<subsectionHeader confidence="0.999358">
4.3 Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.99998725">
We score the word-pairs in Miller-Charles dataset
using the page counts-based similarity measures,
previous work on web-based semantic similarity
measures (Sahami (2006), Chen (2006)) and the
proposed method (SVM). Results are shown in Ta-
ble 4.3. All figures except for the Miller-Charles
ratings are normalized into [0, 1] range for the ease
of comparison 6. Proposed method (SVM) re-
</bodyText>
<footnote confidence="0.4523585">
6Pearson correlation coefficient is invariant against a linear
transformation
</footnote>
<figure confidence="0.99788075">
0.800
0.798
0.796
0.794
0.792
0.790
0.788
0.786
0.784
0.782
0 200 400 600 800 100012001400160018002000
0.780
</figure>
<page confidence="0.997684">
344
</page>
<tableCaption confidence="0.998394">
Table 3: Semantic Similarity of Human Ratings and baselines on Miller-Charles dataset
</tableCaption>
<table confidence="0.999583">
Word Pair Miller- Web Web Web Web Sahami Chen (CODC) Proposed
Charles Jaccard Dice Overlap PMI (2006) (2006) (SVM)
cord-smile 0.13 0.102 0.108 0.036 0.207 0.090 0 0
rooster-voyage 0.08 0.011 0.012 0.021 0.228 0.197 0 0.017
noon-string 0.08 0.126 0.133 0.060 0.101 0.082 0 0.018
glass-magician 0.11 0.117 0.124 0.408 0.598 0.143 0 0.180
monk-slave 0.55 0.181 0.191 0.067 0.610 0.095 0 0.375
coast-forest 0.42 0.862 0.870 0.310 0.417 0.248 0 0.405
monk-oracle 1.1 0.016 0.017 0.023 0 0.045 0 0.328
lad-wizard 0.42 0.072 0.077 0.070 0.426 0.149 0 0.220
forest-graveyard 0.84 0.068 0.072 0.246 0.494 0 0 0.547
food-rooster 0.89 0.012 0.013 0.425 0.207 0.075 0 0.060
coast-hill 0.87 0.963 0.965 0.279 0.350 0.293 0 0.874
car-journey 1.16 0.444 0.460 0.378 0.204 0.189 0.290 0.286
crane-implement 1.68 0.071 0.076 0.119 0.193 0.152 0 0.133
brother-lad 1.66 0.189 0.199 0.369 0.644 0.236 0.379 0.344
bird-crane 2.97 0.235 0.247 0.226 0.515 0.223 0 0.879
bird-cock 3.05 0.153 0.162 0.162 0.428 0.058 0.502 0.593
food-fruit 3.08 0.753 0.765 1 0.448 0.181 0.338 0.998
brother-monk 2.82 0.261 0.274 0.340 0.622 0.267 0.547 0.377
asylum-madhouse 3.61 0.024 0.025 0.102 0.813 0.212 0 0.773
furnace-stove 3.11 0.401 0.417 0.118 1 0.310 0.928 0.889
magician-wizard 3.5 0.295 0.309 0.383 0.863 0.233 0.671 1
journey-voyage 3.84 0.415 0.431 0.182 0.467 0.524 0.417 0.996
coast-shore 3.7 0.786 0.796 0.521 0.561 0.381 0.518 0.945
implement-tool 2.95 1 1 0.517 0.296 0.419 0.419 0.684
boy-lad 3.76 0.186 0.196 0.601 0.631 0.471 0 0.974
automobile-car 3.92 0.654 0.668 0.834 0.427 1 0.686 0.980
midday-noon 3.42 0.106 0.112 0.135 0.586 0.289 0.856 0.819
gem-jewel 3.84 0.295 0.309 0.094 0.687 0.211 1 0.686
Correlation 1 0.259 0.267 0.382 0.548 0.579 0.693 0.834
</table>
<bodyText confidence="0.99870645">
ports the highest correlation of 0.8129 in our ex-
periments. Our implementation of Co-occurrence
Double Checking (CODC) measure (Chen et al.,
2006) reports the second best correlation of 0.6936.
However, CODC measure reports zero similarity for
many word-pairs. This is because for a word-pair
(P, Q), we might not necessarily find Q among the
top snippets for P (and vice versa). CODC mea-
sure returns zero under these conditions. Sahami
et al. (2006) is ranked third with a correlation of
0.5797. Among the four page counts based mea-
sures WebPMI reports the highest correlation (r =
0.5489). Overall, the results in Table 4.3 suggest
that snippet-based measures are more accurate than
page counts-based measures in capturing semantic
similarity. This is evident for word-pairs where at
least one of the words is a polysemous word (e.g.,
pairs that include cock, brother). Page counts-based
measures do not consider the context in which the
words appear in a page, thus cannot disambiguate
</bodyText>
<tableCaption confidence="0.99983">
Table 4: Comparison with taxonomy based methods
</tableCaption>
<table confidence="0.9645597">
Method correlation
Human replication 0.901
Resnik (1995) 0.745
Lin (1998) 0.822
Li et al (2003) 0.891
Edge-counting 0.664
Information content 0.745
Jiang &amp; Conrath (1998) 0.848
proposed (SVM) 0.834
the multiple senses.
</table>
<bodyText confidence="0.990315777777778">
As summarized in Table 4.3, proposed method
is comparable with the WordNet based methods.
In fact, the proposed method outperforms simple
WordNet based approaches such as Edge-Counting
and Information Content measures. However, con-
sidering the high correlation between human sub-
jects (0.9), there is still room for improvement.
Figure 2 illustrates the effect of the number
of snippets on the performance of the proposed
</bodyText>
<page confidence="0.996986">
345
</page>
<table confidence="0.8826156">
0 100 200 300 400 500 600 700 800 900 1000
Table 5: Performance of named entity clustering
Method Precision Recall F Measure
WebJaccard 0.5926 0.712 0.6147
WebOverlap 0.5976 0.68 0.5965
WebDice 0.5895 0.716 0.6179
WebPMI 0.2649 0.428 0.2916
Sahami(2006) 0.6384 0.668 0.6426
Chen (2006) 0.4763 0.624 0.4984
Proposed 0.7958 0.804 0.7897
</table>
<figure confidence="0.986124461538462">
0.80
Correlation Coefficient
0.79
0.78
0.77
0.76
0.75
0.74
0.73
0.72
0.71
0.70
Number of snippets
</figure>
<figureCaption confidence="0.989112666666667">
Figure 2: Correlation vs No of snippets
Figure 3: Correlation vs No of positive and negative
training instances
</figureCaption>
<bodyText confidence="0.9972128">
method. Correlation coefficient steadily improves
with the number of snippets used for extracting pat-
terns. When few snippets are processed only a few
patterns are found, thus the feature vector becomes
sparse, resulting in poor performance. Figure 3 de-
picts the correlation with human ratings for various
combinations of positive and negative training in-
stances. Maximum correlation coefficient of 0.834
is achieved with 1900 positive training examples and
2400 negative training examples. Moreover, Fig-
ure 3 reveals that correlation does not improve be-
yond 2500 positive and negative training examples.
Therefore, we can conclude that 2500 examples are
sufficient to leverage the proposed semantic similar-
ity measure.
</bodyText>
<subsectionHeader confidence="0.980479">
4.4 Named Entity Clustering
</subsectionHeader>
<bodyText confidence="0.999991392857143">
Measuring semantic similarity between named en-
tities is vital in many applications such as query
expansion (Sahami and Heilman, 2006) and com-
munity mining (Matsuo et al., 2006a). Since most
named entities are not covered by WordNet, simi-
larity measures based on WordNet alone cannot be
used in such tasks. Unlike common English words,
named entities are constantly being created. Manu-
ally maintaining an up-to-date taxonomy of named
entities is costly, if not impossible. The proposed
semantic similarity measure is appealing as it does
not require pre-compiled taxonomies. In order to
evaluate the performance of the proposed measure
in capturing the semantic similarity between named
entities, we set up a named entity clustering task.
We selected 50 person names from 5 categories :
tennis players, golfers, actors, politicians and scien-
tists, (10 names from each category) from the dmoz
directory 7. For each pair of names in our dataset,
we measure the association between the two names
using the proposed method and baselines. We use
group-average agglomerative hierarchical clustering
to cluster the names in our dataset into five clusters.
We employed the B-CUBED metric (Bagga and
Baldwin, 1998) to evaluate the clustering results. As
summarized in Table 5 the proposed method outper-
forms all the baselines with a statistically significant
(p &lt; 0.01 Tukey HSD) F score of 0.7897.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999931833333333">
We propose an SVM-based approach to combine
page counts and lexico-syntactic patterns extracted
from snippets to leverage a robust web-based seman-
tic similarity measure. The proposed similarity mea-
sure outperforms existing web-based similarity mea-
sures and competes with models trained on Word-
Net. It requires just 2500 synonymous word-pairs,
automatically extracted from WordNet synsets, for
training. Moreover, the proposed method proves
useful in a named entity clustering task. In future,
we intend to apply the proposed method to automat-
ically extract synonyms from the web.
</bodyText>
<footnote confidence="0.864814">
7http://dmoz.org
</footnote>
<figure confidence="0.9980907">
correlation
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
4000
3500
3000
2500
2000
1500 positive examples
1000
500 1000 1500 2000
2500 3000 3500 4000
negative examples 500
</figure>
<page confidence="0.995604">
346
</page>
<sectionHeader confidence="0.995522" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999323482758621">
A. Bagga and B. Baldwin. 1998. Entity-based cross doc-
ument coreferencing using the vector space model. In
Proc. of 36th COLING-ACL, pages 79–85.
Z. Bar-Yossef and M. Gurevich. 2006. Random sam-
pling from a search engine’s index. In Proceedings of
15th International World Wide Web Conference.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proc. of the COLING/ACL 2006, pages 1009–1016.
J. Curran. 2002. Ensemble menthods for automatic the-
saurus extraction. In Proc. of EMNLP.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of 14th COLING,
pages 539–545.
J.J. Jiang and D.W. Conrath. 1998. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of the International Conference on Research in
Computational Linguistics ROCLING X.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459–484.
M. Lapata and F. Keller. 2005. Web-based models ofr
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1–31.
D. Lin. 1998a. Automatic retreival and clustering of sim-
ilar words. In Proc. of the 17th COLING, pages 768–
774.
D. Lin. 1998b. An information-theoretic definition of
similarity. In Proc. of the 15th ICML, pages 296–304.
C. D. Manning and H. Sch¨utze. 2002. Foundations of
Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Y. Matsuo, J. Mori, M. Hamasaki, K. Ishida,
T. Nishimura, H. Takeda, K. Hasida, and M. Ishizuka.
2006a. Polyphonet: An advanced social network ex-
traction system. In Proc. of 15th International World
Wide Web Conference.
Y. Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.
2006b. Graph-based word clustering using web search
engine. In Proc. of EMNLP 2006.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1–28.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts - step one: the one-million fact extraction chal-
lenge. In Proc. ofAAAI-2006.
J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. Advances in Large Margin Classifiers, pages
61–74.
R. Rada, H. Mili, E. Bichnell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. IEEE Transactions on Systems, Man and Cyber-
netics, 9(1):17–30.
P. Resnik and N. A. Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349–380.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of
14th International Joint Conference on Aritificial In-
telligence.
P. Resnik. 1999. Semantic similarity in a taxonomy: An
information based measure and its application to prob-
lems of ambiguity in natural language. Journal ofAr-
itificial Intelligence Research, 11:95–130.
R. Rosenfield. 1996. A maximum entropy approach to
adaptive statistical modelling. Computer Speech and
Language, 10:187–228.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8:627–633.
M. Sahami and T. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proc. of 15th International World Wide
Web Conference.
E. Terra and C.L.A. Clarke. 2003. Frequency estimates
for statistical word similarity measures. In Proc. of the
NAACL/HLT, pages 165–172.
P. D. Turney. 2001. Minning the web for synonyms:
Pmi-ir versus lsa on toefl. In Proc. of ECML-2001,
pages 491–502.
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, GB.
D. McLean Y. Li, Zuhair A. Bandar. 2003. An approch
for measuring semantic similarity between words us-
ing multiple information sources. IEEE Transactions
on Knowledge and Data Engineering, 15(4):871–882.
</reference>
<page confidence="0.998541">
347
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.347538">
<title confidence="0.9992365">An Integrated Approach to Measuring Semantic Similarity between Words Using Information available on the Web</title>
<author confidence="0.998632">Danushka Yutaka Mitsuru</author>
<affiliation confidence="0.998286">The University of National Institute of The University of</affiliation>
<address confidence="0.9688925">7-3-1, Hongo, Industrial Science 7-3-1, Hongo, 113-8656, 1-18-13, Sotokanda, 113-8656,</address>
<email confidence="0.6050735">tokyo.ac.jp101-0021,tokyo.ac.jpy.matsuo@aist.go.jp</email>
<abstract confidence="0.99831536">Measuring semantic similarity between words is vital for various applications in natural language processing, such as language modeling, information retrieval, and document clustering. We propose a method that utilizes the information available on the Web to measure semantic similarity between a pair of words or entities. We integrate page counts for each word in the pair and lexico-syntactic patterns that occur among the top ranking snippets for using support vector machines. Experimental results on Miller- Charles’ benchmark data set show that the proposed measure outperforms all the existing web based semantic similarity measures by a wide margin, achieving a corcoefficient of Moreover, the proposed semantic similarity measure improves the accuracy of in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based cross document coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proc. of 36th COLING-ACL,</booktitle>
<pages>79--85</pages>
<contexts>
<context position="24810" citStr="Bagga and Baldwin, 1998" startWordPosition="4087" endWordPosition="4090">er to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering task. We selected 50 person names from 5 categories : tennis players, golfers, actors, politicians and scientists, (10 names from each category) from the dmoz directory 7. For each pair of names in our dataset, we measure the association between the two names using the proposed method and baselines. We use group-average agglomerative hierarchical clustering to cluster the names in our dataset into five clusters. We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. As summarized in Table 5 the proposed method outperforms all the baselines with a statistically significant (p &lt; 0.01 Tukey HSD) F score of 0.7897. 5 Conclusion We propose an SVM-based approach to combine page counts and lexico-syntactic patterns extracted from snippets to leverage a robust web-based semantic similarity measure. The proposed similarity measure outperforms existing web-based similarity measures and competes with models trained on WordNet. It requires just 2500 synonymous word-pairs, automatically extracted from WordNet synsets, for training.</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-based cross document coreferencing using the vector space model. In Proc. of 36th COLING-ACL, pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Bar-Yossef</author>
<author>M Gurevich</author>
</authors>
<title>Random sampling from a search engine’s index.</title>
<date>2006</date>
<booktitle>In Proceedings of 15th International World Wide Web Conference.</booktitle>
<contexts>
<context position="10586" citStr="Bar-Yossef and Gurevich, 2006" startWordPosition="1735" endWordPosition="1738">ebDice as a variant of Dice coefficient. WebDice(P, Q) is defined as, WebDice(P, Q) (0 if H(P ∩ Q) ≤ c Sl =(3) H(P)+H(Q) 2H(PnQ) otherwise We define WebPMI as a variant form of PMI using page counts by, WebPMI(P, Q) 0 if H(P ∩ Q) ≤ c �(4) ) otherwise Here, N is the number of documents indexed by the search engine. Probabilities in Formula 4 are estimated according to the maximum likelihood principle. In order to accurately calculate PMI using Formula 4, we must know N, the number of documents indexed by the search engine. Although estimating the number of documents indexed by a search engine (Bar-Yossef and Gurevich, 2006) is an interesting task itself, it is beyond the scope of this work. In this work, we set N = 1010 according to the number of indexed pages reported by Google. 3.2 Snippets-based Synonymous Word Patterns Page counts-based similarity measures do not consider the relative distance between P and Q in a page or the length of the page. Although P and Q occur in a page they might not be related at all. Therefore, page counts-based similarity measures are prone to noise and are not reliable when H(P ∩Q) is low. On the other hand snippets capture the local context of query words. We propose lexico-syn</context>
</contexts>
<marker>Bar-Yossef, Gurevich, 2006</marker>
<rawString>Z. Bar-Yossef and M. Gurevich. 2006. Random sampling from a search engine’s index. In Proceedings of 15th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
<author>M Lin</author>
<author>Y Wei</author>
</authors>
<title>Novel association measures using web search with double checking.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING/ACL</booktitle>
<pages>1009--1016</pages>
<contexts>
<context position="7781" citStr="Chen et al., (2006)" startWordPosition="1229" endWordPosition="1232">eb hits for each individual name and their conjunction. Sahami et al., (2006) measure semantic similarity between two queries using the snippets returned for those queries by a search engine. For each query, they collect snippets from a search engine and represent each snippet as a TF-IDF weighted term vector. Each vector is L2 normalized and the centroid of the set of vectors is computed. Semantic similarity between two queries is then defined as the inner product between the corresponding centroid vectors. They do not compare their similarity measure with taxonomy based similarity measures. Chen et al., (2006) propose a web-based double341 checking model to compute semantic similarity between words. For two words P and Q, they collect snippets for each word from a web search engine. Then they count the number of occurrences of word P in the snippets for word Q and the number of occurrences of word Q in the snippets for word P. These values are combined non-linearly to compute the similarity between P and Q. This method heavily depends on the search engine’s ranking algorithm. Although two words P and Q may be very similar, there is no reason to believe that one can find Q in the snippets for P, or </context>
<context position="20778" citStr="Chen et al., 2006" startWordPosition="3461" endWordPosition="3464"> 0.863 0.233 0.671 1 journey-voyage 3.84 0.415 0.431 0.182 0.467 0.524 0.417 0.996 coast-shore 3.7 0.786 0.796 0.521 0.561 0.381 0.518 0.945 implement-tool 2.95 1 1 0.517 0.296 0.419 0.419 0.684 boy-lad 3.76 0.186 0.196 0.601 0.631 0.471 0 0.974 automobile-car 3.92 0.654 0.668 0.834 0.427 1 0.686 0.980 midday-noon 3.42 0.106 0.112 0.135 0.586 0.289 0.856 0.819 gem-jewel 3.84 0.295 0.309 0.094 0.687 0.211 1 0.686 Correlation 1 0.259 0.267 0.382 0.548 0.579 0.693 0.834 ports the highest correlation of 0.8129 in our experiments. Our implementation of Co-occurrence Double Checking (CODC) measure (Chen et al., 2006) reports the second best correlation of 0.6936. However, CODC measure reports zero similarity for many word-pairs. This is because for a word-pair (P, Q), we might not necessarily find Q among the top snippets for P (and vice versa). CODC measure returns zero under these conditions. Sahami et al. (2006) is ranked third with a correlation of 0.5797. Among the four page counts based measures WebPMI reports the highest correlation (r = 0.5489). Overall, the results in Table 4.3 suggest that snippet-based measures are more accurate than page counts-based measures in capturing semantic similarity. </context>
</contexts>
<marker>Chen, Lin, Wei, 2006</marker>
<rawString>H. Chen, M. Lin, and Y. Wei. 2006. Novel association measures using web search with double checking. In Proc. of the COLING/ACL 2006, pages 1009–1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>Ensemble menthods for automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1854" citStr="Curran, 2002" startWordPosition="263" endWordPosition="264"> significantly improves the accuracy (Fmeasure of 0.78) in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, location names, product names) and the new uses of existing words. For example, apple is frequently associat</context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>J. Curran. 2002. Ensemble menthods for automatic thesaurus extraction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of 14th COLING,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="4543" citStr="Hearst, 1992" startWordPosition="698" endWordPosition="699">that contain the query words 2. A snippet is a brief window of text extracted by a search engine around the query term in a document. Snippets provide useful information about the immediate context of the query term. This paper proposes a Web-based semantic similarity metric which combines page counts and snippets using support vector machines. We extract lexico-syntactic patterns from snippets. For example, X is a Y indicates there is a high semantic similarity between X and Y. Automatically extracted lexico-syntactic patterns have been successfully employed in various term extraction tasks (Hearst, 1992). Our contributions are summarized as follows: • We propose a lexico-syntactic patterns-based approach to compute semantic similarity using snippets obtained from a Web search engine. • We integrate different Web-based similarity scores using WordNet synsets and support vector machines to create a robust semantic similarity measure. The integrated measure outperforms all existing Web-based semantic similarity measures in a benchmark dataset and a named entity clustering task. To the best of our knowledge, this is the first attempt to combine both WordNet synsets and Web content to leverage a r</context>
<context position="11927" citStr="Hearst, 1992" startWordPosition="1974" endWordPosition="1975">in our experiments = I H(PnQ) N H(P) N lo92( H(Q) N 342 To illustrate our pattern extraction algorithm consider the following snippet from Google for the query jaguar AND cat. ”The Jaguar is the largest cat in Western Hemisphere and can subdue a larger prey than can the puma” Here, the phrase is the largest indicates a hypernymic relationship between Jaguar and the cat. Phrases such as also known as, is a, part of, is an example of all indicate various of semantic relations. Such indicative phrases have been successfully applied in various tasks such as synonym extraction, hyponym extraction (Hearst, 1992) and fact extraction (Pasca et al., 2006). We describe our pattern extraction algorithm in three steps. Step 1 We replace the two query terms in a snippet by two wildcards X and Y. We extract all word n-grams that contain both X and Y. In our experiments we extracted n-grams for n = 2 to 5. For example, from the previous snippet we extract the pattern, X is the largest X. In order to leverage the pattern extraction process, we randomly select 5000 pairs of synonymous nouns from WordNet synsets. We ignore the nouns which do not have synonyms in the WordNet. For nouns with more than one sense, w</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M.A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of 14th COLING, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1998</date>
<booktitle>In Proc. of the International Conference on Research in Computational Linguistics ROCLING X.</booktitle>
<contexts>
<context position="2020" citStr="Jiang and Conrath, 1998" startWordPosition="288" endWordPosition="291">c similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, location names, product names) and the new uses of existing words. For example, apple is frequently associated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing </context>
<context position="21839" citStr="Jiang &amp; Conrath (1998)" startWordPosition="3630" endWordPosition="3633">5489). Overall, the results in Table 4.3 suggest that snippet-based measures are more accurate than page counts-based measures in capturing semantic similarity. This is evident for word-pairs where at least one of the words is a polysemous word (e.g., pairs that include cock, brother). Page counts-based measures do not consider the context in which the words appear in a page, thus cannot disambiguate Table 4: Comparison with taxonomy based methods Method correlation Human replication 0.901 Resnik (1995) 0.745 Lin (1998) 0.822 Li et al (2003) 0.891 Edge-counting 0.664 Information content 0.745 Jiang &amp; Conrath (1998) 0.848 proposed (SVM) 0.834 the multiple senses. As summarized in Table 4.3, proposed method is comparable with the WordNet based methods. In fact, the proposed method outperforms simple WordNet based approaches such as Edge-Counting and Information Content measures. However, considering the high correlation between human subjects (0.9), there is still room for improvement. Figure 2 illustrates the effect of the number of snippets on the performance of the proposed 345 0 100 200 300 400 500 600 700 800 900 1000 Table 5: Performance of named entity clustering Method Precision Recall F Measure W</context>
</contexts>
<marker>Jiang, Conrath, 1998</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1998. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the International Conference on Research in Computational Linguistics ROCLING X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
<author>M Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="2960" citStr="Keller and Lapata, 2003" startWordPosition="446" endWordPosition="449">l names, location names, product names) and the new uses of existing words. For example, apple is frequently associated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible. The Web can be regarded as a large-scale, dynamic corpus of text. Regarding the Web as a live corpus has become an active research topic recently. Simple, unsupervised models have shown to perform better when n-gram counts are obtained from the Web rather than from a large corpus (Keller and Lapata, 2003; Lapata and Keller, 2005). Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine translation. Turney (2001) defines a point wise mutual information (PMI-IR) measure using the number of hits returned by a Web search engine to recognize synonyms. Matsuo et. al, (2006b) follows a similar 1http://wordnet.princeton.edu/ 340 Proceedings of NAACL HLT 2007, pages 340–347, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics approach to measure the similarity between words and apply their method in a graph-based word clustering a</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>F. Keller and M. Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>Web-based models ofr natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="2986" citStr="Lapata and Keller, 2005" startWordPosition="450" endWordPosition="453">product names) and the new uses of existing words. For example, apple is frequently associated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible. The Web can be regarded as a large-scale, dynamic corpus of text. Regarding the Web as a live corpus has become an active research topic recently. Simple, unsupervised models have shown to perform better when n-gram counts are obtained from the Web rather than from a large corpus (Keller and Lapata, 2003; Lapata and Keller, 2005). Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine translation. Turney (2001) defines a point wise mutual information (PMI-IR) measure using the number of hits returned by a Web search engine to recognize synonyms. Matsuo et. al, (2006b) follows a similar 1http://wordnet.princeton.edu/ 340 Proceedings of NAACL HLT 2007, pages 340–347, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics approach to measure the similarity between words and apply their method in a graph-based word clustering algorithm. Due to the huge </context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>M. Lapata and F. Keller. 2005. Web-based models ofr natural language processing. ACM Transactions on Speech and Language Processing, 2(1):1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retreival and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of the 17th COLING,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="1802" citStr="Lin, 1998" startWordPosition="257" endWordPosition="258">reover, the proposed semantic similarity measure significantly improves the accuracy (Fmeasure of 0.78) in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, location names, product names) and the new uses of exist</context>
<context position="21742" citStr="Lin (1998)" startWordPosition="3617" endWordPosition="3618">ng the four page counts based measures WebPMI reports the highest correlation (r = 0.5489). Overall, the results in Table 4.3 suggest that snippet-based measures are more accurate than page counts-based measures in capturing semantic similarity. This is evident for word-pairs where at least one of the words is a polysemous word (e.g., pairs that include cock, brother). Page counts-based measures do not consider the context in which the words appear in a page, thus cannot disambiguate Table 4: Comparison with taxonomy based methods Method correlation Human replication 0.901 Resnik (1995) 0.745 Lin (1998) 0.822 Li et al (2003) 0.891 Edge-counting 0.664 Information content 0.745 Jiang &amp; Conrath (1998) 0.848 proposed (SVM) 0.834 the multiple senses. As summarized in Table 4.3, proposed method is comparable with the WordNet based methods. In fact, the proposed method outperforms simple WordNet based approaches such as Edge-Counting and Information Content measures. However, considering the high correlation between human subjects (0.9), there is still room for improvement. Figure 2 illustrates the effect of the number of snippets on the performance of the proposed 345 0 100 200 300 400 500 600 700</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998a. Automatic retreival and clustering of similar words. In Proc. of the 17th COLING, pages 768– 774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proc. of the 15th ICML,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="1802" citStr="Lin, 1998" startWordPosition="257" endWordPosition="258">reover, the proposed semantic similarity measure significantly improves the accuracy (Fmeasure of 0.78) in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, location names, product names) and the new uses of exist</context>
<context position="21742" citStr="Lin (1998)" startWordPosition="3617" endWordPosition="3618">ng the four page counts based measures WebPMI reports the highest correlation (r = 0.5489). Overall, the results in Table 4.3 suggest that snippet-based measures are more accurate than page counts-based measures in capturing semantic similarity. This is evident for word-pairs where at least one of the words is a polysemous word (e.g., pairs that include cock, brother). Page counts-based measures do not consider the context in which the words appear in a page, thus cannot disambiguate Table 4: Comparison with taxonomy based methods Method correlation Human replication 0.901 Resnik (1995) 0.745 Lin (1998) 0.822 Li et al (2003) 0.891 Edge-counting 0.664 Information content 0.745 Jiang &amp; Conrath (1998) 0.848 proposed (SVM) 0.834 the multiple senses. As summarized in Table 4.3, proposed method is comparable with the WordNet based methods. In fact, the proposed method outperforms simple WordNet based approaches such as Edge-Counting and Information Content measures. However, considering the high correlation between human subjects (0.9), there is still room for improvement. Figure 2 illustrates the effect of the number of snippets on the performance of the proposed 345 0 100 200 300 400 500 600 700</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998b. An information-theoretic definition of similarity. In Proc. of the 15th ICML, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>2002</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 2002</marker>
<rawString>C. D. Manning and H. Sch¨utze. 2002. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>J Mori</author>
<author>M Hamasaki</author>
<author>K Ishida</author>
<author>T Nishimura</author>
<author>H Takeda</author>
<author>K Hasida</author>
<author>M Ishizuka</author>
</authors>
<title>Polyphonet: An advanced social network extraction system.</title>
<date>2006</date>
<booktitle>In Proc. of 15th International World Wide Web Conference.</booktitle>
<contexts>
<context position="6963" citStr="Matsuo et al., (2006" startWordPosition="1095" endWordPosition="1098">d using the Brown corpus. Li et al., (2003) combines structural semantic information from a lexical taxonomy and information content from a corpus in a non-linear model. They propose a similarity measure that uses shortest path length, depth and local density in a taxonomy. Their experiments using WordNet and the Brown corpus reports a Pearson correlation coefficient of 0.8914 on the Miller and Charles’ (1998) benchmark dataset. They do not evaluate their method on similarities between named entities. Recently, some work has been carried out on measuring semantic similarity using web content. Matsuo et al., (2006a) propose the use of Web hits for the extraction of communities on the Web. They measure the association between two personal names using the overlap coefficient, calculated based on the number of Web hits for each individual name and their conjunction. Sahami et al., (2006) measure semantic similarity between two queries using the snippets returned for those queries by a search engine. For each query, they collect snippets from a search engine and represent each snippet as a TF-IDF weighted term vector. Each vector is L2 normalized and the centroid of the set of vectors is computed. Semantic</context>
<context position="23781" citStr="Matsuo et al., 2006" startWordPosition="3927" endWordPosition="3930">s combinations of positive and negative training instances. Maximum correlation coefficient of 0.834 is achieved with 1900 positive training examples and 2400 negative training examples. Moreover, Figure 3 reveals that correlation does not improve beyond 2500 positive and negative training examples. Therefore, we can conclude that 2500 examples are sufficient to leverage the proposed semantic similarity measure. 4.4 Named Entity Clustering Measuring semantic similarity between named entities is vital in many applications such as query expansion (Sahami and Heilman, 2006) and community mining (Matsuo et al., 2006a). Since most named entities are not covered by WordNet, similarity measures based on WordNet alone cannot be used in such tasks. Unlike common English words, named entities are constantly being created. Manually maintaining an up-to-date taxonomy of named entities is costly, if not impossible. The proposed semantic similarity measure is appealing as it does not require pre-compiled taxonomies. In order to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering task. We selected 50 person names from 5 ca</context>
</contexts>
<marker>Matsuo, Mori, Hamasaki, Ishida, Nishimura, Takeda, Hasida, Ishizuka, 2006</marker>
<rawString>Y. Matsuo, J. Mori, M. Hamasaki, K. Ishida, T. Nishimura, H. Takeda, K. Hasida, and M. Ishizuka. 2006a. Polyphonet: An advanced social network extraction system. In Proc. of 15th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>T Sakaki</author>
<author>K Uchiyama</author>
<author>M Ishizuka</author>
</authors>
<title>Graph-based word clustering using web search engine.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="6963" citStr="Matsuo et al., (2006" startWordPosition="1095" endWordPosition="1098">d using the Brown corpus. Li et al., (2003) combines structural semantic information from a lexical taxonomy and information content from a corpus in a non-linear model. They propose a similarity measure that uses shortest path length, depth and local density in a taxonomy. Their experiments using WordNet and the Brown corpus reports a Pearson correlation coefficient of 0.8914 on the Miller and Charles’ (1998) benchmark dataset. They do not evaluate their method on similarities between named entities. Recently, some work has been carried out on measuring semantic similarity using web content. Matsuo et al., (2006a) propose the use of Web hits for the extraction of communities on the Web. They measure the association between two personal names using the overlap coefficient, calculated based on the number of Web hits for each individual name and their conjunction. Sahami et al., (2006) measure semantic similarity between two queries using the snippets returned for those queries by a search engine. For each query, they collect snippets from a search engine and represent each snippet as a TF-IDF weighted term vector. Each vector is L2 normalized and the centroid of the set of vectors is computed. Semantic</context>
<context position="23781" citStr="Matsuo et al., 2006" startWordPosition="3927" endWordPosition="3930">s combinations of positive and negative training instances. Maximum correlation coefficient of 0.834 is achieved with 1900 positive training examples and 2400 negative training examples. Moreover, Figure 3 reveals that correlation does not improve beyond 2500 positive and negative training examples. Therefore, we can conclude that 2500 examples are sufficient to leverage the proposed semantic similarity measure. 4.4 Named Entity Clustering Measuring semantic similarity between named entities is vital in many applications such as query expansion (Sahami and Heilman, 2006) and community mining (Matsuo et al., 2006a). Since most named entities are not covered by WordNet, similarity measures based on WordNet alone cannot be used in such tasks. Unlike common English words, named entities are constantly being created. Manually maintaining an up-to-date taxonomy of named entities is costly, if not impossible. The proposed semantic similarity measure is appealing as it does not require pre-compiled taxonomies. In order to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering task. We selected 50 person names from 5 ca</context>
</contexts>
<marker>Matsuo, Sakaki, Uchiyama, Ishizuka, 2006</marker>
<rawString>Y. Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka. 2006b. Graph-based word clustering using web search engine. In Proc. of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1998</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="8556" citStr="Miller and Charles (1998)" startWordPosition="1374" endWordPosition="1377"> from a web search engine. Then they count the number of occurrences of word P in the snippets for word Q and the number of occurrences of word Q in the snippets for word P. These values are combined non-linearly to compute the similarity between P and Q. This method heavily depends on the search engine’s ranking algorithm. Although two words P and Q may be very similar, there is no reason to believe that one can find Q in the snippets for P, or vice versa. This observation is confirmed by the experimental results in their paper which reports 0 similarity scores for many pairs of words in the Miller and Charles (1998) data set. 3 Method In this section we will describe the various similarity features we use in our model. We utilize page counts and snippets returned by the Google 3 search engine for simple text queries to define various similarity scores. 3.1 Page Counts-based Similarity Scores For the rest of this paper we use the notation H(P) to denote the page count for the query P in a search engine. Terra and Clarke (2003) compare various similarity scores for measuring similarity between words in a corpus. We modify the traditional Jaccard, overlap (Simpson), Dice and PMI measures for the purpose of </context>
</contexts>
<marker>Miller, Charles, 1998</marker>
<rawString>G. Miller and W. Charles. 1998. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts - step one: the one-million fact extraction challenge.</title>
<date>2006</date>
<booktitle>In Proc. ofAAAI-2006.</booktitle>
<contexts>
<context position="11968" citStr="Pasca et al., 2006" startWordPosition="1980" endWordPosition="1983">) N lo92( H(Q) N 342 To illustrate our pattern extraction algorithm consider the following snippet from Google for the query jaguar AND cat. ”The Jaguar is the largest cat in Western Hemisphere and can subdue a larger prey than can the puma” Here, the phrase is the largest indicates a hypernymic relationship between Jaguar and the cat. Phrases such as also known as, is a, part of, is an example of all indicate various of semantic relations. Such indicative phrases have been successfully applied in various tasks such as synonym extraction, hyponym extraction (Hearst, 1992) and fact extraction (Pasca et al., 2006). We describe our pattern extraction algorithm in three steps. Step 1 We replace the two query terms in a snippet by two wildcards X and Y. We extract all word n-grams that contain both X and Y. In our experiments we extracted n-grams for n = 2 to 5. For example, from the previous snippet we extract the pattern, X is the largest X. In order to leverage the pattern extraction process, we randomly select 5000 pairs of synonymous nouns from WordNet synsets. We ignore the nouns which do not have synonyms in the WordNet. For nouns with more than one sense, we select synonyms from its dominant sense</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Organizing and searching the world wide web of facts - step one: the one-million fact extraction challenge. In Proc. ofAAAI-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparison to regularized likelihood methods.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<contexts>
<context position="15444" citStr="Platt, 2000" startWordPosition="2602" endWordPosition="2603">ach pattern by dividing from the total frequency of all patterns. Moreover, we compute the page counts-based features as given by formulae (1-4). Using the 200 pattern features and the 4 page counts-based features we create 204 dimensional feature vectors for each training instance in our synonymous and nonsynonymous datasets. We train a two class support vector machine (SVM) (Vapnik, 1998), where class 2 (P + N)(pv(N − nv) − nv(P − pv))2 x= 343 +1 represents synonymous word-pairs and class −1 represents non-synonymous word-pairs. Finally, SVM outputs are converted to posterior probabilities (Platt, 2000). We consider the posterior probability of a given pair of words belonging to class +1 as the semantic similarity between the two words. 4 Experiments To evaluate the performance of the proposed semantic similarity measure, we conduct two sets of experiments. Firstly, we compare the similarity scores produced by the proposed measure against the Miller-Charles’ benchmark dataset. We analyze the performance of the proposed measure with the number of snippets and the size of the training data set. Secondly, we apply the proposed measure in a real-world named entity clustering task and measure its</context>
</contexts>
<marker>Platt, 2000</marker>
<rawString>J. Platt. 2000. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. Advances in Large Margin Classifiers, pages 61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rada</author>
<author>H Mili</author>
<author>E Bichnell</author>
<author>M Blettner</author>
</authors>
<title>Development and application of a metric on semantic nets.</title>
<date>1989</date>
<journal>IEEE Transactions on Systems, Man and Cybernetics,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="5528" citStr="Rada et al., 1989" startWordPosition="854" endWordPosition="857">ms all existing Web-based semantic similarity measures in a benchmark dataset and a named entity clustering task. To the best of our knowledge, this is the first attempt to combine both WordNet synsets and Web content to leverage a robust semantic similarity measure. 2 Previous Work Given a taxonomy of concepts, a straightforward method for calculating similarity between two words (concepts) is to find the length of the shortest path 2page count may not necessarily be equal to the word frequency because the queried word may appear many times in a page connecting the two words in the taxonomy (Rada et al., 1989). If a word is polysemous (i.e., having more than one sense) then multiple paths may exist between the two words. In such cases only the shortest path between any two senses of the words is considered for the calculation of similarity. A problem frequently acknowledged with this approach is that it relies on the notion that all links in the taxonomy represent uniform distances. Resnik (1995) proposes a similarity measure based on information content. He defines the similarity between two concepts C1 and C2 in the taxonomy as the maximum of the information content of all concepts C that subsume</context>
</contexts>
<marker>Rada, Mili, Bichnell, Blettner, 1989</marker>
<rawString>R. Rada, H. Mili, E. Bichnell, and M. Blettner. 1989. Development and application of a metric on semantic nets. IEEE Transactions on Systems, Man and Cybernetics, 9(1):17–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>N A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="3011" citStr="Resnik and Smith (2003)" startWordPosition="454" endWordPosition="457"> uses of existing words. For example, apple is frequently associated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible. The Web can be regarded as a large-scale, dynamic corpus of text. Regarding the Web as a live corpus has become an active research topic recently. Simple, unsupervised models have shown to perform better when n-gram counts are obtained from the Web rather than from a large corpus (Keller and Lapata, 2003; Lapata and Keller, 2005). Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine translation. Turney (2001) defines a point wise mutual information (PMI-IR) measure using the number of hits returned by a Web search engine to recognize synonyms. Matsuo et. al, (2006b) follows a similar 1http://wordnet.princeton.edu/ 340 Proceedings of NAACL HLT 2007, pages 340–347, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics approach to measure the similarity between words and apply their method in a graph-based word clustering algorithm. Due to the huge number of documents and t</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>P. Resnik and N. A. Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proc. of 14th International Joint Conference on Aritificial Intelligence.</booktitle>
<contexts>
<context position="1995" citStr="Resnik, 1995" startWordPosition="286" endWordPosition="287">apture semantic similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, location names, product names) and the new uses of existing words. For example, apple is frequently associated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words an</context>
<context position="5922" citStr="Resnik (1995)" startWordPosition="925" endWordPosition="926">o find the length of the shortest path 2page count may not necessarily be equal to the word frequency because the queried word may appear many times in a page connecting the two words in the taxonomy (Rada et al., 1989). If a word is polysemous (i.e., having more than one sense) then multiple paths may exist between the two words. In such cases only the shortest path between any two senses of the words is considered for the calculation of similarity. A problem frequently acknowledged with this approach is that it relies on the notion that all links in the taxonomy represent uniform distances. Resnik (1995) proposes a similarity measure based on information content. He defines the similarity between two concepts C1 and C2 in the taxonomy as the maximum of the information content of all concepts C that subsume both C1 and C2. Then the similarity between two words are defined as the maximum of the similarity between any concepts that the words belong to. He uses WordNet as the taxonomy and information content is calculated using the Brown corpus. Li et al., (2003) combines structural semantic information from a lexical taxonomy and information content from a corpus in a non-linear model. They prop</context>
<context position="21725" citStr="Resnik (1995)" startWordPosition="3614" endWordPosition="3615">ation of 0.5797. Among the four page counts based measures WebPMI reports the highest correlation (r = 0.5489). Overall, the results in Table 4.3 suggest that snippet-based measures are more accurate than page counts-based measures in capturing semantic similarity. This is evident for word-pairs where at least one of the words is a polysemous word (e.g., pairs that include cock, brother). Page counts-based measures do not consider the context in which the words appear in a page, thus cannot disambiguate Table 4: Comparison with taxonomy based methods Method correlation Human replication 0.901 Resnik (1995) 0.745 Lin (1998) 0.822 Li et al (2003) 0.891 Edge-counting 0.664 Information content 0.745 Jiang &amp; Conrath (1998) 0.848 proposed (SVM) 0.834 the multiple senses. As summarized in Table 4.3, proposed method is comparable with the WordNet based methods. In fact, the proposed method outperforms simple WordNet based approaches such as Edge-Counting and Information Content measures. However, considering the high correlation between human subjects (0.9), there is still room for improvement. Figure 2 illustrates the effect of the number of snippets on the performance of the proposed 345 0 100 200 30</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proc. of 14th International Joint Conference on Aritificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>Journal ofAritificial Intelligence Research,</journal>
<pages>11--95</pages>
<contexts>
<context position="1733" citStr="Resnik, 1999" startWordPosition="248" endWordPosition="249">asures by a wide margin, achieving a correlation coefficient of 0.834. Moreover, the proposed semantic similarity measure significantly improves the accuracy (Fmeasure of 0.78) in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., per</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>P. Resnik. 1999. Semantic similarity in a taxonomy: An information based measure and its application to problems of ambiguity in natural language. Journal ofAritificial Intelligence Research, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfield</author>
</authors>
<title>A maximum entropy approach to adaptive statistical modelling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="1771" citStr="Rosenfield, 1996" startWordPosition="253" endWordPosition="254"> a correlation coefficient of 0.834. Moreover, the proposed semantic similarity measure significantly improves the accuracy (Fmeasure of 0.78) in a named entity clustering task, proving the capability of the proposed measure to capture semantic similarity using web content. 1 Introduction The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language modeling (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002). Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, semantic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, location names, product n</context>
</contexts>
<marker>Rosenfield, 1996</marker>
<rawString>R. Rosenfield. 1996. A maximum entropy approach to adaptive statistical modelling. Computer Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<pages>8--627</pages>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J.B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahami</author>
<author>T Heilman</author>
</authors>
<title>A web-based kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proc. of 15th International World Wide Web Conference.</booktitle>
<contexts>
<context position="23739" citStr="Sahami and Heilman, 2006" startWordPosition="3919" endWordPosition="3922">ts the correlation with human ratings for various combinations of positive and negative training instances. Maximum correlation coefficient of 0.834 is achieved with 1900 positive training examples and 2400 negative training examples. Moreover, Figure 3 reveals that correlation does not improve beyond 2500 positive and negative training examples. Therefore, we can conclude that 2500 examples are sufficient to leverage the proposed semantic similarity measure. 4.4 Named Entity Clustering Measuring semantic similarity between named entities is vital in many applications such as query expansion (Sahami and Heilman, 2006) and community mining (Matsuo et al., 2006a). Since most named entities are not covered by WordNet, similarity measures based on WordNet alone cannot be used in such tasks. Unlike common English words, named entities are constantly being created. Manually maintaining an up-to-date taxonomy of named entities is costly, if not impossible. The proposed semantic similarity measure is appealing as it does not require pre-compiled taxonomies. In order to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering t</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>M. Sahami and T. Heilman. 2006. A web-based kernel function for measuring the similarity of short text snippets. In Proc. of 15th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Terra</author>
<author>C L A Clarke</author>
</authors>
<title>Frequency estimates for statistical word similarity measures.</title>
<date>2003</date>
<booktitle>In Proc. of the NAACL/HLT,</booktitle>
<pages>165--172</pages>
<contexts>
<context position="8974" citStr="Terra and Clarke (2003)" startWordPosition="1448" endWordPosition="1451">d Q in the snippets for P, or vice versa. This observation is confirmed by the experimental results in their paper which reports 0 similarity scores for many pairs of words in the Miller and Charles (1998) data set. 3 Method In this section we will describe the various similarity features we use in our model. We utilize page counts and snippets returned by the Google 3 search engine for simple text queries to define various similarity scores. 3.1 Page Counts-based Similarity Scores For the rest of this paper we use the notation H(P) to denote the page count for the query P in a search engine. Terra and Clarke (2003) compare various similarity scores for measuring similarity between words in a corpus. We modify the traditional Jaccard, overlap (Simpson), Dice and PMI measures for the purpose of measuring similarity using page counts. WebJaccard coefficient between words (or phrases) P and Q, WebJaccard(P, Q), is defined by, WebJaccard(P, Q) if H(P ∩ Q) ≤ c � 0 ( 1 )� = H(PnQ) otherwise H(P)+H(Q)−H(PnQ) Here, P ∩ Q denotes the conjunction query P AND Q. Given the scale and noise in the Web, some words might occur arbitrarily, i.e. by random chance, on some pages. Given the scale and noise in web data, it i</context>
</contexts>
<marker>Terra, Clarke, 2003</marker>
<rawString>E. Terra and C.L.A. Clarke. 2003. Frequency estimates for statistical word similarity measures. In Proc. of the NAACL/HLT, pages 165–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Minning the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In Proc. of ECML-2001,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="3118" citStr="Turney (2001)" startWordPosition="472" endWordPosition="473">e is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible. The Web can be regarded as a large-scale, dynamic corpus of text. Regarding the Web as a live corpus has become an active research topic recently. Simple, unsupervised models have shown to perform better when n-gram counts are obtained from the Web rather than from a large corpus (Keller and Lapata, 2003; Lapata and Keller, 2005). Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine translation. Turney (2001) defines a point wise mutual information (PMI-IR) measure using the number of hits returned by a Web search engine to recognize synonyms. Matsuo et. al, (2006b) follows a similar 1http://wordnet.princeton.edu/ 340 Proceedings of NAACL HLT 2007, pages 340–347, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics approach to measure the similarity between words and apply their method in a graph-based word clustering algorithm. Due to the huge number of documents and the high growth rate of the Web, it is difficult to directly analyze each individual document separately. Se</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. D. Turney. 2001. Minning the web for synonyms: Pmi-ir versus lsa on toefl. In Proc. of ECML-2001, pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley,</publisher>
<location>Chichester, GB.</location>
<contexts>
<context position="15225" citStr="Vapnik, 1998" startWordPosition="2568" endWordPosition="2569">erns are shown in Table 2. 3.3 Training For each pair of synonymous and non-synonymous words in our datasets, we count the frequency of occurrence of the patterns selected in Step 3. We normalize the frequency count of each pattern by dividing from the total frequency of all patterns. Moreover, we compute the page counts-based features as given by formulae (1-4). Using the 200 pattern features and the 4 page counts-based features we create 204 dimensional feature vectors for each training instance in our synonymous and nonsynonymous datasets. We train a two class support vector machine (SVM) (Vapnik, 1998), where class 2 (P + N)(pv(N − nv) − nv(P − pv))2 x= 343 +1 represents synonymous word-pairs and class −1 represents non-synonymous word-pairs. Finally, SVM outputs are converted to posterior probabilities (Platt, 2000). We consider the posterior probability of a given pair of words belonging to class +1 as the semantic similarity between the two words. 4 Experiments To evaluate the performance of the proposed semantic similarity measure, we conduct two sets of experiments. Firstly, we compare the similarity scores produced by the proposed measure against the Miller-Charles’ benchmark dataset.</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. Vapnik. 1998. Statistical Learning Theory. Wiley, Chichester, GB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McLean Y Li</author>
<author>Zuhair A Bandar</author>
</authors>
<title>An approch for measuring semantic similarity between words using multiple information sources.</title>
<date>2003</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<marker>Li, Bandar, 2003</marker>
<rawString>D. McLean Y. Li, Zuhair A. Bandar. 2003. An approch for measuring semantic similarity between words using multiple information sources. IEEE Transactions on Knowledge and Data Engineering, 15(4):871–882.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>