<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000140">
<title confidence="0.98414">
Linking Entities to a Knowledge Base with Query Expansion
</title>
<author confidence="0.990589">
Swapna Gottipati
</author>
<affiliation confidence="0.8963265">
School of Information Systems
Singapore Management University
</affiliation>
<address confidence="0.816581">
Singapore
</address>
<email confidence="0.997194">
swapnag.2010@smu.edu.sg
</email>
<author confidence="0.995498">
Jing Jiang
</author>
<affiliation confidence="0.8962885">
School of Information Systems
Singapore Management University
</affiliation>
<address confidence="0.816456">
Singapore
</address>
<email confidence="0.997347">
jingjiang@smu.edu.sg
</email>
<sectionHeader confidence="0.995809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995815">
In this paper we present a novel approach
to entity linking based on a statistical lan-
guage model-based information retrieval with
query expansion. We use both local con-
texts and global world knowledge to expand
query language models. We place a strong
emphasis on named entities in the local con-
texts and explore a positional language model
to weigh them differently based on their dis-
tances to the query. Our experiments on
the TAC-KBP 2010 data show that incor-
porating such contextual information indeed
aids in disambiguating the named entities and
consistently improves the entity linking per-
formance. Compared with the official re-
sults from KBP 2010 participants, our system
shows competitive performance.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955041666667">
When people read news articles, Web pages and
other documents online, they may encounter named
entities which they are not familiar with and there-
fore would like to look them up in an encyclope-
dia. It would be very useful if these entities could be
automatically linked to their corresponding encyclo-
pedic entries. This task of linking mentions of enti-
ties within specific contexts to their corresponding
entries in an existing knowledge base is called en-
tity linking and has been proposed and studied in the
Knowledge Base Population (KBP) track of the Text
Analysis Conference (TAC) (McNamee and Dang,
2009). Besides improving an online surfer’s brows-
ing experience, entity linking also has potential us-
age in many other applications such as normalizing
entity mentions for information extraction.
The major challenge of entity linking is to resolve
name ambiguities. There are generally two types of
ambiguities: (1) Polysemy: This type of ambigu-
ities refers to the case when more than one entity
shares the same name. E.g. George Bush may re-
fer to the 41st President of the U.S., the 43rd Presi-
dent of the U.S., or any other individual who has the
same name. Clearly polysemous names cause diffi-
culties for entity linking. (2) Synonymy: This type
of ambiguities refers to the case when more than
one name variation refers to the same entity. E.g.
Metro-Goldwyn-Mayer Inc. is often abbreviated as
MGM. Synonymy affects entity linking when the en-
tity mention in the document uses a name variation
not covered in the entity’s knowledge base entry.
Intuitively, to disambiguate a polysemous entity
name, we should make use of the context in which
the name occurs, and to address synonymy, exter-
nal world knowledge is usually needed to expand
acronyms or find other name variations. Indeed
both strategies have been explored in existing litera-
ture (Zhang et al., 2010; Dredze et al., 2010; Zheng
et al., 2010). However, most existing work uses
supervised learning approaches that require careful
feature engineering and a large amount of training
data. In this paper, we take a simpler unsupervised
approach using statistical language model-based in-
formation retrieval. We use the KL-divergence re-
trieval model (Zhai and Lafferty, 2001) and ex-
pand the query language models by considering both
the local contexts within the query documents and
global world knowledge obtained from the Web.
</bodyText>
<page confidence="0.977281">
804
</page>
<note confidence="0.9730195">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 804–813,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.998513619047619">
Symbol Description
Q Query
DQ Query document
NQ Query name string
E KB entity node
NE KB entity name string
DE KB entity disambiguation text
SQ Set of alternate query name strings
Nl,i Local alternative name strings
Ng Q Global alternative name strings
Q Candidate KB entries for Q
EQ Query Language Model
θQ KB entry language model using local context from DQ
θL KB entry language model using global knowledge
Q KB entry language model using local context and global knowledge
θG KB entry language model with named entities only
Q KB entry language model with named entities and disambiguation text
θL+G
Q
θNE
θNE+DE
</table>
<tableCaption confidence="0.999837">
Table 1: Notation
</tableCaption>
<bodyText confidence="0.99970525">
We evaluate our retrieval method with query ex-
pansion on the 2010 TAC-KBP data set. We find that
our expanded query language models can indeed
improve the performance significantly, demonstrat-
ing the effectiveness of our principled and yet sim-
ple techniques. Comparison with the official results
from KBP participants also shows that our system is
competitive. In particular, when no disambiguation
text from the knowledge base is used, our system can
achieve an overall 85.2% accuracy and 9.3% relative
improvement over the best performance reported in
KBP 2010.
</bodyText>
<sectionHeader confidence="0.724652" genericHeader="method">
2 Task Definition and System Overview
</sectionHeader>
<bodyText confidence="0.999956133333333">
Following TAC-KBP (Ji et al., 2010), we define the
entity linking task as follows. First, we assume
the existence of a Knowledge Base (KB) of enti-
ties. Each KB entry E represents a unique entity
and has three fields: (1) a name string NE, which
can be regarded as the official name of the entity,
(2) an entity type TE, which is one of {PER, ORG,
GPE, UNKNOWN}, and (3) some disambiguation
text DE. Given a query Q which consists of a query
name string NQ and a query document DQ where
the name occurs, the task is to return a single KB
entry to which the query name string refers or Nil if
there is no such KB entry.
It is fairly natural to address entity linking by
ranking the KB entries given a query. In this section
we present an overview of our system, which con-
sists of two major stages: a candidate selection stage
to identify a set of candidate KB entries through
name matching, and a ranking stage to link the query
entity to the most likely KB entry. In both stages,
we consider the query’s local context in the query
document and world knowledge obtained from the
Web. It is important to note that the selection stage
is based on string matching where the order of the
word matters. It is different from the ranking stage
where a probabilistic retrieval model based on bag-
of-word representation is used. Our preliminary ex-
periments demonstrate that without the first candi-
date selection stage the linking process results in low
performance.
</bodyText>
<subsectionHeader confidence="0.998694">
2.1 Selecting Candidate KB Entries
</subsectionHeader>
<bodyText confidence="0.99994975">
The first stage of our system aims to filter out irrel-
evant KB entries and select only a set of candidates
that are potentially the correct match to the query.
Intuitively, we determine whether two entities are
the same by comparing their name strings. We there-
fore need to compare the query name string NQ with
the name string NE of each KB entry. However,
because of the name ambiguity problem, we cannot
expect the correct KB entry to always have exactly
the same name string as the query. To address this
problem, we use a set of alternative name strings ex-
panded from NQ and select KB entries whose name
</bodyText>
<page confidence="0.997345">
805
</page>
<bodyText confidence="0.999483541666666">
strings match at least one of them. These alterna-
tive name strings come from two sources: the query
document DQ and the Web.
First, we observe that some useful alternative
name strings come from the query document. For
example, a PER query name string may contain only
a person’s last name but the query document con-
tains the person’s full name, which is clearly a less
ambiguous name string to use. Similarly, a GPE
query name string may contain only the name of a
city or town but the query document contains the
state or province, which also helps disambiguate the
query entity. Based on this observation, we do the
following. Given query Q, let SQ denote the set of
alternative query name strings. Initially SQ contains
only NQ. We then use an off-the-shelf NER tagger
to identify named entities from the query document
DQ. For PER and ORG queries, we select named
entities in DQ that contain NQ as a substring. For
GPE queries, we select named entities that are of the
type GPE, and we then combine each of them with
NQ. We denote these alternative name strings as
{Nl ,iQ}i Q, where l indicates that these name strings
come locally from DQ and KQ is the total number of
such name strings. {Nl,i
Q } are added to SQ. Figure
1 and Figure 2 show two example queries together
with their SQ.
Sometimes alternative name strings have to come
from external knowledge. For example, one of the
queries we have contains the name string “AMPAS,”
and the query document also uses only this acronym
to refer to this entity. But the full name of the entity,
“Academy of Motion Pictures Arts and Sciences,” is
needed in order to locate the correct KB entry. To
tackle this problem, we leverage Wikipedia to find
the most likely official name. Given query name
string NQ, we check whether the following link ex-
ists: http://en.wikipedia.org/NQ. If NQ
is an abbreviation, Wikipedia will redirect the link
to the Wikipedia page of the corresponding entity
with its official name. So if the link exists, we use
the title of the Wikipedia page as another alternative
name string for NQ. We refer to this name string as
NgQ to indicate that it is a global name variant. NgQ is
also added to SQ. Figure 2 shows such an example.
For each name string N in SQ, we find KB entries
whose name strings match N. We take the union of
</bodyText>
<figureCaption confidence="0.997032">
Figure 1: An example GPE query from TAC 2010.
</figureCaption>
<bodyText confidence="0.417542">
Query name string (NQ): Coppola
</bodyText>
<construct confidence="0.947943">
Query document (DQ): I had no idea of all these
semi-obscure connections, felicia! Alex Greenwald
and Claire Oswalt aren’t names I’m at all familiar
with, but Jason Schwartzman I’ve heard of. Isn’t he
Sophia Coppola’s cousin? I think I once saw a pic-
ture of him sometime ago
</construct>
<figure confidence="0.919647">
Alternative Query Strings (SQ):
from local context: Coppola, Sophia Coppola, Sofia
Coppola
from world knowledge(Wikipedia): Sofia Coppola
</figure>
<figureCaption confidence="0.999008">
Figure 2: An example PER query from TAC 2010.
</figureCaption>
<bodyText confidence="0.9956815">
these sets of KB entries and refer to it as EQ. These
are the candidate KB entries for query Q.
</bodyText>
<subsectionHeader confidence="0.999522">
2.2 Ranking KB Entries
</subsectionHeader>
<bodyText confidence="0.999997">
Given the candidate KB entries EQ, we need to
decide which one of them is the correct match.
We adopt the widely-used KL-divergence retrieval
model, a statistical language model-based retrieval
method proposed by Lafferty and Zhai (2001).
Given a KB entry E and query Q, we score E based
on the KL-divergence defined below:
</bodyText>
<equation confidence="0.998165">
p(w|BQ) log p(w |BQ)
p( |E)
(1)
</equation>
<bodyText confidence="0.9973652">
Here BQ and BE are the query language model and
the KB entry language model, respectively. A lan-
guage model here is a multinomial distribution over
words (i.e. a unigram language model). V is the
vocabulary and w is a single word.
To estimate BE, we follow the standard maxi-
mum likelihood estimation with Dirichlet smooth-
Query name string (NQ): Mobile
Query document (DQ): The site is near Mount Ver-
non in the Calvert community on the Tombigbee River,
some 25 miles (40 kilometers) north of Mobile. It’s on
a river route to the Gulf of Mexico and near Mobile’s
rails and interstates. Along with tax breaks and $400
million (euro297 million) in financial incentives, Al-
abama offered a site with a route to a Brazil plant that
will provide slabs for processing in Mobile.
Alternative Query Strings (SQ):
from local context: Mobile, Mobile Mount Vernon,
Mobile Calvert, Mobile River, Mobile Mexico, Mobile
Alabama, Mobile Brazil
</bodyText>
<equation confidence="0.961047">
∑
s(E,Q) = −Div(BQIIBE) = −
WEV
</equation>
<page confidence="0.961978">
806
</page>
<bodyText confidence="0.870833">
ing (Zhai and Lafferty, 2004):
</bodyText>
<equation confidence="0.9997895">
c(w, E) + µp(w|θC)
p(w|θ�) =
</equation>
<bodyText confidence="0.9984928">
where c(w, E) is the count of w in E, JEJ is the
number of words in E, BC is a background lan-
guage model estimated from the whole KB, and p
is the Dirichlet prior. Recall that E contains NE, TE
and DE. We consider using either NE only or both
NE and DE to obtain c(w, E) and JEJ. We refer
to the former estimated BE as BNE and the latter as
BNE+DE.
To estimate BQ, typically we can use the empirical
query word distribution:
</bodyText>
<equation confidence="0.995976">
p(w |θQ) = c( |,NNQ) , (3)
Q|
</equation>
<bodyText confidence="0.999992666666667">
where c(w, NQ) is the count of w in NQ and JNQJ
is the length of NQ. We call this model the original
query language model.
After ranking the candidate KB entries in £Q us-
ing Equation (1), we perform entity linking as fol-
lows. First, using an NER tagger, we determine the
entity type of the query name string NQ. Let TQ de-
note this entity type. We then pick the top-ranked
KB entry whose score is higher than a threshold T
and whose TE is the same as TQ. The system links
the query entity to this KB entry. If no such entry
exists, the system returns Nil.
</bodyText>
<sectionHeader confidence="0.987561" genericHeader="method">
3 Query Expansion
</sectionHeader>
<bodyText confidence="0.999958833333333">
We have shown in Section 2.1 that using the origi-
nal query name string NQ itself may not be enough
to obtain the correct KB entry, and additional words
from both the query document and external knowl-
edge can be useful. However, in the KB entry se-
lection stage, these additional words are only used
to enlarge the set of candidate KB entries; they have
not been used to rank KB entries. In this section, we
discuss how to expand the query language model BQ
with these additional words in a principled way in
order to rank KB entries based on how likely they
match the query entity.
</bodyText>
<subsectionHeader confidence="0.99991">
3.1 Using Local Contexts
</subsectionHeader>
<bodyText confidence="0.999974608695652">
Let us look at the example from Figure 2 again.
During the KB entry ranking stage, if we use BQ
estimated from NQ, which contains only the word
“Coppola,” the retrieval function is unlikely to rank
the correct KB entry on the top. But if we include
the contextual word “Sophia” from the query doc-
ument when estimating the query language model,
KL-divergence retrieval model is likely to rank the
correct KB entry on the top. This idea of using
contextual words to expand the query is very sim-
ilar to (pseudo) relevance feedback in information
retrieval. We can treat the query document DQ as
our only feedback document.
Many different (pseudo) relevance feedback
methods have been proposed. Here we apply the
relevance model (Lavrenko and Croft, 2001), which
has been shown to be effective and robust in a re-
cent comparative study (Lv and Zhai, 2009). We
first briefly review the relevance model. Given a set
of (pseudo) relevant documents Dr, where for each
D E Dr there is a document language model BD,
we can estimate a feedback language model BfbQ as
follows:
</bodyText>
<equation confidence="0.9770905">
p(w|θfbQ) ∝ ∑ p(w|θD)p(θD)p(Q|θD). (4)
DEV,
</equation>
<bodyText confidence="0.999844375">
For our problem, since we have only a single feed-
back document DQ, the equation above can be sim-
plified. In fact, in this case the feedback language
model is the same as the document language model
of the only feedback document, i.e. BDQ.
We then linearly interpolate the feedback lan-
guage model with the original query language model
to form an expanded query language model:
</bodyText>
<equation confidence="0.999693">
p(w|θQ) = αp(w|θQ) + (1 − α)p(w|θDe), (5)
</equation>
<bodyText confidence="0.9997332">
where α is a parameter between 0 and 1, to control
the amount of feedback. The larger α is, the less we
rely on the local context. L indicates that the query
expansion comes from local context. This BQ can
then replace BQ in Equation (1) to rank KB entries.
</bodyText>
<subsectionHeader confidence="0.984397">
Special Treatment of Named Entities
</subsectionHeader>
<bodyText confidence="0.999935">
Usually the document language model BDQ is es-
timated using the entire text from DQ. For entity
linking, we suspect that named entities surrounding
the query name string in DQ are particularly useful
for disambiguation and thus should be emphasized
over other words. This can be done by weighting
</bodyText>
<equation confidence="0.9974815">
|E |+ µ
, (2)
</equation>
<page confidence="0.97594">
807
</page>
<bodyText confidence="0.997194333333333">
NE and non-NE words differently. In the extreme
case, we can use only NEs to estimate the document
language model BDQ as follows:
</bodyText>
<equation confidence="0.988">
1
p(w|θDQ) =
KQ
</equation>
<bodyText confidence="0.933395">
where {Nl,i
Q } are defined in Section 2.
</bodyText>
<subsectionHeader confidence="0.972161">
Positional Model
</subsectionHeader>
<bodyText confidence="0.998220777777778">
Another observation is that words closer to the
query name string in the query document are likely
to be more important than words farther away. Intu-
itively, we can use the distance between a word and
the query name string to help weigh the word. Here
we apply a recently proposed positional pseudo rel-
evance feedback method (Lv and Zhai, 2010). The
document language model BDQ now has the follow-
ing form:
</bodyText>
<equation confidence="0.998515">
c(|NNi i) , (7)
Q
</equation>
<bodyText confidence="0.993796">
where pi and q are the absolute positions of Nl,i
</bodyText>
<equation confidence="0.6392">
Q
</equation>
<bodyText confidence="0.8841745">
and NQ in DQ. The function f is Gaussian function
defined as follows:
</bodyText>
<equation confidence="0.77855225">
)
1 (−(p − q)2
√exp .
2πσ2 2σ2
</equation>
<bodyText confidence="0.99788">
where variance a controls the spread of the curve.
</bodyText>
<subsectionHeader confidence="0.999877">
3.2 Using Global World Knowledge
</subsectionHeader>
<bodyText confidence="0.9998318">
Similar to the way we incorporate words from DQ
into the query language model, we can also con-
struct a feedback language model using the most
likely official name of the query entity obtained from
Wikipedia. Specifically, we define
</bodyText>
<equation confidence="0.995447333333333">
c(w, NQ)
p(w|θNQ) = (9)
|N� Q |.
</equation>
<bodyText confidence="0.999761666666667">
We can then linearly interpolate BNQ with the orig-
inal query language model BQ to form an expanded
query language model BGQ:
</bodyText>
<equation confidence="0.999173">
p(w|θGQ) = αp(w|θQ) + (1 − α)p(w|θNQ). (10)
</equation>
<bodyText confidence="0.9483555">
Here G indicates that the query expansion comes
from global world knowledge.
</bodyText>
<table confidence="0.99961225">
Entity Type %Nil %non-Nil
GPE 32.8% 67.2 %
ORG 59.5% 40.5 %
PER 71.7% 28.3 %
</table>
<tableCaption confidence="0.997593">
Table 2: Percentages of Nil and non-Nil queries.
</tableCaption>
<subsectionHeader confidence="0.9727045">
3.3 Combining Local Context and World
Knowledge
</subsectionHeader>
<bodyText confidence="0.999672">
We can further combine the two kinds of additional
words into the query language model as follows:
</bodyText>
<equation confidence="0.998506333333333">
p(w |θQ+G) = αp(w |θQ) + (1 − α) (βp(w |θDQ)
)
+(1 − β)p(w|θN� Q) . (11)
</equation>
<bodyText confidence="0.999798333333333">
Note that here we have two parameters a and 0 to
control the amount of contributions from the local
context and from global world knowledge.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994245">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.985235166666667">
Data Set: We evaluate our system on the TAC-KBP
2010 data set (Ji et al., 2010). The knowledge base
was constructed from Wikipedia with 818,741 en-
tries. The data set contains 2250 queries and query
documents come from news wire and Web pages.
Around 45% of the queries have non-Nil entries in
the KB. Some statistics of the queries are shown in
Table 2.
Tools: In our experiments, to extract named entities
within DQ and to determine TQ, we use the Stanford
NER tagger1. An example output of the NER tagger
is shown below:
</bodyText>
<equation confidence="0.8557685">
&lt;PERSON&gt;Hugh Jackman&lt;PERSON&gt; is
Jacked!!
</equation>
<bodyText confidence="0.9249055">
This piece of text comes from a query document
where the query name string is “Jackman.” We can
see that the NER tagger can help locate the full name
of the person.
We use the Lemur/Indri2 search engine for re-
trieval. It implements the KL-divergence retrieval
model as well as many other useful functionalities.
Evaluation Metric: We adopt the Micro-averaged
accuracy metric, which is the mean accuracy over
all queries. It was used in TAC-KBP 2010 (Ji et
lhttp://nlp.stanford.edu/software/CRF-NER.shtml
zhttp://www.lemurproject.org/indri.php
</bodyText>
<equation confidence="0.669112333333333">
, (6)
|N��i
Q |
∑KQ
i=1
c(w,N��i
Q )
1 ∑KQ f(pi, q) ·
p(w|θDQ) = i=1
KQ
f(p, q) =
(8)
</equation>
<page confidence="0.967316">
808
</page>
<bodyText confidence="0.998180727272727">
al., 2010) as the official metric to evaluate the per-
formance of entity linking. This metric is simply
defined as the percentage of queries that have been
correctly linked.
Methods to Compare: Recall that our system con-
sists of a KB entry selection stage and a KB entry
ranking stage. At the selection stage, a set SQ of
alternative name strings are used to select candidate
KB entries. We first define a few settings where dif-
ferent alternative name string sets are used to select
candidate KB entries:
</bodyText>
<listItem confidence="0.9976424">
• Q represents the baseline setting which uses
only the original query name string NQ to se-
lect candidate KB entries.
• Q+L represents the setting where alternative
name strings obtained from the query docu-
ment DQ are combined with NQ to select can-
didate KB entries.
• Q+G represents the setting where the alterna-
tive name string obtained from Wikipedia is
combined with NQ to select candidate KB en-
tries.
• Q+L+G represents the setting as we described
in Section 2.1, that is, alternative name strings
from both DQ and Wikipedia are used together
with NQ to select candidate KB entries.
</listItem>
<bodyText confidence="0.999943266666667">
After selecting candidate KB entries, in the KB
entry ranking stage, we have four options for the
query language model and two options for the KB
entry language model. For the query language
model, we have (1) BQ, the original query language
model, (2) BQ, an expanded query language model
using local context from DQ, (3) BGQ, an expanded
query language model using global world knowl-
edge, and (4) B�+G
Q , an expanded query language
model using both local context and global world
knowledge. For the KB entry language model, we
can choose whether or not to use the KB disam-
biguation text DE and obtain BN_, and BN_,+D_,, re-
spectively.
</bodyText>
<subsectionHeader confidence="0.63351">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999982333333333">
First, we compare the performance of KB entry se-
lection stage for all four settings on non-Nil queries.
The performance measure recall is defined as
</bodyText>
<equation confidence="0.98234775">
recall
11, if E that ref ers to Q, exists in EQ
= SI
0, otherwise
</equation>
<bodyText confidence="0.9944295">
The recall statistics in Table 3 shows that, Q+L+G
has the highest recall of the KB candidate entries.
</bodyText>
<table confidence="0.9979378">
Method Recall(%)
Q 67.1
Q+L 89.7
Q+G 94.9
Q+L+G 98.2
</table>
<tableCaption confidence="0.921567">
Table 3: Comparing the effect of candidate entry selec-
tion using different methods - KB entry selection stage
recall.
</tableCaption>
<bodyText confidence="0.99905848">
Before examining the effect of query expansion
in ranking, we now compare the effect of using dif-
ferent sets of alternative query name strings in the
candidate KB entry selection stage. For this set of
experiments, we fix the query language model to BQ
and the KB entry language model to BN_, in the rank-
ing stage.
Table 4 shows the performance of all the settings
in terms of micro-averaged accuracy. The results
shown in Tables 4, 5 and 6 are based on the opti-
mum parameter settings. We can see that in terms
of the overall performance, both Q+L and Q+G give
better performance than Q with a 7.7% and a 9.9%
relative improvement, respectively. Q+L+G gives
the best performance with a 12.8% relative improve-
ment over Q. If we further zoom into the results, we
see that for ORG and PER queries, when no correct
KB entry exists (i.e. the Nil case), the performance
of Q, Q+L, Q+G and Q+L+G is very close, indicat-
ing that the additional alternative query name strings
do not help. It shows that the alternative query name
strings are most useful for queries that do have their
correct entries in the KB.
We now further analyze the impact of the ex-
panded query language models BQ, BG Q and B�+G
</bodyText>
<equation confidence="0.602442">
Q .
</equation>
<bodyText confidence="0.9999342">
We first analyze the results without using the KB
disambiguation text, i.e. using BN_,. Table 5 shows
the comparison between BQ and other expanded
query language models in terms of micro-averaged
accuracy. The results reveal that the expanded query
language models can indeed improve the overall per-
formance (the both Nil and non-Nil case) under all
settings. This shows the effectiveness of using the
principled query expansion technique coupled with
KL-divergence retrieval model to rank KB entries.
</bodyText>
<page confidence="0.997133">
809
</page>
<table confidence="0.999227166666667">
Method All Nil Non-Nil
ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789
Q+L 0.7449 0.7156 0.6533 0.8655 0.9472 0.9888 0.9944 0.6024 0.1612 0.5399
Q+G 0.7604 0.7009 0.6893 0.8908 0.9431 0.9888 0.9944 0.5825 0.2500 0.6291
Q+L+G 0.7800 0.7583 0.6893 0.8921 0.9431 0.9888 0.9944 0.6680 0.2500 0.6338
</table>
<tableCaption confidence="0.99146">
Table 4: Comparing the performance of using different sets of query name strings for candidate KB entry selection.
OQ and ONE are used in KB entry ranking.
</tableCaption>
<table confidence="0.999878181818182">
Method QueryModel All Nil Non-Nil
ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q+L �Q 0.7449 0.7156 0.6533 0.8655 0.9472 0.9888 0.9944 0.6024 0.1612 0.5399
�L 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493
Q
Q+G �Q 0.7604 0.7009 0.6893 0.8908 0.9431 0.9888 0.9944 0.5825 0.2500 0.6291
�G 0.8160 0.7423 0.7867 0.9188 0.9106 0.9372 0.9796 0.6600 0.5658 0.7653
Q
Q+L+G �Q 0.7800 0.7583 0.6893 0.8921 0.9431 0.9888 0.9944 0.6680 0.2500 0.6338
B+G 0.8516 0.8278 0.7867 0.9401 0.8821 0.9372 0.9814 0.8012 0.5658 0.8357
Q
</table>
<tableCaption confidence="0.997258">
Table 5: Comparison between the performance of OQ and expanded query language models in terms of micro average
accuracy. ONE was used in ranking.
</tableCaption>
<bodyText confidence="0.999690073170732">
On the other hand, again we observe that the ef-
fects on the Nil and the non-Nil queries are differ-
ent. While in Table 4 the alternative name strings
do not affect the performance much for Nil queries,
now the expanded query language models actually
hurt the performance for Nil queries. It is not sur-
prising to see this result. When we expand the query
language model, we can possibly introduce noise,
especially when we use the external knowledge ob-
tained from Wikipedia, which largely depends on
what Wikipedia considers to be the most popular
official name of a query name string. With noisy
terms in the expanded query language model we in-
crease the chance to link the query to a KB entry
which is not the correct match. The challenge is that
we do not know when additional terms in the ex-
panded query language model are noise and when
they are not, because for non-Nil queries we do ob-
serve a substantial amount of improvement brought
by query expansion, especially with external world
knowledge. We will further investigate this research
question in the future.
We now further study the impact of using the KB
disambiguation text associated with each entry to es-
timate the KB entry language model used in the KL-
divergence ranking function. The results are shown
in Table 6 for all the methods on BNE vs. BNE+DE
using the expanded query language models. We can
see that for all methods the impact of using the KB
disambiguation text is very minimal and is observed
only for GPE and ORG queries. Table 7 shows an
example of the KL-divergence scores for a query,
Mobile whose context is previously shown in the
Figure 1. Without the KB disambiguation text both
the KB entry Mobile Alabama and the entry Mobile
River are given the same score, resulting in inaccu-
rate linking in the BNE case. But with BNE+DE, Mo-
bile Alabama was scored higher, resulting in an ac-
curate linking. However, we observe that such cases
are very rare in the TAC 2010 query list and thus the
overall improvement observed is minimal.
</bodyText>
<table confidence="0.984459">
KB Entry KB Name w/o text w/ text
E0583976 Mobile Alabama -6.28514 -6.3839
E0183287 Mobile River -6.28514 -6.69372
</table>
<tableCaption confidence="0.987897">
Table 7: The KL-divergence scores of KB entities for the
query Mobile.
</tableCaption>
<bodyText confidence="0.999820153846154">
Finally, we compare our performance with the
highest scores from TAC-KBP 2010 as shown in the
Table 8. It is important to note that the highest TAC
results shown in the table under each setting are not
necessarily obtained by the same team. We can see
that our overall performance when KB text is used is
competitive compared with the highest TAC score,
and is substantially higher than the TAC score when
KB text is not used. Lehmann et al. (2010) achieved
highest TAC scores. They used a variety of evidence
from Wikipedia like disambiguation pages, anchors,
expanded acronyms and redirects to build a rich fea-
ture set. But as we discussed, building a rich fea-
</bodyText>
<page confidence="0.990733">
810
</page>
<table confidence="0.9969033">
Method KB Text All Nil Non-Nil
ALL GPE ORG PER GPE ORG PER GPE ORG PER
Q ONE 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789
ONE+DE 0.6888 0.5607 0.6533 0.8495 0.8618 0.9888 0.9963 0.4135 0.1612 0.4789
Q+L ONE 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493
ONE+DE 0.7707 0.7904 0.6533 0.8682 0.9390 0.9888 0.9944 0.7177 0.1612 0.5493
Q+G ONE 0.8160 0.7423 0.7867 0.9188 0.9106 0.9372 0.9796 0.6600 0.5658 0.7653
ONE+DE 0.8222 0.7450 0.7827 0.9387 0.8902 0.9372 0.9814 0.6740 0.5559 0.8310
Q+L+G ONE 0.8516 0.8278 0.7867 0.9401 0.8821 0.9372 0.9814 0.8012 0.5658 0.8357
ONE+DE 0.8524 0.8291 0.7880 0.9401 0.8740 0.9372 0.9814 0.8072 0.5691 0.8357
</table>
<tableCaption confidence="0.99463">
Table 6: Comparing the performance using KB text and without using KB text for all methods using expanded query
models in terms of micro average accuracy on 2250 queries. θNE+DE represents method using KB text and θNE
represents methods without using KB text.
</tableCaption>
<bodyText confidence="0.99988247368421">
ture set is an expensive task. Their overall accu-
racy is 1.5% higher than our model. Table 8 shows
that the performance of ORG entities is lower when
compared with the TAC results when we used KB
text. In our analysis, we observed that, even though
some entities like AMPAS are linked correctly, the
entities like CCC (Consolidated Contractors Com-
pany) failed due to ambiguity in the title. Here, we
may benefit by leveraging more global knowledge,
i.e, we should expand the NQ with Wikipedia global
context entities together with the title to fully benefit
from global knowledge. In particular, when KB text
is not used, our system outperforms the highest TAC
results for all three types of queries.
From the analysis by Ji et al. (2010), overall the
participating teams generally performed the best on
PER queries and the worst on GPE queries. With our
system, we can achieve good performance on GPE
queries.
</bodyText>
<table confidence="0.999714">
KB Text Usage Type Our System TAC Highest
θNE+DE All 0.8524 0.8680
GPE 0.8291 0.7957
ORG 0.7880 0.8520
PER 0.9401 0.9601
θNE All 0.8516 0.7791
GPE 0.8278 0.7076
ORG 0.7867 0.7333
PER 0.9401 0.9001
</table>
<tableCaption confidence="0.873561">
Table 8: Comparison of the best configuration of our sys-
tem (Q+L+G with θ�+G
</tableCaption>
<bodyText confidence="0.9926935">
� ) with the TAC-KBP 2010 results
in terms of micro-averaged accuracy. θNE+DE represents
the method using KB disambiguation text and θNE repre-
sents the method without using KB disambiguation text.
</bodyText>
<subsectionHeader confidence="0.997567">
4.3 Parameter Sensitivity
</subsectionHeader>
<bodyText confidence="0.986901411764706">
In all our experiments, we set the Dirichlet prior µ to
2500 following previous studies. For the threshold 7-
we empirically set it to -12.0 in all the experiments
based on preliminary results. Recall that all the ex-
panded query language models also have a control
parameters α. The local context-based models OQ
and O�+G
� have an additional parameter a which
controls the proximity weighing. The O�+G
� model
has another additional parameter 0 that controls the
balance between local context and world knowledge.
In this subsection, we study the sensitivity of these
parameters. We plot the sensitivity graphs for all the
methods that involve α (0 set to 0.5) in Figure 3. As
we can see, all the curves appear to be stable and
α=0.4 appears to work well.
</bodyText>
<figure confidence="0.9460485">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
alpha
</figure>
<figureCaption confidence="0.992854">
Figure 3: Sensitivity of α in regard to micro-averaged
accuracy.
</figureCaption>
<bodyText confidence="0.953024166666667">
Similarly, we set α=0.4 and examine how 0 af-
fects micro averaged accuracy. We plot the sensi-
tivity curve for 0 for the Q+L+G setting with O�+G
�
in Figure 5. As we can see, the best performance
is achieved when 0=0.5. This implies that the local
</bodyText>
<figure confidence="0.976272615384615">
Micro Averaged Accuracy
0.95
0.85
0.75
0.9
0.8
0.7
q+L
q+G
q+L+G
811
0 0.2 0.4 0.6 0.8 1
beta
</figure>
<figureCaption confidence="0.9963505">
Figure 4: Sensitivity of a in regard to micro-averaged
accuracy.
</figureCaption>
<bodyText confidence="0.991649">
context and the global world knowledge are weighed
equally for aiding disambiguation and improving the
entity linking performance.
</bodyText>
<figure confidence="0.840013">
40 60 80 100 120
sigma
</figure>
<figureCaption confidence="0.9962975">
Figure 5: Sensitivity of u with respect to micro-averaged
accuracy.
</figureCaption>
<bodyText confidence="0.99998875">
Furthermore, we systematically test a fixed set of
o, values from 25 to 125 with an intervals of 25 and
examine how o, affects micro averaged accuracy. We
set α=0.4 and 0=0.5, which is the best parameter
setting as discussed above. We plot the sensitivity
curves for the parameter o, for methods that utilize
the local context, i.e. BQ and B�+G
� , in Figure 5. We
observe that all the curves are stable and 75 &lt;= o,
&lt;= 100 appears to work well. We set o,=100 for all
our experiments. Moreover, after 100, the graph be-
comes stable, which indicates that proximity has less
impact on the method from this point on. This im-
plies that an equal weighing scheme actually would
work the same for these experiments. Part of the
reason may be that by using only named entities in
the context rather than all words, we have effectively
picked the most useful contextual terms. Therefore,
positional feedback models do have exhibit much
benefit for our problem.
</bodyText>
<sectionHeader confidence="0.999898" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999926934782609">
Bunescu and Pasca (2006) and Cucerzan (2007) ex-
plored the entity linking task using Vector Space
Models for ranking. They took a classification ap-
proach together with the novel idea of exploiting
Wikipedia knowledge. In their pioneering work,
they used Wikipedia’s category information for en-
tity disambiguation. They show that using differ-
ent background knowledge, we can find efficient ap-
proaches for disambiguation. In their work, they
took an assumption that every entity has a KB en-
try and thus the NIL entries are not handled.
Similar to other researchers, Zhang et al. (2010)
took an approach of classification and used a two-
stage approach for entity liking. They proposed a
supervised model with SVM ranking to filter out the
candidates and deal with disambiguation effectively.
For entity diambiguation they used the contextual
comparisons between the Wikipedia article and the
KB article. However, their work ignores the possi-
bilities of acronyms in the entities. Also, the am-
biguous geo-political names are not handled in their
work.
Dredze et al. (2010) took the approach that large
number of entities will be unlinkable, as there is
a probability that the relevant KB entry is unavail-
able. Their algorithm for learning NIL has shown
very good results. But their proposal for handling
the alias name or stage name via multiple lists is not
scalable. Unlike their approach, we use the global
knowledge to handle the stage names and thus this
gives an optimized solution to handle alias names.
Similarly, for acronyms we use the global knowl-
edge that aids unabbreviating and thus entity dis-
ambiguation. Similar to other approaches, Zheng
et al. (2010) took a learning to rank approach and
compared list-wise rank model to the pair-wise rank
model. They achieved good results on the list-wise
ranking approach. They handled acronyms and dis-
ambiguity through wiki redirect pages and the an-
chor texts which is similar to others ideas.
Challenges in supervised learning includes care-
ful feature selection. The features can be selected in
ad hoc manner - similarity based or semantic based.
Also machine learning approach induces challenges
of handling heterogenous cases. Unlike their ma-
chine learning approach which requires careful fea-
</bodyText>
<figure confidence="0.9992513">
q+L+G
q+L
q+L+G
Micro Averaged Accuracy 0.86
0.85
0.84
0.83
0.82
0.81
0.8
0.79
0.78
0.77
0.76
Micro Averaged Accuracy 0.95
0.9
0.85
0.8
0.75
0.7
</figure>
<page confidence="0.990476">
812
</page>
<bodyText confidence="0.99997225">
ture engineering and heterogenous training data, our
method is simple as we use simple similarity mea-
sures. At the same time, we propose a statistical
language modeling approach to the linking prob-
lem. Many researchers have proposed efficient ideas
in their works. We integrated some of their ideas
like world knowledge with our new techniques to
achieve efficient entity linking accuracy.
</bodyText>
<sectionHeader confidence="0.999695" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999506529411765">
In this paper we proposed a novel approach to entity
linking based on statistical language model-based
information retrieval with query expansion using the
local context from the query document as well as
world knowledge from the Web. Our model is a sim-
ple unsupervised one that follows principled exist-
ing information retrieval techniques. And yet it per-
forms the entity linking task effectively compared
with the best performance achieved in the TAC-KBP
2010 evaluation.
Currently our model does not exploit world
knowledge from the Web completely. World knowl-
edge, especially obtained from Wikipedia, has
shown to be useful in previous studies. As our future
work, we plan to explore how to further incorporate
such world knowledge into our model in a principled
way.
</bodyText>
<sectionHeader confidence="0.99902" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998313537313433">
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceesings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), pages 9–16, Trento, Italy.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 708–716.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 277–285.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the TAC 2010
knowledge base population track. In Proceedings of
the Third Text Analysis Conference.
John Lafferty and ChengXiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 111–119.
Victor Lavrenko and W. Bruce Croft. 2001. Rele-
vance based language models. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 120–127.
John Lehmann, Sean Monahan, Luke Nezda, Arnold
Jung, and Ying Shi. 2010. Lcc approaches to knowl-
edge base population at tac 2010. In Proceedings TAC
2010 Workshop. TAC 2010.
Yuanhua Lv and ChengXiang Zhai. 2009. A compar-
ative study of methods for estimating query language
models with pseudo feedback. In Proceeding of the
18th ACM Conference on Information and Knowledge
Management, pages 1895–1898.
Yuanhua Lv and ChengXiang Zhai. 2010. Positional rel-
evance model for pseudo-relevance feedback. In Pro-
ceeding of the 33rd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 579–586.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Second Text Analysis Conference.
ChengXiang Zhai and John Lafferty. 2001. Model-based
feedback in the language modeling approach to infor-
mation retrieval. In Proceedings of the 10th Inter-
national Conference on Information and Knowledge
Management, pages 403–410.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Transactions on Information
Systems, 22(2):179–214, April.
Wei Zhang, Jian Su, Chew Lim Tan, and Wen Ting Wang.
2010. Entity linking leveraging automatically gener-
ated annotation. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 1290–1298.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
483–491.
</reference>
<page confidence="0.999185">
813
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.352885">
<title confidence="0.999896">Linking Entities to a Knowledge Base with Query Expansion</title>
<author confidence="0.949306">Swapna</author>
<affiliation confidence="0.9671585">School of Information Singapore Management</affiliation>
<email confidence="0.660687">swapnag.2010@smu.edu.sg</email>
<author confidence="0.68193">Jing</author>
<affiliation confidence="0.9164675">School of Information Singapore Management</affiliation>
<email confidence="0.938922">jingjiang@smu.edu.sg</email>
<abstract confidence="0.999530555555556">In this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion. We use both local contexts and global world knowledge to expand query language models. We place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query. Our experiments on the TAC-KBP 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance. Compared with the official results from KBP 2010 participants, our system shows competitive performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceesings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06),</booktitle>
<pages>9--16</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="30989" citStr="Bunescu and Pasca (2006)" startWordPosition="5365" endWordPosition="5368">bserve that all the curves are stable and 75 &lt;= o, &lt;= 100 appears to work well. We set o,=100 for all our experiments. Moreover, after 100, the graph becomes stable, which indicates that proximity has less impact on the method from this point on. This implies that an equal weighing scheme actually would work the same for these experiments. Part of the reason may be that by using only named entities in the context rather than all words, we have effectively picked the most useful contextual terms. Therefore, positional feedback models do have exhibit much benefit for our problem. 5 Related Work Bunescu and Pasca (2006) and Cucerzan (2007) explored the entity linking task using Vector Space Models for ranking. They took a classification approach together with the novel idea of exploiting Wikipedia knowledge. In their pioneering work, they used Wikipedia’s category information for entity disambiguation. They show that using different background knowledge, we can find efficient approaches for disambiguation. In their work, they took an assumption that every entity has a KB entry and thus the NIL entries are not handled. Similar to other researchers, Zhang et al. (2010) took an approach of classification and us</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceesings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06), pages 9–16, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>708--716</pages>
<contexts>
<context position="31009" citStr="Cucerzan (2007)" startWordPosition="5370" endWordPosition="5371">e stable and 75 &lt;= o, &lt;= 100 appears to work well. We set o,=100 for all our experiments. Moreover, after 100, the graph becomes stable, which indicates that proximity has less impact on the method from this point on. This implies that an equal weighing scheme actually would work the same for these experiments. Part of the reason may be that by using only named entities in the context rather than all words, we have effectively picked the most useful contextual terms. Therefore, positional feedback models do have exhibit much benefit for our problem. 5 Related Work Bunescu and Pasca (2006) and Cucerzan (2007) explored the entity linking task using Vector Space Models for ranking. They took a classification approach together with the novel idea of exploiting Wikipedia knowledge. In their pioneering work, they used Wikipedia’s category information for entity disambiguation. They show that using different background knowledge, we can find efficient approaches for disambiguation. In their work, they took an assumption that every entity has a KB entry and thus the NIL entries are not handled. Similar to other researchers, Zhang et al. (2010) took an approach of classification and used a twostage approa</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Paul McNamee</author>
<author>Delip Rao</author>
<author>Adam Gerber</author>
<author>Tim Finin</author>
</authors>
<title>Entity disambiguation for knowledge base population.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>277--285</pages>
<contexts>
<context position="2898" citStr="Dredze et al., 2010" startWordPosition="456" endWordPosition="459">fers to the case when more than one name variation refers to the same entity. E.g. Metro-Goldwyn-Mayer Inc. is often abbreviated as MGM. Synonymy affects entity linking when the entity mention in the document uses a name variation not covered in the entity’s knowledge base entry. Intuitively, to disambiguate a polysemous entity name, we should make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations. Indeed both strategies have been explored in existing literature (Zhang et al., 2010; Dredze et al., 2010; Zheng et al., 2010). However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data. In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval. We use the KL-divergence retrieval model (Zhai and Lafferty, 2001) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web. 804 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages </context>
<context position="32030" citStr="Dredze et al. (2010)" startWordPosition="5531" endWordPosition="5534">n assumption that every entity has a KB entry and thus the NIL entries are not handled. Similar to other researchers, Zhang et al. (2010) took an approach of classification and used a twostage approach for entity liking. They proposed a supervised model with SVM ranking to filter out the candidates and deal with disambiguation effectively. For entity diambiguation they used the contextual comparisons between the Wikipedia article and the KB article. However, their work ignores the possibilities of acronyms in the entities. Also, the ambiguous geo-political names are not handled in their work. Dredze et al. (2010) took the approach that large number of entities will be unlinkable, as there is a probability that the relevant KB entry is unavailable. Their algorithm for learning NIL has shown very good results. But their proposal for handling the alias name or stage name via multiple lists is not scalable. Unlike their approach, we use the global knowledge to handle the stage names and thus this gives an optimized solution to handle alias names. Similarly, for acronyms we use the global knowledge that aids unabbreviating and thus entity disambiguation. Similar to other approaches, Zheng et al. (2010) too</context>
</contexts>
<marker>Dredze, McNamee, Rao, Gerber, Finin, 2010</marker>
<rawString>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. 2010. Entity disambiguation for knowledge base population. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 277–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa Trang Dang</author>
<author>Kira Griffitt</author>
<author>Joe Ellis</author>
</authors>
<title>knowledge base population track.</title>
<date>2010</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Third Text Analysis Conference.</booktitle>
<contexts>
<context position="4872" citStr="Ji et al., 2010" startWordPosition="767" endWordPosition="770">eval method with query expansion on the 2010 TAC-KBP data set. We find that our expanded query language models can indeed improve the performance significantly, demonstrating the effectiveness of our principled and yet simple techniques. Comparison with the official results from KBP participants also shows that our system is competitive. In particular, when no disambiguation text from the knowledge base is used, our system can achieve an overall 85.2% accuracy and 9.3% relative improvement over the best performance reported in KBP 2010. 2 Task Definition and System Overview Following TAC-KBP (Ji et al., 2010), we define the entity linking task as follows. First, we assume the existence of a Knowledge Base (KB) of entities. Each KB entry E represents a unique entity and has three fields: (1) a name string NE, which can be regarded as the official name of the entity, (2) an entity type TE, which is one of {PER, ORG, GPE, UNKNOWN}, and (3) some disambiguation text DE. Given a query Q which consists of a query name string NQ and a query document DQ where the name occurs, the task is to return a single KB entry to which the query name string refers or Nil if there is no such KB entry. It is fairly natu</context>
<context position="17030" citStr="Ji et al., 2010" startWordPosition="2963" endWordPosition="2966">d knowledge. Entity Type %Nil %non-Nil GPE 32.8% 67.2 % ORG 59.5% 40.5 % PER 71.7% 28.3 % Table 2: Percentages of Nil and non-Nil queries. 3.3 Combining Local Context and World Knowledge We can further combine the two kinds of additional words into the query language model as follows: p(w |θQ+G) = αp(w |θQ) + (1 − α) (βp(w |θDQ) ) +(1 − β)p(w|θN� Q) . (11) Note that here we have two parameters a and 0 to control the amount of contributions from the local context and from global world knowledge. 4 Experiments 4.1 Experimental Setup Data Set: We evaluate our system on the TAC-KBP 2010 data set (Ji et al., 2010). The knowledge base was constructed from Wikipedia with 818,741 entries. The data set contains 2250 queries and query documents come from news wire and Web pages. Around 45% of the queries have non-Nil entries in the KB. Some statistics of the queries are shown in Table 2. Tools: In our experiments, to extract named entities within DQ and to determine TQ, we use the Stanford NER tagger1. An example output of the NER tagger is shown below: &lt;PERSON&gt;Hugh Jackman&lt;PERSON&gt; is Jacked!! This piece of text comes from a query document where the query name string is “Jackman.” We can see that the NER ta</context>
<context position="27840" citStr="Ji et al. (2010)" startWordPosition="4817" endWordPosition="4820"> ORG entities is lower when compared with the TAC results when we used KB text. In our analysis, we observed that, even though some entities like AMPAS are linked correctly, the entities like CCC (Consolidated Contractors Company) failed due to ambiguity in the title. Here, we may benefit by leveraging more global knowledge, i.e, we should expand the NQ with Wikipedia global context entities together with the title to fully benefit from global knowledge. In particular, when KB text is not used, our system outperforms the highest TAC results for all three types of queries. From the analysis by Ji et al. (2010), overall the participating teams generally performed the best on PER queries and the worst on GPE queries. With our system, we can achieve good performance on GPE queries. KB Text Usage Type Our System TAC Highest θNE+DE All 0.8524 0.8680 GPE 0.8291 0.7957 ORG 0.7880 0.8520 PER 0.9401 0.9601 θNE All 0.8516 0.7791 GPE 0.8278 0.7076 ORG 0.7867 0.7333 PER 0.9401 0.9001 Table 8: Comparison of the best configuration of our system (Q+L+G with θ�+G � ) with the TAC-KBP 2010 results in terms of micro-averaged accuracy. θNE+DE represents the method using KB disambiguation text and θNE represents the m</context>
</contexts>
<marker>Ji, Grishman, Dang, Griffitt, Ellis, 2010</marker>
<rawString>Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and Joe Ellis. 2010. Overview of the TAC 2010 knowledge base population track. In Proceedings of the Third Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>111--119</pages>
<contexts>
<context position="10140" citStr="Lafferty and Zhai (2001)" startWordPosition="1706" endWordPosition="1709"> Isn’t he Sophia Coppola’s cousin? I think I once saw a picture of him sometime ago Alternative Query Strings (SQ): from local context: Coppola, Sophia Coppola, Sofia Coppola from world knowledge(Wikipedia): Sofia Coppola Figure 2: An example PER query from TAC 2010. these sets of KB entries and refer to it as EQ. These are the candidate KB entries for query Q. 2.2 Ranking KB Entries Given the candidate KB entries EQ, we need to decide which one of them is the correct match. We adopt the widely-used KL-divergence retrieval model, a statistical language model-based retrieval method proposed by Lafferty and Zhai (2001). Given a KB entry E and query Q, we score E based on the KL-divergence defined below: p(w|BQ) log p(w |BQ) p( |E) (1) Here BQ and BE are the query language model and the KB entry language model, respectively. A language model here is a multinomial distribution over words (i.e. a unigram language model). V is the vocabulary and w is a single word. To estimate BE, we follow the standard maximum likelihood estimation with Dirichlet smoothQuery name string (NQ): Mobile Query document (DQ): The site is near Mount Vernon in the Calvert community on the Tombigbee River, some 25 miles (40 kilometers)</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>John Lafferty and ChengXiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance based language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="13666" citStr="Lavrenko and Croft, 2001" startWordPosition="2353" endWordPosition="2356">ins only the word “Coppola,” the retrieval function is unlikely to rank the correct KB entry on the top. But if we include the contextual word “Sophia” from the query document when estimating the query language model, KL-divergence retrieval model is likely to rank the correct KB entry on the top. This idea of using contextual words to expand the query is very similar to (pseudo) relevance feedback in information retrieval. We can treat the query document DQ as our only feedback document. Many different (pseudo) relevance feedback methods have been proposed. Here we apply the relevance model (Lavrenko and Croft, 2001), which has been shown to be effective and robust in a recent comparative study (Lv and Zhai, 2009). We first briefly review the relevance model. Given a set of (pseudo) relevant documents Dr, where for each D E Dr there is a document language model BD, we can estimate a feedback language model BfbQ as follows: p(w|θfbQ) ∝ ∑ p(w|θD)p(θD)p(Q|θD). (4) DEV, For our problem, since we have only a single feedback document DQ, the equation above can be simplified. In fact, in this case the feedback language model is the same as the document language model of the only feedback document, i.e. BDQ. We t</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lehmann</author>
<author>Sean Monahan</author>
<author>Luke Nezda</author>
<author>Arnold Jung</author>
<author>Ying Shi</author>
</authors>
<title>Lcc approaches to knowledge base population at tac</title>
<date>2010</date>
<booktitle>In Proceedings TAC 2010 Workshop. TAC</booktitle>
<contexts>
<context position="25933" citStr="Lehmann et al. (2010)" startWordPosition="4503" endWordPosition="4506">ame w/o text w/ text E0583976 Mobile Alabama -6.28514 -6.3839 E0183287 Mobile River -6.28514 -6.69372 Table 7: The KL-divergence scores of KB entities for the query Mobile. Finally, we compare our performance with the highest scores from TAC-KBP 2010 as shown in the Table 8. It is important to note that the highest TAC results shown in the table under each setting are not necessarily obtained by the same team. We can see that our overall performance when KB text is used is competitive compared with the highest TAC score, and is substantially higher than the TAC score when KB text is not used. Lehmann et al. (2010) achieved highest TAC scores. They used a variety of evidence from Wikipedia like disambiguation pages, anchors, expanded acronyms and redirects to build a rich feature set. But as we discussed, building a rich fea810 Method KB Text All Nil Non-Nil ALL GPE ORG PER GPE ORG PER GPE ORG PER Q ONE 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789 ONE+DE 0.6888 0.5607 0.6533 0.8495 0.8618 0.9888 0.9963 0.4135 0.1612 0.4789 Q+L ONE 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493 ONE+DE 0.7707 0.7904 0.6533 0.8682 0.9390 0.9888 0.9944 0.7177 0.1612 0.5493 Q+G</context>
</contexts>
<marker>Lehmann, Monahan, Nezda, Jung, Shi, 2010</marker>
<rawString>John Lehmann, Sean Monahan, Luke Nezda, Arnold Jung, and Ying Shi. 2010. Lcc approaches to knowledge base population at tac 2010. In Proceedings TAC 2010 Workshop. TAC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanhua Lv</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A comparative study of methods for estimating query language models with pseudo feedback.</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>1895--1898</pages>
<contexts>
<context position="13765" citStr="Lv and Zhai, 2009" startWordPosition="2372" endWordPosition="2375">ut if we include the contextual word “Sophia” from the query document when estimating the query language model, KL-divergence retrieval model is likely to rank the correct KB entry on the top. This idea of using contextual words to expand the query is very similar to (pseudo) relevance feedback in information retrieval. We can treat the query document DQ as our only feedback document. Many different (pseudo) relevance feedback methods have been proposed. Here we apply the relevance model (Lavrenko and Croft, 2001), which has been shown to be effective and robust in a recent comparative study (Lv and Zhai, 2009). We first briefly review the relevance model. Given a set of (pseudo) relevant documents Dr, where for each D E Dr there is a document language model BD, we can estimate a feedback language model BfbQ as follows: p(w|θfbQ) ∝ ∑ p(w|θD)p(θD)p(Q|θD). (4) DEV, For our problem, since we have only a single feedback document DQ, the equation above can be simplified. In fact, in this case the feedback language model is the same as the document language model of the only feedback document, i.e. BDQ. We then linearly interpolate the feedback language model with the original query language model to form</context>
</contexts>
<marker>Lv, Zhai, 2009</marker>
<rawString>Yuanhua Lv and ChengXiang Zhai. 2009. A comparative study of methods for estimating query language models with pseudo feedback. In Proceeding of the 18th ACM Conference on Information and Knowledge Management, pages 1895–1898.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanhua Lv</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Positional relevance model for pseudo-relevance feedback.</title>
<date>2010</date>
<booktitle>In Proceeding of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>579--586</pages>
<contexts>
<context position="15599" citStr="Lv and Zhai, 2010" startWordPosition="2699" endWordPosition="2702">ed over other words. This can be done by weighting |E |+ µ , (2) 807 NE and non-NE words differently. In the extreme case, we can use only NEs to estimate the document language model BDQ as follows: 1 p(w|θDQ) = KQ where {Nl,i Q } are defined in Section 2. Positional Model Another observation is that words closer to the query name string in the query document are likely to be more important than words farther away. Intuitively, we can use the distance between a word and the query name string to help weigh the word. Here we apply a recently proposed positional pseudo relevance feedback method (Lv and Zhai, 2010). The document language model BDQ now has the following form: c(|NNi i) , (7) Q where pi and q are the absolute positions of Nl,i Q and NQ in DQ. The function f is Gaussian function defined as follows: ) 1 (−(p − q)2 √exp . 2πσ2 2σ2 where variance a controls the spread of the curve. 3.2 Using Global World Knowledge Similar to the way we incorporate words from DQ into the query language model, we can also construct a feedback language model using the most likely official name of the query entity obtained from Wikipedia. Specifically, we define c(w, NQ) p(w|θNQ) = (9) |N� Q |. We can then linear</context>
</contexts>
<marker>Lv, Zhai, 2010</marker>
<rawString>Yuanhua Lv and ChengXiang Zhai. 2010. Positional relevance model for pseudo-relevance feedback. In Proceeding of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 579–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Trang Dang</author>
</authors>
<title>knowledge base population track.</title>
<date>2009</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Second Text Analysis Conference.</booktitle>
<contexts>
<context position="1620" citStr="McNamee and Dang, 2009" startWordPosition="244" endWordPosition="247"> Introduction When people read news articles, Web pages and other documents online, they may encounter named entities which they are not familiar with and therefore would like to look them up in an encyclopedia. It would be very useful if these entities could be automatically linked to their corresponding encyclopedic entries. This task of linking mentions of entities within specific contexts to their corresponding entries in an existing knowledge base is called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang, 2009). Besides improving an online surfer’s browsing experience, entity linking also has potential usage in many other applications such as normalizing entity mentions for information extraction. The major challenge of entity linking is to resolve name ambiguities. There are generally two types of ambiguities: (1) Polysemy: This type of ambiguities refers to the case when more than one entity shares the same name. E.g. George Bush may refer to the 41st President of the U.S., the 43rd President of the U.S., or any other individual who has the same name. Clearly polysemous names cause difficulties fo</context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>Paul McNamee and Hoa Trang Dang. 2009. Overview of the TAC 2009 knowledge base population track. In Proceedings of the Second Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>Model-based feedback in the language modeling approach to information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th International Conference on Information and Knowledge Management,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="3246" citStr="Zhai and Lafferty, 2001" startWordPosition="507" endWordPosition="510">ld make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations. Indeed both strategies have been explored in existing literature (Zhang et al., 2010; Dredze et al., 2010; Zheng et al., 2010). However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data. In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval. We use the KL-divergence retrieval model (Zhai and Lafferty, 2001) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web. 804 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 804–813, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Symbol Description Q Query DQ Query document NQ Query name string E KB entity node NE KB entity name string DE KB entity disambiguation text SQ Set of alternate query name strings Nl,i Local alternative name strings Ng Q Global alternative name st</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>ChengXiang Zhai and John Lafferty. 2001. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of the 10th International Conference on Information and Knowledge Management, pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to information retrieval.</title>
<date>2004</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="11250" citStr="Zhai and Lafferty, 2004" startWordPosition="1899" endWordPosition="1902">(DQ): The site is near Mount Vernon in the Calvert community on the Tombigbee River, some 25 miles (40 kilometers) north of Mobile. It’s on a river route to the Gulf of Mexico and near Mobile’s rails and interstates. Along with tax breaks and $400 million (euro297 million) in financial incentives, Alabama offered a site with a route to a Brazil plant that will provide slabs for processing in Mobile. Alternative Query Strings (SQ): from local context: Mobile, Mobile Mount Vernon, Mobile Calvert, Mobile River, Mobile Mexico, Mobile Alabama, Mobile Brazil ∑ s(E,Q) = −Div(BQIIBE) = − WEV 806 ing (Zhai and Lafferty, 2004): c(w, E) + µp(w|θC) p(w|θ�) = where c(w, E) is the count of w in E, JEJ is the number of words in E, BC is a background language model estimated from the whole KB, and p is the Dirichlet prior. Recall that E contains NE, TE and DE. We consider using either NE only or both NE and DE to obtain c(w, E) and JEJ. We refer to the former estimated BE as BNE and the latter as BNE+DE. To estimate BQ, typically we can use the empirical query word distribution: p(w |θQ) = c( |,NNQ) , (3) Q| where c(w, NQ) is the count of w in NQ and JNQJ is the length of NQ. We call this model the original query languag</context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2004. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems, 22(2):179–214, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Zhang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
<author>Wen Ting Wang</author>
</authors>
<title>Entity linking leveraging automatically generated annotation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1290--1298</pages>
<contexts>
<context position="2877" citStr="Zhang et al., 2010" startWordPosition="452" endWordPosition="455">pe of ambiguities refers to the case when more than one name variation refers to the same entity. E.g. Metro-Goldwyn-Mayer Inc. is often abbreviated as MGM. Synonymy affects entity linking when the entity mention in the document uses a name variation not covered in the entity’s knowledge base entry. Intuitively, to disambiguate a polysemous entity name, we should make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations. Indeed both strategies have been explored in existing literature (Zhang et al., 2010; Dredze et al., 2010; Zheng et al., 2010). However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data. In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval. We use the KL-divergence retrieval model (Zhai and Lafferty, 2001) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web. 804 Proceedings of the 2011 Conference on Empirical Methods in Natural Langua</context>
<context position="31547" citStr="Zhang et al. (2010)" startWordPosition="5455" endWordPosition="5458">efit for our problem. 5 Related Work Bunescu and Pasca (2006) and Cucerzan (2007) explored the entity linking task using Vector Space Models for ranking. They took a classification approach together with the novel idea of exploiting Wikipedia knowledge. In their pioneering work, they used Wikipedia’s category information for entity disambiguation. They show that using different background knowledge, we can find efficient approaches for disambiguation. In their work, they took an assumption that every entity has a KB entry and thus the NIL entries are not handled. Similar to other researchers, Zhang et al. (2010) took an approach of classification and used a twostage approach for entity liking. They proposed a supervised model with SVM ranking to filter out the candidates and deal with disambiguation effectively. For entity diambiguation they used the contextual comparisons between the Wikipedia article and the KB article. However, their work ignores the possibilities of acronyms in the entities. Also, the ambiguous geo-political names are not handled in their work. Dredze et al. (2010) took the approach that large number of entities will be unlinkable, as there is a probability that the relevant KB e</context>
</contexts>
<marker>Zhang, Su, Tan, Wang, 2010</marker>
<rawString>Wei Zhang, Jian Su, Chew Lim Tan, and Wen Ting Wang. 2010. Entity linking leveraging automatically generated annotation. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1290–1298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Zheng</author>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to link entities with knowledge base.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>483--491</pages>
<contexts>
<context position="2919" citStr="Zheng et al., 2010" startWordPosition="460" endWordPosition="463"> more than one name variation refers to the same entity. E.g. Metro-Goldwyn-Mayer Inc. is often abbreviated as MGM. Synonymy affects entity linking when the entity mention in the document uses a name variation not covered in the entity’s knowledge base entry. Intuitively, to disambiguate a polysemous entity name, we should make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations. Indeed both strategies have been explored in existing literature (Zhang et al., 2010; Dredze et al., 2010; Zheng et al., 2010). However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data. In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval. We use the KL-divergence retrieval model (Zhai and Lafferty, 2001) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web. 804 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 804–813, Edinburgh, S</context>
<context position="32626" citStr="Zheng et al. (2010)" startWordPosition="5631" endWordPosition="5634">k. Dredze et al. (2010) took the approach that large number of entities will be unlinkable, as there is a probability that the relevant KB entry is unavailable. Their algorithm for learning NIL has shown very good results. But their proposal for handling the alias name or stage name via multiple lists is not scalable. Unlike their approach, we use the global knowledge to handle the stage names and thus this gives an optimized solution to handle alias names. Similarly, for acronyms we use the global knowledge that aids unabbreviating and thus entity disambiguation. Similar to other approaches, Zheng et al. (2010) took a learning to rank approach and compared list-wise rank model to the pair-wise rank model. They achieved good results on the list-wise ranking approach. They handled acronyms and disambiguity through wiki redirect pages and the anchor texts which is similar to others ideas. Challenges in supervised learning includes careful feature selection. The features can be selected in ad hoc manner - similarity based or semantic based. Also machine learning approach induces challenges of handling heterogenous cases. Unlike their machine learning approach which requires careful feaq+L+G q+L q+L+G Mi</context>
</contexts>
<marker>Zheng, Li, Huang, Zhu, 2010</marker>
<rawString>Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Learning to link entities with knowledge base. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 483–491.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>