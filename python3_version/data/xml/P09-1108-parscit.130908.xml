<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99348">
K-Best A* Parsing
</title>
<author confidence="0.997218">
Adam Pauls and Dan Klein
</author>
<affiliation confidence="0.998503">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.999379">
{adpauls,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999197">
A* parsing makes 1-best search efficient by
suppressing unlikely 1-best items. Existing k-
best extraction methods can efficiently search
for top derivations, but only after an exhaus-
tive 1-best pass. We present a unified algo-
rithm for k-best A* parsing which preserves
the efficiency of k-best extraction while giv-
ing the speed-ups of A* methods. Our algo-
rithm produces optimal k-best parses under the
same conditions required for optimality in a
1-best A* parser. Empirically, optimal k-best
lists can be extracted significantly faster than
with other approaches, over a range of gram-
mar types.
</bodyText>
<sectionHeader confidence="0.998424" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913651162791">
Many situations call for a parser to return the k-
best parses rather than only the 1-best. Uses for
k-best lists include minimum Bayes risk decod-
ing (Goodman, 1998; Kumar and Byrne, 2004),
discriminative reranking (Collins, 2000; Char-
niak and Johnson, 2005), and discriminative train-
ing (Och, 2003; McClosky et al., 2006). The
most efficient known algorithm for k-best parsing
(Jim´enez and Marzal, 2000; Huang and Chiang,
2005) performs an initial bottom-up dynamic pro-
gramming pass before extracting the k-best parses.
In that algorithm, the initial pass is, by far, the bot-
tleneck (Huang and Chiang, 2005).
In this paper, we propose an extension of A*
parsing which integrates k-best search with an A*-
based exploration of the 1-best chart. A* pars-
ing can avoid significant amounts of computation
by guiding 1-best search with heuristic estimates
of parse completion costs, and has been applied
successfully in several domains (Klein and Man-
ning, 2002; Klein and Manning, 2003c; Haghighi
et al., 2007). Our algorithm extends the speed-
ups achieved in the 1-best case to the k-best case
and is optimal under the same conditions as a stan-
dard A* algorithm. The amount of work done in
the k-best phase is no more than the amount of
work done by the algorithm of Huang and Chiang
(2005). Our algorithm is also equivalent to stan-
dard A* parsing (up to ties) if it is terminated after
the 1-best derivation is found. Finally, our algo-
rithm can be written down in terms of deduction
rules, and thus falls into the well-understood view
of parsing as weighted deduction (Shieber et al.,
1995; Goodman, 1998; Nederhof, 2003).
In addition to presenting the algorithm, we
show experiments in which we extract k-best lists
for three different kinds of grammars: the lexi-
calized grammars of Klein and Manning (2003b),
the state-split grammars of Petrov et al. (2006),
and the tree transducer grammars of Galley et al.
(2006). We demonstrate that optimal k-best lists
can be extracted significantly faster using our al-
gorithm than with previous methods.
</bodyText>
<sectionHeader confidence="0.973699" genericHeader="method">
2 A k-Best A* Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.999325666666667">
We build up to our full algorithm in several stages,
beginning with standard 1-best A* parsing and
making incremental modifications.
</bodyText>
<subsectionHeader confidence="0.996643">
2.1 Parsing as Weighted Deduction
</subsectionHeader>
<bodyText confidence="0.999747">
Our algorithm can be formulated in terms of
prioritized weighted deduction rules (Shieber et
al., 1995; Nederhof, 2003; Felzenszwalb and
McAllester, 2007). A prioritized weighted deduc-
tion rule has the form
</bodyText>
<equation confidence="0.9822515">
Awl,...,wn)
01 : w1,...,0. : w. 1 00 : g(w1,...,w.)
</equation>
<bodyText confidence="0.9655325">
where 01, ... , 0n are the antecedent items of the
deduction rule and 00 is the conclusion item. A
deduction rule states that, given the antecedents
01, ... , 0n with weights w1, ... , wn, the conclu-
sion 00 can be formed with weight g(w1, ... , wn)
and priority p(w1, ... , wn).
</bodyText>
<page confidence="0.94857">
958
</page>
<note confidence="0.9996145">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 958–966,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999831">
These deduction rules are “executed” within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of un-
processed items), as well as a chart of items al-
ready processed. The fundamental operation of
the algorithm is to pop the highest priority item 0
from the agenda, put it into the chart with its cur-
rent weight, and form using deduction rules any
items which can be built by combining 0 with
items already in the chart. If new or improved,
resulting items are put on the agenda with priority
given by p(·).
</bodyText>
<subsectionHeader confidence="0.992809">
2.2 A* Parsing
</subsectionHeader>
<bodyText confidence="0.999864636363636">
The A* parsing algorithm of Klein and Manning
(2003c) can be formulated in terms of weighted
deduction rules (Felzenszwalb and McAllester,
2007). We do so here both to introduce notation
and to build to our final algorithm.
First, we must formalize some notation. As-
sume we have a PCFG1 G and an input sentence
s1 ... s,,, of length n. The grammar G has a set of
symbols E, including a distinguished goal (root)
symbol G. Without loss of generality, we assume
Chomsky normal form, so each non-terminal rule
r in G has the form r = A → B C with weight wr
(the negative log-probability of the rule). Edges
are labeled spans e = (A, i, j). Inside derivations
of an edge (A, i, j) are trees rooted at A and span-
ning sz+1 ... sj. The total weight of the best (min-
imum) inside derivation for an edge e is called the
Viterbi inside score Q(e). The goal of the 1-best
A* parsing algorithm is to compute the Viterbi in-
side score of the edge (G, 0, n); backpointers al-
low the reconstruction of a Viterbi parse in the
standard way.
The basic A* algorithm operates on deduc-
tion items I(A, i, j) which represent in a col-
lapsed way the possible inside derivations of edges
(A, i, j). We call these items inside edge items or
simply inside items where clear; a graphical rep-
resentation of an inside item can be seen in Fig-
ure 1(a). The space whose items are inside edges
is called the edge space.
These inside items are combined using the sin-
gle IN deduction schema shown in Table 1. This
schema is instantiated for every grammar rule r
</bodyText>
<footnote confidence="0.992622333333333">
1While we present the algorithm specialized to parsing
with a PCFG, it generalizes to a wide range of hypergraph
search problems as shown in Klein and Manning (2001).
</footnote>
<figureCaption confidence="0.997592125">
Figure 1: Representations of the different types of
items used in parsing. (a) An inside edge item:
I(VP, 2, 5). (b) An outside edge item: O(VP, 2, 5).
(c) An inside derivation item: D(TVP, 2, 5) for a tree
TVP. (d) A ranked derivation item: K(VP, 2, 5, 6).
(e) A modified inside derivation item (with back-
pointers to ranked items): D(VP, 2, 5, 3, VP →
VBZ NP, 1, 4).
</figureCaption>
<bodyText confidence="0.986217730769231">
in G. For IN, the function g(·) simply sums the
weights of the antecedent items and the gram-
mar rule r, while the priority function p(·) adds
a heuristic to this sum. The heuristic is a bound
on the Viterbi outside score a(e) of an edge e;
see Klein and Manning (2003c) for details. A
good heuristic allows A* to reach the goal item
I(G, 0, n) while constructing few inside items.
If the heuristic is consistent, then A* guarantees
that whenever an inside item comes off the agenda,
its weight is its true Viterbi inside score (Klein and
Manning, 2003c). In particular, this guarantee im-
plies that the goal item I(G, 0, n) will be popped
with the score of the 1-best parse of the sentence.
Consistency also implies that items are popped off
the agenda in increasing order of bounded Viterbi
scores:
O(e) + h(e)
We will refer to this monotonicity as the order-
ing property of A* (Felzenszwalb and McAllester,
2007). One final property implied by consistency
is admissibility, which states that the heuristic
never overestimates the true Viterbi outside score
for an edge, i.e. h(e) ≤ a(e). For the remain-
der of this paper, we will assume our heuristics
are consistent.
</bodyText>
<subsectionHeader confidence="0.998668">
2.3 A Naive k-Best A* Algorithm
</subsectionHeader>
<bodyText confidence="0.999816">
Due to the optimal substructure of 1-best PCFG
derivations, a 1-best parser searches over the space
of edges; this is the essence of 1-best dynamic
programming. Although most edges can be built
</bodyText>
<equation confidence="0.878503666666667">
s3 s4 sS s1 ...s2 s6 ...sn
(a) VP (b)
G
VP
(c) (d) (e)
VP VP6 VP
VBZ NP
DT NN
s3 s4 sS
VBZ NP
UrNN
s3 s4 sS
VBZ1 NP4
DT NN
s3 s4 sS
</equation>
<page confidence="0.990269">
959
</page>
<table confidence="0.267955">
Inside Edge Deductions (Used in A∗ and KA∗)
w1+w2+wr+h(A,i,j)
IN: I(B, i, l) : w1 I(C, l, j) : w2 → I(A, i, j) : w1 + w2 + wr
</table>
<tableCaption confidence="0.990383">
Table 1: The deduction schema (IN) for building inside edge items, using a supplied heuristic. This schema is
sufficient on its own for 1-best A∗, and it is used in KA∗. Here, r is the rule A → B C.
</tableCaption>
<subsectionHeader confidence="0.712645">
Inside Derivation Deductions (Used in NAIVE)
</subsectionHeader>
<equation confidence="0.73137">
DERIV: D(TB, i,l) : w1 D(TC,l, j) : w2 w1+w2+wr+h(A,i,j) D A
(TB TC
l,i,j : w1 + w2 + wr
</equation>
<tableCaption confidence="0.9911705">
Table 2: The deduction schema for building derivations, using a supplied heuristic. TB and TC denote full tree
structures rooted at symbols B and C. This schema is the same as the IN deduction schema, but operates on the
space of fully specified inside derivations rather than dynamic programming edges. This schema forms the NAIVE
k-best algorithm.
</tableCaption>
<subsectionHeader confidence="0.385168">
Outside Edge Deductions (Used in KA∗)
</subsectionHeader>
<bodyText confidence="0.262874">
O(G, 0, n) : 0
</bodyText>
<equation confidence="0.96322675">
OUT-L: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3 −−−−−−−−−−→ O(B, i, l) : w1 + w3 + wr
w1+w3+wr+w2
OUT-R: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3 −−−−−−−−−−→ O(C, l, j) : w1 + w2 + wr
w1+w2+wr+w3
</equation>
<tableCaption confidence="0.991352">
Table 3: The deduction schemata for building ouside edge items. The first schema is a base case that constructs an
outside item for the goal (G, 0, n) from the inside item I(G, 0, n). The second two schemata build outside items
in a top-down fashion. Note that for outside items, the completion cost is the weight of an inside item rather than
a value computed by a heuristic.
</tableCaption>
<subsectionHeader confidence="0.327999">
Delayed Inside Derivation Deductions (Used in KA∗)
</subsectionHeader>
<bodyText confidence="0.315973">
OUT-B: I(G, 0, n) : w1 −−→ w1
</bodyText>
<equation confidence="0.52915075">
DERIV: D(TB, i,l) : w1 D(TC,l, j) : w2 O(A, i, j) : w3 w1
+w2+wr+w3 D A
(TB TC
l,i,j : w1 + w2 + wr
</equation>
<tableCaption confidence="0.771926">
Table 4: The deduction schema for building derivations, using exact outside scores computed using OUT deduc-
tions. The dependency on the outside item O(A, i, j) delays building derivation items until exact Viterbi outside
scores have been computed. This is the final search space for the KA∗ algorithm.
Ranked Inside Derivation Deductions (Lazy Version of NAIVE)
</tableCaption>
<equation confidence="0.8268605">
w1+w2+wr+h(A,i,j)
BUILD: K(B, i, l, u) : w1 K(C, l, j, v) : w2 → D(A, i, j, l, r, u, v) : w1 + w2 + wr
maxm wm+h(A,i,j)
RANK: D1(A, i, j, ·) : w1 ... Dk(A, i, j, ·) : wk −−−−−−−−−−−−→ K(A, i, j, k) : maxm wm
</equation>
<tableCaption confidence="0.635283">
Table 5: The schemata for simultaneously building and ranking derivations, using a supplied heuristic, for the lazier
</tableCaption>
<bodyText confidence="0.9809925">
form of the NAIVE algorithm. BUILD builds larger derivations from smaller ones. RANK numbers derivations
for each edge. Note that RANK requires distinct Di, so a rank k RANK rule will first apply (optimally) as soon as
the kth-best inside derivation item for a given edge is removed from the queue. However, it will also still formally
apply (suboptimally) for all derivation items dequeued after the kth. In practice, the RANK schema need not be
implemented explicitly – one can simply assign a rank to each inside derivation item when it is removed from the
agenda, and directly add the appropriate ranked inside item to the chart.
</bodyText>
<subsectionHeader confidence="0.805166">
Delayed Ranked Inside Derivation Deductions (Lazy Version of KA∗)
</subsectionHeader>
<bodyText confidence="0.48123">
: maxm wm
Table 6: The deduction schemata for building and ranking derivations, using exact outside scores computed from
OUT deductions, used for the lazier form of the KA∗ algorithm.
</bodyText>
<equation confidence="0.26475625">
BUILD: K(B, i, l, u) : w1 K(C, l, j, v) : w2 O(A, i, j) : w3 −−−−−−−−−−→D(A, i, j, l, r, u, v) : w1 + w2 + wr
w1+w2+wr+w3
maxm wm+wk+1
RANK: D1(A,i,j,·) : w1 ... Dk(A,i,j,·) : wk O(A,i,j) : wk+1 −−−−−−−−−−−→ K(A,i,j,k)
</equation>
<page confidence="0.976578">
960
</page>
<bodyText confidence="0.999776613636364">
using many derivations, each inside edge item
will be popped exactly once during parsing, with
a score and backpointers representing its 1-best
derivation.
However, k-best lists involve suboptimal
derivations. One way to compute k-best deriva-
tions is therefore to abandon optimal substructure
and dynamic programming entirely, and to search
over the derivation space, the much larger space
of fully specified trees. The items in this space are
called inside derivation items, or derivation items
where clear, and are of the form D(TA, i, j), spec-
ifying an entire tree TA rooted at symbol A and
spanning si+1 ... sj (see Figure 1(c)). Derivation
items are combined using the DERIV schema of
Table 2. The goals in this space, representing root
parses, are any derivation items rooted at symbol
G that span the entire input.
In this expanded search space, each distinct
parse has its own derivation item, derivable only
in one way. If we continue to search long enough,
we will pop multiple goal items. The first k which
come off the agenda will be the k-best derivations.
We refer to this approach as NAIVE. It is very in-
efficient on its own, but it leads to the full algo-
rithm.
The correctness of this k-best algorithm follows
from the correctness of A* parsing. The derivation
space of full trees is simply the edge space of a
much larger grammar (see Section 2.5).
Note that the DERIV schema’s priority includes
a heuristic just like 1-best A*. Because of the
context freedom of the grammar, any consistent
heuristic for inside edge items usable in 1-best A*
is also consistent for inside derivation items (and
vice versa). In particular, the 1-best Viterbi out-
side score for an edge is a “perfect” heuristic for
any derivation of that edge.
While correct, NAIVE is massively inefficient.
In comparison with A* parsing over g, where there
are O(n2) inside items, the size of the derivation
space is exponential in the sentence length. By
the ordering property, we know that NAIVE will
process all derivation items d with
</bodyText>
<equation confidence="0.911019">
δ(d) + h(d) c δ(gk)
</equation>
<bodyText confidence="0.984048055555556">
where gk is the kth-best root parse and δ(·) is the
inside score of a derivation item (analogous to β
for edges).2 Even for reasonable heuristics, this
2The new symbol emphasizes that S scores a specific
derivation rather than a minimum over a set of derivations.
number can be very large; see Section 3 for empir-
ical results.
This naive algorithm is, of course, not novel, ei-
ther in general approach or specific computation.
Early k-best parsers functioned by abandoning dy-
namic programming and performing beam search
on derivations (Ratnaparkhi, 1999; Collins, 2000).
Huang (2005) proposes an extension of Knuth’s
algorithm (Knuth, 1977) to produce k-best lists
by searching in the space of derivations, which
is essentially this algorithm. While Huang (2005)
makes no explicit mention of a heuristic, it would
be easy to incorporate one into their formulation.
</bodyText>
<subsectionHeader confidence="0.976655">
2.4 A New k-Best A* Parser
</subsectionHeader>
<bodyText confidence="0.999967">
While NAIVE suffers severe performance degra-
dation for loose heuristics, it is in fact very effi-
cient if h(·) is “perfect,” i.e. h(e) = α(e) be. In
this case, the ordering property of A* guarantees
that only inside derivation items d satisfying
</bodyText>
<equation confidence="0.973222">
δ(d) + α(d) c δ(gk)
</equation>
<bodyText confidence="0.999857172413793">
will be placed in the chart. The set of derivation
items d satisfying this inequality is exactly the set
which appear in the k-best derivations of (G, 0, n)
(as always, modulo ties). We could therefore use
NAIVE quite efficiently if we could obtain exact
Viterbi outside scores.
One option is to compute outside scores with
exhaustive dynamic programming over the orig-
inal grammar. In a certain sense, described in
greater detail below, this precomputation of exact
heuristics is equivalent to the k-best extraction al-
gorithm of Huang and Chiang (2005). However,
this exhaustive 1-best work is precisely what we
want to use A* to avoid.
Our algorithm solves this problem by integrat-
ing three searches into a single agenda-driven pro-
cess. First, an A* search in the space of inside
edge items with an (imperfect) external heuristic
h(·) finds exact inside scores. Second, exact out-
side scores are computed from inside and outside
items. Finally, these exact outside scores guide the
search over derivations. It can be useful to imag-
ine these three operations as operating in phases,
but they are all interleaved, progressing in order of
their various priorities.
In order to calculate outside scores, we intro-
duce outside items O(A, i, j), which represent
best derivations of G —* s1 ... si A sj+1 . . . sn;
see Figure 1(b). Where the weights of inside items
</bodyText>
<page confidence="0.990678">
961
</page>
<bodyText confidence="0.999241083333334">
compute Viterbi inside scores, the weights of out-
side items compute Viterbi outside scores.
Table 3 shows deduction schemata for building
outside items. These schemata are adapted from
the schemata used in the general hierarchical A∗
algorithm of Felzenszwalb and McAllester (2007).
In that work, it is shown that such schemata main-
tain the property that the weight of an outside item
is the true Viterbi outside score when it is removed
from the agenda. They also show that outside
items o follow an ordering property, namely that
they are processed in increasing order of
</bodyText>
<equation confidence="0.970528">
Q(o) + α(o)
</equation>
<bodyText confidence="0.999313">
This quantity is the score of the best root deriva-
tion which includes the edge corresponding to o.
Felzenszwalb and McAllester (2007) also show
that both inside and outside items can be processed
on the same queue and the ordering property holds
jointly for both types of items.
If we delay the construction of a derivation
item until its corresponding outside item has been
popped, then we can gain the benefits of using an
exact heuristic h(·) in the naive algorithm. We re-
alize this delay by modifying the DERIV deduc-
tion schema as shown in Table 4 to trigger on and
prioritize with the appropriate outside scores.
We now have our final algorithm, which we call
KA∗. It is the union of the IN, OUT, and new “de-
layed” DERIV deduction schemata. In words, our
algorithm functions as follows: we initialize the
agenda with I(si, i − 1, i) and D(si, i − 1, i) for
i = 1... n. We compute inside scores in standard
A∗ fashion using the IN deduction rule, using any
heuristic we might provide to 1-best A∗. Once the
inside item I(G, 0, n) is found, we automatically
begin to compute outside scores via the OUT de-
duction rules. Once 0(si, i − 1, i) is found, we
can begin to also search in the space of deriva-
tion items, using the perfect heuristics given by
the just-computed outside scores. Note, however,
that all computation is done with a single agenda,
so the processing of all three types of items is in-
terleaved, with the k-best search possibly termi-
nating without a full inside computation. As with
NAIVE, the algorithm terminates when a k-th goal
derivation is dequeued.
</bodyText>
<subsectionHeader confidence="0.977029">
2.5 Correctness
</subsectionHeader>
<bodyText confidence="0.999924945945946">
We prove the correctness of this algorithm by a re-
duction to the hierarchical A∗ (HA∗) algorithm of
Felzenszwalb and McAllester (2007). The input
to HA∗ is a target grammar G,,t and a list of gram-
mars G0 ... G,,t−1 in which Gt−1 is a relaxed pro-
jection of Gt for all t = 1... m. A grammar Gt−1
is a projection of Gt if there exists some onto func-
tion 7rt : Et 7→ Et−1 defined for all symbols in Gt.
We use At−1 to represent 7rt(At). A projection is
relaxed if, for every rule r = At → BtCt with
weight wr there is a rule r0 = At−1 → Bt−1Ct−1
in Gt−1 with weight wry ≤ wr.
We assume that our external heuristic function
h(·) is constructed by parsing our input sentence
with a relaxed projection of our target grammar.
This assumption, though often true anyway, is
to allow proof by reduction to Felzenszwalb and
McAllester (2007).3
We construct an instance of HA∗ as follows: Let
G0 be the relaxed projection which computes the
heuristic. Let G1 be the input grammar G, and let
G2, the target grammar of our HA∗ instance, be the
grammar of derivations in G formed by expanding
each symbol A in G to all possible inside deriva-
tions TA rooted at A. The rules in G2 have the form
TA → TB TC with weight given by the weight of
the rule A → B C. By construction, G1 is a re-
laxed projection of G2; by assumption G0 is a re-
laxed projection of G1. The deduction rules that
describe KA∗ build the same items as HA∗ with
same weights and priorities, and so the guarantees
from HA∗ carry over to KA∗.
We can characterize the amount of work done
using the ordering property. Let gk be the kth-best
derivation item for the goal edge g. Our algorithm
processes all derivation items d, outside items o,
and inside items i satisfying
</bodyText>
<equation confidence="0.999979666666667">
6(d) + α(d) ≤ 6(gk)
Q(o) + α(o) ≤ 6(gk)
Q(i) + h(i) ≤ 6(gk)
</equation>
<bodyText confidence="0.999569555555556">
We have already argued that the set of deriva-
tion items satisfying the first inequality is the set of
subtrees that appear in the optimal k-best parses,
modulo ties. Similarly, it can be shown that the
second inequality is satisfied only for edges that
appear in the optimal k-best parses. The last in-
equality characterizes the amount of work done in
the bottom-up pass. We compare this to 1-best A∗,
which pops all inside items i satisfying
</bodyText>
<equation confidence="0.884401">
Q(i) + h(i) ≤ Q(g) = 6(g1)
</equation>
<footnote confidence="0.9891015">
3KA* is correct for any consistent heuristic but a non-
reductive proof is not possible in the present space.
</footnote>
<page confidence="0.99065">
962
</page>
<bodyText confidence="0.842508">
Derivation items pushed (millions)
Thus, the “extra” inside items popped in the
bottom-up pass during k-best parsing as compared
to 1-best parsing are those items i satisfying
</bodyText>
<equation confidence="0.970686">
δ(g1) G β(i) + h(i) G δ(gk)
</equation>
<bodyText confidence="0.999893375">
The question of how many items satisfy these
inequalities is empirical; we show in our experi-
ments that it is small for reasonable heuristics. At
worst, the bottom-up phase pops all inside items
and reduces to exhaustive dynamic programming.
Additionally, it is worth noting that our algo-
rithm is naturally online in that it can be stopped
at any k without advance specification.
</bodyText>
<subsectionHeader confidence="0.98448">
2.6 Lazy Successor Functions
</subsectionHeader>
<bodyText confidence="0.98936927027027">
The global ordering property guarantees that we
will only dequeue derivation fragments of top
parses. However, we will enqueue all combina-
tions of such items, which is wasteful. By ex-
ploiting a local ordering amongst derivations, we
can be more conservative about combination and
gain the advantages of a lazy successor function
(Huang and Chiang, 2005).
To do so, we represent inside derivations not
by explicitly specifying entire trees, but rather
by using ranked backpointers. In this represen-
tation, inside derivations are represented in two
ways, shown in Figure 1(d) and (e). The first
way (d) simply adds a rank u to an edge, giving
a tuple (A, i, j, u). The corresponding item is the
ranked derivation item K(A, i, j, u), which repre-
sents the uth-best derivation of A over (i, j). The
second representation (e) is a backpointer of the
form (A, i, j,l, r, u, v), specifying the derivation
formed by combining the uth-best derivation of
(B, i, l) and the vth-best derivation of (C, l, j) us-
ing rule r = A —* B C. The corresponding items
D(A, i, j,l, r, u, v) are the new form of our inside
derivation items.
The modified deduction schemata for the
NAIVE algorithm over these representations are
shown in Table 5. The BUILD schema pro-
duces new inside derivation items from ranked
derivation items, while the RANK schema as-
signs each derivation item a rank; together they
function like DERIV. We can find the k-best list
by searching until K(G, 0, n, k) is removed from
the agenda. The k-best derivations can then
be extracted by following the backpointers for
K(G, 0, n,1) ... K(G, 0, n, k). The KA* algo-
rithm can be modified in the same way, shown in
Table 6.
</bodyText>
<figure confidence="0.7427945">
5-split 4-split 3-split 2-split 1-split 0-split
Heuristic
</figure>
<figureCaption confidence="0.995779333333333">
Figure 2: Number of derivation items enqueued as a
function of heuristic. Heuristics are shown in decreas-
ing order of tightness. The y-axis is on a log-scale.
</figureCaption>
<bodyText confidence="0.99981075">
The actual laziness is provided by addition-
ally delaying the combination of ranked items.
When an item K(B, i, l, u) is popped off the
queue, a naive implementation would loop over
items K(C, l, j, v) for all v, C, and j (and
similarly for left combinations). Fortunately,
little looping is actually necessary: there is
a partial ordering of derivation items, namely,
that D(A, i, j, l, r, u, v) will have a lower com-
puted priority than D(A, i, j, l, r, u − 1,v) and
D(A, i, j, l, r, u, v − 1) (Jim´enez and Marzal,
2000). So, we can wait until one of the latter two
is built before “triggering” the construction of the
former. This triggering is similar to the “lazy fron-
tier” used by Huang and Chiang (2005). All of our
experiments use this lazy representation.
</bodyText>
<sectionHeader confidence="0.999761" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999883">
3.1 State-Split Grammars
</subsectionHeader>
<bodyText confidence="0.999852133333333">
We performed our first experiments with the gram-
mars of Petrov et al. (2006). The training pro-
cedure for these grammars produces a hierarchy
of increasingly refined grammars through state-
splitting. We followed Pauls and Klein (2009) in
computing heuristics for the most refined grammar
from outside scores for less-split grammars.
We used the Berkeley Parser4 to learn such
grammars from Sections 2-21 of the Penn Tree-
bank (Marcus et al., 1993). We trained with 6
split-merge cycles, producing 7 grammars. We
tested these grammars on 100 sentences of length
at most 30 of Section 23 of the Treebank. Our
“target grammar” was in all cases the most split
grammar.
</bodyText>
<footnote confidence="0.961366">
4http://berkeleyparser.googlecode.com
</footnote>
<table confidence="0.422680666666667">
1 5 50 500
NAIVE
KA*
</table>
<page confidence="0.760288">
963
</page>
<figure confidence="0.999598615384615">
Items pushed (millions)
0 5000 15000 25000
EXH
KA*
Items pushed (millions)
0 5000 15000 25000
K Best
Bottom-up
K Best
Bottom-up
Heuristic
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
k k
</figure>
<figureCaption confidence="0.995271">
Figure 3: The cost of k-best extraction as a function of k for state-split grammars, for both KA* and EXH. The
amount of time spent in the k-best phase is negligible compared to the cost of the bottom-up phase in both cases.
</figureCaption>
<bodyText confidence="0.999930428571429">
Heuristics computed from projections to suc-
cessively smaller grammars in the hierarchy form
successively looser bounds on the outside scores.
This allows us to examine the performance as a
function of the tightness of the heuristic. We first
compared our algorithm KA* against the NAIVE
algorithm. We extracted 1000-best lists using each
algorithm, with heuristics computed using each of
the 6 smaller grammars.
In Figure 2, we evaluate only the k-best extrac-
tion phase by plotting the number of derivation
items and outside items added to the agenda as
a function of the heuristic used, for increasingly
loose heuristics. We follow earlier work (Pauls
and Klein, 2009) in using number of edges pushed
as the primary, hardware-invariant metric for eval-
uating performance of our algorithms.5 While
KA* scales roughly linearly with the looseness of
the heuristic, NAIVE degrades very quickly as the
heuristics get worse. For heuristics given by gram-
mars weaker than the 4-split grammar, NAIVE ran
out of memory.
Since the bottom-up pass of k-best parsing is
the bottleneck, we also examine the time spent
in the 1-best phase of k-best parsing. As a base-
line, we compared KA* to the approach of Huang
and Chiang (2005), which we will call EXH (see
below for more explanation) since it requires ex-
haustive parsing in the bottom-up pass. We per-
formed the exhaustive parsing needed for EXH
in our agenda-based parser to facilitate compar-
ison. For KA*, we included the cost of com-
puting the heuristic, which was done by running
our agenda-based parser exhaustively on a smaller
grammar to compute outside items; we chose the
</bodyText>
<footnote confidence="0.9915355">
5We found that edges pushed was generally well corre-
lated with parsing time.
</footnote>
<figure confidence="0.924297">
0 2000 4000 6000 8000 10000
k
</figure>
<figureCaption confidence="0.75010375">
Figure 4: The performance of KA* for lexicalized
grammars. The performance is dominated by the com-
putation of the heuristic, so that both the bottom-up
phase and the k-best phase are barely visible.
</figureCaption>
<bodyText confidence="0.99996225">
3-split grammar for the heuristic since it gives the
best overall tradeoff of heuristic and bottom-up
parsing time. We separated the items enqueued
into items enqueued while computing the heuris-
tic (not strictly part of the algorithm), inside items
(“bottom-up”), and derivation and outside items
(together “k-best”). The results are shown in Fig-
ure 3. The cost of k-best extraction is clearly
dwarfed by the the 1-best computation in both
cases. However, KA* is significantly faster over
the bottom-up computations, even when the cost
of computing the heuristic is included.
</bodyText>
<subsectionHeader confidence="0.997495">
3.2 Lexicalized Parsing
</subsectionHeader>
<bodyText confidence="0.9999448">
We also experimented with the lexicalized parsing
model described in Klein and Manning (2003b).
This model is constructed as the product of a
dependency model and the unlexicalized PCFG
model in Klein and Manning (2003a). We
</bodyText>
<figure confidence="0.9862371">
KA*
Items pushed (millions)
0 200 600 1000
K Best
Bottom-up
Heuristic
964
Items pushed (millions)
0 500 1500 2500
EXH
KA*
Items pushed (millions)
0 500 1500 2500
K Best
Bottom-up
K Best
Bottom-up
Heuristic
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
k k
</figure>
<figureCaption confidence="0.999939">
Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA* and EXH.
</figureCaption>
<bodyText confidence="0.999661142857143">
constructed these grammars using the Stanford
Parser.6 The model was trained on Sections 2-20
of the Penn Treebank and tested on 100 sentences
of Section 21 of length at most 30 words.
For this grammar, Klein and Manning (2003b)
showed that a very accurate heuristic can be con-
structed by taking the sum of outside scores com-
puted with the dependency model and the PCFG
model individually. We report performance as a
function of k for KA* in Figure 4. Both NAIVE
and EXH are impractical on these grammars due
to memory limitations. For KA*, computing the
heuristic is the bottleneck, after which bottom-up
parsing and k-best extraction are very fast.
</bodyText>
<subsectionHeader confidence="0.998567">
3.3 Tree Transducer Grammars
</subsectionHeader>
<bodyText confidence="0.9998575">
Syntactic machine translation (Galley et al., 2004)
uses tree transducer grammars to translate sen-
tences. Transducer rules are synchronous context-
free productions that have both a source and a tar-
get side. We examine the cost of k-best parsing in
the source side of such grammars with KA*, which
can be a first step in translation.
We extracted a grammar from 220 million
words of Arabic-English bitext using the approach
of Galley et al. (2006), extracting rules with at
most 3 non-terminals. These rules are highly lex-
icalized. About 300K rules are applicable for a
typical 30-word sentence; we filter the rest. We
tested on 100 sentences of length at most 40 from
the NIST05 Arabic-English test set.
We used a simple but effective heuristic for
these grammars, similar to the FILTER heuristic
suggested in Klein and Manning (2003c). We pro-
jected the source projection to a smaller grammar
by collapsing all non-terminal symbols to X, and
</bodyText>
<footnote confidence="0.876479">
6http://nlp.stanford.edu/software/
</footnote>
<bodyText confidence="0.9999365">
also collapsing pre-terminals into related clusters.
For example, we collapsed the tags NN, NNS,
NNP, and NNPS to N. This projection reduced
the number of grammar symbols from 149 to 36.
Using it as a heuristic for the full grammar sup-
pressed — 60% of the total items (Figure 5).
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9999635">
While formulated very differently, one limiting
case of our algorithm relates closely to the EXH
algorithm of Huang and Chiang (2005). In par-
ticular, if all inside items are processed before any
derivation items, the subsequent number of deriva-
tion items and outside items popped by KA* is
nearly identical to the number popped by EXH in
our experiments (both algorithms have the same
ordering bounds on which derivation items are
popped). The only real difference between the al-
gorithms in this limited case is that EXH places
k-best items on local priority queues per edge,
while KA* makes use of one global queue. Thus,
in addition to providing a method for speeding
up k-best extraction with A*, our algorithm also
provides an alternate form of Huang and Chiang
(2005)’s k-best extraction that can be phrased in a
weighted deduction system.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999982777777778">
We have presented KA*, an extension of A* pars-
ing that allows extraction of optimal k-best parses
without the need for an exhaustive 1-best pass. We
have shown in several domains that, with an ap-
propriate heuristic, our algorithm can extract k-
best lists in a fraction of the time required by cur-
rent approaches to k-best extraction, giving the
best of both A* parsing and efficient k-best extrac-
tion, in a unified procedure.
</bodyText>
<page confidence="0.997399">
965
</page>
<sectionHeader confidence="0.98375" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992836">
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning (ICML).
P. Felzenszwalb and D. McAllester. 2007. The gener-
alized A* architecture. Journal of Artificial Intelli-
gence Research.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation
rule? In Human Language Technologies: The An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
ACL).
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The
Annual Conference of the Association for Compu-
tational Linguistics (ACL).
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D.
thesis, Harvard University.
Aria Haghighi, John DeNero, and Dan Klein. 2007.
Approximate factoring for A* search. In Proceed-
ings of HLT-NAACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT), pages 53–64.
Liang Huang. 2005. Unpublished manuscript.
http://www.cis.upenn.edu/˜lhuang3/
knuth.pdf.
V´ıctor M. Jim´enez and Andr´es Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183–192, Lon-
don, UK. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In IWPT, pages 123–134.
Dan Klein and Chris Manning. 2002. Fast exact in-
ference with a factored model for natural language
processing,. In Proceedings of NIPS.
Dan Klein and Chris Manning. 2003a. Accurate unlex-
icalized parsing. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL).
Dan Klein and Chris Manning. 2003b. Factored A*
search for models over sequences and trees. In Pro-
ceedings of the International Joint Conference on
Artificial Intelligence (IJCAI).
Dan Klein and Christopher D. Manning. 2003c. A*
parsing: Fast exact Viterbi parse selection. In
In Proceedings of the Human Language Technol-
ogy Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119–126.
Donald Knuth. 1977. A generalization of Dijkstra’s
algorithm. Information Processing Letters, 6(1):1–
5.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 152–159.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Computationl Linguis-
tics, 29(1):135–143.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 160–167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL 2006.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. In Ma-
chine Learning, volume 34, pages 151–5175.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3–36.
</reference>
<page confidence="0.998668">
966
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.870755">
<author confidence="0.966914">Pauls Klein</author>
<affiliation confidence="0.99599">Computer Science Division University of California, Berkeley</affiliation>
<abstract confidence="0.993644733333333">makes 1-best search efficient by unlikely 1-best items. Existing best extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algofor which preserves efficiency of extraction while givthe speed-ups of Our algoproduces optimal parses under the same conditions required for optimality in a Empirically, optimal lists can be extracted significantly faster than with other approaches, over a range of grammar types.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1011" citStr="Charniak and Johnson, 2005" startWordPosition="150" endWordPosition="154">t A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of par</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="982" citStr="Collins, 2000" startWordPosition="148" endWordPosition="149">rithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search wi</context>
<context position="13918" citStr="Collins, 2000" startWordPosition="2469" endWordPosition="2470">l derivation items d with δ(d) + h(d) c δ(gk) where gk is the kth-best root parse and δ(·) is the inside score of a derivation item (analogous to β for edges).2 Even for reasonable heuristics, this 2The new symbol emphasizes that S scores a specific derivation rather than a minimum over a set of derivations. number can be very large; see Section 3 for empirical results. This naive algorithm is, of course, not novel, either in general approach or specific computation. Early k-best parsers functioned by abandoning dynamic programming and performing beam search on derivations (Ratnaparkhi, 1999; Collins, 2000). Huang (2005) proposes an extension of Knuth’s algorithm (Knuth, 1977) to produce k-best lists by searching in the space of derivations, which is essentially this algorithm. While Huang (2005) makes no explicit mention of a heuristic, it would be easy to incorporate one into their formulation. 2.4 A New k-Best A* Parser While NAIVE suffers severe performance degradation for loose heuristics, it is in fact very efficient if h(·) is “perfect,” i.e. h(e) = α(e) be. In this case, the ordering property of A* guarantees that only inside derivation items d satisfying δ(d) + α(d) c δ(gk) will be plac</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of the Seventeenth International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Felzenszwalb</author>
<author>D McAllester</author>
</authors>
<title>The generalized A* architecture.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="3149" citStr="Felzenszwalb and McAllester, 2007" startWordPosition="500" endWordPosition="503">alized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. 2 A k-Best A* Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A* parsing and making incremental modifications. 2.1 Parsing as Weighted Deduction Our algorithm can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003; Felzenszwalb and McAllester, 2007). A prioritized weighted deduction rule has the form Awl,...,wn) 01 : w1,...,0. : w. 1 00 : g(w1,...,w.) where 01, ... , 0n are the antecedent items of the deduction rule and 00 is the conclusion item. A deduction rule states that, given the antecedents 01, ... , 0n with weights w1, ... , wn, the conclusion 00 can be formed with weight g(w1, ... , wn) and priority p(w1, ... , wn). 958 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 958–966, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP These deduction rules are “executed” within a generic age</context>
<context position="4441" citStr="Felzenszwalb and McAllester, 2007" startWordPosition="732" endWordPosition="735">zed fashion. The algorithm maintains an agenda (a priority queue of unprocessed items), as well as a chart of items already processed. The fundamental operation of the algorithm is to pop the highest priority item 0 from the agenda, put it into the chart with its current weight, and form using deduction rules any items which can be built by combining 0 with items already in the chart. If new or improved, resulting items are put on the agenda with priority given by p(·). 2.2 A* Parsing The A* parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). We do so here both to introduce notation and to build to our final algorithm. First, we must formalize some notation. Assume we have a PCFG1 G and an input sentence s1 ... s,,, of length n. The grammar G has a set of symbols E, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r in G has the form r = A → B C with weight wr (the negative log-probability of the rule). Edges are labeled spans e = (A, i, j). Inside derivations of an edge (A, i, j) are trees rooted at A and spanning sz+1 ... sj. The total weight of</context>
<context position="7266" citStr="Felzenszwalb and McAllester, 2007" startWordPosition="1260" endWordPosition="1263">ils. A good heuristic allows A* to reach the goal item I(G, 0, n) while constructing few inside items. If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). In particular, this guarantee implies that the goal item I(G, 0, n) will be popped with the score of the 1-best parse of the sentence. Consistency also implies that items are popped off the agenda in increasing order of bounded Viterbi scores: O(e) + h(e) We will refer to this monotonicity as the ordering property of A* (Felzenszwalb and McAllester, 2007). One final property implied by consistency is admissibility, which states that the heuristic never overestimates the true Viterbi outside score for an edge, i.e. h(e) ≤ a(e). For the remainder of this paper, we will assume our heuristics are consistent. 2.3 A Naive k-Best A* Algorithm Due to the optimal substructure of 1-best PCFG derivations, a 1-best parser searches over the space of edges; this is the essence of 1-best dynamic programming. Although most edges can be built s3 s4 sS s1 ...s2 s6 ...sn (a) VP (b) G VP (c) (d) (e) VP VP6 VP VBZ NP DT NN s3 s4 sS VBZ NP UrNN s3 s4 sS VBZ1 NP4 DT</context>
<context position="16149" citStr="Felzenszwalb and McAllester (2007)" startWordPosition="2839" endWordPosition="2842"> can be useful to imagine these three operations as operating in phases, but they are all interleaved, progressing in order of their various priorities. In order to calculate outside scores, we introduce outside items O(A, i, j), which represent best derivations of G —* s1 ... si A sj+1 . . . sn; see Figure 1(b). Where the weights of inside items 961 compute Viterbi inside scores, the weights of outside items compute Viterbi outside scores. Table 3 shows deduction schemata for building outside items. These schemata are adapted from the schemata used in the general hierarchical A∗ algorithm of Felzenszwalb and McAllester (2007). In that work, it is shown that such schemata maintain the property that the weight of an outside item is the true Viterbi outside score when it is removed from the agenda. They also show that outside items o follow an ordering property, namely that they are processed in increasing order of Q(o) + α(o) This quantity is the score of the best root derivation which includes the edge corresponding to o. Felzenszwalb and McAllester (2007) also show that both inside and outside items can be processed on the same queue and the ordering property holds jointly for both types of items. If we delay the </context>
<context position="18179" citStr="Felzenszwalb and McAllester (2007)" startWordPosition="3203" endWordPosition="3206">s via the OUT deduction rules. Once 0(si, i − 1, i) is found, we can begin to also search in the space of derivation items, using the perfect heuristics given by the just-computed outside scores. Note, however, that all computation is done with a single agenda, so the processing of all three types of items is interleaved, with the k-best search possibly terminating without a full inside computation. As with NAIVE, the algorithm terminates when a k-th goal derivation is dequeued. 2.5 Correctness We prove the correctness of this algorithm by a reduction to the hierarchical A∗ (HA∗) algorithm of Felzenszwalb and McAllester (2007). The input to HA∗ is a target grammar G,,t and a list of grammars G0 ... G,,t−1 in which Gt−1 is a relaxed projection of Gt for all t = 1... m. A grammar Gt−1 is a projection of Gt if there exists some onto function 7rt : Et 7→ Et−1 defined for all symbols in Gt. We use At−1 to represent 7rt(At). A projection is relaxed if, for every rule r = At → BtCt with weight wr there is a rule r0 = At−1 → Bt−1Ct−1 in Gt−1 with weight wry ≤ wr. We assume that our external heuristic function h(·) is constructed by parsing our input sentence with a relaxed projection of our target grammar. This assumption,</context>
</contexts>
<marker>Felzenszwalb, McAllester, 2007</marker>
<rawString>P. Felzenszwalb and D. McAllester. 2007. The generalized A* architecture. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLTACL).</booktitle>
<contexts>
<context position="28697" citStr="Galley et al., 2004" startWordPosition="5045" endWordPosition="5048">ank and tested on 100 sentences of Section 21 of length at most 30 words. For this grammar, Klein and Manning (2003b) showed that a very accurate heuristic can be constructed by taking the sum of outside scores computed with the dependency model and the PCFG model individually. We report performance as a function of k for KA* in Figure 4. Both NAIVE and EXH are impractical on these grammars due to memory limitations. For KA*, computing the heuristic is the bottleneck, after which bottom-up parsing and k-best extraction are very fast. 3.3 Tree Transducer Grammars Syntactic machine translation (Galley et al., 2004) uses tree transducer grammars to translate sentences. Transducer rules are synchronous contextfree productions that have both a source and a target side. We examine the cost of k-best parsing in the source side of such grammars with KA*, which can be a first step in translation. We extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al. (2006), extracting rules with at most 3 non-terminals. These rules are highly lexicalized. About 300K rules are applicable for a typical 30-word sentence; we filter the rest. We tested on 100 sentences of length </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLTACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2667" citStr="Galley et al. (2006)" startWordPosition="428" endWordPosition="431">gorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. 2 A k-Best A* Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A* parsing and making incremental modifications. 2.1 Parsing as Weighted Deduction Our algorithm can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003; Felzenszwalb and McAllester, 2007). A prioritized weighted deduction rule has the form Awl,...,wn) 01 : w1,...,0. : w. 1 00 : g(w1,...,w.) where 01, ...</context>
<context position="29091" citStr="Galley et al. (2006)" startWordPosition="5113" endWordPosition="5116">due to memory limitations. For KA*, computing the heuristic is the bottleneck, after which bottom-up parsing and k-best extraction are very fast. 3.3 Tree Transducer Grammars Syntactic machine translation (Galley et al., 2004) uses tree transducer grammars to translate sentences. Transducer rules are synchronous contextfree productions that have both a source and a target side. We examine the cost of k-best parsing in the source side of such grammars with KA*, which can be a first step in translation. We extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al. (2006), extracting rules with at most 3 non-terminals. These rules are highly lexicalized. About 300K rules are applicable for a typical 30-word sentence; we filter the rest. We tested on 100 sentences of length at most 40 from the NIST05 Arabic-English test set. We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). We projected the source projection to a smaller grammar by collapsing all non-terminal symbols to X, and 6http://nlp.stanford.edu/software/ also collapsing pre-terminals into related clusters. For example, we </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In The Annual Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing Inside-Out.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="917" citStr="Goodman, 1998" startWordPosition="140" endWordPosition="141">t only after an exhaustive 1-best pass. We present a unified algorithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avo</context>
<context position="2354" citStr="Goodman, 1998" startWordPosition="380" endWordPosition="381">ighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A* algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. 2 A k-Best A* Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A* parsing and making incremental modifica</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Approximate factoring for A* search.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1759" citStr="Haghighi et al., 2007" startWordPosition="270" endWordPosition="273">nez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A* algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Ned</context>
</contexts>
<marker>Haghighi, DeNero, Klein, 2007</marker>
<rawString>Aria Haghighi, John DeNero, and Dan Klein. 2007. Approximate factoring for A* search. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>53--64</pages>
<contexts>
<context position="1182" citStr="Huang and Chiang, 2005" startWordPosition="177" endWordPosition="180">ns required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends</context>
<context position="15058" citStr="Huang and Chiang (2005)" startWordPosition="2657" endWordPosition="2660">ees that only inside derivation items d satisfying δ(d) + α(d) c δ(gk) will be placed in the chart. The set of derivation items d satisfying this inequality is exactly the set which appear in the k-best derivations of (G, 0, n) (as always, modulo ties). We could therefore use NAIVE quite efficiently if we could obtain exact Viterbi outside scores. One option is to compute outside scores with exhaustive dynamic programming over the original grammar. In a certain sense, described in greater detail below, this precomputation of exact heuristics is equivalent to the k-best extraction algorithm of Huang and Chiang (2005). However, this exhaustive 1-best work is precisely what we want to use A* to avoid. Our algorithm solves this problem by integrating three searches into a single agenda-driven process. First, an A* search in the space of inside edge items with an (imperfect) external heuristic h(·) finds exact inside scores. Second, exact outside scores are computed from inside and outside items. Finally, these exact outside scores guide the search over derivations. It can be useful to imagine these three operations as operating in phases, but they are all interleaved, progressing in order of their various pr</context>
<context position="21370" citStr="Huang and Chiang, 2005" startWordPosition="3787" endWordPosition="3790">cs. At worst, the bottom-up phase pops all inside items and reduces to exhaustive dynamic programming. Additionally, it is worth noting that our algorithm is naturally online in that it can be stopped at any k without advance specification. 2.6 Lazy Successor Functions The global ordering property guarantees that we will only dequeue derivation fragments of top parses. However, we will enqueue all combinations of such items, which is wasteful. By exploiting a local ordering amongst derivations, we can be more conservative about combination and gain the advantages of a lazy successor function (Huang and Chiang, 2005). To do so, we represent inside derivations not by explicitly specifying entire trees, but rather by using ranked backpointers. In this representation, inside derivations are represented in two ways, shown in Figure 1(d) and (e). The first way (d) simply adds a rank u to an edge, giving a tuple (A, i, j, u). The corresponding item is the ranked derivation item K(A, i, j, u), which represents the uth-best derivation of A over (i, j). The second representation (e) is a backpointer of the form (A, i, j,l, r, u, v), specifying the derivation formed by combining the uth-best derivation of (B, i, l)</context>
<context position="23610" citStr="Huang and Chiang (2005)" startWordPosition="4196" endWordPosition="4199">ms. When an item K(B, i, l, u) is popped off the queue, a naive implementation would loop over items K(C, l, j, v) for all v, C, and j (and similarly for left combinations). Fortunately, little looping is actually necessary: there is a partial ordering of derivation items, namely, that D(A, i, j, l, r, u, v) will have a lower computed priority than D(A, i, j, l, r, u − 1,v) and D(A, i, j, l, r, u, v − 1) (Jim´enez and Marzal, 2000). So, we can wait until one of the latter two is built before “triggering” the construction of the former. This triggering is similar to the “lazy frontier” used by Huang and Chiang (2005). All of our experiments use this lazy representation. 3 Experiments 3.1 State-Split Grammars We performed our first experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting. We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars. We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. </context>
<context position="26067" citStr="Huang and Chiang (2005)" startWordPosition="4605" endWordPosition="4608">increasingly loose heuristics. We follow earlier work (Pauls and Klein, 2009) in using number of edges pushed as the primary, hardware-invariant metric for evaluating performance of our algorithms.5 While KA* scales roughly linearly with the looseness of the heuristic, NAIVE degrades very quickly as the heuristics get worse. For heuristics given by grammars weaker than the 4-split grammar, NAIVE ran out of memory. Since the bottom-up pass of k-best parsing is the bottleneck, we also examine the time spent in the 1-best phase of k-best parsing. As a baseline, we compared KA* to the approach of Huang and Chiang (2005), which we will call EXH (see below for more explanation) since it requires exhaustive parsing in the bottom-up pass. We performed the exhaustive parsing needed for EXH in our agenda-based parser to facilitate comparison. For KA*, we included the cost of computing the heuristic, which was done by running our agenda-based parser exhaustively on a smaller grammar to compute outside items; we chose the 5We found that edges pushed was generally well correlated with parsing time. 0 2000 4000 6000 8000 10000 k Figure 4: The performance of KA* for lexicalized grammars. The performance is dominated by</context>
<context position="30050" citStr="Huang and Chiang (2005)" startWordPosition="5270" endWordPosition="5273">LTER heuristic suggested in Klein and Manning (2003c). We projected the source projection to a smaller grammar by collapsing all non-terminal symbols to X, and 6http://nlp.stanford.edu/software/ also collapsing pre-terminals into related clusters. For example, we collapsed the tags NN, NNS, NNP, and NNPS to N. This projection reduced the number of grammar symbols from 149 to 36. Using it as a heuristic for the full grammar suppressed — 60% of the total items (Figure 5). 4 Related Work While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005). In particular, if all inside items are processed before any derivation items, the subsequent number of derivation items and outside items popped by KA* is nearly identical to the number popped by EXH in our experiments (both algorithms have the same ordering bounds on which derivation items are popped). The only real difference between the algorithms in this limited case is that EXH places k-best items on local priority queues per edge, while KA* makes use of one global queue. Thus, in addition to providing a method for speeding up k-best extraction with A*, our algorithm also provides an al</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the International Workshop on Parsing Technologies (IWPT), pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<date>2005</date>
<note>Unpublished manuscript. http://www.cis.upenn.edu/˜lhuang3/ knuth.pdf.</note>
<contexts>
<context position="13932" citStr="Huang (2005)" startWordPosition="2471" endWordPosition="2472">ms d with δ(d) + h(d) c δ(gk) where gk is the kth-best root parse and δ(·) is the inside score of a derivation item (analogous to β for edges).2 Even for reasonable heuristics, this 2The new symbol emphasizes that S scores a specific derivation rather than a minimum over a set of derivations. number can be very large; see Section 3 for empirical results. This naive algorithm is, of course, not novel, either in general approach or specific computation. Early k-best parsers functioned by abandoning dynamic programming and performing beam search on derivations (Ratnaparkhi, 1999; Collins, 2000). Huang (2005) proposes an extension of Knuth’s algorithm (Knuth, 1977) to produce k-best lists by searching in the space of derivations, which is essentially this algorithm. While Huang (2005) makes no explicit mention of a heuristic, it would be easy to incorporate one into their formulation. 2.4 A New k-Best A* Parser While NAIVE suffers severe performance degradation for loose heuristics, it is in fact very efficient if h(·) is “perfect,” i.e. h(e) = α(e) be. In this case, the ordering property of A* guarantees that only inside derivation items d satisfying δ(d) + α(d) c δ(gk) will be placed in the char</context>
</contexts>
<marker>Huang, 2005</marker>
<rawString>Liang Huang. 2005. Unpublished manuscript. http://www.cis.upenn.edu/˜lhuang3/ knuth.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V´ıctor M Jim´enez</author>
<author>Andr´es Marzal</author>
</authors>
<title>Computation of the n best parse trees for weighted and stochastic context-free grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint IAPR International Workshops on Advances in Pattern Recognition,</booktitle>
<pages>183--192</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<marker>Jim´enez, Marzal, 2000</marker>
<rawString>V´ıctor M. Jim´enez and Andr´es Marzal. 2000. Computation of the n best parse trees for weighted and stochastic context-free grammars. In Proceedings of the Joint IAPR International Workshops on Advances in Pattern Recognition, pages 183–192, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In IWPT,</booktitle>
<pages>123--134</pages>
<contexts>
<context position="5984" citStr="Klein and Manning (2001)" startWordPosition="1025" endWordPosition="1028">on items I(A, i, j) which represent in a collapsed way the possible inside derivations of edges (A, i, j). We call these items inside edge items or simply inside items where clear; a graphical representation of an inside item can be seen in Figure 1(a). The space whose items are inside edges is called the edge space. These inside items are combined using the single IN deduction schema shown in Table 1. This schema is instantiated for every grammar rule r 1While we present the algorithm specialized to parsing with a PCFG, it generalizes to a wide range of hypergraph search problems as shown in Klein and Manning (2001). Figure 1: Representations of the different types of items used in parsing. (a) An inside edge item: I(VP, 2, 5). (b) An outside edge item: O(VP, 2, 5). (c) An inside derivation item: D(TVP, 2, 5) for a tree TVP. (d) A ranked derivation item: K(VP, 2, 5, 6). (e) A modified inside derivation item (with backpointers to ranked items): D(VP, 2, 5, 3, VP → VBZ NP, 1, 4). in G. For IN, the function g(·) simply sums the weights of the antecedent items and the grammar rule r, while the priority function p(·) adds a heuristic to this sum. The heuristic is a bound on the Viterbi outside score a(e) of a</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In IWPT, pages 123–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language processing,.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1709" citStr="Klein and Manning, 2002" startWordPosition="261" endWordPosition="265">efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A* algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted d</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Chris Manning. 2002. Fast exact inference with a factored model for natural language processing,. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="1734" citStr="Klein and Manning, 2003" startWordPosition="266" endWordPosition="269"> for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A* algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al.,</context>
<context position="4348" citStr="Klein and Manning (2003" startWordPosition="719" endWordPosition="722">ed” within a generic agenda-driven algorithm, which constructs items in a prioritized fashion. The algorithm maintains an agenda (a priority queue of unprocessed items), as well as a chart of items already processed. The fundamental operation of the algorithm is to pop the highest priority item 0 from the agenda, put it into the chart with its current weight, and form using deduction rules any items which can be built by combining 0 with items already in the chart. If new or improved, resulting items are put on the agenda with priority given by p(·). 2.2 A* Parsing The A* parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). We do so here both to introduce notation and to build to our final algorithm. First, we must formalize some notation. Assume we have a PCFG1 G and an input sentence s1 ... s,,, of length n. The grammar G has a set of symbols E, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r in G has the form r = A → B C with weight wr (the negative log-probability of the rule). Edges are labeled spans e = (A, i, j). Inside derivat</context>
<context position="6621" citStr="Klein and Manning (2003" startWordPosition="1148" endWordPosition="1151">presentations of the different types of items used in parsing. (a) An inside edge item: I(VP, 2, 5). (b) An outside edge item: O(VP, 2, 5). (c) An inside derivation item: D(TVP, 2, 5) for a tree TVP. (d) A ranked derivation item: K(VP, 2, 5, 6). (e) A modified inside derivation item (with backpointers to ranked items): D(VP, 2, 5, 3, VP → VBZ NP, 1, 4). in G. For IN, the function g(·) simply sums the weights of the antecedent items and the grammar rule r, while the priority function p(·) adds a heuristic to this sum. The heuristic is a bound on the Viterbi outside score a(e) of an edge e; see Klein and Manning (2003c) for details. A good heuristic allows A* to reach the goal item I(G, 0, n) while constructing few inside items. If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). In particular, this guarantee implies that the goal item I(G, 0, n) will be popped with the score of the 1-best parse of the sentence. Consistency also implies that items are popped off the agenda in increasing order of bounded Viterbi scores: O(e) + h(e) We will refer to this monotonicity as the ordering prope</context>
<context position="27468" citStr="Klein and Manning (2003" startWordPosition="4833" endWordPosition="4836">deoff of heuristic and bottom-up parsing time. We separated the items enqueued into items enqueued while computing the heuristic (not strictly part of the algorithm), inside items (“bottom-up”), and derivation and outside items (together “k-best”). The results are shown in Figure 3. The cost of k-best extraction is clearly dwarfed by the the 1-best computation in both cases. However, KA* is significantly faster over the bottom-up computations, even when the cost of computing the heuristic is included. 3.2 Lexicalized Parsing We also experimented with the lexicalized parsing model described in Klein and Manning (2003b). This model is constructed as the product of a dependency model and the unlexicalized PCFG model in Klein and Manning (2003a). We KA* Items pushed (millions) 0 200 600 1000 K Best Bottom-up Heuristic 964 Items pushed (millions) 0 500 1500 2500 EXH KA* Items pushed (millions) 0 500 1500 2500 K Best Bottom-up K Best Bottom-up Heuristic 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 k k Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA* and EXH. constructed these grammars using the Stanford Parser.6 The model was trained on Sections 2-20 of the P</context>
<context position="29478" citStr="Klein and Manning (2003" startWordPosition="5177" endWordPosition="5180">examine the cost of k-best parsing in the source side of such grammars with KA*, which can be a first step in translation. We extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al. (2006), extracting rules with at most 3 non-terminals. These rules are highly lexicalized. About 300K rules are applicable for a typical 30-word sentence; we filter the rest. We tested on 100 sentences of length at most 40 from the NIST05 Arabic-English test set. We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). We projected the source projection to a smaller grammar by collapsing all non-terminal symbols to X, and 6http://nlp.stanford.edu/software/ also collapsing pre-terminals into related clusters. For example, we collapsed the tags NN, NNS, NNP, and NNPS to N. This projection reduced the number of grammar symbols from 149 to 36. Using it as a heuristic for the full grammar suppressed — 60% of the total items (Figure 5). 4 Related Work While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005). In particular, if all insi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003a. Accurate unlexicalized parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Factored A* search for models over sequences and trees.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="1734" citStr="Klein and Manning, 2003" startWordPosition="266" endWordPosition="269"> for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A* algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al.,</context>
<context position="4348" citStr="Klein and Manning (2003" startWordPosition="719" endWordPosition="722">ed” within a generic agenda-driven algorithm, which constructs items in a prioritized fashion. The algorithm maintains an agenda (a priority queue of unprocessed items), as well as a chart of items already processed. The fundamental operation of the algorithm is to pop the highest priority item 0 from the agenda, put it into the chart with its current weight, and form using deduction rules any items which can be built by combining 0 with items already in the chart. If new or improved, resulting items are put on the agenda with priority given by p(·). 2.2 A* Parsing The A* parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). We do so here both to introduce notation and to build to our final algorithm. First, we must formalize some notation. Assume we have a PCFG1 G and an input sentence s1 ... s,,, of length n. The grammar G has a set of symbols E, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r in G has the form r = A → B C with weight wr (the negative log-probability of the rule). Edges are labeled spans e = (A, i, j). Inside derivat</context>
<context position="6621" citStr="Klein and Manning (2003" startWordPosition="1148" endWordPosition="1151">presentations of the different types of items used in parsing. (a) An inside edge item: I(VP, 2, 5). (b) An outside edge item: O(VP, 2, 5). (c) An inside derivation item: D(TVP, 2, 5) for a tree TVP. (d) A ranked derivation item: K(VP, 2, 5, 6). (e) A modified inside derivation item (with backpointers to ranked items): D(VP, 2, 5, 3, VP → VBZ NP, 1, 4). in G. For IN, the function g(·) simply sums the weights of the antecedent items and the grammar rule r, while the priority function p(·) adds a heuristic to this sum. The heuristic is a bound on the Viterbi outside score a(e) of an edge e; see Klein and Manning (2003c) for details. A good heuristic allows A* to reach the goal item I(G, 0, n) while constructing few inside items. If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). In particular, this guarantee implies that the goal item I(G, 0, n) will be popped with the score of the 1-best parse of the sentence. Consistency also implies that items are popped off the agenda in increasing order of bounded Viterbi scores: O(e) + h(e) We will refer to this monotonicity as the ordering prope</context>
<context position="27468" citStr="Klein and Manning (2003" startWordPosition="4833" endWordPosition="4836">deoff of heuristic and bottom-up parsing time. We separated the items enqueued into items enqueued while computing the heuristic (not strictly part of the algorithm), inside items (“bottom-up”), and derivation and outside items (together “k-best”). The results are shown in Figure 3. The cost of k-best extraction is clearly dwarfed by the the 1-best computation in both cases. However, KA* is significantly faster over the bottom-up computations, even when the cost of computing the heuristic is included. 3.2 Lexicalized Parsing We also experimented with the lexicalized parsing model described in Klein and Manning (2003b). This model is constructed as the product of a dependency model and the unlexicalized PCFG model in Klein and Manning (2003a). We KA* Items pushed (millions) 0 200 600 1000 K Best Bottom-up Heuristic 964 Items pushed (millions) 0 500 1500 2500 EXH KA* Items pushed (millions) 0 500 1500 2500 K Best Bottom-up K Best Bottom-up Heuristic 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 k k Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA* and EXH. constructed these grammars using the Stanford Parser.6 The model was trained on Sections 2-20 of the P</context>
<context position="29478" citStr="Klein and Manning (2003" startWordPosition="5177" endWordPosition="5180">examine the cost of k-best parsing in the source side of such grammars with KA*, which can be a first step in translation. We extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al. (2006), extracting rules with at most 3 non-terminals. These rules are highly lexicalized. About 300K rules are applicable for a typical 30-word sentence; we filter the rest. We tested on 100 sentences of length at most 40 from the NIST05 Arabic-English test set. We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). We projected the source projection to a smaller grammar by collapsing all non-terminal symbols to X, and 6http://nlp.stanford.edu/software/ also collapsing pre-terminals into related clusters. For example, we collapsed the tags NN, NNS, NNP, and NNPS to N. This projection reduced the number of grammar symbols from 149 to 36. Using it as a heuristic for the full grammar suppressed — 60% of the total items (Figure 5). 4 Related Work While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005). In particular, if all insi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003b. Factored A* search for models over sequences and trees. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection. In</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>119--126</pages>
<contexts>
<context position="1734" citStr="Klein and Manning, 2003" startWordPosition="266" endWordPosition="269"> for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in several domains (Klein and Manning, 2002; Klein and Manning, 2003c; Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A* algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al.,</context>
<context position="4348" citStr="Klein and Manning (2003" startWordPosition="719" endWordPosition="722">ed” within a generic agenda-driven algorithm, which constructs items in a prioritized fashion. The algorithm maintains an agenda (a priority queue of unprocessed items), as well as a chart of items already processed. The fundamental operation of the algorithm is to pop the highest priority item 0 from the agenda, put it into the chart with its current weight, and form using deduction rules any items which can be built by combining 0 with items already in the chart. If new or improved, resulting items are put on the agenda with priority given by p(·). 2.2 A* Parsing The A* parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). We do so here both to introduce notation and to build to our final algorithm. First, we must formalize some notation. Assume we have a PCFG1 G and an input sentence s1 ... s,,, of length n. The grammar G has a set of symbols E, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r in G has the form r = A → B C with weight wr (the negative log-probability of the rule). Edges are labeled spans e = (A, i, j). Inside derivat</context>
<context position="6621" citStr="Klein and Manning (2003" startWordPosition="1148" endWordPosition="1151">presentations of the different types of items used in parsing. (a) An inside edge item: I(VP, 2, 5). (b) An outside edge item: O(VP, 2, 5). (c) An inside derivation item: D(TVP, 2, 5) for a tree TVP. (d) A ranked derivation item: K(VP, 2, 5, 6). (e) A modified inside derivation item (with backpointers to ranked items): D(VP, 2, 5, 3, VP → VBZ NP, 1, 4). in G. For IN, the function g(·) simply sums the weights of the antecedent items and the grammar rule r, while the priority function p(·) adds a heuristic to this sum. The heuristic is a bound on the Viterbi outside score a(e) of an edge e; see Klein and Manning (2003c) for details. A good heuristic allows A* to reach the goal item I(G, 0, n) while constructing few inside items. If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). In particular, this guarantee implies that the goal item I(G, 0, n) will be popped with the score of the 1-best parse of the sentence. Consistency also implies that items are popped off the agenda in increasing order of bounded Viterbi scores: O(e) + h(e) We will refer to this monotonicity as the ordering prope</context>
<context position="27468" citStr="Klein and Manning (2003" startWordPosition="4833" endWordPosition="4836">deoff of heuristic and bottom-up parsing time. We separated the items enqueued into items enqueued while computing the heuristic (not strictly part of the algorithm), inside items (“bottom-up”), and derivation and outside items (together “k-best”). The results are shown in Figure 3. The cost of k-best extraction is clearly dwarfed by the the 1-best computation in both cases. However, KA* is significantly faster over the bottom-up computations, even when the cost of computing the heuristic is included. 3.2 Lexicalized Parsing We also experimented with the lexicalized parsing model described in Klein and Manning (2003b). This model is constructed as the product of a dependency model and the unlexicalized PCFG model in Klein and Manning (2003a). We KA* Items pushed (millions) 0 200 600 1000 K Best Bottom-up Heuristic 964 Items pushed (millions) 0 500 1500 2500 EXH KA* Items pushed (millions) 0 500 1500 2500 K Best Bottom-up K Best Bottom-up Heuristic 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 k k Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA* and EXH. constructed these grammars using the Stanford Parser.6 The model was trained on Sections 2-20 of the P</context>
<context position="29478" citStr="Klein and Manning (2003" startWordPosition="5177" endWordPosition="5180">examine the cost of k-best parsing in the source side of such grammars with KA*, which can be a first step in translation. We extracted a grammar from 220 million words of Arabic-English bitext using the approach of Galley et al. (2006), extracting rules with at most 3 non-terminals. These rules are highly lexicalized. About 300K rules are applicable for a typical 30-word sentence; we filter the rest. We tested on 100 sentences of length at most 40 from the NIST05 Arabic-English test set. We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). We projected the source projection to a smaller grammar by collapsing all non-terminal symbols to X, and 6http://nlp.stanford.edu/software/ also collapsing pre-terminals into related clusters. For example, we collapsed the tags NN, NNS, NNP, and NNPS to N. This projection reduced the number of grammar symbols from 149 to 36. Using it as a heuristic for the full grammar suppressed — 60% of the total items (Figure 5). 4 Related Work While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005). In particular, if all insi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003c. A* parsing: Fast exact Viterbi parse selection. In In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL), pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Knuth</author>
</authors>
<title>A generalization of Dijkstra’s algorithm.</title>
<date>1977</date>
<journal>Information Processing Letters,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>5</pages>
<contexts>
<context position="13989" citStr="Knuth, 1977" startWordPosition="2479" endWordPosition="2480">ot parse and δ(·) is the inside score of a derivation item (analogous to β for edges).2 Even for reasonable heuristics, this 2The new symbol emphasizes that S scores a specific derivation rather than a minimum over a set of derivations. number can be very large; see Section 3 for empirical results. This naive algorithm is, of course, not novel, either in general approach or specific computation. Early k-best parsers functioned by abandoning dynamic programming and performing beam search on derivations (Ratnaparkhi, 1999; Collins, 2000). Huang (2005) proposes an extension of Knuth’s algorithm (Knuth, 1977) to produce k-best lists by searching in the space of derivations, which is essentially this algorithm. While Huang (2005) makes no explicit mention of a heuristic, it would be easy to incorporate one into their formulation. 2.4 A New k-Best A* Parser While NAIVE suffers severe performance degradation for loose heuristics, it is in fact very efficient if h(·) is “perfect,” i.e. h(e) = α(e) be. In this case, the ordering property of A* guarantees that only inside derivation items d satisfying δ(d) + α(d) c δ(gk) will be placed in the chart. The set of derivation items d satisfying this inequali</context>
</contexts>
<marker>Knuth, 1977</marker>
<rawString>Donald Knuth. 1977. A generalization of Dijkstra’s algorithm. Information Processing Letters, 6(1):1– 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="941" citStr="Kumar and Byrne, 2004" startWordPosition="142" endWordPosition="145"> exhaustive 1-best pass. We present a unified algorithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts o</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="24148" citStr="Marcus et al., 1993" startWordPosition="4281" endWordPosition="4284"> This triggering is similar to the “lazy frontier” used by Huang and Chiang (2005). All of our experiments use this lazy representation. 3 Experiments 3.1 State-Split Grammars We performed our first experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting. We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars. We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 100 sentences of length at most 30 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. 4http://berkeleyparser.googlecode.com 1 5 50 500 NAIVE KA* 963 Items pushed (millions) 0 5000 15000 25000 EXH KA* Items pushed (millions) 0 5000 15000 25000 K Best Bottom-up K Best Bottom-up Heuristic 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 k k Figure 3: The cost of k-best extraction as a function of k for state-split grammars, for both KA* and EXH. The amount of </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>152--159</pages>
<contexts>
<context position="1075" citStr="McClosky et al., 2006" startWordPosition="161" endWordPosition="164">e giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applied successfully in severa</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Computationl Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2371" citStr="Nederhof, 2003" startWordPosition="382" endWordPosition="383">07). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A* algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. 2 A k-Best A* Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A* parsing and making incremental modifications. 2.1 Parsin</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Computationl Linguistics, 29(1):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1051" citStr="Och, 2003" startWordPosition="159" endWordPosition="160">action while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types. 1 Introduction Many situations call for a parser to return the kbest parses rather than only the 1-best. Uses for k-best lists include minimum Bayes risk decoding (Goodman, 1998; Kumar and Byrne, 2004), discriminative reranking (Collins, 2000; Charniak and Johnson, 2005), and discriminative training (Och, 2003; McClosky et al., 2006). The most efficient known algorithm for k-best parsing (Jim´enez and Marzal, 2000; Huang and Chiang, 2005) performs an initial bottom-up dynamic programming pass before extracting the k-best parses. In that algorithm, the initial pass is, by far, the bottleneck (Huang and Chiang, 2005). In this paper, we propose an extension of A* parsing which integrates k-best search with an A*- based exploration of the 1-best chart. A* parsing can avoid significant amounts of computation by guiding 1-best search with heuristic estimates of parse completion costs, and has been applie</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL), pages 160–167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Hierarchical search for parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="23936" citStr="Pauls and Klein (2009)" startWordPosition="4246" endWordPosition="4249"> computed priority than D(A, i, j, l, r, u − 1,v) and D(A, i, j, l, r, u, v − 1) (Jim´enez and Marzal, 2000). So, we can wait until one of the latter two is built before “triggering” the construction of the former. This triggering is similar to the “lazy frontier” used by Huang and Chiang (2005). All of our experiments use this lazy representation. 3 Experiments 3.1 State-Split Grammars We performed our first experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting. We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars. We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 100 sentences of length at most 30 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. 4http://berkeleyparser.googlecode.com 1 5 50 500 NAIVE KA* 963 Items pushed (millions) 0 5000 15000 25000 EXH KA* Items pushed (millions) 0 5000 15000 25000 K Best Bottom</context>
<context position="25521" citStr="Pauls and Klein, 2009" startWordPosition="4513" endWordPosition="4516">ively smaller grammars in the hierarchy form successively looser bounds on the outside scores. This allows us to examine the performance as a function of the tightness of the heuristic. We first compared our algorithm KA* against the NAIVE algorithm. We extracted 1000-best lists using each algorithm, with heuristics computed using each of the 6 smaller grammars. In Figure 2, we evaluate only the k-best extraction phase by plotting the number of derivation items and outside items added to the agenda as a function of the heuristic used, for increasingly loose heuristics. We follow earlier work (Pauls and Klein, 2009) in using number of edges pushed as the primary, hardware-invariant metric for evaluating performance of our algorithms.5 While KA* scales roughly linearly with the looseness of the heuristic, NAIVE degrades very quickly as the heuristics get worse. For heuristics given by grammars weaker than the 4-split grammar, NAIVE ran out of memory. Since the bottom-up pass of k-best parsing is the bottleneck, we also examine the time spent in the 1-best phase of k-best parsing. As a baseline, we compared KA* to the approach of Huang and Chiang (2005), which we will call EXH (see below for more explanati</context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>Adam Pauls and Dan Klein. 2009. Hierarchical search for parsing. In Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<contexts>
<context position="2609" citStr="Petrov et al. (2006)" startWordPosition="418" endWordPosition="421">k done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. 2 A k-Best A* Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A* parsing and making incremental modifications. 2.1 Parsing as Weighted Deduction Our algorithm can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003; Felzenszwalb and McAllester, 2007). A prioritized weighted deduction rule has the form Awl,...</context>
<context position="23780" citStr="Petrov et al. (2006)" startWordPosition="4223" endWordPosition="4226"> Fortunately, little looping is actually necessary: there is a partial ordering of derivation items, namely, that D(A, i, j, l, r, u, v) will have a lower computed priority than D(A, i, j, l, r, u − 1,v) and D(A, i, j, l, r, u, v − 1) (Jim´enez and Marzal, 2000). So, we can wait until one of the latter two is built before “triggering” the construction of the former. This triggering is similar to the “lazy frontier” used by Huang and Chiang (2005). All of our experiments use this lazy representation. 3 Experiments 3.1 State-Split Grammars We performed our first experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through statesplitting. We followed Pauls and Klein (2009) in computing heuristics for the most refined grammar from outside scores for less-split grammars. We used the Berkeley Parser4 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 100 sentences of length at most 30 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. 4http://berkel</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<volume>34</volume>
<pages>151--5175</pages>
<contexts>
<context position="13902" citStr="Ratnaparkhi, 1999" startWordPosition="2467" endWordPosition="2468">IVE will process all derivation items d with δ(d) + h(d) c δ(gk) where gk is the kth-best root parse and δ(·) is the inside score of a derivation item (analogous to β for edges).2 Even for reasonable heuristics, this 2The new symbol emphasizes that S scores a specific derivation rather than a minimum over a set of derivations. number can be very large; see Section 3 for empirical results. This naive algorithm is, of course, not novel, either in general approach or specific computation. Early k-best parsers functioned by abandoning dynamic programming and performing beam search on derivations (Ratnaparkhi, 1999; Collins, 2000). Huang (2005) proposes an extension of Knuth’s algorithm (Knuth, 1977) to produce k-best lists by searching in the space of derivations, which is essentially this algorithm. While Huang (2005) makes no explicit mention of a heuristic, it would be easy to incorporate one into their formulation. 2.4 A New k-Best A* Parser While NAIVE suffers severe performance degradation for loose heuristics, it is in fact very efficient if h(·) is “perfect,” i.e. h(e) = α(e) be. In this case, the ordering property of A* guarantees that only inside derivation items d satisfying δ(d) + α(d) c δ(</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. In Machine Learning, volume 34, pages 151–5175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<contexts>
<context position="2339" citStr="Shieber et al., 1995" startWordPosition="376" endWordPosition="379">d Manning, 2003c; Haghighi et al., 2007). Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a standard A* algorithm. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). Our algorithm is also equivalent to standard A* parsing (up to ties) if it is terminated after the 1-best derivation is found. Finally, our algorithm can be written down in terms of deduction rules, and thus falls into the well-understood view of parsing as weighted deduction (Shieber et al., 1995; Goodman, 1998; Nederhof, 2003). In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006). We demonstrate that optimal k-best lists can be extracted significantly faster using our algorithm than with previous methods. 2 A k-Best A* Parsing Algorithm We build up to our full algorithm in several stages, beginning with standard 1-best A* parsing and making incre</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3–36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>