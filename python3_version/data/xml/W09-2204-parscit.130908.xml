<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000282">
<title confidence="0.967123">
Is Unlabeled Data Suitable for Multiclass SVM-based Web Page
Classification?
</title>
<author confidence="0.514431">
Arkaitz Zubiaga, Victor Fresno, Raquel Martinez
</author>
<affiliation confidence="0.295986">
NLP &amp; IR Group at UNED
</affiliation>
<address confidence="0.5016375">
Lenguajes y Sistemas Inform´aticos
E.T.S.I. Inform´atica, UNED
</address>
<email confidence="0.968555">
{azubiaga, vfresno, raquel}@lsi.uned.es
</email>
<sectionHeader confidence="0.994525" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.957563095238095">
Support Vector Machines present an interest-
ing and effective approach to solve automated
classification tasks. Although it only han-
dles binary and supervised problems by na-
ture, it has been transformed into multiclass
and semi-supervised approaches in several
works. A previous study on supervised and
semi-supervised SVM classification over bi-
nary taxonomies showed how the latter clearly
outperforms the former, proving the suitability
of unlabeled data for the learning phase in this
kind of tasks. However, the suitability of un-
labeled data for multiclass tasks using SVM
has never been tested before. In this work,
we present a study on whether unlabeled data
could improve results for multiclass web page
classification tasks using Support Vector Ma-
chines. As a conclusion, we encourage to rely
only on labeled data, both for improving (or at
least equaling) performance and for reducing
the computational cost.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981866666667">
The amount of web documents is increasing in a
very fast way in the last years, what makes more
and more complicated its organization. For this rea-
son, web page classification has gained importance
as a task to ease and improve information access.
Web page classification can be defined as the task
of labeling and organizing web documents within a
set of predefined categories. In this work, we focus
on web page classification based on Support Vec-
tor Machines (SVM, (Joachims, 1998)). This kind
of classification tasks rely on a previously labeled
training set of documents, with which the classi-
fier acquires the required ability to classify new un-
known documents.
Different settings can be distinguished for web
page classification problems. On the one hand, at-
tending to the learning technique the system bases
on, it may be supervised, with all the training docu-
ments previously labeled, or semi-supervised, where
unlabeled documents are also taken into account
during the learning phase. On the other hand, attend-
ing to the number of classes, the classification may
be binary, where only two possible categories can
be assigned to each document, or multiclass, where
three or more categories can be set. The former is
commonly used for filtering systems, whereas the
latter is necessary for bigger taxonomies, e.g. topi-
cal classification.
Although multiple studies have been made for
text classification, its application to the web page
classification area remains without enough attention
(Qi and Davison, 2007). Analyzing the nature of
a web page classification task, we can consider it
to be, generally, multiclass problems, where it is
usual to find numerous classes. In the same way,
if we take into account that the number of available
labeled documents is tiny compared to the size of
the Web, this task becomes semi-supervised besides
multiclass.
However, the original SVM algorithm supports
neither semi-supervised learning nor multiclass tax-
onomies, due to its dichotomic and supervised na-
ture. To solve this issue, different studies for
both multiclass SVM and semi-supervised SVM ap-
proaches have been proposed, but a little effort has
</bodyText>
<page confidence="0.983123">
28
</page>
<note confidence="0.992173">
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 28–36,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999522783783784">
been invested in the combination of them.
(Joachims, 1999) compares supervised and semi-
supervised approaches for binary tasks using SVM.
It shows encouraging results for the transductive
semi-supervised approach, clearly improving the su-
pervised, and so he proved unlabeled data to be
suitable to optimize binary SVM classifiers’ results.
On the other hand, the few works presented for
semi-supervised multiclass SVM classification do
not provide clear information on whether the unla-
beled data improves the classification results in com-
parison with the only use of labeled data.
In this work, we performed an experiment among
different SVM-based multiclass approaches, both
supervised and semi-supervised. The experiments
were focused on web page classification, and
were carried out over three benchmark datasets:
BankSearch, WebKB and Yahoo! Science. Using
the results of the comparison, we analyze and study
the suitability of unlabeled data for multiclass SVM
classification tasks. We discuss these results and
evaluate whether it is worthy to rely on a semi-
supervised SVM approach to conduct this kind of
tasks.
The remainder of this document is organized as
follows. Next, in section 2, we briefly explain how
SVM classifiers work for binary classifications, both
for a supervised and a semi-supervised view. In sec-
tion 3, we continue with the adaptation of SVM to
multiclass environments, and show what has been
done in the literature. Section 4 presents the details
of the experiments carried out in this work, aim at
evaluating the suitability of unlabeled data for mul-
ticlass SVM classification. In section 5 we show and
discuss the results of the experiments. Finally, in
section 6, we conclude with our thoughts and future
work.
</bodyText>
<sectionHeader confidence="0.968691" genericHeader="method">
2 Binary SVM
</sectionHeader>
<bodyText confidence="0.997660916666667">
In the last decade, SVM has become one of the most
studied techniques for text classification, due to the
positive results it has shown. This technique uses the
vector space model for the documents’ representa-
tion, and assumes that documents in the same class
should fall into separable spaces of the representa-
tion. Upon this, it looks for a hyperplane that sepa-
rates the classes; therefore, this hyperplane should
maximize the distance between it and the nearest
documents, what is called the margin. The following
function is used to define the hyperplane (see Figure
1):
</bodyText>
<equation confidence="0.941823">
f(x) = w · x + b
</equation>
<figureCaption confidence="0.9930055">
Figure 1: An example of binary SVM classification, sep-
arating two classes (black dots from white dots)
</figureCaption>
<bodyText confidence="0.99953180952381">
In order to resolve this function, all the possible
values should be considered and, after that, the val-
ues of w and b that maximize the margin should be
selected. This would be computationally expensive,
so the following equivalent function is used to relax
it (Boser et al. , 1992) (Cortes and Vapnik, 1995):
where C is the penalty parameter, i is an stack
variable for the ith document, and l is the number of
labeled documents.
This function can only resolve linearly separable
problems, thus the use of a kernel function is com-
monly required for the redimension of the space; in
this manner, the new space will be linearly separa-
ble. After that, the redimension is undone, so the
found hyperplane will be transformed to the original
space, respecting the classification function. Best-
known kernel functions include linear, polynomial,
radial basis function (RBF) and sigmoid, among oth-
ers. Different kernel functions’ performance has
been studied in (Sch¨olkopf and Smola, 1999) and
(Kivinen et al., 2002).
</bodyText>
<equation confidence="0.9963142">
I
Subject to: yi(w · xi + b) &gt; 1 − i, i &gt; 0
rr i
minl1llw||2 + CSC,
LL i=�
</equation>
<page confidence="0.993117">
29
</page>
<bodyText confidence="0.99989375">
Note that the function above can only resolve bi-
nary and supervised problems, so different variants
are necessary to perform semi-supervised or multi-
class tasks.
</bodyText>
<subsectionHeader confidence="0.974105">
2.1 Semi-supervised Learning for SVM
</subsectionHeader>
<bodyText confidence="0.985194">
(S3VM)
Semi-supervised learning approaches differ in the
learning phase. As opposed to supervised ap-
proaches, unlabeled data is used during the learn-
ing phase, and so classifier’s predictions over them
is also included as labeled data to learn. The fact of
taking into account unlabeled data to learn can im-
prove the classification done by supervised methods,
specially when its predictions provide new useful in-
formation, as shown in figure 2. However, the noise
added by erroneus predictions can make worse the
learning phase and, therefore, its final performance.
This makes interesting the study on whether relying
on semi-supervised approaches is suitable for each
kind of task.
Semi-supervised learning for SVM, also known
as S3VM, was first introduced by (Joachims, 1999)
in a transductive way, by modifying the original
SVM function. To do that, he proposed to add an
additional term to the optimization function:
</bodyText>
<equation confidence="0.9937195">
2 · ||w||2 + C ·
1
</equation>
<bodyText confidence="0.999884294117647">
where u is the number of unlabeled data.
Nevertheless, the adaptation of SVM to semi-
supervised learning significantly increases its com-
putational cost, due to the non-convex nature of the
resulting function, and so obtaining the minimum
value is even more complicated. In order to relax
the function, convex optimization techniques such
as semi-definite programming are commonly used
(Xu et al. , 2007), where minimizing the function
gets much easier.
By means of this approach, (Joachims, 1999)
demonstrated a large performance gap between the
original supervised SVM and his semi-supervised
proposal, in favour of the latter one. He showed
that for binary classification tasks, the smaller is
the training set size, the larger gets the difference
among these two approaches. Although he worked
</bodyText>
<figureCaption confidence="0.987917">
Figure 2: SVM vs S3VM, where white balls are unla-
beled documents
</figureCaption>
<bodyText confidence="0.9999555">
with multiclass datasets, he splitted the problems
into smaller binary ones, and so he did not demon-
strate whether the same performance gap occurs for
multiclass classification. This paper tries to cover
this issue. (Chapelle et al., 2008) present a compre-
hensive study on S3VM approaches.
</bodyText>
<sectionHeader confidence="0.9852" genericHeader="method">
3 Multiclass SVM
</sectionHeader>
<bodyText confidence="0.999659875">
Due to the dichotomic nature of SVM, it came up
the need to implement new methods to solve multi-
class problems, where more than two classes must
be considered. Different approaches have been pro-
posed to achieve this. On the one hand, as a direct
approach, (Weston, 1999) proposed modifying the
optimization function getting into account all the k
classes at once:
</bodyText>
<equation confidence="0.79137575">
⎡
min ⎣
Subject to:
wyi · xi + byi ≥ wm · xi + bm + 2 − �m i , mi ≥ 0
</equation>
<bodyText confidence="0.999868">
On the other hand, the original binary SVM clas-
sifier has usually been combined to obtain a multi-
class solution. As combinations of binary SVM clas-
sifiers, two different approaches to k-class classifiers
can be emphasized (Hsu and Lin, 2002):
</bodyText>
<listItem confidence="0.7412316">
• one-against-all constructs k classifiers defining
that many hyperplanes; each of them separates
the class i from the rest k-1. For instance, for
a problem with 4 classes, 1 vs 2-3-4, 2 vs 1-3-
4, 3 vs 1-2-4 and 4 vs 1-2-3 classifiers would
</listItem>
<equation confidence="0.991988318181818">
⎡
min ⎣
Xl
i=1
�di + C* ·
⎤tt*d
Sj
u
X
j=1
||wm||2 + C
1
2
k
X
m=1
Xl
i=1
X
m7�yi
⎤
tm ⎦i
</equation>
<page confidence="0.967417">
30
</page>
<bodyText confidence="0.9927474">
be created. New documents will be categorized
in the class of the classifier that maximizes the
margin: Ci = argmaxi=1,...,k(wix + bi). As
the number of classes increases, the amount of
classifiers will increase linearly.
</bodyText>
<listItem confidence="0.8873905">
• one-against-one constructs k(k−1)
2 classifiers,
</listItem>
<bodyText confidence="0.999375103448276">
one for each possible category pair. For in-
stance, for a problem with 4 classes, 1 vs 2,
1 vs 3, 1 vs 4, 2 vs 3, 2 vs 4 and 3 vs 4 clas-
sifiers would be created. After that, it classi-
fies each new document by using all the clas-
sifiers, where a vote is added for the winning
class over each classifier; the method will pro-
pose the class with more votes as the result. As
the number of classes increases, the amount of
classifiers will increase in an exponential way,
and so the problem could became very expen-
sive for large taxonomies.
Both (Weston, 1999) and (Hsu and Lin, 2002)
compare the direct multiclass approach to the one-
against-one and one-against-all binary classifier
combining approaches. They agree concluding that
the direct approach does not outperform the results
by one-against-one nor one-against-all, although
it considerably reduces the computational cost be-
cause the number of support vector machines it
constructs is lower. Among the binary combin-
ing approaches, they show the performance of one-
against-one to be superior to one-against-all.
Although these approaches have been widely
used in supervised learning environments, they have
scarcely been applied to semi-supervised learning.
Because of this, we believe the study on its appli-
cability and performance for this type of problems
could be interesting.
</bodyText>
<subsectionHeader confidence="0.99351">
3.1 Multiclass S3VM
</subsectionHeader>
<bodyText confidence="0.999952588235294">
When the taxonomy is defined by more than two
classes and the number of previously labeled doc-
uments is very small, the combination of both mul-
ticlass and semi-supervised approaches could be re-
quired. That is, a multiclass S3VM approach. The
usual web page classification problem meets with
these characteristics, since more than two classes
are usually needed, and the tiny amount of labeled
documents requires the use of unlabeled data for the
learning phase.
Actually, there are a few works focused on trans-
forming SVM into a semi-supervised and multiclass
approach. As a direct approach, a proposal by (Ya-
jima and Kuo, 2006) can be found. They modify the
function for multiclass SVM classification and get it
usable for semi-supervised tasks. The resulting op-
timization function is as follows:
</bodyText>
<equation confidence="0.967105">
QiT K−1Qi
max{0,1 − (Qyj
j − Qij)}2
</equation>
<bodyText confidence="0.999948882352941">
where Q represents the product of a vector of vari-
ables and a kernel matrix defined by the author.
On the other hand, some other works are based on
different approaches to achieve a multiclass S3VM
classifier.
(Qi et al., 2004) use Fuzzy C-Means (FCM) to
predict labels for unlabeled documents. After that,
multiclass SVM is used to learn with the augmented
training set, classifying the test set. (Xu y Schu-
urmans, 2005) rely on a clustering-based approach
to label the unlabeled data. Afterwards, they ap-
ply a multiclass SVM classifier to the fully labeled
training set. (Chapelle et al., 2006) present a direct
multiclass S3VM approach by using the Continua-
tion Method. On the other hand, this is the only
work, to the best of our knowledge, that has tested
the one-against-all and one-against-one approaches
in a semi-supervised environment. They apply these
methods to some news datasets, for which they get
low performance. Additionally, they show that one-
against-one is not sufficient for real-world multi-
class semi-supervised learning, since the unlabeled
data cannot be restricted to the two classes under
consideration.
It is noteworthy that most of the above works
only presented their approaches and compared them
to other semi-supervised classifying methods, such
as Expectation-Maximization (EM) or Naive Bayes.
As an exception, (Chapelle et al., 2006) compared
a semi-supervised and a supervised SVM approach,
but only over image datasets. Against this, we felt
the need to evaluate and compare multiclass SVM
and multiclass S3VM approaches, for the sake of
discovering whether learning with unlabeled web
</bodyText>
<figure confidence="0.6336787">
1
min
2
h
i=1
�l
j=1
+C
E
i=,4yj
</figure>
<page confidence="0.999145">
31
</page>
<bodyText confidence="0.998212">
documents is helpful for multiclass problems when
using SVM as a classifier.
</bodyText>
<sectionHeader confidence="0.989446" genericHeader="method">
4 Multiclass SVM versus Multiclass S3VM
</sectionHeader>
<bodyText confidence="0.999970555555555">
The main goal of this work is to evaluate the real
contribution of unlabeled data for multiclass SVM-
based web page classification tasks. There are a few
works using semi-supervised multiclass SVM clas-
sifiers, but nobody has demonstrated it improves su-
pervised SVM classifier’s performance. Next, we
detail the experiments we carried out to clear up any
doubts and to ensure which is better for multiclass
SVM-based web page classifications.
</bodyText>
<subsectionHeader confidence="0.964346">
4.1 Approaches
</subsectionHeader>
<bodyText confidence="0.986239793103448">
In order to evaluate and compare multiclass SVM
and multiclass S3VM, we decided to use three differ-
ent but equivalent approaches for each view, super-
vised and semi-supervised. For further information
on these approaches, see section 3. We add a suffix,
-SVM or -S3VM, to the names of the approaches, to
differentiate whether they are based in a supervised
or a semi-supervised algorithm.
On the part of the semi-supervised view, the fol-
lowing three approaches were selected:
• 2-steps-SVM: we called 2-steps-SVM to the
technique based on the direct multiclass su-
pervised approach exposed in section 3. This
method works, on its first step, with the train-
ing collection, learning with the labeled docu-
ments and predicting the unlabeled ones; after
that, the latter documents are labeled based on
the generated predictions. On the second step,
now with a fully labeled training set, the usual
supervised classification process is done, learn-
ing with the training documents and predicting
the documents in the test set.
This approach is somehow similar to those pro-
posed by (Qi et al., 2004) and (Xu y Schu-
urmans, 2005). Nonetheless, the 2-steps-SVM
approach uses the same method for both the
first and second steps. A supervised multiclass
SVM is used to increase the labeled set and, af-
ter that, to classify the test set.
</bodyText>
<listItem confidence="0.755304416666667">
• one-against-all-S3VM: the one-against-all ap-
proach has not sufficiently been tested for semi-
supervised environments, and seems interest-
ing to evaluate its performance.
• one-against-one-S3VM: the one-against-one
does not seem to be suitable for semi-
supervised environments, since the classifier is
not able to ignore the inadecuate unlabeled doc-
uments for each 1-vs-1 binary task, as stated by
(Chapelle et al., 2006). Anyway, since it has
scarcely been tested, we also consider this ap-
proach.
</listItem>
<bodyText confidence="0.9936224">
On the other hand, the approaches selected for
the supervised view were these: (1) 1-step-SVM;
(2) one-against-all-SVM, and (3) one-against-one-
SVM.
The three approaches mentioned above are anal-
ogous to the semi-supervised approaches, 2-steps-
SVM, one-against-all-S3VM and one-against-one-
S3VM, respectively. They differ in the learning
phase: unlike the semi-supervised approaches, these
three supervised approaches only rely on the labeled
documents for the learning task, but after that they
classify the same test documents. These approaches
allow to evaluate whether the unlabeled documents
are contributing in a positive or negative way in the
learning phase.
</bodyText>
<subsectionHeader confidence="0.940501">
4.2 Datasets
</subsectionHeader>
<bodyText confidence="0.999001333333333">
For these experiments we have used three web page
benchmark datasets previously used for classifica-
tion tasks:
</bodyText>
<listItem confidence="0.748313692307692">
• BankSearch (Sinka and Corne, 2002), a col-
lection of 11,000 web pages over 11 classes,
with very different topics: commercial banks,
building societies, insurance agencies, java, c,
visual basic, astronomy, biology, soccer, mo-
torsports and sports. We removed the category
sports, since it includes both soccer and motor-
sports in it, as a parent category. This results
10,000 web pages over 10 categories. 4,000 in-
stances were assigned to the training set, while
the other 6,000 were left on the test set.
• WebKB1, with a total of 4,518 documents of
4 universities, and classified into 7 classes
</listItem>
<footnote confidence="0.998348">
1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-
20/www/data/
</footnote>
<page confidence="0.99923">
32
</page>
<bodyText confidence="0.8751693125">
(student, faculty, personal, department, course,
project and other). The class named other was
removed due to its ambiguity, and so we finally
got 6 classes. 2,000 instances fell into the train-
ing set, and 2,518 to the test set.
• Yahoo! Science (Tan, 2002), with 788 scien-
tific documents, classified into 6 classes (agri-
culture, biology, earth science, math, chemistry
and others). We selected 200 documents for the
training set, and 588 for the test set.
Within the training set, for each dataset, multiple
versions were created, modifying the number of la-
beled documents, while the rest were left unlabeled.
Thus, the size of labeled subset within the training
set changes, ranging from 50 web documents to the
whole training set.
</bodyText>
<subsectionHeader confidence="0.998558">
4.3 Document Representation
</subsectionHeader>
<bodyText confidence="0.999984470588235">
SVM requires a vectorial representation of the docu-
ments as an input for the classifier, both for train and
test phases. To obtain this vectorial representation,
we first converted the original html files into plain
text files, removing all the html tags. After that, we
removed the noisy tokens, such as URLs, email ad-
dresses or some stopwords. For these edited docu-
ments, the tf-idf term weighting function was used
to define the values for the uniterms found on the
texts. As the term dimensionality became too large,
we then removed the least-frequent terms by its doc-
ument frequency; terms appearing in less than 0.5%
of the documents were removed for the representa-
tion. The remaining uniterms define the vector space
dimensions. That derived term vectors with 8285 di-
mensions for BankSearch dataset, 3115 for WebKB
and 8437 for Yahoo! Science.
</bodyText>
<subsectionHeader confidence="0.998507">
4.4 Implementation
</subsectionHeader>
<bodyText confidence="0.995647875">
To carry out our experiments, we based on freely
available and already tested and experimented soft-
ware. Different SVM classifiers were needed to im-
plement the methods described in section 4.1.
SVMlight2 was used to work with binary semi-
supervised classifiers for the one-against-all-S3VM
and one-against-one-S3VM approaches. In the same
way, we implemented their supervised versions,
</bodyText>
<footnote confidence="0.928343">
2http://svmlight.joachims.org
</footnote>
<bodyText confidence="0.995679166666667">
one-against-all-SVM and one-against-one-SVM, in
order to evaluate the contribution of unlabeled data.
To achieve the supervised approaches, we ignored
the unlabeled data during the training phase and, af-
ter that, tested with the same test set used for semi-
supervised approaches. The default settings using
a polynomial kernel were selected for the experi-
ments.
SVMmulticlass3 was used to implement the 2-
steps-SVM approach, by using it two times. Firstly,
to train the labeled data and classify unlabeled data.
After that, to train with the whole training set labeled
with classifier’s predictions, and to test with the test
set. In the same way, the 1-step-SVM method was
implemented by ignoring unlabeled data and train-
ing only the labeled data. This method allows to
evaluate the contribution of unlabeled data for the
2-steps-SVM method.
</bodyText>
<subsectionHeader confidence="0.989161">
4.5 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999964722222222">
For the evaluation of the experiments we used the
accuracy to measure the performance, since it has
been frequently used for text classification prob-
lems, specially for multiclass tasks. The accuracy
offers the percent of the correct predictions for the
whole test set. We have considered the same weight
for all the correct guesses for any class. A correct
prediction in any of the classes has the same value,
thus no weighting exists.
On the other hand, an averaged accuracy evalu-
ation is also possible for the binary combining ap-
proaches. An averaged accuracy makes possible to
evaluate the results by each binary classifier, and
provides an averaged value for the whole binary
classifier set. It is worth to note that these values do
not provide any information for the evaluation of the
combined multiclass results, but only for evaluating
each binary classifier before combining them.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999601833333333">
Next, we show and discuss the results of our experi-
ments. It is remarkable that both one-against-one-
SVM and one-against-one-S3VM approaches were
very inferior to the rest, and so we decided not to plot
them in order to maintain graphs’ clarity. Hence,
figures 3, 4 and 5 show the results in accordance
</bodyText>
<footnote confidence="0.995399">
3http://www.cs.cornell.edu/People/tj/svm light/svm multiclass.html
</footnote>
<page confidence="0.999124">
33
</page>
<bodyText confidence="0.99985475">
with the labeled subset size for the 2-steps-SVM, 1-
step-SVM, one-against-all-S3VM and one-against-
all-SVM approaches within our experiments. For
the results to be more representative, nine execu-
tions were done for each subset, obtaining the mean
value. These nine executions vary on the labeled
subset within the training set.
The fact that one-against-one-S3VM has been the
worst approach for our experiments confirms that
the noise added by the unlabeled documents within
each 1-vs-1 binary classification task is harmful to
the learning phase, and it is not corrected when
merging all the binary tasks.
The averaged accuracy for the combined bi-
nary classifiers allows to compare the one-against-
one and one-against-all views. The averaged ac-
curacy for one-against-one-S3VM shows very low
performance (about 60% in most cases), whereas
the same value for one-against-all-S3VM is much
higher (about 90% in most cases). This is obvi-
ous to happen for the one-against-all view, since
it is much easier to predict documents not pertain-
ing to the class under consideration for each 1-vs-
all binary classifier. Although each binary classifier
gets about 90% accuracy for the one-against-one-
S3VM approach, this value falls considerably when
combining them to get the multiclass result. This
shows the additional difficulty for multiclass prob-
lems compared to binary ones. Hence, the difficulty
to correctly predict unlabeled data increases for mul-
ticlass tasks, and it is more likely to add noise during
the learning phase.
</bodyText>
<figureCaption confidence="0.999955333333333">
Figure 3: Results for BankSearch dataset
Figure 4: Results for WebKB dataset
Figure 5: Results for Yahoo! Science dataset
</figureCaption>
<bodyText confidence="0.999949625">
For all the datasets we worked with, there is a
noticeable performance gap between direct multi-
class and binary combining approaches. Both 2-
steps-SVM and 1-step-SVM are always on the top
of the graphs, and one-against-all-S3VM and one-
against-all-SVM approaches are so far from catch-
ing up with their results, except for WebKB dataset,
where the gap is not so noticeable. This seems en-
couraging, since considering less support vectors in
a direct multiclass approach reduces the computa-
tional cost and improves the final results.
Comparing the two analogous approaches among
them, different conclusions could be extracted.
On the one hand, one-against-all-S3VM shows
slightly better results than one-against-all-SVM, and
so considering unlabeled documents seems to be
</bodyText>
<page confidence="0.996541">
34
</page>
<bodyText confidence="0.999977147540984">
favourable for the one-against-all view. On the other
hand, the direct multiclass view shows varying re-
sults. Both 2-steps-SVM and 1-step-SVM show very
similar results for BankSearch and Yahoo! Science
datasets, but superior for 1-step-SVM over the We-
bKB dataset. As a conclusion of this, ignoring un-
labeled documents by means of the 1-step-SVM ap-
proach seems to be advisable, since it reduces the
computation cost, obtaining at least the same results
than the semi-supervised 2-steps-SVM.
Although their results are so poor, as we said
above, the supervised approach wins for the one-
against-one view; this confirms, again, that the one-
against-one view is not an adecuate view to be ap-
plied in a semi-supervised environment, due to the
noise existing during the learning phase.
When analyzing the performance gaps between
the analogous approaches, a general conclusion can
be extracted: the smaller is the labeled subset the
bigger is the performance gap, except for the Ya-
hoo! Science dataset. Comparing the two best
approaches, 1-step-SVM and 2-steps-SVM, the per-
formance gap increases when the number of la-
beled documents decrease for BankSearch; for this
dataset, the accuracy by 1-step-SVM is 0.92 times
the one by 2-steps-SVM when the number of labeled
documents is only 50, but this proportion goes to
0.99 with 500 labeled documents. This reflects how
the contribution of unlabeled data decreases while
the labeled set increases. For WebKB, the perfor-
mance gap is in favour of 1-step-SVM, and varies
between 1.01 and 1.05 times 2-steps-SVM method’s
accuracy, even with only 50 labeled documents.
Again, increasing the labeled set negatively affects
semi-supervised algorithm’s performance. Last, for
Yahoo! Science, the performance gap among these
two approaches is not considerable, since their re-
sults are very similar.
Our conjecture for the performance difference be-
tween 1-step-SVM and 2-steps-SVM for the three
datasets is the nature of the classes. The accuracy
by semi-supervised 2-steps-SVM is slightly higher
for BankSearch and Yahoo! Science, where the
classes are quite heterogeneous. On the other hand,
the accuracy by supervised 1-step-SVM is clearly
higher for WebKB, where all the classes are an aca-
demic topic, and so more homogeneous. The semi-
supervised classifiers show a major problem for pre-
dicting the unlabeled documents when the collection
is more homogeneous, and so more difficult to differ
between classes.
In summary, the main idea is that unlabeled doc-
uments do not seem to contribute as they would for
multiclass tasks using SVM. Within the approaches
we tested, the supervised 1-step-SVM approach
shows the best (or very similar to the best in some
cases) results in accuracy and, taking into account
it is the least-expensive approach, we strongly en-
courage to use this approach to solve multiclass web
page classification tasks, mainly when the classes
under consideration are homogeneous.
</bodyText>
<sectionHeader confidence="0.997656" genericHeader="conclusions">
6 Conclusions and Outlook
</sectionHeader>
<bodyText confidence="0.999989652173913">
We have studied and analyzed the contribution of
considering unlabeled data during the learning phase
for multiclass web page classification tasks using
SVM. Our results show that ignoring unlabeled doc-
ument to learn reduces computational cost and, ad-
ditionaly, obtains similar or slightly worse accuracy
values for heterogeneus taxonomies, but higher for
homogeneous ones. Therefore we show that, unlike
for binary cases, as was shown by (Joachims, 1999),
a supervised view outperforms a semi-supervised
one for multiclass environments. Our thought is that
predicting unlabeled documents’ class is much more
difficult when the number of classes increases, and
so, the mistaken labeled documents are harmful for
classifier’s learning phase.
As a future work, a direct semi-supervised multi-
class approach, such as those proposed by (Yajima
and Kuo, 2006) and (Chapelle et al., 2006), should
also be considered, as well as setting the classifier
with different parameters or kernels. Balancing the
weight of previously and newly labeled data could
also be interesting to improve semi-supervised ap-
proaches’ results.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998196571428571">
We wish to thank the anonymous reviewers for their
helpful and instructive comments. This work has
been supported by the Research Network MAVIR
(S-0505/TIC-0267), the Regional Ministry of Ed-
ucation of the Community of Madrid, and by the
Spanish Ministry of Science and Innovation project
QEAVis-Catiex (TIN2007-67581-C02-01).
</bodyText>
<page confidence="0.998995">
35
</page>
<sectionHeader confidence="0.993911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999927171875">
B. E. Boser, I. Guyon and V. Vapnik. 1992. A Training
Algorithm for Optimal Margin Classifiers. Proceed-
ings of the 5th Annual Workshop on computational
Learning Theory.
C. Campbell. 2000. Algorithmic Approaches to Training
Support Vector Machines: A Survey Proceedings of
ESANN’2000, European Symposium on Artificial Neu-
ral Networks.
O. Chapelle, M. Chi y A. Zien 2006. A Continuation
Method for Semi-supervised SVMs. Proceedings of
ICML’06, the 23rd International Conference on Ma-
chine Learning.
O. Chapelle, V. Sindhwani, S. Keerthi 2008. Optimiza-
tion Techniques for Semi-Supervised Support Vector
Machines. J. Mach. Learn. Res..
C. Cortes and V. Vapnik. 1995. Support Vector Network.
Machine Learning.
C.-H. Hsu and C.-J. Lin. 2002. A Comparison of Meth-
ods for Multiclass Support Vector Machines. IEEE
Transactions on Neural Networks.
T. Joachims. 1998. Text Categorization with Support
Vector Machines: Learning with many Relevant Fea-
tures. Proceedings of ECML98, 10th European Con-
ference on Machine Learning.
T. Joachims. 1999. Transductive Inference for Text
Classification Using Support Vector Machines. Pro-
ceedings of ICML99, 16th International Conference
on Machine Learning.
J. Kivinen and E.J. Smola and R.C. Williamson. 2002.
Learning with Kernels.
T. Mitchell. 1997. Machine Learning. McGraw Hill.
H.-N. Qi, J.-G. Yang, Y.-W. Zhong y C. Deng 2004.
Multi-class SVM Based Remote Sensing Image
Classification and its Semi-supervised Improvement
Scheme. Proceedings of the 3rd ICMLC.
X. Qi and B.D. Davison. 2007. Web Page Classification:
Features and Algorithms. Technical Report LU-CSE-
07-010.
B. Sch¨olkopf and A. Smola. 1999. Advances in Kernel
Methods: Support Vector Learning. MIT Press.
F. Sebastiani. 2002. Machine Learning in Automated
Text Categorization. ACM Computing Surveys, pp. 1-
47.
M.P. Sinka and D.W. Corne. 2002. A New Benchmark
Dataset for Web Document Clustering. Soft Comput-
ing Systems.
C.M. Tan, Y.F. Wang and C.D. Lee. 2002. The Use of
Bigrams to Enhance Text Categorization. Information
Processing and Management.
J. Weston and C. Watkins. 1999. Multi-class Support
Vector Machines. Proceedings of ESAAN, the Euro-
pean Symposium on Artificial Neural Networks.
L. Xu y D. Schuurmans. 2005. Unsupervised and Semi-
supervised Multiclass Support Vector Machines. Pro-
ceedings ofAAAI’05, the 20th National Conference on
Artificial Intelligence.
Z. Xu, R. Jin, J. Zhu, I. King and M. R. Lyu. 2007. Ef-
ficient Convex Optimization for Transductive Support
Vector Machine. Advances in Neural Information Pro-
cessing Systems.
Y. Yajima and T.-F. Kuo. 2006. Optimization Ap-
proaches for Semi-Supervised Multiclass Classifica-
tion. Proceedings of ICDM’06 Workshops, the 6th In-
ternational Conference on Data Mining.
</reference>
<page confidence="0.998938">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.567820">
<title confidence="0.8469934">Is Unlabeled Data Suitable for Multiclass SVM-based Web Classification? Arkaitz Zubiaga, Victor Fresno, Raquel NLP &amp; IR Group at Lenguajes y Sistemas</title>
<author confidence="0.911748">E T S I Inform´atica</author>
<email confidence="0.968194">vfresno,</email>
<abstract confidence="0.9967105">Support Vector Machines present an interesting and effective approach to solve automated classification tasks. Although it only handles binary and supervised problems by nature, it has been transformed into multiclass and semi-supervised approaches in several works. A previous study on supervised and semi-supervised SVM classification over binary taxonomies showed how the latter clearly outperforms the former, proving the suitability of unlabeled data for the learning phase in this kind of tasks. However, the suitability of unlabeled data for multiclass tasks using SVM has never been tested before. In this work, we present a study on whether unlabeled data could improve results for multiclass web page classification tasks using Support Vector Machines. As a conclusion, we encourage to rely only on labeled data, both for improving (or at least equaling) performance and for reducing the computational cost.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B E Boser</author>
<author>I Guyon</author>
<author>V Vapnik</author>
</authors>
<title>A Training Algorithm for Optimal Margin Classifiers.</title>
<date>1992</date>
<booktitle>Proceedings of the 5th Annual Workshop on computational Learning Theory.</booktitle>
<marker>Boser, Guyon, Vapnik, 1992</marker>
<rawString>B. E. Boser, I. Guyon and V. Vapnik. 1992. A Training Algorithm for Optimal Margin Classifiers. Proceedings of the 5th Annual Workshop on computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Campbell</author>
</authors>
<title>Algorithmic Approaches to Training Support Vector Machines: A Survey</title>
<date>2000</date>
<booktitle>Proceedings of ESANN’2000, European Symposium on Artificial Neural Networks.</booktitle>
<marker>Campbell, 2000</marker>
<rawString>C. Campbell. 2000. Algorithmic Approaches to Training Support Vector Machines: A Survey Proceedings of ESANN’2000, European Symposium on Artificial Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>M Chi y A Zien</author>
</authors>
<title>A Continuation Method for Semi-supervised SVMs.</title>
<date>2006</date>
<booktitle>Proceedings of ICML’06, the 23rd International Conference on Machine Learning.</booktitle>
<marker>Chapelle, Zien, 2006</marker>
<rawString>O. Chapelle, M. Chi y A. Zien 2006. A Continuation Method for Semi-supervised SVMs. Proceedings of ICML’06, the 23rd International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>V Sindhwani</author>
<author>S Keerthi</author>
</authors>
<title>Optimization Techniques for Semi-Supervised Support Vector Machines.</title>
<date>2008</date>
<journal>J. Mach. Learn. Res..</journal>
<contexts>
<context position="9321" citStr="Chapelle et al., 2008" startWordPosition="1481" endWordPosition="1484"> (Joachims, 1999) demonstrated a large performance gap between the original supervised SVM and his semi-supervised proposal, in favour of the latter one. He showed that for binary classification tasks, the smaller is the training set size, the larger gets the difference among these two approaches. Although he worked Figure 2: SVM vs S3VM, where white balls are unlabeled documents with multiclass datasets, he splitted the problems into smaller binary ones, and so he did not demonstrate whether the same performance gap occurs for multiclass classification. This paper tries to cover this issue. (Chapelle et al., 2008) present a comprehensive study on S3VM approaches. 3 Multiclass SVM Due to the dichotomic nature of SVM, it came up the need to implement new methods to solve multiclass problems, where more than two classes must be considered. Different approaches have been proposed to achieve this. On the one hand, as a direct approach, (Weston, 1999) proposed modifying the optimization function getting into account all the k classes at once: ⎡ min ⎣ Subject to: wyi · xi + byi ≥ wm · xi + bm + 2 − �m i , mi ≥ 0 On the other hand, the original binary SVM classifier has usually been combined to obtain a multic</context>
</contexts>
<marker>Chapelle, Sindhwani, Keerthi, 2008</marker>
<rawString>O. Chapelle, V. Sindhwani, S. Keerthi 2008. Optimization Techniques for Semi-Supervised Support Vector Machines. J. Mach. Learn. Res..</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support Vector Network.</title>
<date>1995</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="6300" citStr="Cortes and Vapnik, 1995" startWordPosition="990" endWordPosition="993">; therefore, this hyperplane should maximize the distance between it and the nearest documents, what is called the margin. The following function is used to define the hyperplane (see Figure 1): f(x) = w · x + b Figure 1: An example of binary SVM classification, separating two classes (black dots from white dots) In order to resolve this function, all the possible values should be considered and, after that, the values of w and b that maximize the margin should be selected. This would be computationally expensive, so the following equivalent function is used to relax it (Boser et al. , 1992) (Cortes and Vapnik, 1995): where C is the penalty parameter, i is an stack variable for the ith document, and l is the number of labeled documents. This function can only resolve linearly separable problems, thus the use of a kernel function is commonly required for the redimension of the space; in this manner, the new space will be linearly separable. After that, the redimension is undone, so the found hyperplane will be transformed to the original space, respecting the classification function. Bestknown kernel functions include linear, polynomial, radial basis function (RBF) and sigmoid, among others. Different kern</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support Vector Network. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-H Hsu</author>
<author>C-J Lin</author>
</authors>
<title>A Comparison of Methods for Multiclass Support Vector Machines.</title>
<date>2002</date>
<journal>IEEE Transactions on Neural Networks.</journal>
<contexts>
<context position="10064" citStr="Hsu and Lin, 2002" startWordPosition="1619" endWordPosition="1622">to implement new methods to solve multiclass problems, where more than two classes must be considered. Different approaches have been proposed to achieve this. On the one hand, as a direct approach, (Weston, 1999) proposed modifying the optimization function getting into account all the k classes at once: ⎡ min ⎣ Subject to: wyi · xi + byi ≥ wm · xi + bm + 2 − �m i , mi ≥ 0 On the other hand, the original binary SVM classifier has usually been combined to obtain a multiclass solution. As combinations of binary SVM classifiers, two different approaches to k-class classifiers can be emphasized (Hsu and Lin, 2002): • one-against-all constructs k classifiers defining that many hyperplanes; each of them separates the class i from the rest k-1. For instance, for a problem with 4 classes, 1 vs 2-3-4, 2 vs 1-3- 4, 3 vs 1-2-4 and 4 vs 1-2-3 classifiers would ⎡ min ⎣ Xl i=1 �di + C* · ⎤tt*d Sj u X j=1 ||wm||2 + C 1 2 k X m=1 Xl i=1 X m7�yi ⎤ tm ⎦i 30 be created. New documents will be categorized in the class of the classifier that maximizes the margin: Ci = argmaxi=1,...,k(wix + bi). As the number of classes increases, the amount of classifiers will increase linearly. • one-against-one constructs k(k−1) 2 cla</context>
</contexts>
<marker>Hsu, Lin, 2002</marker>
<rawString>C.-H. Hsu and C.-J. Lin. 2002. A Comparison of Methods for Multiclass Support Vector Machines. IEEE Transactions on Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with many Relevant Features.</title>
<date>1998</date>
<booktitle>Proceedings of ECML98, 10th European Conference on Machine Learning.</booktitle>
<contexts>
<context position="1677" citStr="Joachims, 1998" startWordPosition="257" endWordPosition="258">nly on labeled data, both for improving (or at least equaling) performance and for reducing the computational cost. 1 Introduction The amount of web documents is increasing in a very fast way in the last years, what makes more and more complicated its organization. For this reason, web page classification has gained importance as a task to ease and improve information access. Web page classification can be defined as the task of labeling and organizing web documents within a set of predefined categories. In this work, we focus on web page classification based on Support Vector Machines (SVM, (Joachims, 1998)). This kind of classification tasks rely on a previously labeled training set of documents, with which the classifier acquires the required ability to classify new unknown documents. Different settings can be distinguished for web page classification problems. On the one hand, attending to the learning technique the system bases on, it may be supervised, with all the training documents previously labeled, or semi-supervised, where unlabeled documents are also taken into account during the learning phase. On the other hand, attending to the number of classes, the classification may be binary, </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with many Relevant Features. Proceedings of ECML98, 10th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive Inference for Text Classification Using Support Vector Machines.</title>
<date>1999</date>
<booktitle>Proceedings of ICML99, 16th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3599" citStr="Joachims, 1999" startWordPosition="553" endWordPosition="554">size of the Web, this task becomes semi-supervised besides multiclass. However, the original SVM algorithm supports neither semi-supervised learning nor multiclass taxonomies, due to its dichotomic and supervised nature. To solve this issue, different studies for both multiclass SVM and semi-supervised SVM approaches have been proposed, but a little effort has 28 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 28–36, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics been invested in the combination of them. (Joachims, 1999) compares supervised and semisupervised approaches for binary tasks using SVM. It shows encouraging results for the transductive semi-supervised approach, clearly improving the supervised, and so he proved unlabeled data to be suitable to optimize binary SVM classifiers’ results. On the other hand, the few works presented for semi-supervised multiclass SVM classification do not provide clear information on whether the unlabeled data improves the classification results in comparison with the only use of labeled data. In this work, we performed an experiment among different SVM-based multiclass </context>
<context position="8058" citStr="Joachims, 1999" startWordPosition="1279" endWordPosition="1280"> so classifier’s predictions over them is also included as labeled data to learn. The fact of taking into account unlabeled data to learn can improve the classification done by supervised methods, specially when its predictions provide new useful information, as shown in figure 2. However, the noise added by erroneus predictions can make worse the learning phase and, therefore, its final performance. This makes interesting the study on whether relying on semi-supervised approaches is suitable for each kind of task. Semi-supervised learning for SVM, also known as S3VM, was first introduced by (Joachims, 1999) in a transductive way, by modifying the original SVM function. To do that, he proposed to add an additional term to the optimization function: 2 · ||w||2 + C · 1 where u is the number of unlabeled data. Nevertheless, the adaptation of SVM to semisupervised learning significantly increases its computational cost, due to the non-convex nature of the resulting function, and so obtaining the minimum value is even more complicated. In order to relax the function, convex optimization techniques such as semi-definite programming are commonly used (Xu et al. , 2007), where minimizing the function get</context>
<context position="28381" citStr="Joachims, 1999" startWordPosition="4534" endWordPosition="4535"> to use this approach to solve multiclass web page classification tasks, mainly when the classes under consideration are homogeneous. 6 Conclusions and Outlook We have studied and analyzed the contribution of considering unlabeled data during the learning phase for multiclass web page classification tasks using SVM. Our results show that ignoring unlabeled document to learn reduces computational cost and, additionaly, obtains similar or slightly worse accuracy values for heterogeneus taxonomies, but higher for homogeneous ones. Therefore we show that, unlike for binary cases, as was shown by (Joachims, 1999), a supervised view outperforms a semi-supervised one for multiclass environments. Our thought is that predicting unlabeled documents’ class is much more difficult when the number of classes increases, and so, the mistaken labeled documents are harmful for classifier’s learning phase. As a future work, a direct semi-supervised multiclass approach, such as those proposed by (Yajima and Kuo, 2006) and (Chapelle et al., 2006), should also be considered, as well as setting the classifier with different parameters or kernels. Balancing the weight of previously and newly labeled data could also be i</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Transductive Inference for Text Classification Using Support Vector Machines. Proceedings of ICML99, 16th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kivinen</author>
<author>E J Smola</author>
<author>R C Williamson</author>
</authors>
<title>Machine Learning.</title>
<date>2002</date>
<publisher>McGraw Hill.</publisher>
<note>Learning with</note>
<contexts>
<context position="7001" citStr="Kivinen et al., 2002" startWordPosition="1103" endWordPosition="1106"> and l is the number of labeled documents. This function can only resolve linearly separable problems, thus the use of a kernel function is commonly required for the redimension of the space; in this manner, the new space will be linearly separable. After that, the redimension is undone, so the found hyperplane will be transformed to the original space, respecting the classification function. Bestknown kernel functions include linear, polynomial, radial basis function (RBF) and sigmoid, among others. Different kernel functions’ performance has been studied in (Sch¨olkopf and Smola, 1999) and (Kivinen et al., 2002). I Subject to: yi(w · xi + b) &gt; 1 − i, i &gt; 0 rr i minl1llw||2 + CSC, LL i=� 29 Note that the function above can only resolve binary and supervised problems, so different variants are necessary to perform semi-supervised or multiclass tasks. 2.1 Semi-supervised Learning for SVM (S3VM) Semi-supervised learning approaches differ in the learning phase. As opposed to supervised approaches, unlabeled data is used during the learning phase, and so classifier’s predictions over them is also included as labeled data to learn. The fact of taking into account unlabeled data to learn can improve the clas</context>
</contexts>
<marker>Kivinen, Smola, Williamson, 2002</marker>
<rawString>J. Kivinen and E.J. Smola and R.C. Williamson. 2002. Learning with Kernels. T. Mitchell. 1997. Machine Learning. McGraw Hill.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H-N Qi</author>
<author>J-G Yang</author>
<author>Y-W</author>
</authors>
<title>Zhong y C. Deng 2004. Multi-class SVM Based Remote Sensing Image Classification and its Semi-supervised Improvement Scheme.</title>
<booktitle>Proceedings of the 3rd ICMLC.</booktitle>
<marker>Qi, Yang, Y-W, </marker>
<rawString>H.-N. Qi, J.-G. Yang, Y.-W. Zhong y C. Deng 2004. Multi-class SVM Based Remote Sensing Image Classification and its Semi-supervised Improvement Scheme. Proceedings of the 3rd ICMLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Qi</author>
<author>B D Davison</author>
</authors>
<title>Web Page Classification: Features and Algorithms.</title>
<date>2007</date>
<tech>Technical Report LU-CSE07-010.</tech>
<contexts>
<context position="2712" citStr="Qi and Davison, 2007" startWordPosition="416" endWordPosition="419">emi-supervised, where unlabeled documents are also taken into account during the learning phase. On the other hand, attending to the number of classes, the classification may be binary, where only two possible categories can be assigned to each document, or multiclass, where three or more categories can be set. The former is commonly used for filtering systems, whereas the latter is necessary for bigger taxonomies, e.g. topical classification. Although multiple studies have been made for text classification, its application to the web page classification area remains without enough attention (Qi and Davison, 2007). Analyzing the nature of a web page classification task, we can consider it to be, generally, multiclass problems, where it is usual to find numerous classes. In the same way, if we take into account that the number of available labeled documents is tiny compared to the size of the Web, this task becomes semi-supervised besides multiclass. However, the original SVM algorithm supports neither semi-supervised learning nor multiclass taxonomies, due to its dichotomic and supervised nature. To solve this issue, different studies for both multiclass SVM and semi-supervised SVM approaches have been</context>
</contexts>
<marker>Qi, Davison, 2007</marker>
<rawString>X. Qi and B.D. Davison. 2007. Web Page Classification: Features and Algorithms. Technical Report LU-CSE07-010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>A Smola</author>
</authors>
<title>Advances in Kernel Methods: Support Vector Learning.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Sch¨olkopf, Smola, 1999</marker>
<rawString>B. Sch¨olkopf and A. Smola. 1999. Advances in Kernel Methods: Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine Learning in Automated Text Categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<pages>1--47</pages>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine Learning in Automated Text Categorization. ACM Computing Surveys, pp. 1-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Sinka</author>
<author>D W Corne</author>
</authors>
<title>A New Benchmark Dataset for Web Document Clustering. Soft Computing Systems.</title>
<date>2002</date>
<contexts>
<context position="17729" citStr="Sinka and Corne, 2002" startWordPosition="2867" endWordPosition="2870"> the semi-supervised approaches, 2-stepsSVM, one-against-all-S3VM and one-against-oneS3VM, respectively. They differ in the learning phase: unlike the semi-supervised approaches, these three supervised approaches only rely on the labeled documents for the learning task, but after that they classify the same test documents. These approaches allow to evaluate whether the unlabeled documents are contributing in a positive or negative way in the learning phase. 4.2 Datasets For these experiments we have used three web page benchmark datasets previously used for classification tasks: • BankSearch (Sinka and Corne, 2002), a collection of 11,000 web pages over 11 classes, with very different topics: commercial banks, building societies, insurance agencies, java, c, visual basic, astronomy, biology, soccer, motorsports and sports. We removed the category sports, since it includes both soccer and motorsports in it, as a parent category. This results 10,000 web pages over 10 categories. 4,000 instances were assigned to the training set, while the other 6,000 were left on the test set. • WebKB1, with a total of 4,518 documents of 4 universities, and classified into 7 classes 1http://www.cs.cmu.edu/afs/cs.cmu.edu/p</context>
</contexts>
<marker>Sinka, Corne, 2002</marker>
<rawString>M.P. Sinka and D.W. Corne. 2002. A New Benchmark Dataset for Web Document Clustering. Soft Computing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Tan</author>
<author>Y F Wang</author>
<author>C D Lee</author>
</authors>
<title>The Use of Bigrams to Enhance Text Categorization. Information Processing and Management.</title>
<date>2002</date>
<marker>Tan, Wang, Lee, 2002</marker>
<rawString>C.M. Tan, Y.F. Wang and C.D. Lee. 2002. The Use of Bigrams to Enhance Text Categorization. Information Processing and Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>C Watkins</author>
</authors>
<title>Multi-class Support Vector Machines.</title>
<date>1999</date>
<booktitle>Proceedings of ESAAN, the European Symposium on Artificial Neural Networks.</booktitle>
<marker>Weston, Watkins, 1999</marker>
<rawString>J. Weston and C. Watkins. 1999. Multi-class Support Vector Machines. Proceedings of ESAAN, the European Symposium on Artificial Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Xu y D Schuurmans</author>
</authors>
<title>Unsupervised and Semisupervised Multiclass Support Vector Machines.</title>
<date>2005</date>
<booktitle>Proceedings ofAAAI’05, the 20th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="13280" citStr="Schuurmans, 2005" startWordPosition="2171" endWordPosition="2173">und. They modify the function for multiclass SVM classification and get it usable for semi-supervised tasks. The resulting optimization function is as follows: QiT K−1Qi max{0,1 − (Qyj j − Qij)}2 where Q represents the product of a vector of variables and a kernel matrix defined by the author. On the other hand, some other works are based on different approaches to achieve a multiclass S3VM classifier. (Qi et al., 2004) use Fuzzy C-Means (FCM) to predict labels for unlabeled documents. After that, multiclass SVM is used to learn with the augmented training set, classifying the test set. (Xu y Schuurmans, 2005) rely on a clustering-based approach to label the unlabeled data. Afterwards, they apply a multiclass SVM classifier to the fully labeled training set. (Chapelle et al., 2006) present a direct multiclass S3VM approach by using the Continuation Method. On the other hand, this is the only work, to the best of our knowledge, that has tested the one-against-all and one-against-one approaches in a semi-supervised environment. They apply these methods to some news datasets, for which they get low performance. Additionally, they show that oneagainst-one is not sufficient for real-world multiclass sem</context>
<context position="16205" citStr="Schuurmans, 2005" startWordPosition="2639" endWordPosition="2641"> 2-steps-SVM to the technique based on the direct multiclass supervised approach exposed in section 3. This method works, on its first step, with the training collection, learning with the labeled documents and predicting the unlabeled ones; after that, the latter documents are labeled based on the generated predictions. On the second step, now with a fully labeled training set, the usual supervised classification process is done, learning with the training documents and predicting the documents in the test set. This approach is somehow similar to those proposed by (Qi et al., 2004) and (Xu y Schuurmans, 2005). Nonetheless, the 2-steps-SVM approach uses the same method for both the first and second steps. A supervised multiclass SVM is used to increase the labeled set and, after that, to classify the test set. • one-against-all-S3VM: the one-against-all approach has not sufficiently been tested for semisupervised environments, and seems interesting to evaluate its performance. • one-against-one-S3VM: the one-against-one does not seem to be suitable for semisupervised environments, since the classifier is not able to ignore the inadecuate unlabeled documents for each 1-vs-1 binary task, as stated by</context>
</contexts>
<marker>Schuurmans, 2005</marker>
<rawString>L. Xu y D. Schuurmans. 2005. Unsupervised and Semisupervised Multiclass Support Vector Machines. Proceedings ofAAAI’05, the 20th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Xu</author>
<author>R Jin</author>
<author>J Zhu</author>
<author>I King</author>
<author>M R Lyu</author>
</authors>
<title>Efficient Convex Optimization for Transductive Support Vector Machine. Advances in Neural Information Processing Systems.</title>
<date>2007</date>
<marker>Xu, Jin, Zhu, King, Lyu, 2007</marker>
<rawString>Z. Xu, R. Jin, J. Zhu, I. King and M. R. Lyu. 2007. Efficient Convex Optimization for Transductive Support Vector Machine. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yajima</author>
<author>T-F Kuo</author>
</authors>
<title>Optimization Approaches for Semi-Supervised Multiclass Classification.</title>
<date>2006</date>
<booktitle>Proceedings of ICDM’06 Workshops, the 6th International Conference on Data Mining.</booktitle>
<contexts>
<context position="12653" citStr="Yajima and Kuo, 2006" startWordPosition="2062" endWordPosition="2066">e taxonomy is defined by more than two classes and the number of previously labeled documents is very small, the combination of both multiclass and semi-supervised approaches could be required. That is, a multiclass S3VM approach. The usual web page classification problem meets with these characteristics, since more than two classes are usually needed, and the tiny amount of labeled documents requires the use of unlabeled data for the learning phase. Actually, there are a few works focused on transforming SVM into a semi-supervised and multiclass approach. As a direct approach, a proposal by (Yajima and Kuo, 2006) can be found. They modify the function for multiclass SVM classification and get it usable for semi-supervised tasks. The resulting optimization function is as follows: QiT K−1Qi max{0,1 − (Qyj j − Qij)}2 where Q represents the product of a vector of variables and a kernel matrix defined by the author. On the other hand, some other works are based on different approaches to achieve a multiclass S3VM classifier. (Qi et al., 2004) use Fuzzy C-Means (FCM) to predict labels for unlabeled documents. After that, multiclass SVM is used to learn with the augmented training set, classifying the test s</context>
<context position="28779" citStr="Yajima and Kuo, 2006" startWordPosition="4591" endWordPosition="4594">tional cost and, additionaly, obtains similar or slightly worse accuracy values for heterogeneus taxonomies, but higher for homogeneous ones. Therefore we show that, unlike for binary cases, as was shown by (Joachims, 1999), a supervised view outperforms a semi-supervised one for multiclass environments. Our thought is that predicting unlabeled documents’ class is much more difficult when the number of classes increases, and so, the mistaken labeled documents are harmful for classifier’s learning phase. As a future work, a direct semi-supervised multiclass approach, such as those proposed by (Yajima and Kuo, 2006) and (Chapelle et al., 2006), should also be considered, as well as setting the classifier with different parameters or kernels. Balancing the weight of previously and newly labeled data could also be interesting to improve semi-supervised approaches’ results. Acknowledgments We wish to thank the anonymous reviewers for their helpful and instructive comments. This work has been supported by the Research Network MAVIR (S-0505/TIC-0267), the Regional Ministry of Education of the Community of Madrid, and by the Spanish Ministry of Science and Innovation project QEAVis-Catiex (TIN2007-67581-C02-01</context>
</contexts>
<marker>Yajima, Kuo, 2006</marker>
<rawString>Y. Yajima and T.-F. Kuo. 2006. Optimization Approaches for Semi-Supervised Multiclass Classification. Proceedings of ICDM’06 Workshops, the 6th International Conference on Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>