<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<note confidence="0.959783333333333">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 48-55
Edmonton, May-June 2003
</note>
<title confidence="0.991863">
An Analysis of Clarification Dialogue for Question Answering
</title>
<author confidence="0.996514">
Marco De Boni
</author>
<affiliation confidence="0.928824">
School of Computing
Leeds Metropolitan University
</affiliation>
<address confidence="0.639839">
Leeds LS6 3QS, UK
</address>
<affiliation confidence="0.997754">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.943388">
York Y010 5DD, UK
</address>
<email confidence="0.994297">
mdeboni@cs.york.ac.uk
</email>
<author confidence="0.979608">
Suresh Manandhar
</author>
<affiliation confidence="0.998176">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.967919">
York Y010 5DD, UK
</address>
<email confidence="0.999005">
suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.997541" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970384615385">
We examine clarification dialogue, a
mechanism for refining user questions with
follow-up questions, in the context of open
domain Question Answering systems. We
develop an algorithm for clarification dialogue
recognition through the analysis of collected
data on clarification dialogues and examine
the importance of clarification dialogue
recognition for question answering. The
algorithm is evaluated and shown to
successfully recognize the occurrence of
clarification dialogue in the majority of cases
and to simplify the task of answer retrieval.
</bodyText>
<sectionHeader confidence="0.9804185" genericHeader="keywords">
1 Clarification dialogues in Question
Answering
</sectionHeader>
<bodyText confidence="0.99796806122449">
Question Answering Systems aim to determine an
answer to a question by searching for a response in a
collection of documents (see Voorhees 2002 for an
overview of current systems). In order to achieve this
(see for example Harabagiu et al. 2002), systems narrow
down the search by using information retrieval
techniques to select a subset of documents, or
paragraphs within documents, containing keywords
from the question and a concept which corresponds to
the correct question type (e.g. a question starting with
the word “Who?” would require an answer containing a
person). The exact answer sentence is then sought by
either attempting to unify the answer semantically with
the question, through some kind of logical
transformation (e.g. Moldovan and Rus 2001) or by
some form of pattern matching (e.g. Soubbotin 2002;
Harabagiu et al. 1999).
Often, though, a single question is not enough to meet
user’s goals and an elaboration or clarification dialogue
is required, i.e. a dialogue with the user which would
enable the answering system to refine its understanding
of the questioner&apos;s needs (for reasons of space we shall
not investigate here the difference between elaboration
dialogues, clarification dialogues and coherent topical
subdialogues and we shall hence refer to this type of
dialogue simply as “clarification dialogue”, noting that
this may not be entirely satisfactory from a theoretical
linguistic point of view). While a number of researchers
have looked at clarification dialogue from a theoretical
point of view (e.g. Ginzburg 1998; Ginzburg and Sag
2000; van Beek at al. 1993), or from the point of view
of task oriented dialogue within a narrow domain (e.g.
Ardissono and Sestero 1996), we are not aware of any
work on clarification dialogue for open domain question
answering systems such as the ones presented at the
TREC workshops, apart from the experiments carried
out for the (subsequently abandoned) “context” task in
the TREC-10 QA workshop (Voorhees 2002; Harabagiu
et al. 2002). Here we seek to partially address this
problem by looking at some particular aspect of
clarification dialogues in the context of open domain
question answering. In particular, we examine the
problem of recognizing that a clarification dialogue is
occurring, i.e. how to recognize that the current question
under consideration is part of a previous series (i.e.
clarifying previous questions) or the start of a new
series; we then show how the recognition that a
clarification dialogue is occurring can simplify the
problem of answer retrieval.
</bodyText>
<sectionHeader confidence="0.991449" genericHeader="method">
2 The TREC Context Experiments
</sectionHeader>
<bodyText confidence="0.999981807692308">
The TREC-2001 QA track included a &amp;quot;context&amp;quot; task
which aimed at testing systems&apos; ability to track context
through a series of questions (Voorhees 2002). In other
words, systems were required to respond correctly to a
kind of clarification dialogue in which a full
understanding of questions depended on an
understanding of previous questions. In order to test the
ability to answer such questions correctly, a total of 42
questions were prepared by NIST staff, divided into 10
series of related question sentences which therefore
constituted a type of clarification dialogue; the
sentences varied in length between 3 and 8 questions,
with an average of 4 questions per dialogue. These
clarification dialogues were however presented to the
question answering systems already classified and hence
systems did not need to recognize that clarification was
actually taking place. Consequently systems that simply
looked for an answer in the subset of documents
retrieved for the first question in a series performed well
without any understanding of the fact that the questions
constituted a coherent series.
In a more realistic approach, systems would not be
informed in advance of the start and end of a series of
clarification questions and would not be able to use this
information to limit the subset of documents in which
an answer is to be sought.
</bodyText>
<sectionHeader confidence="0.543516" genericHeader="method">
3 Analysis of the TREC context questions
</sectionHeader>
<bodyText confidence="0.9998435">
We manually analysed the TREC context question
collection in order to determine what features could be
used to determine the start and end of a question series,
with the following conclusions:
</bodyText>
<listItem confidence="0.987362">
• Pronouns and possessive adjectives: questions such
as “When was it born?”, which followed “What was
the first transgenic mammal?”, were referring to
some previously mentioned object through a
pronoun (“it”). The use of personal pronouns (“he”,
“it”, ...) and possessive adjectives (“his”, “her”,...)
which did not have any referent in the question
under consideration was therefore considered an
indication of a clarification question..
• Absence of verbs: questions such as “On what body
of water?” clearly referred to some previous
question or answer.
• Repetition of proper nouns: the question series
starting with “What type of vessel was the modern
Varyag?” had a follow-up question “How long was
the Varyag?”, where the repetition of the proper
noun indicates that the same subject matter is under
investigation.
• Importance of semantic relations: the first question
series started with the question “Which museum in
Florence was damaged by a major bomb
</listItem>
<bodyText confidence="0.999039888888889">
explosion?”; follow-up questions included “How
many people were killed?” and “How much
explosive was used?”, where there is a clear
semantic relation between the “explosion” of the
initial question and the “killing” and “explosive” of
the following questions. Questions belonging to a
series were “about” the same subject, and this
aboutness could be seen in the use of semantically
related words.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="method">
4 Experiments in Clarification Dialogue
Recognition
</sectionHeader>
<bodyText confidence="0.992644972222222">
It was therefore speculated that an algorithm which
made use of these features would successfully recognize
the occurrence of clarification dialogue. Given that the
only available data was the collection of “context”
questions used in TREC-10, it was felt necessary to
collect further data in order to test our algorithm
rigorously. This was necessary both because of the
small number of questions in the TREC data and the
fact that there was no guarantee that an algorithm built
for this dataset would perform well on “real” user
questions. A collection of 253 questions was therefore
put together by asking potential users to seek
information on a particular topic by asking a prototype
question answering system a series of questions, with
“cue” questions derived from the TREC question
collection given as starting points for the dialogues.
These questions made up 24 clarification dialogues,
varying in length from 3 questions to 23, with an
average length of 12 questions (the data is available
from the main author upon request).
The differences between the TREC “context”
collection and the new collection are summarized in the
following table:
Groups Qs Av. len Max Min
TREC 10 41 4 8 4
New 24 253 12 23 3
The questions were recorded and manually tagged to
recognize the occurrence of clarification dialogue.
The questions thus collected were then fed into a
system implementing the algorithm, with no indication
as to where a clarification dialogue occurred. The
system then attempted to recognize the occurrence of a
clarification dialogue. Finally the results given by the
system were compared to the manually recognized
clarification dialogue tags. In particular the algorithm
was evaluated for its capacity to:
</bodyText>
<listItem confidence="0.995263">
• recognize a new series of questions (i.e. to tell that
the current question is not a clarification of any
previous question) (indicated by New in the results
table)
• recognize that the current question is clarifying a
previous question (indicated by Clarification in the
table)
</listItem>
<sectionHeader confidence="0.89336" genericHeader="method">
5 Clarification Recognition Algorithm
</sectionHeader>
<bodyText confidence="0.999959294117647">
Our approach to clarification dialogue recognition
looks at certain features of the question currently under
consideration (e.g. pronouns and proper nouns) and
compares the meaning of the current question with the
meanings of previous questions to determine whether
they are “about” the same matter.
Given a question q0 and n previously asked
questions q-1..q-n we have a function
Clarification_Question which is true if a question is
considered a clarification of a previously asked
question. In the light of empirical work such as
(Ginzburg 1998), which indicates that questioners do
not usually refer back to questions which are very
distant, we only considered the set of the previously
mentioned 10 questions.
A question is deemed to be a clarification of a
previous question if:
</bodyText>
<listItem confidence="0.85267672">
1. There are direct references to nouns mentioned in
the previous n questions through the use of
pronouns (he, she, it, ...) or possessive adjectives
(his, her, its...) which have no references in the
current question.
2. The question does not contain any verbs
3. There are explicit references to proper and common
nouns mentioned in the previous n questions, i.e.
repetitions which refer to an identical object; or
there is a strong sentence similarity between the
current question and the previously asked
questions.
In other words:
Clarification_Question
(qn,q-1..q-n)
is true if
1. q0 has pronoun and
possessive adjective
references to q-1..q-n
2. q0 does not contain any
verbs
3. q0 has repetition of
common or proper nouns
in q-1..q-n or q0 has a
strong semantic
</listItem>
<bodyText confidence="0.919543">
similarity to some q e
q-1..q-n
</bodyText>
<sectionHeader confidence="0.91828" genericHeader="method">
6 Sentence Similarity Metric
</sectionHeader>
<bodyText confidence="0.999763382352941">
A major part of our clarification dialogue recognition
algorithm is the sentence similarity metric which looks
at the similarity in meaning between the current
question and previous questions. WordNet (Miller 1999;
Fellbaum 1998), a lexical database which organizes
words into synsets, sets of synonymous words, and
specifies a number of relationships such as hypernym,
synonym, meronym which can exist between the synsets
in the lexicon, has been shown to be fruitful in the
calculation of semantic similarity. One approach has
been to determine similarity by calculating the length of
the path or relations connecting the words which
constitute sentences (see for example Green 1997 and
Hirst and St-Onge 1998); different approaches have
been proposed (for an evaluation see (Budanitsky and
Hirst 2001)), either using all WordNet relations
(Budanitsky and Hirst 2001) or only is-a relations
(Resnik 1995; Jiang and Conrath 1997; Mihalcea and
Moldvoan 1999). Miller (1999), Harabagiu et al. (2002)
and De Boni and Manandhar (2002) found WordNet
glosses, considered as micro-contexts, to be useful in
determining conceptual similarity. (Lee et al. 2002)
have applied conceptual similarity to the Question
Answering task, giving an answer A a score dependent
on the number of matching terms in A and the question.
Our sentence similarity measure followed on these
ideas, adding to the use of WordNet relations, part-of-
speech information, compound noun and word
frequency information.
In particular, sentence similarity was considered as a
function which took as arguments a sentence s1 and a
second sentence s2 and returned a value representing the
semantic relevance of s1 in respect of s2 in the context of
knowledge B, i.e.
</bodyText>
<equation confidence="0.996811">
semantic-relevance( s1, s2, B ) = n e
semantic-relevance(s1,s,B) &lt; semantic-
</equation>
<bodyText confidence="0.982803416666667">
relevance(s2,s, B) represents the fact that sentence s1 is
less relevant than s2 in respect to the sentence s and the
context B. In our experiments, B was taken to be the set
of semantic relations given by WordNet. Clearly, the
use of a different knowledge base would give different
results, depending on its completeness and correctness.
In order to calculate the semantic similarity between
a sentence s1 and another sentence s2, s1 and s2 were
considered as sets P and Q of word stems. The
similarity between each word in the question and each
word in the answer was then calculated and the sum of
the closest matches gave the overall similarity. In other
words, given two sets Q and P, where
Q={qw1,qw2,...,qwn} and P={pw1,pw2,...,pwm}, the
similarity between Q and P is given by
1&lt;p&lt;n Argmaxm similarity( qwp, pwm)
The function similarity( w1, w2) maps the stems of
the two words w1 and w2 to a similarity measure m
representing how semantically related the two words
are; similarity( wi, wj)&lt; similarity( wi, wk) represents the
fact that the word wj is less semantically related than wk
in respect to the word wi. In particular similarity=0 if
two words are not at all semantically related and
similarity=1 if the words are the same.
</bodyText>
<equation confidence="0.735">
similarity( w1, w2) = h e
</equation>
<bodyText confidence="0.999964447368421">
where 0 !5 h !5 1. In particular, similarity( w1, w2) = 0 if
w1eST V w2eST, where ST is a set containing a number
of stop-words (e.g. “the”, “a”, “to”) which are too
common to be able to be usefully employed to estimate
semantic similarity. In all other cases, h is calculated as
follows: the words w1 and w2 are compared using all the
available WordNet relationships (is-a, satellite, similar,
pertains, meronym, entails, etc.), with the additional
relationship, “same-as”, which indicated that two words
were identical. Each relationship is given a weighting
indicating how related two words are, with a “same as”
relationship indicating the closest relationship, followed
by synonym relationships, hypernym, hyponym, then
satellite, meronym, pertains, entails.
So, for example, given the question “Who went to
the mountains yesterday?” and the second question “Did
Fred walk to the big mountain and then to mount
Pleasant?”, Q would be the set {who, go, to, the,
mountain, yesterday} and P would be the set {Did,
Fred, walk, to, the, big, mountain, and, then, to, mount,
Pleasant}.
In order to calculate similarity the algorithm would
consider each word in turn. “Who” would be ignored as
it is a common word and hence part of the list of stop-
words. “Go” would be related to “walk” in a is-a
relationship and receive a score h1. “To” and “the”
would be found in the list of stop-words and ignored.
“Mountain” would be considered most similar to
“mountain” (same-as relationship) and receive a score
h2: “mount” would be in a synonym relationship with
“mountain” and give a lower score, so it is ignored.
“Yesterday” would receive a score of 0 as there are no
semantically related words in Q. The similarity measure
of Q in respect to P would therefore be given by h1 + h2.
In order to improve performance of the similarity
measure, additional information was considered in
addition to simple word matching (see De Boni and
Manandhar 2003 for a complete discussion):
</bodyText>
<listItem confidence="0.568874">
• Compound noun information. The motivation
behind is similar to the reason for using chunking
</listItem>
<bodyText confidence="0.892760846153846">
information, i.e. the fact that the word “United” in
“United States” should not be considered similar to
“United” as in “Manchester United”. As opposed to
when using chunking information, however, when
using noun compound information, the compound
is considered a single word, as opposed to a group
of words: chunking and compound noun
information may therefore be combined as in “[the
[United States] official team]”.
• Proper noun information. The intuition behind this
is that titles (of books, films, etc.) should not be
confused with the “normal” use of the same words:
“blue lagoon” as in the sentence “the film Blue
Lagoon was rather strange” should not be
considered as similar to the same words in the
sentence “they swan in the blue lagoon” as they are
to the sentence “I enjoyed Blue Lagoon when I was
younger”.
• Word frequency information. This is a step beyond
the use of stop-words, following the intuition that
the more a word is common the less it is useful in
determining similarity between sentence. So, given
the sentences “metatheoretical reasoning is
common in philosophy” and “metatheoretical
arguments are common in philosophy”, the word
“metatheoretical” should be considered more
important in determining relevance than the words
“common”, “philosophy” and “is” as it is much
more rare and therefore less probably found in
irrelevant sentences. Word frequency data was
taken from the Given that the questions examined
were generic queries which did not necessarily refer
to a specific set of documents, the word frequency
for individual words was taken to be the word
frequency given in the British National Corpus (see
BNCFreq 2003). The top 100 words, making up
43% of the English Language, were then used as
stop-words and were not used in calculating
semantic similarity.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.955410588235294">
An implementation of the algorithm was evaluated
on the TREC context questions used to develop the
algorithm and then on the collection of 500 new
clarification dialogue questions. The results on the
TREC data, which was used to develop the algorithm,
were as follows (see below for discussion and an
explanation of each method):
TREC Meth.0 Meth.1 Meth.2 Meth.3a Meth.3b
New 90 90 90 60 80
Clarif. 47 53 59 78 72
Where “New” indicates the ability to recognize
whether the current question is the first in a new series
of clarification questions and “Clarif.” (for
“Clarification”) indicates the ability to recognize
whether the current question is a clarification question.
The results for the same experiments conducted on
the collected data were as follows:
</bodyText>
<table confidence="0.909474666666667">
Collected Meth.0 Meth.1 Meth.2 Meth.3a Meth.3b
New 100 100 100 67 83
Clarif. 64 62 66 91 89
</table>
<bodyText confidence="0.997012">
Method 0. This method did not use any linguistic
information and simply took a question to be a
clarification question if it had any words in common
with the previous n questions, else took the question to
be the beginning of a new series. 64% of questions in
the new collection could be recognized with this simple
algorithm, which did not misclassify any &amp;quot;new&amp;quot;
questions.
Method 1. This method employed point 1 of the
algorithm described in section 5: 62% of questions in
the new collection could be recognized as clarification
questions simply by looking for &amp;quot;reference&amp;quot; keywords
such as he, she, this, so, etc. which clearly referred to
previous questions. Interestingly this did not misclassify
any &amp;quot;new&amp;quot; questions.
Method 2. This method employed points 1 and 2 of
the algorithm described in section 5: 5% of questions in
the new collection could be recognized simply by
looking for the absence of verbs, which, combined with
keyword lookup (Method 1), improved performance to
66%. Again this did not misclassify any &amp;quot;new&amp;quot;
questions.
Method 3a. This method employed the full
algorithm described in section 5 (point 3 is the
similarity measure algorithm described in section 6):
clarification recognition rose to 91% of the new
collection by looking at the similarity between nouns in
the current question and nouns in the previous
questions, in addition to reference words and the
absence of verbs. Misclassification was a serious
problem, however with correctly classified &amp;quot;new&amp;quot;
questions falling to 67%.
Method 3b. This was the same as method 3a, but
specified a similarity threshold when employing the
similarity measure described in section 6: this required
the nouns in the current question to be similar to nouns
in the previous question beyond a specified similarity
threshold. This brought clarification question
recognition down to 89% of the new collection, but
misclassification of &amp;quot;new&amp;quot; questions was reduced
significantly, with &amp;quot;new&amp;quot; questions being correctly
classified 83% of the time.
Problems noted were:
</bodyText>
<listItem confidence="0.892435730769231">
• False positives: questions following a similar but
unrelated question series. E.g. &amp;quot;Are they all Muslim
countries?&amp;quot; (talking about religion, but in the
context of a general conversation about Saudi
Arabia) followed by &amp;quot;What is the chief religion in
Peru?&amp;quot; (also about religion, but in a totally
unrelated context).
• Questions referring to answers, not previous
questions (e.g. clarifying the meaning of a word
contained in the answer, or building upon a concept
defined in the answer: e.g. &amp;quot;What did Antonio
Carlos Tobim play?&amp;quot; following &amp;quot;Which famous
musicians did he play with?&amp;quot; in the context of a
series of questions about Fank Sinatra: Antonio
Carlos Tobim was referred to in the answer to the
previous question, and nowhere else in the
exchange. These made up 3% of the missed
clarifications.
• Absence of relationships in WordNet, e.g. between
&amp;quot;NASDAQ&amp;quot; and &amp;quot;index&amp;quot; (as in share index).
Absence of verb-noun relationships in WordNet,
e.g. between to die and death, between &amp;quot;battle&amp;quot; and
&amp;quot;win&amp;quot; (i.e. after a battle one side generally wins and
another side loses), &amp;quot;airport&amp;quot; and &amp;quot;visit&amp;quot; (i.e. people
who are visiting another country use an airport to
get there)
</listItem>
<bodyText confidence="0.999604166666667">
As can be seen from the tables above, the same
experiments conducted on the TREC context questions
yielded worse results; it was difficult to say, however,
whether this was due to the small size of the TREC data
or the nature of the data itself, which perhaps did not
fully reflect “real” dialogues.
As regards the recognition of question in a series
(the recognition that a clarification I taking place), the
number of sentences recognized by keyword alone was
smaller in the TREC data (53% compared to 62%),
while the number of questions not containing verbs was
roughly similar (about 6%). The improvement given by
computing noun similarity between successive
questions gave worse results on the TREC data: using
method 3a resulted in an improvement to the overall
correctness of 19 percentage points, or a 32% increase
(compared to an improvement of 25 percentage points,
or a 38% increase on the collected data); using method
3b resulted in an improvement of 13 percentage points,
or a 22% increase (compared to an improvement of 23
percentage points or a 35% increase on the collected
data), perhaps indicating that in &amp;quot;real&amp;quot; conversation
speakers tend to use simpler semantic relationships than
what was observed in the TREC data.
</bodyText>
<note confidence="0.465968">
The results are summarized in the following diagram
(Fig. 1):
8 Usefulness of Clarification Dialogue
Recognition
</note>
<bodyText confidence="0.98220224590164">
Recognizing that a clarification dialogue is occurring
only makes sense if this information can then be used to
improve answer retrieval performance.
We therefore hypothesized that noting that a
questioner is trying to clarify previously asked questions
is important in order to determine the context in which
an answer is to be sought: in other words, the answers to
certain questions are constrained by the context in
which they have been uttered. The question “What does
attenuate mean?”, for example, may require a generic
answer outlining all the possible meanings of
“attenuate” if asked in isolation, or a particular meaning
if asked after the word has been seen in an answer (i.e.
in a definite context which constrains its meaning). In
other cases, questions do not make sense at all out of a
context. For example, no answer could be given to the
question “where?” asked on its own, while following a
question such as “Does Sean have a house anywhere
apart from Scotland?” it becomes an easily intelligible
query.
The usual way in which Question Answering
systems constrain possible answers is by restricting the
number of documents in which an answer is sought by
filtering the total number of available documents
through the use of an information retrieval engine. The
information retrieval engine selects a subset of the
available documents based on a number of keywords
derived from the question at hand. In the simplest case,
it is necessary to note that some words in the current
question refer to words in previous questions or answers
and hence use these other words when formulating the
IR query. For example, the question “Is he married?”
cannot be used as is in order to select documents, as the
only word passed to the IR engine would be “married”
(possibly the root version “marry”) which would return
too many documents to be of any use. Noting that the
“he” refers to a previously mentioned person (e.g. “Sean
Connery”) would enable the answerer to seek an answer
in a smaller number of documents. Moreover, given that
the current question is asked in the context of a previous
question, the documents retrieved for the previous
related question could provide a context in which to
initially seek an answer.
In order to verify the usefulness of constraining the
set of documents from in which to seek an answer, a
subset made of 15 clarification dialogues (about 100
questions) from the given question data was analyzed by
taking the initial question for a series, submitting it to
the Google Internet Search Engine and then manually
checking to see how many of the questions in the series
could be answered simply by using the first 20
documents retrieved for the first question in a series.
• 69% of clarification questions could be answered
by looking within the documents used for the
previous question in the series, thus indicating the
usefulness of noting the occurrence of clarification
dialogue.
• The remaining 31% could not be answered by
making reference to the previously retrieved
documents, and to find an answer a different
approach had to be taken. In particular:
</bodyText>
<listItem confidence="0.9550142">
• 6% could be answered after retrieving documents
simply by using the words in the question as search
terms (e.g. “What caused the boxer uprising?”);
• 14% required some form of coreference resolution
and could be answered only by combining the
</listItem>
<bodyText confidence="0.988943166666667">
words in the question with the words to which the
relative pronouns in the question referred (e.g.
“What film is he working on at the moment”, with
the reference to “he” resolved, which gets passed to
the search engine as “What film is Sean Connery
working on at the moment?”);
</bodyText>
<listItem confidence="0.8683495">
• 7% required more than 20 documents to be
retrieved by the search engine or other, more
</listItem>
<bodyText confidence="0.951041875">
complex techniques. An example is a question such
as “Where exactly?” which requires both an
understanding of the context in which the question
is asked (“Where?” makes no sense on its own) and
the previously given answer (which was probably a
place, but not restrictive enough for the questioner).
• 4% constituted mini-clarification dialogues within a
larger clarification dialogue (a slight deviation from
the main topic which was being investigated by the
questioner) and could be answered by looking at
the documents retrieved for the first question in the
mini-series.
Recognizing that a clarification dialogue is
occurring therefore can simplify the task of retrieving an
answer by specifying that an answer must be in the set
of documents used the previous questions. This is
</bodyText>
<figureCaption confidence="0.857765">
Fig. 1: Search technique used for Question
</figureCaption>
<figure confidence="0.5456482">
First Q in series
Words in Q
Coreference
Mini-clarification
Other
</figure>
<bodyText confidence="0.984587266666667">
consistent with the results found in the TREC context
task (Voorhees 2002), which indicated that systems
were capable of finding most answers to questions in a
context dialogue simply by looking at the documents
retrieved for the initial question in a series. As in the
case of clarification dialogue recognition, therefore,
simple techniques can resolve the majority of cases;
nevertheless, a full solution to the problem requires
more complex methods. The last case indicates that it is
not enough simply to look at the documents provided by
the first question in a series in order to seek an answer:
it is necessary to use the documents found for a
previously asked question which is related to the current
question (i.e. the questioner could &amp;quot;jump&amp;quot; between
topics). For example, given the following series of
questions starting with Q1:
Q1: When was the Hellenistic Age?
[...]
Q5: How did Alexander the great become ruler?
Q6: Did he conquer anywhere else?
Q7: What was the Greek religion in the Hellenistic Age?
where Q6 should be related to Q5 but Q7 should be
related to Q1, and not Q6. In this case, given that the
subject matter of Q1 is more immediately related to the
subject matter of Q7 than Q6 (although the subject
matter of Q6 is still broadly related, it is more of a
specialized subtopic), the documents retrieved for Q1
will probably be more relevant to Q7 than the
documents retrieved for Q6 (which would probably be
the same documents retrieved for Q5)
</bodyText>
<sectionHeader confidence="0.996809" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999984576923077">
It has been shown that recognizing that a clarification
dialogue is occurring can simplify the task of retrieving
an answer by constraining the subset of documents in
which an answer is to be found. An algorithm was
presented to recognize the occurrence of clarification
dialogue and is shown to have a good performance. The
major limitation of our algorithm is the fact that it only
considers series of questions, not series of answers. As
noted above, it is often necessary to look at an answer to
a question to determine whether the current question is a
clarification question or not. Our sentence similarity
algorithm was limited by the number of semantic
relationships in WordNet: for example, a big
improvement would come from the use of noun-verb
relationships. Future work will be directed on extending
WordNet in this direction and in providing other useful
semantic relationships. Work also needs to be done on
using information given by answers, not just questions
in recognizing clarification dialogue and on coping with
the cases in which clarification dialogue recognition is
not enough to retrieve an answer and where other, more
complex, techniques need to be used. It would also be
beneficial to examine the use of a similarity function in
which similarity decayed in function of the distance in
time between the current question and the past
questions.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999842552631579">
Ardissono, L. and Sestero, D. 1996. &amp;quot;Using dynamic
user models in the recognition of the plans of the
user&amp;quot;. User Modeling and User-Adapted Interaction,
5(2):157-190.
BNCFreq. 2003. English Word Frequency List.
http://www.eecs.umich.edu/~qstout/586/bncfreq.html
(last accessed March 2003).
Budanitsky, A., and Hirst, G. 2001. “Semantic distance
in WordNet: and experimental, application-oriented
evaluation of five measures”, in Proceedings of the
NAACL 2001 Workshop on WordNet and other
lexical resources, Pittsburgh.
De Boni, M. and Manandhar, S. 2003. “The Use of
Sentence Similarity as a Semantic Relevance Metric
for Question Answering”. Proceedings of the AAAI
Symposium on New Directions in Question
Answering, Stanford.
De Boni, M. and Manandhar, S. 2002. “Automated
Discovery of Telic Relations for WordNet”.
Proceedings of the First International WordNet
Conference, India.
Fellbaum, C. 1998. WordNet, An electronic Lexical
Database, MIT Press.
Ginzburg , J. 1998. &amp;quot;Clarifying Utterances&amp;quot; In: J.
Hulstijn and A. Nijholt (eds.) Proceedings of the 2nd
Workshop on the Formal Semantics and Pragmatics
of Dialogue, Twente.
Ginzburg and Sag, 2000. Interrogative Investigations,
CSLI.
Green, S. J. 1997. Automatically generating hypertext
by computing semantic similarity, Technical Report
n. 366, University of Toronto.
Harabagiu, S., Miller, A. G., Moldovan, D. 1999.
“WordNet2 - a morphologically and semantically
enhanced resource”, In Proceedings of SIGLEX-99,
University of Maryland.
Harabagiu, S., et al. 2002. “Answering Complex, List
and Context Questions with LCC’s Question-
Answering Server”, Proceedings of TREC-10, NIST.
Hirst, G., and St-Onge, D. 1998. “Lexical chains as
representations of context for the detection and
correction of malapropisms”, in Fellbaum (ed.),
WordNet: and electronic lexical database, MIT
Press.
Jiang, J. J., and Conrath, D. W. 1997. “Semantic
similarity based on corpus statistics and lexical
taxonomy”, in Proceedings of ICRCL, Taiwan.
Lee, G. G., et al. 2002. “SiteQ: Engineering High
Performance QA System Using Lexico-Semantic
Pattern Matching and Shallow NLP”, Proceedings of
TREC-10, NIST.
Lin, D. 1998. “An information-theoretic definition of
similarity”, in Proceedings of the 15th International
Conference on Machine Learning, Madison.
Mihalcea, R. and Moldovan, D. 1999. “A Method for
Word Sense Disambiguation of Unrestricted Text”,
in Proceedings of ACL ‘99, Maryland, NY.
Miller, G. A. 1999. “WordNet: A Lexical Database”,
Communications of the ACM, 38 (11).
Moldovan, D. and Rus, V. 2001. “Logic Form
Transformation of WordNet and its Applicability to
Question Answering”, Proceedings of the 39th
conference of ACL, Toulouse.
Resnik, P. 1995. “Using information content to evaluate
semantic similarity”, in Proceedings of the 14th
IJCAI, Montreal.
Soubbotin, M. M. 2002. :“Patterns of Potential Answer
Expressions as Clues to the Right Answers”,
Proceedings of TREC-10, NIST.
van Beek, P., Cohen, R. and Schmidt, K., 1993. “From
plan critiquing to clarification dialogue for
cooperative response generation”, Computational
Intelligence 9:132-154.
Voorhees, E. 2002. “Overview of the TREC 2001
Question Answering Track”, Proceedings of TREC-
10, NIST.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.894743">Proceedings of HLT-NAACL 2003 Main Papers , pp. 48-55 Edmonton, May-June 2003</note>
<title confidence="0.995116">An Analysis of Clarification Dialogue for Question Answering</title>
<author confidence="0.999947">Marco De_Boni</author>
<affiliation confidence="0.9005452">School of Leeds Metropolitan Leeds LS6 3QS, Department of Computer University of</affiliation>
<address confidence="0.996981">York Y010 5DD, UK</address>
<email confidence="0.9943">mdeboni@cs.york.ac.uk</email>
<author confidence="0.931698">Suresh Manandhar</author>
<affiliation confidence="0.9996705">Department of Computer Science University of York</affiliation>
<address confidence="0.999557">York Y010 5DD, UK</address>
<email confidence="0.997915">suresh@cs.york.ac.uk</email>
<abstract confidence="0.995760403225806">We examine clarification dialogue, a mechanism for refining user questions with follow-up questions, in the context of open domain Question Answering systems. We develop an algorithm for clarification dialogue recognition through the analysis of collected data on clarification dialogues and examine the importance of clarification dialogue recognition for question answering. The algorithm is evaluated and shown to successfully recognize the occurrence of clarification dialogue in the majority of cases and to simplify the task of answer retrieval. 1 Clarification dialogues in Question Answering Question Answering Systems aim to determine an answer to a question by searching for a response in a collection of documents (see Voorhees 2002 for an overview of current systems). In order to achieve this (see for example Harabagiu et al. 2002), systems narrow down the search by using information retrieval techniques to select a subset of documents, or paragraphs within documents, containing keywords from the question and a concept which corresponds to the correct question type (e.g. a question starting with the word “Who?” would require an answer containing a person). The exact answer sentence is then sought by either attempting to unify the answer semantically with the question, through some kind of logical transformation (e.g. Moldovan and Rus 2001) or by some form of pattern matching (e.g. Soubbotin 2002; Harabagiu et al. 1999). Often, though, a single question is not enough to meet user’s goals and an elaboration or clarification dialogue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not be entirely satisfactory from a theoretical linguistic point of view). While a number of researchers have looked at clarification dialogue from a theoretical point of view (e.g. Ginzburg 1998; Ginzburg and Sag 2000; van Beek at al. 1993), or from the point of view of task oriented dialogue within a narrow domain (e.g. Ardissono and Sestero 1996), we are not aware of any work on clarification dialogue for open domain question answering systems such as the ones presented at the TREC workshops, apart from the experiments carried out for the (subsequently abandoned) “context” task in the TREC-10 QA workshop (Voorhees 2002; Harabagiu et al. 2002). Here we seek to partially address this problem by looking at some particular aspect of clarification dialogues in the context of open domain question answering. In particular, we examine the problem of recognizing that a clarification dialogue is occurring, i.e. how to recognize that the current question under consideration is part of a previous series (i.e. clarifying previous questions) or the start of a new series; we then show how the recognition that a clarification dialogue is occurring can simplify the problem of answer retrieval. 2 The TREC Context Experiments The TREC-2001 QA track included a &amp;quot;context&amp;quot; task which aimed at testing systems&apos; ability to track context through a series of questions (Voorhees 2002). In other words, systems were required to respond correctly to a kind of clarification dialogue in which a full understanding of questions depended on an understanding of previous questions. In order to test the ability to answer such questions correctly, a total of 42 questions were prepared by NIST staff, divided into 10 series of related question sentences which therefore constituted a type of clarification dialogue; the sentences varied in length between 3 and 8 questions, with an average of 4 questions per dialogue. These clarification dialogues were however presented to the question answering systems already classified and hence systems did not need to recognize that clarification was actually taking place. Consequently systems that simply looked for an answer in the subset of documents retrieved for the first question in a series performed well without any understanding of the fact that the questions constituted a coherent series. In a more realistic approach, systems would not be informed in advance of the start and end of a series of clarification questions and would not be able to use this information to limit the subset of documents in which an answer is to be sought. 3 Analysis of the TREC context questions We manually analysed the TREC context question collection in order to determine what features could be used to determine the start and end of a question series, with the following conclusions: • Pronouns and possessive adjectives: questions such was it which followed was first transgenic were referring to some previously mentioned object through a pronoun (“it”). The use of personal pronouns (“he”, “it”, ...) and possessive adjectives (“his”, “her”,...) which did not have any referent in the question under consideration was therefore considered an indication of a clarification question.. Absence of verbs: questions such as what body water?” referred to some previous question or answer. • Repetition of proper nouns: the question series with type of vessel was the modern had a follow-up question long was where the repetition of the proper noun indicates that the same subject matter is under investigation. • Importance of semantic relations: the first question started with the question museum in Florence was damaged by a major bomb follow-up questions included people were and much was where there is a clear semantic relation between the “explosion” of the initial question and the “killing” and “explosive” of the following questions. Questions belonging to a series were “about” the same subject, and this aboutness could be seen in the use of semantically related words. 4 Experiments in Clarification Dialogue Recognition It was therefore speculated that an algorithm which made use of these features would successfully recognize the occurrence of clarification dialogue. Given that the only available data was the collection of “context” questions used in TREC-10, it was felt necessary to collect further data in order to test our algorithm rigorously. This was necessary both because of the small number of questions in the TREC data and the fact that there was no guarantee that an algorithm built for this dataset would perform well on “real” user questions. A collection of 253 questions was therefore put together by asking potential users to seek information on a particular topic by asking a prototype question answering system a series of questions, with “cue” questions derived from the TREC question collection given as starting points for the dialogues. These questions made up 24 clarification dialogues, varying in length from 3 questions to 23, with an average length of 12 questions (the data is available from the main author upon request). The differences between the TREC “context” collection and the new collection are summarized in the following table: Groups Qs Av. len Max Min TREC 10 41 4 8 4 New 24 253 12 23 3 The questions were recorded and manually tagged to recognize the occurrence of clarification dialogue. The questions thus collected were then fed into a system implementing the algorithm, with no indication as to where a clarification dialogue occurred. The system then attempted to recognize the occurrence of a clarification dialogue. Finally the results given by the system were compared to the manually recognized clarification dialogue tags. In particular the algorithm was evaluated for its capacity to: • recognize a new series of questions (i.e. to tell that the current question is not a clarification of any previous question) (indicated by New in the results table) • recognize that the current question is clarifying a previous question (indicated by Clarification in the table) 5 Clarification Recognition Algorithm Our approach to clarification dialogue recognition looks at certain features of the question currently under consideration (e.g. pronouns and proper nouns) and compares the meaning of the current question with the meanings of previous questions to determine whether they are “about” the same matter. a question asked we have a Clarification_Question which is true if a question is considered a clarification of a previously asked question. In the light of empirical work such as (Ginzburg 1998), which indicates that questioners do not usually refer back to questions which are very distant, we only considered the set of the previously mentioned 10 questions. A question is deemed to be a clarification of a previous question if: 1. There are direct references to nouns mentioned in previous through the use of pronouns (he, she, it, ...) or possessive adjectives (his, her, its...) which have no references in the current question. 2. The question does not contain any verbs 3. There are explicit references to proper and common mentioned in the previous i.e. repetitions which refer to an identical object; or there is a strong sentence similarity between the current question and the previously asked questions. In other words: Clarification_Question is true if 1. pronoun and possessive adjective to 2. not contain any verbs 3. repetition of common or proper nouns or a strong semantic to some q 6 Sentence Similarity Metric A major part of our clarification dialogue recognition algorithm is the sentence similarity metric which looks at the similarity in meaning between the current question and previous questions. WordNet (Miller 1999; Fellbaum 1998), a lexical database which organizes words into synsets, sets of synonymous words, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either using all WordNet relations (Budanitsky and Hirst 2001) or only is-a relations (Resnik 1995; Jiang and Conrath 1997; Mihalcea and Moldvoan 1999). Miller (1999), Harabagiu et al. (2002) and De Boni and Manandhar (2002) found WordNet glosses, considered as micro-contexts, to be useful in determining conceptual similarity. (Lee et al. 2002) have applied conceptual similarity to the Question task, giving an answer score dependent the number of matching terms in the question. Our sentence similarity measure followed on these ideas, adding to the use of WordNet relations, part-ofspeech information, compound noun and word frequency information. In particular, sentence similarity was considered as a which took as arguments a sentence a sentence returned a value representing the relevance of respect of the context of i.e. = n &lt; semanticrepresents the fact that sentence relevant than respect to the sentence the In our experiments, taken to be the set of semantic relations given by WordNet. Clearly, the use of a different knowledge base would give different results, depending on its completeness and correctness. In order to calculate the semantic similarity between sentence another sentence as sets word stems. The similarity between each word in the question and each word in the answer was then calculated and the sum of the closest matches gave the overall similarity. In other given two sets where and the between given by similarity( function maps the stems of two words a similarity measure m representing how semantically related the two words represents the that the word less semantically related than respect to the word In particular if two words are not at all semantically related and if the words are the same. = 0 In particular, = 0 if where ST is a set containing a number of stop-words (e.g. “the”, “a”, “to”) which are too common to be able to be usefully employed to estimate similarity. In all other cases, calculated as the words compared using all the available WordNet relationships (is-a, satellite, similar, pertains, meronym, entails, etc.), with the additional relationship, “same-as”, which indicated that two words were identical. Each relationship is given a weighting indicating how related two words are, with a “same as” relationship indicating the closest relationship, followed by synonym relationships, hypernym, hyponym, then satellite, meronym, pertains, entails. So, for example, given the question “Who went to the mountains yesterday?” and the second question “Did Fred walk to the big mountain and then to mount be the set {who, go, to, the, yesterday} and be the set {Did, Fred, walk, to, the, big, mountain, and, then, to, mount, Pleasant}. In order to calculate similarity the algorithm would consider each word in turn. “Who” would be ignored as it is a common word and hence part of the list of stopwords. “Go” would be related to “walk” in a is-a and receive a score “To” and “the” would be found in the list of stop-words and ignored. “Mountain” would be considered most similar to “mountain” (same-as relationship) and receive a score “mount” would be in a synonym relationship with “mountain” and give a lower score, so it is ignored. “Yesterday” would receive a score of 0 as there are no related words in The similarity measure respect to therefore be given by In order to improve performance of the similarity measure, additional information was considered in addition to simple word matching (see De Boni and Manandhar 2003 for a complete discussion): Compound noun information. motivation behind is similar to the reason for using chunking information, i.e. the fact that the word “United” in “United States” should not be considered similar to “United” as in “Manchester United”. As opposed to when using chunking information, however, when using noun compound information, the compound is considered a single word, as opposed to a group of words: chunking and compound noun information may therefore be combined as in “[the [United States] official team]”. Proper noun The intuition behind this is that titles (of books, films, etc.) should not be confused with the “normal” use of the same words: “blue lagoon” as in the sentence “the film Blue Lagoon was rather strange” should not be considered as similar to the same words in the sentence “they swan in the blue lagoon” as they are to the sentence “I enjoyed Blue Lagoon when I was younger”. Word frequency information. is a step beyond the use of stop-words, following the intuition that the more a word is common the less it is useful in determining similarity between sentence. So, given the sentences “metatheoretical reasoning is common in philosophy” and “metatheoretical arguments are common in philosophy”, the word “metatheoretical” should be considered more important in determining relevance than the words “common”, “philosophy” and “is” as it is much more rare and therefore less probably found in irrelevant sentences. Word frequency data was taken from the Given that the questions examined were generic queries which did not necessarily refer to a specific set of documents, the word frequency for individual words was taken to be the word frequency given in the British National Corpus (see BNCFreq 2003). The top 100 words, making up 43% of the English Language, were then used as stop-words and were not used in calculating semantic similarity. 7 Results An implementation of the algorithm was evaluated on the TREC context questions used to develop the algorithm and then on the collection of 500 new clarification dialogue questions. The results on the TREC data, which was used to develop the algorithm, were as follows (see below for discussion and an explanation of each method): TREC Meth.0 Meth.1 Meth.2 Meth.3a Meth.3b New 90 90 90 60 80 Clarif. 47 53 59 78 72 Where “New” indicates the ability to recognize whether the current question is the first in a new series of clarification questions and “Clarif.” (for “Clarification”) indicates the ability to recognize whether the current question is a clarification question.</abstract>
<note confidence="0.73828875">The results for the same experiments conducted on the collected data were as follows: Collected Meth.0 Meth.1 Meth.2 Meth.3a Meth.3b New 100 100 100 67 83</note>
<phone confidence="0.632009">Clarif. 64 62 66 91 89</phone>
<abstract confidence="0.979632176">method did not use any linguistic information and simply took a question to be a clarification question if it had any words in common the previous else took the question to be the beginning of a new series. 64% of questions in the new collection could be recognized with this simple algorithm, which did not misclassify any &amp;quot;new&amp;quot; questions. method employed point 1 of the algorithm described in section 5: 62% of questions in the new collection could be recognized as clarification questions simply by looking for &amp;quot;reference&amp;quot; keywords such as he, she, this, so, etc. which clearly referred to previous questions. Interestingly this did not misclassify any &amp;quot;new&amp;quot; questions. method employed points 1 and 2 of the algorithm described in section 5: 5% of questions in the new collection could be recognized simply by looking for the absence of verbs, which, combined with keyword lookup (Method 1), improved performance to 66%. Again this did not misclassify any &amp;quot;new&amp;quot; questions. method employed the full algorithm described in section 5 (point 3 is the similarity measure algorithm described in section 6): clarification recognition rose to 91% of the new collection by looking at the similarity between nouns in the current question and nouns in the previous questions, in addition to reference words and the absence of verbs. Misclassification was a serious problem, however with correctly classified &amp;quot;new&amp;quot; questions falling to 67%. This the same as method 3a, but specified a similarity threshold when employing the similarity measure described in section 6: this required the nouns in the current question to be similar to nouns in the previous question beyond a specified similarity threshold. This brought clarification question recognition down to 89% of the new collection, but misclassification of &amp;quot;new&amp;quot; questions was reduced significantly, with &amp;quot;new&amp;quot; questions being correctly classified 83% of the time. Problems noted were: • False positives: questions following a similar but unrelated question series. E.g. &amp;quot;Are they all Muslim countries?&amp;quot; (talking about religion, but in the context of a general conversation about Saudi Arabia) followed by &amp;quot;What is the chief religion in Peru?&amp;quot; (also about religion, but in a totally unrelated context). • Questions referring to answers, not previous questions (e.g. clarifying the meaning of a word contained in the answer, or building upon a concept defined in the answer: e.g. &amp;quot;What did Antonio Carlos Tobim play?&amp;quot; following &amp;quot;Which famous musicians did he play with?&amp;quot; in the context of a series of questions about Fank Sinatra: Antonio Carlos Tobim was referred to in the answer to the previous question, and nowhere else in the exchange. These made up 3% of the missed clarifications. • Absence of relationships in WordNet, e.g. between &amp;quot;NASDAQ&amp;quot; and &amp;quot;index&amp;quot; (as in share index). Absence of verb-noun relationships in WordNet, e.g. between to die and death, between &amp;quot;battle&amp;quot; and &amp;quot;win&amp;quot; (i.e. after a battle one side generally wins and another side loses), &amp;quot;airport&amp;quot; and &amp;quot;visit&amp;quot; (i.e. people who are visiting another country use an airport to get there) As can be seen from the tables above, the same experiments conducted on the TREC context questions yielded worse results; it was difficult to say, however, whether this was due to the small size of the TREC data or the nature of the data itself, which perhaps did not fully reflect “real” dialogues. As regards the recognition of question in a series (the recognition that a clarification I taking place), the number of sentences recognized by keyword alone was smaller in the TREC data (53% compared to 62%), while the number of questions not containing verbs was roughly similar (about 6%). The improvement given by computing noun similarity between successive questions gave worse results on the TREC data: using method 3a resulted in an improvement to the overall correctness of 19 percentage points, or a 32% increase (compared to an improvement of 25 percentage points, or a 38% increase on the collected data); using method 3b resulted in an improvement of 13 percentage points, or a 22% increase (compared to an improvement of 23 percentage points or a 35% increase on the collected data), perhaps indicating that in &amp;quot;real&amp;quot; conversation speakers tend to use simpler semantic relationships than what was observed in the TREC data. The results are summarized in the following diagram (Fig. 1): 8 Usefulness of Clarification Dialogue Recognition Recognizing that a clarification dialogue is occurring only makes sense if this information can then be used to improve answer retrieval performance. We therefore hypothesized that noting that a questioner is trying to clarify previously asked questions is important in order to determine the context in which an answer is to be sought: in other words, the answers to certain questions are constrained by the context in which they have been uttered. The question “What does attenuate mean?”, for example, may require a generic answer outlining all the possible meanings of “attenuate” if asked in isolation, or a particular meaning if asked after the word has been seen in an answer (i.e. in a definite context which constrains its meaning). In other cases, questions do not make sense at all out of a context. For example, no answer could be given to the question “where?” asked on its own, while following a question such as “Does Sean have a house anywhere apart from Scotland?” it becomes an easily intelligible query. The usual way in which Question Answering systems constrain possible answers is by restricting the number of documents in which an answer is sought by filtering the total number of available documents through the use of an information retrieval engine. The information retrieval engine selects a subset of the available documents based on a number of keywords derived from the question at hand. In the simplest case, it is necessary to note that some words in the current question refer to words in previous questions or answers and hence use these other words when formulating the IR query. For example, the question “Is he married?” be used is order to select documents, as the only word passed to the IR engine would be “married” (possibly the root version “marry”) which would return too many documents to be of any use. Noting that the “he” refers to a previously mentioned person (e.g. “Sean Connery”) would enable the answerer to seek an answer in a smaller number of documents. Moreover, given that the current question is asked in the context of a previous question, the documents retrieved for the previous related question could provide a context in which to initially seek an answer. In order to verify the usefulness of constraining the set of documents from in which to seek an answer, a subset made of 15 clarification dialogues (about 100 questions) from the given question data was analyzed by taking the initial question for a series, submitting it to the Google Internet Search Engine and then manually checking to see how many of the questions in the series could be answered simply by using the first 20 documents retrieved for the first question in a series. • 69% of clarification questions could be answered by looking within the documents used for the previous question in the series, thus indicating the usefulness of noting the occurrence of clarification dialogue. • The remaining 31% could not be answered by making reference to the previously retrieved documents, and to find an answer a different approach had to be taken. In particular: • 6% could be answered after retrieving documents simply by using the words in the question as search terms (e.g. “What caused the boxer uprising?”); • 14% required some form of coreference resolution and could be answered only by combining the words in the question with the words to which the relative pronouns in the question referred “What film is he working on at the moment”, with the reference to “he” resolved, which gets passed to the search engine as “What film is Sean Connery working on at the moment?”); • 7% required more than 20 documents to be retrieved by the search engine or other, more complex techniques. An example is a question such as “Where exactly?” which requires both understanding of the context in which the question is asked (“Where?” makes no sense on its own) and the previously given answer (which was probably a place, but not restrictive enough for the questioner). • 4% constituted mini-clarification dialogues within a larger clarification dialogue (a slight deviation from the main topic which was being investigated by the questioner) and could be answered by looking at the documents retrieved for the first question in the mini-series. Recognizing that a clarification dialogue is occurring therefore can simplify the task of retrieving an answer by specifying that an answer must be in the set of documents used the previous questions. This is Fig. 1: Search technique used for Question First Q in series Words in Q Coreference Mini-clarification Other consistent with the results found in the TREC context task (Voorhees 2002), which indicated that systems were capable of finding most answers to questions in a context dialogue simply by looking at the documents retrieved for the initial question in a series. As in the case of clarification dialogue recognition, therefore, simple techniques can resolve the majority of cases; nevertheless, a full solution to the problem requires complex methods. The last case indicates that is not enough simply to look at the documents provided by the first question in a series in order to seek an answer: it is necessary to use the documents found for a previously asked question which is related to the current question (i.e. the questioner could &amp;quot;jump&amp;quot; between topics). For example, given the following series of starting with When was the Hellenistic Age? [...] Q5: How did Alexander the great become ruler? Q6: Did he conquer anywhere else? Q7: What was the Greek religion in the Hellenistic Age? be related to be to and not In this case, given that the matter of more immediately related to the matter of the subject of still broadly related, it is more of a subtopic), the documents retrieved for probably be more relevant to the retrieved for would probably be same documents retrieved for 9 Conclusion It has been shown that recognizing that a clarification dialogue is occurring can simplify the task of retrieving an answer by constraining the subset of documents in which an answer is to be found. An algorithm was presented to recognize the occurrence of clarification dialogue and is shown to have a good performance. The major limitation of our algorithm is the fact that it only considers series of questions, not series of answers. As noted above, it is often necessary to look at an answer to a question to determine whether the current question is a clarification question or not. Our sentence similarity algorithm was limited by the number of semantic relationships in WordNet: for example, a big improvement would come from the use of noun-verb relationships. Future work will be directed on extending WordNet in this direction and in providing other useful semantic relationships. Work also needs to be done on using information given by answers, not just questions in recognizing clarification dialogue and on coping with the cases in which clarification dialogue recognition is not enough to retrieve an answer and where other, more complex, techniques need to be used. It would also be beneficial to examine the use of a similarity function in which similarity decayed in function of the distance in time between the current question and the past questions.</abstract>
<note confidence="0.887701333333333">References Ardissono, L. and Sestero, D. 1996. &amp;quot;Using dynamic user models in the recognition of the plans of the Modeling and User-Adapted 5(2):157-190. 2003. Word Frequency</note>
<web confidence="0.601563">http://www.eecs.umich.edu/~qstout/586/bncfreq.html</web>
<note confidence="0.965372428571429">(last accessed March 2003). Budanitsky, A., and Hirst, G. 2001. “Semantic distance in WordNet: and experimental, application-oriented of five measures”, in of the NAACL 2001 Workshop on WordNet and other Pittsburgh. De Boni, M. and Manandhar, S. 2003. “The Use of</note>
<title confidence="0.984004666666667">Sentence Similarity as a Semantic Relevance Metric Question Answering”. of the AAAI Symposium on New Directions in Question</title>
<note confidence="0.972990277777778">Stanford. De Boni, M. and Manandhar, S. 2002. “Automated Discovery of Telic Relations for WordNet”. Proceedings of the First International WordNet India. C. 1998. An electronic Lexical MIT Press. Ginzburg , J. 1998. &amp;quot;Clarifying Utterances&amp;quot; In: J. and A. Nijholt (eds.) of the 2nd Workshop on the Formal Semantics and Pragmatics Twente. and Sag, 2000. CSLI. S. J. 1997. generating hypertext computing semantic Technical Report n. 366, University of Toronto. Harabagiu, S., Miller, A. G., Moldovan, D. 1999. “WordNet2 a morphologically and semantically</note>
<author confidence="0.367939">In of resource”</author>
<affiliation confidence="0.982697">University of Maryland.</affiliation>
<address confidence="0.831115">Harabagiu, S., et al. 2002. “Answering Complex, List</address>
<affiliation confidence="0.7564455">and Context Questions with LCC’s Question- Proceedings of NIST.</affiliation>
<address confidence="0.471582">Hirst, G., and St-Onge, D. 1998. “Lexical chains as</address>
<abstract confidence="0.958745571428571">representations of context for the detection and correction of malapropisms”, in Fellbaum (ed.), and electronic lexical MIT Press. Jiang, J. J., and Conrath, D. W. 1997. “Semantic similarity based on corpus statistics and lexical in of Taiwan.</abstract>
<author confidence="0.298832">G G Lee</author>
<title confidence="0.686941">Performance QA System Using Lexico-Semantic Matching and Shallow NLP”, of</title>
<note confidence="0.46141448">NIST. Lin, D. 1998. “An information-theoretic definition of in of the 15th International on Machine Madison. Mihalcea, R. and Moldovan, D. 1999. “A Method for Word Sense Disambiguation of Unrestricted Text”, of ACL Maryland, NY. Miller, G. A. 1999. “WordNet: A Lexical Database”, of the 38 (11). Moldovan, D. and Rus, V. 2001. “Logic Form Transformation of WordNet and its Applicability to Answering”, of the 39th of Toulouse. Resnik, P. 1995. “Using information content to evaluate similarity”, in of the 14th Montreal. Soubbotin, M. M. 2002. :“Patterns of Potential Answer Expressions as Clues to the Right Answers”, of NIST. van Beek, P., Cohen, R. and Schmidt, K., 1993. “From plan critiquing to clarification dialogue for response generation”, Voorhees, E. 2002. “Overview of the TREC 2001 Answering Track”, of TREC- NIST.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Ardissono</author>
<author>D Sestero</author>
</authors>
<title>Using dynamic user models in the recognition of the plans of the user&amp;quot;. User Modeling and User-Adapted Interaction,</title>
<date>1996</date>
<pages>5--2</pages>
<contexts>
<context position="2736" citStr="Ardissono and Sestero 1996" startWordPosition="410" endWordPosition="413">oner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not be entirely satisfactory from a theoretical linguistic point of view). While a number of researchers have looked at clarification dialogue from a theoretical point of view (e.g. Ginzburg 1998; Ginzburg and Sag 2000; van Beek at al. 1993), or from the point of view of task oriented dialogue within a narrow domain (e.g. Ardissono and Sestero 1996), we are not aware of any work on clarification dialogue for open domain question answering systems such as the ones presented at the TREC workshops, apart from the experiments carried out for the (subsequently abandoned) “context” task in the TREC-10 QA workshop (Voorhees 2002; Harabagiu et al. 2002). Here we seek to partially address this problem by looking at some particular aspect of clarification dialogues in the context of open domain question answering. In particular, we examine the problem of recognizing that a clarification dialogue is occurring, i.e. how to recognize that the current</context>
</contexts>
<marker>Ardissono, Sestero, 1996</marker>
<rawString>Ardissono, L. and Sestero, D. 1996. &amp;quot;Using dynamic user models in the recognition of the plans of the user&amp;quot;. User Modeling and User-Adapted Interaction, 5(2):157-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BNCFreq</author>
</authors>
<title>English Word Frequency List. http://www.eecs.umich.edu/~qstout/586/bncfreq.html (last accessed</title>
<date>2003</date>
<contexts>
<context position="17062" citStr="BNCFreq 2003" startWordPosition="2719" endWordPosition="2720">tatheoretical reasoning is common in philosophy” and “metatheoretical arguments are common in philosophy”, the word “metatheoretical” should be considered more important in determining relevance than the words “common”, “philosophy” and “is” as it is much more rare and therefore less probably found in irrelevant sentences. Word frequency data was taken from the Given that the questions examined were generic queries which did not necessarily refer to a specific set of documents, the word frequency for individual words was taken to be the word frequency given in the British National Corpus (see BNCFreq 2003). The top 100 words, making up 43% of the English Language, were then used as stop-words and were not used in calculating semantic similarity. 7 Results An implementation of the algorithm was evaluated on the TREC context questions used to develop the algorithm and then on the collection of 500 new clarification dialogue questions. The results on the TREC data, which was used to develop the algorithm, were as follows (see below for discussion and an explanation of each method): TREC Meth.0 Meth.1 Meth.2 Meth.3a Meth.3b New 90 90 90 60 80 Clarif. 47 53 59 78 72 Where “New” indicates the ability</context>
</contexts>
<marker>BNCFreq, 2003</marker>
<rawString>BNCFreq. 2003. English Word Frequency List. http://www.eecs.umich.edu/~qstout/586/bncfreq.html (last accessed March 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Semantic distance in WordNet: and experimental, application-oriented evaluation of five measures”,</title>
<date>2001</date>
<booktitle>in Proceedings of the NAACL 2001 Workshop on WordNet and other lexical resources,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="11076" citStr="Budanitsky and Hirst 2001" startWordPosition="1728" endWordPosition="1731">ous questions. WordNet (Miller 1999; Fellbaum 1998), a lexical database which organizes words into synsets, sets of synonymous words, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either using all WordNet relations (Budanitsky and Hirst 2001) or only is-a relations (Resnik 1995; Jiang and Conrath 1997; Mihalcea and Moldvoan 1999). Miller (1999), Harabagiu et al. (2002) and De Boni and Manandhar (2002) found WordNet glosses, considered as micro-contexts, to be useful in determining conceptual similarity. (Lee et al. 2002) have applied conceptual similarity to the Question Answering task, giving an answer A a score dependent on the number of matching terms in A and the question. Our sentence similarity measure followed on these ideas, adding to the use of WordNet relat</context>
</contexts>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>Budanitsky, A., and Hirst, G. 2001. “Semantic distance in WordNet: and experimental, application-oriented evaluation of five measures”, in Proceedings of the NAACL 2001 Workshop on WordNet and other lexical resources, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M De Boni</author>
<author>S Manandhar</author>
</authors>
<title>The Use of Sentence Similarity as a Semantic Relevance Metric for Question Answering”.</title>
<date>2003</date>
<booktitle>Proceedings of the AAAI Symposium on New Directions in Question Answering,</booktitle>
<location>Stanford.</location>
<marker>De Boni, Manandhar, 2003</marker>
<rawString>De Boni, M. and Manandhar, S. 2003. “The Use of Sentence Similarity as a Semantic Relevance Metric for Question Answering”. Proceedings of the AAAI Symposium on New Directions in Question Answering, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M De Boni</author>
<author>S Manandhar</author>
</authors>
<title>Automated Discovery of Telic Relations for WordNet”.</title>
<date>2002</date>
<booktitle>Proceedings of the First International WordNet Conference,</booktitle>
<marker>De Boni, Manandhar, 2002</marker>
<rawString>De Boni, M. and Manandhar, S. 2002. “Automated Discovery of Telic Relations for WordNet”. Proceedings of the First International WordNet Conference, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet, An electronic Lexical Database,</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10501" citStr="Fellbaum 1998" startWordPosition="1641" endWordPosition="1642">milarity between the current question and the previously asked questions. In other words: Clarification_Question (qn,q-1..q-n) is true if 1. q0 has pronoun and possessive adjective references to q-1..q-n 2. q0 does not contain any verbs 3. q0 has repetition of common or proper nouns in q-1..q-n or q0 has a strong semantic similarity to some q e q-1..q-n 6 Sentence Similarity Metric A major part of our clarification dialogue recognition algorithm is the sentence similarity metric which looks at the similarity in meaning between the current question and previous questions. WordNet (Miller 1999; Fellbaum 1998), a lexical database which organizes words into synsets, sets of synonymous words, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either using all WordN</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. 1998. WordNet, An electronic Lexical Database, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ginzburg</author>
</authors>
<title>Clarifying Utterances&amp;quot; In:</title>
<date>1998</date>
<booktitle>Proceedings of the 2nd Workshop on the Formal Semantics and Pragmatics of Dialogue,</booktitle>
<editor>J. Hulstijn and A. Nijholt (eds.)</editor>
<location>Twente.</location>
<contexts>
<context position="2580" citStr="Ginzburg 1998" startWordPosition="384" endWordPosition="385">fication dialogue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not be entirely satisfactory from a theoretical linguistic point of view). While a number of researchers have looked at clarification dialogue from a theoretical point of view (e.g. Ginzburg 1998; Ginzburg and Sag 2000; van Beek at al. 1993), or from the point of view of task oriented dialogue within a narrow domain (e.g. Ardissono and Sestero 1996), we are not aware of any work on clarification dialogue for open domain question answering systems such as the ones presented at the TREC workshops, apart from the experiments carried out for the (subsequently abandoned) “context” task in the TREC-10 QA workshop (Voorhees 2002; Harabagiu et al. 2002). Here we seek to partially address this problem by looking at some particular aspect of clarification dialogues in the context of open domain</context>
<context position="9204" citStr="Ginzburg 1998" startWordPosition="1433" endWordPosition="1434">Clarification in the table) 5 Clarification Recognition Algorithm Our approach to clarification dialogue recognition looks at certain features of the question currently under consideration (e.g. pronouns and proper nouns) and compares the meaning of the current question with the meanings of previous questions to determine whether they are “about” the same matter. Given a question q0 and n previously asked questions q-1..q-n we have a function Clarification_Question which is true if a question is considered a clarification of a previously asked question. In the light of empirical work such as (Ginzburg 1998), which indicates that questioners do not usually refer back to questions which are very distant, we only considered the set of the previously mentioned 10 questions. A question is deemed to be a clarification of a previous question if: 1. There are direct references to nouns mentioned in the previous n questions through the use of pronouns (he, she, it, ...) or possessive adjectives (his, her, its...) which have no references in the current question. 2. The question does not contain any verbs 3. There are explicit references to proper and common nouns mentioned in the previous n questions, i.</context>
</contexts>
<marker>Ginzburg, 1998</marker>
<rawString>Ginzburg , J. 1998. &amp;quot;Clarifying Utterances&amp;quot; In: J. Hulstijn and A. Nijholt (eds.) Proceedings of the 2nd Workshop on the Formal Semantics and Pragmatics of Dialogue, Twente.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ginzburg</author>
<author>Sag</author>
</authors>
<date>2000</date>
<journal>Interrogative Investigations, CSLI.</journal>
<contexts>
<context position="2603" citStr="Ginzburg and Sag 2000" startWordPosition="386" endWordPosition="389">ue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not be entirely satisfactory from a theoretical linguistic point of view). While a number of researchers have looked at clarification dialogue from a theoretical point of view (e.g. Ginzburg 1998; Ginzburg and Sag 2000; van Beek at al. 1993), or from the point of view of task oriented dialogue within a narrow domain (e.g. Ardissono and Sestero 1996), we are not aware of any work on clarification dialogue for open domain question answering systems such as the ones presented at the TREC workshops, apart from the experiments carried out for the (subsequently abandoned) “context” task in the TREC-10 QA workshop (Voorhees 2002; Harabagiu et al. 2002). Here we seek to partially address this problem by looking at some particular aspect of clarification dialogues in the context of open domain question answering. In</context>
</contexts>
<marker>Ginzburg, Sag, 2000</marker>
<rawString>Ginzburg and Sag, 2000. Interrogative Investigations, CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Green</author>
</authors>
<title>Automatically generating hypertext by computing semantic similarity,</title>
<date>1997</date>
<tech>Technical Report n. 366,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="10956" citStr="Green 1997" startWordPosition="1712" endWordPosition="1713">ntence similarity metric which looks at the similarity in meaning between the current question and previous questions. WordNet (Miller 1999; Fellbaum 1998), a lexical database which organizes words into synsets, sets of synonymous words, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either using all WordNet relations (Budanitsky and Hirst 2001) or only is-a relations (Resnik 1995; Jiang and Conrath 1997; Mihalcea and Moldvoan 1999). Miller (1999), Harabagiu et al. (2002) and De Boni and Manandhar (2002) found WordNet glosses, considered as micro-contexts, to be useful in determining conceptual similarity. (Lee et al. 2002) have applied conceptual similarity to the Question Answering task, giving an answer A a score dependent on the number of matching </context>
</contexts>
<marker>Green, 1997</marker>
<rawString>Green, S. J. 1997. Automatically generating hypertext by computing semantic similarity, Technical Report n. 366, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A G Miller</author>
<author>D Moldovan</author>
</authors>
<title>WordNet2 - a morphologically and semantically enhanced resource”,</title>
<date>1999</date>
<booktitle>In Proceedings of SIGLEX-99,</booktitle>
<institution>University of Maryland.</institution>
<contexts>
<context position="1870" citStr="Harabagiu et al. 1999" startWordPosition="274" endWordPosition="277"> Harabagiu et al. 2002), systems narrow down the search by using information retrieval techniques to select a subset of documents, or paragraphs within documents, containing keywords from the question and a concept which corresponds to the correct question type (e.g. a question starting with the word “Who?” would require an answer containing a person). The exact answer sentence is then sought by either attempting to unify the answer semantically with the question, through some kind of logical transformation (e.g. Moldovan and Rus 2001) or by some form of pattern matching (e.g. Soubbotin 2002; Harabagiu et al. 1999). Often, though, a single question is not enough to meet user’s goals and an elaboration or clarification dialogue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not be entirely satisfactory from a theoretical linguistic point of view). While a nu</context>
</contexts>
<marker>Harabagiu, Miller, Moldovan, 1999</marker>
<rawString>Harabagiu, S., Miller, A. G., Moldovan, D. 1999. “WordNet2 - a morphologically and semantically enhanced resource”, In Proceedings of SIGLEX-99, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
</authors>
<title>Answering Complex, List and Context Questions with LCC’s QuestionAnswering Server”,</title>
<date>2002</date>
<booktitle>Proceedings of TREC-10,</booktitle>
<pages>NIST.</pages>
<marker>Harabagiu, 2002</marker>
<rawString>Harabagiu, S., et al. 2002. “Answering Complex, List and Context Questions with LCC’s QuestionAnswering Server”, Proceedings of TREC-10, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms”,</title>
<date>1998</date>
<editor>in Fellbaum (ed.),</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10984" citStr="Hirst and St-Onge 1998" startWordPosition="1715" endWordPosition="1718">ty metric which looks at the similarity in meaning between the current question and previous questions. WordNet (Miller 1999; Fellbaum 1998), a lexical database which organizes words into synsets, sets of synonymous words, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either using all WordNet relations (Budanitsky and Hirst 2001) or only is-a relations (Resnik 1995; Jiang and Conrath 1997; Mihalcea and Moldvoan 1999). Miller (1999), Harabagiu et al. (2002) and De Boni and Manandhar (2002) found WordNet glosses, considered as micro-contexts, to be useful in determining conceptual similarity. (Lee et al. 2002) have applied conceptual similarity to the Question Answering task, giving an answer A a score dependent on the number of matching terms in A and the question.</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Hirst, G., and St-Onge, D. 1998. “Lexical chains as representations of context for the detection and correction of malapropisms”, in Fellbaum (ed.), WordNet: and electronic lexical database, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy”,</title>
<date>1997</date>
<booktitle>in Proceedings of ICRCL,</booktitle>
<contexts>
<context position="11201" citStr="Jiang and Conrath 1997" startWordPosition="1747" endWordPosition="1750">rds, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either using all WordNet relations (Budanitsky and Hirst 2001) or only is-a relations (Resnik 1995; Jiang and Conrath 1997; Mihalcea and Moldvoan 1999). Miller (1999), Harabagiu et al. (2002) and De Boni and Manandhar (2002) found WordNet glosses, considered as micro-contexts, to be useful in determining conceptual similarity. (Lee et al. 2002) have applied conceptual similarity to the Question Answering task, giving an answer A a score dependent on the number of matching terms in A and the question. Our sentence similarity measure followed on these ideas, adding to the use of WordNet relations, part-ofspeech information, compound noun and word frequency information. In particular, sentence similarity was conside</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jiang, J. J., and Conrath, D. W. 1997. “Semantic similarity based on corpus statistics and lexical taxonomy”, in Proceedings of ICRCL, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G G Lee</author>
</authors>
<title>SiteQ: Engineering High Performance QA System Using Lexico-Semantic Pattern Matching and Shallow NLP”,</title>
<date>2002</date>
<booktitle>Proceedings of TREC-10,</booktitle>
<pages>NIST.</pages>
<marker>Lee, 2002</marker>
<rawString>Lee, G. G., et al. 2002. “SiteQ: Engineering High Performance QA System Using Lexico-Semantic Pattern Matching and Shallow NLP”, Proceedings of TREC-10, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity”,</title>
<date>1998</date>
<booktitle>in Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<location>Madison.</location>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. “An information-theoretic definition of similarity”, in Proceedings of the 15th International Conference on Machine Learning, Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>D Moldovan</author>
</authors>
<title>A Method for Word Sense Disambiguation of Unrestricted Text”,</title>
<date>1999</date>
<booktitle>in Proceedings of ACL ‘99,</booktitle>
<location>Maryland, NY.</location>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Mihalcea, R. and Moldovan, D. 1999. “A Method for Word Sense Disambiguation of Unrestricted Text”, in Proceedings of ACL ‘99, Maryland, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: A Lexical Database”,</title>
<date>1999</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="10485" citStr="Miller 1999" startWordPosition="1639" endWordPosition="1640">g sentence similarity between the current question and the previously asked questions. In other words: Clarification_Question (qn,q-1..q-n) is true if 1. q0 has pronoun and possessive adjective references to q-1..q-n 2. q0 does not contain any verbs 3. q0 has repetition of common or proper nouns in q-1..q-n or q0 has a strong semantic similarity to some q e q-1..q-n 6 Sentence Similarity Metric A major part of our clarification dialogue recognition algorithm is the sentence similarity metric which looks at the similarity in meaning between the current question and previous questions. WordNet (Miller 1999; Fellbaum 1998), a lexical database which organizes words into synsets, sets of synonymous words, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either</context>
</contexts>
<marker>Miller, 1999</marker>
<rawString>Miller, G. A. 1999. “WordNet: A Lexical Database”, Communications of the ACM, 38 (11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>V Rus</author>
</authors>
<title>Logic Form Transformation of WordNet and its Applicability to Question Answering”,</title>
<date>2001</date>
<booktitle>Proceedings of the 39th conference of ACL,</booktitle>
<location>Toulouse.</location>
<contexts>
<context position="1789" citStr="Moldovan and Rus 2001" startWordPosition="260" endWordPosition="263">02 for an overview of current systems). In order to achieve this (see for example Harabagiu et al. 2002), systems narrow down the search by using information retrieval techniques to select a subset of documents, or paragraphs within documents, containing keywords from the question and a concept which corresponds to the correct question type (e.g. a question starting with the word “Who?” would require an answer containing a person). The exact answer sentence is then sought by either attempting to unify the answer semantically with the question, through some kind of logical transformation (e.g. Moldovan and Rus 2001) or by some form of pattern matching (e.g. Soubbotin 2002; Harabagiu et al. 1999). Often, though, a single question is not enough to meet user’s goals and an elaboration or clarification dialogue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not </context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>Moldovan, D. and Rus, V. 2001. “Logic Form Transformation of WordNet and its Applicability to Question Answering”, Proceedings of the 39th conference of ACL, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity”,</title>
<date>1995</date>
<booktitle>in Proceedings of the 14th IJCAI,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="11177" citStr="Resnik 1995" startWordPosition="1745" endWordPosition="1746">synonymous words, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity. One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example Green 1997 and Hirst and St-Onge 1998); different approaches have been proposed (for an evaluation see (Budanitsky and Hirst 2001)), either using all WordNet relations (Budanitsky and Hirst 2001) or only is-a relations (Resnik 1995; Jiang and Conrath 1997; Mihalcea and Moldvoan 1999). Miller (1999), Harabagiu et al. (2002) and De Boni and Manandhar (2002) found WordNet glosses, considered as micro-contexts, to be useful in determining conceptual similarity. (Lee et al. 2002) have applied conceptual similarity to the Question Answering task, giving an answer A a score dependent on the number of matching terms in A and the question. Our sentence similarity measure followed on these ideas, adding to the use of WordNet relations, part-ofspeech information, compound noun and word frequency information. In particular, sentenc</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik, P. 1995. “Using information content to evaluate semantic similarity”, in Proceedings of the 14th IJCAI, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Soubbotin</author>
</authors>
<title>Patterns of Potential Answer Expressions as Clues to the Right Answers”,</title>
<date>2002</date>
<booktitle>Proceedings of TREC-10,</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="1846" citStr="Soubbotin 2002" startWordPosition="272" endWordPosition="273">(see for example Harabagiu et al. 2002), systems narrow down the search by using information retrieval techniques to select a subset of documents, or paragraphs within documents, containing keywords from the question and a concept which corresponds to the correct question type (e.g. a question starting with the word “Who?” would require an answer containing a person). The exact answer sentence is then sought by either attempting to unify the answer semantically with the question, through some kind of logical transformation (e.g. Moldovan and Rus 2001) or by some form of pattern matching (e.g. Soubbotin 2002; Harabagiu et al. 1999). Often, though, a single question is not enough to meet user’s goals and an elaboration or clarification dialogue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not be entirely satisfactory from a theoretical linguistic po</context>
</contexts>
<marker>Soubbotin, 2002</marker>
<rawString>Soubbotin, M. M. 2002. :“Patterns of Potential Answer Expressions as Clues to the Right Answers”, Proceedings of TREC-10, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P van Beek</author>
<author>R Cohen</author>
<author>K Schmidt</author>
</authors>
<title>From plan critiquing to clarification dialogue for cooperative response generation”,</title>
<date>1993</date>
<journal>Computational Intelligence</journal>
<pages>9--132</pages>
<marker>van Beek, Cohen, Schmidt, 1993</marker>
<rawString>van Beek, P., Cohen, R. and Schmidt, K., 1993. “From plan critiquing to clarification dialogue for cooperative response generation”, Computational Intelligence 9:132-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>Proceedings of TREC10,</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="1169" citStr="Voorhees 2002" startWordPosition="165" endWordPosition="166"> Question Answering systems. We develop an algorithm for clarification dialogue recognition through the analysis of collected data on clarification dialogues and examine the importance of clarification dialogue recognition for question answering. The algorithm is evaluated and shown to successfully recognize the occurrence of clarification dialogue in the majority of cases and to simplify the task of answer retrieval. 1 Clarification dialogues in Question Answering Question Answering Systems aim to determine an answer to a question by searching for a response in a collection of documents (see Voorhees 2002 for an overview of current systems). In order to achieve this (see for example Harabagiu et al. 2002), systems narrow down the search by using information retrieval techniques to select a subset of documents, or paragraphs within documents, containing keywords from the question and a concept which corresponds to the correct question type (e.g. a question starting with the word “Who?” would require an answer containing a person). The exact answer sentence is then sought by either attempting to unify the answer semantically with the question, through some kind of logical transformation (e.g. Mo</context>
<context position="3014" citStr="Voorhees 2002" startWordPosition="456" endWordPosition="457">satisfactory from a theoretical linguistic point of view). While a number of researchers have looked at clarification dialogue from a theoretical point of view (e.g. Ginzburg 1998; Ginzburg and Sag 2000; van Beek at al. 1993), or from the point of view of task oriented dialogue within a narrow domain (e.g. Ardissono and Sestero 1996), we are not aware of any work on clarification dialogue for open domain question answering systems such as the ones presented at the TREC workshops, apart from the experiments carried out for the (subsequently abandoned) “context” task in the TREC-10 QA workshop (Voorhees 2002; Harabagiu et al. 2002). Here we seek to partially address this problem by looking at some particular aspect of clarification dialogues in the context of open domain question answering. In particular, we examine the problem of recognizing that a clarification dialogue is occurring, i.e. how to recognize that the current question under consideration is part of a previous series (i.e. clarifying previous questions) or the start of a new series; we then show how the recognition that a clarification dialogue is occurring can simplify the problem of answer retrieval. 2 The TREC Context Experiments</context>
<context position="27298" citStr="Voorhees 2002" startWordPosition="4401" endWordPosition="4402"> larger clarification dialogue (a slight deviation from the main topic which was being investigated by the questioner) and could be answered by looking at the documents retrieved for the first question in the mini-series. Recognizing that a clarification dialogue is occurring therefore can simplify the task of retrieving an answer by specifying that an answer must be in the set of documents used the previous questions. This is Fig. 1: Search technique used for Question First Q in series Words in Q Coreference Mini-clarification Other consistent with the results found in the TREC context task (Voorhees 2002), which indicated that systems were capable of finding most answers to questions in a context dialogue simply by looking at the documents retrieved for the initial question in a series. As in the case of clarification dialogue recognition, therefore, simple techniques can resolve the majority of cases; nevertheless, a full solution to the problem requires more complex methods. The last case indicates that it is not enough simply to look at the documents provided by the first question in a series in order to seek an answer: it is necessary to use the documents found for a previously asked quest</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>Voorhees, E. 2002. “Overview of the TREC 2001 Question Answering Track”, Proceedings of TREC10, NIST.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>