<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001957">
<title confidence="0.995589">
Better Filtration and Augmentation for Hierarchical Phrase-Based
Translation Rules
</title>
<author confidence="0.997362">
Zhiyang Wang † Yajuan L¨u † Qun Liu † Young-Sook Hwang ‡
</author>
<affiliation confidence="0.916532">
†Key Lab. of Intelligent Information Processing ‡HILab Convergence Technology Center
Institute of Computing Technology C&amp;I Business
Chinese Academy of Sciences SKTelecom
</affiliation>
<address confidence="0.727599">
P.O. Box 2704, Beijing 100190, China 11, Euljiro2-ga, Jung-gu, Seoul 100-999, Korea
</address>
<email confidence="0.99825">
wangzhiyang@ict.ac.cn yshwang@sktelecom.com
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998324">
This paper presents a novel filtration cri-
terion to restrict the rule extraction for
the hierarchical phrase-based translation
model, where a bilingual but relaxed well-
formed dependency restriction is used to
filter out bad rules. Furthermore, a new
feature which describes the regularity that
the source/target dependency edge trig-
gers the target/source word is also pro-
posed. Experimental results show that, the
new criteria weeds out about 40% rules
while with translation performance im-
provement, and the new feature brings an-
other improvement to the baseline system,
especially on larger corpus.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.966736577777778">
Hierarchical phrase-based (HPB) model (Chiang,
2005) is the state-of-the-art statistical machine
translation (SMT) model. By looking for phrases
that contain other phrases and replacing the sub-
phrases with nonterminal symbols, it gets hierar-
chical rules. Hierarchical rules are more powerful
than conventional phrases since they have better
generalization capability and could capture long
distance reordering. However, when the train-
ing corpus becomes larger, the number of rules
will grow exponentially, which inevitably results
in slow and memory-consuming decoding.
In this paper, we address the problem of reduc-
ing the hierarchical translation rule table resorting
to the dependency information of bilingual lan-
guages. We only keep rules that both sides are
relaxed-well-formed (RWF) dependency structure
(see the definition in Section 3), and discard others
which do not satisfy this constraint. In this way,
about 40% bad rules are weeded out from the orig-
inal rule table. However, the performance is even
better than the traditional HPB translation system.
Figure 1: Solid wire reveals the dependency rela-
tion pointing from the child to the parent. Target
word e is triggered by the source word f and it’s
head word f′, p(e|f — f′).
Based on the relaxed-well-formed dependency
structure, we also introduce a new linguistic fea-
ture to enhance translation performance. In the
traditional phrase-based SMT model, there are
always lexical translation probabilities based on
IBM model 1 (Brown et al., 1993), i.e. p(e|f),
namely, the target word e is triggered by the source
word f. Intuitively, however, the generation of e
is not only involved with f, sometimes may also
be triggered by other context words in the source
side. Here we assume that the dependency edge
(f —* f′) of word f generates target word e (we
call it head word trigger in Section 4). Therefore,
two words in one language trigger one word in
another, which provides a more sophisticated and
better choice for the target word, i.e. Figure 1.
Similarly, the dependency feature works well in
Chinese-to-English translation task, especially on
large corpus.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999798111111111">
In the past, a significant number of techniques
have been presented to reduce the hierarchical rule
table. He et al. (2009) just used the key phrases
of source side to filter the rule table without taking
advantage of any linguistic information. Iglesias
et al. (2009) put rules into syntactic classes based
on the number of non-terminals and patterns, and
applied various filtration strategies to improve the
rule table quality. Shen et al. (2008) discarded
</bodyText>
<figure confidence="0.964547">
e
f f’
Source
Target
</figure>
<page confidence="0.940273">
142
</page>
<note confidence="0.8298585">
Proceedings of the ACL 2010 Conference Short Papers, pages 142–146,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.946965">
Figure 2: An example of dependency tree. The
corresponding plain sentence is The lovely girl
found a beautiful house.
</figureCaption>
<bodyText confidence="0.999954066666667">
most entries of the rule table by using the con-
straint that rules of the target-side are well-formed
(WF) dependency structure, but this filtering led to
degradation in translation performance. They ob-
tained improvements by adding an additional de-
pendency language model. The basic difference
of our method from (Shen et al., 2008) is that we
keep rules that both sides should be relaxed-well-
formed dependency structure, not just the target
side. Besides, our system complexity is not in-
creased because no additional language model is
introduced.
The feature of head word trigger which we ap-
ply to the log-linear model is motivated by the
trigger-based approach (Hasan and Ney, 2009).
Hasan and Ney (2009) introduced a second word
to trigger the target word without considering any
linguistic information. Furthermore, since the sec-
ond word can come from any part of the sentence,
there may be a prohibitively large number of pa-
rameters involved. Besides, He et al. (2008) built
a maximum entropy model which combines rich
context information for selecting translation rules
during decoding. However, as the size of the cor-
pus increases, the maximum entropy model will
become larger. Similarly, In (Shen et al., 2009),
context language model is proposed for better rule
selection. Taking the dependency edge as condi-
tion, our approach is very different from previous
approaches of exploring context information.
</bodyText>
<sectionHeader confidence="0.9231745" genericHeader="method">
3 Relaxed-well-formed Dependency
Structure
</sectionHeader>
<bodyText confidence="0.99972624">
Dependency models have recently gained consid-
erable interest in SMT (Ding and Palmer, 2005;
Quirk et al., 2005; Shen et al., 2008). Depen-
dency tree can represent richer structural infor-
mation. It reveals long-distance relation between
words and directly models the semantic structure
of a sentence without any constituent labels. Fig-
ure 2 shows an example of a dependency tree. In
this example, the word found is the root of the tree.
Shen et al. (2008) propose the well-formed de-
pendency structure to filter the hierarchical rule ta-
ble. A well-formed dependency structure could be
either a single-rooted dependency tree or a set of
sibling trees. Although most rules are discarded
with the constraint that the target side should be
well-formed, this filtration leads to degradation in
translation performance.
As an extension of the work of (Shen et
al., 2008), we introduce the so-called relaxed-
well-formed dependency structure to filter the hi-
erarchical rule table. Given a sentence 5 =
w1w2...wn. Let d1d2...dn represent the position of
parent word for each word. For example, d3 = 4
means that w3 depends on w4. If wi is a root, we
define di = −1.
</bodyText>
<listItem confidence="0.954582625">
Definition A dependency structure wi...wj is
a relaxed-well-formed structure, where there is
h E� [i, j], all the words wi...wj are directly or
indirectly depended on wh or -1 (here we define
h = −1). If and only if it satisfies the following
conditions
• dh E� [i,j]
• bk E [i, j], dk E [i, j] or dk = h
</listItem>
<bodyText confidence="0.999826777777778">
From the definition above, we can see that
the relaxed-well-formed structure obviously cov-
ers the well-formed one. In this structure, we
don’t constrain that all the children of the sub-root
should be complete. Let’s review the dependency
tree in Figure 2 as an example. Except for the well-
formed structure, we could also extract girl found
a beautiful house. Therefore, if the modifier The
lovely changes to The cute, this rule also works.
</bodyText>
<sectionHeader confidence="0.987157" genericHeader="method">
4 Head Word Trigger
</sectionHeader>
<bodyText confidence="0.999790916666667">
(Koehn et al., 2003) introduced the concept of
lexical weighting to check how well words of
the phrase translate to each other. Source word
f aligns with target word e, according to the
IBM model 1, the lexical translation probability
is p(e|f). However, in the sense of dependency
relationship, we believe that the generation of the
target word e, is not only triggered by the aligned
source word f, but also associated with f ’s head
word f′. Therefore, the lexical translation prob-
ability becomes p(e|f —* f′), which of course
allows for a more fine-grained lexical choice of
</bodyText>
<figure confidence="0.9845145">
The
girl
a beautiful
lovely
found
house
</figure>
<page confidence="0.995409">
143
</page>
<bodyText confidence="0.968713416666667">
the target word. More specifically, the probabil-
ity could be estimated by the maximum likelihood
(MLE) approach,
count(e, f — f′) (1 )
p(e |f — f′) = re′ count(e′, f — f′)
Given a phrase pair f , e and word alignment
a, and the dependent relation of the source sen-
tence di (J is the length of the source sentence,
I is the length of the target sentence). Therefore,
given the lexical translation probability distribu-
tion p(e|f — f′), we compute the feature score of
a phrase pair (f , e) as
</bodyText>
<equation confidence="0.965658">
p(e|f, di , a)
Π�e |1|{ 1 p(ei  |fj — fd,) (2)
�I(� ,i) E a}|
</equation>
<bodyText confidence="0.99994">
Now we get p(e|f, di , a), we could obtain
p(f|e, di, a) (di represents dependent relation of
the target side) in the similar way. This new fea-
ture can be easily integrated into the log-linear
model as lexical weighting does.
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999515">
In this section, we describe the experimental set-
ting used in this work, and verify the effect of
the relaxed-well-formed structure filtering and the
new feature, head word trigger.
</bodyText>
<subsectionHeader confidence="0.928897">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.996671666666667">
Experiments are carried out on the NIST1
Chinese-English translation task with two differ-
ent size of training corpora.
</bodyText>
<listItem confidence="0.992835777777778">
• FBIS: We use the FBIS corpus as the first
training corpus, which contains 239K sen-
tence pairs with 6.9M Chinese words and
8.9M English words.
• GQ: This is manually selected from the
LDC2 corpora. GQ contains 1.5M sentence
pairs with 41M Chinese words and 48M En-
glish words. In fact, FBIS is the subset of
GQ.
</listItem>
<footnote confidence="0.64639725">
1www.nist.gov/speech/tests/mt
2It consists of six LDC corpora:
LDC2002E18, LDC2003E07, LDC2003E14, Hansards part
of LDC2004T07, LDC2004T08, LDC2005T06.
</footnote>
<bodyText confidence="0.99917375">
For language model, we use the SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram model on the first 1/3 of the Xinhua portion
of GIGAWORD corpus. And we use the NIST
2002 MT evaluation test set as our development
set, and NIST 2004, 2005 test sets as our blind
test sets. We evaluate the translation quality us-
ing case-insensitive BLEU metric (Papineni et
al., 2002) without dropping OOV words, and the
feature weights are tuned by minimum error rate
training (Och, 2003).
In order to get the dependency relation of the
training corpus, we re-implement a beam-search
style monolingual dependency parser according
to (Nivre and Scholz, 2004). Then we use the
same method suggested in (Chiang, 2005) to
extract SCFG grammar rules within dependency
constraint on both sides except that unaligned
words are allowed at the edge of phrases. Pa-
rameters of head word trigger are estimated as de-
scribed in Section 4. As a default, the maximum
initial phrase length is set to 10 and the maximum
rule length of the source side is set to 5. Besides,
we also re-implement the decoder of Hiero (Chi-
ang, 2007) as our baseline. In fact, we just exploit
the dependency structure during the rule extrac-
tion phase. Therefore, we don’t need to change
the main decoding algorithm of the SMT system.
</bodyText>
<subsectionHeader confidence="0.950323">
5.2 Results on FBIS Corpus
</subsectionHeader>
<bodyText confidence="0.976583565217391">
A series of experiments was done on the FBIS cor-
pus. We first parse the bilingual languages with
monolingual dependency parser respectively, and
then only retain the rules that both sides are in line
with the constraint of dependency structure. In
Table 1, the relaxed-well-formed structure filtered
out 35% of the rule table and the well-formed dis-
carded 74%. RWF extracts additional 39% com-
pared to WF, which can be seen as some kind
of evidence that the rules we additional get seem
common in the sense of linguistics. Compared to
(Shen et al., 2008), we just use the dependency
structure to constrain rules, not to maintain the tree
structures to guide decoding.
Table 2 shows the translation result on FBIS.
We can see that the RWF structure constraint can
improve translation quality substantially both at
development set and different test sets. On the
Test04 task, it gains +0.86% BLEU, and +0.84%
on Test05. Besides, we also used Shen et al.
(2008)’s WF structure to filter both sides. Al-
though it discard about 74% of the rule table, the
∀(j,i)∈a
</bodyText>
<page confidence="0.977964">
144
</page>
<table confidence="0.9781255">
System Rule table size
HPB 30,152,090
RWF 19,610,255
WF 7,742,031
</table>
<tableCaption confidence="0.8614432">
Table 1: Rule table size with different con-
straint on FBIS. Here HPB refers to the base-
line hierarchal phrase-based system, RWF means
relaxed-well-formed constraint and WF represents
the well-formed structure.
</tableCaption>
<table confidence="0.999892">
System Dev02 Test04 Test05
HPB 0.3285 0.3284 0.2965
WF 0.3125 0.3218 0.2887
RWF 0.3326 0.3370** 0.3050
RWF+Tri 0.3281 / 0.2965
</table>
<tableCaption confidence="0.996278">
Table 2: Results of FBIS corpus. Here Tri means
</tableCaption>
<bodyText confidence="0.980040461538461">
the feature of head word trigger on both sides. And
we don’t test the new feature on Test04 because of
the bad performance on development set. * or **
= significantly better than baseline (p &lt; 0.05 or
0.01, respectively).
over-all BLEU is decreased by 0.66%-0.78% on
the test sets.
As for the feature of head word trigger, it seems
not work on the FBIS corpus. On Test05, it gets
the same score with the baseline, but lower than
RWF filtering. This may be caused by the data
sparseness problem, which results in inaccurate
parameter estimation of the new feature.
</bodyText>
<subsectionHeader confidence="0.99648">
5.3 Result on GQ Corpus
</subsectionHeader>
<bodyText confidence="0.99958425">
In this part, we increased the size of the training
corpus to check whether the feature of head word
trigger works on large corpus.
We get 152M rule entries from the GQ corpus
according to (Chiang, 2007)’s extraction method.
If we use the RWF structure to constrain both
sides, the number of rules is 87M, about 43% of
rule entries are discarded. From Table 3, the new
</bodyText>
<table confidence="0.9992965">
System Dev02 Test04 Test05
HPB 0.3473 0.3386 0.3206
RWF 0.3539 0.3485** 0.3228
RWF+Tri 0.3540 0.3607** 0.3339*
</table>
<tableCaption confidence="0.999099">
Table 3: Results of GQ corpus. * or ** = sig-
</tableCaption>
<bodyText confidence="0.904267375">
nificantly better than baseline (p &lt; 0.05 or 0.01,
respectively).
feature works well on two different test sets. The
gain is +2.21% BLEU on Test04, and +1.33% on
Test05. Compared to the result of the baseline,
only using the RWF structure to filter performs the
same as the baseline on Test05, and +0.99% gains
on Test04.
</bodyText>
<sectionHeader confidence="0.996306" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999959">
This paper proposes a simple strategy to filter the
hierarchal rule table, and introduces a new feature
to enhance the translation performance. We em-
ploy the relaxed-well-formed dependency struc-
ture to constrain both sides of the rule, and about
40% of rules are discarded with improvement of
the translation performance. In order to make full
use of the dependency information, we assume
that the target word a is triggered by dependency
edge of the corresponding source word f. And
this feature works well on large parallel training
corpus.
How to estimate the probability of head word
trigger is very important. Here we only get the pa-
rameters in a generative way. In the future, we we
are plan to exploit some discriminative approach
to train parameters of this feature, such as EM al-
gorithm (Hasan et al., 2008) or maximum entropy
(He et al., 2008).
Besides, the quality of the parser is another ef-
fect for this method. As the next step, we will
try to exploit bilingual knowledge to improve the
monolingual parser, i.e. (Huang et al., 2009).
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999901">
This work was partly supported by National
Natural Science Foundation of China Contract
60873167. It was also funded by SK Telecom,
Korea under the contract 4360002953. We show
our special thanks to Wenbin Jiang and Shu Cai
for their valuable suggestions. We also thank
the anonymous reviewers for their insightful com-
ments.
</bodyText>
<sectionHeader confidence="0.999239" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992506857142857">
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263–
311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
</reference>
<page confidence="0.994369">
145
</page>
<reference confidence="0.99902368292683">
’05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263–
270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201–228.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In ACL ’05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 541–548.
Saˇsa Hasan and Hermann Ney. 2009. Comparison of
extended lexicon models in search and rescoring for
smt. In NAACL ’09: Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 17–20.
Saˇsa Hasan, Juri Ganitkevitch, Hermann Ney, and
Jes´us Andr´es-Ferrer. 2008. Triplet lexicon models
for statistical machine translation. In EMNLP ’08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 372–
381.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In COLING ’08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 321–328.
Zhongjun He, Yao Meng, Yajuan L¨u, Hao Yu, and Qun
Liu. 2009. Reducing smt rule table with monolin-
gual key phrase. In ACL-IJCNLP ’09: Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 121–124.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP ’09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1222–1231.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In EACL ’09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 380–388.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In COLING
’04: Proceedings of the 20th international confer-
ence on Computational Linguistics, pages 64–70.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ’03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ’02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311–318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In ACL ’05: Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 271–279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL-08: HLT, pages 577–585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In EMNLP ’09: Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 72–80.
Andreas Stolcke. 2002. Srilman extensible language
modeling toolkit. In In Proceedings of the 7th Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), pages 901–904.
</reference>
<page confidence="0.998773">
146
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805401">
<title confidence="0.9993775">Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</title>
<author confidence="0.993044">Wang L¨u Liu Hwang</author>
<affiliation confidence="0.9989765">Lab. of Intelligent Information Processing Convergence Technology Center Institute of Computing Technology C&amp;I Business</affiliation>
<address confidence="0.908517">Chinese Academy of Sciences SKTelecom P.O. Box 2704, Beijing 100190, China 11, Euljiro2-ga, Jung-gu, Seoul 100-999, Korea</address>
<email confidence="0.991463">wangzhiyang@ict.ac.cnyshwang@sktelecom.com</email>
<abstract confidence="0.9987165">This paper presents a novel filtration criterion to restrict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed wellformed dependency restriction is used to filter out bad rules. Furthermore, a new feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed. Experimental results show that, the new criteria weeds out about 40% rules while with translation performance improvement, and the new feature brings another improvement to the baseline system, especially on larger corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="2573" citStr="Brown et al., 1993" startWordPosition="376" endWordPosition="379">onstraint. In this way, about 40% bad rules are weeded out from the original rule table. However, the performance is even better than the traditional HPB translation system. Figure 1: Solid wire reveals the dependency relation pointing from the child to the parent. Target word e is triggered by the source word f and it’s head word f′, p(e|f — f′). Based on the relaxed-well-formed dependency structure, we also introduce a new linguistic feature to enhance translation performance. In the traditional phrase-based SMT model, there are always lexical translation probabilities based on IBM model 1 (Brown et al., 1993), i.e. p(e|f), namely, the target word e is triggered by the source word f. Intuitively, however, the generation of e is not only involved with f, sometimes may also be triggered by other context words in the source side. Here we assume that the dependency edge (f —* f′) of word f generates target word e (we call it head word trigger in Section 4). Therefore, two words in one language trigger one word in another, which provides a more sophisticated and better choice for the target word, i.e. Figure 1. Similarly, the dependency feature works well in Chinese-to-English translation task, especial</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1115" citStr="Chiang, 2005" startWordPosition="154" endWordPosition="155">trict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed wellformed dependency restriction is used to filter out bad rules. Furthermore, a new feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed. Experimental results show that, the new criteria weeds out about 40% rules while with translation performance improvement, and the new feature brings another improvement to the baseline system, especially on larger corpus. 1 Introduction Hierarchical phrase-based (HPB) model (Chiang, 2005) is the state-of-the-art statistical machine translation (SMT) model. By looking for phrases that contain other phrases and replacing the subphrases with nonterminal symbols, it gets hierarchical rules. Hierarchical rules are more powerful than conventional phrases since they have better generalization capability and could capture long distance reordering. However, when the training corpus becomes larger, the number of rules will grow exponentially, which inevitably results in slow and memory-consuming decoding. In this paper, we address the problem of reducing the hierarchical translation rul</context>
<context position="10250" citStr="Chiang, 2005" startWordPosition="1669" endWordPosition="1670">el on the first 1/3 of the Xinhua portion of GIGAWORD corpus. And we use the NIST 2002 MT evaluation test set as our development set, and NIST 2004, 2005 test sets as our blind test sets. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). In order to get the dependency relation of the training corpus, we re-implement a beam-search style monolingual dependency parser according to (Nivre and Scholz, 2004). Then we use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules within dependency constraint on both sides except that unaligned words are allowed at the edge of phrases. Parameters of head word trigger are estimated as described in Section 4. As a default, the maximum initial phrase length is set to 10 and the maximum rule length of the source side is set to 5. Besides, we also re-implement the decoder of Hiero (Chiang, 2007) as our baseline. In fact, we just exploit the dependency structure during the rule extraction phase. Therefore, we don’t need to change the main decoding algorithm of the SMT system. 5.2 Results on FBIS</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263– 270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="10647" citStr="Chiang, 2007" startWordPosition="1740" endWordPosition="1742"> to get the dependency relation of the training corpus, we re-implement a beam-search style monolingual dependency parser according to (Nivre and Scholz, 2004). Then we use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules within dependency constraint on both sides except that unaligned words are allowed at the edge of phrases. Parameters of head word trigger are estimated as described in Section 4. As a default, the maximum initial phrase length is set to 10 and the maximum rule length of the source side is set to 5. Besides, we also re-implement the decoder of Hiero (Chiang, 2007) as our baseline. In fact, we just exploit the dependency structure during the rule extraction phase. Therefore, we don’t need to change the main decoding algorithm of the SMT system. 5.2 Results on FBIS Corpus A series of experiments was done on the FBIS corpus. We first parse the bilingual languages with monolingual dependency parser respectively, and then only retain the rules that both sides are in line with the constraint of dependency structure. In Table 1, the relaxed-well-formed structure filtered out 35% of the rule table and the well-formed discarded 74%. RWF extracts additional 39% </context>
<context position="13161" citStr="Chiang, 2007" startWordPosition="2170" endWordPosition="2171"> baseline (p &lt; 0.05 or 0.01, respectively). over-all BLEU is decreased by 0.66%-0.78% on the test sets. As for the feature of head word trigger, it seems not work on the FBIS corpus. On Test05, it gets the same score with the baseline, but lower than RWF filtering. This may be caused by the data sparseness problem, which results in inaccurate parameter estimation of the new feature. 5.3 Result on GQ Corpus In this part, we increased the size of the training corpus to check whether the feature of head word trigger works on large corpus. We get 152M rule entries from the GQ corpus according to (Chiang, 2007)’s extraction method. If we use the RWF structure to constrain both sides, the number of rules is 87M, about 43% of rule entries are discarded. From Table 3, the new System Dev02 Test04 Test05 HPB 0.3473 0.3386 0.3206 RWF 0.3539 0.3485** 0.3228 RWF+Tri 0.3540 0.3607** 0.3339* Table 3: Results of GQ corpus. * or ** = significantly better than baseline (p &lt; 0.05 or 0.01, respectively). feature works well on two different test sets. The gain is +2.21% BLEU on Test04, and +1.33% on Test05. Compared to the result of the baseline, only using the RWF structure to filter performs the same as the basel</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="5506" citStr="Ding and Palmer, 2005" startWordPosition="846" endWordPosition="849">f parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information. 3 Relaxed-well-formed Dependency Structure Dependency models have recently gained considerable interest in SMT (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency tree can represent richer structural information. It reveals long-distance relation between words and directly models the semantic structure of a sentence without any constituent labels. Figure 2 shows an example of a dependency tree. In this example, the word found is the root of the tree. Shen et al. (2008) propose the well-formed dependency structure to filter the hierarchical rule table. A well-formed dependency structure could be either a single-rooted dependency tree or a set of sibling trees. Although most rules are discarded with the</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Comparison of extended lexicon models in search and rescoring for smt.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="4644" citStr="Hasan and Ney, 2009" startWordPosition="714" endWordPosition="717">es of the target-side are well-formed (WF) dependency structure, but this filtering led to degradation in translation performance. They obtained improvements by adding an additional dependency language model. The basic difference of our method from (Shen et al., 2008) is that we keep rules that both sides should be relaxed-wellformed dependency structure, not just the target side. Besides, our system complexity is not increased because no additional language model is introduced. The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Ta</context>
</contexts>
<marker>Hasan, Ney, 2009</marker>
<rawString>Saˇsa Hasan and Hermann Ney. 2009. Comparison of extended lexicon models in search and rescoring for smt. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Hasan</author>
<author>Juri Ganitkevitch</author>
<author>Hermann Ney</author>
<author>Jes´us Andr´es-Ferrer</author>
</authors>
<title>Triplet lexicon models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>372--381</pages>
<marker>Hasan, Ganitkevitch, Ney, Andr´es-Ferrer, 2008</marker>
<rawString>Saˇsa Hasan, Juri Ganitkevitch, Hermann Ney, and Jes´us Andr´es-Ferrer. 2008. Triplet lexicon models for statistical machine translation. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 372– 381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="4933" citStr="He et al. (2008)" startWordPosition="762" endWordPosition="765">at both sides should be relaxed-wellformed dependency structure, not just the target side. Besides, our system complexity is not increased because no additional language model is introduced. The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information. 3 Relaxed-well-formed Dependency Structure Dependency models have recently gained considerable interest in SMT (Ding and Palmer, 2005; Quirk et al., 2005; Shen </context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Yao Meng</author>
<author>Yajuan L¨u</author>
<author>Hao Yu</author>
<author>Qun Liu</author>
</authors>
<title>Reducing smt rule table with monolingual key phrase.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP ’09: Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>121--124</pages>
<marker>He, Meng, L¨u, Yu, Liu, 2009</marker>
<rawString>Zhongjun He, Yao Meng, Yajuan L¨u, Hao Yu, and Qun Liu. 2009. Reducing smt rule table with monolingual key phrase. In ACL-IJCNLP ’09: Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 121–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1222--1231</pages>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222–1231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Rule filtering by pattern for efficient hierarchical translation.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>380--388</pages>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule filtering by pattern for efficient hierarchical translation. In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 380–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="7355" citStr="Koehn et al., 2003" startWordPosition="1167" endWordPosition="1170">wh or -1 (here we define h = −1). If and only if it satisfies the following conditions • dh E� [i,j] • bk E [i, j], dk E [i, j] or dk = h From the definition above, we can see that the relaxed-well-formed structure obviously covers the well-formed one. In this structure, we don’t constrain that all the children of the sub-root should be complete. Let’s review the dependency tree in Figure 2 as an example. Except for the wellformed structure, we could also extract girl found a beautiful house. Therefore, if the modifier The lovely changes to The cute, this rule also works. 4 Head Word Trigger (Koehn et al., 2003) introduced the concept of lexical weighting to check how well words of the phrase translate to each other. Source word f aligns with target word e, according to the IBM model 1, the lexical translation probability is p(e|f). However, in the sense of dependency relationship, we believe that the generation of the target word e, is not only triggered by the aligned source word f, but also associated with f ’s head word f′. Therefore, the lexical translation probability becomes p(e|f —* f′), which of course allows for a more fine-grained lexical choice of The girl a beautiful lovely found house 1</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>64--70</pages>
<contexts>
<context position="10193" citStr="Nivre and Scholz, 2004" startWordPosition="1657" endWordPosition="1660">RI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram model on the first 1/3 of the Xinhua portion of GIGAWORD corpus. And we use the NIST 2002 MT evaluation test set as our development set, and NIST 2004, 2005 test sets as our blind test sets. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). In order to get the dependency relation of the training corpus, we re-implement a beam-search style monolingual dependency parser according to (Nivre and Scholz, 2004). Then we use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules within dependency constraint on both sides except that unaligned words are allowed at the edge of phrases. Parameters of head word trigger are estimated as described in Section 4. As a default, the maximum initial phrase length is set to 10 and the maximum rule length of the source side is set to 5. Besides, we also re-implement the decoder of Hiero (Chiang, 2007) as our baseline. In fact, we just exploit the dependency structure during the rule extraction phase. Therefore, we don’t need to change the main </context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of english text. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, pages 64–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="10024" citStr="Och, 2003" startWordPosition="1634" endWordPosition="1635">t 2It consists of six LDC corpora: LDC2002E18, LDC2003E07, LDC2003E14, Hansards part of LDC2004T07, LDC2004T08, LDC2005T06. For language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram model on the first 1/3 of the Xinhua portion of GIGAWORD corpus. And we use the NIST 2002 MT evaluation test set as our development set, and NIST 2004, 2005 test sets as our blind test sets. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). In order to get the dependency relation of the training corpus, we re-implement a beam-search style monolingual dependency parser according to (Nivre and Scholz, 2004). Then we use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules within dependency constraint on both sides except that unaligned words are allowed at the edge of phrases. Parameters of head word trigger are estimated as described in Section 4. As a default, the maximum initial phrase length is set to 10 and the maximum rule length of the source side is set to 5. Besides, we also re-implement the decoder </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="9919" citStr="Papineni et al., 2002" startWordPosition="1615" endWordPosition="1618">e pairs with 41M Chinese words and 48M English words. In fact, FBIS is the subset of GQ. 1www.nist.gov/speech/tests/mt 2It consists of six LDC corpora: LDC2002E18, LDC2003E07, LDC2003E14, Hansards part of LDC2004T07, LDC2004T08, LDC2005T06. For language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram model on the first 1/3 of the Xinhua portion of GIGAWORD corpus. And we use the NIST 2002 MT evaluation test set as our development set, and NIST 2004, 2005 test sets as our blind test sets. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). In order to get the dependency relation of the training corpus, we re-implement a beam-search style monolingual dependency parser according to (Nivre and Scholz, 2004). Then we use the same method suggested in (Chiang, 2005) to extract SCFG grammar rules within dependency constraint on both sides except that unaligned words are allowed at the edge of phrases. Parameters of head word trigger are estimated as described in Section 4. As a default, the maximum initial phrase length is set to </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="5526" citStr="Quirk et al., 2005" startWordPosition="850" endWordPosition="853">Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information. 3 Relaxed-well-formed Dependency Structure Dependency models have recently gained considerable interest in SMT (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency tree can represent richer structural information. It reveals long-distance relation between words and directly models the semantic structure of a sentence without any constituent labels. Figure 2 shows an example of a dependency tree. In this example, the word found is the root of the tree. Shen et al. (2008) propose the well-formed dependency structure to filter the hierarchical rule table. A well-formed dependency structure could be either a single-rooted dependency tree or a set of sibling trees. Although most rules are discarded with the constraint that the</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: syntactically informed phrasal smt. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="3656" citStr="Shen et al. (2008)" startWordPosition="557" endWordPosition="560">hoice for the target word, i.e. Figure 1. Similarly, the dependency feature works well in Chinese-to-English translation task, especially on large corpus. 2 Related Work In the past, a significant number of techniques have been presented to reduce the hierarchical rule table. He et al. (2009) just used the key phrases of source side to filter the rule table without taking advantage of any linguistic information. Iglesias et al. (2009) put rules into syntactic classes based on the number of non-terminals and patterns, and applied various filtration strategies to improve the rule table quality. Shen et al. (2008) discarded e f f’ Source Target 142 Proceedings of the ACL 2010 Conference Short Papers, pages 142–146, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Figure 2: An example of dependency tree. The corresponding plain sentence is The lovely girl found a beautiful house. most entries of the rule table by using the constraint that rules of the target-side are well-formed (WF) dependency structure, but this filtering led to degradation in translation performance. They obtained improvements by adding an additional dependency language model. The basic difference of</context>
<context position="5546" citStr="Shen et al., 2008" startWordPosition="854" endWordPosition="857">2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information. 3 Relaxed-well-formed Dependency Structure Dependency models have recently gained considerable interest in SMT (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency tree can represent richer structural information. It reveals long-distance relation between words and directly models the semantic structure of a sentence without any constituent labels. Figure 2 shows an example of a dependency tree. In this example, the word found is the root of the tree. Shen et al. (2008) propose the well-formed dependency structure to filter the hierarchical rule table. A well-formed dependency structure could be either a single-rooted dependency tree or a set of sibling trees. Although most rules are discarded with the constraint that the target side should </context>
<context position="11411" citStr="Shen et al., 2008" startWordPosition="1870" endWordPosition="1873">decoding algorithm of the SMT system. 5.2 Results on FBIS Corpus A series of experiments was done on the FBIS corpus. We first parse the bilingual languages with monolingual dependency parser respectively, and then only retain the rules that both sides are in line with the constraint of dependency structure. In Table 1, the relaxed-well-formed structure filtered out 35% of the rule table and the well-formed discarded 74%. RWF extracts additional 39% compared to WF, which can be seen as some kind of evidence that the rules we additional get seem common in the sense of linguistics. Compared to (Shen et al., 2008), we just use the dependency structure to constrain rules, not to maintain the tree structures to guide decoding. Table 2 shows the translation result on FBIS. We can see that the RWF structure constraint can improve translation quality substantially both at development set and different test sets. On the Test04 task, it gains +0.86% BLEU, and +0.84% on Test05. Besides, we also used Shen et al. (2008)’s WF structure to filter both sides. Although it discard about 74% of the rule table, the ∀(j,i)∈a 144 System Rule table size HPB 30,152,090 RWF 19,610,255 WF 7,742,031 Table 1: Rule table size w</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL-08: HLT, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="5178" citStr="Shen et al., 2009" startWordPosition="800" endWordPosition="803">he log-linear model is motivated by the trigger-based approach (Hasan and Ney, 2009). Hasan and Ney (2009) introduced a second word to trigger the target word without considering any linguistic information. Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved. Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding. However, as the size of the corpus increases, the maximum entropy model will become larger. Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection. Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information. 3 Relaxed-well-formed Dependency Structure Dependency models have recently gained considerable interest in SMT (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Dependency tree can represent richer structural information. It reveals long-distance relation between words and directly models the semantic structure of a sentence without any constituent labels. Figure 2 shows an example of a d</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilman extensible language modeling toolkit. In</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP</booktitle>
<pages>901--904</pages>
<contexts>
<context position="9614" citStr="Stolcke, 2002" startWordPosition="1561" endWordPosition="1562">ese-English translation task with two different size of training corpora. • FBIS: We use the FBIS corpus as the first training corpus, which contains 239K sentence pairs with 6.9M Chinese words and 8.9M English words. • GQ: This is manually selected from the LDC2 corpora. GQ contains 1.5M sentence pairs with 41M Chinese words and 48M English words. In fact, FBIS is the subset of GQ. 1www.nist.gov/speech/tests/mt 2It consists of six LDC corpora: LDC2002E18, LDC2003E07, LDC2003E14, Hansards part of LDC2004T07, LDC2004T08, LDC2005T06. For language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram model on the first 1/3 of the Xinhua portion of GIGAWORD corpus. And we use the NIST 2002 MT evaluation test set as our development set, and NIST 2004, 2005 test sets as our blind test sets. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). In order to get the dependency relation of the training corpus, we re-implement a beam-search style monolingual dependency parser according to (Nivre and Scholz, 2004). Then we use the sam</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilman extensible language modeling toolkit. In In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002), pages 901–904.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>