<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.9970875">
Capturing Salience with a Trainable Cache Model for Zero-anaphora
Resolution
</title>
<author confidence="0.997093">
Ryu Iida
</author>
<affiliation confidence="0.999717">
Department of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.807966">
2-12-1, ˆOokayama, Meguro,
Tokyo 152-8552, Japan
</address>
<email confidence="0.998781">
ryu-i@cl.cs.titech.ac.jp
</email>
<author confidence="0.968628">
Kentaro Inui Yuji Matsumoto
</author>
<affiliation confidence="0.991998">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.783364">
8916-5, Takayama, Ikoma
Nara 630-0192, Japan
</address>
<email confidence="0.999279">
{inui,matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.993907" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997865">
This paper explores how to apply the notion
of caching introduced by Walker (1996) to
the task of zero-anaphora resolution. We
propose a machine learning-based imple-
mentation of a cache model to reduce the
computational cost of identifying an an-
tecedent. Our empirical evaluation with
Japanese newspaper articles shows that the
number of candidate antecedents for each
zero-pronoun can be dramatically reduced
while preserving the accuracy of resolving
it.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979865671642">
There have been recently increasing concerns
with the need for anaphora resolution to make
NLP applications such as IE and MT more reli-
able. In particular, for languages such as Japanese,
anaphora resolution is crucial for resolving a
phrase in a text to its referent since phrases, es-
pecially nominative arguments of predicates, are
frequently omitted by anaphoric functions in dis-
course (Iida et al., 2007b).
Many researchers have recently explored ma-
chine learning-based methods using considerable
amounts of annotated data provided by, for exam-
ple, the Message Understanding Conference and
Automatic Context Extraction programs (Soon et
al., 2001; Ng and Cardie, 2002; Yang et al., 2008;
McCallum and Wellner, 2003, etc.). These meth-
ods reach a level comparable to or better than the
state-of-the-art rule-based systems (e.g. Baldwin
(1995)) by recasting the task of anaphora resolution
into classification or clustering problems. How-
ever, such approaches tend to disregard theoretical
findings from discourse theories, such as Center-
ing Theory (Grosz et al., 1995). Therefore, one of
the challenging issues in this area is to incorporate
such findings from linguistic theories into machine
learning-based approaches.
A typical machine learning-based approach
to zero-anaphora resolution searches for an an-
tecedent in the set of candidates appearing in all
the preceding contexts. However, computational
time makes this approach largely infeasible for
long texts. An alternative approach is to heuristi-
cally limit the search space (e.g. the system deals
with candidates only occurring in the N previous
sentences). Various research such as Yang et al.
(2008) has adopted this approach, but it also leads
to problems when an antecedent is located far from
its anaphor, causing it to be excluded from target
candidate antecedents.
On the other hand, rule-based methods derived
from theoretical background such as Centering
Theory (Grosz et al., 1995) only deal with the
salient discourse entities at each point of the dis-
course status. By incrementally updating the dis-
course status, the set of candidates in question
is automatically limited. Although these meth-
ods have a theoretical advantage, they have a
serious drawback in that Centering Theory only
retains information about the previous sentence.
A few methods have attempted to overcome this
fault (Suri and McCoy, 1994; Hahn and Strube,
1997), but they are overly dependent upon the re-
strictions fundamental to the notion of centering.
We hope that by relaxing such restrictions it will
be possible for an anaphora resolution system to
achieve a good balance between accuracy and com-
putational cost.
From this background, we focus on the issue
of reducing candidate antecedents (discourse en-
tities) for a given anaphor. Inspired by Walker’s
argument (Walker, 1996), we propose a machine
learning-based caching mechanism that captures
the most salient candidates at each point of the
discourse for efficient anaphora resolution. More
specifically, we choose salient candidates for each
sentence from the set of candidates appearing in
that sentence and the candidates which are already
</bodyText>
<page confidence="0.974405">
647
</page>
<note confidence="0.9996125">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 647–655,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999941368421053">
in the cache. Searching only through the set of
salient candidates, the computational cost of zero-
anaphora resolution is effectively reduced. In the
empirical evaluation, we investigate how efficiently
this caching mechanism contributes to reducing the
search space while preserving accuracy. This pa-
per focuses on Japanese though the proposed cache
mechanism may be applicable to any language.
This paper is organized as follows. First,
Section 2 presents the task of zero-anaphora res-
olution and then Section 3 gives an overview
of previous work. Next, in Section 4 we pro-
pose a machine learning-based cache model.
Section 5 presents the antecedent identification and
anaphoricity determination models used in the ex-
periments. To evaluate the model, we conduct sev-
eral empirical evaluations and report their results
in Section 6. Finally, we conclude and discuss the
future direction of this research in Section 7.
</bodyText>
<sectionHeader confidence="0.988937" genericHeader="method">
2 Zero-anaphora resolution
</sectionHeader>
<bodyText confidence="0.999942913043478">
In this paper, we consider only zero-pronouns
that function as an obligatory argument of a predi-
cate. A zero-pronoun may or may not have its an-
tecedent in the discourse; in the case it does, we say
the zero-pronoun is anaphoric. On the other hand,
a zero-pronoun whose referent does not explicitly
appear in the discourse is called a non-anaphoric
zero-pronoun. A zero-pronoun is typically non-
anaphoric when it refers to an extralinguistic entity
(e.g. the first or second person) or its referent is
unspecified in the context.
The task of zero-anaphora resolution can be
decomposed into two subtasks: anaphoricity de-
termination and antecedent identification. In
anaphoricity determination, the model judges
whether a zero-pronoun is anaphoric (i.e. a zero-
pronoun has an antecedent in a text) or not. If a
zero-pronoun is anaphoric, the model must detect
its antecedent. For example, in example (1) the
model has to judge whether or not the zero-pronoun
in the second sentence (i.e. the nominative argu-
ment of the predicate ‘to hate’) is anaphoric, and
then identify its correct antecedent as ‘Mary.’
</bodyText>
<equation confidence="0.975162571428571">
(1) Maryi-wa Johnj-ni (φj-ga) tabako-o
Maryi-TOP Johnj-DAT (φj-NOM) smoking-OBJ
yameru-youni it-ta .
quit-COMP say-PAST PUNC
Mary told John to quit smoking.
(φi-ga) tabako-o kirai-dakarada .
(φi-NOM) smoking-OBJ hate-BECAUSE PUNC
</equation>
<bodyText confidence="0.857048">
Because (she) hates people smoking.
</bodyText>
<sectionHeader confidence="0.997413" genericHeader="method">
3 Previous work
</sectionHeader>
<bodyText confidence="0.999826365853658">
Early methods for zero-anaphora resolution were
developed with rule-based approaches in mind.
Theory-oriented rule-based methods (Kameyama,
1986; Walker et al., 1994), for example, focus
on the Centering Theory (Grosz et al., 1995) and
are designed to collect the salient candidate an-
tecedents in the forward-looking center (Cf) list,
and then choose the most salient candidate, Cp,
as an antecedent of a zero-pronoun according to
heuristic rules (e.g. topic &gt; subject &gt; indirect ob-
ject &gt; direct object &gt; others1). Although these
methods have a theoretical advantage, they have
a serious drawback in that the original Centering
Theory is restricted to keeping information about
the previous sentence only. In order to loosen this
restriction, the Centering-based methods have been
extended for reaching an antecedent appearing fur-
ther from its anaphor. For example, Suri and Mc-
Coy (1994) proposed a method for capturing two
kinds of Cp, that correspond to the most salient
discourse entities within the local transition and
within the global focus of a text. Hahn and Strube
(1997) estimate hierarchical discourse segments of
a text by taking into account a series of Cp and then
the resolution model searches for an antecedent in
the estimated segment. Although these methods
remedy the drawback of Centering, they still overly
depend on the notion of Centering such as Cp.
On the other hand, the existing machine
learning-based methods (Aone and Bennett, 1995;
McCarthy and Lehnert, 1995; Soon et al., 2001;
Ng and Cardie, 2002; Seki et al., 2002; Isozaki
and Hirao, 2003; Iida et al., 2005; Iida et al.,
2007a, etc.) have been developed with less atten-
tion given to such a problem. These methods ex-
haustively search for an antecedent within the list
of all candidate antecedents until the beginning of
the text. Otherwise, the process to search for an-
tecedents is heuristically carried out in a limited
search space (e.g. the previous N sentences of an
anaphor) (Yang et al., 2008).
</bodyText>
<sectionHeader confidence="0.987105" genericHeader="method">
4 Machine learning-based cache model
</sectionHeader>
<bodyText confidence="0.999780666666667">
As mentioned in Section 2, the procedure for
zero-anaphora resolution can be decomposed into
two subtasks, namely anaphoricity determination
and antecedent identification. In this paper,
these two subtasks are carried out according to
the selection-then-classification model (Iida et al.,
</bodyText>
<footnote confidence="0.81333">
1‘A &gt; B’ means A is more salient than B.
</footnote>
<page confidence="0.99586">
648
</page>
<bodyText confidence="0.999973322580645">
2005), chosen because it it has the advantage of
using broader context information for determining
the anaphoricity of a zero-pronoun. It does this by
examining whether the context preceding the zero-
pronoun in the discourse has a plausible candidate
antecedent or not. With this model, antecedent
identification is performed first, and anaphoricity
determination second, that is, the model identifies
the most likely candidate antecedent for a given
zero-pronoun and then it judges whether or not the
zero-pronoun is anaphoric.
As discussed by Iida et al. (2007a), intra-
sentential and inter-sentential zero-anaphora reso-
lution should be dealt with by taking into account
different kinds of information. Syntactic patterns
are useful clues for intra-sentential zero-anaphora
resolution, whereas rhetorical clues such as con-
nectives may be more useful for inter-sentential
cases. Therefore, the intra-sentential and inter-
sentential zero-anaphora resolution models are sep-
arately trained by exploiting different feature sets
as shown in Table 2.
In addition, as mentioned in Section 3, inter-
sentential cases have a serious problem where the
search space of zero-pronouns grows linearly with
the length of the text. In order to avoid this prob-
lem, we incorporate a caching mechanism origi-
nally addressed by Walker (1996) into the follow-
ing procedure of zero-anaphora resolution by lim-
iting the search space at step 3 and by updating the
cache at step 5.
</bodyText>
<subsubsectionHeader confidence="0.407124">
Zero-anaphora resolution process:
</subsubsectionHeader>
<listItem confidence="0.996472705882353">
1. Intra-sentential antecedent identification: For
a given zero-pronoun ZP in a given sentence S,
select the most-likely candidate antecedent A1
from the candidates appearing in S by the intra-
sentential antecedent identification model.
2. Intra-sentential anaphoricity determination:
Estimate plausibility p1 that A1 is the true an-
tecedent, and return A1 if p1 ≥ Bintra2 or go to
3 otherwise.
3. Inter-sentential antecedent identification: Se-
lect the most-likely candidate antecedent A2
from the candidates appearing in the cache as
explained in Section 4.1 by the inter-sentential
antecedent identification model.
4. Inter-sentential anaphoricity determination:
Estimate plausibility p2 that A2 is the true an-
tecedent, and return A2 if p2 ≥ Binter3 or return
</listItem>
<footnote confidence="0.913431">
20,nt&apos;a is a preselected threshold.
30,nt&amp;quot; is a preselected threshold.
</footnote>
<listItem confidence="0.89254">
non-anaphoric otherwise.
5. After processing all zero-pronouns in S, the
cache is updated. The resolution process is con-
tinued until the end of the discourse.
</listItem>
<subsectionHeader confidence="0.969268">
4.1 Dynamic cache model
</subsectionHeader>
<bodyText confidence="0.99992421875">
Because the original work of the cache model by
Walker (1996) is not fully specified for implemen-
tation, we specify how to retain the salient candi-
dates based on machine learning in order to capture
both local and global foci of discourse.
In Walker (1996)’s discussion of the cache
model in discourse processing, it was presumed to
operate under a limited attention constraint. Ac-
cording to this constraint, only a limited number of
candidates can be considered in processing. Ap-
plying the concept of cache to computer hardware,
the cache represents working memory and the main
memory represents long-term memory. The cache
only holds the most salient entities, while the rest
are moved to the main memory for possible later
consideration as a cache candidate. If a new can-
didate antecedent is retrieved from main memory
and inserted into the cache, or enters the cache di-
rectly during processing, other candidates in the
cache have to be displaced due to the limited ca-
pacity of the cache. Which candidate to displace is
determined by a cache replacement policy. How-
ever, the best policy for this is still unknown.
In this paper, we recast the cache replacement
policy as a ranking problem in machine learning.
More precisely, we choose the N best candidates
for each sentence from the set of candidates ap-
pearing in that sentence and the candidates that are
already in the cache. Following this cache model,
named the dynamic cache model, anaphora resolu-
tion is performed by repeating the following two
processes.
</bodyText>
<listItem confidence="0.996530111111111">
1. Cache update: cache Ci for sentence Si is cre-
ated from the candidates in the previous sen-
tence Si−1 and the ones in the previous cache
Ci−1.
2. Inter-sentential zero-anaphora resolution:
cache Ci is used as the search space for
inter-sentential zero-anaphora resolution in
sentence Si (see Step 3 of the aforementioned
zero-anaphora resolution process).
</listItem>
<bodyText confidence="0.9990178">
For each cache update (see Figure 1), a current
cache Ci is created by choosing the N most salient
candidates from the M candidates in Si−1 and the
N candidates in the previous cache Ci−1. In order
to implement this mechanism, we train the model
</bodyText>
<page confidence="0.998092">
649
</page>
<figureCaption confidence="0.979839">
Figure 1: Anaphora resolution using the dynamic
cache model
</figureCaption>
<bodyText confidence="0.958509511627907">
so that it captures the salience of each candidate.
To reflect this, each training instance is labeled
as either retained or discarded. If an instance is re-
ferred to by an zero-pronoun appearing in any of
the following sentences, it is labeled as retained;
otherwise, it is labeled as discarded. Training in-
stances are created in the algorithm detailed in
Figure 2. The algorithm is designed with the fol-
lowing two points in mind.
First, the cache model must capture the salience
of each discourse entity according to the recency
of its entity at each discourse status because typi-
cally the more recently an entity appears, the more
salient it is. To reflect this, training instances
are created from candidates as they appear in the
text, and are labeled as retained from the point of
their appearance until their referring zero-pronoun
is reached, at which time they are labeled as dis-
carded if they are never referred to by any zero-
pronouns in the succeeding context.
Suppose, the situation shown in Figure 3, where
cij is the j-th candidate in sentence Si. In this
situation, for example, candidate c12 is labeled
as retained when creating training instances for
sentence S1, but labeled as discarded from S2
onwards, because of the appearance of its zero-
pronoun. Another candidate c13 which is never re-
ferred to in the text is labeled as discarded for all
training instances.
Second, we need to capture the ‘relative’
salience of candidates appearing in the current dis-
course for each cache update, as also exploited in
the tournament-based or ranking-based approaches
to anaphora resolution (Iida et al., 2003; Yang et
al., 2003; Denis and Baldridge, 2008). To solve
it, we use a ranker trained on the instances created
as described above. In order to train the ranker,
we adopt the Ranking SVM algorithm (Joachims,
2002), which learns a weight vector to rank candi-
dates for a given partial ranking of each discourse
entity. Each training instance is created from the
set of retained candidates, Ri, paired with the set
of discarded candidates, Di, in each sentence. To
</bodyText>
<equation confidence="0.9724792">
Function makeTrainingInstances (T: input text)
C := NULL // set ofpreceding candidates
S := NULL // set of training instances
i := 1; // init
while (exists si) // si: i-th sentence in T
Ei := extractCandidates(si)
Ri := extractRetainedInstances(Ei, T)
Di := Ei\Ri
ri:= extractRetainedInstances(C, T)
Ri := Ri U ri
Di := Di U (C\ri)
S:=SU{(Ri,Di)}
C := updateSalienceInfo(C)
C := C U Ei
i := i + 1
</equation>
<figure confidence="0.619218666666667">
endwhile
return S
end
Function extractRetainedInstances (S, T)
R := NULL // init
while (elm E S)
if (elm is anaphoric with a zero-pronoun located
in the following sentences of T)
R:=RUelm
endif
endwhile
return R
end
Function updateSalienceInfo (C, si)
while (c E C)
if (c is anaphoric with a zero pronoun in si)
c.position := i; // update the position information
endif
endwhile
return C
end
</figure>
<figureCaption confidence="0.998268666666667">
Figure 2: Pseudo-code for creating training in-
stances
Figure 3: Creating training instnaces
</figureCaption>
<bodyText confidence="0.995132">
define the partial ranking of candidates, we simply
rank candidates in Ri as first place and candidates
in Di as second place.
</bodyText>
<subsectionHeader confidence="0.979386">
4.2 Static cache model
</subsectionHeader>
<bodyText confidence="0.999976">
Other research on discourse such as Grosz and
Sidner (1986) has studied global focus, which gen-
erally refers to the entity or set of entities that
are salient throughout the entire discourse. Since
global focus may not be captured by Centering-
based models, we also propose another cache
model which directly captures the global salience
of a text.
To train the model, all the candidates in a text
which have an inter-sentential anaphoric relation
with zero-pronouns are used as positive instances
and the others used as negative ones. Unlike the
</bodyText>
<page confidence="0.998452">
650
</page>
<tableCaption confidence="0.999392">
Table 1: Feature set used in the cache models
</tableCaption>
<table confidence="0.932287777777778">
Feature Description
POS Part-of-speech of C followed by
IPADIC4.
IN QUOTE 1 if C is located in a quoted sentence;
otherwise 0.
BEGINNING 1 if C is located in the beginnig of a text;
otherwise 0.
CASE MARKER Case marker, such as wa (TOPIC) and
ga (SUBJECT), of C.
DEP END 1 if C has a dependency relation with
the last bunsetsu unit (i.e. a basic unit
in Japanese) in a sentence ; otherwise 0.
CONN* The set of connectives intervening be-
tween C and Z. Each conjunction is en-
coded into a binary feature.
IN CACHE* 1 if C is currently stored in the cache;
otherwise 0.
SENT DIST* Distance between C and Z in terms of a
sentence.
CHAIN NUM The number of anaphoric chain, i.e. the
number of antecedents of Z in the situa-
tion that zero-pronouns in the preceding
contexts are completely resolved by the
zero-anaphora resolution model.
C is a candidate antecedent, and Z stands for a target zero-
pronoun. Features marked with an asterisk are only used in
the dynamic cache model.
</table>
<bodyText confidence="0.9986905">
dynamic cache model, this model does not update
the cache dynamically, but simply selects for each
given zero-pronoun the N most salient candidates
from the preceding sentences according to the rank
provided by the trained ranker. We call this model
the static cache model.
</bodyText>
<subsectionHeader confidence="0.9938">
4.3 Features used in the cache models
</subsectionHeader>
<bodyText confidence="0.999968095238095">
The feature set used in the cache model is shown
in Table 1. The ‘CASE MARKER’ feature roughly
captures the salience of the local transition dealt
with in Centering Theory, and is also intended to
capture the global foci of a text coupled with the
BEGINNING feature. The CONN feature is expected
to capture the transitions of a discourse relation be-
cause each connective functions as a marker of a
discourse relation between two adjacent discourse
segments.
In addition, the recency of a candidate an-
tecedent can be even important when an entity oc-
curs as a zero-pronoun in discourse. For example,
when a discourse entity e appearing in sentence si
is referred to by a zero-pronoun later in sentence
sj(i&lt;j), entity e is considered salient again at the
point of sj. To reflect this way of updating salience,
we overwrite the information about the appearance
position of candidate e in sj, which is performed by
the function updateSalienceInfo in Figure 2. This
allows the cache model to handle updated salience
</bodyText>
<footnote confidence="0.821625">
4http://chasen.naist.jp/stable/ipadic/
</footnote>
<bodyText confidence="0.984284317073171">
features such as CHAIN NUM in proceeding cache
updates.
5 Antecedent identification and anaphoric-
ity determination models
As an antecedent identification model, we adopt
the tournament model (Iida et al., 2003) because
in a preliminary experiment it achieved better per-
formance than other state-of-the-art ranking-based
models (Denis and Baldridge, 2008) in this task
setting. To train the tournament model, the training
instances are created by extracting an antecedent
paired with each of the other candidates for learn-
ing a preference of which candidate is more likely
to be an antecedent. At the test phase, the model
conducts a tournament consisting of a series of
matches in which candidate antecedents compete
with one another. Note that in the case of inter-
sentential zero-anaphora resolution the tournament
is arranged between candidates in the cache. For
learning the difference of two candidates in the
cache, training instances are also created by only
extracting candidates from the cache.
For anaphoricity determination, the model has to
judge whether a zero-pronoun is anaphoric or not.
To create the training instances for the binary clas-
sifier, the most likely candidate of each given zero-
pronoun is chosen by the tournament model and
then it is labeled as anaphoric (positive) if the cho-
sen candidate is indeed the antecedent of the zero-
pronoun5, or otherwise labeled as non-anaphoric
(negative).
To create models for antecedent identification
and anaphoricity determination, we use a Support
Vector Machine (Vapnik, 1998)6 with a linear ker-
nel and its default parameters. To use the feature
set shown in Table 2, morpho-syntactic analysis of
a text is performed by the Japanese morpheme ana-
lyzer Chasen and the dependency parser CaboCha.
In the tournament model, the features of two com-
peting candidates are distinguished from each other
by adding the prefix of either ‘left’ or ‘right.’
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999773">
We investigate how the cache model contributes
to candidate reduction. More specifically, we ex-
</bodyText>
<footnote confidence="0.938262714285714">
5In the original selection-then-classification model (Iida et
al., 2005), positive instances are created by all the correct pairs
of a zero-pronoun and its antecedent, however in this paper we
use only antecedents selected by the tournament model as the
most likely candidates in the set of candidates because this
method leads to better performance.
6http://svmlight.joachims.org/
</footnote>
<page confidence="0.998779">
651
</page>
<tableCaption confidence="0.996706">
Table 2: Feature set used in zero-anaphora resolution
</tableCaption>
<table confidence="0.997509545454545">
Feature Type Feature Description
Lexical HEAD BF Characters of right-most morpheme in NP (PRED).
PRED FUNC Characters of functional words followed by PRED.
Grammatical PRED VOICE 1 if PRED contains auxiliaries such as ‘(ra)reru’; otherwise 0.
POS Part-of-speech of NP (PRED) followed by IPADIC (Asahara and Matsumoto, 2003).
PARTICLE Particle followed by NP, such as ‘wa (topic)’, ‘ga (subject)’, ‘o (object)’.
Semantic NE Named entity of NP: PERSON, ORGANIZATION, LOCATION, ARTIFACT, DATE, TIME,
MONEY, PERCENT or N/A.
SELECT PREF The score of selectional preference, which is the mutual information estimated from a
large number of triplets (Noun, Case, Predicate).
Positional SENTNUM Distance between NP and PRED.
BEGINNING 1 if NP is located in the beggining of sentence; otherwise 0.
END 1 if NP is located in the end of sentence; otherwise 0.
PRED NP 1 if PRED precedes NP; otherwise 0.
NP PRED 1 if NP precedes PRED; otherwise 0.
Discourse CL RANK A rank of NP in forward looking-center list.
CL ORDER A order of NP in forward looking-center list.
CONN** The connectives intervesing between NP and PRED.
Path PATH FUNC* Characters of functional words in the shortest path in the dependency tree between
PRED and NP.
PATH POS* Part-of-speech of functional words in shortest patn in the dependency tree between
PRED and NP.
</table>
<bodyText confidence="0.965443">
NP and PRED stand for a bunsetsu-chunk of a candidate antecedent and a bunsetsu-chunk of a predicate which has a target
zero-pronoun respectively. The features marked with an asterisk are used during intra-sentential zero-anaphora resolution. The
feature marked with two asterisks is used during inter-sentential zero-anaphora resolution.
plore the candidate reduction ratio of each cache
model as well as its coverage, i.e. how of-
ten each cache model retains correct antecedents
(Section 6.2). We also evaluate the performance
of both antecedent identification on inter-sentential
zero-anaphora resolution (Section 6.3) and the
overall zero-anaphora resolution (Section 6.4).
</bodyText>
<subsectionHeader confidence="0.997848">
6.1 Data set
</subsectionHeader>
<bodyText confidence="0.999987166666667">
In this experiment, we take the ellipsis of nom-
inative arguments of predicates as target zero-
pronouns because they are most frequently omitted
in Japanese, for example, 45.5% of the nominative
arguments of predicates are omitted in the NAIST
Text Corpus (Iida et al., 2007b).
As the data set, we use part of the NAIST Text
Corpus, which is publicly available, consisting of
287 newspaper articles in Japanese. The data set
contains 1,007 intra-sentential zero-pronouns, 699
inter-sentential zero-pronouns and 593 exophoric
zero-pronouns, totalling 2299 zero-pronouns. We
conduct 5-fold cross-validation using this data set.
A development data set consists of 60 articles for
setting parameters of inter-sentential anaphoricity
determination, θZnter, on overall zero-anaphora res-
olution. It contains 417 intra-sentential, 298 inter-
sentential and 174 exophoric zero-pronouns.
</bodyText>
<subsectionHeader confidence="0.999424">
6.2 Evaluation of the caching mechanism
</subsectionHeader>
<bodyText confidence="0.9054574">
In this experiment, we directly compare the pro-
posed static and dynamic cache models with the
heuristic methods presented in Section 2. Note that
0.2 0.4 0.6 0.8 1
# of classification in antecedent identification process
CM: centering-based cache model, SM: sentence-based cache
model, SCM: static cache model, DCM (w/o ZAR): dynamic
cache model disregarding updateSalienceInfo, DCM (with
ZAR): dynamic cache model using the information of correct
zero-anaphoric relations, n: cache size and s: # of sentences.
</bodyText>
<figureCaption confidence="0.996933">
Figure 4: Coverage of each cache model
</figureCaption>
<bodyText confidence="0.999869153846154">
the salience information (i.e. the function update-
SalienceInfo) in the dynamic cache model is disre-
garded in this experiment because its performance
crucially depends on the performance of the zero-
anaphora resolution model. The performance of
the cache model is evaluated by coverage, which
is a percentage of retained antecedents when ap-
pearing zero-pronouns refer to an antecedent in a
preceding sentence, i.e. we evaluate the cases of
inter-sentential anaphora resolution.
As a baseline, we adopt the following two cache
models. One is the Centering-derived model which
only stores the preceding ‘wa’ (topic)-marked or
</bodyText>
<figure confidence="0.998467272727272">
coverage
0.95
0.85
0.75
0.65
0.55
0.9
0.8
0.7
0.6
0.5
n=5
n=10
SM (s=1)
n=15 n=20
CM
SM (s=2)
SM (s=3)
DCM (w/o ZAR)
DCM (with ZAR)
SCM
n=all
</figure>
<page confidence="0.994875">
652
</page>
<bodyText confidence="0.999993520833334">
‘ga’ (subject)-marked candidate antecedents in the
cache. It is an approximation of the model pro-
posed by Nariyama (2002) for extending the lo-
cal focus transition defined by Centering Theory.
We henceforth call this model the centering-based
cache model. The other baseline model stores can-
didates appearing in the N previous sentences of a
zero-pronoun to simulate a heuristic approach used
in works like Soon et al. (2001). We call this model
the sentence-based cache model. By comparing
these baselines with our cache models, we can see
whether our models contribute to more efficiently
storing salient candidates or not.
The above dynamic cache model retains the
salient candidates independently of the results of
antecedent identification conducted in the preced-
ing contexts. However, if the zero-anaphora res-
olution in the current utterance is performed cor-
rectly, it will be available for use as information
about the recency of candidates and the anaphoric
chain of each candidate. Therefore, we also in-
vestigate whether correct zero-anaphora resolution
contributes to the dynamic cache model or not.
To integrate zero-anaphora resolution information,
we create training instances of the dynamic cache
model by updating the recency using the function
‘updateSalienceInfo’ shown in Figure 2 and also
using an additional feature, CHAIN NUM, defined
in Table 1.
The results are shown in Figure 47. We can
see the effect of the machine learning-based cache
models in comparison to the other two heuristic
models. The results demonstrate that the former
achieves good coverage at each point compared to
the latter. In addition, the difference between the
static and dynamic cache models demonstrates that
the dynamic one is always better then the static. It
may be this way because the dynamic cache model
simultaneously retains global focus of a given text
and the locally salient entities in the current dis-
course.
By comparing the dynamic cache model using
correct zero-anaphora resolution (denoted by DCM
(with ZAR) in Figure 4) and the one without it
(DCM (w/o ZAR)), we can see that correct zero-
anaphora resolution contributes to improving the
caching for every cache size. However, in the
practical setting the current zero-anaphora resolu-
</bodyText>
<footnote confidence="0.9884915">
7Expressions such as verbs were rarely annotated as an-
tecedents, so these are not extracted as candidate antecedents
in our current setting. This is the reason why the coverage of
using all the candidates is less than 1.0.
</footnote>
<bodyText confidence="0.999789428571429">
tion system sometimes chooses the wrong candi-
date as an antecedent or does not choose any can-
didate due to wrong anaphoricity determination,
negatively impacting the performance of the cache
model. For this reason, in the following two exper-
iments we decided not to use zero-anaphora reso-
lution in the dynamic cache model.
</bodyText>
<subsectionHeader confidence="0.996823">
6.3 Evaluation of inter-sentential zero-
anaphora resolution
</subsectionHeader>
<bodyText confidence="0.9999862">
We next investigate the impact of the dynamic
cache model shown in Section 4.1 on the an-
tecedent identification task of inter-sentential zero-
anaphora resolution altering the cache size from
5 to the number of all candidates. We compare
the following three cache model within the task
of inter-sentential antecedent identification: the
centering-based cache model, the sentence-based
cache model and the dynamic cache model disre-
garding updateSalienceInfo (i.e. DCM (w/o ZAR)
in Figure 4). We also investigate the computational
time of the process of inter-sentential antecedent
identification with each cache model altering its pa-
rameter 8.
The results are shown in Table 3. From these
results, we can see the antecedent identification
model using the dynamic cache model obtains al-
most the same accuracy for every cache size. It
indicates that if the model can acquire a small num-
ber of the most salient discourse entities in the cur-
rent discourse, the model achieves accuracy com-
parable to the model which searches all the pre-
ceding discourse entities, while drastically reduc-
ing the computational time.
The results also show that the current antecedent
identification model with the dynamic cache model
does not necessarily outperform the model with the
baseline cache models.
For example, the sentence-based cache model
using the preceding two sentences (SM (s=2))
achieved an accuracy comparable to the dynamic
cache model with the cache size 15 (DCM (n=15)),
both spending almost the same computational time.
This is supposed to be due to the limited accu-
racy of the current antecedent identification model.
Since the dynamic cache models provide much bet-
ter search spaces than the baseline models as shown
in Figure 4, there is presumably more room for im-
provement with the dynamic cache models. More
investigations are to be concluded in our future
</bodyText>
<footnote confidence="0.982785">
8All experiments were conducted on a 2.80 GHz Intel
Xeon with 16 Gb of RAM.
</footnote>
<page confidence="0.995323">
653
</page>
<table confidence="0.684325">
precision
</table>
<tableCaption confidence="0.998502">
Table 3: Results on antecedent identification
</tableCaption>
<table confidence="0.998699636363636">
model accuracy runtime coverage
(Figure 4)
CM 0.441 (308/699) 11m03s 0.651
SM(s=1) 0.381 (266/699) 6m54s 0.524
SM(s=2) 0.448 (313/699) 13m14s 0.720
SM(s=3) 0.466 (326/699) 19m01s 0.794
DCM(n=5) 0.446 (312/699) 4m39s 0.664
DCM(n=10) 0.441 (308/699) 8m56s 0.764
DCM(n=15) 0.442 (309/699) 12m53s 0.858
DCM(n=20) 0.443 (310/699) 16m35s 0.878
DCM(n=1000) 0.452 (316/699) 53m44s 0.928
</table>
<tableCaption confidence="0.629662">
CM: centering-based cache model, SM: sentence-based cache
model, DCM: dynamic cache model, n: cache size, s: number
of the preceding sentences.
</tableCaption>
<bodyText confidence="0.637035">
work.
</bodyText>
<subsectionHeader confidence="0.987618">
6.4 Overall zero-anaphora resolution
</subsectionHeader>
<bodyText confidence="0.999995066666666">
We finally investigate the effects of introducing
the proposed model on overall zero-anaphora res-
olution including intra-sentential cases. The res-
olution is carried out according to the procedure
described in Section 4. By comparing the zero-
anaphora resolution model with different cache
sizes, we can see whether or not the model using
a small number of discourse entities in the cache
achieves performance comparable to the original
one in a practical setting.
For intra-sentential zero-anaphora resolution, we
adopt the model proposed by Iida et al. (2007a),
which exploits syntactic patterns as features that
appear in the dependency path of a zero-pronoun
and its candidate antecedent. Note that for sim-
plicity we use bag-of-functional words and their
part-of-speech intervening between a zero-pronoun
and its candidate antecedent as features instead
of learning syntactic patterns with the Bact algo-
rithm (Kudo and Matsumoto, 2004).
We illustrated the recall-precision curve of each
model by altering the threshold parameter of intra-
sentential anaphoricity determination, which is
shown in Figure 5. The results show that all mod-
els achieved almost the same performance when
decreasing the cache size. It indicates that it is
enough to cache a small number of the most salient
candidates in the current zero-anaphora resolution
model, while coverage decreases when the cache
size is smaller as shown in Figure 4.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9995248">
We propose a machine learning-based cache
model in order to reduce the computational cost of
zero-anaphora resolution. We recast discourse sta-
tus updates as ranking problems of discourse en-
tities by adopting the notion of caching originally
</bodyText>
<figure confidence="0.998435416666667">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0.1 0.2 0.3 0.4 0.5 0.6
recall
</figure>
<figureCaption confidence="0.96196">
Figure 5: Recall-precision curves on overall ze-
ro-anaphora resolution
</figureCaption>
<bodyText confidence="0.999586">
introduced by Walker (1996). More specifically,
we choose the N most salient candidates for each
sentence from the set of candidates appearing in
that sentence and the candidates which are already
in the cache. Using this mechanism, the compu-
tational cost of the zero-anaphora resolution pro-
cess is reduced by searching only the set of salient
candidates. Our empirical evaluation on Japanese
zero-anaphora resolution shows that our learning-
based cache model drastically reduces the search
space while preserving accuracy.
The procedure for zero-anaphora resolution
adopted in our model assumes that resolution is
carried out linearly, i.e. an antecedent is inde-
pendently selected without taking into account any
other zero-pronouns. However, trends in anaphora
resolution have shifted from such linear approaches
to more sophisticated ones which globally opti-
mize the interpretation of all the referring expres-
sions in a text. For example, Poon and Domingos
(2008) has empirically reported that such global
approaches achieve performance better than the
ones based on incrementally processing a text. Be-
cause their work basically builds on inductive logic
programing, we can naturally extend this to incor-
porate our caching mechanism into the global op-
timization by expressing cache constraints as pred-
icate logic, which is one of our next challenges in
this research area.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998886769230769">
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proceedings of 33th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 122–129.
M. Asahara and Y. Matsumoto, 2003. IPADIC User Manual.
Nara Institute of Science and Technology, Japan.
B. Baldwin. 1995. CogNIAC: A Discourse Processing En-
gine. Ph.D. thesis, Department of Computer and Informa-
tion Sciences, University of Pennsylvania.
P. Denis and J. Baldridge. 2008. Specialized models and
ranking for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 660–669.
</reference>
<figure confidence="0.9923014">
n=5
n=10
n=15
n=20
n=1000
</figure>
<page confidence="0.99134">
654
</page>
<reference confidence="0.999281623529412">
B. J. Grosz and C. L. Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Linguistics,
12:175–204.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: A
framework for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203–226.
U. Hahn and M. Strube. 1997. Centering in-the-large: com-
puting referential discourse segments. In Proceedings of
the 8th conference on European chapter of the Association
for Computational Linguistics, pages 104–111.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. In-
corporating contextual cues in trainable models for coref-
erence resolution. In Proceedings of the 10th EACL Work-
shop on The Computational Treatment ofAnaphora, pages
23–30.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora resolu-
tion by antecedent identification followed by anaphoricity
determination. ACM Transactions on Asian Language In-
formation Processing (TALIP), 4(4):417–434.
R. Iida, K. Inui, and Y. Matsumoto. 2007a. Zero-anaphora
resolution by learning rich syntactic pattern features. ACM
Transactions on Asian Language Information Processing
(TALIP), 6(4).
R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007b.
Annotating a japanese text corpus with predicate-argument
and coreference relations. In Proceeding of the ACL Work-
shop ‘Linguistic Annotation Workshop’, pages 132–139.
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun res-
olution based on ranking rules and machine learning. In
Proceedings of the 2003 Conference on Empirical Methods
in Natural Language Processing, pages 184–191.
T. Joachims. 2002. Optimizing search engines using click-
through data. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD), pages
133–142.
M. Kameyama. 1986. A property-sharing constraint in cen-
tering. In Proceedings of the 24th ACL, pages 200–206.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for
classification of semi-structured text. In Proceedings of the
2004 EMNLP, pages 301–308.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In Proceedings of the IJCAI Workshop on In-
formation Integration on the Web, pages 79–84.
J. F. McCarthy and W. G. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the 14th
International Joint Conference on Artificial Intelligence,
pages 1050–1055.
S. Nariyama. 2002. Grammar for ellipsis resolution in
japanese. In Proceedings of the 9th International Confer-
ence on Theoretical and Methodological Issues in Machine
Translation, pages 135–145.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th ACL, pages 104–111.
H. Poon and P. Domingos. 2008. Joint unsupervised corefer-
ence resolution with Markov Logic. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 650–659.
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic
method for analyzing japanese anaphora integrating zero
pronoun detection and resolution. In Proceedings of the
19th COLING, pages 911–917.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521–544.
L. Z. Suri and K. F. McCoy. 1994. Raft/rapr and center-
ing: a comparison and discussion of problems related to
processing complex sentences. Computational Linguistics,
20(2):301–317.
V. N. Vapnik. 1998. Statistical Learning Theory. Adaptive
and Learning Systems for Signal Processing Communica-
tions, and control. John Wiley &amp; Sons.
M. Walker, M. Iida, and S. Cote. 1994. Japanese discourse
and the process of centering. Computational Linguistics,
20(2):193–233.
M. A. Walker. 1996. Limited attention and discourse struc-
ture. Computational Linguistics, 22(2):255–264.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st ACL, pages 176–183.
X. Yang, J. Su, J. Lang, C. L. Tan, T. Liu, and S. Li. 2008.
An entity-mention model for coreference resolution with
inductive logic programming. In Proceedings of ACL-08:
HLT, pages 843–851.
</reference>
<page confidence="0.998881">
655
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.473566">
<title confidence="0.9990355">Capturing Salience with a Trainable Cache Model for Zero-anaphora Resolution</title>
<author confidence="0.998305">Ryu Iida</author>
<affiliation confidence="0.957151">Department of Computer Science Tokyo Institute of Technology Meguro,</affiliation>
<address confidence="0.99473">Tokyo 152-8552, Japan</address>
<email confidence="0.983046">ryu-i@cl.cs.titech.ac.jp</email>
<author confidence="0.646838">Kentaro Inui Yuji Matsumoto</author>
<affiliation confidence="0.999552">Graduate School of Information Science Nara Institute of Science and Technology</affiliation>
<address confidence="0.9412695">8916-5, Takayama, Ikoma Nara 630-0192, Japan</address>
<abstract confidence="0.998243307692308">This paper explores how to apply the notion of caching introduced by Walker (1996) to the task of zero-anaphora resolution. We propose a machine learning-based implementation of a cache model to reduce the computational cost of identifying an antecedent. Our empirical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>S W Bennett</author>
</authors>
<title>Evaluating automated and manual acquisition of anaphora resolution strategies.</title>
<date>1995</date>
<booktitle>In Proceedings of 33th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>122--129</pages>
<contexts>
<context position="7974" citStr="Aone and Bennett, 1995" startWordPosition="1218" endWordPosition="1221"> its anaphor. For example, Suri and McCoy (1994) proposed a method for capturing two kinds of Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedu</context>
</contexts>
<marker>Aone, Bennett, 1995</marker>
<rawString>C. Aone and S. W. Bennett. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. In Proceedings of 33th Annual Meeting of the Association for Computational Linguistics (ACL), pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Asahara</author>
<author>Y Matsumoto</author>
</authors>
<date>2003</date>
<institution>IPADIC User Manual. Nara Institute of Science and Technology,</institution>
<contexts>
<context position="22367" citStr="Asahara and Matsumoto, 2003" startWordPosition="3561" endWordPosition="3564">zero-pronoun and its antecedent, however in this paper we use only antecedents selected by the tournament model as the most likely candidates in the set of candidates because this method leads to better performance. 6http://svmlight.joachims.org/ 651 Table 2: Feature set used in zero-anaphora resolution Feature Type Feature Description Lexical HEAD BF Characters of right-most morpheme in NP (PRED). PRED FUNC Characters of functional words followed by PRED. Grammatical PRED VOICE 1 if PRED contains auxiliaries such as ‘(ra)reru’; otherwise 0. POS Part-of-speech of NP (PRED) followed by IPADIC (Asahara and Matsumoto, 2003). PARTICLE Particle followed by NP, such as ‘wa (topic)’, ‘ga (subject)’, ‘o (object)’. Semantic NE Named entity of NP: PERSON, ORGANIZATION, LOCATION, ARTIFACT, DATE, TIME, MONEY, PERCENT or N/A. SELECT PREF The score of selectional preference, which is the mutual information estimated from a large number of triplets (Noun, Case, Predicate). Positional SENTNUM Distance between NP and PRED. BEGINNING 1 if NP is located in the beggining of sentence; otherwise 0. END 1 if NP is located in the end of sentence; otherwise 0. PRED NP 1 if PRED precedes NP; otherwise 0. NP PRED 1 if NP precedes PRED;</context>
</contexts>
<marker>Asahara, Matsumoto, 2003</marker>
<rawString>M. Asahara and Y. Matsumoto, 2003. IPADIC User Manual. Nara Institute of Science and Technology, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Baldwin</author>
</authors>
<title>CogNIAC: A Discourse Processing Engine.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Sciences, University of Pennsylvania.</institution>
<contexts>
<context position="1724" citStr="Baldwin (1995)" startWordPosition="249" endWordPosition="250">ng a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolution searches for an antecedent in the set of candidates appearing in all the preceding contexts. However, computational time makes this approach largely inf</context>
</contexts>
<marker>Baldwin, 1995</marker>
<rawString>B. Baldwin. 1995. CogNIAC: A Discourse Processing Engine. Ph.D. thesis, Department of Computer and Information Sciences, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>660--669</pages>
<contexts>
<context position="15214" citStr="Denis and Baldridge, 2008" startWordPosition="2383" endWordPosition="2386">andidate in sentence Si. In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1, but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri, paired with the set of discarded candidates, Di, in each sentence. To Function makeTrainingInstances (T: input text) C := NULL // set ofpreceding candidates S := NULL // set of training instances i := 1; // init while (exists si) // si: i-th sentence in T </context>
<context position="19942" citStr="Denis and Baldridge, 2008" startWordPosition="3183" endWordPosition="3186">his way of updating salience, we overwrite the information about the appearance position of candidate e in sj, which is performed by the function updateSalienceInfo in Figure 2. This allows the cache model to handle updated salience 4http://chasen.naist.jp/stable/ipadic/ features such as CHAIN NUM in proceeding cache updates. 5 Antecedent identification and anaphoricity determination models As an antecedent identification model, we adopt the tournament model (Iida et al., 2003) because in a preliminary experiment it achieved better performance than other state-of-the-art ranking-based models (Denis and Baldridge, 2008) in this task setting. To train the tournament model, the training instances are created by extracting an antecedent paired with each of the other candidates for learning a preference of which candidate is more likely to be an antecedent. At the test phase, the model conducts a tournament consisting of a series of matches in which candidate antecedents compete with one another. Note that in the case of intersentential zero-anaphora resolution the tournament is arranged between candidates in the cache. For learning the difference of two candidates in the cache, training instances are also creat</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>P. Denis and J. Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 660–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--175</pages>
<contexts>
<context position="16718" citStr="Grosz and Sidner (1986)" startWordPosition="2642" endWordPosition="2645">LL // init while (elm E S) if (elm is anaphoric with a zero-pronoun located in the following sentences of T) R:=RUelm endif endwhile return R end Function updateSalienceInfo (C, si) while (c E C) if (c is anaphoric with a zero pronoun in si) c.position := i; // update the position information endif endwhile return C end Figure 2: Pseudo-code for creating training instances Figure 3: Creating training instnaces define the partial ranking of candidates, we simply rank candidates in Ri as first place and candidates in Di as second place. 4.2 Static cache model Other research on discourse such as Grosz and Sidner (1986) has studied global focus, which generally refers to the entity or set of entities that are salient throughout the entire discourse. Since global focus may not be captured by Centeringbased models, we also propose another cache model which directly captures the global salience of a text. To train the model, all the candidates in a text which have an inter-sentential anaphoric relation with zero-pronouns are used as positive instances and the others used as negative ones. Unlike the 650 Table 1: Feature set used in the cache models Feature Description POS Part-of-speech of C followed by IPADIC4</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B. J. Grosz and C. L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12:175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1949" citStr="Grosz et al., 1995" startWordPosition="280" endWordPosition="283"> machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolution searches for an antecedent in the set of candidates appearing in all the preceding contexts. However, computational time makes this approach largely infeasible for long texts. An alternative approach is to heuristically limit the search space (e.g. the system deals with candidates only occurring in the N previous sentences). Various research such as Yang et al. (2008) has ad</context>
<context position="6744" citStr="Grosz et al., 1995" startWordPosition="1020" endWordPosition="1023">ve argument of the predicate ‘to hate’) is anaphoric, and then identify its correct antecedent as ‘Mary.’ (1) Maryi-wa Johnj-ni (φj-ga) tabako-o Maryi-TOP Johnj-DAT (φj-NOM) smoking-OBJ yameru-youni it-ta . quit-COMP say-PAST PUNC Mary told John to quit smoking. (φi-ga) tabako-o kirai-dakarada . (φi-NOM) smoking-OBJ hate-BECAUSE PUNC Because (she) hates people smoking. 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic &gt; subject &gt; indirect object &gt; direct object &gt; others1). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction, the Centering-based methods have been extended for reaching an antecedent appearing furt</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>M Strube</author>
</authors>
<title>Centering in-the-large: computing referential discourse segments.</title>
<date>1997</date>
<booktitle>In Proceedings of the 8th conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="3284" citStr="Hahn and Strube, 1997" startWordPosition="486" endWordPosition="489"> to be excluded from target candidate antecedents. On the other hand, rule-based methods derived from theoretical background such as Centering Theory (Grosz et al., 1995) only deal with the salient discourse entities at each point of the discourse status. By incrementally updating the discourse status, the set of candidates in question is automatically limited. Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution. More specifica</context>
<context position="7594" citStr="Hahn and Strube (1997)" startWordPosition="1157" endWordPosition="1160">&gt; subject &gt; indirect object &gt; direct object &gt; others1). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction, the Centering-based methods have been extended for reaching an antecedent appearing further from its anaphor. For example, Suri and McCoy (1994) proposed a method for capturing two kinds of Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a probl</context>
</contexts>
<marker>Hahn, Strube, 1997</marker>
<rawString>U. Hahn and M. Strube. 1997. Centering in-the-large: computing referential discourse segments. In Proceedings of the 8th conference on European chapter of the Association for Computational Linguistics, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>H Takamura</author>
<author>Y Matsumoto</author>
</authors>
<title>Incorporating contextual cues in trainable models for coreference resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th EACL Workshop on The Computational Treatment ofAnaphora,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="15167" citStr="Iida et al., 2003" startWordPosition="2375" endWordPosition="2378">n in Figure 3, where cij is the j-th candidate in sentence Si. In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1, but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri, paired with the set of discarded candidates, Di, in each sentence. To Function makeTrainingInstances (T: input text) C := NULL // set ofpreceding candidates S := NULL // set of training instances i := 1; // in</context>
<context position="19798" citStr="Iida et al., 2003" startWordPosition="3164" endWordPosition="3167">nce si is referred to by a zero-pronoun later in sentence sj(i&lt;j), entity e is considered salient again at the point of sj. To reflect this way of updating salience, we overwrite the information about the appearance position of candidate e in sj, which is performed by the function updateSalienceInfo in Figure 2. This allows the cache model to handle updated salience 4http://chasen.naist.jp/stable/ipadic/ features such as CHAIN NUM in proceeding cache updates. 5 Antecedent identification and anaphoricity determination models As an antecedent identification model, we adopt the tournament model (Iida et al., 2003) because in a preliminary experiment it achieved better performance than other state-of-the-art ranking-based models (Denis and Baldridge, 2008) in this task setting. To train the tournament model, the training instances are created by extracting an antecedent paired with each of the other candidates for learning a preference of which candidate is more likely to be an antecedent. At the test phase, the model conducts a tournament consisting of a series of matches in which candidate antecedents compete with one another. Note that in the case of intersentential zero-anaphora resolution the tourn</context>
</contexts>
<marker>Iida, Inui, Takamura, Matsumoto, 2003</marker>
<rawString>R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. Incorporating contextual cues in trainable models for coreference resolution. In Proceedings of the 10th EACL Workshop on The Computational Treatment ofAnaphora, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Anaphora resolution by antecedent identification followed by anaphoricity determination.</title>
<date>2005</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="8105" citStr="Iida et al., 2005" startWordPosition="1242" endWordPosition="1245">scourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolution can be decomposed into two subtasks, namely anaphoricity determination and antecedent identificatio</context>
<context position="21676" citStr="Iida et al., 2005" startWordPosition="3458" endWordPosition="3461">horicity determination, we use a Support Vector Machine (Vapnik, 1998)6 with a linear kernel and its default parameters. To use the feature set shown in Table 2, morpho-syntactic analysis of a text is performed by the Japanese morpheme analyzer Chasen and the dependency parser CaboCha. In the tournament model, the features of two competing candidates are distinguished from each other by adding the prefix of either ‘left’ or ‘right.’ 6 Experiments We investigate how the cache model contributes to candidate reduction. More specifically, we ex5In the original selection-then-classification model (Iida et al., 2005), positive instances are created by all the correct pairs of a zero-pronoun and its antecedent, however in this paper we use only antecedents selected by the tournament model as the most likely candidates in the set of candidates because this method leads to better performance. 6http://svmlight.joachims.org/ 651 Table 2: Feature set used in zero-anaphora resolution Feature Type Feature Description Lexical HEAD BF Characters of right-most morpheme in NP (PRED). PRED FUNC Characters of functional words followed by PRED. Grammatical PRED VOICE 1 if PRED contains auxiliaries such as ‘(ra)reru’; ot</context>
</contexts>
<marker>Iida, Inui, Matsumoto, 2005</marker>
<rawString>R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora resolution by antecedent identification followed by anaphoricity determination. ACM Transactions on Asian Language Information Processing (TALIP), 4(4):417–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Zero-anaphora resolution by learning rich syntactic pattern features.</title>
<date>2007</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>6</volume>
<issue>4</issue>
<contexts>
<context position="1287" citStr="Iida et al., 2007" startWordPosition="183" endWordPosition="186">irical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it. 1 Introduction There have been recently increasing concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from dis</context>
<context position="8124" citStr="Iida et al., 2007" startWordPosition="1246" endWordPosition="1249">thin the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolution can be decomposed into two subtasks, namely anaphoricity determination and antecedent identification. In this paper, t</context>
<context position="9431" citStr="Iida et al. (2007" startWordPosition="1450" endWordPosition="1453">da et al., 1‘A &gt; B’ means A is more salient than B. 648 2005), chosen because it it has the advantage of using broader context information for determining the anaphoricity of a zero-pronoun. It does this by examining whether the context preceding the zeropronoun in the discourse has a plausible candidate antecedent or not. With this model, antecedent identification is performed first, and anaphoricity determination second, that is, the model identifies the most likely candidate antecedent for a given zero-pronoun and then it judges whether or not the zero-pronoun is anaphoric. As discussed by Iida et al. (2007a), intrasentential and inter-sentential zero-anaphora resolution should be dealt with by taking into account different kinds of information. Syntactic patterns are useful clues for intra-sentential zero-anaphora resolution, whereas rhetorical clues such as connectives may be more useful for inter-sentential cases. Therefore, the intra-sentential and intersentential zero-anaphora resolution models are separately trained by exploiting different feature sets as shown in Table 2. In addition, as mentioned in Section 3, intersentential cases have a serious problem where the search space of zero-pr</context>
<context position="24335" citStr="Iida et al., 2007" startWordPosition="3871" endWordPosition="3874">the candidate reduction ratio of each cache model as well as its coverage, i.e. how often each cache model retains correct antecedents (Section 6.2). We also evaluate the performance of both antecedent identification on inter-sentential zero-anaphora resolution (Section 6.3) and the overall zero-anaphora resolution (Section 6.4). 6.1 Data set In this experiment, we take the ellipsis of nominative arguments of predicates as target zeropronouns because they are most frequently omitted in Japanese, for example, 45.5% of the nominative arguments of predicates are omitted in the NAIST Text Corpus (Iida et al., 2007b). As the data set, we use part of the NAIST Text Corpus, which is publicly available, consisting of 287 newspaper articles in Japanese. The data set contains 1,007 intra-sentential zero-pronouns, 699 inter-sentential zero-pronouns and 593 exophoric zero-pronouns, totalling 2299 zero-pronouns. We conduct 5-fold cross-validation using this data set. A development data set consists of 60 articles for setting parameters of inter-sentential anaphoricity determination, θZnter, on overall zero-anaphora resolution. It contains 417 intra-sentential, 298 intersentential and 174 exophoric zero-pronouns</context>
<context position="32266" citStr="Iida et al. (2007" startWordPosition="5100" endWordPosition="5103">ding sentences. work. 6.4 Overall zero-anaphora resolution We finally investigate the effects of introducing the proposed model on overall zero-anaphora resolution including intra-sentential cases. The resolution is carried out according to the procedure described in Section 4. By comparing the zeroanaphora resolution model with different cache sizes, we can see whether or not the model using a small number of discourse entities in the cache achieves performance comparable to the original one in a practical setting. For intra-sentential zero-anaphora resolution, we adopt the model proposed by Iida et al. (2007a), which exploits syntactic patterns as features that appear in the dependency path of a zero-pronoun and its candidate antecedent. Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). We illustrated the recall-precision curve of each model by altering the threshold parameter of intrasentential anaphoricity determination, which is shown in Figure 5. The results show that all models achieved almost the</context>
</contexts>
<marker>Iida, Inui, Matsumoto, 2007</marker>
<rawString>R. Iida, K. Inui, and Y. Matsumoto. 2007a. Zero-anaphora resolution by learning rich syntactic pattern features. ACM Transactions on Asian Language Information Processing (TALIP), 6(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>M Komachi</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Annotating a japanese text corpus with predicate-argument and coreference relations.</title>
<date>2007</date>
<booktitle>In Proceeding of the ACL Workshop ‘Linguistic Annotation Workshop’,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1287" citStr="Iida et al., 2007" startWordPosition="183" endWordPosition="186">irical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it. 1 Introduction There have been recently increasing concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from dis</context>
<context position="8124" citStr="Iida et al., 2007" startWordPosition="1246" endWordPosition="1249">thin the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolution can be decomposed into two subtasks, namely anaphoricity determination and antecedent identification. In this paper, t</context>
<context position="9431" citStr="Iida et al. (2007" startWordPosition="1450" endWordPosition="1453">da et al., 1‘A &gt; B’ means A is more salient than B. 648 2005), chosen because it it has the advantage of using broader context information for determining the anaphoricity of a zero-pronoun. It does this by examining whether the context preceding the zeropronoun in the discourse has a plausible candidate antecedent or not. With this model, antecedent identification is performed first, and anaphoricity determination second, that is, the model identifies the most likely candidate antecedent for a given zero-pronoun and then it judges whether or not the zero-pronoun is anaphoric. As discussed by Iida et al. (2007a), intrasentential and inter-sentential zero-anaphora resolution should be dealt with by taking into account different kinds of information. Syntactic patterns are useful clues for intra-sentential zero-anaphora resolution, whereas rhetorical clues such as connectives may be more useful for inter-sentential cases. Therefore, the intra-sentential and intersentential zero-anaphora resolution models are separately trained by exploiting different feature sets as shown in Table 2. In addition, as mentioned in Section 3, intersentential cases have a serious problem where the search space of zero-pr</context>
<context position="24335" citStr="Iida et al., 2007" startWordPosition="3871" endWordPosition="3874">the candidate reduction ratio of each cache model as well as its coverage, i.e. how often each cache model retains correct antecedents (Section 6.2). We also evaluate the performance of both antecedent identification on inter-sentential zero-anaphora resolution (Section 6.3) and the overall zero-anaphora resolution (Section 6.4). 6.1 Data set In this experiment, we take the ellipsis of nominative arguments of predicates as target zeropronouns because they are most frequently omitted in Japanese, for example, 45.5% of the nominative arguments of predicates are omitted in the NAIST Text Corpus (Iida et al., 2007b). As the data set, we use part of the NAIST Text Corpus, which is publicly available, consisting of 287 newspaper articles in Japanese. The data set contains 1,007 intra-sentential zero-pronouns, 699 inter-sentential zero-pronouns and 593 exophoric zero-pronouns, totalling 2299 zero-pronouns. We conduct 5-fold cross-validation using this data set. A development data set consists of 60 articles for setting parameters of inter-sentential anaphoricity determination, θZnter, on overall zero-anaphora resolution. It contains 417 intra-sentential, 298 intersentential and 174 exophoric zero-pronouns</context>
<context position="32266" citStr="Iida et al. (2007" startWordPosition="5100" endWordPosition="5103">ding sentences. work. 6.4 Overall zero-anaphora resolution We finally investigate the effects of introducing the proposed model on overall zero-anaphora resolution including intra-sentential cases. The resolution is carried out according to the procedure described in Section 4. By comparing the zeroanaphora resolution model with different cache sizes, we can see whether or not the model using a small number of discourse entities in the cache achieves performance comparable to the original one in a practical setting. For intra-sentential zero-anaphora resolution, we adopt the model proposed by Iida et al. (2007a), which exploits syntactic patterns as features that appear in the dependency path of a zero-pronoun and its candidate antecedent. Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). We illustrated the recall-precision curve of each model by altering the threshold parameter of intrasentential anaphoricity determination, which is shown in Figure 5. The results show that all models achieved almost the</context>
</contexts>
<marker>Iida, Komachi, Inui, Matsumoto, 2007</marker>
<rawString>R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007b. Annotating a japanese text corpus with predicate-argument and coreference relations. In Proceeding of the ACL Workshop ‘Linguistic Annotation Workshop’, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>T Hirao</author>
</authors>
<title>Japanese zero pronoun resolution based on ranking rules and machine learning.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="8086" citStr="Isozaki and Hirao, 2003" startWordPosition="1238" endWordPosition="1241">nd to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolution can be decomposed into two subtasks, namely anaphoricity determination and antec</context>
</contexts>
<marker>Isozaki, Hirao, 2003</marker>
<rawString>H. Isozaki and T. Hirao. 2003. Japanese zero pronoun resolution based on ranking rules and machine learning. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 184–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>133--142</pages>
<contexts>
<context position="15379" citStr="Joachims, 2002" startWordPosition="2414" endWordPosition="2415"> onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri, paired with the set of discarded candidates, Di, in each sentence. To Function makeTrainingInstances (T: input text) C := NULL // set ofpreceding candidates S := NULL // set of training instances i := 1; // init while (exists si) // si: i-th sentence in T Ei := extractCandidates(si) Ri := extractRetainedInstances(Ei, T) Di := Ei\Ri ri:= extractRetainedInstances(C, T) Ri := Ri U ri Di := Di U (C\ri) S:=SU{(Ri,Di)} C :=</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kameyama</author>
</authors>
<title>A property-sharing constraint in centering.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th ACL,</booktitle>
<pages>200--206</pages>
<contexts>
<context position="6657" citStr="Kameyama, 1986" startWordPosition="1007" endWordPosition="1008">to judge whether or not the zero-pronoun in the second sentence (i.e. the nominative argument of the predicate ‘to hate’) is anaphoric, and then identify its correct antecedent as ‘Mary.’ (1) Maryi-wa Johnj-ni (φj-ga) tabako-o Maryi-TOP Johnj-DAT (φj-NOM) smoking-OBJ yameru-youni it-ta . quit-COMP say-PAST PUNC Mary told John to quit smoking. (φi-ga) tabako-o kirai-dakarada . (φi-NOM) smoking-OBJ hate-BECAUSE PUNC Because (she) hates people smoking. 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic &gt; subject &gt; indirect object &gt; direct object &gt; others1). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction, t</context>
</contexts>
<marker>Kameyama, 1986</marker>
<rawString>M. Kameyama. 1986. A property-sharing constraint in centering. In Proceedings of the 24th ACL, pages 200–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>A boosting algorithm for classification of semi-structured text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 EMNLP,</booktitle>
<pages>301--308</pages>
<contexts>
<context position="32645" citStr="Kudo and Matsumoto, 2004" startWordPosition="5155" endWordPosition="5158">r or not the model using a small number of discourse entities in the cache achieves performance comparable to the original one in a practical setting. For intra-sentential zero-anaphora resolution, we adopt the model proposed by Iida et al. (2007a), which exploits syntactic patterns as features that appear in the dependency path of a zero-pronoun and its candidate antecedent. Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). We illustrated the recall-precision curve of each model by altering the threshold parameter of intrasentential anaphoricity determination, which is shown in Figure 5. The results show that all models achieved almost the same performance when decreasing the cache size. It indicates that it is enough to cache a small number of the most salient candidates in the current zero-anaphora resolution model, while coverage decreases when the cache size is smaller as shown in Figure 4. 7 Conclusion We propose a machine learning-based cache model in order to reduce the computational cost of zero-anaphor</context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for classification of semi-structured text. In Proceedings of the 2004 EMNLP, pages 301–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In Proceedings of the IJCAI Workshop on Information Integration on the Web,</booktitle>
<pages>79--84</pages>
<contexts>
<context position="1598" citStr="McCallum and Wellner, 2003" startWordPosition="228" endWordPosition="231">LP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolution searches for an antecedent</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In Proceedings of the IJCAI Workshop on Information Integration on the Web, pages 79–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F McCarthy</author>
<author>W G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="8002" citStr="McCarthy and Lehnert, 1995" startWordPosition="1222" endWordPosition="1225">e, Suri and McCoy (1994) proposed a method for capturing two kinds of Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolut</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>J. F. McCarthy and W. G. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nariyama</author>
</authors>
<title>Grammar for ellipsis resolution in japanese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 9th International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>135--145</pages>
<contexts>
<context position="26416" citStr="Nariyama (2002)" startWordPosition="4187" endWordPosition="4188"> coverage, which is a percentage of retained antecedents when appearing zero-pronouns refer to an antecedent in a preceding sentence, i.e. we evaluate the cases of inter-sentential anaphora resolution. As a baseline, we adopt the following two cache models. One is the Centering-derived model which only stores the preceding ‘wa’ (topic)-marked or coverage 0.95 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 n=5 n=10 SM (s=1) n=15 n=20 CM SM (s=2) SM (s=3) DCM (w/o ZAR) DCM (with ZAR) SCM n=all 652 ‘ga’ (subject)-marked candidate antecedents in the cache. It is an approximation of the model proposed by Nariyama (2002) for extending the local focus transition defined by Centering Theory. We henceforth call this model the centering-based cache model. The other baseline model stores candidates appearing in the N previous sentences of a zero-pronoun to simulate a heuristic approach used in works like Soon et al. (2001). We call this model the sentence-based cache model. By comparing these baselines with our cache models, we can see whether our models contribute to more efficiently storing salient candidates or not. The above dynamic cache model retains the salient candidates independently of the results of ant</context>
</contexts>
<marker>Nariyama, 2002</marker>
<rawString>S. Nariyama. 2002. Grammar for ellipsis resolution in japanese. In Proceedings of the 9th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 135–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1551" citStr="Ng and Cardie, 2002" startWordPosition="220" endWordPosition="223">e need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero</context>
<context position="8042" citStr="Ng and Cardie, 2002" startWordPosition="1230" endWordPosition="1233">capturing two kinds of Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolution can be decomposed into two subtasks,</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov Logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>650--659</pages>
<marker>Poon, Domingos, 2008</marker>
<rawString>H. Poon and P. Domingos. 2008. Joint unsupervised coreference resolution with Markov Logic. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650–659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Seki</author>
<author>A Fujii</author>
<author>T Ishikawa</author>
</authors>
<title>A probabilistic method for analyzing japanese anaphora integrating zero pronoun detection and resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th COLING,</booktitle>
<pages>911--917</pages>
<contexts>
<context position="8061" citStr="Seki et al., 2002" startWordPosition="1234" endWordPosition="1237">f Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolution can be decomposed into two subtasks, namely anaphoricit</context>
</contexts>
<marker>Seki, Fujii, Ishikawa, 2002</marker>
<rawString>K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic method for analyzing japanese anaphora integrating zero pronoun detection and resolution. In Proceedings of the 19th COLING, pages 911–917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1530" citStr="Soon et al., 2001" startWordPosition="216" endWordPosition="219">ng concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-b</context>
<context position="8021" citStr="Soon et al., 2001" startWordPosition="1226" endWordPosition="1229">posed a method for capturing two kinds of Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolution can be decompos</context>
<context position="26719" citStr="Soon et al. (2001)" startWordPosition="4234" endWordPosition="4237">nly stores the preceding ‘wa’ (topic)-marked or coverage 0.95 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 n=5 n=10 SM (s=1) n=15 n=20 CM SM (s=2) SM (s=3) DCM (w/o ZAR) DCM (with ZAR) SCM n=all 652 ‘ga’ (subject)-marked candidate antecedents in the cache. It is an approximation of the model proposed by Nariyama (2002) for extending the local focus transition defined by Centering Theory. We henceforth call this model the centering-based cache model. The other baseline model stores candidates appearing in the N previous sentences of a zero-pronoun to simulate a heuristic approach used in works like Soon et al. (2001). We call this model the sentence-based cache model. By comparing these baselines with our cache models, we can see whether our models contribute to more efficiently storing salient candidates or not. The above dynamic cache model retains the salient candidates independently of the results of antecedent identification conducted in the preceding contexts. However, if the zero-anaphora resolution in the current utterance is performed correctly, it will be available for use as information about the recency of candidates and the anaphoric chain of each candidate. Therefore, we also investigate whe</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Z Suri</author>
<author>K F McCoy</author>
</authors>
<title>Raft/rapr and centering: a comparison and discussion of problems related to processing complex sentences.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="3260" citStr="Suri and McCoy, 1994" startWordPosition="482" endWordPosition="485">ts anaphor, causing it to be excluded from target candidate antecedents. On the other hand, rule-based methods derived from theoretical background such as Centering Theory (Grosz et al., 1995) only deal with the salient discourse entities at each point of the discourse status. By incrementally updating the discourse status, the set of candidates in question is automatically limited. Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora re</context>
<context position="7400" citStr="Suri and McCoy (1994)" startWordPosition="1123" endWordPosition="1127">lient candidate antecedents in the forward-looking center (Cf) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic &gt; subject &gt; indirect object &gt; direct object &gt; others1). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction, the Centering-based methods have been extended for reaching an antecedent appearing further from its anaphor. For example, Suri and McCoy (1994) proposed a method for capturing two kinds of Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 19</context>
</contexts>
<marker>Suri, McCoy, 1994</marker>
<rawString>L. Z. Suri and K. F. McCoy. 1994. Raft/rapr and centering: a comparison and discussion of problems related to processing complex sentences. Computational Linguistics, 20(2):301–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>Statistical Learning Theory. Adaptive and Learning Systems for Signal Processing Communications, and control.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="21128" citStr="Vapnik, 1998" startWordPosition="3373" endWordPosition="3374">ng instances are also created by only extracting candidates from the cache. For anaphoricity determination, the model has to judge whether a zero-pronoun is anaphoric or not. To create the training instances for the binary classifier, the most likely candidate of each given zeropronoun is chosen by the tournament model and then it is labeled as anaphoric (positive) if the chosen candidate is indeed the antecedent of the zeropronoun5, or otherwise labeled as non-anaphoric (negative). To create models for antecedent identification and anaphoricity determination, we use a Support Vector Machine (Vapnik, 1998)6 with a linear kernel and its default parameters. To use the feature set shown in Table 2, morpho-syntactic analysis of a text is performed by the Japanese morpheme analyzer Chasen and the dependency parser CaboCha. In the tournament model, the features of two competing candidates are distinguished from each other by adding the prefix of either ‘left’ or ‘right.’ 6 Experiments We investigate how the cache model contributes to candidate reduction. More specifically, we ex5In the original selection-then-classification model (Iida et al., 2005), positive instances are created by all the correct </context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. N. Vapnik. 1998. Statistical Learning Theory. Adaptive and Learning Systems for Signal Processing Communications, and control. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>M Iida</author>
<author>S Cote</author>
</authors>
<title>Japanese discourse and the process of centering.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="6679" citStr="Walker et al., 1994" startWordPosition="1009" endWordPosition="1012"> or not the zero-pronoun in the second sentence (i.e. the nominative argument of the predicate ‘to hate’) is anaphoric, and then identify its correct antecedent as ‘Mary.’ (1) Maryi-wa Johnj-ni (φj-ga) tabako-o Maryi-TOP Johnj-DAT (φj-NOM) smoking-OBJ yameru-youni it-ta . quit-COMP say-PAST PUNC Mary told John to quit smoking. (φi-ga) tabako-o kirai-dakarada . (φi-NOM) smoking-OBJ hate-BECAUSE PUNC Because (she) hates people smoking. 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic &gt; subject &gt; indirect object &gt; direct object &gt; others1). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction, the Centering-based met</context>
</contexts>
<marker>Walker, Iida, Cote, 1994</marker>
<rawString>M. Walker, M. Iida, and S. Cote. 1994. Japanese discourse and the process of centering. Computational Linguistics, 20(2):193–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
</authors>
<title>Limited attention and discourse structure.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="3706" citStr="Walker, 1996" startWordPosition="555" endWordPosition="556">rious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution. More specifically, we choose salient candidates for each sentence from the set of candidates appearing in that sentence and the candidates which are already 647 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 647–655, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP in the cache. Searching only through the set of salient candidates, the computational cost of zeroanaphora resolution is</context>
<context position="10186" citStr="Walker (1996)" startWordPosition="1563" endWordPosition="1564">Syntactic patterns are useful clues for intra-sentential zero-anaphora resolution, whereas rhetorical clues such as connectives may be more useful for inter-sentential cases. Therefore, the intra-sentential and intersentential zero-anaphora resolution models are separately trained by exploiting different feature sets as shown in Table 2. In addition, as mentioned in Section 3, intersentential cases have a serious problem where the search space of zero-pronouns grows linearly with the length of the text. In order to avoid this problem, we incorporate a caching mechanism originally addressed by Walker (1996) into the following procedure of zero-anaphora resolution by limiting the search space at step 3 and by updating the cache at step 5. Zero-anaphora resolution process: 1. Intra-sentential antecedent identification: For a given zero-pronoun ZP in a given sentence S, select the most-likely candidate antecedent A1 from the candidates appearing in S by the intrasentential antecedent identification model. 2. Intra-sentential anaphoricity determination: Estimate plausibility p1 that A1 is the true antecedent, and return A1 if p1 ≥ Bintra2 or go to 3 otherwise. 3. Inter-sentential antecedent identifi</context>
<context position="11429" citStr="Walker (1996)" startWordPosition="1752" endWordPosition="1753"> candidate antecedent A2 from the candidates appearing in the cache as explained in Section 4.1 by the inter-sentential antecedent identification model. 4. Inter-sentential anaphoricity determination: Estimate plausibility p2 that A2 is the true antecedent, and return A2 if p2 ≥ Binter3 or return 20,nt&apos;a is a preselected threshold. 30,nt&amp;quot; is a preselected threshold. non-anaphoric otherwise. 5. After processing all zero-pronouns in S, the cache is updated. The resolution process is continued until the end of the discourse. 4.1 Dynamic cache model Because the original work of the cache model by Walker (1996) is not fully specified for implementation, we specify how to retain the salient candidates based on machine learning in order to capture both local and global foci of discourse. In Walker (1996)’s discussion of the cache model in discourse processing, it was presumed to operate under a limited attention constraint. According to this constraint, only a limited number of candidates can be considered in processing. Applying the concept of cache to computer hardware, the cache represents working memory and the main memory represents long-term memory. The cache only holds the most salient entities</context>
<context position="33549" citStr="Walker (1996)" startWordPosition="5304" endWordPosition="5305"> is enough to cache a small number of the most salient candidates in the current zero-anaphora resolution model, while coverage decreases when the cache size is smaller as shown in Figure 4. 7 Conclusion We propose a machine learning-based cache model in order to reduce the computational cost of zero-anaphora resolution. We recast discourse status updates as ranking problems of discourse entities by adopting the notion of caching originally 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 recall Figure 5: Recall-precision curves on overall zero-anaphora resolution introduced by Walker (1996). More specifically, we choose the N most salient candidates for each sentence from the set of candidates appearing in that sentence and the candidates which are already in the cache. Using this mechanism, the computational cost of the zero-anaphora resolution process is reduced by searching only the set of salient candidates. Our empirical evaluation on Japanese zero-anaphora resolution shows that our learningbased cache model drastically reduces the search space while preserving accuracy. The procedure for zero-anaphora resolution adopted in our model assumes that resolution is carried out l</context>
</contexts>
<marker>Walker, 1996</marker>
<rawString>M. A. Walker. 1996. Limited attention and discourse structure. Computational Linguistics, 22(2):255–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>G Zhou</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Coreference resolution using competition learning approach.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st ACL,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="15186" citStr="Yang et al., 2003" startWordPosition="2379" endWordPosition="2382">e cij is the j-th candidate in sentence Si. In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1, but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri, paired with the set of discarded candidates, Di, in each sentence. To Function makeTrainingInstances (T: input text) C := NULL // set ofpreceding candidates S := NULL // set of training instances i := 1; // init while (exists si</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Coreference resolution using competition learning approach. In Proceedings of the 41st ACL, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
<author>J Lang</author>
<author>C L Tan</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>An entity-mention model for coreference resolution with inductive logic programming.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>843--851</pages>
<contexts>
<context position="1570" citStr="Yang et al., 2008" startWordPosition="224" endWordPosition="227">esolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolutio</context>
<context position="8497" citStr="Yang et al., 2008" startWordPosition="1309" endWordPosition="1312">h as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). 4 Machine learning-based cache model As mentioned in Section 2, the procedure for zero-anaphora resolution can be decomposed into two subtasks, namely anaphoricity determination and antecedent identification. In this paper, these two subtasks are carried out according to the selection-then-classification model (Iida et al., 1‘A &gt; B’ means A is more salient than B. 648 2005), chosen because it it has the advantage of using broader context information for determining the anaphoricity of a zero-pronoun. It does this by examining whether the context preceding the zeropronoun in the discourse has</context>
</contexts>
<marker>Yang, Su, Lang, Tan, Liu, Li, 2008</marker>
<rawString>X. Yang, J. Su, J. Lang, C. L. Tan, T. Liu, and S. Li. 2008. An entity-mention model for coreference resolution with inductive logic programming. In Proceedings of ACL-08: HLT, pages 843–851.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>