<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015820">
<title confidence="0.981648">
Insertion and Deletion Models for Statistical Machine Translation
</title>
<author confidence="0.994535">
Matthias Huck and Hermann Ney
</author>
<affiliation confidence="0.991316">
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
</affiliation>
<address confidence="0.645429">
D-52056 Aachen, Germany
</address>
<email confidence="0.997553">
{huck,ney}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.995608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998693190476191">
We investigate insertion and deletion models
for hierarchical phrase-based statistical ma-
chine translation. Insertion and deletion mod-
els are designed as a means to avoid the omis-
sion of content words in the hypotheses. In
our case, they are implemented as phrase-level
feature functions which count the number of
inserted or deleted words. An English word is
considered inserted or deleted based on lex-
ical probabilities with the words on the for-
eign language side of the phrase. Related tech-
niques have been employed before by Och et
al. (2003) in an n-best reranking framework
and by Mauser et al. (2006) and Zens (2008)
in a standard phrase-based translation system.
We propose novel thresholding methods in
this work and study insertion and deletion fea-
tures which are based on two different types of
lexicon models. We give an extensive exper-
imental evaluation of all these variants on the
NIST Chinese—*English translation task.
</bodyText>
<sectionHeader confidence="0.983839" genericHeader="categories and subject descriptors">
1 Insertion and Deletion Models
</sectionHeader>
<bodyText confidence="0.999860464285715">
In hierarchical phrase-based translation (Chiang,
2005), we deal with rules X —* (α, Q,— ) where
(α, Q) is a bilingual phrase pair that may contain
symbols from a non-terminal set, i.e. α E (N U
VF)+ and Q E (N UVE)+, where VF and VE are the
source and target vocabulary, respectively, and N is
a non-terminal set which is shared by source and tar-
get. The left-hand side of the rule is a non-terminal
symbol X E N, and the — relation denotes a one-
to-one correspondence between the non-terminals in
α and in Q. Let Jα denote the number of terminal
symbols in α and I,3 the number of terminal sym-
bols in Q. Indexing α with j, i.e. the symbol αj,
1 &lt; j &lt; Jα, denotes the j-th terminal symbol on
the source side of the phrase pair (α, Q), and analo-
gous with Qz, 1 &lt; i &lt; I,3, on the target side.
With these notational conventions, we now de-
fine our insertion and deletion models, each in both
source-to-target and target-to-source direction. We
give phrase-level scoring functions for the four fea-
tures. In our implementation, the feature values are
precomputed and written to the phrase table. The
features are then incorporated directly into the log-
linear model combination of the decoder.
Our insertion model in source-to-target direction
ts2tIns(�) counts the number of inserted words on the
target side Q of a hierarchical rule with respect to the
source side α of the rule:
</bodyText>
<equation confidence="0.986565333333333">
Ia Ja
ts2tIns(α, Q) = H [p(Qz|αj) &lt; Tαj] (1)
z=1 j=1
</equation>
<bodyText confidence="0.999769461538462">
Here, [�] denotes a true or false statement: The result
is 1 if the condition is true and 0 if the condition is
false. The model considers an occurrence of a tar-
get word e an insertion iff no source word f exists
within the phrase where the lexical translation prob-
ability p(e|f) is greater than a corresponding thresh-
old Tf. We employ lexical translation probabilities
from two different types of lexicon models, a model
which is extracted from word-aligned training data
and—given the word alignment matrix—relies on
pure relative frequencies, and the IBM model 1 lex-
icon (cf. Section 2). For Tf, previous authors have
used a fixed heuristic value which was equal for all
</bodyText>
<page confidence="0.906500333333333">
347
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 347–351,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.9997564">
f ∈ Vf. In Section 3, we describe how such a global
threshold can be computed and set in a reasonable
way based on the characteristics of the model. We
also propose several novel thresholding techniques
with distinct thresholds τf for each source word f.
In an analogous manner to the source-to-target di-
rection, the insertion model in target-to-source di-
rection tt2sIns(·) counts the number of inserted words
on the source side α of a hierarchical rule with re-
spect to the target side β of the rule:
</bodyText>
<equation confidence="0.859038">
[p(αj|βi) &lt; τβz] (2)
</equation>
<bodyText confidence="0.99995505882353">
Target-to-source lexical translation probabilities
p(f|e) are thresholded with values τe which may be
distinct for each target word e. The model consid-
ers an occurrence of a source word f an insertion iff
no target word e exists within the phrase with p(f|e)
greater than or equal to τe.
Our deletion model, compared to the insertion
model, interchanges the connection of the direction
of the lexical probabilities and the order of source
and target in the sum and product of the term. The
source-to-target deletion model thus differs from the
target-to-source insertion model in that it employs a
source-to-target word-based lexicon model.
The deletion model in source-to-target direction
ts2tDel(·) counts the number of deleted words on the
source side α of a hierarchical rule with respect to
the target side β of the rule:
</bodyText>
<equation confidence="0.695527">
[p(βi|αj) &lt; ταj] (3)
</equation>
<bodyText confidence="0.999783571428572">
It considers an occurrence of a source word f a dele-
tion iff no target word e exists within the phrase with
p(e|f) greater than or equal to τf.
The target-to-source deletion model tt2sDel(·) cor-
respondingly considers an occurrence of a target
word e a deletion iff no source word f exists within
the phrase with p(f|e) greater than or equal to τe:
</bodyText>
<equation confidence="0.499026">
[p(αj|βi) &lt; τβz] (4)
</equation>
<sectionHeader confidence="0.981017" genericHeader="method">
2 Lexicon Models
</sectionHeader>
<bodyText confidence="0.99995">
We restrict ourselves to the description of the
source-to-target direction of the models.
</bodyText>
<subsectionHeader confidence="0.997287">
2.1 Word Lexicon from Word-Aligned Data
</subsectionHeader>
<bodyText confidence="0.9996595">
Given a word-aligned parallel training corpus, we
are able to estimate single-word based translation
probabilities pRF(e|f) by relative frequency (Koehn
et al., 2003). With N(e, f) denoting counts of
aligned cooccurrences of target word e and source
word f, we can compute
</bodyText>
<equation confidence="0.979711333333333">
N(e,f)
pRF(e|f) = (5)
Pe, N(el, f)
</equation>
<bodyText confidence="0.9998446">
If an occurrence of e has multiple aligned source
words, each of the alignment links contributes with
a fractional count.
We denote this model as relative frequency (RF)
word lexicon.
</bodyText>
<subsectionHeader confidence="0.637394">
2.2 IBM Model 1
</subsectionHeader>
<bodyText confidence="0.999906888888889">
The IBM model 1 lexicon (IBM-1) is the first and
most basic one in a sequence of probabilistic genera-
tive models (Brown et al., 1993). For IBM-1, several
simplifying assumptions are made, so that the proba-
bility of a target sentence eI1 given a source sentence
fJ0 (with f0 = NULL) can be modeled as
The parameters of IBM-1 are estimated iteratively
by means of the Expectation-Maximization algo-
rithm with maximum likelihood as training criterion.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="method">
3 Thresholding Methods
</sectionHeader>
<bodyText confidence="0.999926166666667">
We introduce thresholding methods for insertion and
deletion models which set thresholds based on the
characteristics of the lexicon model that is applied.
For all the following thresholding methods, we dis-
regard entries in the lexicon model with probabilities
that are below a fixed floor value of 10−6. Again, we
restrict ourselves to the description of the source-to-
target direction.
individual τf is a distinct value for each f, com-
puted as the arithmetic average of all entries
p(e|f) of any e with the given f in the lexicon
model.
</bodyText>
<equation confidence="0.999037043478261">
tt2sIns(α, β) =
YIa
i=1
J.
X
j=1
ts2tDel(α, β) =
YIa
i=1
XJ.
j=1
tt2sDel(α, β) =
J.
Y
j=1
XIa
i=1
J
1 YI
Pr(eI 1|fJ 1 ) = (J + 1)I
X
pibm1(ei|fj) . (6)
i=1 j=0
</equation>
<page confidence="0.989967">
348
</page>
<table confidence="0.999957451612903">
NIST Chinese→English MT06 (Dev) MT08 (Test)
BLEU [%] TER [%] BLEU [%] TER [%]
Baseline (with s2t+t2s RF word lexicons) 32.6 61.2 25.2 66.6
+ s2t+t2s insertion model (RF, individual) 32.9 61.4 25.7 66.2
+ s2t+t2s insertion model (RF, global) 32.8 61.8 25.7 66.7
+ s2t+t2s insertion model (RF, histogram 10) 32.9 61.7 25.5 66.5
+ s2t+t2s insertion model (RF, all) 32.8 62.0 26.1 66.7
+ s2t+t2s insertion model (RF, median) 32.9 62.1 25.7 67.1
+ s2t+t2s deletion model (RF, individual) 32.7 61.4 25.6 66.5
+ s2t+t2s deletion model (RF, global) 33.0 61.3 25.8 66.1
+ s2t+t2s deletion model (RF, histogram 10) 32.9 61.4 26.0 66.1
+ s2t+t2s deletion model (RF, all) 33.0 61.4 25.9 66.4
+ s2t+t2s deletion model (RF, median) 32.9 61.5 25.8 66.7
+ s2t+t2s insertion model (IBM-1, individual) 33.0 61.4 26.1 66.4
+ s2t+t2s insertion model (IBM-1, global) 33.0 61.6 25.9 66.5
+ s2t+t2s insertion model (IBM-1, histogram 10) 33.7 61.3 26.2 66.5
+ s2t+t2s insertion model (IBM-1, median) 33.0 61.3 26.0 66.4
+ s2t+t2s deletion model (IBM-1, individual) 32.8 61.5 26.0 66.2
+ s2t+t2s deletion model (IBM-1, global) 32.9 61.3 25.9 66.1
+ s2t+t2s deletion model (IBM-1, histogram 10) 32.8 61.2 25.7 66.0
+ s2t+t2s deletion model (IBM-1, median) 32.8 61.6 25.6 66.7
+ s2t insertion + s2t deletion model (IBM-1, individual) 32.7 62.3 25.7 67.1
+ s2t insertion + t2s deletion model (IBM-1, individual) 32.7 62.2 25.9 66.8
+ t2s insertion + s2t deletion model (IBM-1, individual) 33.1 61.3 25.9 66.2
+ t2s insertion + t2s deletion model (IBM-1, individual) 33.0 61.3 26.1 66.0
+ source+target unaligned word count 32.3 61.8 25.6 66.7
+ phrase-level s2t+t2s IBM-1 word lexicons 33.8 60.5 26.9 65.4
+ source+target unaligned word count 34.0 60.4 26.7 65.8
+ s2t+t2s insertion model (IBM-1, histogram 10) 34.0 60.3 26.8 65.2
+ phrase-level s2t+t2s DWL + triplets + discrim. RO 34.8 59.8 27.7 64.7
+ s2t+t2s insertion model (RF, individual) 35.0 59.5 27.8 64.4
</table>
<tableCaption confidence="0.9988045">
Table 1: Experimental results for the NIST Chinese→English translation task (truecase). s2t denotes source-to-target
scoring, t2s target-to-source scoring. Bold font indicates results that are significantly better than the baseline (p &lt; .1).
</tableCaption>
<bodyText confidence="0.98146495">
global The same value Tf = T is used for all f.
We compute this global threshold by averaging
over the individual thresholds.1
histogram n Tf is a distinct value for each f. Tf is
set to the value of the n + 1-th largest probabil-
ity p(e|f) of any e with the given f.
1Concrete values from our experiments are: 0.395847 for
the source-to-target RF lexicon, 0.48127 for the target-to-source
RF lexicon. 0.0512856 for the source-to-target IBM-1, and
0.0453709 for the target-to-source IBM-1. Mauser et al. (2006)
mention that they chose their heuristic thresholds for use with
IBM-1 between 10−1 and 10−4.
all All entries with probabilities larger than the floor
value are not thresholded. This variant may be
considered as histogram oc. We only apply it
with RF lexicons.
median Tf is a median-based distinct value for each
f, i.e. it is set to the value that separates the
higher half of the entries from the lower half of
the entries p(e|f) for the given f.
</bodyText>
<sectionHeader confidence="0.998769" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999804">
We present empirical results obtained with the dif-
ferent insertion and deletion model variants on the
</bodyText>
<page confidence="0.995052">
349
</page>
<note confidence="0.243781">
Chinese—*English 2008 NIST task.2
</note>
<subsectionHeader confidence="0.967027">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999980428571429">
To set up our systems, we employ the open source
statistical machine translation toolkit Jane (Vilar et
al., 2010; Vilar et al., 2012), which is freely avail-
able for non-commercial use. Jane provides efficient
C++ implementations for hierarchical phrase extrac-
tion, optimization of log-linear feature weights, and
parsing-based decoding algorithms. In our experi-
ments, we use the cube pruning algorithm (Huang
and Chiang, 2007) to carry out the search.
We work with a parallel training corpus of 3.0M
Chinese-English sentence pairs (77.5M Chinese /
81.0M English running words). The counts for
the RF lexicon models are computed from a sym-
metrized word alignment (Och and Ney, 2003), the
IBM-1 models are produced with GIZA++. When
extracting phrases, we apply several restrictions, in
particular a maximum length of 10 on source and
target side for lexical phrases, a length limit of five
(including non-terminal symbols) for hierarchical
phrases, and no more than two gaps per phrase.
The models integrated into the baseline are: phrase
translation probabilities and RF lexical translation
probabilities on phrase level, each for both transla-
tion directions, length penalties on word and phrase
level, binary features marking hierarchical phrases,
glue rule, and rules with non-terminals at the bound-
aries, source-to-target and target-to-source phrase
length ratios, four binary features marking phrases
that have been seen more than one, two, three or
five times, respectively, and an n-gram language
model. The language model is a 4-gram with modi-
fied Kneser-Ney smoothing which was trained with
the SRILM toolkit (Stolcke, 2002) on a large collec-
tion of English data including the target side of the
parallel corpus and the LDC Gigaword v3.
Model weights are optimized against BLEU (Pa-
pineni et al., 2002) with standard Minimum Error
Rate Training (Och, 2003), performance is measured
with BLEU and TER (Snover et al., 2006). We em-
ploy MT06 as development set, MT08 is used as un-
seen test set. The empirical evaluation of all our se-
tups is presented in Table 1.
</bodyText>
<footnote confidence="0.9972575">
2http://www.itl.nist.gov/iad/mig/tests/
mt/2008/
</footnote>
<subsectionHeader confidence="0.996026">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999989047619048">
With the best model variant, we obtain a significant
improvement (90% confidence) of +1.0 points BLEU
over the baseline on MT08. A consistent trend to-
wards one of the variants cannot be observed. The
results on the test set with RF lexicons or IBM-1, in-
sertion or deletion models, and (in most of the cases)
with all of the thresholding methods are roughly at
the same level. For comparison we also give a result
with an unaligned word count model (+0.4 BLEU).
Huck et al. (2011) recently reported substantial
improvements over typical hierarchical baseline se-
tups by just including phrase-level IBM-1 scores.
When we add the IBM-1 models directly, our base-
line is outperformed by +1.7 BLEU. We tried to
get improvements with insertion and deletion mod-
els over this setup again, but the positive effect was
largely diminished. In one of our strongest setups,
which includes discriminative word lexicon models
(DWL), triplet lexicon models and a discriminative
reordering model (discrim. RO) (Huck et al., 2012),
insertion models still yield a minimal gain, though.
</bodyText>
<sectionHeader confidence="0.99386" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999971277777778">
Our results with insertion and deletion models for
Chinese—*English hierarchical machine translation
are twofold. On the one hand, we achieved sig-
nificant improvements over a standard hierarchical
baseline. We were also able to report a slight gain
by adding the models to a very strong setup with
discriminative word lexicons, triplet lexicon mod-
els and a discriminative reordering model. On the
other hand, the positive impact of the models was
mainly noticeable when we exclusively applied lex-
ical smoothing with word lexicons which are simply
extracted from word-aligned training data, which
is however the standard technique in most state-of-
the-art systems. If we included phrase-level lexical
scores with IBM model 1 as well, the systems barely
benefited from our insertion and deletion models.
Compared to an unaligned word count model, inser-
tion and deletion models perform well.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999793666666667">
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
</bodyText>
<page confidence="0.99542">
350
</page>
<sectionHeader confidence="0.99007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857329411765">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathemat-
ics of Statistical Machine Translation: Parameter Es-
timation. Computational Linguistics, 19(2):263–311,
June.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc. of
the 43rd Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263–270, Ann Arbor,
MI, USA, June.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proc. of the Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL), pages 144–151, Prague,
Czech Republic, June.
Matthias Huck, Saab Mansour, Simon Wiesler, and Her-
mann Ney. 2011. Lexicon Models for Hierarchi-
cal Phrase-Based Machine Translation. In Proc. of
the Int. Workshop on Spoken Language Translation
(IWSLT), pages 191–198, San Francisco, CA, USA,
December.
Matthias Huck, Stephan Peitz, Markus Freitag, and Her-
mann Ney. 2012. Discriminative Reordering Exten-
sions for Hierarchical Phrase-Based Machine Transla-
tion. In Proc. of the 16th Annual Conference of the Eu-
ropean Association for Machine Translation (EAMT),
Trento, Italy, May.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Human Language Technology Conf. / North
American Chapter of the Assoc. for Computational
Linguistics (HLT-NAACL), pages 127–133, Edmonton,
Canada, May/June.
Arne Mauser, Richard Zens, Evgeny Matusov, Saˇsa
Hasan, and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of the Int. Workshop on Spoken
Language Translation (IWSLT), pages 103–110, Ky-
oto, Japan, November.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51, March.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for Statistical Machine Translation. Technical re-
port, Johns Hopkins University 2003 Summer Work-
shop on Language Engineering, Center for Language
and Speech Processing, Baltimore, MD, USA, August.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 160–167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Evalu-
ation of Machine Translation. In Proc. of the 40th An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 311–318, Philadelphia, PA, USA,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), pages 223–231, Cambridge,
MA, USA, August.
Andreas Stolcke. 2002. SRILM – an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, USA, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Transla-
tion, Extended with Reordering and Lexicon Models.
In ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, pages 262–270,
Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2012. Jane: an advanced freely available hier-
archical machine translation toolkit. Machine Trans-
lation, pages 1–20. http://dx.doi.org/10.1007/s10590-
011-9120-y.
Richard Zens. 2008. Phrase-based Statistical Machine
Translation: Models, Search, Training. Ph.D. thesis,
RWTH Aachen University, Aachen, Germany, Febru-
ary.
</reference>
<page confidence="0.998669">
351
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.666305">Insertion and Deletion Models for Statistical Machine Translation Huck Human Language Technology and Pattern Recognition</title>
<affiliation confidence="0.9333995">Computer Science RWTH Aachen</affiliation>
<address confidence="0.989497">D-52056 Aachen,</address>
<abstract confidence="0.916538034482759">We investigate insertion and deletion models for hierarchical phrase-based statistical machine translation. Insertion and deletion models are designed as a means to avoid the omission of content words in the hypotheses. In our case, they are implemented as phrase-level feature functions which count the number of inserted or deleted words. An English word is considered inserted or deleted based on lexical probabilities with the words on the foreign language side of the phrase. Related techniques have been employed before by Och et al. (2003) in an n-best reranking framework and by Mauser et al. (2006) and Zens (2008) in a standard phrase-based translation system. We propose novel thresholding methods in this work and study insertion and deletion features which are based on two different types of lexicon models. We give an extensive experimental evaluation of all these variants on the translation task. 1 Insertion and Deletion Models In hierarchical phrase-based translation (Chiang, we deal with rules X a bilingual phrase pair that may contain from a non-terminal set, i.e. α U Q where the and target vocabulary, respectively, and a non-terminal set which is shared by source and target. The left-hand side of the rule is a non-terminal X and the denotes a oneto-one correspondence between the non-terminals in and in Q. Let the number of terminal in α and number of terminal symin Q. Indexing α with j, i.e. the symbol denotes the j-th terminal symbol on source side of the phrase pair and analowith on the target side. With these notational conventions, we now define our insertion and deletion models, each in both source-to-target and target-to-source direction. We give phrase-level scoring functions for the four features. In our implementation, the feature values are precomputed and written to the phrase table. The features are then incorporated directly into the loglinear model combination of the decoder. Our insertion model in source-to-target direction the number of inserted words on the target side Q of a hierarchical rule with respect to the source side α of the rule: = a true or false statement: The result is 1 if the condition is true and 0 if the condition is false. The model considers an occurrence of a target word e an insertion iff no source word f exists within the phrase where the lexical translation probgreater than a corresponding thresh- We employ lexical translation probabilities from two different types of lexicon models, a model which is extracted from word-aligned training data and—given the word alignment matrix—relies on pure relative frequencies, and the IBM model 1 lex- (cf. Section 2). For previous authors have used a fixed heuristic value which was equal for all 347 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language pages 347–351, Canada, June 3-8, 2012. Association for Computational Linguistics In Section 3, we describe how such a global threshold can be computed and set in a reasonable way based on the characteristics of the model. We also propose several novel thresholding techniques distinct thresholds each source word In an analogous manner to the source-to-target direction, the insertion model in target-to-source dithe number of inserted words the source side a hierarchical rule with reto the target side the rule: Target-to-source lexical translation probabilities thresholded with values may be for each target word The model considan occurrence of a source word insertion iff target word within the phrase with than or equal to Our deletion model, compared to the insertion model, interchanges the connection of the direction of the lexical probabilities and the order of source and target in the sum and product of the term. The source-to-target deletion model thus differs from the target-to-source insertion model in that it employs a source-to-target word-based lexicon model. The deletion model in source-to-target direction the number of deleted words on the side a hierarchical rule with respect to target side the rule: considers an occurrence of a source word deleiff no target word within the phrase with than or equal to target-to-source deletion model correspondingly considers an occurrence of a target deletion iff no source word within phrase with than or equal to 2 Lexicon Models We restrict ourselves to the description of the source-to-target direction of the models. 2.1 Word Lexicon from Word-Aligned Data Given a word-aligned parallel training corpus, we are able to estimate single-word based translation relative frequency (Koehn al., 2003). With counts of cooccurrences of target word source we can compute = an occurrence of multiple aligned source words, each of the alignment links contributes with a fractional count. We denote this model as relative frequency (RF) word lexicon. 2.2 IBM Model 1 The IBM model 1 lexicon (IBM-1) is the first and most basic one in a sequence of probabilistic generative models (Brown et al., 1993). For IBM-1, several simplifying assumptions are made, so that the probaof a target sentence a source sentence can be modeled as The parameters of IBM-1 are estimated iteratively by means of the Expectation-Maximization algorithm with maximum likelihood as training criterion. 3 Thresholding Methods We introduce thresholding methods for insertion and deletion models which set thresholds based on the characteristics of the lexicon model that is applied. For all the following thresholding methods, we disregard entries in the lexicon model with probabilities are below a fixed floor value of Again, we restrict ourselves to the description of the source-totarget direction. a distinct value for each computed as the arithmetic average of all entries any the given the lexicon model. = X = = Y J = X 348 MT06 (Dev) MT08 (Test) Baseline (with s2t+t2s RF word lexicons) 32.6 61.2 25.2 66.6 + s2t+t2s insertion model (RF, individual) 32.9 61.4 25.7 66.2 + s2t+t2s insertion model (RF, global) 32.8 61.8 25.7 66.7 + s2t+t2s insertion model (RF, histogram 10) 32.9 61.7 25.5 66.5 + s2t+t2s insertion model (RF, all) 32.8 62.0 26.1 66.7 + s2t+t2s insertion model (RF, median) 32.9 62.1 25.7 67.1 + s2t+t2s deletion model (RF, individual) 32.7 61.4 25.6 66.5 + s2t+t2s deletion model (RF, global) 33.0 61.3 25.8 66.1 + s2t+t2s deletion model (RF, histogram 10) 32.9 61.4 26.0 66.1 + s2t+t2s deletion model (RF, all) 33.0 61.4 25.9 66.4 + s2t+t2s deletion model (RF, median) 32.9 61.5 25.8 66.7 + s2t+t2s insertion model (IBM-1, individual) 33.0 61.4 26.1 66.4 + s2t+t2s insertion model (IBM-1, global) 33.0 61.6 25.9 66.5 + s2t+t2s insertion model (IBM-1, histogram 10) 33.7 61.3 26.2 66.5 + s2t+t2s insertion model (IBM-1, median) 33.0 61.3 26.0 66.4 + s2t+t2s deletion model (IBM-1, individual) 32.8 61.5 26.0 66.2 + s2t+t2s deletion model (IBM-1, global) 32.9 61.3 25.9 66.1 + s2t+t2s deletion model (IBM-1, histogram 10) 32.8 61.2 25.7 66.0 + s2t+t2s deletion model (IBM-1, median) 32.8 61.6 25.6 66.7 + s2t insertion + s2t deletion model (IBM-1, individual) 32.7 62.3 25.7 67.1 + s2t insertion + t2s deletion model (IBM-1, individual) 32.7 62.2 25.9 66.8 + t2s insertion + s2t deletion model (IBM-1, individual) 33.1 61.3 25.9 66.2 + t2s insertion + t2s deletion model (IBM-1, individual) 33.0 61.3 26.1 66.0 + source+target unaligned word count 32.3 61.8 25.6 66.7 + phrase-level s2t+t2s IBM-1 word lexicons 33.8 60.5 26.9 65.4 + source+target unaligned word count 34.0 60.4 26.7 65.8 + s2t+t2s insertion model (IBM-1, histogram 10) 34.0 60.3 26.8 65.2 + phrase-level s2t+t2s DWL + triplets + discrim. RO 34.8 59.8 27.7 64.7 + s2t+t2s insertion model (RF, individual) 35.0 59.5 27.8 64.4 1: Experimental results for the NIST translation task (truecase). source-to-target scoring. Bold font indicates results that are significantly better than the baseline &lt; same value used for all We compute this global threshold by averaging the individual a distinct value for each to the value of the largest probabilany the given values from our experiments are: 0.395847 for the source-to-target RF lexicon, 0.48127 for the target-to-source RF lexicon. 0.0512856 for the source-to-target IBM-1, and 0.0453709 for the target-to-source IBM-1. Mauser et al. (2006) mention that they chose their heuristic thresholds for use with between and entries with probabilities larger than the floor value are not thresholded. This variant may be as We only apply it with RF lexicons. a median-based distinct value for each i.e. it is set to the value that separates the higher half of the entries from the lower half of entries the given 4 Experimental Evaluation We present empirical results obtained with the different insertion and deletion model variants on the 349 2008 NIST 4.1 Experimental Setup To set up our systems, we employ the open source statistical machine translation toolkit Jane (Vilar et al., 2010; Vilar et al., 2012), which is freely available for non-commercial use. Jane provides efficient C++ implementations for hierarchical phrase extraction, optimization of log-linear feature weights, and parsing-based decoding algorithms. In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search. We work with a parallel training corpus of 3.0M Chinese-English sentence pairs (77.5M Chinese / 81.0M English running words). The counts for the RF lexicon models are computed from a symmetrized word alignment (Och and Ney, 2003), the models are produced with When extracting phrases, we apply several restrictions, in particular a maximum length of 10 on source and target side for lexical phrases, a length limit of five (including non-terminal symbols) for hierarchical phrases, and no more than two gaps per phrase. The models integrated into the baseline are: phrase translation probabilities and RF lexical translation probabilities on phrase level, each for both translation directions, length penalties on word and phrase level, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, source-to-target and target-to-source phrase length ratios, four binary features marking phrases that have been seen more than one, two, three or times, respectively, and an language model. The language model is a 4-gram with modified Kneser-Ney smoothing which was trained with the SRILM toolkit (Stolcke, 2002) on a large collection of English data including the target side of the parallel corpus and the LDC Gigaword v3. weights are optimized against (Papineni et al., 2002) with standard Minimum Error Rate Training (Och, 2003), performance is measured et al., 2006). We employ MT06 as development set, MT08 is used as unseen test set. The empirical evaluation of all our setups is presented in Table 1. mt/2008/ 4.2 Experimental Results With the best model variant, we obtain a significant (90% confidence) of +1.0 points over the baseline on MT08. A consistent trend towards one of the variants cannot be observed. The results on the test set with RF lexicons or IBM-1, insertion or deletion models, and (in most of the cases) with all of the thresholding methods are roughly at the same level. For comparison we also give a result an unaligned word count model (+0.4 Huck et al. (2011) recently reported substantial improvements over typical hierarchical baseline setups by just including phrase-level IBM-1 scores. When we add the IBM-1 models directly, our baseis outperformed by +1.7 We tried to get improvements with insertion and deletion models over this setup again, but the positive effect was largely diminished. In one of our strongest setups, which includes discriminative word lexicon models (DWL), triplet lexicon models and a discriminative reordering model (discrim. RO) (Huck et al., 2012), insertion models still yield a minimal gain, though. 5 Conclusion Our results with insertion and deletion models for hierarchical machine translation are twofold. On the one hand, we achieved significant improvements over a standard hierarchical baseline. We were also able to report a slight gain by adding the models to a very strong setup with discriminative word lexicons, triplet lexicon models and a discriminative reordering model. On the other hand, the positive impact of the models was mainly noticeable when we exclusively applied lexical smoothing with word lexicons which are simply extracted from word-aligned training data, which is however the standard technique in most state-ofthe-art systems. If we included phrase-level lexical scores with IBM model 1 as well, the systems barely benefited from our insertion and deletion models.</abstract>
<note confidence="0.772685285714286">Compared to an unaligned word count model, insertion and deletion models perform well. Acknowledgments This work was achieved as part of the Quaero Programme, funded by OSEO, French State agency for innovation. 350</note>
<title confidence="0.905903">References</title>
<author confidence="0.967369">Peter F Brown</author>
<author confidence="0.967369">Stephen A Della Pietra</author>
<author confidence="0.967369">Vincent J Della</author>
<note confidence="0.747738905882353">Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Es- 19(2):263–311, June. David Chiang. 2005. A Hierarchical Phrase-Based for Statistical Machine Translation. In of the 43rd Annual Meeting of the Assoc. for Computa- Linguistics pages 263–270, Ann Arbor, MI, USA, June. Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In Proc. of the Annual Meeting of the Assoc. for Com- Linguistics pages 144–151, Prague, Czech Republic, June. Matthias Huck, Saab Mansour, Simon Wiesler, and Hermann Ney. 2011. Lexicon Models for Hierarchi- Phrase-Based Machine Translation. In of the Int. Workshop on Spoken Language Translation pages 191–198, San Francisco, CA, USA, December. Matthias Huck, Stephan Peitz, Markus Freitag, and Hermann Ney. 2012. Discriminative Reordering Extensions for Hierarchical Phrase-Based Machine Transla- In of the 16th Annual Conference of the Eu- Association for Machine Translation Trento, Italy, May. Philipp Koehn, Franz Joseph Och, and Daniel Marcu. Statistical Phrase-Based Translation. In of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational pages 127–133, Edmonton, Canada, May/June. Arne Mauser, Richard Zens, Evgeny Matusov, Saˇsa Hasan, and Hermann Ney. 2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 In of the Int. Workshop on Spoken Translation pages 103–110, Kyoto, Japan, November. Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. 29(1):19–51, March. Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syntax for Statistical Machine Translation. Technical report, Johns Hopkins University 2003 Summer Workshop on Language Engineering, Center for Language and Speech Processing, Baltimore, MD, USA, August. Franz Josef Och. 2003. Minimum Error Rate Training Statistical Machine Translation. In of the Annual Meeting of the Assoc. for Computational Linguispages 160–167, Sapporo, Japan, July. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a Method for Automatic Evaluof Machine Translation. In of the 40th Annual Meeting of the Assoc. for Computational Linguispages 311–318, Philadelphia, PA, USA, July. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annota- In of the Assoc. for Machine Translation the Americas pages 223–231, Cambridge, MA, USA, August. Andreas Stolcke. 2002. SRILM – an Extensible Lan- Modeling Toolkit. In of the Int. Conf. Spoken Language Processing volume 3, Denver, CO, USA, September. David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models. 2010 Joint Fifth Workshop on Statistical Ma- Translation and Metrics pages 262–270, Uppsala, Sweden, July. David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2012. Jane: an advanced freely available hiermachine translation toolkit. Transpages 1–20. http://dx.doi.org/10.1007/s10590- 011-9120-y. Zens. 2008. Statistical Machine Models, Search, Ph.D. thesis, RWTH Aachen University, Aachen, Germany, February. 351</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6090" citStr="Brown et al., 1993" startWordPosition="1018" endWordPosition="1021">arallel training corpus, we are able to estimate single-word based translation probabilities pRF(e|f) by relative frequency (Koehn et al., 2003). With N(e, f) denoting counts of aligned cooccurrences of target word e and source word f, we can compute N(e,f) pRF(e|f) = (5) Pe, N(el, f) If an occurrence of e has multiple aligned source words, each of the alignment links contributes with a fractional count. We denote this model as relative frequency (RF) word lexicon. 2.2 IBM Model 1 The IBM model 1 lexicon (IBM-1) is the first and most basic one in a sequence of probabilistic generative models (Brown et al., 1993). For IBM-1, several simplifying assumptions are made, so that the probability of a target sentence eI1 given a source sentence fJ0 (with f0 = NULL) can be modeled as The parameters of IBM-1 are estimated iteratively by means of the Expectation-Maximization algorithm with maximum likelihood as training criterion. 3 Thresholding Methods We introduce thresholding methods for insertion and deletion models which set thresholds based on the characteristics of the lexicon model that is applied. For all the following thresholding methods, we disregard entries in the lexicon model with probabilities t</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="1288" citStr="Chiang, 2005" startWordPosition="192" endWordPosition="193">obabilities with the words on the foreign language side of the phrase. Related techniques have been employed before by Och et al. (2003) in an n-best reranking framework and by Mauser et al. (2006) and Zens (2008) in a standard phrase-based translation system. We propose novel thresholding methods in this work and study insertion and deletion features which are based on two different types of lexicon models. We give an extensive experimental evaluation of all these variants on the NIST Chinese—*English translation task. 1 Insertion and Deletion Models In hierarchical phrase-based translation (Chiang, 2005), we deal with rules X —* (α, Q,— ) where (α, Q) is a bilingual phrase pair that may contain symbols from a non-terminal set, i.e. α E (N U VF)+ and Q E (N UVE)+, where VF and VE are the source and target vocabulary, respectively, and N is a non-terminal set which is shared by source and target. The left-hand side of the rule is a non-terminal symbol X E N, and the — relation denotes a oneto-one correspondence between the non-terminals in α and in Q. Let Jα denote the number of terminal symbols in α and I,3 the number of terminal symbols in Q. Indexing α with j, i.e. the symbol αj, 1 &lt; j &lt; Jα,</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proc. of the 43rd Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 263–270, Ann Arbor, MI, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10884" citStr="Huang and Chiang, 2007" startWordPosition="1815" endWordPosition="1818">ven f. 4 Experimental Evaluation We present empirical results obtained with the different insertion and deletion model variants on the 349 Chinese—*English 2008 NIST task.2 4.1 Experimental Setup To set up our systems, we employ the open source statistical machine translation toolkit Jane (Vilar et al., 2010; Vilar et al., 2012), which is freely available for non-commercial use. Jane provides efficient C++ implementations for hierarchical phrase extraction, optimization of log-linear feature weights, and parsing-based decoding algorithms. In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search. We work with a parallel training corpus of 3.0M Chinese-English sentence pairs (77.5M Chinese / 81.0M English running words). The counts for the RF lexicon models are computed from a symmetrized word alignment (Och and Ney, 2003), the IBM-1 models are produced with GIZA++. When extracting phrases, we apply several restrictions, in particular a maximum length of 10 on source and target side for lexical phrases, a length limit of five (including non-terminal symbols) for hierarchical phrases, and no more than two gaps per phrase. The models integrated into the baseline </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 144–151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Saab Mansour</author>
<author>Simon Wiesler</author>
<author>Hermann Ney</author>
</authors>
<title>Lexicon Models for Hierarchical Phrase-Based Machine Translation.</title>
<date>2011</date>
<booktitle>In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>191--198</pages>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="13077" citStr="Huck et al. (2011)" startWordPosition="2171" endWordPosition="2174"> empirical evaluation of all our setups is presented in Table 1. 2http://www.itl.nist.gov/iad/mig/tests/ mt/2008/ 4.2 Experimental Results With the best model variant, we obtain a significant improvement (90% confidence) of +1.0 points BLEU over the baseline on MT08. A consistent trend towards one of the variants cannot be observed. The results on the test set with RF lexicons or IBM-1, insertion or deletion models, and (in most of the cases) with all of the thresholding methods are roughly at the same level. For comparison we also give a result with an unaligned word count model (+0.4 BLEU). Huck et al. (2011) recently reported substantial improvements over typical hierarchical baseline setups by just including phrase-level IBM-1 scores. When we add the IBM-1 models directly, our baseline is outperformed by +1.7 BLEU. We tried to get improvements with insertion and deletion models over this setup again, but the positive effect was largely diminished. In one of our strongest setups, which includes discriminative word lexicon models (DWL), triplet lexicon models and a discriminative reordering model (discrim. RO) (Huck et al., 2012), insertion models still yield a minimal gain, though. 5 Conclusion O</context>
</contexts>
<marker>Huck, Mansour, Wiesler, Ney, 2011</marker>
<rawString>Matthias Huck, Saab Mansour, Simon Wiesler, and Hermann Ney. 2011. Lexicon Models for Hierarchical Phrase-Based Machine Translation. In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT), pages 191–198, San Francisco, CA, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Stephan Peitz</author>
<author>Markus Freitag</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Reordering Extensions for Hierarchical Phrase-Based Machine Translation.</title>
<date>2012</date>
<booktitle>In Proc. of the 16th Annual Conference of the European Association for Machine Translation (EAMT),</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="13608" citStr="Huck et al., 2012" startWordPosition="2251" endWordPosition="2254">e also give a result with an unaligned word count model (+0.4 BLEU). Huck et al. (2011) recently reported substantial improvements over typical hierarchical baseline setups by just including phrase-level IBM-1 scores. When we add the IBM-1 models directly, our baseline is outperformed by +1.7 BLEU. We tried to get improvements with insertion and deletion models over this setup again, but the positive effect was largely diminished. In one of our strongest setups, which includes discriminative word lexicon models (DWL), triplet lexicon models and a discriminative reordering model (discrim. RO) (Huck et al., 2012), insertion models still yield a minimal gain, though. 5 Conclusion Our results with insertion and deletion models for Chinese—*English hierarchical machine translation are twofold. On the one hand, we achieved significant improvements over a standard hierarchical baseline. We were also able to report a slight gain by adding the models to a very strong setup with discriminative word lexicons, triplet lexicon models and a discriminative reordering model. On the other hand, the positive impact of the models was mainly noticeable when we exclusively applied lexical smoothing with word lexicons wh</context>
</contexts>
<marker>Huck, Peitz, Freitag, Ney, 2012</marker>
<rawString>Matthias Huck, Stephan Peitz, Markus Freitag, and Hermann Ney. 2012. Discriminative Reordering Extensions for Hierarchical Phrase-Based Machine Translation. In Proc. of the 16th Annual Conference of the European Association for Machine Translation (EAMT), Trento, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada, May/June.</location>
<contexts>
<context position="5615" citStr="Koehn et al., 2003" startWordPosition="933" endWordPosition="936"> target word e exists within the phrase with p(e|f) greater than or equal to τf. The target-to-source deletion model tt2sDel(·) correspondingly considers an occurrence of a target word e a deletion iff no source word f exists within the phrase with p(f|e) greater than or equal to τe: [p(αj|βi) &lt; τβz] (4) 2 Lexicon Models We restrict ourselves to the description of the source-to-target direction of the models. 2.1 Word Lexicon from Word-Aligned Data Given a word-aligned parallel training corpus, we are able to estimate single-word based translation probabilities pRF(e|f) by relative frequency (Koehn et al., 2003). With N(e, f) denoting counts of aligned cooccurrences of target word e and source word f, we can compute N(e,f) pRF(e|f) = (5) Pe, N(el, f) If an occurrence of e has multiple aligned source words, each of the alignment links contributes with a fractional count. We denote this model as relative frequency (RF) word lexicon. 2.2 IBM Model 1 The IBM model 1 lexicon (IBM-1) is the first and most basic one in a sequence of probabilistic generative models (Brown et al., 1993). For IBM-1, several simplifying assumptions are made, so that the probability of a target sentence eI1 given a source senten</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 127–133, Edmonton, Canada, May/June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Richard Zens</author>
<author>Evgeny Matusov</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Evaluation.</title>
<date>2006</date>
<booktitle>The RWTH Statistical Machine Translation System for the IWSLT</booktitle>
<pages>103--110</pages>
<location>Kyoto, Japan,</location>
<contexts>
<context position="872" citStr="Mauser et al. (2006)" startWordPosition="128" endWordPosition="131">.de Abstract We investigate insertion and deletion models for hierarchical phrase-based statistical machine translation. Insertion and deletion models are designed as a means to avoid the omission of content words in the hypotheses. In our case, they are implemented as phrase-level feature functions which count the number of inserted or deleted words. An English word is considered inserted or deleted based on lexical probabilities with the words on the foreign language side of the phrase. Related techniques have been employed before by Och et al. (2003) in an n-best reranking framework and by Mauser et al. (2006) and Zens (2008) in a standard phrase-based translation system. We propose novel thresholding methods in this work and study insertion and deletion features which are based on two different types of lexicon models. We give an extensive experimental evaluation of all these variants on the NIST Chinese—*English translation task. 1 Insertion and Deletion Models In hierarchical phrase-based translation (Chiang, 2005), we deal with rules X —* (α, Q,— ) where (α, Q) is a bilingual phrase pair that may contain symbols from a non-terminal set, i.e. α E (N U VF)+ and Q E (N UVE)+, where VF and VE are t</context>
<context position="9819" citStr="Mauser et al. (2006)" startWordPosition="1644" endWordPosition="1647">arget-to-source scoring. Bold font indicates results that are significantly better than the baseline (p &lt; .1). global The same value Tf = T is used for all f. We compute this global threshold by averaging over the individual thresholds.1 histogram n Tf is a distinct value for each f. Tf is set to the value of the n + 1-th largest probability p(e|f) of any e with the given f. 1Concrete values from our experiments are: 0.395847 for the source-to-target RF lexicon, 0.48127 for the target-to-source RF lexicon. 0.0512856 for the source-to-target IBM-1, and 0.0453709 for the target-to-source IBM-1. Mauser et al. (2006) mention that they chose their heuristic thresholds for use with IBM-1 between 10−1 and 10−4. all All entries with probabilities larger than the floor value are not thresholded. This variant may be considered as histogram oc. We only apply it with RF lexicons. median Tf is a median-based distinct value for each f, i.e. it is set to the value that separates the higher half of the entries from the lower half of the entries p(e|f) for the given f. 4 Experimental Evaluation We present empirical results obtained with the different insertion and deletion model variants on the 349 Chinese—*English 20</context>
</contexts>
<marker>Mauser, Zens, Matusov, Hasan, Ney, 2006</marker>
<rawString>Arne Mauser, Richard Zens, Evgeny Matusov, Saˇsa Hasan, and Hermann Ney. 2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation. In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT), pages 103–110, Kyoto, Japan, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11139" citStr="Och and Ney, 2003" startWordPosition="1858" endWordPosition="1861">hine translation toolkit Jane (Vilar et al., 2010; Vilar et al., 2012), which is freely available for non-commercial use. Jane provides efficient C++ implementations for hierarchical phrase extraction, optimization of log-linear feature weights, and parsing-based decoding algorithms. In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search. We work with a parallel training corpus of 3.0M Chinese-English sentence pairs (77.5M Chinese / 81.0M English running words). The counts for the RF lexicon models are computed from a symmetrized word alignment (Och and Ney, 2003), the IBM-1 models are produced with GIZA++. When extracting phrases, we apply several restrictions, in particular a maximum length of 10 on source and target side for lexical phrases, a length limit of five (including non-terminal symbols) for hierarchical phrases, and no more than two gaps per phrase. The models integrated into the baseline are: phrase translation probabilities and RF lexical translation probabilities on phrase level, each for both translation directions, length penalties on word and phrase level, binary features marking hierarchical phrases, glue rule, and rules with non-te</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2003</date>
<booktitle>Summer Workshop on Language Engineering, Center for Language and Speech Processing,</booktitle>
<tech>Technical report,</tech>
<institution>Johns Hopkins University</institution>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2003</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syntax for Statistical Machine Translation. Technical report, Johns Hopkins University 2003 Summer Workshop on Language Engineering, Center for Language and Speech Processing, Baltimore, MD, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="12321" citStr="Och, 2003" startWordPosition="2043" endWordPosition="2044"> rule, and rules with non-terminals at the boundaries, source-to-target and target-to-source phrase length ratios, four binary features marking phrases that have been seen more than one, two, three or five times, respectively, and an n-gram language model. The language model is a 4-gram with modified Kneser-Ney smoothing which was trained with the SRILM toolkit (Stolcke, 2002) on a large collection of English data including the target side of the parallel corpus and the LDC Gigaword v3. Model weights are optimized against BLEU (Papineni et al., 2002) with standard Minimum Error Rate Training (Och, 2003), performance is measured with BLEU and TER (Snover et al., 2006). We employ MT06 as development set, MT08 is used as unseen test set. The empirical evaluation of all our setups is presented in Table 1. 2http://www.itl.nist.gov/iad/mig/tests/ mt/2008/ 4.2 Experimental Results With the best model variant, we obtain a significant improvement (90% confidence) of +1.0 points BLEU over the baseline on MT08. A consistent trend towards one of the variants cannot be observed. The results on the test set with RF lexicons or IBM-1, insertion or deletion models, and (in most of the cases) with all of the</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training for Statistical Machine Translation. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="12267" citStr="Papineni et al., 2002" startWordPosition="2032" endWordPosition="2036">d phrase level, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, source-to-target and target-to-source phrase length ratios, four binary features marking phrases that have been seen more than one, two, three or five times, respectively, and an n-gram language model. The language model is a 4-gram with modified Kneser-Ney smoothing which was trained with the SRILM toolkit (Stolcke, 2002) on a large collection of English data including the target side of the parallel corpus and the LDC Gigaword v3. Model weights are optimized against BLEU (Papineni et al., 2002) with standard Minimum Error Rate Training (Och, 2003), performance is measured with BLEU and TER (Snover et al., 2006). We employ MT06 as development set, MT08 is used as unseen test set. The empirical evaluation of all our setups is presented in Table 1. 2http://www.itl.nist.gov/iad/mig/tests/ mt/2008/ 4.2 Experimental Results With the best model variant, we obtain a significant improvement (90% confidence) of +1.0 points BLEU over the baseline on MT08. A consistent trend towards one of the variants cannot be observed. The results on the test set with RF lexicons or IBM-1, insertion or delet</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proc. of the 40th Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Conf. of the Assoc. for Machine Translation in the Americas (AMTA),</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA, USA,</location>
<contexts>
<context position="12386" citStr="Snover et al., 2006" startWordPosition="2052" endWordPosition="2055">source-to-target and target-to-source phrase length ratios, four binary features marking phrases that have been seen more than one, two, three or five times, respectively, and an n-gram language model. The language model is a 4-gram with modified Kneser-Ney smoothing which was trained with the SRILM toolkit (Stolcke, 2002) on a large collection of English data including the target side of the parallel corpus and the LDC Gigaword v3. Model weights are optimized against BLEU (Papineni et al., 2002) with standard Minimum Error Rate Training (Och, 2003), performance is measured with BLEU and TER (Snover et al., 2006). We employ MT06 as development set, MT08 is used as unseen test set. The empirical evaluation of all our setups is presented in Table 1. 2http://www.itl.nist.gov/iad/mig/tests/ mt/2008/ 4.2 Experimental Results With the best model variant, we obtain a significant improvement (90% confidence) of +1.0 points BLEU over the baseline on MT08. A consistent trend towards one of the variants cannot be observed. The results on the test set with RF lexicons or IBM-1, insertion or deletion models, and (in most of the cases) with all of the thresholding methods are roughly at the same level. For comparis</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Conf. of the Assoc. for Machine Translation in the Americas (AMTA), pages 223–231, Cambridge, MA, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Spoken Language Processing (ICSLP),</booktitle>
<volume>3</volume>
<location>Denver, CO, USA,</location>
<contexts>
<context position="12090" citStr="Stolcke, 2002" startWordPosition="2003" endWordPosition="2004">aseline are: phrase translation probabilities and RF lexical translation probabilities on phrase level, each for both translation directions, length penalties on word and phrase level, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, source-to-target and target-to-source phrase length ratios, four binary features marking phrases that have been seen more than one, two, three or five times, respectively, and an n-gram language model. The language model is a 4-gram with modified Kneser-Ney smoothing which was trained with the SRILM toolkit (Stolcke, 2002) on a large collection of English data including the target side of the parallel corpus and the LDC Gigaword v3. Model weights are optimized against BLEU (Papineni et al., 2002) with standard Minimum Error Rate Training (Och, 2003), performance is measured with BLEU and TER (Snover et al., 2006). We employ MT06 as development set, MT08 is used as unseen test set. The empirical evaluation of all our setups is presented in Table 1. 2http://www.itl.nist.gov/iad/mig/tests/ mt/2008/ 4.2 Experimental Results With the best model variant, we obtain a significant improvement (90% confidence) of +1.0 po</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Spoken Language Processing (ICSLP), volume 3, Denver, CO, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models.</title>
<date>2010</date>
<booktitle>In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>262--270</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10570" citStr="Vilar et al., 2010" startWordPosition="1770" endWordPosition="1773">er than the floor value are not thresholded. This variant may be considered as histogram oc. We only apply it with RF lexicons. median Tf is a median-based distinct value for each f, i.e. it is set to the value that separates the higher half of the entries from the lower half of the entries p(e|f) for the given f. 4 Experimental Evaluation We present empirical results obtained with the different insertion and deletion model variants on the 349 Chinese—*English 2008 NIST task.2 4.1 Experimental Setup To set up our systems, we employ the open source statistical machine translation toolkit Jane (Vilar et al., 2010; Vilar et al., 2012), which is freely available for non-commercial use. Jane provides efficient C++ implementations for hierarchical phrase extraction, optimization of log-linear feature weights, and parsing-based decoding algorithms. In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search. We work with a parallel training corpus of 3.0M Chinese-English sentence pairs (77.5M Chinese / 81.0M English running words). The counts for the RF lexicon models are computed from a symmetrized word alignment (Och and Ney, 2003), the IBM-1 models are produced</context>
</contexts>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models. In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 262–270, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: an advanced freely available hierarchical machine translation toolkit. Machine Translation,</title>
<date>2012</date>
<pages>1--20</pages>
<contexts>
<context position="10591" citStr="Vilar et al., 2012" startWordPosition="1774" endWordPosition="1777">lue are not thresholded. This variant may be considered as histogram oc. We only apply it with RF lexicons. median Tf is a median-based distinct value for each f, i.e. it is set to the value that separates the higher half of the entries from the lower half of the entries p(e|f) for the given f. 4 Experimental Evaluation We present empirical results obtained with the different insertion and deletion model variants on the 349 Chinese—*English 2008 NIST task.2 4.1 Experimental Setup To set up our systems, we employ the open source statistical machine translation toolkit Jane (Vilar et al., 2010; Vilar et al., 2012), which is freely available for non-commercial use. Jane provides efficient C++ implementations for hierarchical phrase extraction, optimization of log-linear feature weights, and parsing-based decoding algorithms. In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search. We work with a parallel training corpus of 3.0M Chinese-English sentence pairs (77.5M Chinese / 81.0M English running words). The counts for the RF lexicon models are computed from a symmetrized word alignment (Och and Ney, 2003), the IBM-1 models are produced with GIZA++. When ex</context>
</contexts>
<marker>Vilar, Stein, Huck, Ney, 2012</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2012. Jane: an advanced freely available hierarchical machine translation toolkit. Machine Translation, pages 1–20. http://dx.doi.org/10.1007/s10590-011-9120-y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
</authors>
<title>Phrase-based Statistical Machine Translation: Models, Search, Training.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen University,</institution>
<location>Aachen, Germany,</location>
<contexts>
<context position="888" citStr="Zens (2008)" startWordPosition="133" endWordPosition="134">te insertion and deletion models for hierarchical phrase-based statistical machine translation. Insertion and deletion models are designed as a means to avoid the omission of content words in the hypotheses. In our case, they are implemented as phrase-level feature functions which count the number of inserted or deleted words. An English word is considered inserted or deleted based on lexical probabilities with the words on the foreign language side of the phrase. Related techniques have been employed before by Och et al. (2003) in an n-best reranking framework and by Mauser et al. (2006) and Zens (2008) in a standard phrase-based translation system. We propose novel thresholding methods in this work and study insertion and deletion features which are based on two different types of lexicon models. We give an extensive experimental evaluation of all these variants on the NIST Chinese—*English translation task. 1 Insertion and Deletion Models In hierarchical phrase-based translation (Chiang, 2005), we deal with rules X —* (α, Q,— ) where (α, Q) is a bilingual phrase pair that may contain symbols from a non-terminal set, i.e. α E (N U VF)+ and Q E (N UVE)+, where VF and VE are the source and ta</context>
</contexts>
<marker>Zens, 2008</marker>
<rawString>Richard Zens. 2008. Phrase-based Statistical Machine Translation: Models, Search, Training. Ph.D. thesis, RWTH Aachen University, Aachen, Germany, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>