<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027625">
<title confidence="0.996494">
A Best-First Probabilistic Shift-Reduce Parser
</title>
<author confidence="0.944418">
Kenji Sagae and Alon Lavie
</author>
<affiliation confidence="0.9692135">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.750671">
Pittsburgh, PA 15213
</address>
<email confidence="0.999691">
{sagae,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934740740741">
Recently proposed deterministic classifier-
based parsers (Nivre and Scholz, 2004;
Sagae and Lavie, 2005; Yamada and Mat-
sumoto, 2003) offer attractive alternatives
to generative statistical parsers. Determin-
istic parsers are fast, efficient, and sim-
ple to implement, but generally less ac-
curate than optimal (or nearly optimal)
statistical parsers. We present a statis-
tical shift-reduce parser that bridges the
gap between deterministic and probabilis-
tic parsers. The parsing model is essen-
tially the same as one previously used
for deterministic parsing, but the parser
performs a best-first search instead of a
greedy search. Using the standard sec-
tions of the WSJ corpus of the Penn Tree-
bank for training and testing, our parser
has 88.1% precision and 87.8% recall (us-
ing automatically assigned part-of-speech
tags). Perhaps more interestingly, the pars-
ing model is significantly different from
the generative models used by other well-
known accurate parsers, allowing for a
simple combination that produces preci-
sion and recall of 90.9% and 90.7%, re-
spectively.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948577777778">
Over the past decade, researchers have devel-
oped several constituent parsers trained on an-
notated data that achieve high levels of accu-
racy. Some of the more popular and more ac-
curate of these approaches to data-driven parsing
(Charniak, 2000; Collins, 1997; Klein and Man-
ning, 2002) have been based on generative mod-
els that are closely related to probabilistic context-
free grammars. Recently, classifier-based depen-
dency parsing (Nivre and Scholz, 2004; Yamada
and Matsumoto, 2003) has showed that determin-
istic parsers are capable of high levels of accu-
racy, despite great simplicity. This work has led to
the development of deterministic parsers for con-
stituent structures as well (Sagae and Lavie, 2005;
Tsuruoka and Tsujii, 2005). However, evaluations
on the widely used WSJ corpus of the Penn Tree-
bank (Marcus et al., 1993) show that the accuracy
of these parsers still lags behind the state-of-the-
art.
A reasonable and commonly held assumption is
that the accuracy of deterministic classifier-based
parsers can be improved if determinism is aban-
doned in favor of a search over a larger space of
possible parses. While this assumption was shown
to be true for the parser of Tsuruoka and Tsu-
jii (2005), only a moderate improvement resulted
from the addition of a non-greedy search strategy,
and overall parser accuracy was still well below
that of state-of-the-art statistical parsers.
We present a statistical parser that is based on
a shift-reduce algorithm, like the parsers of Sagae
and Lavie (2005) and Nivre and Scholz (2004), but
performs a best-first search instead of pursuing a
single analysis path in deterministic fashion. The
parser retains much of the simplicity of determin-
istic classifier-based parsers, but achieves results
that are closer in accuracy to state-of-the-art statis-
tical parsers. Furthermore, a simple combination
of the shift-reduce parsing model with an existing
generative parsing model produces results with ac-
curacy that surpasses any that of any single (non-
reranked) parser tested on the WSJ Penn Tree-
bank, and comes close to the best results obtained
with discriminative reranking (Charniak and John-
</bodyText>
<page confidence="0.977641">
691
</page>
<note confidence="0.818024">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 691–698,
Sydney, July 2006. c�2006 Association for Computational Linguistics
son, 2005).
</note>
<sectionHeader confidence="0.975178" genericHeader="introduction">
2 Parser Description
</sectionHeader>
<bodyText confidence="0.999946714285714">
Our parser uses an extended version of the basic
bottom-up shift-reduce algorithm for constituent
structures used in Sagae and Lavie’s (2005) de-
terministic parser. For clarity, we will first de-
scribe the deterministic version of the algorithm,
and then show how it can be extended into a proba-
bilistic algorithm that performs a best-first search.
</bodyText>
<subsectionHeader confidence="0.9991755">
2.1 A Shift-Reduce Algorithm for
Deterministic Constituent Parsing
</subsectionHeader>
<bodyText confidence="0.99997046875">
In its deterministic form, our parsing algorithm
is the same single-pass shift-reduce algorithm as
the one used in the classifer-based parser of Sagae
and Lavie (2005). That algorithm, in turn, is sim-
ilar to the dependency parsing algorithm of Nivre
and Scholz (2004), but it builds a constituent tree
and a dependency tree simultaneously. The al-
gorithm considers only trees with unary and bi-
nary productions. Training the parser with arbi-
trary branching trees is accomplished by a sim-
ple procedure to transform those trees into trees
with at most binary productions. This is done
by converting each production with n children,
where n &gt; 2, into n − 1 binary productions.
This binarization process is similar to the one de-
scribed in (Charniak et al., 1998). Additional non-
terminal nodes introduced in this conversion must
be clearly marked. Transforming the parser’s out-
put into arbitrary branching trees is accomplished
using the reverse process.
The deterministic parsing algorithm involves
two main data structures: a stack S, and a queue
W. Items in S may be terminal nodes (part-of-
speech-tagged words), or (lexicalized) subtrees of
the final parse tree for the input string. Items in W
are terminals (words tagged with parts-of-speech)
corresponding to the input string. When parsing
begins, S is empty and W is initialized by insert-
ing every word from the input string in order, so
that the first word is in front of the queue.
The algorithm defines two types of parser ac-
tions, shift and reduce, explained below:
</bodyText>
<listItem confidence="0.98753425">
• Shift: A shift action consists only of remov-
ing (shifting) the first item (part-of-speech-
tagged word) from W (at which point the
next word becomes the new first item), and
placing it on top of S.
• Reduce: Reduce actions are subdivided into
unary and binary cases. In a unary reduction,
the item on top of S is popped, and a new
</listItem>
<bodyText confidence="0.984011966666667">
item is pushed onto S. The new item consists
of a tree formed by a non-terminal node with
the popped item as its single child. The lex-
ical head of the new item is the same as the
lexical head of the popped item. In a binary
reduction, two items are popped from S in
sequence, and a new item is pushed onto S.
The new item consists of a tree formed by a
non-terminal node with two children: the first
item popped from S is the right child, and the
second item is the left child. The lexical head
of the new item may be the lexical head of its
left child, or the lexical head of its right child.
If S is empty, only a shift action is allowed. If
W is empty, only a reduce action is allowed. If
both S and W are non-empty, either shift or re-
duce actions are possible, and the parser must de-
cide whether to shift or reduce. If it decides to re-
duce, it must also choose between a unary-reduce
or a binary-reduce, what non-terminal should be at
the root of the newly created subtree to be pushed
onto the stack S, and whether the lexical head of
the newly created subtree will be taken from the
right child or the left child of its root node. Fol-
lowing the work of Sagae and Lavie, we consider
the complete set of decisions associated with a re-
duce action to be part of that reduce action. Pars-
ing terminates when W is empty and S contains
only one item, and the single item in S is the parse
tree for the input string.
</bodyText>
<subsectionHeader confidence="0.999011">
2.2 Shift-Reduce Best-First Parsing
</subsectionHeader>
<bodyText confidence="0.9998643125">
A deterministic shift-reduce parser based on the
algorithm described in section 2.1 does not handle
ambiguity. By choosing a single parser action at
each opportunity, the input string is parsed deter-
ministically, and a single constituent structure is
built during the parsing process from beginning to
end (no other structures are even considered).
A simple extension to this idea is to eliminate
determinism by allowing the parser to choose sev-
eral actions at each opportunity, creating different
paths that lead to different parse trees. This is es-
sentially the difference between deterministic LR
parsing (Knuth, 1965) and Generalized-LR pars-
ing (Tomita, 1987; Tomita, 1990). Furthermore,
if a probability is assigned to every parser action,
the probability of a parse tree can be computed
</bodyText>
<page confidence="0.992522">
692
</page>
<bodyText confidence="0.9999880875">
simply as the product of the probabilities of each
action in the path that resulted in that parse tree
(the derivation of the tree). This produces a prob-
abilistic shift-reduce parser that resembles a gen-
eralized probabilistic LR parser (Briscoe and Car-
roll, 1993), where probabilities are associated with
an LR parsing table. In our case, although there
is no LR table, the action probabilities are associ-
ated with several aspects of the current state of the
parser, which to some extent parallel the informa-
tion contained in an LR table. Instead of having
an explicit LR table and pushing LR states onto
the stack, the state of the parser is implicitly de-
fined by the configurations of the stack and queue.
In a way, there is a parallel between how mod-
ern PCFG-like parsers use markov grammars as
a distribution that is used to determine the proba-
bility of any possible grammar rules, and the way
a statistical model is used in our parser to assign
a probability to any transition of parser states (in-
stead of a symbolic LR table).
Pursuing every possible sequence of parser ac-
tions creates a very large space of actions for
even moderately sized sentences. To find the most
likely parse tree efficiently according to the prob-
abilistic shift-reduce parsing scheme described so
far, we use a best-first strategy. This involves an
extension of the deterministic shift-reduce algo-
rithm into a best-first shift-reduce algorithm. To
describe this extension, we first introduce a new
data structure Ti that represents a parser state,
which includes a stack Si and a queue Wi. In
the deterministic algorithm, we would have a sin-
gle parser state T that contains S and W. The
best-first algorithm, on the other hand, has a heap
H containing multiple parser states Ti ... Tn.
These states are ordered in the heap according to
their probabilities, so that the state with the highest
probability is at the top. State probabilities are de-
termined by multiplying the probabilities of each
of the actions that resulted in that state. Parser ac-
tions are determined from and applied to a parser
state Ti popped from the top of H. The parser
actions are the same as in the deterministic ver-
sion of the algorithm. When the item popped from
the top of the heap H contains a stack Si with a
single item and an empty queue (in other words,
meets the acceptance criteria for the determinis-
tic version of the algorithm), the item on top of
Si is the tree with the highest probability. At that
point, parsing terminates if we are searching for
the most probable parse. To obtain a list of n-best
parses, we simply continue parsing once the first
parse tree is found, until either n trees are found,
or H is empty.
We note that this approach does not use dy-
namic programming, and relies only on the best-
first search strategy to arrive at the most prob-
able parse efficiently. Without any pruning of
the search space, the distribution of probability
mass among different possible actions for a parse
state has a large impact on the behavior of the
search. We do not use any normalization to ac-
count for the size (in number of actions) of dif-
ferent derivations when calculating their probabili-
ties, so it may seem that shorter derivations usually
have higher probabilities than longer ones, causing
the best-first search to approximate a breadth-first
search in practice. However, this is not the case if
for a given parser state only a few actions (or, ide-
ally, only one action) have high probability, and all
other actions have very small probabilities. In this
case, only likely derivations would reach the top of
the heap, resulting in the desired search behavior.
The accuracy of deterministic parsers suggest that
this may in fact be the types of probabilities a clas-
sifier would produce given features that describe
the parser state, and thus the context of the parser
action, specifically enough. The experiments de-
scribed in section 4 support this assumption.
</bodyText>
<subsectionHeader confidence="0.995197">
2.3 Classifier-Based Best-First Parsing
</subsectionHeader>
<bodyText confidence="0.999924272727273">
To build a parser based on the deterministic al-
gorithm described in section 2.1, a classifier is
used to determine parser actions. Sagae and Lavie
(2005) built two deterministic parsers this way,
one using support vector machines, and one using
k-nearest neighbors. In each case, the set of fea-
tures and classes used with each classifier was the
same. Items 1 – 13 in figure 1 shows the features
used by Sagae and Lavie. The classes produced
by the classifier encode every aspect of a parser
action. Classes have one of the following forms:
</bodyText>
<equation confidence="0.891645">
SHIFT : represents a shift action;
REDUCE-UNARY-XX : represents a unary re-
</equation>
<bodyText confidence="0.960967833333333">
duce action, where the root of the new sub-
tree pushed onto S is of type XX (where XX
is a non-terminal symbol, typically NP, V P,
PP, for example);
REDUCE-LEFT-XX : represents a binary re-
duce action, where the root of the new sub-
</bodyText>
<page confidence="0.996803">
693
</page>
<bodyText confidence="0.999438333333333">
tree pushed onto 5 is of non-terminal type
XX. Additionally, the head of the new subtree
is the same as the head of the left child of the
root node;
REDUCE-RIGHT-XX : represents a binary re-
duce action, where the root of the new sub-
tree pushed onto 5 is of non-terminal type
XX. Additionally, the head of the new sub-
tree is the same as the head of the right child
of the root node.
To implement a parser based on the best-first al-
gorithm, instead of just using a classifier to de-
termine one parser action given a stack and a
queue, we need a classification approach that pro-
vides us with probabilities for different parser ac-
tions associated with a given parser state. One
such approach is maximum entropy classification
(Berger et al., 1996), which we use in the form
of a library implemented by Tsuruoka1 and used
in his classifier-based parser (Tsuruoka and Tsujii,
2005). We used the same classes and the same fea-
tures as Sagae and Lavie, and an additional feature
that represents the previous parser action applied
the current parser state (figure 1).
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999823045454546">
As mentioned in section 2, our parsing approach
can be seen as an extension of the approach of
Sagae and Lavie (2005). Sagae and Lavie eval-
uated their deterministic classifier-based parsing
framework using two classifiers: support vector
machines (SVM) and k-nearest neighbors (kNN).
Although the kNN-based parser performed poorly,
the SVM-based parser achieved about 86% preci-
sion and recall (or 87.5% using gold-standard POS
tags) on the WSJ test section of the Penn Tree-
bank, taking only 11 minutes to parse the test set.
Sagae and Lavie’s parsing algorithm is similar to
the one used by Nivre and Scholz (2004) for de-
terministic dependency parsing (using kNN). Ya-
mada and Matsumoto (2003) have also presented
a deterministic classifier-based (SVM-based) de-
pendency parser, but using a different parsing al-
gorithm, and using only unlabeled dependencies.
Tsuruoka and Tsujii (2005) developed a
classifier-based parser that uses the chunk-parsing
algorithm and achieves extremely high parsing
speed, but somewhat low recall. The algorithm
</bodyText>
<footnote confidence="0.998366">
1The SS MaxEnt library is publicly available from
http://www-tsujii.is.s.u-tokyo.ac.jp/ tsuruoka/maxent/.
</footnote>
<bodyText confidence="0.999196038461539">
is based on reframing the parsing task as several
sequential chunking tasks.
Finally, our parser is in many ways similar to
the parser of Ratnaparkhi (1997). Ratnaparkhi’s
parser uses maximum-entropy models to deter-
mine the actions of a parser based to some extent
on the shift-reduce framework, and it is also capa-
ble of pursuing several paths and returning the top-
n highest scoring parses for a sentence. However,
in addition to using different features for parsing,
Ratnaparkhi’s parser uses a different, more com-
plex algorithm. The use of a more involved algo-
rithm allows Ratnaparkhi’s parser to work with ar-
bitrary branching trees without the need of the bi-
narization transform employed here. It breaks the
usual reduce actions into smaller pieces (CHECK
and BUILD), and uses two separate passes (not
including the part-of-speech tagging pass) for de-
termining chunks and higher syntactic structures
separately. Instead of keeping a stack, the parser
makes multiple passes over the input string, like
the dependency parsing algorithm used by Ya-
mada and Matsumoto. Our parser, on the other
hand, uses a simpler stack-based shift-reduce (LR-
like) algorithm for trees with only unary and bi-
nary productions.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999847608695652">
We evaluated our classifier-based best-first parser
on the Wall Street Journal corpus of the Penn Tree-
bank (Marcus et al., 1993) using the standard split:
sections 2-21 were used for training, section 22
was used for development and tuning of parame-
ters and features, and section 23 was used for
testing. Every experiment reported here was per-
formed on a Pentium4 3.2GHz with 2GB of RAM.
Each tree in the training set had empty-node and
function tag information removed, and the trees
were lexicalized using the same head-table rules as
in the Collins (1999) parser (these rules were taken
from Bikel’s (2002) implementation of the Collins
parser). The trees were then converted into trees
containing only unary and binary productions, us-
ing the binarization transform described in section
2. Classifier training instances of features paired
with classes (parser actions) were extracted from
the trees in the training set, and the total number
of training instances was about 1.9 million. It is in-
teresting to note that the procedure of training the
best-first parser is identical to the training of a de-
terministic version of the parser: the deterministic
</bodyText>
<page confidence="0.997541">
694
</page>
<bodyText confidence="0.8253005">
Let:
5(n) denote the nth item from the top of the stack 5, and
W (n) denote the nth item from the front of the queue W.
Features:
</bodyText>
<listItem confidence="0.986586454545455">
1. The head-word (and its POS tag) of: 5(0), 5(1), 5(2), and5(3)
2. The head-word (and its POS tag) of: W(0), W(1), W(2) and W(3)
3. The non-terminal node of the root of: 5(0), and 5(1)
4. The non-terminal node of the left child of the root of: 5(0), and 5(1)
5. The non-terminal node of the right child of the root of: 5(0), and 5(1)
6. The POS tag of the head-word of the left child of the root of: 5(0), and
5(1)
7. The POS tag of the head-word of the right child of the root of: 5(0),
and 5(1)
8. The linear distance (number of words apart) between the head-words of
5(0) and 5(1)
9. The number of lexical items (words) that have been found (so far) to
be dependents of the head-words of: 5(0), and 5(1)
10. The most recently found lexical dependent of the head-word of 5(0)
that is to the left of 5(0)’s head
11. The most recently found lexical dependent of the head-word of 5(0)
that is to the right of 5(0)’s head
12. The most recently found lexical dependent of the head-word of 5(1)
that is to the left of 5(1)’s head
13. The most recently found lexical dependent of the head-word of 5(1)
that is to the right of 5(1)’s head
14. The previous parser action applied to the current parser state
</listItem>
<figureCaption confidence="0.62637275">
Figure 1: Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). The
features described in items 1 – 7 are more directly related to the lexicalized constituent trees that are built
during parsing, while the features described in items 8 – 13 are more directly related to the dependency
structures that are built simultaneously to the constituent structures.
</figureCaption>
<page confidence="0.997885">
695
</page>
<bodyText confidence="0.999962295454546">
algorithm is simply run over all sentences in the
training set, and since the correct trees are known
in advance, we can simply record the features and
correct parser actions that lead to the construction
of the correct tree.
Training the maximum entropy classifier with
such a large number (1.9 million) of training in-
stances and features required more memory than
was available (the maximum training set size we
were able to train with 2GB of RAM was about
200,000 instances), so we employed the training
set splitting idea used by Yamada and Matsumoto
(2003) and Sagae and Lavie (2005). In our case,
we split the training data according to the part-
of-speech (POS) tag of the head-word of the item
on top of the stack, and trained each split of the
training data separately. At run-time, every trained
classifier is loaded, and the choice of classifier
to use is made by looking at the head-word of
the item on top of the stack in the current parser
state. The total training time (a single machine
was used and each classifier was trained in se-
ries) was slightly under nine hours. For compar-
ison, Sagae and Lavie (2005) report that train-
ing support vector machines for one-against-all
multi-class classification on the same set of fea-
tures for their deterministic parser took 62 hours,
and training a k-nearest neighbors classifier took
11 minutes.
When given perfectly tagged text (gold part-of-
speech tags extracted from the Penn Treebank),
our parser has labeled constituent precision and re-
call of 89.40% and 88.79% respectively over all
sentences in the test set, and 90.01% and 89.32%
over sentences with length of at most 40 words.
These results are at the same level of accuracy as
those obtained with other state-of-the-art statisti-
cal parsers, although still well below the best pub-
lished results for this test set (Bod, 2003; Char-
niak and Johnson, 2005). Although the parser is
quite accurate, parsing the test set took 41 minutes.
By implementing a very simple pruning strategy,
the parser can be made much faster. Pruning the
search space is done by only adding a new parser
state to the heap if its probability is greater than
1/b of the probability of the most likely state in
the heap that has had the same number of parser
actions. By setting b to 50, the parser’s accuracy
is only affected minimally, and we obtain 89.3%
precision and 88.7% recall, while parsing the test
set in slightly under 17 minutes and taking less
than 60 megabytes of RAM. Under the same con-
ditions, but using automatically assigned part-of-
speech tags (at 97.1% accuracy) using the SVM-
Tool tagger (Gimenez and Marquez, 2004), we
obtain 88.1% precision and 87.8% recall. It is
likely that the deterioration in accuracy is aggra-
vated by the training set splitting scheme based on
POS tags.
A deterministic version of our parser, obtained
by simply taking the most likely parser action as
the only action at each step (in other words, by set-
ting b to 1), has precision and recall of 85.4% and
84.8%, respectively (86.5% and 86.0% using gold-
standard POS tags). More interestingly, it parses
all 2,416 sentences (more than 50,000 words) in
only 46 seconds, 10 times faster than the deter-
ministic SVM parser of Sagae and Lavie (2005).
The parser of Tsuruoka and Tsujii (Tsuruoka and
Tsujii, 2005) has comparable speed, but we obtain
more accurate results. In addition to being fast,
our deterministic parser is also lean, requiring only
about 25 megabytes of RAM.
A summary of these results is shown in table 1,
along with the results obtained with other parsers
for comparison purposes. The figures shown in
table 1 only include experiments using automat-
ically assigned POS tags. Results obtained with
gold-standard POS tags are not shown, since they
serve little purpose in a comparison with existing
parsers. Although the time figures reflect the per-
formance of each parser at the stated level of ac-
curacy, all of the search-based parsers can trade
accuracy for increased speed. For example, the
Charniak parser can be made twice as fast at the
cost of a 0.5% decrease in precision/recall, or ten
times as fast at the cost of a 4% decrease in preci-
sion/recall (Roark and Charniak, 2002).
</bodyText>
<subsectionHeader confidence="0.9998045">
4.1 Reranking with the Probabililstic
Shift-Reduce Model
</subsectionHeader>
<bodyText confidence="0.999973909090909">
One interesting aspect of having an accurate pars-
ing model that is significantly different from other
well-known generative models is that the com-
bination of two accurate parsers may produce
even more accurate results. A probabilistic shift-
reduce LR-like model, such as the one used in
our parser, is different in many ways from a lex-
icalized PCFG-like model (using markov a gram-
mar), such as those used in the Collins (1999)
and Charniak (2000) parsers. In the probabilis-
tic LR model, probabilities are assigned to tree
</bodyText>
<page confidence="0.997071">
696
</page>
<table confidence="0.999986">
Precision Recall F-score Time (min)
Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17
Deterministic (MazEnt) (this paper) 85.4 84.8 85.1 &lt; 1
Charniak &amp; Johnson (2005) 91.3 90.6 91.0 Unk
Bod (2003) 90.8 90.7 90.7 145*
Charniak (2000) 89.5 89.6 89.5 23
Collins (1999) 88.3 88.1 88.2 39
Ratnaparkhi (1997) 87.5 86.3 86.9 Unk
Tsuruoka &amp; Tsujii (2005): deterministic 86.5 81.2 83.8 &lt; 1*
Tsuruoka &amp; Tsujii (2005): search 86.8 85.0 85.9 2*
Sagae &amp; Lavie (2005) 86.0 86.1 86.0 11*
</table>
<tableCaption confidence="0.998993">
Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse
</tableCaption>
<bodyText confidence="0.996868555555556">
the test set. We first show results for the parsers described here, then for four of the most accurate or
most widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally for three recent
classifier-based parsers. For the purposes of direct comparisons, only results obtained with automatically
assigned part-of-speech tags are shown (tags are assigned by the parser itself or by a separate part-of-
speech tagger). * Times reported by authors running on different hardware.
derivations (not the constituents themselves) based
on the sequence of parser shift/reduce actions.
PCFG-like models, on the other hand, assign prob-
abilities to the trees directly. With models that dif-
fer in such fundamental ways, it is possible that
the probabilities assigned to different trees are in-
dependent enough that even a very simple combi-
nation of the two models may result in increased
accuracy.
We tested this hypothesis by using the Char-
niak (2000) parser in n-best mode, producing the
top 10 trees with corresponding probabilities. We
then rescored the trees produced by the Charniak
parser using our probabilistic LR model, and sim-
ply multiplied the probabilities assigned by the
Charniak model and our LR model to get a com-
bined score for each tree2. On development data
this resulted in a 1.3% absolute improvement in f-
score over the 1-best trees produced by the Char-
niak parser. On the test set (WSJ Penn Treebank
section 23), this reranking scheme produces preci-
sion of 90.9% and recall of 90.7%, for an f-score
of 90.8%.
2The trees produced by the Charniak parser may include
the part-of-speech tags AUX and AUXG, which are not part
of the original Penn Treebank tagset. See (Charniak, 2000)
for details. These are converted deterministically into the ap-
propriate Penn Treebank verb tags, possibly introducing a
small number of minor POS tagging errors. Gold-standard
tags or the output of a separate part-of-speech tagger are not
used at any point in rescoring the trees.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99997070967742">
We have presented a best-first classifier-based
parser that achieves high levels of precision and
recall, with fast parsing times and low memory re-
quirements. One way to view the parser is as an
extension of recent work on classifier-based deter-
ministic parsing. It retains the modularity between
parsing algorithms and learning mechanisms asso-
ciated with deterministic parsers, making it simple
to understand, implement, and experiment with.
Another way to view the parser is as a variant of
probabilistic GLR parsers without an explicit LR
table.
We have shown that our best-first strategy re-
sults in significant improvements in accuracy over
deterministic parsing. Although the best-first
search makes parsing slower, we have imple-
mented a beam strategy that prunes much of the
search space with very little cost in accuracy. This
strategy involves a parameter that can be used to
control the trade-off between accuracy and speed.
At one extreme, the parser is very fast (more than
1,000 words per second) and still moderately ac-
curate (about 85% f-score, or 86% using gold-
standard POS tags). This makes it possible to
apply parsing to natural language tasks involv-
ing very large amounts of text (such as question-
answering or information extraction with large
corpora). A less aggressive pruning setting results
in an f-score of about 88% (or 89%, using gold-
standard POS tags), taking 17 minutes to parse the
WSJ test set.
</bodyText>
<page confidence="0.994494">
697
</page>
<bodyText confidence="0.999985666666667">
Finally, we have shown that by multiplying the
probabilities assigned by our maximum entropy
shift-reduce model to the probabilities of the 10-
best trees produced for each sentence by the Char-
niak parser, we can rescore the trees to obtain
more accurate results than those produced by ei-
ther model in isolation. This simple combination
of the two models produces an f-score of 90.8%
for the standard WSJ test set.
</bodyText>
<sectionHeader confidence="0.996401" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998754">
We thank John Carroll for insightful discussions at
various stages of this work, and the reviewers for
their detailed comments. This work was supported
in part by the National Science Foundation under
grant IIS-0414630.
</bodyText>
<sectionHeader confidence="0.999414" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906517241379">
A. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings ofHLT2002. San Diego, CA.
R. Bod. 2003. An efficient implementation of a new
dop model. In Proceedings of the European chapter
of the 2003 meeting of the Association for Computa-
tional Linguistics. Budapest, Hungary.
E. Briscoe and J. Carroll. 1993. Generalised proba-
bilistic lr parsing of natural language (corpora) with
unification-based grammars. Computational Lin-
guistics, 19(1):25–59.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd meeting of
the Association for Computational Linguistics. Ann
Arbor, MI.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora. Montreal, Canada.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 132–139.
Seattle, WA.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics, pages 16–23.
M. Collins. 1999. Head-Driven Models for Natural
Language Parsing. Phd thesis, University of Penn-
sylvania.
J. Gimenez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector
machines. In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Lisbon, Portugal.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002). Vancouver,
BC.
D. E. Knuth. 1965. On the translation of lan-
guages from left to right. Information and Control,
8(6):607–639.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewics.
1993. Building a large annotated corpus of english:
The penn treebank. Computational Linguistics, 19.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics, pages 64–70. Geneva, Switzer-
land.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of the Second Conference on Empir-
ical Methods in Natural Language Processing. Prov-
idence, RI.
B. Roark and E. Charniak. 2002. Measuring effi-
ciency in high-accuracy, broad coverage statistical
parsing. In Proceedings of the Efficiency in Large-
scale Parsing Systems Workshop at COLING-2000.
Luxembourg.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies. Vancouver, BC.
Masaru Tomita. 1987. An efficient augmented
context-free parsing algorithm. Computational Lin-
guistics, 13:31–46.
Masaru Tomita. 1990. The generalized lr
parser/compiler - version 8.4. In Proceedings of
the International Conference on Computational Lin-
guistics (COLING’90), pages 59–63. Helsinki, Fin-
land.
Y. Tsuruoka and K. Tsujii. 2005. Chunk parsing
revisited. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technologies. Vancou-
ver, Canada.
H. Yamada and Yuji Matsumoto. 2003. Statistical de-
pendency analysis using support vector machines.
In Proceedings of the Eighth International Work-
shop on Parsing Technologies. Nancy, France.
</reference>
<page confidence="0.997284">
698
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.359547">
<title confidence="0.928391">A Best-First Probabilistic Shift-Reduce Parser</title>
<affiliation confidence="0.827712666666667">Sagae Lavie Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.999911">Pittsburgh, PA 15213</address>
<abstract confidence="0.992163">Recently proposed deterministic classifierbased parsers (Nivre and Scholz, 2004; Sagae and Lavie, 2005; Yamada and Matsumoto, 2003) offer attractive alternatives to generative statistical parsers. Deterministic parsers are fast, efficient, and simple to implement, but generally less accurate than optimal (or nearly optimal) statistical parsers. We present a statistical shift-reduce parser that bridges the gap between deterministic and probabilistic parsers. The parsing model is essentially the same as one previously used for deterministic parsing, but the parser performs a best-first search instead of a greedy search. Using the standard sections of the WSJ corpus of the Penn Treebank for training and testing, our parser has 88.1% precision and 87.8% recall (using automatically assigned part-of-speech tags). Perhaps more interestingly, the parsing model is significantly different from the generative models used by other wellknown accurate parsers, allowing for a simple combination that produces precision and recall of 90.9% and 90.7%, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="13680" citStr="Berger et al., 1996" startWordPosition="2309" endWordPosition="2312">eft child of the root node; REDUCE-RIGHT-XX : represents a binary reduce action, where the root of the new subtree pushed onto 5 is of non-terminal type XX. Additionally, the head of the new subtree is the same as the head of the right child of the root node. To implement a parser based on the best-first algorithm, instead of just using a classifier to determine one parser action given a stack and a queue, we need a classification approach that provides us with probabilities for different parser actions associated with a given parser state. One such approach is maximum entropy classification (Berger et al., 1996), which we use in the form of a library implemented by Tsuruoka1 and used in his classifier-based parser (Tsuruoka and Tsujii, 2005). We used the same classes and the same features as Sagae and Lavie, and an additional feature that represents the previous parser action applied the current parser state (figure 1). 3 Related Work As mentioned in section 2, our parsing approach can be seen as an extension of the approach of Sagae and Lavie (2005). Sagae and Lavie evaluated their deterministic classifier-based parsing framework using two classifiers: support vector machines (SVM) and k-nearest nei</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<title>Design of a multi-lingual, parallelprocessing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT2002.</booktitle>
<location>San Diego, CA.</location>
<marker>Bikel, 2002</marker>
<rawString>D. Bikel. 2002. Design of a multi-lingual, parallelprocessing statistical parsing engine. In Proceedings ofHLT2002. San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>An efficient implementation of a new dop model.</title>
<date>2003</date>
<booktitle>In Proceedings of the European chapter of the 2003 meeting of the Association for Computational Linguistics.</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="21105" citStr="Bod, 2003" startWordPosition="3558" endWordPosition="3559">et of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. When given perfectly tagged text (gold part-ofspeech tags extracted from the Penn Treebank), our parser has labeled constituent precision and recall of 89.40% and 88.79% respectively over all sentences in the test set, and 90.01% and 89.32% over sentences with length of at most 40 words. These results are at the same level of accuracy as those obtained with other state-of-the-art statistical parsers, although still well below the best published results for this test set (Bod, 2003; Charniak and Johnson, 2005). Although the parser is quite accurate, parsing the test set took 41 minutes. By implementing a very simple pruning strategy, the parser can be made much faster. Pruning the search space is done by only adding a new parser state to the heap if its probability is greater than 1/b of the probability of the most likely state in the heap that has had the same number of parser actions. By setting b to 50, the parser’s accuracy is only affected minimally, and we obtain 89.3% precision and 88.7% recall, while parsing the test set in slightly under 17 minutes and taking l</context>
<context position="24234" citStr="Bod (2003)" startWordPosition="4088" endWordPosition="4089">ive models is that the combination of two accurate parsers may produce even more accurate results. A probabilistic shiftreduce LR-like model, such as the one used in our parser, is different in many ways from a lexicalized PCFG-like model (using markov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MazEnt) (this paper) 85.4 84.8 85.1 &lt; 1 Charniak &amp; Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka &amp; Tsujii (2005): deterministic 86.5 81.2 83.8 &lt; 1* Tsuruoka &amp; Tsujii (2005): search 86.8 85.0 85.9 2* Sagae &amp; Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse the test set. We first show results for the parsers described here, then for four of the most accurate or most widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally for three recent classifier-ba</context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>R. Bod. 2003. An efficient implementation of a new dop model. In Proceedings of the European chapter of the 2003 meeting of the Association for Computational Linguistics. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalised probabilistic lr parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="8373" citStr="Briscoe and Carroll, 1993" startWordPosition="1378" endWordPosition="1382"> choose several actions at each opportunity, creating different paths that lead to different parse trees. This is essentially the difference between deterministic LR parsing (Knuth, 1965) and Generalized-LR parsing (Tomita, 1987; Tomita, 1990). Furthermore, if a probability is assigned to every parser action, the probability of a parse tree can be computed 692 simply as the product of the probabilities of each action in the path that resulted in that parse tree (the derivation of the tree). This produces a probabilistic shift-reduce parser that resembles a generalized probabilistic LR parser (Briscoe and Carroll, 1993), where probabilities are associated with an LR parsing table. In our case, although there is no LR table, the action probabilities are associated with several aspects of the current state of the parser, which to some extent parallel the information contained in an LR table. Instead of having an explicit LR table and pushing LR states onto the stack, the state of the parser is implicitly defined by the configurations of the stack and queue. In a way, there is a parallel between how modern PCFG-like parsers use markov grammars as a distribution that is used to determine the probability of any p</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>E. Briscoe and J. Carroll. 1993. Generalised probabilistic lr parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd meeting of the Association for Computational Linguistics.</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="21134" citStr="Charniak and Johnson, 2005" startWordPosition="3560" endWordPosition="3564">res for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. When given perfectly tagged text (gold part-ofspeech tags extracted from the Penn Treebank), our parser has labeled constituent precision and recall of 89.40% and 88.79% respectively over all sentences in the test set, and 90.01% and 89.32% over sentences with length of at most 40 words. These results are at the same level of accuracy as those obtained with other state-of-the-art statistical parsers, although still well below the best published results for this test set (Bod, 2003; Charniak and Johnson, 2005). Although the parser is quite accurate, parsing the test set took 41 minutes. By implementing a very simple pruning strategy, the parser can be made much faster. Pruning the search space is done by only adding a new parser state to the heap if its probability is greater than 1/b of the probability of the most likely state in the heap that has had the same number of parser actions. By setting b to 50, the parser’s accuracy is only affected minimally, and we obtain 89.3% precision and 88.7% recall, while parsing the test set in slightly under 17 minutes and taking less than 60 megabytes of RAM.</context>
<context position="24204" citStr="Charniak &amp; Johnson (2005)" startWordPosition="4080" endWordPosition="4083">antly different from other well-known generative models is that the combination of two accurate parsers may produce even more accurate results. A probabilistic shiftreduce LR-like model, such as the one used in our parser, is different in many ways from a lexicalized PCFG-like model (using markov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MazEnt) (this paper) 85.4 84.8 85.1 &lt; 1 Charniak &amp; Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka &amp; Tsujii (2005): deterministic 86.5 81.2 83.8 &lt; 1* Tsuruoka &amp; Tsujii (2005): search 86.8 85.0 85.9 2* Sagae &amp; Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse the test set. We first show results for the parsers described here, then for four of the most accurate or most widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd meeting of the Association for Computational Linguistics. Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Edge-based best-first chart parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="4768" citStr="Charniak et al., 1998" startWordPosition="737" endWordPosition="740"> parser of Sagae and Lavie (2005). That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. The algorithm considers only trees with unary and binary productions. Training the parser with arbitrary branching trees is accomplished by a simple procedure to transform those trees into trees with at most binary productions. This is done by converting each production with n children, where n &gt; 2, into n − 1 binary productions. This binarization process is similar to the one described in (Charniak et al., 1998). Additional nonterminal nodes introduced in this conversion must be clearly marked. Transforming the parser’s output into arbitrary branching trees is accomplished using the reverse process. The deterministic parsing algorithm involves two main data structures: a stack S, and a queue W. Items in S may be terminal nodes (part-ofspeech-tagged words), or (lexicalized) subtrees of the final parse tree for the input string. Items in W are terminals (words tagged with parts-of-speech) corresponding to the input string. When parsing begins, S is empty and W is initialized by inserting every word fro</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998. Edge-based best-first chart parsing. In Proceedings of the Sixth Workshop on Very Large Corpora. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1510" citStr="Charniak, 2000" startWordPosition="224" endWordPosition="225">nd testing, our parser has 88.1% precision and 87.8% recall (using automatically assigned part-of-speech tags). Perhaps more interestingly, the parsing model is significantly different from the generative models used by other wellknown accurate parsers, allowing for a simple combination that produces precision and recall of 90.9% and 90.7%, respectively. 1 Introduction Over the past decade, researchers have developed several constituent parsers trained on annotated data that achieve high levels of accuracy. Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that t</context>
<context position="23949" citStr="Charniak (2000)" startWordPosition="4043" endWordPosition="4044">cision/recall, or ten times as fast at the cost of a 4% decrease in precision/recall (Roark and Charniak, 2002). 4.1 Reranking with the Probabililstic Shift-Reduce Model One interesting aspect of having an accurate parsing model that is significantly different from other well-known generative models is that the combination of two accurate parsers may produce even more accurate results. A probabilistic shiftreduce LR-like model, such as the one used in our parser, is different in many ways from a lexicalized PCFG-like model (using markov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MazEnt) (this paper) 85.4 84.8 85.1 &lt; 1 Charniak &amp; Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka &amp; Tsujii (2005): deterministic 86.5 81.2 83.8 &lt; 1* Tsuruoka &amp; Tsujii (2005): search 86.8 85.0 85.9 2* Sagae &amp; Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary of results on labeled</context>
<context position="25568" citStr="Charniak (2000)" startWordPosition="4304" endWordPosition="4306">ags are shown (tags are assigned by the parser itself or by a separate part-ofspeech tagger). * Times reported by authors running on different hardware. derivations (not the constituents themselves) based on the sequence of parser shift/reduce actions. PCFG-like models, on the other hand, assign probabilities to the trees directly. With models that differ in such fundamental ways, it is possible that the probabilities assigned to different trees are independent enough that even a very simple combination of the two models may result in increased accuracy. We tested this hypothesis by using the Charniak (2000) parser in n-best mode, producing the top 10 trees with corresponding probabilities. We then rescored the trees produced by the Charniak parser using our probabilistic LR model, and simply multiplied the probabilities assigned by the Charniak model and our LR model to get a combined score for each tree2. On development data this resulted in a 1.3% absolute improvement in fscore over the 1-best trees produced by the Charniak parser. On the test set (WSJ Penn Treebank section 23), this reranking scheme produces precision of 90.9% and recall of 90.7%, for an f-score of 90.8%. 2The trees produced </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1525" citStr="Collins, 1997" startWordPosition="226" endWordPosition="227">parser has 88.1% precision and 87.8% recall (using automatically assigned part-of-speech tags). Perhaps more interestingly, the parsing model is significantly different from the generative models used by other wellknown accurate parsers, allowing for a simple combination that produces precision and recall of 90.9% and 90.7%, respectively. 1 Introduction Over the past decade, researchers have developed several constituent parsers trained on annotated data that achieve high levels of accuracy. Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that the accuracy of </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Models for Natural Language Parsing. Phd thesis,</title>
<date>1999</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="16936" citStr="Collins (1999)" startWordPosition="2826" endWordPosition="2827">s with only unary and binary productions. 4 Experiments We evaluated our classifier-based best-first parser on the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993) using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing. Every experiment reported here was performed on a Pentium4 3.2GHz with 2GB of RAM. Each tree in the training set had empty-node and function tag information removed, and the trees were lexicalized using the same head-table rules as in the Collins (1999) parser (these rules were taken from Bikel’s (2002) implementation of the Collins parser). The trees were then converted into trees containing only unary and binary productions, using the binarization transform described in section 2. Classifier training instances of features paired with classes (parser actions) were extracted from the trees in the training set, and the total number of training instances was about 1.9 million. It is interesting to note that the procedure of training the best-first parser is identical to the training of a deterministic version of the parser: the deterministic 6</context>
<context position="23929" citStr="Collins (1999)" startWordPosition="4040" endWordPosition="4041">.5% decrease in precision/recall, or ten times as fast at the cost of a 4% decrease in precision/recall (Roark and Charniak, 2002). 4.1 Reranking with the Probabililstic Shift-Reduce Model One interesting aspect of having an accurate parsing model that is significantly different from other well-known generative models is that the combination of two accurate parsers may produce even more accurate results. A probabilistic shiftreduce LR-like model, such as the one used in our parser, is different in many ways from a lexicalized PCFG-like model (using markov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MazEnt) (this paper) 85.4 84.8 85.1 &lt; 1 Charniak &amp; Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka &amp; Tsujii (2005): deterministic 86.5 81.2 83.8 &lt; 1* Tsuruoka &amp; Tsujii (2005): search 86.8 85.0 85.9 2* Sagae &amp; Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary o</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Models for Natural Language Parsing. Phd thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gimenez</author>
<author>L Marquez</author>
</authors>
<title>Svmtool: A general pos tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation.</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="21886" citStr="Gimenez and Marquez, 2004" startWordPosition="3693" endWordPosition="3696"> the parser can be made much faster. Pruning the search space is done by only adding a new parser state to the heap if its probability is greater than 1/b of the probability of the most likely state in the heap that has had the same number of parser actions. By setting b to 50, the parser’s accuracy is only affected minimally, and we obtain 89.3% precision and 88.7% recall, while parsing the test set in slightly under 17 minutes and taking less than 60 megabytes of RAM. Under the same conditions, but using automatically assigned part-ofspeech tags (at 97.1% accuracy) using the SVMTool tagger (Gimenez and Marquez, 2004), we obtain 88.1% precision and 87.8% recall. It is likely that the deterioration in accuracy is aggravated by the training set splitting scheme based on POS tags. A deterministic version of our parser, obtained by simply taking the most likely parser action as the only action at each step (in other words, by setting b to 1), has precision and recall of 85.4% and 84.8%, respectively (86.5% and 86.0% using goldstandard POS tags). More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (20</context>
</contexts>
<marker>Gimenez, Marquez, 2004</marker>
<rawString>J. Gimenez and L. Marquez. 2004. Svmtool: A general pos tagger generator based on support vector machines. In Proceedings of the 4th International Conference on Language Resources and Evaluation. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS 2002).</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="1551" citStr="Klein and Manning, 2002" startWordPosition="228" endWordPosition="232">% precision and 87.8% recall (using automatically assigned part-of-speech tags). Perhaps more interestingly, the parsing model is significantly different from the generative models used by other wellknown accurate parsers, allowing for a simple combination that produces precision and recall of 90.9% and 90.7%, respectively. 1 Introduction Over the past decade, researchers have developed several constituent parsers trained on annotated data that achieve high levels of accuracy. Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that the accuracy of these parsers still lags b</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15 (NIPS 2002). Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>On the translation of languages from left to right.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<issue>6</issue>
<contexts>
<context position="7934" citStr="Knuth, 1965" startWordPosition="1309" endWordPosition="1310">nistic shift-reduce parser based on the algorithm described in section 2.1 does not handle ambiguity. By choosing a single parser action at each opportunity, the input string is parsed deterministically, and a single constituent structure is built during the parsing process from beginning to end (no other structures are even considered). A simple extension to this idea is to eliminate determinism by allowing the parser to choose several actions at each opportunity, creating different paths that lead to different parse trees. This is essentially the difference between deterministic LR parsing (Knuth, 1965) and Generalized-LR parsing (Tomita, 1987; Tomita, 1990). Furthermore, if a probability is assigned to every parser action, the probability of a parse tree can be computed 692 simply as the product of the probabilities of each action in the path that resulted in that parse tree (the derivation of the tree). This produces a probabilistic shift-reduce parser that resembles a generalized probabilistic LR parser (Briscoe and Carroll, 1993), where probabilities are associated with an LR parsing table. In our case, although there is no LR table, the action probabilities are associated with several a</context>
</contexts>
<marker>Knuth, 1965</marker>
<rawString>D. E. Knuth. 1965. On the translation of languages from left to right. Information and Control, 8(6):607–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewics</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="2098" citStr="Marcus et al., 1993" startWordPosition="316" endWordPosition="319">a-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that the accuracy of these parsers still lags behind the state-of-theart. A reasonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is abandoned in favor of a search over a larger space of possible parses. While this assumption was shown to be true for the parser of Tsuruoka and Tsujii (2005), only a moderate improvement resulted from the addition of a non-greedy search strategy, and overall parser accuracy was still well below that of state-of-the-art statistical parsers. We present a statistical parser that i</context>
<context position="16506" citStr="Marcus et al., 1993" startWordPosition="2751" endWordPosition="2754">ns into smaller pieces (CHECK and BUILD), and uses two separate passes (not including the part-of-speech tagging pass) for determining chunks and higher syntactic structures separately. Instead of keeping a stack, the parser makes multiple passes over the input string, like the dependency parsing algorithm used by Yamada and Matsumoto. Our parser, on the other hand, uses a simpler stack-based shift-reduce (LRlike) algorithm for trees with only unary and binary productions. 4 Experiments We evaluated our classifier-based best-first parser on the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993) using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing. Every experiment reported here was performed on a Pentium4 3.2GHz with 2GB of RAM. Each tree in the training set had empty-node and function tag information removed, and the trees were lexicalized using the same head-table rules as in the Collins (1999) parser (these rules were taken from Bikel’s (2002) implementation of the Collins parser). The trees were then converted into trees containing only unary and binary produ</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewics, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewics. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>64--70</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1722" citStr="Nivre and Scholz, 2004" startWordPosition="254" endWordPosition="257"> models used by other wellknown accurate parsers, allowing for a simple combination that produces precision and recall of 90.9% and 90.7%, respectively. 1 Introduction Over the past decade, researchers have developed several constituent parsers trained on annotated data that achieve high levels of accuracy. Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that the accuracy of these parsers still lags behind the state-of-theart. A reasonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is aband</context>
<context position="4279" citStr="Nivre and Scholz (2004)" startWordPosition="653" endWordPosition="656"> shift-reduce algorithm for constituent structures used in Sagae and Lavie’s (2005) deterministic parser. For clarity, we will first describe the deterministic version of the algorithm, and then show how it can be extended into a probabilistic algorithm that performs a best-first search. 2.1 A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. The algorithm considers only trees with unary and binary productions. Training the parser with arbitrary branching trees is accomplished by a simple procedure to transform those trees into trees with at most binary productions. This is done by converting each production with n children, where n &gt; 2, into n − 1 binary productions. This binarization process is similar to the one described in (Charniak et al., 1998). Additional nonterminal nodes introduced in this conversion must be clearly marked. Transforming the parser’s </context>
<context position="14624" citStr="Nivre and Scholz (2004)" startWordPosition="2465" endWordPosition="2468">ted Work As mentioned in section 2, our parsing approach can be seen as an extension of the approach of Sagae and Lavie (2005). Sagae and Lavie evaluated their deterministic classifier-based parsing framework using two classifiers: support vector machines (SVM) and k-nearest neighbors (kNN). Although the kNN-based parser performed poorly, the SVM-based parser achieved about 86% precision and recall (or 87.5% using gold-standard POS tags) on the WSJ test section of the Penn Treebank, taking only 11 minutes to parse the test set. Sagae and Lavie’s parsing algorithm is similar to the one used by Nivre and Scholz (2004) for deterministic dependency parsing (using kNN). Yamada and Matsumoto (2003) have also presented a deterministic classifier-based (SVM-based) dependency parser, but using a different parsing algorithm, and using only unlabeled dependencies. Tsuruoka and Tsujii (2005) developed a classifier-based parser that uses the chunk-parsing algorithm and achieves extremely high parsing speed, but somewhat low recall. The algorithm 1The SS MaxEnt library is publicly available from http://www-tsujii.is.s.u-tokyo.ac.jp/ tsuruoka/maxent/. is based on reframing the parsing task as several sequential chunkin</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of english text. In Proceedings of the 20th International Conference on Computational Linguistics, pages 64–70. Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing.</booktitle>
<location>Providence, RI.</location>
<contexts>
<context position="15312" citStr="Ratnaparkhi (1997)" startWordPosition="2562" endWordPosition="2563">o (2003) have also presented a deterministic classifier-based (SVM-based) dependency parser, but using a different parsing algorithm, and using only unlabeled dependencies. Tsuruoka and Tsujii (2005) developed a classifier-based parser that uses the chunk-parsing algorithm and achieves extremely high parsing speed, but somewhat low recall. The algorithm 1The SS MaxEnt library is publicly available from http://www-tsujii.is.s.u-tokyo.ac.jp/ tsuruoka/maxent/. is based on reframing the parsing task as several sequential chunking tasks. Finally, our parser is in many ways similar to the parser of Ratnaparkhi (1997). Ratnaparkhi’s parser uses maximum-entropy models to determine the actions of a parser based to some extent on the shift-reduce framework, and it is also capable of pursuing several paths and returning the topn highest scoring parses for a sentence. However, in addition to using different features for parsing, Ratnaparkhi’s parser uses a different, more complex algorithm. The use of a more involved algorithm allows Ratnaparkhi’s parser to work with arbitrary branching trees without the need of the binarization transform employed here. It breaks the usual reduce actions into smaller pieces (CH</context>
<context position="24340" citStr="Ratnaparkhi (1997)" startWordPosition="4106" endWordPosition="4107"> A probabilistic shiftreduce LR-like model, such as the one used in our parser, is different in many ways from a lexicalized PCFG-like model (using markov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MazEnt) (this paper) 85.4 84.8 85.1 &lt; 1 Charniak &amp; Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka &amp; Tsujii (2005): deterministic 86.5 81.2 83.8 &lt; 1* Tsuruoka &amp; Tsujii (2005): search 86.8 85.0 85.9 2* Sagae &amp; Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse the test set. We first show results for the parsers described here, then for four of the most accurate or most widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally for three recent classifier-based parsers. For the purposes of direct comparisons, only results obtained with automatically assigned par</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing. Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Measuring efficiency in high-accuracy, broad coverage statistical parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of the Efficiency in Largescale Parsing Systems Workshop at COLING-2000. Luxembourg.</booktitle>
<contexts>
<context position="23445" citStr="Roark and Charniak, 2002" startWordPosition="3959" endWordPosition="3962">or comparison purposes. The figures shown in table 1 only include experiments using automatically assigned POS tags. Results obtained with gold-standard POS tags are not shown, since they serve little purpose in a comparison with existing parsers. Although the time figures reflect the performance of each parser at the stated level of accuracy, all of the search-based parsers can trade accuracy for increased speed. For example, the Charniak parser can be made twice as fast at the cost of a 0.5% decrease in precision/recall, or ten times as fast at the cost of a 4% decrease in precision/recall (Roark and Charniak, 2002). 4.1 Reranking with the Probabililstic Shift-Reduce Model One interesting aspect of having an accurate parsing model that is significantly different from other well-known generative models is that the combination of two accurate parsers may produce even more accurate results. A probabilistic shiftreduce LR-like model, such as the one used in our parser, is different in many ways from a lexicalized PCFG-like model (using markov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall</context>
</contexts>
<marker>Roark, Charniak, 2002</marker>
<rawString>B. Roark and E. Charniak. 2002. Measuring efficiency in high-accuracy, broad coverage statistical parsing. In Proceedings of the Efficiency in Largescale Parsing Systems Workshop at COLING-2000. Luxembourg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies.</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="1975" citStr="Sagae and Lavie, 2005" startWordPosition="295" endWordPosition="298">n annotated data that achieve high levels of accuracy. Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that the accuracy of these parsers still lags behind the state-of-theart. A reasonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is abandoned in favor of a search over a larger space of possible parses. While this assumption was shown to be true for the parser of Tsuruoka and Tsujii (2005), only a moderate improvement resulted from the addition of a non-greedy search strategy, and overal</context>
<context position="4179" citStr="Sagae and Lavie (2005)" startWordPosition="636" endWordPosition="639">uistics son, 2005). 2 Parser Description Our parser uses an extended version of the basic bottom-up shift-reduce algorithm for constituent structures used in Sagae and Lavie’s (2005) deterministic parser. For clarity, we will first describe the deterministic version of the algorithm, and then show how it can be extended into a probabilistic algorithm that performs a best-first search. 2.1 A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. The algorithm considers only trees with unary and binary productions. Training the parser with arbitrary branching trees is accomplished by a simple procedure to transform those trees into trees with at most binary productions. This is done by converting each production with n children, where n &gt; 2, into n − 1 binary productions. This binarization process is similar to the one described in (Charniak et al., 1998). Additiona</context>
<context position="12244" citStr="Sagae and Lavie (2005)" startWordPosition="2046" endWordPosition="2049"> probabilities. In this case, only likely derivations would reach the top of the heap, resulting in the desired search behavior. The accuracy of deterministic parsers suggest that this may in fact be the types of probabilities a classifier would produce given features that describe the parser state, and thus the context of the parser action, specifically enough. The experiments described in section 4 support this assumption. 2.3 Classifier-Based Best-First Parsing To build a parser based on the deterministic algorithm described in section 2.1, a classifier is used to determine parser actions. Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors. In each case, the set of features and classes used with each classifier was the same. Items 1 – 13 in figure 1 shows the features used by Sagae and Lavie. The classes produced by the classifier encode every aspect of a parser action. Classes have one of the following forms: SHIFT : represents a shift action; REDUCE-UNARY-XX : represents a unary reduce action, where the root of the new subtree pushed onto S is of type XX (where XX is a non-terminal symbol, typically NP, V P, PP, for </context>
<context position="14127" citStr="Sagae and Lavie (2005)" startWordPosition="2387" endWordPosition="2390">hat provides us with probabilities for different parser actions associated with a given parser state. One such approach is maximum entropy classification (Berger et al., 1996), which we use in the form of a library implemented by Tsuruoka1 and used in his classifier-based parser (Tsuruoka and Tsujii, 2005). We used the same classes and the same features as Sagae and Lavie, and an additional feature that represents the previous parser action applied the current parser state (figure 1). 3 Related Work As mentioned in section 2, our parsing approach can be seen as an extension of the approach of Sagae and Lavie (2005). Sagae and Lavie evaluated their deterministic classifier-based parsing framework using two classifiers: support vector machines (SVM) and k-nearest neighbors (kNN). Although the kNN-based parser performed poorly, the SVM-based parser achieved about 86% precision and recall (or 87.5% using gold-standard POS tags) on the WSJ test section of the Penn Treebank, taking only 11 minutes to parse the test set. Sagae and Lavie’s parsing algorithm is similar to the one used by Nivre and Scholz (2004) for deterministic dependency parsing (using kNN). Yamada and Matsumoto (2003) have also presented a de</context>
<context position="18969" citStr="Sagae and Lavie (2005)" startWordPosition="3195" endWordPosition="3198">head-words of: 5(0), and 5(1) 10. The most recently found lexical dependent of the head-word of 5(0) that is to the left of 5(0)’s head 11. The most recently found lexical dependent of the head-word of 5(0) that is to the right of 5(0)’s head 12. The most recently found lexical dependent of the head-word of 5(1) that is to the left of 5(1)’s head 13. The most recently found lexical dependent of the head-word of 5(1) that is to the right of 5(1)’s head 14. The previous parser action applied to the current parser state Figure 1: Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). The features described in items 1 – 7 are more directly related to the lexicalized constituent trees that are built during parsing, while the features described in items 8 – 13 are more directly related to the dependency structures that are built simultaneously to the constituent structures. 695 algorithm is simply run over all sentences in the training set, and since the correct trees are known in advance, we can simply record the features and correct parser actions that lead to the construction of the correct tree. Training the maximum entropy classifier with such a large number (1.9 milli</context>
<context position="20390" citStr="Sagae and Lavie (2005)" startWordPosition="3441" endWordPosition="3444">aining set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). In our case, we split the training data according to the partof-speech (POS) tag of the head-word of the item on top of the stack, and trained each split of the training data separately. At run-time, every trained classifier is loaded, and the choice of classifier to use is made by looking at the head-word of the item on top of the stack in the current parser state. The total training time (a single machine was used and each classifier was trained in series) was slightly under nine hours. For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. When given perfectly tagged text (gold part-ofspeech tags extracted from the Penn Treebank), our parser has labeled constituent precision and recall of 89.40% and 88.79% respectively over all sentences in the test set, and 90.01% and 89.32% over sentences with length of at most 40 words. These results are at the same level of accuracy as those obtained with other stat</context>
<context position="22489" citStr="Sagae and Lavie (2005)" startWordPosition="3798" endWordPosition="3801"> and Marquez, 2004), we obtain 88.1% precision and 87.8% recall. It is likely that the deterioration in accuracy is aggravated by the training set splitting scheme based on POS tags. A deterministic version of our parser, obtained by simply taking the most likely parser action as the only action at each step (in other words, by setting b to 1), has precision and recall of 85.4% and 84.8%, respectively (86.5% and 86.0% using goldstandard POS tags). More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). The parser of Tsuruoka and Tsujii (Tsuruoka and Tsujii, 2005) has comparable speed, but we obtain more accurate results. In addition to being fast, our deterministic parser is also lean, requiring only about 25 megabytes of RAM. A summary of these results is shown in table 1, along with the results obtained with other parsers for comparison purposes. The figures shown in table 1 only include experiments using automatically assigned POS tags. Results obtained with gold-standard POS tags are not shown, since they serve little purpose in a comparison with existing parsers. Although the time fig</context>
<context position="24491" citStr="Sagae &amp; Lavie (2005)" startWordPosition="4131" endWordPosition="4134">arkov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MazEnt) (this paper) 85.4 84.8 85.1 &lt; 1 Charniak &amp; Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka &amp; Tsujii (2005): deterministic 86.5 81.2 83.8 &lt; 1* Tsuruoka &amp; Tsujii (2005): search 86.8 85.0 85.9 2* Sagae &amp; Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse the test set. We first show results for the parsers described here, then for four of the most accurate or most widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally for three recent classifier-based parsers. For the purposes of direct comparisons, only results obtained with automatically assigned part-of-speech tags are shown (tags are assigned by the parser itself or by a separate part-ofspeech tagger). * Times reported by authors running on diffe</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of the Ninth International Workshop on Parsing Technologies. Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An efficient augmented context-free parsing algorithm.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<pages>13--31</pages>
<contexts>
<context position="7975" citStr="Tomita, 1987" startWordPosition="1315" endWordPosition="1316">lgorithm described in section 2.1 does not handle ambiguity. By choosing a single parser action at each opportunity, the input string is parsed deterministically, and a single constituent structure is built during the parsing process from beginning to end (no other structures are even considered). A simple extension to this idea is to eliminate determinism by allowing the parser to choose several actions at each opportunity, creating different paths that lead to different parse trees. This is essentially the difference between deterministic LR parsing (Knuth, 1965) and Generalized-LR parsing (Tomita, 1987; Tomita, 1990). Furthermore, if a probability is assigned to every parser action, the probability of a parse tree can be computed 692 simply as the product of the probabilities of each action in the path that resulted in that parse tree (the derivation of the tree). This produces a probabilistic shift-reduce parser that resembles a generalized probabilistic LR parser (Briscoe and Carroll, 1993), where probabilities are associated with an LR parsing table. In our case, although there is no LR table, the action probabilities are associated with several aspects of the current state of the parser</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>Masaru Tomita. 1987. An efficient augmented context-free parsing algorithm. Computational Linguistics, 13:31–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>The generalized lr parser/compiler - version 8.4.</title>
<date>1990</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING’90),</booktitle>
<pages>59--63</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="7990" citStr="Tomita, 1990" startWordPosition="1317" endWordPosition="1318">ibed in section 2.1 does not handle ambiguity. By choosing a single parser action at each opportunity, the input string is parsed deterministically, and a single constituent structure is built during the parsing process from beginning to end (no other structures are even considered). A simple extension to this idea is to eliminate determinism by allowing the parser to choose several actions at each opportunity, creating different paths that lead to different parse trees. This is essentially the difference between deterministic LR parsing (Knuth, 1965) and Generalized-LR parsing (Tomita, 1987; Tomita, 1990). Furthermore, if a probability is assigned to every parser action, the probability of a parse tree can be computed 692 simply as the product of the probabilities of each action in the path that resulted in that parse tree (the derivation of the tree). This produces a probabilistic shift-reduce parser that resembles a generalized probabilistic LR parser (Briscoe and Carroll, 1993), where probabilities are associated with an LR parsing table. In our case, although there is no LR table, the action probabilities are associated with several aspects of the current state of the parser, which to some</context>
</contexts>
<marker>Tomita, 1990</marker>
<rawString>Masaru Tomita. 1990. The generalized lr parser/compiler - version 8.4. In Proceedings of the International Conference on Computational Linguistics (COLING’90), pages 59–63. Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>K Tsujii</author>
</authors>
<title>Chunk parsing revisited.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies.</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2003" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="299" endWordPosition="302">chieve high levels of accuracy. Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that the accuracy of these parsers still lags behind the state-of-theart. A reasonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is abandoned in favor of a search over a larger space of possible parses. While this assumption was shown to be true for the parser of Tsuruoka and Tsujii (2005), only a moderate improvement resulted from the addition of a non-greedy search strategy, and overall parser accuracy was still </context>
<context position="13812" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="2331" endWordPosition="2334"> 5 is of non-terminal type XX. Additionally, the head of the new subtree is the same as the head of the right child of the root node. To implement a parser based on the best-first algorithm, instead of just using a classifier to determine one parser action given a stack and a queue, we need a classification approach that provides us with probabilities for different parser actions associated with a given parser state. One such approach is maximum entropy classification (Berger et al., 1996), which we use in the form of a library implemented by Tsuruoka1 and used in his classifier-based parser (Tsuruoka and Tsujii, 2005). We used the same classes and the same features as Sagae and Lavie, and an additional feature that represents the previous parser action applied the current parser state (figure 1). 3 Related Work As mentioned in section 2, our parsing approach can be seen as an extension of the approach of Sagae and Lavie (2005). Sagae and Lavie evaluated their deterministic classifier-based parsing framework using two classifiers: support vector machines (SVM) and k-nearest neighbors (kNN). Although the kNN-based parser performed poorly, the SVM-based parser achieved about 86% precision and recall (or 87.5%</context>
<context position="22552" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="3808" endWordPosition="3811">call. It is likely that the deterioration in accuracy is aggravated by the training set splitting scheme based on POS tags. A deterministic version of our parser, obtained by simply taking the most likely parser action as the only action at each step (in other words, by setting b to 1), has precision and recall of 85.4% and 84.8%, respectively (86.5% and 86.0% using goldstandard POS tags). More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). The parser of Tsuruoka and Tsujii (Tsuruoka and Tsujii, 2005) has comparable speed, but we obtain more accurate results. In addition to being fast, our deterministic parser is also lean, requiring only about 25 megabytes of RAM. A summary of these results is shown in table 1, along with the results obtained with other parsers for comparison purposes. The figures shown in table 1 only include experiments using automatically assigned POS tags. Results obtained with gold-standard POS tags are not shown, since they serve little purpose in a comparison with existing parsers. Although the time figures reflect the performance of each parser at the stated level</context>
<context position="24384" citStr="Tsuruoka &amp; Tsujii (2005)" startWordPosition="4112" endWordPosition="4115">odel, such as the one used in our parser, is different in many ways from a lexicalized PCFG-like model (using markov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MazEnt) (this paper) 85.4 84.8 85.1 &lt; 1 Charniak &amp; Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka &amp; Tsujii (2005): deterministic 86.5 81.2 83.8 &lt; 1* Tsuruoka &amp; Tsujii (2005): search 86.8 85.0 85.9 2* Sagae &amp; Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse the test set. We first show results for the parsers described here, then for four of the most accurate or most widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally for three recent classifier-based parsers. For the purposes of direct comparisons, only results obtained with automatically assigned part-of-speech tags are shown (tags are assigne</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Y. Tsuruoka and K. Tsujii. 2005. Chunk parsing revisited. In Proceedings of the Ninth International Workshop on Parsing Technologies. Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighth International Workshop on Parsing Technologies.</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="1751" citStr="Yamada and Matsumoto, 2003" startWordPosition="258" endWordPosition="261">llknown accurate parsers, allowing for a simple combination that produces precision and recall of 90.9% and 90.7%, respectively. 1 Introduction Over the past decade, researchers have developed several constituent parsers trained on annotated data that achieve high levels of accuracy. Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that the accuracy of these parsers still lags behind the state-of-theart. A reasonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is abandoned in favor of a search ove</context>
<context position="14702" citStr="Yamada and Matsumoto (2003)" startWordPosition="2476" endWordPosition="2480"> extension of the approach of Sagae and Lavie (2005). Sagae and Lavie evaluated their deterministic classifier-based parsing framework using two classifiers: support vector machines (SVM) and k-nearest neighbors (kNN). Although the kNN-based parser performed poorly, the SVM-based parser achieved about 86% precision and recall (or 87.5% using gold-standard POS tags) on the WSJ test section of the Penn Treebank, taking only 11 minutes to parse the test set. Sagae and Lavie’s parsing algorithm is similar to the one used by Nivre and Scholz (2004) for deterministic dependency parsing (using kNN). Yamada and Matsumoto (2003) have also presented a deterministic classifier-based (SVM-based) dependency parser, but using a different parsing algorithm, and using only unlabeled dependencies. Tsuruoka and Tsujii (2005) developed a classifier-based parser that uses the chunk-parsing algorithm and achieves extremely high parsing speed, but somewhat low recall. The algorithm 1The SS MaxEnt library is publicly available from http://www-tsujii.is.s.u-tokyo.ac.jp/ tsuruoka/maxent/. is based on reframing the parsing task as several sequential chunking tasks. Finally, our parser is in many ways similar to the parser of Ratnapar</context>
<context position="19829" citStr="Yamada and Matsumoto (2003)" startWordPosition="3338" endWordPosition="3341"> that are built simultaneously to the constituent structures. 695 algorithm is simply run over all sentences in the training set, and since the correct trees are known in advance, we can simply record the features and correct parser actions that lead to the construction of the correct tree. Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). In our case, we split the training data according to the partof-speech (POS) tag of the head-word of the item on top of the stack, and trained each split of the training data separately. At run-time, every trained classifier is loaded, and the choice of classifier to use is made by looking at the head-word of the item on top of the stack in the current parser state. The total training time (a single machine was used and each classifier was trained in series) was slightly under nine hours. For comparison, Sagae and Lavie (2005) report that training support vector ma</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis using support vector machines. In Proceedings of the Eighth International Workshop on Parsing Technologies. Nancy, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>