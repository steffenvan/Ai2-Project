<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037165">
<title confidence="0.99741">
Comparing Objective and Subjective Measures of Usability
in a Human-Robot Dialogue System
</title>
<author confidence="0.700155">
Mary Ellen Foster and Manuel Giuliani and Alois Knoll
</author>
<affiliation confidence="0.6019555">
Informatik VI: Robotics and Embedded Systems
Technische Universit¨at M¨unchen
</affiliation>
<address confidence="0.915174">
Boltzmannstraße 3, 85748 Garching bei M¨unchen, Germany
</address>
<email confidence="0.998693">
{foster,giuliani,knoll}@in.tum.de
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954409090909">
We present a human-robot dialogue sys-
tem that enables a robot to work together
with a human user to build wooden con-
struction toys. We then describe a study in
which naive subjects interacted with this
system under a range of conditions and
then completed a user-satisfaction ques-
tionnaire. The results of this study pro-
vide a wide range of subjective and ob-
jective measures of the quality of the in-
teractions. To assess which aspects of the
interaction had the greatest impact on the
users’ opinions of the system, we used a
method based on the PARADISE evalua-
tion framework (Walker et al., 1997) to de-
rive a performance function from our data.
The major contributors to user satisfac-
tion were the number of repetition requests
(which had a negative effect on satisfac-
tion), the dialogue length, and the users’
recall of the system instructions (both of
which contributed positively).
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999806549019608">
Evaluating the usability of a spoken language dia-
logue system generally requires a large-scale user
study, which can be a time-consuming process
both for the experimenters and for the experimen-
tal subjects. In fact, it can be difficult even to
define what the criteria are for evaluating such a
system (cf. Novick, 1997). In recent years, tech-
niques have been introduced that are designed to
predict user satisfaction based on more easily mea-
sured properties of an interaction such as dialogue
length and speech-recognition error rate. The de-
sign of such performance methods for evaluating
dialogue systems is still an area of open research.
The PARADISE framework (PARAdigm for
DIalogue System Evaluation; Walker et al. (1997))
describes a method for using data to derive a per-
formance function that predicts user-satisfaction
scores from the results on other, more easily com-
puted measures. PARADISE uses stepwise mul-
tiple linear regression to model user satisfaction
based on measures representing the performance
dimensions of task success, dialogue quality, and
dialogue efficiency, and has been applied to a wide
range of systems (e.g., Walker et al., 2000; Litman
and Pan, 2002; M¨oller et al., 2008). If the result-
ing performance function can be shown to predict
user satisfaction as a function of other, more eas-
ily measured system properties, it will be widely
applicable: in addition to making it possible to
evaluate systems based on automatically available
data from log files without the need for extensive
experiments with users, for example, such a per-
formance function can be used in an online, incre-
mental manner to adapt system behaviour to avoid
entering a state that is likely to reduce user satis-
faction, or can be used as a reward function in a
reinforcement-learning scenario (Walker, 2000).
Automated evaluation metrics that rate sys-
tem behaviour based on automatically computable
properties have been developed in a number of
other fields: widely used measures include BLEU
(Papineni et al., 2002) for machine translation and
ROUGE (Lin, 2004) for summarisation, for exam-
ple. When employing any such metric, it is cru-
cial to verify that the predictions of the automated
evaluation process agree with human judgements
of the important aspects of the system output. If
not, the risk arises that the automated measures do
not capture the behaviour that is actually relevant
for the human users of a system. For example,
Callison-Burch et al. (2006) presented a number of
</bodyText>
<page confidence="0.982565">
879
</page>
<note confidence="0.999612">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879–887,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999194307692308">
counter-examples to the claim that BLEU agrees
with human judgements. Also, Foster (2008) ex-
amined a range of automated metrics for evalua-
tion generated multimodal output and found that
few agreed with the preferences expressed by hu-
man judges.
In this paper, we apply a PARADISE-style pro-
cess to the results of a user study of a human-robot
dialogue system. We build models to predict the
results on a set of subjective user-satisfaction mea-
sures, based on objective measures that were either
gathered automatically from the system logs or de-
rived from the video recordings of the interactions.
The results indicate that the most significant con-
tributors to user satisfaction were the number of
system turns in the dialogues, the users’ ability to
recall the instructions given by the robot, and the
number of times that the user had to ask for in-
structions to be repeated. The former two mea-
sures were positively correlated with user satisfac-
tion, while the latter had a negative impact on user
satisfaction; however the correlation in all cases
was relatively low. At the end of the paper, we
discuss possible reasons for these results and pro-
pose other measures that might have a larger effect
on users’ judgements.
</bodyText>
<sectionHeader confidence="0.991476" genericHeader="method">
2 Task-Based Human-Robot Dialogue
</sectionHeader>
<bodyText confidence="0.971368564102564">
This study makes use of the JAST human-robot
dialogue system (Rickert et al., 2007) which sup-
ports multimodal human-robot collaboration on a
joint construction task. The user and the robot
work together to assemble wooden construction
toys on a common workspace, coordinating their
actions through speech, gestures, and facial dis-
plays. The robot (Figure 1) consists of a pair
of manipulator arms with grippers, mounted in
a position to resemble human arms, and an an-
imatronic talking head (van Breemen, 2005) ca-
pable of producing facial expressions, rigid head
motion, and lip-synchronised synthesised speech.
The system can interact in English or German.
The robot is able to manipulate objects in the
workspace and to perform simple assembly tasks.
In the system that was used in the current study,
the robot instructs the user on building a partic-
ular compound object, explaining the necessary
assembly steps and retrieving pieces as required,
with the user performing the actual assembly ac-
tions. To make joint action necessary for success
in the assembly task, the workspace is divided into
SYSTEM First we will build a windmill. Okay?
USER Okay.
SYSTEM To make a windmill, we must make a
snowman.
SYSTEM [picking up and holding out red cube]
To make a snowman, insert the green bolt
through the end of this red cube and screw
it into the blue cube.
USER [takes cube, performs action] Okay.
SYSTEM [picking up and holding out a small
slat] To make a windmill, insert the yellow
bolt through the middle of this short slat and
the middle of another short slat and screw it
into the snowman.
USER [takes slat, performs action] Okay.
SYSTEM Very good!
</bodyText>
<figureCaption confidence="0.9998195">
Figure 2: Sample human-robot dialogue
Figure 1: The JAST dialogue robot
</figureCaption>
<page confidence="0.915796">
880
</page>
<figure confidence="0.997846">
(a) Windmill (b) Snowman (c) L Shape (d) Railway signal
</figure>
<figureCaption confidence="0.999987">
Figure 3: The four target objects used in the experiment
</figureCaption>
<bodyText confidence="0.999283333333333">
two areas—one belonging to the robot and one to
the user—so that the robot must hand over some
pieces to the user. Figure 2 shows a sample dia-
logue in which the system explains to the user how
to build an object called a ‘windmill’, which has a
sub-component called a ‘snowman’.
</bodyText>
<sectionHeader confidence="0.997668" genericHeader="method">
3 Experiment Design
</sectionHeader>
<bodyText confidence="0.9999850625">
The human-robot system was evaluated via a user
study in which subjects interacted with the com-
plete system; all interactions were in German. As
a between-subjects factor, we manipulated two as-
pects of the generated output: the strategy used by
the dialogue manager to explain a plan to the user,
and the type of referring expressions produced by
the system. Foster et al. (2009) give the details
of these factors and describes the effects of each
individual manipulation. In this paper, we concen-
trate on the relationships among the different fac-
tors that were measured during the study: the effi-
ciency and quality of the dialogues, the users’ suc-
cess at building the required objects and at learn-
ing the construction plans for new objects, and the
users’ subjective reactions to the system.
</bodyText>
<subsectionHeader confidence="0.998598">
3.1 Subjects
</subsectionHeader>
<bodyText confidence="0.999950923076923">
43 subjects (27 male) took part in this experi-
ment; the results of one additional subject were
discarded due to technical problems with the sys-
tem. The mean age of the subjects was 24.5, with a
minimum of 14 and a maximum of 55. Of the sub-
jects who indicated an area of study, the two most
common areas were Informatics (12 subjects) and
Mathematics (10). On a scale of 1–5, subjects
gave a mean assessment of their knowledge of
computers at 3.4, of speech-recognition systems
at 2.3, and of human-robot systems at 2.0. The
subjects were compensated for their participation
in the experiment.
</bodyText>
<subsectionHeader confidence="0.999179">
3.2 Scenario
</subsectionHeader>
<bodyText confidence="0.9999458125">
In this experiment, each subject built the same
three objects in collaboration with the system,
always in the same order. The first target
was a ‘windmill’ (Figure 3a), which has a sub-
component called a ‘snowman’ (Figure 3b). Once
the windmill was completed, the system then
walked the user through building an ‘L shape’
(Figure 3c). Finally, the robot instructed the user
to build a ‘railway signal’ (Figure 3d), which com-
bines an L shape with a snowman. During the con-
struction of the railway signal, the system asked
the user if they remembered how to build a snow-
man and an L shape. If the user did not remember,
the system explained the building process again; if
they did remember, the system simply told them to
build another one.
</bodyText>
<subsectionHeader confidence="0.999406">
3.3 Dependent Variables
</subsectionHeader>
<bodyText confidence="0.9999882">
We gathered a wide range of dependent measures:
objective measures derived from the system logs
and video recordings, as well as subjective mea-
sures based on the users’ own ratings of their ex-
perience interacting with the system.
</bodyText>
<subsectionHeader confidence="0.818336">
3.3.1 Objective Measures
</subsectionHeader>
<bodyText confidence="0.999984333333333">
We collected a range of objective measures from
the log files and videos of the interactions. Like
Litman and Pan (2002), we divided our objective
measures into three categories based on those used
in the PARADISE framework: dialogue efficiency,
dialogue quality, and task success.
The dialogue efficiency measures concentrated
on the timing of the interaction: the time taken to
complete the three construction tasks, the number
of system turns required for the complete interac-
tion, and the mean time taken by the system to re-
spond to the user’s requests.
We considered four measures of dialogue qual-
ity. The first two measures looked specifically for
signs of problems in the interaction, using data au-
</bodyText>
<page confidence="0.988594">
881
</page>
<bodyText confidence="0.999984363636363">
tomatically extracted from the logs: the number of
times that the user asked the system to repeat its
instructions, and the number of times that the user
failed to take an object that the robot attempted to
hand over. The other two dialogue quality mea-
sures were computed based on the video record-
ings: the number of times that the user looked at
the robot, and the percentage of the total inter-
action that they spent looking at the robot. We
considered these gaze-based measures to be mea-
sures of dialogue quality since it has previously
been shown that, in this sort of task-based interac-
tion where there is a visually salient object, par-
ticipants tend to look at their partner more often
when there is a problem in the interaction (e.g.,
Argyle and Graham, 1976).
The task success measures addressed user suc-
cess in the two main tasks undertaken in these in-
teractions: assembling the target objects following
the robot’s instructions, and learning and remem-
bering to make a snowman and an L shape. We
measured task success in two ways, correspond-
ing to these two main tasks. The user’s success in
the overall assembly task was assessed by count-
ing the proportion of target objects that were as-
sembled as intended (i.e., as in Figure 3), which
was judged based on the video recordings. To
test whether the subjects had learned how to build
the sub-components that were required more than
once (the snowman and the L shape), we recorded
whether they said yes or no when they were asked
if they remembered each of these components dur-
ing the construction of the railway signal.
</bodyText>
<subsectionHeader confidence="0.802484">
3.3.2 Subjective Measures
</subsectionHeader>
<bodyText confidence="0.999973411764706">
In addition to the above objective measures, we
also gathered a range of subjective measures. Be-
fore the interaction, we asked subjects to rate their
current level on a set of 22 emotions (Ortony
et al., 1988) on a scale from 1 to 4; the subjects
then rated their level on the same emotional scales
again after the interaction. After the interaction,
the subjects also filled out a user-satisfaction ques-
tionnaire, which was based on that used in the user
evaluation of the COMIC dialogue system (White
et al., 2005), with modifications to address spe-
cific aspects of the current dialogue system and the
experimental manipulations in this study. There
were 47 items in total, each of which requested
that the user choose their level of agreement with
a given statement on a five-point Likert scale. The
items were divided into the following categories:
</bodyText>
<table confidence="0.99580275">
Mean (Stdev) Min Max
Length (sec) 305.1 (54.0) 195.2 488.4
System turns 13.4 (1.73) 11 18
Response time (sec) 2.79 (1.13) 1.27 7.21
</table>
<tableCaption confidence="0.999377">
Table 1: Dialogue efficiency results
</tableCaption>
<bodyText confidence="0.999042615384615">
Opinion of the robot as a partner 21 items ad-
dressing the ease with which subjects were
able to interact with the robot
Instruction quality 6 items specifically address-
ing the quality of the assembly instructions
given by the robot
Task success 11 items asking the user to rate how
well they felt they performed on the various
assembly tasks
Feelings of the user 9 items asking users to rate
their feelings while using the system
At the end of the questionnaire, subjects were also
invited to give free-form comments.
</bodyText>
<sectionHeader confidence="0.999975" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999977">
In this section, we present the results of each of
the individual dependent measures; in the follow-
ing section, we examine the relationship among
the different types of measures. These results are
based on the data from 40 subjects: we excluded
results from two subjects for whom the video data
was not clear, and from one additional subject who
appeared to be ‘testing’ the system rather than
making a serious effort to interact with it.
</bodyText>
<subsectionHeader confidence="0.962374">
4.1 Objective Measures
</subsectionHeader>
<bodyText confidence="0.99987175">
Dialogue efficiency The results on the dialogue
efficiency measures are shown in Table 1. The
average subject took 305.1 seconds—that is, just
over five minutes—to build all three of the objects,
and an average dialogue took 13 system turns to
complete. When a user made a request, the mean
delay before the beginning of the system response
was about three seconds, although for one user this
time was more than twice as long. This response
delay resulted from two factors. First, prepar-
ing long system utterances with several referring
expressions (such as the third and fourth system
turns in Figure 2) takes some time; second, if a
user made a request during a system turn (i.e., a
‘barge-in’ attempt), the system was not able to re-
spond until the current turn was completed.
</bodyText>
<page confidence="0.982095">
882
</page>
<table confidence="0.9603015">
Mean (Stdev) Min Max
Repetition requests 1.86 (1.79) 0 6
Failed hand-overs 1.07 (1.35) 0 6
Looks at the robot 23.55 (8.21) 14 50
Time looking at robot (%) 27 (8.6) 12 51
Table 2: Dialogue quality results
Object Rate Memory
Snowman 0.76
Windmill 0.55
L shape 0.90
L shape 0.90 0.88
Snowman 0.86 0.70
Railway signal 0.71
Overall 0.72 0.79
</table>
<bodyText confidence="0.9986055">
These three measures of efficiency were cor-
related with each other: the correlation between
length and turns was 0.38; between length and re-
sponse time 0.47; and between turns and response
time 0.19 (all p &lt; 0.0001).
Dialogue quality Table 2 shows the results for
the dialogue quality measures: the two indica-
tions of problems, and the two measures of the
frequency with which the subjects looked at the
robot’s head. On average, a subject asked for an
instruction to be repeated nearly two times per
interaction, while failed hand-overs occurred just
over once per interaction; however, as can be seen
from the standard-deviation values, these mea-
sures varied widely across the data. In fact, 18
subjects never failed to take an object from the
robot when it was offered, while one subject did so
five times and one six times. Similarly, 11 subjects
never asked for any repetitions, while five subjects
asked for repetitions five or more times.1 On aver-
age, the subjects in this study spent about a quarter
of the interaction looking at the robot head, and
changed their gaze to the robot 23.5 times over
the course of the interaction. Again, there was a
wide range of results for both of these measures:
15 subjects looked at the robot fewer than 20 times
during the interaction, 20 subjects looked at the
robot between 20 to 30 times, while 5 subjects
looked at the robot more than 30 times.
The two measures that count problems were
mildly correlated with each other (R2 = 0.26, p &lt;
0.001), as were the two measures of looking at the
robot (R2 = 0.13, p &lt; 0.05); there was no correla-
tion between the two classes of measures.
Task success Table 3 shows the success rate for
assembling each object in the sequence. Objects
in italics represent sub-components, as follows:
the first snowman was constructed as part of the
windmill, while the second formed part of the rail-
way signal; the first L-shape was a goal in itself,
</bodyText>
<footnote confidence="0.930699333333333">
1The requested repetition rate was significantly affected
by the description strategy used by the dialogue manager; see
Foster et al. (2009) for details.
</footnote>
<tableCaption confidence="0.99673">
Table 3: Task success results
</tableCaption>
<bodyText confidence="0.999788238095238">
while the second was also part of the process of
building the railway signal. The Rate column indi-
cates subjects’ overall success at building the rel-
evant component—for example, 55% of the sub-
jects built the windmill correctly, while both of
the L-shapes were built with 90% accuracy. For
the second occurrence of the snowman and the L-
shape, the Memory column indicates the percent-
age of subjects who claimed to remember how to
build it when asked. The Overall row at the bottom
indicates subjects’ overall success rate at building
the three main target objects (windmill, L shape,
railway signal): on average, a subject built about
two of the three objects correctly.
The overall correct-assembly rate was corre-
lated with the overall rate of remembering objects:
R2 = 0.20, p &lt; 0.005. However, subjects who said
that they did remember how to build a snowman or
an L shape the second time around were no more
likely to do it correctly than those who said that
they did not remember.
</bodyText>
<subsectionHeader confidence="0.970747">
4.2 Subjective Measures
</subsectionHeader>
<bodyText confidence="0.999988944444444">
Two types of subjective measures were gath-
ered during this study: responses on the user-
satisfaction questionnaire, and self-assessment of
emotions. Table 4 shows the mean results for each
category from the user-satisfaction questionnaire
across all of the subjects, in all cases on a 5-point
Likert scale. The subjects in this study gave a
generally positive assessment of their interactions
with the system—with a mean overall satisfaction
score of 3.75—and rated their perceived task suc-
cess particularly highly, with a mean score of 4.1.
To analyse the emotional data, we averaged all
of the subjects’ emotional self-ratings before and
after the experiment, counting negative emotions
on an inverse scale, and then computed the differ-
ence between the two means. Table 5 shows the re-
sults from this analysis; note that this value was as-
sessed on a 1–4 scale. While the mean emotional
</bodyText>
<page confidence="0.998031">
883
</page>
<table confidence="0.999758666666667">
Question category Mean (Stdev)
Robot as partner 3.63 (0.65)
Instruction quality 3.69 (0.71)
Task success 4.10 (0.68)
Feelings 3.66 (0.61)
Overall 3.75 (0.57)
</table>
<tableCaption confidence="0.979489">
Table 4: User-satisfaction questionnaire results
</tableCaption>
<table confidence="0.9990265">
Mean (Stdev) Min Max
Before the study 2.99 (0.32) 2.32 3.68
After the study 3.05 (0.32) 2.32 3.73
Change +0.06 (0.24) −0.55 +0.45
</table>
<tableCaption confidence="0.999674">
Table 5: Mean emotional assessments
</tableCaption>
<bodyText confidence="0.999828181818182">
score across all of the subjects did not change over
the course of the experiment, the ratings of indi-
vidual subjects did show larger changes. As shown
in the final row of the table, one subject’s mean rat-
ing decreased by 0.55 over the course of the inter-
action, while that of another subject increased by
0.45. There was a slight correlation between the
subjects’ description of their emotional state after
the experiment and their responses to the question-
naire items asking for feelings about the interac-
tion: R2 = 0.14, p &lt; 0.01.
</bodyText>
<sectionHeader confidence="0.966431" genericHeader="method">
5 Building Performance Functions
</sectionHeader>
<bodyText confidence="0.999996761904762">
In the preceding section, we presented results on a
number of objective and subjective measures, and
also examined the correlation among measures of
the same type. The results on the objective mea-
sures varied widely across the subjects; also, the
subjects generally rated their experience of using
the system positively, but again with some varia-
tion. In this section, we examine the relationship
among measures of different types in order to de-
termine which of the objective measures had the
largest effect on users’ subjective reactions to the
dialogue system.
To determine the relationship among the fac-
tors, we employed the procedure used in the
PARADISE evaluation framework (Walker et al.,
1997). The PARADISE model uses stepwise mul-
tiple linear regression to predict subjective user
satisfaction based on measures representing the
performance dimensions of task success, dialogue
quality, and dialogue efficiency, resulting in a pre-
dictor function of the following form:
</bodyText>
<equation confidence="0.978199333333333">
n
Satisfaction = � wi * -iV (mi)
i=1
</equation>
<bodyText confidence="0.999981347826087">
The mi terms represent the value of each measure,
while the ✓&apos; function transforms each measure
into a normal distribution using z-score normali-
sation. Stepwise linear regression produces coef-
ficients (wi) describing the relative contribution of
each predictor to the user satisfaction. If a predic-
tor does not contribute significantly, its wi value is
zero after the stepwise process.
Using stepwise linear regression, we computed
a predictor function for each of the subjective mea-
sures that we gathered during our study: the mean
score for each of the individual user-satisfaction
categories (Table 4), the mean score across the
whole questionnaire (the last line of Table 4), as
well as the difference between the users’ emo-
tional states before and after the study (the last line
of Table 5). We included all of the objective mea-
sures from Section 4.1 as initial predictors.
The resulting predictor functions are shown in
Table 6. The following abbreviations are used for
the factors that occur in the table: Rep for the num-
ber of repetition requests, Turns for the number of
system turns, Len for the length of the dialogue,
and Mem for the subjects’ memory for the com-
ponents that were built twice. The R2 column in-
dicates the percentage of the variance that is ex-
plained by the performance function, while the
Significance column gives significance values for
each term in the function.
Although the R2 values for the predictor func-
tions in Table 6 are generally quite low, indicat-
ing that the functions do not explain most of the
variance in the data, the factors that remain after
stepwise regression still provide an indication as
to which of the objective measures had an effect
on users’ opinions of the system. In general, users
who had longer interactions with the system (in
terms of system turns) and who said that they re-
membered the robot’s instructions tended to give
the system higher scores, while users who asked
for more instructions to be repeated tended to give
it lower scores; for the robot-as-partner questions,
the length of the dialogue in seconds also made a
slight negative contribution. None of the other ob-
jective factors contributed significantly to any of
the predictor functions.
</bodyText>
<sectionHeader confidence="0.999148" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999137333333333">
That the factors included in Table 6 were the most
significant contributors to user satisfaction is not
surprising. If a user asks for instructions to be re-
</bodyText>
<page confidence="0.995877">
884
</page>
<table confidence="0.999889461538462">
Measure Function R2 Significance
Robot as partner 3.60 + 0.53 * N (Turns) − 0.39 * N (Rep) − 0.18 * N (Len) 0.12 Turns: p &lt; 0.01,
Rep: p &lt; 0.05,
Length: p ,zt� 0.17
Instruction quality 3.66 − 0.22 * N (Rep) 0.081 Rep: p &lt; 0.05
Task success 4.07 +0.20 * N (Mem) 0.058 Mem: p ,,zt� 0.07
Feelings 3.63 +0.34 * N (Turns) − 0.32 * N (Rep) 0.044 Turns: p ,,zt� 0.06, Rep:
p ,,zt� 0.08
Overall 3.73 − 0.36 * N (Rep) + 0.31 * N (Turns) 0.062 Rep: p &lt; 0.05,
Turns: p ,,zt� 0.06
Emotion change 0.07 +0.14 * N (Turns) +0.11 * N (Mem) − 0.090 * N (Rep) 0.20 Turns: p &lt; 0.05,
Mem: p &lt; 0.01,
Rep: p,,zt�0.17
</table>
<tableCaption confidence="0.99858">
Table 6: Predictor functions
</tableCaption>
<bodyText confidence="0.999943930555556">
peated, this is a clear indication of a problem in
the dialogue; similarly, users who remembered the
system’s instructions were equally clearly having
a relatively successful interaction.
In the current study, increased dialogue length
had a positive contribution to user satisfaction; this
contrasts with results such as those of Litman and
Pan (2002), who found that increased dialogue
length was associated with decreased user satis-
faction. We propose two possible explanations for
this difference. First, the system analysed by Lit-
man and Pan (2002) was an information-seeking
dialogue system, in which efficient access to the
information is an important criterion. The current
system, on the other hand, has the goal of joint task
execution, and pure efficiency is a less compelling
measure of dialogue quality in this setting. Sec-
ond, it is possible that the sheer novelty factor of
interacting with a fully-embodied humanoid robot
affected people’s subjective responses to the sys-
tem, so that subjects who had longer interactions
also enjoyed the experience more. Support for this
explanation is provided by the fact that dialogue
length was only a significant factor in the more
‘subjective’ parts of the questionnaire, but did not
have a significant impact on the users’ judgements
about instruction quality or task success. Other
studies of human-robot dialogue systems have also
had similar results: for example, the subjects in the
study described by Sidner et al. (2005) who used
a robot that moved while talking reported higher
levels of engagement in the interaction, and also
tended to have longer conversations with the robot.
While the predictor functions give useful in-
sights into the relative contribution of the objective
measures to the subjective user satisfaction, the
R2 values are generally lower than those found in
other PARADISE-style evaluations. For example,
Walker et al. (1998) reported an R2 value of 0.38,
the values reported by Walker et al. (2000) on the
training sets ranged from 0.39 to 0.56, Litman and
Pan (2002) reported an R2 value of 0.71, while
the R2 values reported by M¨oller et al. (2008)
for linear regression models similar to those pre-
sented here were between 0.22 and 0.57. The low
R2 values from this analysis clearly suggest that,
while the factors included in Table 6 did affect
users’ opinions—particularly their opinion of the
robot as a partner and the change in their reported
emotional state—the users’ subjective judgements
were also affected by factors other than those cap-
tured by the objective measures considered here.
In most of the previous PARADISE-style stud-
ies, measures addressing the performance of the
automated speech-recognition system and other
input-processing components were included in the
models. For example, the factors listed by M¨oller
et al. (2008) include several measures of word er-
ror rate and of parsing accuracy. However, the sce-
nario that was used in the current study required
minimal speech input from the user (see Figure 2),
so we did not include any such input-processing
factors in our models.
Other objective factors that might be relevant
for predicting user satisfaction in the current study
include a range of non-verbal behaviour from the
users. For example, the user’s reaction time to in-
structions from the robot, the time the users need
to adapt to the robot’s movements during hand-
over actions (Huber et al., 2008), or the time taken
for the actual assembly of the objects. Also, other
measures of the user’s gaze behaviour might be
</bodyText>
<page confidence="0.99506">
885
</page>
<bodyText confidence="0.999900555555556">
useful: more global measures such as how often
the users look at the robot arms or at the objects on
the table, as well as more targeted measures exam-
ining factors such as the user’s gaze and other be-
haviour during and after different types of system
outputs. In future studies, we will also gather data
on these additional non-verbal behaviours, and we
expect to find higher correlations with subjective
judgements.
</bodyText>
<sectionHeader confidence="0.997829" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999983404255319">
We have presented the JAST human-robot dia-
logue system and described a user study in which
the system instructed users to build a series of tar-
get objects out of wooden construction toys. This
study resulted in a range of objective and subjec-
tive measures, which were used to derive perfor-
mance functions in the style of the PARADISE
evaluation framework. Three main factors were
found to affect the users’ subjective ratings: longer
dialogues and higher recall performance were as-
sociated with increased user satisfaction, while di-
alogues with more repetition requests tended to be
associated with lower satisfaction scores. The ex-
plained variance of the performance functions was
generally low, suggesting that factors other than
those measured in this study contributed to the
user satisfaction scores; we have suggested several
such factors.
The finding that longer dialogues were associ-
ated with higher user satisfaction disagrees with
the results of many previous PARADISE-style
evaluation studies. However, it does confirm and
extend the results of previous studies specifically
addressing interactions between users and embod-
ied agents: as in the previous studies, the users in
this case seem to view the agent as a social entity
with whom they enjoy having a conversation.
A newer version of the JAST system is currently
under development and will shortly undergo a user
evaluation. This new system will support an ex-
tended set of interactions where both agents know
the target assembly plan, and will will also in-
corporate enhanced components for vision, object
recognition, and goal inference. When evaluat-
ing this new system, we will include similar mea-
sures to those described here to enable the eval-
uations of the two systems to be compared. We
will also gather additional objective measures in
order to measure their influence on the subjective
results. These additional measures will include
those mentioned at the end of the preceding sec-
tion, as well as measures targeted at the revised
scenario and the updated system capabilities—for
example, an additional dialogue quality measure
will assess how often the goal-inference system
was able to detect and correctly respond to an error
by the user.
</bodyText>
<sectionHeader confidence="0.996401" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999847">
This research was supported by the Euro-
pean Commission through the JAST2 (IST-
FP6-003747-IP) and INDIGO3 (IST-FP6-045388)
projects. Thanks to Pawel Dacka for his help in
running the experiment and analysing the data.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999434266666666">
M. Argyle and J. A. Graham. 1976. The Cen-
tral Europe experiment: Looking at persons and
looking at objects. Environmental Psychology
and Nonverbal Behavior, 1(1):6–16. doi:10.
1007/BF01115461.
A. J. N. van Breemen. 2005. iCat: Experimenting
with animabotics. In Proceedings of the AISB
2005 Creative Robotics Symposium.
C. Callison-Burch, M. Osborne, and P. Koehn.
2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of
EACL 2006. ACL Anthology E06-1032.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for
an embodied conversational agent. In Proceed-
ings ofINLG 2008. ACL Anthology W08-1113.
M. E. Foster, M. Giuliani, A. Isard, C. Matheson,
J. Oberlander, and A. Knoll. 2009. Evaluating
description and reference strategies in a coop-
erative human-robot dialogue system. In Pro-
ceedings of IJCAI 2009.
M. Huber, M. Rickert, A. Knoll, T. Brandt, and
S. Glasauer. 2008. Human-robot interaction in
handing-over tasks. In Proceedings of IEEE
RO-MAN 2008. doi:10.1109/ROMAN.2008.
4600651.
C. Y. Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings
of the ACL 2004 Workshop on Text Summariza-
tion. ACL Anthology W04-1013.
</reference>
<footnote confidence="0.9973245">
2http://www.euprojects-jast.net/
3http://www.ics.forth.gr/indigo/
</footnote>
<page confidence="0.996329">
886
</page>
<reference confidence="0.999410392857143">
D. J. Litman and S. Pan. 2002. Designing and
evaluating an adaptive spoken dialogue sys-
tem. User Modeling and User-Adapted Inter-
action, 12(2–3):111–137. doi:10.1023/A:
1015036910358.
S. M¨oller, K.-P. Engelbrecht, and R. Schleicher.
2008. Predicting the quality and usability of
spoken dialogue systems. Speech Communica-
tion, 50:730–744. doi:10.1016/j.specom.
2008.03.001.
D. G. Novick. 1997. What is effective-
ness? In Working notes, CHI ’97 Work-
shop on HCI Research and Practice Agenda
Based on Human Needs and Social Responsi-
bility. http://www.cs.utep.edu/novick/
papers/eff.chi.html.
A. Ortony, G. L. Clore, and A. Collins. 1988. The
Cognitive Structure of Emotions. Cambridge
University Press.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2002. BLEU: A method for automatic evalua-
tion of machine translation. In Proceedings of
ACL 2002. ACL Anthology P02-1040.
M. Rickert, M. E. Foster, M. Giuliani, T. By,
G. Panin, and A. Knoll. 2007. Integrating lan-
guage, vision and action for human robot dialog
systems. In Proceedings of HCI International
2007. doi:10.1007/978-3-540-73281-5_
108.
C. L. Sidner, C. Lee, C. D. Kidd, N. Lesh, and
C. Rich. 2005. Explorations in engagement
for humans and robots. Artificial Intelligence,
166(1–2):140–164. doi:10.1016/j.artint.
2005.03.005.
M. Walker, C. Kamm, and D. Litman. 2000. To-
wards developing general models of usability
with PARADISE. Natural Language Engineer-
ing, 6(3–4):363–377.
M. A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in
a spoken dialogue system for email. Journal of
Artificial Intelligence Research, 12:387–416.
M. A. Walker, J. Fromer, G. D. Fabbrizio, C. Mes-
tel, and D. Hindle. 1998. What can I say?: Eval-
uating a spoken language interface to email.
In Proceedings of CHI 1998. doi:10.1145/
274644.274722.
M. A. Walker, D. J. Litman, C. A. Kamm, and
A. Abella. 1997. PARADISE: A framework for
evaluating spoken dialogue agents. In Proceed-
ings of ACL/EACL 1997. ACL Anthology P97-
1035.
M. White, M. E. Foster, J. Oberlander, and
A. Brown. 2005. Using facial feedback to en-
hance turn-taking in a multimodal dialogue sys-
tem. In Proceedings of HCI International 2005.
</reference>
<page confidence="0.998023">
887
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.886682">
<title confidence="0.998157">Comparing Objective and Subjective Measures of Usability in a Human-Robot Dialogue System</title>
<author confidence="0.997763">Ellen Foster Giuliani Knoll</author>
<affiliation confidence="0.9660535">Informatik VI: Robotics and Embedded Systems Technische Universit¨at M¨unchen</affiliation>
<address confidence="0.975919">Boltzmannstraße 3, 85748 Garching bei M¨unchen, Germany</address>
<abstract confidence="0.997537086956522">We present a human-robot dialogue system that enables a robot to work together with a human user to build wooden construction toys. We then describe a study in which naive subjects interacted with this system under a range of conditions and then completed a user-satisfaction questionnaire. The results of this study provide a wide range of subjective and objective measures of the quality of the interactions. To assess which aspects of the interaction had the greatest impact on the users’ opinions of the system, we used a method based on the PARADISE evaluation framework (Walker et al., 1997) to derive a performance function from our data. The major contributors to user satisfaction were the number of repetition requests (which had a negative effect on satisfaction), the dialogue length, and the users’ recall of the system instructions (both of which contributed positively).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Argyle</author>
<author>J A Graham</author>
</authors>
<title>The Central Europe experiment: Looking at persons and looking at objects.</title>
<date>1976</date>
<booktitle>Environmental Psychology and Nonverbal Behavior,</booktitle>
<volume>1</volume>
<issue>1</issue>
<pages>10--1007</pages>
<contexts>
<context position="11174" citStr="Argyle and Graham, 1976" startWordPosition="1856" endWordPosition="1859">hat the user failed to take an object that the robot attempted to hand over. The other two dialogue quality measures were computed based on the video recordings: the number of times that the user looked at the robot, and the percentage of the total interaction that they spent looking at the robot. We considered these gaze-based measures to be measures of dialogue quality since it has previously been shown that, in this sort of task-based interaction where there is a visually salient object, participants tend to look at their partner more often when there is a problem in the interaction (e.g., Argyle and Graham, 1976). The task success measures addressed user success in the two main tasks undertaken in these interactions: assembling the target objects following the robot’s instructions, and learning and remembering to make a snowman and an L shape. We measured task success in two ways, corresponding to these two main tasks. The user’s success in the overall assembly task was assessed by counting the proportion of target objects that were assembled as intended (i.e., as in Figure 3), which was judged based on the video recordings. To test whether the subjects had learned how to build the sub-components that</context>
</contexts>
<marker>Argyle, Graham, 1976</marker>
<rawString>M. Argyle and J. A. Graham. 1976. The Central Europe experiment: Looking at persons and looking at objects. Environmental Psychology and Nonverbal Behavior, 1(1):6–16. doi:10. 1007/BF01115461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J N van Breemen</author>
</authors>
<title>iCat: Experimenting with animabotics.</title>
<date>2005</date>
<booktitle>In Proceedings of the AISB</booktitle>
<marker>van Breemen, 2005</marker>
<rawString>A. J. N. van Breemen. 2005. iCat: Experimenting with animabotics. In Proceedings of the AISB 2005 Creative Robotics Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<journal>ACL Anthology</journal>
<booktitle>In Proceedings of EACL</booktitle>
<pages>06--1032</pages>
<contexts>
<context position="3693" citStr="Callison-Burch et al. (2006)" startWordPosition="585" endWordPosition="588">s that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU (Papineni et al., 2002) for machine translation and ROUGE (Lin, 2004) for summarisation, for example. When employing any such metric, it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output. If not, the risk arises that the automated measures do not capture the behaviour that is actually relevant for the human users of a system. For example, Callison-Burch et al. (2006) presented a number of 879 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879–887, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP counter-examples to the claim that BLEU agrees with human judgements. Also, Foster (2008) examined a range of automated metrics for evaluation generated multimodal output and found that few agreed with the preferences expressed by human judges. In this paper, we apply a PARADISE-style process to the results of a user study of a human-robot dialogue system. We build models to predict the results on a set of subjecti</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In Proceedings of EACL 2006. ACL Anthology E06-1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
</authors>
<title>Automated metrics that agree with human judgements on generated output for an embodied conversational agent.</title>
<date>2008</date>
<journal>ACL Anthology</journal>
<booktitle>In Proceedings ofINLG</booktitle>
<pages>08--1113</pages>
<contexts>
<context position="3964" citStr="Foster (2008)" startWordPosition="632" endWordPosition="633">ric, it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output. If not, the risk arises that the automated measures do not capture the behaviour that is actually relevant for the human users of a system. For example, Callison-Burch et al. (2006) presented a number of 879 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879–887, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP counter-examples to the claim that BLEU agrees with human judgements. Also, Foster (2008) examined a range of automated metrics for evaluation generated multimodal output and found that few agreed with the preferences expressed by human judges. In this paper, we apply a PARADISE-style process to the results of a user study of a human-robot dialogue system. We build models to predict the results on a set of subjective user-satisfaction measures, based on objective measures that were either gathered automatically from the system logs or derived from the video recordings of the interactions. The results indicate that the most significant contributors to user satisfaction were the num</context>
</contexts>
<marker>Foster, 2008</marker>
<rawString>M. E. Foster. 2008. Automated metrics that agree with human judgements on generated output for an embodied conversational agent. In Proceedings ofINLG 2008. ACL Anthology W08-1113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>M Giuliani</author>
<author>A Isard</author>
<author>C Matheson</author>
<author>J Oberlander</author>
<author>A Knoll</author>
</authors>
<title>Evaluating description and reference strategies in a cooperative human-robot dialogue system.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<contexts>
<context position="7650" citStr="Foster et al. (2009)" startWordPosition="1254" endWordPosition="1257">o that the robot must hand over some pieces to the user. Figure 2 shows a sample dialogue in which the system explains to the user how to build an object called a ‘windmill’, which has a sub-component called a ‘snowman’. 3 Experiment Design The human-robot system was evaluated via a user study in which subjects interacted with the complete system; all interactions were in German. As a between-subjects factor, we manipulated two aspects of the generated output: the strategy used by the dialogue manager to explain a plan to the user, and the type of referring expressions produced by the system. Foster et al. (2009) give the details of these factors and describes the effects of each individual manipulation. In this paper, we concentrate on the relationships among the different factors that were measured during the study: the efficiency and quality of the dialogues, the users’ success at building the required objects and at learning the construction plans for new objects, and the users’ subjective reactions to the system. 3.1 Subjects 43 subjects (27 male) took part in this experiment; the results of one additional subject were discarded due to technical problems with the system. The mean age of the subje</context>
<context position="17210" citStr="Foster et al. (2009)" startWordPosition="2895" endWordPosition="2898">orrelated with each other (R2 = 0.26, p &lt; 0.001), as were the two measures of looking at the robot (R2 = 0.13, p &lt; 0.05); there was no correlation between the two classes of measures. Task success Table 3 shows the success rate for assembling each object in the sequence. Objects in italics represent sub-components, as follows: the first snowman was constructed as part of the windmill, while the second formed part of the railway signal; the first L-shape was a goal in itself, 1The requested repetition rate was significantly affected by the description strategy used by the dialogue manager; see Foster et al. (2009) for details. Table 3: Task success results while the second was also part of the process of building the railway signal. The Rate column indicates subjects’ overall success at building the relevant component—for example, 55% of the subjects built the windmill correctly, while both of the L-shapes were built with 90% accuracy. For the second occurrence of the snowman and the Lshape, the Memory column indicates the percentage of subjects who claimed to remember how to build it when asked. The Overall row at the bottom indicates subjects’ overall success rate at building the three main target ob</context>
</contexts>
<marker>Foster, Giuliani, Isard, Matheson, Oberlander, Knoll, 2009</marker>
<rawString>M. E. Foster, M. Giuliani, A. Isard, C. Matheson, J. Oberlander, and A. Knoll. 2009. Evaluating description and reference strategies in a cooperative human-robot dialogue system. In Proceedings of IJCAI 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huber</author>
<author>M Rickert</author>
<author>A Knoll</author>
<author>T Brandt</author>
<author>S Glasauer</author>
</authors>
<title>Human-robot interaction in handing-over tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of IEEE RO-MAN</booktitle>
<pages>10--1109</pages>
<contexts>
<context position="27541" citStr="Huber et al., 2008" startWordPosition="4613" endWordPosition="4616"> listed by M¨oller et al. (2008) include several measures of word error rate and of parsing accuracy. However, the scenario that was used in the current study required minimal speech input from the user (see Figure 2), so we did not include any such input-processing factors in our models. Other objective factors that might be relevant for predicting user satisfaction in the current study include a range of non-verbal behaviour from the users. For example, the user’s reaction time to instructions from the robot, the time the users need to adapt to the robot’s movements during handover actions (Huber et al., 2008), or the time taken for the actual assembly of the objects. Also, other measures of the user’s gaze behaviour might be 885 useful: more global measures such as how often the users look at the robot arms or at the objects on the table, as well as more targeted measures examining factors such as the user’s gaze and other behaviour during and after different types of system outputs. In future studies, we will also gather data on these additional non-verbal behaviours, and we expect to find higher correlations with subjective judgements. 7 Conclusions and Future Work We have presented the JAST hum</context>
</contexts>
<marker>Huber, Rickert, Knoll, Brandt, Glasauer, 2008</marker>
<rawString>M. Huber, M. Rickert, A. Knoll, T. Brandt, and S. Glasauer. 2008. Human-robot interaction in handing-over tasks. In Proceedings of IEEE RO-MAN 2008. doi:10.1109/ROMAN.2008. 4600651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL 2004 Workshop on Text Summarization. ACL Anthology</booktitle>
<pages>04--1013</pages>
<contexts>
<context position="3291" citStr="Lin, 2004" startWordPosition="520" endWordPosition="521"> available data from log files without the need for extensive experiments with users, for example, such a performance function can be used in an online, incremental manner to adapt system behaviour to avoid entering a state that is likely to reduce user satisfaction, or can be used as a reward function in a reinforcement-learning scenario (Walker, 2000). Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU (Papineni et al., 2002) for machine translation and ROUGE (Lin, 2004) for summarisation, for example. When employing any such metric, it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output. If not, the risk arises that the automated measures do not capture the behaviour that is actually relevant for the human users of a system. For example, Callison-Burch et al. (2006) presented a number of 879 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879–887, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP counter-examples</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Y. Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Proceedings of the ACL 2004 Workshop on Text Summarization. ACL Anthology W04-1013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>S Pan</author>
</authors>
<title>Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction,</title>
<date>2002</date>
<volume>12</volume>
<issue>2</issue>
<pages>1015036910358</pages>
<contexts>
<context position="2406" citStr="Litman and Pan, 2002" startWordPosition="375" endWordPosition="378">ce methods for evaluating dialogue systems is still an area of open research. The PARADISE framework (PARAdigm for DIalogue System Evaluation; Walker et al. (1997)) describes a method for using data to derive a performance function that predicts user-satisfaction scores from the results on other, more easily computed measures. PARADISE uses stepwise multiple linear regression to model user satisfaction based on measures representing the performance dimensions of task success, dialogue quality, and dialogue efficiency, and has been applied to a wide range of systems (e.g., Walker et al., 2000; Litman and Pan, 2002; M¨oller et al., 2008). If the resulting performance function can be shown to predict user satisfaction as a function of other, more easily measured system properties, it will be widely applicable: in addition to making it possible to evaluate systems based on automatically available data from log files without the need for extensive experiments with users, for example, such a performance function can be used in an online, incremental manner to adapt system behaviour to avoid entering a state that is likely to reduce user satisfaction, or can be used as a reward function in a reinforcement-le</context>
<context position="9820" citStr="Litman and Pan (2002)" startWordPosition="1626" endWordPosition="1629">ked the user if they remembered how to build a snowman and an L shape. If the user did not remember, the system explained the building process again; if they did remember, the system simply told them to build another one. 3.3 Dependent Variables We gathered a wide range of dependent measures: objective measures derived from the system logs and video recordings, as well as subjective measures based on the users’ own ratings of their experience interacting with the system. 3.3.1 Objective Measures We collected a range of objective measures from the log files and videos of the interactions. Like Litman and Pan (2002), we divided our objective measures into three categories based on those used in the PARADISE framework: dialogue efficiency, dialogue quality, and task success. The dialogue efficiency measures concentrated on the timing of the interaction: the time taken to complete the three construction tasks, the number of system turns required for the complete interaction, and the mean time taken by the system to respond to the user’s requests. We considered four measures of dialogue quality. The first two measures looked specifically for signs of problems in the interaction, using data au881 tomatically</context>
<context position="24471" citStr="Litman and Pan (2002)" startWordPosition="4119" endWordPosition="4122"> 0.06, Rep: p ,,zt� 0.08 Overall 3.73 − 0.36 * N (Rep) + 0.31 * N (Turns) 0.062 Rep: p &lt; 0.05, Turns: p ,,zt� 0.06 Emotion change 0.07 +0.14 * N (Turns) +0.11 * N (Mem) − 0.090 * N (Rep) 0.20 Turns: p &lt; 0.05, Mem: p &lt; 0.01, Rep: p,,zt�0.17 Table 6: Predictor functions peated, this is a clear indication of a problem in the dialogue; similarly, users who remembered the system’s instructions were equally clearly having a relatively successful interaction. In the current study, increased dialogue length had a positive contribution to user satisfaction; this contrasts with results such as those of Litman and Pan (2002), who found that increased dialogue length was associated with decreased user satisfaction. We propose two possible explanations for this difference. First, the system analysed by Litman and Pan (2002) was an information-seeking dialogue system, in which efficient access to the information is an important criterion. The current system, on the other hand, has the goal of joint task execution, and pure efficiency is a less compelling measure of dialogue quality in this setting. Second, it is possible that the sheer novelty factor of interacting with a fully-embodied humanoid robot affected peopl</context>
<context position="26172" citStr="Litman and Pan (2002)" startWordPosition="4390" endWordPosition="4393">ects in the study described by Sidner et al. (2005) who used a robot that moved while talking reported higher levels of engagement in the interaction, and also tended to have longer conversations with the robot. While the predictor functions give useful insights into the relative contribution of the objective measures to the subjective user satisfaction, the R2 values are generally lower than those found in other PARADISE-style evaluations. For example, Walker et al. (1998) reported an R2 value of 0.38, the values reported by Walker et al. (2000) on the training sets ranged from 0.39 to 0.56, Litman and Pan (2002) reported an R2 value of 0.71, while the R2 values reported by M¨oller et al. (2008) for linear regression models similar to those presented here were between 0.22 and 0.57. The low R2 values from this analysis clearly suggest that, while the factors included in Table 6 did affect users’ opinions—particularly their opinion of the robot as a partner and the change in their reported emotional state—the users’ subjective judgements were also affected by factors other than those captured by the objective measures considered here. In most of the previous PARADISE-style studies, measures addressing </context>
</contexts>
<marker>Litman, Pan, 2002</marker>
<rawString>D. J. Litman and S. Pan. 2002. Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction, 12(2–3):111–137. doi:10.1023/A: 1015036910358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨oller</author>
<author>K-P Engelbrecht</author>
<author>R Schleicher</author>
</authors>
<title>Predicting the quality and usability of spoken dialogue systems.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<pages>2008--03</pages>
<marker>M¨oller, Engelbrecht, Schleicher, 2008</marker>
<rawString>S. M¨oller, K.-P. Engelbrecht, and R. Schleicher. 2008. Predicting the quality and usability of spoken dialogue systems. Speech Communication, 50:730–744. doi:10.1016/j.specom. 2008.03.001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Novick</author>
</authors>
<title>What is effectiveness?</title>
<date>1997</date>
<booktitle>In Working notes, CHI ’97 Workshop on HCI Research and Practice Agenda Based on Human Needs and Social Responsibility.</booktitle>
<note>http://www.cs.utep.edu/novick/ papers/eff.chi.html.</note>
<contexts>
<context position="1542" citStr="Novick, 1997" startWordPosition="243" endWordPosition="244">a performance function from our data. The major contributors to user satisfaction were the number of repetition requests (which had a negative effect on satisfaction), the dialogue length, and the users’ recall of the system instructions (both of which contributed positively). 1 Introduction Evaluating the usability of a spoken language dialogue system generally requires a large-scale user study, which can be a time-consuming process both for the experimenters and for the experimental subjects. In fact, it can be difficult even to define what the criteria are for evaluating such a system (cf. Novick, 1997). In recent years, techniques have been introduced that are designed to predict user satisfaction based on more easily measured properties of an interaction such as dialogue length and speech-recognition error rate. The design of such performance methods for evaluating dialogue systems is still an area of open research. The PARADISE framework (PARAdigm for DIalogue System Evaluation; Walker et al. (1997)) describes a method for using data to derive a performance function that predicts user-satisfaction scores from the results on other, more easily computed measures. PARADISE uses stepwise mult</context>
</contexts>
<marker>Novick, 1997</marker>
<rawString>D. G. Novick. 1997. What is effectiveness? In Working notes, CHI ’97 Workshop on HCI Research and Practice Agenda Based on Human Needs and Social Responsibility. http://www.cs.utep.edu/novick/ papers/eff.chi.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ortony</author>
<author>G L Clore</author>
<author>A Collins</author>
</authors>
<title>The Cognitive Structure of Emotions.</title>
<date>1988</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12222" citStr="Ortony et al., 1988" startWordPosition="2037" endWordPosition="2040"> assembled as intended (i.e., as in Figure 3), which was judged based on the video recordings. To test whether the subjects had learned how to build the sub-components that were required more than once (the snowman and the L shape), we recorded whether they said yes or no when they were asked if they remembered each of these components during the construction of the railway signal. 3.3.2 Subjective Measures In addition to the above objective measures, we also gathered a range of subjective measures. Before the interaction, we asked subjects to rate their current level on a set of 22 emotions (Ortony et al., 1988) on a scale from 1 to 4; the subjects then rated their level on the same emotional scales again after the interaction. After the interaction, the subjects also filled out a user-satisfaction questionnaire, which was based on that used in the user evaluation of the COMIC dialogue system (White et al., 2005), with modifications to address specific aspects of the current dialogue system and the experimental manipulations in this study. There were 47 items in total, each of which requested that the user choose their level of agreement with a given statement on a five-point Likert scale. The items </context>
</contexts>
<marker>Ortony, Clore, Collins, 1988</marker>
<rawString>A. Ortony, G. L. Clore, and A. Collins. 1988. The Cognitive Structure of Emotions. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<journal>ACL Anthology</journal>
<booktitle>In Proceedings of ACL</booktitle>
<pages>02--1040</pages>
<contexts>
<context position="3245" citStr="Papineni et al., 2002" startWordPosition="511" endWordPosition="514">ing it possible to evaluate systems based on automatically available data from log files without the need for extensive experiments with users, for example, such a performance function can be used in an online, incremental manner to adapt system behaviour to avoid entering a state that is likely to reduce user satisfaction, or can be used as a reward function in a reinforcement-learning scenario (Walker, 2000). Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU (Papineni et al., 2002) for machine translation and ROUGE (Lin, 2004) for summarisation, for example. When employing any such metric, it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output. If not, the risk arises that the automated measures do not capture the behaviour that is actually relevant for the human users of a system. For example, Callison-Burch et al. (2006) presented a number of 879 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879–887, Suntec, Singapore, 2-7 Augu</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL 2002. ACL Anthology P02-1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rickert</author>
<author>M E Foster</author>
<author>M Giuliani</author>
<author>T By</author>
<author>G Panin</author>
<author>A Knoll</author>
</authors>
<title>Integrating language, vision and action for human robot dialog systems.</title>
<date>2007</date>
<booktitle>In Proceedings of HCI International</booktitle>
<pages>10--1007</pages>
<contexts>
<context position="5212" citStr="Rickert et al., 2007" startWordPosition="840" endWordPosition="843">dialogues, the users’ ability to recall the instructions given by the robot, and the number of times that the user had to ask for instructions to be repeated. The former two measures were positively correlated with user satisfaction, while the latter had a negative impact on user satisfaction; however the correlation in all cases was relatively low. At the end of the paper, we discuss possible reasons for these results and propose other measures that might have a larger effect on users’ judgements. 2 Task-Based Human-Robot Dialogue This study makes use of the JAST human-robot dialogue system (Rickert et al., 2007) which supports multimodal human-robot collaboration on a joint construction task. The user and the robot work together to assemble wooden construction toys on a common workspace, coordinating their actions through speech, gestures, and facial displays. The robot (Figure 1) consists of a pair of manipulator arms with grippers, mounted in a position to resemble human arms, and an animatronic talking head (van Breemen, 2005) capable of producing facial expressions, rigid head motion, and lip-synchronised synthesised speech. The system can interact in English or German. The robot is able to manip</context>
</contexts>
<marker>Rickert, Foster, Giuliani, By, Panin, Knoll, 2007</marker>
<rawString>M. Rickert, M. E. Foster, M. Giuliani, T. By, G. Panin, and A. Knoll. 2007. Integrating language, vision and action for human robot dialog systems. In Proceedings of HCI International 2007. doi:10.1007/978-3-540-73281-5_ 108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
<author>C Lee</author>
<author>C D Kidd</author>
<author>N Lesh</author>
<author>C Rich</author>
</authors>
<title>Explorations in engagement for humans and robots.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>166</volume>
<issue>1</issue>
<pages>10--1016</pages>
<contexts>
<context position="25602" citStr="Sidner et al. (2005)" startWordPosition="4297" endWordPosition="4300">e sheer novelty factor of interacting with a fully-embodied humanoid robot affected people’s subjective responses to the system, so that subjects who had longer interactions also enjoyed the experience more. Support for this explanation is provided by the fact that dialogue length was only a significant factor in the more ‘subjective’ parts of the questionnaire, but did not have a significant impact on the users’ judgements about instruction quality or task success. Other studies of human-robot dialogue systems have also had similar results: for example, the subjects in the study described by Sidner et al. (2005) who used a robot that moved while talking reported higher levels of engagement in the interaction, and also tended to have longer conversations with the robot. While the predictor functions give useful insights into the relative contribution of the objective measures to the subjective user satisfaction, the R2 values are generally lower than those found in other PARADISE-style evaluations. For example, Walker et al. (1998) reported an R2 value of 0.38, the values reported by Walker et al. (2000) on the training sets ranged from 0.39 to 0.56, Litman and Pan (2002) reported an R2 value of 0.71,</context>
</contexts>
<marker>Sidner, Lee, Kidd, Lesh, Rich, 2005</marker>
<rawString>C. L. Sidner, C. Lee, C. D. Kidd, N. Lesh, and C. Rich. 2005. Explorations in engagement for humans and robots. Artificial Intelligence, 166(1–2):140–164. doi:10.1016/j.artint. 2005.03.005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>C Kamm</author>
<author>D Litman</author>
</authors>
<title>Towards developing general models of usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<pages>6--3</pages>
<contexts>
<context position="2384" citStr="Walker et al., 2000" startWordPosition="371" endWordPosition="374">ign of such performance methods for evaluating dialogue systems is still an area of open research. The PARADISE framework (PARAdigm for DIalogue System Evaluation; Walker et al. (1997)) describes a method for using data to derive a performance function that predicts user-satisfaction scores from the results on other, more easily computed measures. PARADISE uses stepwise multiple linear regression to model user satisfaction based on measures representing the performance dimensions of task success, dialogue quality, and dialogue efficiency, and has been applied to a wide range of systems (e.g., Walker et al., 2000; Litman and Pan, 2002; M¨oller et al., 2008). If the resulting performance function can be shown to predict user satisfaction as a function of other, more easily measured system properties, it will be widely applicable: in addition to making it possible to evaluate systems based on automatically available data from log files without the need for extensive experiments with users, for example, such a performance function can be used in an online, incremental manner to adapt system behaviour to avoid entering a state that is likely to reduce user satisfaction, or can be used as a reward function</context>
<context position="26103" citStr="Walker et al. (2000)" startWordPosition="4377" endWordPosition="4380">ialogue systems have also had similar results: for example, the subjects in the study described by Sidner et al. (2005) who used a robot that moved while talking reported higher levels of engagement in the interaction, and also tended to have longer conversations with the robot. While the predictor functions give useful insights into the relative contribution of the objective measures to the subjective user satisfaction, the R2 values are generally lower than those found in other PARADISE-style evaluations. For example, Walker et al. (1998) reported an R2 value of 0.38, the values reported by Walker et al. (2000) on the training sets ranged from 0.39 to 0.56, Litman and Pan (2002) reported an R2 value of 0.71, while the R2 values reported by M¨oller et al. (2008) for linear regression models similar to those presented here were between 0.22 and 0.57. The low R2 values from this analysis clearly suggest that, while the factors included in Table 6 did affect users’ opinions—particularly their opinion of the robot as a partner and the change in their reported emotional state—the users’ subjective judgements were also affected by factors other than those captured by the objective measures considered here.</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>M. Walker, C. Kamm, and D. Litman. 2000. Towards developing general models of usability with PARADISE. Natural Language Engineering, 6(3–4):363–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
</authors>
<title>An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email.</title>
<date>2000</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>12--387</pages>
<contexts>
<context position="3036" citStr="Walker, 2000" startWordPosition="482" endWordPosition="483"> 2008). If the resulting performance function can be shown to predict user satisfaction as a function of other, more easily measured system properties, it will be widely applicable: in addition to making it possible to evaluate systems based on automatically available data from log files without the need for extensive experiments with users, for example, such a performance function can be used in an online, incremental manner to adapt system behaviour to avoid entering a state that is likely to reduce user satisfaction, or can be used as a reward function in a reinforcement-learning scenario (Walker, 2000). Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU (Papineni et al., 2002) for machine translation and ROUGE (Lin, 2004) for summarisation, for example. When employing any such metric, it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output. If not, the risk arises that the automated measures do not capture the behaviour that is actually relevant for the human use</context>
</contexts>
<marker>Walker, 2000</marker>
<rawString>M. A. Walker. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. Journal of Artificial Intelligence Research, 12:387–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>J Fromer</author>
<author>G D Fabbrizio</author>
<author>C Mestel</author>
<author>D Hindle</author>
</authors>
<title>What can I say?: Evaluating a spoken language interface to email.</title>
<date>1998</date>
<booktitle>In Proceedings of CHI</booktitle>
<pages>10--1145</pages>
<contexts>
<context position="26029" citStr="Walker et al. (1998)" startWordPosition="4363" endWordPosition="4366"> about instruction quality or task success. Other studies of human-robot dialogue systems have also had similar results: for example, the subjects in the study described by Sidner et al. (2005) who used a robot that moved while talking reported higher levels of engagement in the interaction, and also tended to have longer conversations with the robot. While the predictor functions give useful insights into the relative contribution of the objective measures to the subjective user satisfaction, the R2 values are generally lower than those found in other PARADISE-style evaluations. For example, Walker et al. (1998) reported an R2 value of 0.38, the values reported by Walker et al. (2000) on the training sets ranged from 0.39 to 0.56, Litman and Pan (2002) reported an R2 value of 0.71, while the R2 values reported by M¨oller et al. (2008) for linear regression models similar to those presented here were between 0.22 and 0.57. The low R2 values from this analysis clearly suggest that, while the factors included in Table 6 did affect users’ opinions—particularly their opinion of the robot as a partner and the change in their reported emotional state—the users’ subjective judgements were also affected by fa</context>
</contexts>
<marker>Walker, Fromer, Fabbrizio, Mestel, Hindle, 1998</marker>
<rawString>M. A. Walker, J. Fromer, G. D. Fabbrizio, C. Mestel, and D. Hindle. 1998. What can I say?: Evaluating a spoken language interface to email. In Proceedings of CHI 1998. doi:10.1145/ 274644.274722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>D J Litman</author>
<author>C A Kamm</author>
<author>A Abella</author>
</authors>
<title>PARADISE: A framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<journal>ACL Anthology</journal>
<booktitle>In Proceedings of ACL/EACL</booktitle>
<pages>97--1035</pages>
<contexts>
<context position="918" citStr="Walker et al., 1997" startWordPosition="140" endWordPosition="143">ll}@in.tum.de Abstract We present a human-robot dialogue system that enables a robot to work together with a human user to build wooden construction toys. We then describe a study in which naive subjects interacted with this system under a range of conditions and then completed a user-satisfaction questionnaire. The results of this study provide a wide range of subjective and objective measures of the quality of the interactions. To assess which aspects of the interaction had the greatest impact on the users’ opinions of the system, we used a method based on the PARADISE evaluation framework (Walker et al., 1997) to derive a performance function from our data. The major contributors to user satisfaction were the number of repetition requests (which had a negative effect on satisfaction), the dialogue length, and the users’ recall of the system instructions (both of which contributed positively). 1 Introduction Evaluating the usability of a spoken language dialogue system generally requires a large-scale user study, which can be a time-consuming process both for the experimenters and for the experimental subjects. In fact, it can be difficult even to define what the criteria are for evaluating such a s</context>
<context position="20792" citStr="Walker et al., 1997" startWordPosition="3486" endWordPosition="3489">ctive measures, and also examined the correlation among measures of the same type. The results on the objective measures varied widely across the subjects; also, the subjects generally rated their experience of using the system positively, but again with some variation. In this section, we examine the relationship among measures of different types in order to determine which of the objective measures had the largest effect on users’ subjective reactions to the dialogue system. To determine the relationship among the factors, we employed the procedure used in the PARADISE evaluation framework (Walker et al., 1997). The PARADISE model uses stepwise multiple linear regression to predict subjective user satisfaction based on measures representing the performance dimensions of task success, dialogue quality, and dialogue efficiency, resulting in a predictor function of the following form: n Satisfaction = � wi * -iV (mi) i=1 The mi terms represent the value of each measure, while the ✓&apos; function transforms each measure into a normal distribution using z-score normalisation. Stepwise linear regression produces coefficients (wi) describing the relative contribution of each predictor to the user satisfaction.</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>M. A. Walker, D. J. Litman, C. A. Kamm, and A. Abella. 1997. PARADISE: A framework for evaluating spoken dialogue agents. In Proceedings of ACL/EACL 1997. ACL Anthology P97-1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>M E Foster</author>
<author>J Oberlander</author>
<author>A Brown</author>
</authors>
<title>Using facial feedback to enhance turn-taking in a multimodal dialogue system.</title>
<date>2005</date>
<booktitle>In Proceedings of HCI International</booktitle>
<contexts>
<context position="12529" citStr="White et al., 2005" startWordPosition="2090" endWordPosition="2093">ered each of these components during the construction of the railway signal. 3.3.2 Subjective Measures In addition to the above objective measures, we also gathered a range of subjective measures. Before the interaction, we asked subjects to rate their current level on a set of 22 emotions (Ortony et al., 1988) on a scale from 1 to 4; the subjects then rated their level on the same emotional scales again after the interaction. After the interaction, the subjects also filled out a user-satisfaction questionnaire, which was based on that used in the user evaluation of the COMIC dialogue system (White et al., 2005), with modifications to address specific aspects of the current dialogue system and the experimental manipulations in this study. There were 47 items in total, each of which requested that the user choose their level of agreement with a given statement on a five-point Likert scale. The items were divided into the following categories: Mean (Stdev) Min Max Length (sec) 305.1 (54.0) 195.2 488.4 System turns 13.4 (1.73) 11 18 Response time (sec) 2.79 (1.13) 1.27 7.21 Table 1: Dialogue efficiency results Opinion of the robot as a partner 21 items addressing the ease with which subjects were able t</context>
</contexts>
<marker>White, Foster, Oberlander, Brown, 2005</marker>
<rawString>M. White, M. E. Foster, J. Oberlander, and A. Brown. 2005. Using facial feedback to enhance turn-taking in a multimodal dialogue system. In Proceedings of HCI International 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>