<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.995479">
Lattice-based System Combination for Statistical Machine Translation
</title>
<author confidence="0.997734">
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, Yajuan L¨u
</author>
<affiliation confidence="0.985894">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.759128">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.991938">
{fengyang, yliu, htmi, liuqun, lvyajuan}@ict.ac.cn
</email>
<sectionHeader confidence="0.994619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.927446">
Current system combination methods usu-
ally use confusion networks to find consensus
translations among different systems. Requir-
ing one-to-one mappings between the words
in candidate translations, confusion networks
have difficulty in handling more general situa-
tions in which several words are connected to
another several words. Instead, we propose a
lattice-based system combination model that
allows for such phrase alignments and uses
lattices to encode all candidate translations.
Experiments show that our approach achieves
significant improvements over the state-of-
the-art baseline system on Chinese-to-English
translation test sets.
</bodyText>
<sectionHeader confidence="0.998862" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99863588">
System combination aims to find consensus transla-
tions among different machine translation systems.
It has been proven that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
In recent several years, the system combination
methods based on confusion networks developed
rapidly (Bangalore et al., 2001; Matusov et al., 2006;
Sim et al., 2007; Rosti et al., 2007a; Rosti et al.,
2007b; Rosti et al., 2008; He et al., 2008), which
show state-of-the-art performance in benchmarks. A
confusion network consists of a sequence of sets of
candidate words. Each candidate word is associated
with a score. The optimal consensus translation can
be obtained by selecting one word from each set to
maximizing the overall score.
To construct a confusion network, one first need
to choose one of the hypotheses (i.e., candidate
translations) as the backbone (also called “skeleton”
in the literature) and then decide the word align-
ments of other hypotheses to the backbone. Hy-
pothesis alignment plays a crucial role in confusion-
network-based system combination because it has a
direct effect on selecting consensus translations.
However, a confusion network is restricted in
such a way that only 1-to-1 mappings are allowed
in hypothesis alignment. This is not the fact even
for word alignments between the same languages. It
is more common that several words are connected
to another several words. For example, “be capa-
ble of” and “be able to” have the same meaning.
Although confusion-network-based approaches re-
sort to inserting null words to alleviate this problem,
they face the risk of producing degenerate transla-
tions such as “be capable to” and “be able of”.
In this paper, we propose a new system combina-
tion method based on lattices. As a more general
form of confusion network, a lattice is capable of
describing arbitrary mappings in hypothesis align-
ment. In a lattice, each edge is associated with a
sequence of words rather than a single word. There-
fore, we select phrases instead of words in each
candidate set and minimize the chance to produce
unexpected translations such as “be capable to”.
We compared our approach with the state-of-the-art
confusion-network-based system (He et al., 2008)
and achieved a significant absolute improvement of
1.23 BLEU points on the NIST 2005 Chinese-to-
English test set and 0.93 BLEU point on the NIST
2008 Chinese-to-English test set.
</bodyText>
<page confidence="0.951924">
1105
</page>
<note confidence="0.892320833333333">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1105–1113,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
</note>
<figure confidence="0.970013923076923">
(a) unidirectional alignments
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(b) bidirectional alignments
ε prefer of
He feels like ε apples
is fond
(c) confusion network
ε prefer
is fond of
(d) lattice
</figure>
<figureCaption confidence="0.9895925">
Figure 1: Comparison of a confusion network and a lat-
tice.
</figureCaption>
<sectionHeader confidence="0.684331" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.824148">
2.1 Confusion Network and Lattice
</subsectionHeader>
<bodyText confidence="0.997262277777778">
We use an example shown in Figure 1 to illustrate
our idea. Suppose that there are three hypotheses:
he feels like apples
lations. Note that the phrase “is fond of” is attached
to an edge. Now, it is unlikely to obtain a translation
like “He is like of apples”.
A lattice G = hV, Ei is a directed acyclic graph,
formally a weighted finite state automation (FSA),
where V is the set of nodes and E is the set of edges.
The nodes in a lattice are usually labeled according
to an appropriate numbering to reflect how to pro-
duce a translation. Each edge in a lattice is attached
with a sequence of words as well as the associated
probability.
As lattice is a more general form of confusion
network (Dyer et al., 2008), we expect that replac-
ing confusion networks with lattices will further im-
prove system combination.
</bodyText>
<subsectionHeader confidence="0.993462">
2.2 IHMM-based Alignment Method
</subsectionHeader>
<bodyText confidence="0.992507864864865">
Since the candidate hypotheses are aligned us-
ing Indirect-HMM-based (IHMM-based) alignment
method (He et al., 2008) in both direction, we briefly
review the IHMM-based alignment method first.
Take the direction that the hypothesis is aligned to
the backbone as an example. The conditional prob-
ability that the hypothesis is generated by the back-
bone is given by
He feels like apples � J [p(aj|aj−1, I)p(e′j|eaj)]l (1)
He prefer apples p(e′ J|eI 1) = j=1
He is fond of apples 1
ai
We choose the first sentence as the backbone.
Then, we perform hypothesis alignment to build a
confusion network, as shown in Figure 1(a). Note
that although “feels like” has the same meaning with
“is fond of”, a confusion network only allows for
one-to-one mappings. In the confusion network
shown in Figure 1(c), several null words ε are in-
serted to ensure that each hypothesis has the same
length. As each edge in the confusion network only
has a single word, it is possible to produce inappro-
priate translations such as “He is like of apples”.
In contrast, we allow many-to-many mappings
in the hypothesis alignment shown in Figure 2(b).
For example, “like” is aligned to three words: “is”,
“fond”, and “of”. Then, we use a lattice shown in
Figure 1(d) to represent all possible candidate trans-
Where eI1 = (e1,..., eI) is the backbone, e′J1 =
(e′1, ..., e′J) is a hypothesis aligned to eI1, and aJ1 =
(a1,.., aJ) is the alignment that specifies the posi-
tion of backbone word that each hypothesis word is
aligned to.
The translation probability p(e′j|ei) is a linear in-
terpolation of semantic similarity psem(e′j|ei) and
surface similarity psur(e′j|ei) and α is the interpo-
lation factor:
</bodyText>
<equation confidence="0.999161">
p(e′j|ei) = α·psem(e′j|ei)+(1−α)·psur(e′j|ei) (2)
</equation>
<bodyText confidence="0.998779">
The semantic similarity model is derived by using
the source word sequence as a hidden layer, so the
bilingual dictionary is necessary. The semantic sim-
</bodyText>
<page confidence="0.792087">
1106
</page>
<equation confidence="0.992938">
p(fk|ei)p(e′j|fk, ei)
(3)
p(fk|ei)p(e′j|fk)
</equation>
<bodyText confidence="0.9789185">
The surface similarity model is estimated by calcu-
lating the literal matching rate:
prefix (LMP) and ρ is a smoothing factor that speci-
fies the mapping.
The distortion probability p(aj = i|aj−1 = i′, I)
is estimated by only considering the jump distance:
</bodyText>
<equation confidence="0.998328333333333">
p(i|i′, I) = c(i − i′)
I (6)
Ei=1 c(l − i′)
</equation>
<bodyText confidence="0.9933465">
The distortion parameters c(d) are grouped into 11
buckets, c(≤ −4), c(−3), ..., c(0), ..., c(5), c(≥ 6).
Since the alignments are in the same language, the
distortion model favor monotonic alignments and
penalize non-monotonic alignments. It is given in
a intuitive way
</bodyText>
<equation confidence="0.999614">
c(d) = (1 + |d − 1|)−K, d = −4,..., 6 (7)
</equation>
<bodyText confidence="0.9946285">
where K is tuned on held-out data.
Also the probability p0 of jumping to a null word
state is tuned on held-out data. So the overall distor-
tion model becomes
</bodyText>
<equation confidence="0.9923645">
�
p0 if i = null state
p(i|i′, I) =
(1 − p0) · p(i|i′, I) otherwise
</equation>
<sectionHeader confidence="0.970321" genericHeader="method">
3 Lattice-based System Combination
Model
</sectionHeader>
<listItem confidence="0.7727852">
Lattice-based system combination involves the fol-
lowing steps:
(1) Collect the hypotheses from the candidate sys-
tems.
(2) Choose the backbone from the hypotheses.
</listItem>
<bodyText confidence="0.774436">
This is performed using a sentence-level Minimum
Bayes Risk (MBR) method. The hypothesis with the
minimum cost of edits against all hypotheses is se-
lected. The backbone is significant for it influences
not only the word order, but also the following align-
ments. The backbone is selected as follows:
</bodyText>
<equation confidence="0.942321666666667">
1:
EB = argmin TER(E′, E) (8)
E′∈E E∈E
</equation>
<bodyText confidence="0.92290316">
(3) Get the alignments of the backbone and hy-
pothesis pairs. First, each pair is aligned in both di-
rections using the IHMM-based alignment method.
In the IHMM alignment model, bilingual dictionar-
ies in both directions are indispensable. Then, we
apply a grow-diag-final algorithm which is widely
used in bilingual phrase extraction (Koehn et al.,
2003) to monolingual alignments. The bidirec-
tional alignments are combined to one resorting to
the grow-diag-final algorithm, allowing n-to-n map-
pings.
(4)Normalize the alignment pairs. The word or-
der of the backbone determines the word order of
consensus outputs, so the word order of hypotheses
must be consistent with that of the backbone. All
words of a hypotheses are reordered according to
the alignment to the backbone. For a word aligned
to null, an actual null word may be inserted to the
proper position. The alignment units are extracted
first and then the hypothesis words in each unit are
shifted as a whole.
(5) Construct the lattice in the light of phrase
pairs extracted on the normalized alignment pairs.
The expression ability of the lattice depends on the
phrase pairs.
</bodyText>
<listItem confidence="0.9058765">
(6) Decode the lattice using a model similar to the
log-linear model.
</listItem>
<bodyText confidence="0.9999933">
The confusion-network-based system combina-
tion model goes in a similar way. The first two steps
are the same as the lattice-based model. The differ-
ence is that the hypothesis pairs are aligned just in
one direction due to the expression limit of the con-
fusion network. As a result, the normalized align-
ments only contain 1-to-1 mappings (Actual null
words are also needed in the case of null alignment).
In the following, we will give more details about the
steps which are different in the two models.
</bodyText>
<equation confidence="0.9072096875">
ilarity model is given by
psem(e′j|ei) = K
≈ k=0
K
k=0
psur(e′j|ei) = exp{ρ · [s(e′j, ei) − 1]} (4)
where s(e′j, ei) is given by
M(e′j, ei)
s(e′j, ei) = (5)
max(|e′ j|, |ei|)
where M(e′j, ei) is the length of the longest matched
1107
EB: e1 e2 e3
Eh : e′1 e′2 (a)
e′2 ε e′1
e1 e2 e3
</equation>
<sectionHeader confidence="0.977196" genericHeader="method">
4 Lattice Construction
</sectionHeader>
<bodyText confidence="0.987762476190476">
Unlike a confusion network that operates words
only, a lattice allows for phrase pairs. So phrase
pairs must be extracted before constructing a lat-
tice. A major difficulty in extracting phrase pairs
is that the word order of hypotheses is not consistent
with that of the backbone. As a result, hypothesis
words belonging to a phrase pair may be discon-
tinuous. Before phrase pairs are extracted, the hy-
pothesis words should be normalized to make sure
the words in a phrase pair is continuous. We call a
phrase pair before normalization a alignment unit.
The problem mentioned above is shown in Fig-
ure 2. In Figure 2 (a), although (e′1e′3, e2) should be
a phrase pair, but &amp;quot;e′1&amp;quot; and &amp;quot;e3&amp;quot; are discontin-
uous, so the phrase pair can not be extracted. Only
after the words of the hypothesis are reordered ac-
cording to the corresponding words in the backbone
as shown in Figure 2 (b), &amp;quot;ei&amp;quot; and &amp;quot;e3&amp;quot; be-
come continuous and the phrase pair (e′1e′3, e2) can
be extracted. The procedure of reordering is called
alignment normalization
</bodyText>
<equation confidence="0.591811333333333">
Eh: e′1 e′2 e′3 Eh: e′2 e′1 e′3
EB : e1 e2 e3 EB : e1 e2 e3
(a) (b)
</equation>
<figureCaption confidence="0.996221">
Figure 2: An example of alignment units
</figureCaption>
<subsectionHeader confidence="0.99319">
4.1 Alignment Normalization
</subsectionHeader>
<bodyText confidence="0.9999589375">
After the final alignments are generated in the grow-
diag-final algorithm, minimum alignment units are
extracted. The hypothesis words of an alignment
unit are packed as a whole in shift operations.
See the example in Figure 2 (a) first. All mini-
mum alignment units are as follows: (e′2, e1), (e′1e′3,
e2) and (ε, e3). (e′1e′2e′3, e1e2) is an alignment unit,
but not a minimum alignment unit.
Let ¯ai = (¯e′i, ¯ei) denote a minimum alignment
unit, and assume that the word string ¯e′i covers words
e′i1 ,..., e′im on the hypothesis side, and the word
string ¯ei covers the consecutive words ei1 ,..., ein on
the backbone side. In an alignment unit, the word
string on the hypothesis side can be discontinuous.
The minimum unit ¯ai = (¯e′i, ¯ei) must observe the
following rules:
</bodyText>
<figureCaption confidence="0.987826">
Figure 3: Different cases of null insertion
</figureCaption>
<listItem confidence="0.9974585">
• b e′ik E ¯e′i, ea′ E ¯ei
ik
• b eik E ¯ei, e′aik = null or e′aik E ¯e′i
• ∄ ¯aj = (¯e′j, ¯ej), ¯ej = ei1, ..., eik or ¯ej =
</listItem>
<bodyText confidence="0.966033785714286">
eik, ..., ein, k E [1, n]
Where a′ik denotes the position of the word in the
backbone that e′ik is aligned to, and aik denotes the
position of the word in the hypothesis that eik is
aligned to.
An actual null word may be inserted to a proper
position if a word, either from the hypothesis or from
the backbone, is aligned to null. In this way, the
minimum alignment set is extended to an alignment
unit set, which includes not only minimum align-
ment units but also alignment units which are gener-
ated by adding null words to minimum alignment
units. In general, the following three conditions
should be taken into consideration:
</bodyText>
<listItem confidence="0.995462538461538">
• A backbone word is aligned to null. A null
word is inserted to the hypothesis as shown in
Figure 3 (a).
• A hypothesis word is aligned to null and it is
between the span of a minimum alignment unit.
A new alignment unit is generated by insert-
ing the hypothesis word aligned to null to the
minimum alignment unit. The new hypothesis
string must remain the original word order of
the hypothesis. It is illustrated in Figure 3 (b).
• A hypothesis word is aligned to null and it is
not between the hypothesis span of any mini-
mum alignment unit. In this case, a null word
</listItem>
<equation confidence="0.996924888888889">
EB: e1 e2
Eh : e′1 e′2 e′3
e′1e′2e′3
e1 e2
e′1e′3
EB: e1 e2
Eh : e′1 e′2 e′3
e′1 e′2 e′3
e1 ε e2
</equation>
<page confidence="0.989517">
1108
</page>
<figure confidence="0.9998544375">
(d)
(e)
¯e′ ¯e′ 5 ¯e′
4 6
(a)
e1 e2 ε e3
¯e′1 ¯e′ ¯e′
2 3
(b)
e1 ε e2 e3
¯e′1 ¯e′ ¯e′
2 3
¯e′
4
e1 ε e2 e3
(c)
</figure>
<figureCaption confidence="0.883495">
Figure 4: A toy instance of lattice construction
</figureCaption>
<figure confidence="0.999589384615385">
¯e′ ¯e′
1 2
e1 ε e2 ε e3
¯e′4
¯e′3
¯e′5
¯e′ ¯e′
1 2
¯e′3
e1 ε e2 ε e3
¯e′ ¯e′
5 6
¯e′4
</figure>
<figureCaption confidence="0.973611">
are inserted to the backbone. This is shown in
Figure 3 (c).
</figureCaption>
<subsectionHeader confidence="0.98316">
4.2 Lattice Construction Algorithm
</subsectionHeader>
<bodyText confidence="0.99401664">
The lattice is constructed by adding the normalized
alignment pairs incrementally. One backbone arc in
a lattice can only span one backbone word. In con-
trast, all hypothesis words in an alignment unit must
be packed into one hypothesis arc. First the lattice is
initialized with a normalized alignment pair. Then
given all other alignment pairs one by one, the lat-
tice is modified dynamically by adding the hypothe-
sis words of an alignment pair in a left-to-right fash-
ion.
A toy instance is given in Figure 4 to illustrate the
procedure of lattice construction. Assume the cur-
rent inputs are: an alignment pair as in Figure 4 (a),
and a lattice as in Figure 4 (b). The backbone words
of the alignment pair are compared to the backbone
words of the lattice one by one. The procedure is as
follows:
• e1 is compared with e1. Since they are the
same, the hypothesis arc ¯e4, which comes from
the same node with e1 in the alignment pair,
is compared with the hypothesis arc ¯ei, which
comes from the same node with e1 in the lat-
tice. The two hypothesis arcs are not the same,
so ¯e4 is added to the lattice as shown in Figure
4(c). Both go to the next backbone words.
</bodyText>
<listItem confidence="0.976611888888889">
• e2 is compared with ε. The lattice remains the
same. The lattice goes to the next backbone
word e2.
• e2 is compared with e2. There is no hypothesis
arc coming from the same node with the bone
arc e2 in the alignment pair, so the lattice re-
mains the same. Both go to the next backbone
words.
• ε is compared with e3. A null backbone arc is
</listItem>
<bodyText confidence="0.863045">
inserted into the lattice between e2 and e3. The
hypothesis arc ¯e5 is inserted to the lattice, too.
The modified lattice is shown in Figure 4(d).
The alignment pair goes to the next backbone
word e3.
</bodyText>
<listItem confidence="0.9943492">
• e3 is compared with e3. For they are the same
and there is no hypothesis arc ¯e6 in the lattice,
¯e6 is inserted to the lattice as in Figure 4(e).
• Both arrive at the end and it is the turn of the
next alignment pair.
</listItem>
<bodyText confidence="0.793857785714286">
When comparing a backbone word of the given
alignment pair with a backbone word of the lattice,
the following three cases should be handled:
• The current backbone word of the given align-
ment pair is a null word while the current back-
bone word of the lattice is not. A null back-
bone word is inserted to the lattice.
• The current backbone word of the lattice is a
null word while the current word of the given
alignment pair is not. The current null back-
bone word of the lattice is skipped with nothing
to do. The next backbone word of the lattice is
compared with the current backbone word of
the given alignment pair.
</bodyText>
<page confidence="0.993545">
1109
</page>
<figure confidence="0.46594125">
Algorithm 1 Lattice construction algorithm.
1: Input: alignmentpairs {pn}Nn=1
2: L ← p1
3: Unique(L)
</figure>
<listItem confidence="0.963446518518518">
4: for n ← 2 .. N do
5: pnode = pn · first
6: lnode = L · first
7: while pnode · barcnext =6 NULL do
8: if lnode · barcnext = NULL or pnode ·
bword = null and lnode · bword =6 null then
9: INSERTBARC(lnode, null)
10: pnode = pnode · barcnext
11: else
12: if pnode · bword =6 null and lnode ·
bword = null then
13: lnode = lnode · barcnext
14: else
15: for each harc of pnode do
16: if NotExist(lnode, pnode · harc)
then
17: INSERTHARC(lnode, pnode ·
harc)
18: pnode = pnode · barcnext
19: lnode = lnode · barcnext
20: Output: lattice L
• The current backbone words of the given align-
ment pair and the lattice are the same. Let
{harcl} denotes the set of hypothesis arcs,
which come from the same node with the cur-
rent backbone arc in the lattice, and harch de-
notes one of the corresponding hypothesis arcs
</listItem>
<bodyText confidence="0.981108458333333">
in the given alignment pair. In the {harcl},
if there is no arc which is the same with the
harch, a hypothesis arc projecting to harch is
added to the lattice.
The algorithm of constructing a lattice is illus-
trated in Algorithm 1. The backbone words of the
alignment pair and the lattice are processed one by
one in a left-to-right manner. Line 2 initializes the
lattice with the first alignment pair, and Line 3 re-
moves the hypothesis arc which contains the same
words with the backbone arc. barc denotes the back-
bone arc, storing one backbone word only, and harc
denotes the hypothesis arc, storing the hypothesis
words. For there may be many alignment units span
the same backbone word range, there may be more
than one harc coming from one node. Line 8 − 10
consider the condition 1 and function InsertBarc in
Line 9 inserts a null bone arc to the position right
before the current node. Line 12−13 deal with con-
dition 2 and jump to the next backbone word of the
lattice. Line 15−19 handle condition 3 and function
InsertHarc inserts to the lattice a harc with the same
hypothesis words and the same backbone word span
with the current hypothesis arc.
</bodyText>
<sectionHeader confidence="0.995013" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.999978322580645">
In confusion network decoding, a translation is gen-
erated by traveling all the nodes from left to right.
So a translation path contains all the nodes. While
in lattice decoding, a translation path may skip some
nodes as some hypothesis arcs may cross more than
one backbone arc.
Similar to the features in Rosti et al. (2007a), the
features adopted by lattice-based model are arc pos-
terior probability, language model probability, the
number of null arcs, the number of hypothesis arcs
possessing more than one non-null word and the
number of all non-null words. The features are com-
bined in a log-linear model with the arc posterior
probabilities being processed specially as follows:
where f denotes the source sentence, e denotes a
translation generated by the lattice-based system,
Narc is the number of arcs the path of e covers,
Ns is the number of candidate systems and λs is the
weight of system s. C is the language model weight
and L(e) is the LM log-probability. Nnullarcs(e) is
the number of the arcs which only contain a null
word, and Nlongarc(e) is the number of the arcs
which store more than one non-null word. The
above two numbers are gotten by counting both
backbone arcs and hypothesis arcs. α and Q are the
corresponding weights of the numbers, respectively.
Nword(e) is the non-null word number and γ is its
weight.
Each arc has different confidences concerned with
different systems, and the confidence of system s
is denoted by ps(arc). ps(arc) is increased by
</bodyText>
<equation confidence="0.9963239">
logp(e/f) =
+ CL(e) + αNnullarc(e)
+ QNlongarc(e) + γNword(e)
Narc
i=1
log (
�Ns
s=1
λsps(arc))
(9)
</equation>
<page confidence="0.867233">
1110
</page>
<bodyText confidence="0.990558">
1/(k+ 1) if the hypothesis ranking k in the system s
contains the arc (Rosti et al., 2007a; He et al., 2008).
Cube pruning algorithm with beam search is em-
ployed to search for the consensus output (Huang
and Chiang, 2005). The nodes in the lattice are
searched in a topological order and each node re-
tains a list of N best candidate partial translations.
</bodyText>
<sectionHeader confidence="0.998672" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99998452">
The candidate systems participating in the system
combination are as listed in Table 1: System A is a
BTG-based system using a MaxEnt-based reorder-
ing model; System B is a hierarchical phrase-based
system; System C is the Moses decoder (Koehn et
al., 2007); System D is a syntax-based system. 10-
best hypotheses from each candidate system on the
dev and test sets were collected as the input of the
system combination.
In our experiments, the weights were all tuned on
the NIST MT02 Chinese-to-English test set, includ-
ing 878 sentences, and the test data was the NIST
MT05 Chinese-to-English test set, including 1082
sentences, except the experiments in Table 2. A 5-
gram language model was used which was trained
on the XinHua portion of Gigaword corpus. The re-
sults were all reported in case sensitive BLEU score
and the weights were tuned in Powell’s method to
maximum BLEU score. The IHMM-based align-
ment module was implemented according to He et
al. (2008), He (2007) and Vogel et al. (1996). In all
experiments, the parameters for IHMM-based align-
ment module were set to: the smoothing factor for
the surface similarity model, p = 3; the controlling
factor for the distortion model, K = 2.
</bodyText>
<subsectionHeader confidence="0.902052">
6.1 Comparison with
Confusion-network-based model
</subsectionHeader>
<bodyText confidence="0.999597666666667">
In order to compare the lattice-based system with
the confusion-network-based system fairly, we used
IHMM-based system combination model on behalf
of the confusion-network-based model described in
He et al. (2008). In both lattice-based and IHMM-
based systems, the bilingual dictionaries were ex-
tracted on the FBIS data set which included 289K
sentence pairs. The interpolation factor of the simi-
larity model was set to α = 0.1.
The results are shown in Table 1. IHMM stands
for the IHMM-based model and Lattice stands for
the lattice-based model. On the dev set, the lattice-
based system was 3.92 BLEU points higher than the
best single system and 0.36 BLEU point higher than
the IHMM-based system. On the test set, the lattice-
based system got an absolute improvement by 3.73
BLEU points over the best single system and 1.23
BLEU points over the IHMM-based system.
</bodyText>
<table confidence="0.999715125">
System MT02 MT05
BLEU% BLEU%
SystemA 31.93 30.68
SystemB 32.16 32.07
SystemC 32.09 31.64
SystemD 33.37 31.26
IHMM 36.93 34.57
Lattice 37.29 35.80
</table>
<tableCaption confidence="0.999973">
Table 1: Results on the MT02 and MT05 test sets
</tableCaption>
<bodyText confidence="0.998723333333333">
The results on another test sets are reported in Ta-
ble 2. The parameters were tuned on the newswire
part of NIST MT06 Chinese-to-English test set, in-
cluding 616 sentences, and the test set was NIST
MT08 Chinese-to-English test set, including 1357
sentences. The BLEU score of the lattice-based sys-
tem is 0.93 BLEU point higher than the IHMM-
based system and 3.0 BLEU points higher than the
best single system.
</bodyText>
<table confidence="0.999366625">
System MT06 MT08
BLEU% BLEU%
SystemA 32.51 25.63
SystemB 31.43 26.32
SystemC 31.50 23.43
SystemD 32.41 26.28
IHMM 36.05 28.39
Lattice 36.53 29.32
</table>
<tableCaption confidence="0.99981">
Table 2: Results on the MT06 and MT08 test sets
</tableCaption>
<bodyText confidence="0.999996375">
We take a real example from the output of the
two systems (in Table 3) to show that higher BLEU
scores correspond to better alignments and better
translations. The translation of System C is selected
as the backbone. From Table 3, we can see that
because of 1-to-1 mappings, “Russia” is aligned to
“Russian” and “’s” to “null” in the IHMM-based
model, which leads to the error translation “Russian
</bodyText>
<page confidence="0.985486">
1111
</page>
<table confidence="0.706367047619048">
Source: ���I Z;d7 JE i# a
SystemA: Russia merger of state-owned oil company and the state-run gas company in Russia
SystemB: Russia ’s state-owned oil company is working with Russia ’s state-run gas company mergers
SystemC: Russian state-run oil company is combined with the Russian state-run gas company
SystemD: Russia ’s state-owned oil companies are combined with Russia ’s state-run gas company
IHMM: Russian ’s state-owned oil company working with Russia ’s state-run gas company
Lattice: Russia ’s state-owned oil company is combined with the Russian state-run gas company
Table 3: A real translation example
’s”. Instead, “Russia ’s” is together aligned to ”Rus-
sian” in the lattice-based model. Also due to 1-to-
1 mappings, null word aligned to “is” is inserted.
As a result, “is” is missed in the output of IHMM-
based model. In contrast, in the lattice-based sys-
tem, “is working with” are aligned to “is combined
with”, forming a phrase pair.
MT02 MT05
BLEU% BLEU%
30k 36.94 35.14
60k 37.09 35.17
289k 37.29 35.80
2500k 37.14 35.62
</table>
<subsectionHeader confidence="0.998959">
6.2 Effect of Dictionary Scale
</subsectionHeader>
<bodyText confidence="0.999987769230769">
The dictionary is important to the semantic similar-
ity model in IHMM-based alignment method. We
evaluated the effect of the dictionary scale by using
dictionaries extracted on different data sets. The dic-
tionaries were respectively extracted on similar data
sets: 30K sentence pairs, 60K sentence pairs, 289K
sentence pairs (FBIS corpus) and 2500K sentence
pairs. The results are illustrated in Table 4. In or-
der to demonstrate the effect of the dictionary size
clearly, the interpolation factor of similarity model
was all set to α = 0.1.
From Table 4, we can see that when the cor-
pus size rise from 30k to 60k, the improvements
were not obvious both on the dev set and on the
test set. As the corpus was expanded to 289K, al-
though on the dev set, the result was only 0.2 BLEU
point higher, on the test set, it was 0.63 BLEU point
higher. As the corpus size was up to 2500K, the
BLEU scores both on the dev and test sets declined.
The reason is that, on one hand, there are more noise
on the 2500K sentence pairs; on the other hand, the
289K sentence pairs cover most of the words appear-
ing on the test set. So we can conclude that in or-
der to get better results, the dictionary scale must be
up to some certain scale. If the dictionary is much
smaller, the result will be impacted dramatically.
</bodyText>
<tableCaption confidence="0.994425">
Table 4: Effect of dictionary scale
</tableCaption>
<subsectionHeader confidence="0.998757">
6.3 Effect of Semantic Alignments
</subsectionHeader>
<bodyText confidence="0.999978642857143">
For the IHMM-based alignment method, the transla-
tion probability of an English word pair is computed
using a linear interpolation of the semantic similar-
ity and the surface similarity. So the two similarity
models decide the translation probability together
and the proportion is controlled by the interpolation
factor. We evaluated the effect of the two similarity
models by varying the interpolation factor α.
We used the dictionaries extracted on the FBIS
data set. The result is shown in Table 5. We got the
best result with α = 0.1. When we excluded the
semantic similarity model (α = 0.0) or excluded the
surface similarity model (α = 1.0), the performance
became worse.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999565">
The alignment model plays an important role in
system combination. Because of the expression
limitation of confusion networks, only 1-to-1 map-
pings are employed in the confusion-network-based
model. This paper proposes a lattice-based system
combination model. As a general form of confusion
networks, lattices can express n-to-n mappings. So
a lattice-based model processes phrase pairs while
</bodyText>
<page confidence="0.979369">
1112
</page>
<table confidence="0.7078551">
MT02 MT05
BLEU% BLEU%
1.0 36.41 34.92
0.7 37.21 35.65
0.5 36.43 35.02
0.4 37.14 35.55
0.3 36.75 35.66
0.2 36.81 35.55
0.1 37.29 35.80
0.0 36.45 35.14
</table>
<tableCaption confidence="0.991739">
Table 5: Effect of semantic alignments
</tableCaption>
<bodyText confidence="0.99974">
a confusion-network-based model processes words
only. As a result, phrase pairs must be extracted be-
fore constructing a lattice.
On NIST MT05 test set, the lattice-based sys-
tem gave better results with an absolute improve-
ment of 1.23 BLEU points over the confusion-
network-based system (He et al., 2008) and 3.73
BLEU points over the best single system. On
NIST MT08 test set, the lattice-based system out-
performed the confusion-network-based system by
0.93 BLEU point and outperformed the best single
system by 3.0 BLEU points.
</bodyText>
<sectionHeader confidence="0.997387" genericHeader="acknowledgments">
8 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999920875">
The authors were supported by National Natural Sci-
ence Foundation of China Contract 60736014, Na-
tional Natural Science Foundation of China Con-
tract 60873167 and High Technology R&amp;D Program
Project No. 2006AA010108. Thank Wenbin Jiang,
Tian Xia and Shu Cai for their help. We are also
grateful to the anonymous reviewers for their valu-
able comments.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917580645161">
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. of
IEEE ASRU, pages 351–354.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings ofACL/HLT 2008, pages 1012–1020, Colum-
bus, Ohio, June.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP, pages
95–100.
Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for computing outputs from ma-
chine translation systems. In Proc. of EMNLP, pages
98–107.
Xiaodong He. 2007. Using word-dependent translation
models in hmm based word alignment for statistical
machine translation. In Proc. of COLING-ACL, pages
961–968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages 53–
64.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstration
Session.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. ofIEEE EACL, pages 33–40.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. of ACL,
pages 312–319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple ma-
chine translation systems. In Proc. of NAACL-HLT,
pages 228–235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translaiton system combination. In
Proc. of the Third ACL WorkShop on Statistical Ma-
chine Translation, pages 183–186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ofICASSP, pages
105–108.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proc. of COLING, pages 836–841.
</reference>
<equation confidence="0.930742625">
α =
α =
α =
α =
α =
α =
α =
α =
</equation>
<page confidence="0.965778">
1113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.802632">
<title confidence="0.999331">Lattice-based System Combination for Statistical Machine Translation</title>
<author confidence="0.99078">Yang Feng</author>
<author confidence="0.99078">Yang Liu</author>
<author confidence="0.99078">Haitao Mi</author>
<author confidence="0.99078">Qun Liu</author>
<author confidence="0.99078">Yajuan</author>
<affiliation confidence="0.954566">Key Laboratory of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.956164">Chinese Academy of P.O. Box 2704, Beijing 100190,</address>
<email confidence="0.967554">yliu,htmi,liuqun,</email>
<abstract confidence="0.9989333125">Current system combination methods usually use confusion networks to find consensus translations among different systems. Requiring one-to-one mappings between the words in candidate translations, confusion networks have difficulty in handling more general situations in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In Proc. of IEEE ASRU,</booktitle>
<pages>351--354</pages>
<contexts>
<context position="1358" citStr="Bangalore et al., 2001" startWordPosition="181" endWordPosition="184"> for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called “skeleton” in the literature) and then decid</context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In Proc. of IEEE ASRU, pages 351–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL/HLT 2008,</booktitle>
<pages>1012--1020</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4727" citStr="Dyer et al., 2008" startWordPosition="752" endWordPosition="755">eses: he feels like apples lations. Note that the phrase “is fond of” is attached to an edge. Now, it is unlikely to obtain a translation like “He is like of apples”. A lattice G = hV, Ei is a directed acyclic graph, formally a weighted finite state automation (FSA), where V is the set of nodes and E is the set of edges. The nodes in a lattice are usually labeled according to an appropriate numbering to reflect how to produce a translation. Each edge in a lattice is attached with a sequence of words as well as the associated probability. As lattice is a more general form of confusion network (Dyer et al., 2008), we expect that replacing confusion networks with lattices will further improve system combination. 2.2 IHMM-based Alignment Method Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al., 2008) in both direction, we briefly review the IHMM-based alignment method first. Take the direction that the hypothesis is aligned to the backbone as an example. The conditional probability that the hypothesis is generated by the backbone is given by He feels like apples � J [p(aj|aj−1, I)p(e′j|eaj)]l (1) He prefer apples p(e′ J|eI 1) = j=1 He is fond of</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings ofACL/HLT 2008, pages 1012–1020, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In Proc. of ANLP,</booktitle>
<pages>95--100</pages>
<contexts>
<context position="1231" citStr="Frederking and Nirenburg, 1994" startWordPosition="163" endWordPosition="166"> in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. To construct a confusion network, one first need to choose</context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In Proc. of ANLP, pages 95–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jangfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmm-based hypothesis alignment for computing outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>98--107</pages>
<contexts>
<context position="1478" citStr="He et al., 2008" startWordPosition="205" endWordPosition="208">s significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called “skeleton” in the literature) and then decide the word alignments of other hypotheses to the backbone. Hypothesis alignment plays a crucial role in confusionnetwork</context>
<context position="3254" citStr="He et al., 2008" startWordPosition="490" endWordPosition="493">oducing degenerate translations such as “be capable to” and “be able of”. In this paper, we propose a new system combination method based on lattices. As a more general form of confusion network, a lattice is capable of describing arbitrary mappings in hypothesis alignment. In a lattice, each edge is associated with a sequence of words rather than a single word. Therefore, we select phrases instead of words in each candidate set and minimize the chance to produce unexpected translations such as “be capable to”. We compared our approach with the state-of-the-art confusion-network-based system (He et al., 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-toEnglish test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. 1105 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1105–1113, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP He feels like apples He prefer apples He feels like apples He is fond of apples (a) unidirectional alignments He feels like apples He prefer apples He feels like apples He is fond of apples (b) bidirectional alignments ε prefer of He feels like ε apples is fond </context>
<context position="4975" citStr="He et al., 2008" startWordPosition="787" endWordPosition="790">automation (FSA), where V is the set of nodes and E is the set of edges. The nodes in a lattice are usually labeled according to an appropriate numbering to reflect how to produce a translation. Each edge in a lattice is attached with a sequence of words as well as the associated probability. As lattice is a more general form of confusion network (Dyer et al., 2008), we expect that replacing confusion networks with lattices will further improve system combination. 2.2 IHMM-based Alignment Method Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al., 2008) in both direction, we briefly review the IHMM-based alignment method first. Take the direction that the hypothesis is aligned to the backbone as an example. The conditional probability that the hypothesis is generated by the backbone is given by He feels like apples � J [p(aj|aj−1, I)p(e′j|eaj)]l (1) He prefer apples p(e′ J|eI 1) = j=1 He is fond of apples 1 ai We choose the first sentence as the backbone. Then, we perform hypothesis alignment to build a confusion network, as shown in Figure 1(a). Note that although “feels like” has the same meaning with “is fond of”, a confusion network only</context>
<context position="20253" citStr="He et al., 2008" startWordPosition="3571" endWordPosition="3574">rcs which store more than one non-null word. The above two numbers are gotten by counting both backbone arcs and hypothesis arcs. α and Q are the corresponding weights of the numbers, respectively. Nword(e) is the non-null word number and γ is its weight. Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by logp(e/f) = + CL(e) + αNnullarc(e) + QNlongarc(e) + γNword(e) Narc i=1 log ( �Ns s=1 λsps(arc)) (9) 1110 1/(k+ 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10- best hypotheses from each candidate system</context>
<context position="21477" citStr="He et al. (2008)" startWordPosition="3779" endWordPosition="3782">dev and test sets were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5- gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, p = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS d</context>
<context position="27843" citStr="He et al., 2008" startWordPosition="4854" endWordPosition="4857">form of confusion networks, lattices can express n-to-n mappings. So a lattice-based model processes phrase pairs while 1112 MT02 MT05 BLEU% BLEU% 1.0 36.41 34.92 0.7 37.21 35.65 0.5 36.43 35.02 0.4 37.14 35.55 0.3 36.75 35.66 0.2 36.81 35.55 0.1 37.29 35.80 0.0 36.45 35.14 Table 5: Effect of semantic alignments a confusion-network-based model processes words only. As a result, phrase pairs must be extracted before constructing a lattice. On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusionnetwork-based system (He et al., 2008) and 3.73 BLEU points over the best single system. On NIST MT08 test set, the lattice-based system outperformed the confusion-network-based system by 0.93 BLEU point and outperformed the best single system by 3.0 BLEU points. 8 Acknowledgement The authors were supported by National Natural Science Foundation of China Contract 60736014, National Natural Science Foundation of China Contract 60873167 and High Technology R&amp;D Program Project No. 2006AA010108. Thank Wenbin Jiang, Tian Xia and Shu Cai for their help. We are also grateful to the anonymous reviewers for their valuable comments. Referen</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmm-based hypothesis alignment for computing outputs from machine translation systems. In Proc. of EMNLP, pages 98–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
</authors>
<title>Using word-dependent translation models in hmm based word alignment for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="21488" citStr="He (2007)" startWordPosition="3783" endWordPosition="3784">were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5- gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, p = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi</context>
</contexts>
<marker>He, 2007</marker>
<rawString>Xiaodong He. 2007. Using word-dependent translation models in hmm based word alignment for statistical machine translation. In Proc. of COLING-ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>53--64</pages>
<contexts>
<context position="20366" citStr="Huang and Chiang, 2005" startWordPosition="3590" endWordPosition="3593">cs and hypothesis arcs. α and Q are the corresponding weights of the numbers, respectively. Nword(e) is the non-null word number and γ is its weight. Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by logp(e/f) = + CL(e) + αNnullarc(e) + QNlongarc(e) + γNword(e) Narc i=1 log ( �Ns s=1 λsps(arc)) (9) 1110 1/(k+ 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10- best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights </context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 53– 64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLTNAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="8514" citStr="Koehn et al., 2003" startWordPosition="1388" endWordPosition="1391">d. The hypothesis with the minimum cost of edits against all hypotheses is selected. The backbone is significant for it influences not only the word order, but also the following alignments. The backbone is selected as follows: 1: EB = argmin TER(E′, E) (8) E′∈E E∈E (3) Get the alignments of the backbone and hypothesis pairs. First, each pair is aligned in both directions using the IHMM-based alignment method. In the IHMM alignment model, bilingual dictionaries in both directions are indispensable. Then, we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction (Koehn et al., 2003) to monolingual alignments. The bidirectional alignments are combined to one resorting to the grow-diag-final algorithm, allowing n-to-n mappings. (4)Normalize the alignment pairs. The word order of the backbone determines the word order of consensus outputs, so the word order of hypotheses must be consistent with that of the backbone. All words of a hypotheses are reordered according to the alignment to the backbone. For a word aligned to null, an actual null word may be inserted to the proper position. The alignment units are extracted first and then the hypothesis words in each unit are shi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLTNAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch Mayne</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th ACL, Demonstration Session.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="20770" citStr="Koehn et al., 2007" startWordPosition="3659" endWordPosition="3662">1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10- best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5- gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s </context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th ACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proc. ofIEEE EACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1380" citStr="Matusov et al., 2006" startWordPosition="185" endWordPosition="188">nts and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called “skeleton” in the literature) and then decide the word alignments </context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proc. ofIEEE EACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>312--319</pages>
<contexts>
<context position="1418" citStr="Rosti et al., 2007" startWordPosition="193" endWordPosition="196">idate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called “skeleton” in the literature) and then decide the word alignments of other hypotheses to the backbone. H</context>
<context position="18878" citStr="Rosti et al. (2007" startWordPosition="3334" endWordPosition="3337"> the current node. Line 12−13 deal with condition 2 and jump to the next backbone word of the lattice. Line 15−19 handle condition 3 and function InsertHarc inserts to the lattice a harc with the same hypothesis words and the same backbone word span with the current hypothesis arc. 5 Decoding In confusion network decoding, a translation is generated by traveling all the nodes from left to right. So a translation path contains all the nodes. While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words. The features are combined in a log-linear model with the arc posterior probabilities being processed specially as follows: where f denotes the source sentence, e denotes a translation generated by the lattice-based system, Narc is the number of arcs the path of e covers, Ns is the number of candidate systems and λs is the weight of system s. C is the language</context>
<context position="20234" citStr="Rosti et al., 2007" startWordPosition="3567" endWordPosition="3570">s the number of the arcs which store more than one non-null word. The above two numbers are gotten by counting both backbone arcs and hypothesis arcs. α and Q are the corresponding weights of the numbers, respectively. Nword(e) is the non-null word number and γ is its weight. Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by logp(e/f) = + CL(e) + αNnullarc(e) + QNlongarc(e) + γNword(e) Narc i=1 log ( �Ns s=1 λsps(arc)) (9) 1110 1/(k+ 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10- best hypotheses from ea</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard Schwartz. 2007a. Improved word-level system combination for machine translation. In Proc. of ACL, pages 312–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>228--235</pages>
<contexts>
<context position="1418" citStr="Rosti et al., 2007" startWordPosition="193" endWordPosition="196">idate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called “skeleton” in the literature) and then decide the word alignments of other hypotheses to the backbone. H</context>
<context position="18878" citStr="Rosti et al. (2007" startWordPosition="3334" endWordPosition="3337"> the current node. Line 12−13 deal with condition 2 and jump to the next backbone word of the lattice. Line 15−19 handle condition 3 and function InsertHarc inserts to the lattice a harc with the same hypothesis words and the same backbone word span with the current hypothesis arc. 5 Decoding In confusion network decoding, a translation is generated by traveling all the nodes from left to right. So a translation path contains all the nodes. While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words. The features are combined in a log-linear model with the arc posterior probabilities being processed specially as follows: where f denotes the source sentence, e denotes a translation generated by the lattice-based system, Narc is the number of arcs the path of e covers, Ns is the number of candidate systems and λs is the weight of system s. C is the language</context>
<context position="20234" citStr="Rosti et al., 2007" startWordPosition="3567" endWordPosition="3570">s the number of the arcs which store more than one non-null word. The above two numbers are gotten by counting both backbone arcs and hypothesis arcs. α and Q are the corresponding weights of the numbers, respectively. Nword(e) is the non-null word number and γ is its weight. Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc). ps(arc) is increased by logp(e/f) = + CL(e) + αNnullarc(e) + QNlongarc(e) + γNword(e) Narc i=1 log ( �Ns s=1 λsps(arc)) (9) 1110 1/(k+ 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10- best hypotheses from ea</context>
</contexts>
<marker>Rosti, Xiang, Matsoukas, Schwartz, Ayan, Dorr, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, Richard Schwartz, Necip Fazil Ayan, and Bonnie J. Dorr. 2007b. Combining outputs from multiple machine translation systems. In Proc. of NAACL-HLT, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental hypothesis alignment for building confusion networks with application to machine translaiton system combination.</title>
<date>2008</date>
<booktitle>In Proc. of the Third ACL WorkShop on Statistical Machine Translation,</booktitle>
<pages>183--186</pages>
<contexts>
<context position="1460" citStr="Rosti et al., 2008" startWordPosition="201" endWordPosition="204">our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called “skeleton” in the literature) and then decide the word alignments of other hypotheses to the backbone. Hypothesis alignment plays a crucial role i</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2008. Incremental hypothesis alignment for building confusion networks with application to machine translaiton system combination. In Proc. of the Third ACL WorkShop on Statistical Machine Translation, pages 183–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William J Byrne</author>
<author>Mark J F Gales</author>
<author>Hichem Sahbi</author>
<author>Phil C Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proc. ofICASSP,</booktitle>
<pages>105--108</pages>
<contexts>
<context position="1398" citStr="Sim et al., 2007" startWordPosition="189" endWordPosition="192">to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called “skeleton” in the literature) and then decide the word alignments of other hypothese</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William J. Byrne, Mark J.F. Gales, Hichem Sahbi, and Phil C. Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In Proc. ofICASSP, pages 105–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="21512" citStr="Vogel et al. (1996)" startWordPosition="3786" endWordPosition="3789"> as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5- gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, p = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set which included 289K sentenc</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proc. of COLING, pages 836–841.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>