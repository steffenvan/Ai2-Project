<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006299">
<title confidence="0.9979805">
On The Feasibility of Open Domain Referring Expression
Generation Using Large Scale Folksonomies
</title>
<author confidence="0.961432">
Fabi´an Pacheco Pablo Ariel Duboue* Martin Ariel Dominguez
</author>
<affiliation confidence="0.798601">
Facultad de Matem´atica, Astronom´ıa y F´ısica
</affiliation>
<address confidence="0.458364">
Universidad Nacional de C´ordoba
C´ordoba, Argentina
</address>
<sectionHeader confidence="0.974804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999872235294118">
Generating referring expressions has received
considerable attention in Natural Language
Generation. In recent years we start seeing
deployments of referring expression genera-
tors moving away from limited domains with
custom-made ontologies. In this work, we ex-
plore the feasibility of using large scale noisy
ontologies (folksonomies) for open domain
referring expression generation, an important
task for summarization by re-generation. Our
experiments on a fully annotated anaphora
resolution training set and a larger, volunteer-
submitted news corpus show that existing al-
gorithms are efficient enough to deal with
large scale ontologies but need to be extended
to deal with undefined values and some mea-
sure for information salience.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987636363637">
Given an entity1 (the referent) and a set of com-
peting entities (the set of distractors), the task of
referring expression generation (REG) involves cre-
ating a mention to the referent so that, in the eyes
of the reader, it is clearly distinguishable from any
other entity in the set of distractors. In a traditional
generation pipeline, referring expression generation
happens at the sentence planning level. As a result,
its output is not a textual nugget but a description
employed later on by the surface realizer. In this pa-
per, we consider the output of the REG system to
</bodyText>
<note confidence="0.6437775">
*To whom correspondence should be addressed. Email:
pablo.duboue@gmail.com.
</note>
<footnote confidence="0.409917">
1Or set of entities, but not in this work.
</footnote>
<bodyText confidence="0.99379825">
be Definite Descriptions (DD) consisting of a set of
positive triples and a set of negative triples, enumer-
ating referent-related properties.
Since the seminal work by Dale and Re-
iter (1995), REG has received a lot of attention in the
Natural Language Generation (NLG) community.
However, most of the early work on REG has been
on traditional NLG systems, using custom-tailored
ontologies. In recent years (Belz et al., 2010) there
has been a shift towards what we term “Open Do-
main Referring Expression Generation,” (OD REG),
that is, a REG task where the properties come from
a folksonomy, a large-scale volunteer-built ontology.
In particular, we are interested in changing
anaphoric references for entities appearing in sen-
tences drafted from different documents, as done
in multi-document summarization (Advaith et al.,
2011). For example, consider the following sum-
mary excerpt2 as produced by Newsblaster (McKe-
own et al., 2002):
</bodyText>
<construct confidence="0.973511777777778">
Thousands of cheering, flag-waving Palestinians gave
Palestinian Authority President Mahmoud Abbas an en-
thusiastic welcome in Ramallah on Sunday, as he told
them triumphantly that a “Palestinian spring” had been
born following his speech to the United Nations last
week.3 The president pressed Israel, in unusually frank
terms, to reach a final peace agreement with the Pales-
tinians, citing the boundaries in place on the eve of the
June 1967 Arab-Israeli War as the starting point for ne-
</construct>
<footnote confidence="0.988977">
2From http://newsblaster.cs.columbia.edu/archives/2011-10-
07-04-51-35/web/summaries/ 2011-10-07-04-51-35-011.html.
3After his stint at UN, Abbas is politically stronger than ever
(haaretz.com, 10/07/2011, 763 words).
</footnote>
<page confidence="0.876829666666667">
641
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 641–645,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.989000767441861">
gotiation about borders.4
Here the second sentence refers to U.S. presi-
dent Barack Obama and a referring expression of the
form “U.S. president” should have been used. Such
expressions depend on the set of distractors present
in the text, a requirement that highlights the dynamic
nature of the problem. Our experiments extracted
thousands of complex cases (such as distinguishing
one musician from a set of five) which we used to
test existing algorithms against a folksonomy, dbPe-
dia5 (Bizer et al., 2009). This folksonomy contains
1.7M triples (for its English version) and has been
curated from Wikipedia.6
We performed two experiments: first we em-
ployed sets of distractors derived from a set of docu-
ments annotated with anaphora resolution informa-
tion (Hasler et al., 2006). We found that roughly
half of the entities annotated in the documents were
present in the folksonomy, which speaks of the feasi-
bility of using a folksonomy for OD REG, given the
fact that Wikipedia has strict notability requirements
for adding information. In the second experiment,
we obtained sets of distractors from Wikinews,7 a
service where volunteers submit news articles inter-
spersed with Wikipedia links. We leveraged said
links to assemble 40k referring expression tasks.
For algorithms, we employed Dale and Re-
iter (1995), Gardent (2002) and Full Brevity (FB)
(Bohnet, 2007). Our results show that the first two
algorithms produce results in a majority of the re-
ferring expression tasks, with the Dale and Reiter
algorithm being the most efficient and resilient of
the three. The results, however, are of mixed quality
and more research is needed to overcome two prob-
lems we have identified in our experiments: dealing
with undefined information in the folksonomy and
the need to incorporate a rough user model in the
form of information salience.
In the next section we briefly summarize the three
algorithms we employed in our experiments. In Sec-
tion 3, we describe the data employed. Section 4
contains the results of our experiments and subse-
quent analysis. We conclude discussing future work.
</bodyText>
<footnote confidence="0.9467636">
4Obama prods Mideast allies to embrace reform, make
peace (Washington Post, 10/07/2011, 371 words).
5http://dbpedia.org
6http://wikipedia.org
7http://wikinews.org
</footnote>
<sectionHeader confidence="0.87048" genericHeader="method">
2 Referring Expression Generation (REG)
</sectionHeader>
<bodyText confidence="0.997899">
REG literature is vast and spans decades of work.
We picked three algorithms with the following
desiderata: all the algorithms can deal with single
entity referents (a significant amount of recent work
went into multi-entity referents) and we wanted to
showcase a classic algorithm (Dale and Reiter’s), an
algorithm generating negations (Gardent’s) and an
algorithm with a more exhaustive search of the solu-
tions space (Full Brevity). We very briefly describe
each of the algorithms in turn, where R is the refer-
ent, C is the set of distractors and P is a list of prop-
erties, triples in the form (entity, property, value),
describing R:
Dale and Reiter (1995). They assume the prop-
erties in P are ordered according to an established
criteria. Then the algorithm iterates over P, adding
each triple one at a time and removing from C all
entities ruled out by the new triple. Triples that do
not eliminate any new entities from C are ignored.
The algorithm terminates when C is empty.
Gardent (2002). The algorithm uses Constraint
Satisfaction Programming to solve two basic con-
straints: find a set of positive properties P+ and neg-
ative properties P−, such that all properties in P+
are true for the referent and all in P− are false, and
it is the smaller P+ U P− such that for every c E C
there exist a property in P+ that does not hold for c
or a property in P− that holds for c.8
Full Brevity (Bohnet, 2007). Starting from a
state E of the form (L, C, P) with L = 0 (selected
properties), it keeps these states into a queue, where
it loops until C = 0. In each loop it generates new
states (added to the end of the queue), as follows:
given a state E = (L, C, P) for each p E P, if p re-
moves elements rem from C, it adds (L U {p}, C −
rem, P − {p}), otherwise (L, C, P − {p}).
</bodyText>
<sectionHeader confidence="0.998311" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.978136666666667">
dbPedia. dbPedia (Bizer et al., 2009) is
an ontology curated from Wikipedia infoboxes,
small tables containing structured information at
the top of most Wikipedia pages. The ver-
sion employed in this paper (“Ontology Infobox
Properties”) contains 1,7520,158 triples. Each
</bodyText>
<footnote confidence="0.972412">
8We employed the Choco CSP solver Java library:
http://www.emn.fr/z-info/choco-solver/.
</footnote>
<page confidence="0.979975">
642
</page>
<table confidence="0.669842857142857">
Former [[New Mexico]] {{w|Governor of New
Mexico|governor}} {{w|Gary Johnson}} ended
his campaign for the {{w|Republican Party
(United States)|Republican Party}} (GOP)
presidential nomination to seek the backing
of the {{w|Libertarian Party (United
States)|Libertarian Party}} (LP).
</table>
<figureCaption confidence="0.997175">
Figure 1: Wikinews example, from http://en.wikinews.org
/wiki/U.S. presidential candidate Gary Johnson leaves GOP to vie for
the LP nom
</figureCaption>
<bodyText confidence="0.999884536585366">
entity is represented by a URI starting with
http://dbpedia.org/resource/ followed by
the name of its associated Wikipedia title. See the
next section for some example triples.
Pilot. While creating unambiguous descriptions
is the NLG task known as referring expression gen-
eration, its NLU counterpart is anaphora resolu-
tion. We took a hand-annotated corpus for training
anaphora resolution algorithms (Hasler et al., 2006)
consisting of 74 documents containing 239 corefer-
ence chains. Each of the chains is an entity that can
be used for our experiments, if the entity is in db-
Pedia and there are other suitable distractors in the
same document. We hand annotated each of those
239 coreference chains by type (person, organiza-
tion and location) and associated them to dbPedia
URIs for the ones we found on Wikipedia. We found
roughly half of the chains in dbPedia (106 out of
239, 44%). This percentage speaks of the coverage
of dbPedia for OD REG. However, only 16 docu-
ments contain multiple entities of the same type and
present in dbPedia, our pilot study criteria. These 16
documents result in the 16 tasks for our pilot. For a
large scale evaluation we turned to Wikinews.
Wikinews. Wikinews is a news service operated
as a wiki. As the news articles are interspersed
with interwiki links, multiple entities can be disam-
biguated as Wikipedia pages (which in turn are db-
Pedia URIs). For example, in Figure 1, both the Lib-
ertarian Party and Republican Party can be consid-
ered potential distractors, as both are organizations.
The Wikimedia Foundation makes a database
dump available for all Wikinews interwiki links (the
links in braces in the above example). If a page con-
tains more than one organization or person, we ex-
tracted the whole set of people (or organizations) as
a referring expression task. To see whether a URI
is a person or an organization we check for a birth
date or creation date, respectively. In this manner,
we obtained 4,230 tasks for people and 12,998 for
organizations. This is dataset is freely available.9
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999870921052632">
Pilot. The 16 tasks were split into 40 runs (a task
spans n runs each, where n is the number of entities
in the task, by rotating through the different alterna-
tive pairs of referent / set of distractors). From these
tasks, Dale and Reiter produced no output 12 times
and FB Brevity was unable to produce a result in 23
times. Gardent produced output for every run. We
consider this an example of the increased expressive
power of negative descriptions (it included a nega-
tion in 25% of the runs). For the other two algo-
rithms, the lack of an unique triple differentiating
one entity from the set of distractors seemed to be
the main issue but there were multiple cases were FB
ran out of memory for its queue of candidate nodes.
With respect to execution timings, Dale and Re-
iter ran into some corner cases and took time com-
parable to Gardent’s algorithm. FB was 16 times
slower (we found this counter-intuitive, as Gardent’s
algorithm is more demanding). Therefore, two of
these algorithms were able to produce results using
large scale ontological information. As FB ran into
problems both in terms of execution time and failure
rates, we omitted it from the large scale experiments.
We adjusted the parameters for the algorithms on
this set to obtain the best possible quality output
given the data and the problem. As such, we do not
report quality assessments on the pilot data.
Wikinews. The tasks obtained from wikinews
contained a large number of entities per task (an av-
erage of 12 people per task) and therefore span a
large number of runs: 17,814 runs for people (from
4,230 tasks) and 44,080 for organizations (from
12,998 tasks).
On these large runs, execution time differences
are in line with our a priori expectations: the greedy
approach of Dale and Reiter is very fast10 with Gar-
dent’s more comprehensive search taking about 40
times more time. Dale and Reiter failure rate was
</bodyText>
<footnote confidence="0.99946525">
9http://www.cs.famaf.unc.edu.ar/˜pduboue/data/ also mirrored
at http://duboue.ca/data.
10Dale and Reiter takes less than 3’ for the 44,080 runs for
organizations in a 2.3 GHz machine.
</footnote>
<page confidence="0.997706">
643
</page>
<figure confidence="0.94580175">
Referent Dale and Reiter Output Gardent Output
EB { (EB occupation Software Freedom Law Center) } { (EB occupation Software Freedom Law Center) }
LL { (LL birthPlace United States), (LL, occupation Harvard Law School) } { (LL birthPlace Rapid City, South Dakota) }
LT { (LT occupation Software engineer) } { (LT nationality Finnish American) }
</figure>
<figureCaption confidence="0.999253">
Figure 2: Example output for the task: {‘Eben Moglen’ (EB), ‘Lawrence Lessig’ (LL), ‘Linus Torvalds’ (LT) }.
</figureCaption>
<bodyText confidence="0.999986879310345">
comparable or better than in the pilot (for organiza-
tions that are more mixed, it was slightly lower but
for people it was as low 2.8%). Gardent missed 2%
of the people (and only 54 organizations), employ-
ing negatives 14% of the time for people and 12% of
the time for organizations.
Evaluating referring expressions is hard. Efforts
to automate this task in NLG (Gatt et al., 2007)
have taken an approach similar to machine transla-
tion BLEU scores (Papinini et al., 2001), for exam-
ple, by asking multiple judges to produce referring
expressions for a given scenario. These settings usu-
ally involve images of physical objects and relate to
small ontologies. While such an approach could be
adapted to the Open Domain case, a major problem
is the need for the judges to be acquainted with some
of the less popular entities in the training set. At
this point in our research, we decided to analyze the
quality of a sample of the output ourselves. This
process involved consulting information about each
entity to determine the soundness of the result.
We looked at a random sample of 20 runs and an-
notated it by two authors, measuring a Cohen’s n of
60% for annotating DD results and 79% for deter-
mining whether the folksonomy had enough infor-
mation to build a satisfactory DD. We then extended
the evaluation to 60 runs and annotated them by one
author. We found that Dale and Reiter produced a
satisfactory DD in 41.6% of the cases and Gardent
in 43.4% of the cases and that the folksonomy con-
tained enough information 81.6% of the time. Fig-
ure 2 shows some example output.
From the evaluation we learned that the default
ordering strategy employed by Dale and Reiter is
not stable across different types of people (compare:
politicians vs. musicians) or organizations. We also
saw that Gardent’s algorithm in many cases selected
a single triple with very little practical value (an ob-
scure fact about the entity) or a negative piece of in-
formation which is actually true for the referent but
it is a missing piece of information.
The first two problems can be solved by either fur-
ther subdividing the taxonomies of entities or (more
interestingly) by incorporating some measure about
the salience of each piece of information, a possibil-
ity which we will discuss next. The last issue can be
addressed by having some form of meaningful de-
fault value.
The negations produced by Gardent’s algorithm
highlighted errors on the folksonomy. For example,
when referring to China with distractors Peru and
Taiwan, it will produce “the place where they do not
speak Chinese,” as China has the different Chinese
dialects spelled out on the folksonomy (and some
Peruvians do speak Chinese). Given these limita-
tions, we find the current results very encouraging
and we believe folksonomies can help focus on ro-
bust NLG for noisy (ontological) inputs.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999561823529412">
We have shown that by using a folksonomy it should
be possible to deploy traditional NLG referring ex-
pression generation algorithms in Open Domain
tasks. To fulfill this vision, three tasks remain:
Dealing with missing information. Some form of
smart default values are needed, we are considering
using a nearest-neighbor approach to find ontologi-
cal siblings which can provide such defaults.
Estimating salience of each piece of ontological
information. The importance for each triple has to
be obtained in a way consistent with the Open Do-
main nature of the task. For this problem, we believe
search engine salience can be of great help.
Transform the extracted triples into actual text.
This problem has received attention in the past. We
would like to explore traditional surface realizer
with a custom-made grammar.
</bodyText>
<sectionHeader confidence="0.998302" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.981369">
We would like to thank the anonymous reviewers as
well as Annie Ying and Victoria Reggiardo.
</bodyText>
<page confidence="0.99841">
644
</page>
<sectionHeader confidence="0.995877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909583333333">
Siddharthan Advaith, Nenkova Ani, and McKeown Kath-
leen. 2011. Information status distinctions and refer-
ring expressions: An empirical study of references to
people in news summaries. Computational Linguis-
tics, 37(4):811–842.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2010. Generating referring expressions in context:
The grec task evaluation challenges. In Emiel Krah-
mer and Marit Theune, editors, Empirical Methods
in Natural Language Generation, volume 5790 of
Lecture Notes in Computer Science, pages 294–327.
Springer.
C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,
R. Cyganiak, and S. Hellmann. 2009. DBpedia-a
crystallization point for the web of data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, 7(3):154–165.
B. Bohnet. 2007. is-fbn, is-fbs, is-iac: The adaptation
of two classic algorithms for the generation of refer-
ring expressions in order to produce expressions like
humans do. MT Summit XI, UCNLG+ MT, pages 84–
86.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2):233–263.
C. Gardent. 2002. Generating minimal definite descrip-
tions. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, pages 96–
103. Association for Computational Linguistics.
A. Gatt, I. Van Der Sluis, and K. Van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proceedings
of the Eleventh European Workshop on Natural Lan-
guage Generation, pages 49–56. Association for Com-
putational Linguistics.
L. Hasler, C. Orasan, and K. Naumann. 2006. NPs
for events: Experiments in coreference annotation. In
Proceedings of the 5th edition of the International
Conference on Language Resources and Evaluation
(LREC2006), pages 1167–1172.
Kathleen R. McKeown, R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Tracking and
summarizing news on a daily basis with columbia’s
newsblaster. In Proc. of HLT.
Kishore Papinini, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. Technical report, IBM.
</reference>
<page confidence="0.998865">
645
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941326">
<title confidence="0.9991275">On The Feasibility of Open Domain Referring Expression Generation Using Large Scale Folksonomies</title>
<author confidence="0.999272">Pacheco Pablo Ariel Ariel Dominguez</author>
<affiliation confidence="0.991023">Facultad de Matem´atica, Astronom´ıa y F´ısica Universidad Nacional de C´ordoba</affiliation>
<address confidence="0.988167">C´ordoba, Argentina</address>
<abstract confidence="0.998378722222222">Generating referring expressions has received considerable attention in Natural Language Generation. In recent years we start seeing deployments of referring expression generators moving away from limited domains with custom-made ontologies. In this work, we explore the feasibility of using large scale noisy ontologies (folksonomies) for open domain referring expression generation, an important task for summarization by re-generation. Our experiments on a fully annotated anaphora resolution training set and a larger, volunteersubmitted news corpus show that existing algorithms are efficient enough to deal with large scale ontologies but need to be extended to deal with undefined values and some measure for information salience.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Siddharthan Advaith</author>
<author>Nenkova Ani</author>
<author>McKeown Kathleen</author>
</authors>
<title>Information status distinctions and referring expressions: An empirical study of references to people in news summaries.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="2544" citStr="Advaith et al., 2011" startWordPosition="385" endWordPosition="388">a lot of attention in the Natural Language Generation (NLG) community. However, most of the early work on REG has been on traditional NLG systems, using custom-tailored ontologies. In recent years (Belz et al., 2010) there has been a shift towards what we term “Open Domain Referring Expression Generation,” (OD REG), that is, a REG task where the properties come from a folksonomy, a large-scale volunteer-built ontology. In particular, we are interested in changing anaphoric references for entities appearing in sentences drafted from different documents, as done in multi-document summarization (Advaith et al., 2011). For example, consider the following summary excerpt2 as produced by Newsblaster (McKeown et al., 2002): Thousands of cheering, flag-waving Palestinians gave Palestinian Authority President Mahmoud Abbas an enthusiastic welcome in Ramallah on Sunday, as he told them triumphantly that a “Palestinian spring” had been born following his speech to the United Nations last week.3 The president pressed Israel, in unusually frank terms, to reach a final peace agreement with the Palestinians, citing the boundaries in place on the eve of the June 1967 Arab-Israeli War as the starting point for ne2From </context>
</contexts>
<marker>Advaith, Ani, Kathleen, 2011</marker>
<rawString>Siddharthan Advaith, Nenkova Ani, and McKeown Kathleen. 2011. Information status distinctions and referring expressions: An empirical study of references to people in news summaries. Computational Linguistics, 37(4):811–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Eric Kow, Jette Viethen, and Albert Gatt.</title>
<date>2010</date>
<booktitle>In Emiel Krahmer and Marit Theune, editors, Empirical Methods in Natural Language Generation,</booktitle>
<volume>5790</volume>
<pages>294--327</pages>
<publisher>Springer.</publisher>
<marker>Belz, 2010</marker>
<rawString>Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2010. Generating referring expressions in context: The grec task evaluation challenges. In Emiel Krahmer and Marit Theune, editors, Empirical Methods in Natural Language Generation, volume 5790 of Lecture Notes in Computer Science, pages 294–327. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bizer</author>
<author>J Lehmann</author>
<author>G Kobilarov</author>
<author>S Auer</author>
<author>C Becker</author>
<author>R Cyganiak</author>
<author>S Hellmann</author>
</authors>
<title>DBpedia-a crystallization point for the web of data. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2009</date>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="4091" citStr="Bizer et al., 2009" startWordPosition="610" endWordPosition="613">echnologies, pages 641–645, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics gotiation about borders.4 Here the second sentence refers to U.S. president Barack Obama and a referring expression of the form “U.S. president” should have been used. Such expressions depend on the set of distractors present in the text, a requirement that highlights the dynamic nature of the problem. Our experiments extracted thousands of complex cases (such as distinguishing one musician from a set of five) which we used to test existing algorithms against a folksonomy, dbPedia5 (Bizer et al., 2009). This folksonomy contains 1.7M triples (for its English version) and has been curated from Wikipedia.6 We performed two experiments: first we employed sets of distractors derived from a set of documents annotated with anaphora resolution information (Hasler et al., 2006). We found that roughly half of the entities annotated in the documents were present in the folksonomy, which speaks of the feasibility of using a folksonomy for OD REG, given the fact that Wikipedia has strict notability requirements for adding information. In the second experiment, we obtained sets of distractors from Wikine</context>
<context position="7700" citStr="Bizer et al., 2009" startWordPosition="1231" endWordPosition="1234">ent and all in P− are false, and it is the smaller P+ U P− such that for every c E C there exist a property in P+ that does not hold for c or a property in P− that holds for c.8 Full Brevity (Bohnet, 2007). Starting from a state E of the form (L, C, P) with L = 0 (selected properties), it keeps these states into a queue, where it loops until C = 0. In each loop it generates new states (added to the end of the queue), as follows: given a state E = (L, C, P) for each p E P, if p removes elements rem from C, it adds (L U {p}, C − rem, P − {p}), otherwise (L, C, P − {p}). 3 Data dbPedia. dbPedia (Bizer et al., 2009) is an ontology curated from Wikipedia infoboxes, small tables containing structured information at the top of most Wikipedia pages. The version employed in this paper (“Ontology Infobox Properties”) contains 1,7520,158 triples. Each 8We employed the Choco CSP solver Java library: http://www.emn.fr/z-info/choco-solver/. 642 Former [[New Mexico]] {{w|Governor of New Mexico|governor}} {{w|Gary Johnson}} ended his campaign for the {{w|Republican Party (United States)|Republican Party}} (GOP) presidential nomination to seek the backing of the {{w|Libertarian Party (United States)|Libertarian Party</context>
</contexts>
<marker>Bizer, Lehmann, Kobilarov, Auer, Becker, Cyganiak, Hellmann, 2009</marker>
<rawString>C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, and S. Hellmann. 2009. DBpedia-a crystallization point for the web of data. Web Semantics: Science, Services and Agents on the World Wide Web, 7(3):154–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>is-fbn, is-fbs, is-iac: The adaptation of two classic algorithms for the generation of referring expressions in order to produce expressions like humans do.</title>
<date>2007</date>
<pages>84--86</pages>
<publisher>MT</publisher>
<contexts>
<context position="4950" citStr="Bohnet, 2007" startWordPosition="747" endWordPosition="748">tion (Hasler et al., 2006). We found that roughly half of the entities annotated in the documents were present in the folksonomy, which speaks of the feasibility of using a folksonomy for OD REG, given the fact that Wikipedia has strict notability requirements for adding information. In the second experiment, we obtained sets of distractors from Wikinews,7 a service where volunteers submit news articles interspersed with Wikipedia links. We leveraged said links to assemble 40k referring expression tasks. For algorithms, we employed Dale and Reiter (1995), Gardent (2002) and Full Brevity (FB) (Bohnet, 2007). Our results show that the first two algorithms produce results in a majority of the referring expression tasks, with the Dale and Reiter algorithm being the most efficient and resilient of the three. The results, however, are of mixed quality and more research is needed to overcome two problems we have identified in our experiments: dealing with undefined information in the folksonomy and the need to incorporate a rough user model in the form of information salience. In the next section we briefly summarize the three algorithms we employed in our experiments. In Section 3, we describe the da</context>
<context position="7286" citStr="Bohnet, 2007" startWordPosition="1140" endWordPosition="1141">e one at a time and removing from C all entities ruled out by the new triple. Triples that do not eliminate any new entities from C are ignored. The algorithm terminates when C is empty. Gardent (2002). The algorithm uses Constraint Satisfaction Programming to solve two basic constraints: find a set of positive properties P+ and negative properties P−, such that all properties in P+ are true for the referent and all in P− are false, and it is the smaller P+ U P− such that for every c E C there exist a property in P+ that does not hold for c or a property in P− that holds for c.8 Full Brevity (Bohnet, 2007). Starting from a state E of the form (L, C, P) with L = 0 (selected properties), it keeps these states into a queue, where it loops until C = 0. In each loop it generates new states (added to the end of the queue), as follows: given a state E = (L, C, P) for each p E P, if p removes elements rem from C, it adds (L U {p}, C − rem, P − {p}), otherwise (L, C, P − {p}). 3 Data dbPedia. dbPedia (Bizer et al., 2009) is an ontology curated from Wikipedia infoboxes, small tables containing structured information at the top of most Wikipedia pages. The version employed in this paper (“Ontology Infobox</context>
</contexts>
<marker>Bohnet, 2007</marker>
<rawString>B. Bohnet. 2007. is-fbn, is-fbs, is-iac: The adaptation of two classic algorithms for the generation of referring expressions in order to produce expressions like humans do. MT Summit XI, UCNLG+ MT, pages 84– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretations of the gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1904" citStr="Dale and Reiter (1995)" startWordPosition="285" endWordPosition="289"> entity in the set of distractors. In a traditional generation pipeline, referring expression generation happens at the sentence planning level. As a result, its output is not a textual nugget but a description employed later on by the surface realizer. In this paper, we consider the output of the REG system to *To whom correspondence should be addressed. Email: pablo.duboue@gmail.com. 1Or set of entities, but not in this work. be Definite Descriptions (DD) consisting of a set of positive triples and a set of negative triples, enumerating referent-related properties. Since the seminal work by Dale and Reiter (1995), REG has received a lot of attention in the Natural Language Generation (NLG) community. However, most of the early work on REG has been on traditional NLG systems, using custom-tailored ontologies. In recent years (Belz et al., 2010) there has been a shift towards what we term “Open Domain Referring Expression Generation,” (OD REG), that is, a REG task where the properties come from a folksonomy, a large-scale volunteer-built ontology. In particular, we are interested in changing anaphoric references for entities appearing in sentences drafted from different documents, as done in multi-docum</context>
<context position="4897" citStr="Dale and Reiter (1995)" startWordPosition="736" endWordPosition="740"> a set of documents annotated with anaphora resolution information (Hasler et al., 2006). We found that roughly half of the entities annotated in the documents were present in the folksonomy, which speaks of the feasibility of using a folksonomy for OD REG, given the fact that Wikipedia has strict notability requirements for adding information. In the second experiment, we obtained sets of distractors from Wikinews,7 a service where volunteers submit news articles interspersed with Wikipedia links. We leveraged said links to assemble 40k referring expression tasks. For algorithms, we employed Dale and Reiter (1995), Gardent (2002) and Full Brevity (FB) (Bohnet, 2007). Our results show that the first two algorithms produce results in a majority of the referring expression tasks, with the Dale and Reiter algorithm being the most efficient and resilient of the three. The results, however, are of mixed quality and more research is needed to overcome two problems we have identified in our experiments: dealing with undefined information in the folksonomy and the need to incorporate a rough user model in the form of information salience. In the next section we briefly summarize the three algorithms we employed</context>
<context position="6536" citStr="Dale and Reiter (1995)" startWordPosition="997" endWordPosition="1000">des of work. We picked three algorithms with the following desiderata: all the algorithms can deal with single entity referents (a significant amount of recent work went into multi-entity referents) and we wanted to showcase a classic algorithm (Dale and Reiter’s), an algorithm generating negations (Gardent’s) and an algorithm with a more exhaustive search of the solutions space (Full Brevity). We very briefly describe each of the algorithms in turn, where R is the referent, C is the set of distractors and P is a list of properties, triples in the form (entity, property, value), describing R: Dale and Reiter (1995). They assume the properties in P are ordered according to an established criteria. Then the algorithm iterates over P, adding each triple one at a time and removing from C all entities ruled out by the new triple. Triples that do not eliminate any new entities from C are ignored. The algorithm terminates when C is empty. Gardent (2002). The algorithm uses Constraint Satisfaction Programming to solve two basic constraints: find a set of positive properties P+ and negative properties P−, such that all properties in P+ are true for the referent and all in P− are false, and it is the smaller P+ U</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gardent</author>
</authors>
<title>Generating minimal definite descriptions.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>96--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4913" citStr="Gardent (2002)" startWordPosition="741" endWordPosition="742">tated with anaphora resolution information (Hasler et al., 2006). We found that roughly half of the entities annotated in the documents were present in the folksonomy, which speaks of the feasibility of using a folksonomy for OD REG, given the fact that Wikipedia has strict notability requirements for adding information. In the second experiment, we obtained sets of distractors from Wikinews,7 a service where volunteers submit news articles interspersed with Wikipedia links. We leveraged said links to assemble 40k referring expression tasks. For algorithms, we employed Dale and Reiter (1995), Gardent (2002) and Full Brevity (FB) (Bohnet, 2007). Our results show that the first two algorithms produce results in a majority of the referring expression tasks, with the Dale and Reiter algorithm being the most efficient and resilient of the three. The results, however, are of mixed quality and more research is needed to overcome two problems we have identified in our experiments: dealing with undefined information in the folksonomy and the need to incorporate a rough user model in the form of information salience. In the next section we briefly summarize the three algorithms we employed in our experime</context>
<context position="6874" citStr="Gardent (2002)" startWordPosition="1059" endWordPosition="1060">stive search of the solutions space (Full Brevity). We very briefly describe each of the algorithms in turn, where R is the referent, C is the set of distractors and P is a list of properties, triples in the form (entity, property, value), describing R: Dale and Reiter (1995). They assume the properties in P are ordered according to an established criteria. Then the algorithm iterates over P, adding each triple one at a time and removing from C all entities ruled out by the new triple. Triples that do not eliminate any new entities from C are ignored. The algorithm terminates when C is empty. Gardent (2002). The algorithm uses Constraint Satisfaction Programming to solve two basic constraints: find a set of positive properties P+ and negative properties P−, such that all properties in P+ are true for the referent and all in P− are false, and it is the smaller P+ U P− such that for every c E C there exist a property in P+ that does not hold for c or a property in P− that holds for c.8 Full Brevity (Bohnet, 2007). Starting from a state E of the form (L, C, P) with L = 0 (selected properties), it keeps these states into a queue, where it loops until C = 0. In each loop it generates new states (adde</context>
</contexts>
<marker>Gardent, 2002</marker>
<rawString>C. Gardent. 2002. Generating minimal definite descriptions. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 96– 103. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>I Van Der Sluis</author>
<author>K Van Deemter</author>
</authors>
<title>Evaluating algorithms for the generation of referring expressions using a balanced corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>49--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gatt, Van Der Sluis, Van Deemter, 2007</marker>
<rawString>A. Gatt, I. Van Der Sluis, and K. Van Deemter. 2007. Evaluating algorithms for the generation of referring expressions using a balanced corpus. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 49–56. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hasler</author>
<author>C Orasan</author>
<author>K Naumann</author>
</authors>
<title>NPs for events: Experiments in coreference annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th edition of the International Conference on Language Resources and Evaluation (LREC2006),</booktitle>
<pages>1167--1172</pages>
<contexts>
<context position="4363" citStr="Hasler et al., 2006" startWordPosition="653" endWordPosition="656">en used. Such expressions depend on the set of distractors present in the text, a requirement that highlights the dynamic nature of the problem. Our experiments extracted thousands of complex cases (such as distinguishing one musician from a set of five) which we used to test existing algorithms against a folksonomy, dbPedia5 (Bizer et al., 2009). This folksonomy contains 1.7M triples (for its English version) and has been curated from Wikipedia.6 We performed two experiments: first we employed sets of distractors derived from a set of documents annotated with anaphora resolution information (Hasler et al., 2006). We found that roughly half of the entities annotated in the documents were present in the folksonomy, which speaks of the feasibility of using a folksonomy for OD REG, given the fact that Wikipedia has strict notability requirements for adding information. In the second experiment, we obtained sets of distractors from Wikinews,7 a service where volunteers submit news articles interspersed with Wikipedia links. We leveraged said links to assemble 40k referring expression tasks. For algorithms, we employed Dale and Reiter (1995), Gardent (2002) and Full Brevity (FB) (Bohnet, 2007). Our results</context>
<context position="8868" citStr="Hasler et al., 2006" startWordPosition="1389" endWordPosition="1392">|Libertarian Party (United States)|Libertarian Party}} (LP). Figure 1: Wikinews example, from http://en.wikinews.org /wiki/U.S. presidential candidate Gary Johnson leaves GOP to vie for the LP nom entity is represented by a URI starting with http://dbpedia.org/resource/ followed by the name of its associated Wikipedia title. See the next section for some example triples. Pilot. While creating unambiguous descriptions is the NLG task known as referring expression generation, its NLU counterpart is anaphora resolution. We took a hand-annotated corpus for training anaphora resolution algorithms (Hasler et al., 2006) consisting of 74 documents containing 239 coreference chains. Each of the chains is an entity that can be used for our experiments, if the entity is in dbPedia and there are other suitable distractors in the same document. We hand annotated each of those 239 coreference chains by type (person, organization and location) and associated them to dbPedia URIs for the ones we found on Wikipedia. We found roughly half of the chains in dbPedia (106 out of 239, 44%). This percentage speaks of the coverage of dbPedia for OD REG. However, only 16 documents contain multiple entities of the same type and</context>
</contexts>
<marker>Hasler, Orasan, Naumann, 2006</marker>
<rawString>L. Hasler, C. Orasan, and K. Naumann. 2006. NPs for events: Experiments in coreference annotation. In Proceedings of the 5th edition of the International Conference on Language Resources and Evaluation (LREC2006), pages 1167–1172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>R Barzilay</author>
<author>D Evans</author>
<author>V Hatzivassiloglou</author>
<author>J L Klavans</author>
<author>A Nenkova</author>
<author>C Sable</author>
<author>B Schiffman</author>
<author>S Sigelman</author>
</authors>
<title>Tracking and summarizing news on a daily basis with columbia’s newsblaster.</title>
<date>2002</date>
<booktitle>In Proc. of HLT.</booktitle>
<contexts>
<context position="2648" citStr="McKeown et al., 2002" startWordPosition="401" endWordPosition="405">n REG has been on traditional NLG systems, using custom-tailored ontologies. In recent years (Belz et al., 2010) there has been a shift towards what we term “Open Domain Referring Expression Generation,” (OD REG), that is, a REG task where the properties come from a folksonomy, a large-scale volunteer-built ontology. In particular, we are interested in changing anaphoric references for entities appearing in sentences drafted from different documents, as done in multi-document summarization (Advaith et al., 2011). For example, consider the following summary excerpt2 as produced by Newsblaster (McKeown et al., 2002): Thousands of cheering, flag-waving Palestinians gave Palestinian Authority President Mahmoud Abbas an enthusiastic welcome in Ramallah on Sunday, as he told them triumphantly that a “Palestinian spring” had been born following his speech to the United Nations last week.3 The president pressed Israel, in unusually frank terms, to reach a final peace agreement with the Palestinians, citing the boundaries in place on the eve of the June 1967 Arab-Israeli War as the starting point for ne2From http://newsblaster.cs.columbia.edu/archives/2011-10- 07-04-51-35/web/summaries/ 2011-10-07-04-51-35-011.</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen R. McKeown, R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman, and S. Sigelman. 2002. Tracking and summarizing news on a daily basis with columbia’s newsblaster. In Proc. of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papinini</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical report, IBM.</tech>
<contexts>
<context position="13502" citStr="Papinini et al., 2001" startWordPosition="2181" endWordPosition="2184"> } { (LT nationality Finnish American) } Figure 2: Example output for the task: {‘Eben Moglen’ (EB), ‘Lawrence Lessig’ (LL), ‘Linus Torvalds’ (LT) }. comparable or better than in the pilot (for organizations that are more mixed, it was slightly lower but for people it was as low 2.8%). Gardent missed 2% of the people (and only 54 organizations), employing negatives 14% of the time for people and 12% of the time for organizations. Evaluating referring expressions is hard. Efforts to automate this task in NLG (Gatt et al., 2007) have taken an approach similar to machine translation BLEU scores (Papinini et al., 2001), for example, by asking multiple judges to produce referring expressions for a given scenario. These settings usually involve images of physical objects and relate to small ontologies. While such an approach could be adapted to the Open Domain case, a major problem is the need for the judges to be acquainted with some of the less popular entities in the training set. At this point in our research, we decided to analyze the quality of a sample of the output ourselves. This process involved consulting information about each entity to determine the soundness of the result. We looked at a random </context>
</contexts>
<marker>Papinini, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papinini, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical report, IBM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>