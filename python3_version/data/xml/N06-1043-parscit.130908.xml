<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000055">
<title confidence="0.9931545">
Cross-Entropy and Estimation of
Probabilistic Context-Free Grammars
</title>
<author confidence="0.996358">
Anna Corazza
</author>
<affiliation confidence="0.7625895">
Department of Physics
University “Federico II”
</affiliation>
<address confidence="0.9444745">
via Cinthia
I-80126 Napoli, Italy
</address>
<email confidence="0.997872">
corazza@na.infn.it
</email>
<author confidence="0.997378">
Giorgio Satta
</author>
<affiliation confidence="0.99714">
Department of Information Engineering
University of Padua
</affiliation>
<address confidence="0.949803">
via Gradenigo, 6/A
I-35131 Padova, Italy
</address>
<email confidence="0.998506">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.983133" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999930625">
We investigate the problem of training
probabilistic context-free grammars on
the basis of a distribution defined over
an infinite set of trees, by minimizing
the cross-entropy. This problem can be
seen as a generalization of the well-known
maximum likelihood estimator on (finite)
tree banks. We prove an unexpected the-
oretical property of grammars that are
trained in this way, namely, we show
that the derivational entropy of the gram-
mar takes the same value as the cross-
entropy between the input distribution and
the grammar itself. We show that the re-
sult also holds for the widely applied max-
imum likelihood estimator on tree banks.
</bodyText>
<sectionHeader confidence="0.997321" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.968611108695652">
Probabilistic context-free grammars are able to de-
scribe hierarchical, tree-shaped structures underly-
ing sentences, and are widely used in statistical nat-
ural language processing; see for instance (Collins,
2003) and references therein. Probabilistic context-
free grammars seem also more suitable than finite-
state devices for language modeling, and several
language models based on these grammars have
been recently proposed in the literature; see for in-
stance (Chelba and Jelinek, 1998), (Charniak, 2001)
and (Roark, 2001).
Empirical estimation of probabilistic context-free
grammars is usually carried out on tree banks, that
is, finite samples of parse trees, through the max-
imization of the likelihood of the sample itself. It
is well-known that this method also minimizes the
cross-entropy between the probability distribution
induced by the tree bank, also called the empirical
distribution, and the tree probability distribution in-
duced by the estimated grammar.
In this paper we generalize the maximum like-
lihood method, proposing an estimation technique
that works on any unrestricted tree distribution de-
fined over an infinite set of trees. This generalization
is theoretically appealing, and allows us to prove un-
expected properties of the already mentioned maxi-
mum likelihood estimator for tree banks, that were
not previously known in the literature on statistical
natural language parsing. More specifically, we in-
vestigate the following information theoretic quanti-
ties
• the cross-entropy between the unrestricted tree
distribution given as input and the tree distri-
bution induced by the estimated probabilistic
context-free grammar; and
• the derivational entropy of the estimated prob-
abilistic context-free grammar.
These two quantities are usually unrelated. We show
that these two quantities take the same value when
the probabilistic context-free grammar is trained us-
ing the minimal cross-entropy criterion. We then
translate back this property to the method of max-
imum likelihood estimation. Our general estima-
tion method also has practical applications in cases
one uses a probabilistic context-free grammar to ap-
proximate strictly more powerful rewriting systems,
</bodyText>
<page confidence="0.976369">
335
</page>
<note confidence="0.9953525">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 335–342,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999738827586207">
as for instance probabilistic tree adjoining gram-
mars (Schabes, 1992).
Not much is found in the literature about the
estimation of probabilistic grammars from infinite
distributions. This line of research was started
in (Nederhof, 2005), investigating the problem of
training an input probabilistic finite automaton from
an infinite tree distribution specified by means of an
input probabilistic context-free grammar. The prob-
lem we consider in this paper can then be seen as
a generalization of the above problem, where the in-
put model to be trained is a probabilistic context-free
grammar and the input distribution is an unrestricted
tree distribution. In (Chi, 1999) an estimator that
maximizes the likelihood of a probability distribu-
tion defined over a finite set of trees is introduced,
as a generalization of the maximum likelihood es-
timator. Again, the problems we consider here can
be thought of as generalizations of such estimator to
the case of distributions over infinite sets of trees or
sentences.
The remainder of this paper is structured as fol-
lows. Section 2 introduces the basic notation and
definitions and Section 3 discusses our new esti-
mation method. Section 4 presents our main re-
sult, which is transferred in Section 5 to the method
of maximum likelihood estimation. Section 6 dis-
cusses some simple examples, and Section 7 closes
with some further discussion.
</bodyText>
<sectionHeader confidence="0.965222" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.714106">
Throughout this paper we use standard notation and
</subsectionHeader>
<bodyText confidence="0.998889571428571">
definitions from the literature on formal languages
and probabilistic grammars, which we briefly sum-
marize below. We refer the reader to (Hopcroft and
Ullman, 1979) and (Booth and Thompson, 1973) for
a more precise presentation.
A context-free grammar (CFG) is a tuple G =
(N, Σ, R, S), where N is a finite set of nonterminal
symbols, Σ is a finite set of terminal symbols dis-
joint from N, S E N is the start symbol and R is a
finite set of rules. Each rule has the form A —* α,
where A E N and α E (Σ U N)∗. We denote by
L(G) and T(G) the set of all strings, resp., trees,
generated by G. For t E T(G), the yield of t is
denoted by y(t).
</bodyText>
<subsectionHeader confidence="0.792714">
For a nonterminal A and a string α, we write
</subsectionHeader>
<bodyText confidence="0.993141272727273">
f(A, α) to denote the number of occurrences of A
in α. For a rule (A —* α) E R and a tree t E T(G),
f(A —* α, t) denotes the number of occurrences of
A —* α in t. We let f(A, t) = Eα f(A —* α, t).
A probabilistic context-free grammar (PCFG) is
a pair !9 = (G, pG), with G a CFG and pG a func-
tion from R to the real numbers in the interval [0, 1].
A PCFG is proper if for every A E N we have
Eα pG(A —* α) = 1. The probability of t E T (G)
is the product of the probabilities of all rules in t,
counted with their multiplicity, that is,
</bodyText>
<equation confidence="0.988881">
pG(t) = ri pG(A —* α)f(A→α,t). (1)
A→α
</equation>
<bodyText confidence="0.927483">
The probability of w E L(G) is the sum of the prob-
abilities of all the trees that generate w, that is,
</bodyText>
<equation confidence="0.986676666666667">
pG(w) = � pG(t). (2)
Y(t)=w
A PCFG is consistent if EtET(G) pG(t) = 1.
</equation>
<bodyText confidence="0.99469675">
In this paper we write log for logarithms in base 2
and ln for logarithms in the natural base e. We also
assume 0 · log 0 = 0. We write Ep to denote the
expectation operator under distribution p. In case !9
is proper and consistent, we can define the deriva-
tional entropy of !9 as the expectation of the infor-
mation of parse trees in T(G), computed under dis-
tribution pG, that is,
</bodyText>
<equation confidence="0.999870333333333">
Hd(pG) = EpG log
�= − pG(t) · log pG(t). (3)
tET (G)
</equation>
<bodyText confidence="0.71653">
Similarly, for each A E N we also define the non-
terminal entropy of A as
</bodyText>
<equation confidence="0.9731026">
HA(pG) =
1
= EpG log pG(A —* α)
pG(A —* α) · log pG(A —* α). (4)
α
</equation>
<sectionHeader confidence="0.816206" genericHeader="method">
3 Estimation based on cross-entropy
</sectionHeader>
<bodyText confidence="0.998582666666667">
Let T be an infinite set of (finite) trees with inter-
nal nodes labeled by symbols in N, root nodes la-
beled by S E N and leaf nodes labeled by symbols
</bodyText>
<equation confidence="0.974729428571429">
1
pG(t)
�= −
336
E
= λA −
t∈T
=
λA −E
t∈T
λA −E
t∈T
1
· ln(2) ·
</equation>
<bodyText confidence="0.9841256">
in Σ. We assume that the set of rules that are ob-
served in the trees in T is drawn from some finite set
R. Let pT be a probability distribution defined over
T, that is, a function from T to set [0, 1] such that
Et∈T pT (t) = 1.
The skeleton CFG underlying T is defined as
G = (N, Σ, R, S). Note that we have T ⊆ T(G)
and, in the general case, there might be trees in T(G)
that do not appear in T. We wish anyway to approx-
imate distribution pT the best we can, by turning
G into some proper PCFG G = (G, pG) and set-
ting parameters pG(A → α) appropriately, for each
(A → α) ∈ R.
One possible criterion is to choose pG in such a
way that the cross-entropy between pT and pG is
minimized, where we now view pG as a probability
distribution defined over T(G). The cross-entropy
between pT and pG is defined as the expectation un-
der distribution pT of the information, computed un-
der distribution pG, of the trees in T(G)
</bodyText>
<equation confidence="0.998458724137931">
∂
pT (t) · ∂pG(A → α) log pG(t)
∂
pT (t) ·
∂pG(A → α)
pT(t) · f(A → α, t) ·
1
pG(A → α)
log � pG(B → β)f(B→β,t)
(B→β)∈R
λA −E ∂
t∈T pT (t) ·
∂pG(A → α)
E f(B → β, t) · log pG(B → β)
(B→β)∈R
λA −E pT (t) · E f(B → β, t) ·
t∈T (B→β)∈R
∂ log pG(B → β)
∂pG(A → α)
1
H(pT  ||pG) = EpT log
λA −
ln(2)
1 ·
1
pG(A → α) ·
pG(t)
E= − pT (t) · log pG(t). (5)
t∈T
</equation>
<bodyText confidence="0.993388166666667">
Since G should be proper, the minimization of (5) is
subject to the constraints Eα pG(A → α) = 1, for
each A ∈ N.
To solve the minimization problem above, we use
Lagrange multipliers λA for each A ∈ N and define
the form
</bodyText>
<equation confidence="0.99320525">
∇ = E λA · ( E pG(A → α) − 1) +
A∈N α
E− pT (t) · log pG(t). (6)
t∈T
</equation>
<bodyText confidence="0.999747333333333">
We now view ∇ as a function of all the λA and the
pG(A → α), and consider all the partial derivatives
of ∇. For each A ∈ N we have
</bodyText>
<equation confidence="0.998866">
pG(A → α) − 1.
For each (A → α) ∈ R we have
∂∇ =
∂pG (A → α)
∂ λA − a E pT(t) · log pG(t)
∂pG (A ) t∈T
E· pT (t) · f(A → α, t)
t∈T
λA −
ln(2)
1 · pG(A → α) ·
1
·EpT f(A → α, t).
</equation>
<bodyText confidence="0.998075333333333">
We now need to solve a system of |N|+|R |equa-
tions obtained by setting to zero all of the above par-
tial derivatives. From each equation ∂∇
</bodyText>
<equation confidence="0.982426029411765">
∂pG(A→α) = 0
we obtain
λA · ln(2) · pG(A → α) =
= EpT f(A → α, t). (7)
We sum over all strings α such that (A → α) ∈ R
λA · ln(2) · E pG(A → α) =
α
E= EpT f(A → α, t)
α
E= E pT(t) · f(A → α, t)
α t∈T
E= pT (t) · E f(A → α, t)
t∈T α
E= pT (t) · f(A, t)
t∈T
=
=
=
= EpT f(A, t). (8)
E=
α
∂∇
∂λA
337
From each equation ∂�
∂λA = 0 we obtain
Pα
pG(A → α) = 1 for each A ∈ N (our origi-
nal constraints). Combining with (8) we obtain
λA · ln(2) = EpT f(A, t). (9)
Replacing (9) into (7) we obtain, for every rule
(A → α) ∈ R,
EpT f(A → α,t)
pG(A → α) = EpT f(A, t) . (10)
</equation>
<bodyText confidence="0.997363227272727">
The equations in (10) define the desired estimator
for our PCFG, assigning to each rule A → α a prob-
ability specified as the ratio between the expected
number of A → α and the expected number of A,
under the distribution pT. We remark here that the
minimization of the cross-entropy above is equiva-
lent to the minimization of the Kullback-Leibler dis-
tance between pT and pG, viewed as tree distribu-
tions. Also, note that the likelihood of an infinite set
of derivations would always be zero and therefore
cannot be considered here.
To be used in the next section, we now show that
the PCFG G obtained as above is consistent. The
line of our argument below follows a proof provided
in (Chi and Geman, 1998) for the maximum like-
lihood estimator based on finite tree distributions.
Without loss of generality, we assume that in G the
start symbol S is never used in the right-hand side
of a rule.
For each A ∈ N, let qA be the probability that a
derivation in G rooted in A fails to terminate. We
can then write
</bodyText>
<equation confidence="0.9904705">
qA ≤ X XqB · pG(A → α)f(B, α).(11)
BEN α
</equation>
<bodyText confidence="0.979599333333333">
The inequality follows from the fact that the events
considered in the right-hand side of (11) are not mu-
tually exclusive. Combining (10) and (11) we obtain
</bodyText>
<equation confidence="0.980513">
qA · EpT f(A, t) ≤
≤ X XqB · EpT f(A → α, t)f(B, α).
BEN α
</equation>
<bodyText confidence="0.9644021">
Summing over all nonterminals we have
where f,(B, t) indicates the number of times a node
labeled by nonterminal B appears in the derivation
tree t as a child of some other node.
From our assumptions on the start symbol S, we
have that S only appears at the root of the trees
in T(G). Then it is easy to see that, for every
A =6 S, we have EpT f,(A, t) = EpT f(A, t), while
EpT f,(S, t) = 0 and EpT f(S, t) = 1. Using these
relations in (12) we obtain
</bodyText>
<equation confidence="0.759703">
qS · EpT f(S, T) ≤ qS · EpT f,(S, T),
</equation>
<bodyText confidence="0.995979">
from which we conclude qS = 0, thus implying the
consistency of G.
</bodyText>
<sectionHeader confidence="0.966079" genericHeader="method">
4 Cross-entropy and derivational entropy
</sectionHeader>
<bodyText confidence="0.9995311">
In this section we present the main result of the pa-
per. We show that, when G = (G, pG) is estimated
by minimizing the cross-entropy in (5), then such
cross-entropy takes the same value as the deriva-
tional entropy of G, defined in (3).
In (Nederhof and Satta, 2004) relations are de-
rived for the exact computation of Hd(pG). For later
use, we report these relations below, under the as-
sumption that G is consistent (see Section 3). We
have
</bodyText>
<equation confidence="0.960791">
Hd(pG) = X outq(A) · HA(pG). (13)
AEN
</equation>
<bodyText confidence="0.991211666666667">
Quantities HA(pG), A ∈ N, have been defined
in (4). For each A ∈ N, quantity outq(A) is the sum
of the probabilities of all trees generated by G, hav-
ing root labeled by S and having a yield composed
of terminal symbols with an unexpanded occurrence
of nonterminal A. Again, we assume that symbol
S does not appear in any of the right-hand sides of
the rules in R. This means that S only appears at
the root of the trees in T(G). Under this condi-
tion, quantities outq(A) can be exactly computed
by solving the following system of linear equations
(see also (Nederhof, 2005))
</bodyText>
<equation confidence="0.998663833333333">
for each A =6 S
outq(A) X =
=
B→β
X qA · EpT f(A, t) ≤
AEN
≤ X XqB · X EpT f(A → α, t)f(B, α)
BEN AEN α
X= qB · EpT f,(B, t), (12)
BEN
outq(S) = 1; (14)
outq(B) · f(A, β) · pG(B → β).(15)
</equation>
<page confidence="0.97878">
338
</page>
<bodyText confidence="0.986724">
We can now prove the equality
</bodyText>
<equation confidence="0.99959">
Hd(pG) = H(pT  ||pG), (16)
</equation>
<bodyText confidence="0.999994">
where G is the PCFG estimated by minimizing the
cross-entropy in (5), as described in Section 3.
We start from the definition of cross-entropy
</bodyText>
<equation confidence="0.9991095">
H(pT  ||pG) =
X= − pT(t) · log pG(t)
tET
X= − pT(t) · log Y pG(A → α)f(A→α,t)
tET A→α
X= − pT(t) ·
tET
X· f(A → α, t) · log pG(A → α)
A→α
X= − log pG(A → α) ·
</equation>
<bodyText confidence="0.804422">
5, we have
</bodyText>
<equation confidence="0.994914">
EpT f(5, t) = 1. (21)
</equation>
<bodyText confidence="0.9985065">
We now observe that, for any A ∈ N with A =6 5
and any t ∈ T (G), we have
</bodyText>
<equation confidence="0.994851714285714">
f(A,t) X= f(B → Q, t) · f(A, Q). (22)
=
B→β
For each A ∈ N with A =6 5 we can then write
EpT f(A, t) =
X= pT (t) · f(A, t)
tET
X= pT(t) · X f(B → Q, t) · f(A, Q)
tET B→β
X= X pT (t) · f(B → Q, t) · f(A, Q)
B→β tET
A→α pT(t) · f(A → α, t) X= EpT f(B → Q, t) · f(A, Q). (23)
X· B→β
tET
</equation>
<bodyText confidence="0.964668">
Once more we use relation (18), which replaced
in (23) provides
</bodyText>
<equation confidence="0.982288368421053">
EpT f(A, t) =
From our estimator in (10) we can write X= EpT f(B, t) ·
B→β
X= − log pG(A → α) ·
A→α
·EpT f(A → α, t). (17)
EpT f(A → α, t) =
= pG(A → α) · EpT f(A, t). (18)
Replacing (18) into (17) gives
H(pT  ||pG) =
X= − log pG(A → α) ·
A→α
·pG(A → α) · EpT f(A, t)
X= − EpT f(A, t) ·
AEN
X· pG(A → α) · log pG(A → α)
α
X= EpT f(A, t) · H(pG, A). (19)
AEN
</equation>
<bodyText confidence="0.996643">
Comparing (19) with (13) we see that, in order to
prove the equality in (16), we need to show relations
</bodyText>
<equation confidence="0.97699">
EpT f(A, t) = outg(A), (20)
</equation>
<bodyText confidence="0.9793805">
for every A ∈ N. We have already observed in Sec-
tion 3 that, under our assumption on the start symbol
</bodyText>
<equation confidence="0.710519">
·f(A, Q) · pG(B → Q). (24)
</equation>
<bodyText confidence="0.999857833333333">
Notice that the linear system in (14) and (15) and the
linear system in (21) and (24) are the same. Thus we
conclude that quantities EpT f(A, t) and outg(A)
are the same for each A ∈ N. This completes our
proof of the equality in (16). Some examples will be
discussed in Section 6.
Besides its theoretical significance, the equality
in (16) can also be exploited in the computation of
the cross-entropy in practical applications. In fact,
cross-entropy is used as a measure of tightness in
comparing different models. In case of estimation
from an infinite distribution pT, the definition of the
cross-entropy H(pT  ||pG) contains an infinite sum-
mation, which is problematic for the computation of
such quantity. In standard practice, this problem is
overcome by generating a finite sample T(n) of large
size n, through the distribution pT, and then comput-
ing the approximation (Manning and Sch¨utze, 1999)
</bodyText>
<equation confidence="0.996123666666667">
H(pT  ||pG) ∼ − n
1 X
tET f(t,T(n)) · log pG(t),
</equation>
<page confidence="0.992199">
339
</page>
<bodyText confidence="0.999896666666667">
where f(t, T(n)) indicates the multiplicity, that is,
the number of occurrences, of t in T(n). However, in
practical applications n must be very large in order
to have a small error. Based on the results in this
section, we can instead compute the exact value of
H(pT  ||pG) by computing the derivational entropy
Hd(pG), using relation (13) and solving the linear
system in (14) and (15), which takes cubic time in
the number of nonterminals of the grammar.
</bodyText>
<sectionHeader confidence="0.76099" genericHeader="method">
5 Estimation based on likelihood
</sectionHeader>
<bodyText confidence="0.999386733333333">
In natural language processing applications, the es-
timation of a PCFG is usually carried out on the ba-
sis of a finite sample of trees, called tree bank. The
so-called maximum likelihood estimation (MLE)
method is exploited, which maximizes the likeli-
hood of the observed data. In this section we show
that the MLE method is a special case of the esti-
mation method presented in Section 3, and that the
results of Section 4 also hold for the MLE method.
Let T be a tree sample, and let T be the under-
lying set of trees. For t ∈ T, we let f(t, T) be the
multiplicity of t in T . We define
and let f(A, T ) = Eα f(A → α, T ). We can in-
duce from T a probability distribution pT , defined
over T, by letting for each t ∈ T
</bodyText>
<equation confidence="0.999769">
pT (t) = f(t, T )
|T  |. (26)
</equation>
<bodyText confidence="0.998985090909091">
Note that &amp;∈T pT (t) = 1. Distribution pT is called
the empirical distribution of T .
Assume that the trees in T have internal nodes
labeled by symbols in N, root nodes labeled by
5 and leaf nodes labeled by symbols in Σ. Let
also R be the finite set of rules that are observed
in T . We define the skeleton CFG underlying T as
G = (N, Σ, R, 5). In the MLE method we proba-
bilistically extend the skeleton CFG G by means of
a function pG that maximizes the likelihood of T ,
defined as
</bodyText>
<equation confidence="0.929524">
pG(T) = H pG(t)f(t,T), (27)
t∈T
</equation>
<bodyText confidence="0.998899666666667">
subject to the usual properness conditions on pG.
Such maximization provides the estimator (see for
instance (Chi and Geman, 1998))
</bodyText>
<equation confidence="0.9949445">
pG(A → α) = f(A → α, T) (28)
f(A,T)
</equation>
<bodyText confidence="0.996010666666667">
Let us consider the estimator in (10). If we replace
distribution pT with our empirical distribution pT ,
we derive
</bodyText>
<equation confidence="0.98521">
pG(A → α) =
EpT f(A → α, t)
f(A,T ) .
(29)
</equation>
<bodyText confidence="0.9988663125">
This is precisely the estimator in (28).
From relation (29) we conclude that the MLE
method can be seen as a special case of the general
estimator in Section 3, with the input distribution de-
fined over a finite set of trees. We can also derive
the well-known fact that, in the finite case, the maxi-
mization of the likelihood pG(T) corresponds to the
minimization of the cross-entropy H(pT  ||pG).
Let now G = (G, pG) be a PCFG trained on T us-
ing the MLE method. Again from relation (29) and
Section 3 we have that G is consistent. This result
has been firstly shown in (Chaudhuri et al., 1983)
and later, with a different proof technique, in (Chi
and Geman, 1998). We can then transfer the results
of Section 4 to the supervised MLE method, show-
ing the equality
</bodyText>
<equation confidence="0.998908">
Hd(pG) = H(pT  ||pG). (30)
</equation>
<bodyText confidence="0.999512666666667">
This result was not previously known in the litera-
ture on statistical parsing of natural language. Some
examples will be discussed in Section 6.
</bodyText>
<sectionHeader confidence="0.945945" genericHeader="method">
6 Some examples
</sectionHeader>
<bodyText confidence="0.999155666666667">
In this section we discuss a simple example with the
aim of clarifying the theoretical results in the previ-
ous sections. For a real number q with 0 &lt; q &lt; 1,
</bodyText>
<equation confidence="0.999773230769231">
f(A → α,T ) =
11 = f(t, T) · f(A → α, t), (25)
t∈T
EpT f(A, t)
�
f(t,T )
|T  |· f(A → α, t)
t∈T
&amp;∈T f (TT) · f (A, t)
&amp;∈T f(t, T ) · f(A → α, t)
�
t∈T f(t, T ) · f(A, t)
f(A → α, T)
</equation>
<page confidence="0.99675">
340
</page>
<figureCaption confidence="0.8163155">
Figure 1: Derivational entropy of Gq and cross-
entropies for three different corpora.
</figureCaption>
<bodyText confidence="0.99910375">
consider the CFG G defined by the two rules S →
aS and S → a, and let Gq = (G, pG,q) be the proba-
bilistic extension of G with pG,q(S → aS) = q and
pG,q(S → a) = 1 − q. This grammar is unambigu-
ous and consistent, and each tree t generated by G
has probability pG,q(t) = qi · (1 − q), where i ≥ 0
is the number of occurrences of rule S → aS in t.
We use below the following well-known relations
</bodyText>
<table confidence="0.8652965">
(0 &lt; r &lt; 1) = 1
+∞E =
ri
i=0
+∞E
i · ri−1
i=1
1 − r ,
1
(1 − r)2 .
</table>
<bodyText confidence="0.9779182">
The derivational entropy of Gq can be directly
computed from its definition as
See Figure 1 for a plot of Hd(pG,q) as a function
of q.
If a tree bank is given, composed of occurrences
of trees generated by G, the value of q can be es-
timated by applying the MLE or, equivalently, by
minimizing the cross-entropy. We consider here sev-
eral tree banks, to exemplify the behaviour of the
cross-entropy depending on the structure of the sam-
ple of trees. The first tree bank T contains a single
tree t with a single occurrence of rule S → aS and
a single occurrence of rule S → a. We then have
pT (t) = 1 and pG,q(t) = q · (1 − q). The cross-
entropy between distributions pT and pG,q is then
</bodyText>
<equation confidence="0.6881005">
H(pT , pG,q) = − log q · (1 − q)
= − log q − log(1 − q). (34)
</equation>
<bodyText confidence="0.965569388888889">
The cross-entropy H(pT ,pG,q), viewed as a func-
tion of q, is a convex-∪ function and is plotted in
Figure 1 (line indicated by Kd = 1, see below). We
can obtain its minimum by finding a zero for the first
derivative
d 1 1
H(pT , pG,q) = −+
dq q 1 − q
which gives q = 0.5. Note from Figure 1 that
the minimum of H(pT ,pG,q) crosses the line cor-
responding to the derivational entropy, as should be
expected from the result in Section 4.
More in general, for integers d &gt; 0 and K &gt; 0,
consider a tree sample Td,K consisting of d trees ti,
1 ≤ i ≤ d. Each ti contains ki ≥ 0 occurrences
of rule S → aS and one occurrence of rule S → a.
Thus we have pTd,K(ti) = 1dand pG,q(ti) = qki ·
(1 − q). We let Edi=1 ki = K. The cross-entropy is
</bodyText>
<equation confidence="0.954627333333333">
H(pTd,K,pG,q) =
1k
d · log qi −
log (1 − q)
K
= −d log q − log(1 − q). (36)
</equation>
<bodyText confidence="0.9999092">
In Figure 1 we plot H(pTd,K,pG,q) in the case Kd =
0.5 and in the case Kd = 1.5. Again, we have that
these curves intersect with the curve corresponding
to the derivational entropy Hd(pG,q) at the points
were they take their minimum values.
</bodyText>
<equation confidence="0.9521006">
2q − 1
=
q · (1 − q)
= 0, (35)
Hd(pG,q) = − +∞E ( )
i=0 qi · (1 − q) · log qi · (1 − q)
= −(1 − q) +∞E qi log qi +
i=0
−(1 − q) · log(1 − q) · +∞E qi
i=0
= −(1 − q) · log q ·
+∞E
i=0
q
· log q − log(1 − q). (33)
i · qi − log(1 − q)
1 − q
d
E
i=0
</equation>
<page confidence="0.995949">
341
</page>
<sectionHeader confidence="0.997612" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999971918918919">
We have shown in this paper that, when a PCFG is
estimated from some tree distribution by minimiz-
ing the cross-entropy, then the cross-entropy takes
the same value as the derivational entropy of the
PCFG itself. As a special case, this result holds for
the maximum likelihood estimator, widely applied
in statistical natural language parsing. The result
also holds for the relative weighted frequency esti-
mator introduced in (Chi, 1999) as a generalization
of the maximum likelihood estimator, and for the es-
timator introduced in (Nederhof, 2005) already dis-
cussed in the introduction. In a journal version of the
present paper, which is under submission, we have
also extended the results of Section 4 to the unsuper-
vised estimation of a PCFG from a distribution de-
fined over an infinite set of (unannotated) sentences
and, as a particular case, to the well-knonw inside-
outside algorithm (Manning and Sch¨utze, 1999).
In practical applications, the results of Section 4
can be exploited in the computation of model tight-
ness. In fact, cross-entropy indicates how much the
estimated model fits the observed data, and is com-
monly exploited in comparison of different models
on the same data set. We can then use the given
relation between cross-entropy and derivational en-
tropy to compute one of these two quantities from
the other. For instance, in the case of the MLE
method we can choose between the computation of
the derivational entropy and the cross-entropy, de-
pending basically on the instance of the problem at
hand. As already mentioned, the computation of the
derivational entropy requires cubic time in the num-
ber of nonterminals of the grammar. If this num-
ber is large, direct computation of (5) on the corpus
might be more efficient. On the other hand, if the
corpus at hand is very large, one might opt for direct
computation of (3).
</bodyText>
<sectionHeader confidence="0.993568" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.906697333333333">
Helpful comments from Zhiyi Chi, Alberto lavelli,
Mark-Jan Nederhof and Khalil Simaan are grate-
fully acknowledged.
</bodyText>
<sectionHeader confidence="0.946084" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999760553191489">
T.L. Booth and R.A. Thompson. 1973. Applying prob-
abilistic measures to abstract languages. IEEE Trans-
actions on Computers, C-22(5):442–450, May.
E. Charniak. 2001. Immediate-head parsing for language
models. In 39th Annual Meeting and 10th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Proceedings of the Conference,
pages 116–123, Toulouse, France, July.
R. Chaudhuri, S. Pham, and O. N. Garcia. 1983. Solution
of an open problem on probabilistic grammars. IEEE
Transactions on Computers, 32(8):748–750.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. In 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics, volume 1, pages 225–231, Montreal, Que-
bec, Canada, August.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguistics,
24(2):299–305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131–160.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
pages 589–638.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
C.D. Manning and H. Sch¨utze. 1999. Foundations
of Statistical Natural Language Processing. Mas-
sachusetts Institute of Technology.
M.-J. Nederhof and G. Satta. 2004. Kullback-Leibler
distance between probabilistic context-free grammars
and probabilistic finite automata. In Proc. of the 20th
COLING, volume 1, pages 71–77, Geneva, Switzer-
land.
M.-J. Nederhof. 2005. A general technique to train lan-
guage models on language models. Computational
Linguistics, 31(2):173–185.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of the fifteenth International
Conference on Computational Linguistics, volume 2,
pages 426–432, Nantes, August.
</reference>
<page confidence="0.998258">
342
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.443742">
<title confidence="0.99972">Cross-Entropy and Estimation Probabilistic Context-Free Grammars</title>
<author confidence="0.988167">Anna</author>
<affiliation confidence="0.848121666666667">Department of University “Federico via</affiliation>
<address confidence="0.911425">I-80126 Napoli,</address>
<email confidence="0.998752">corazza@na.infn.it</email>
<author confidence="0.991408">Giorgio</author>
<affiliation confidence="0.988431333333333">Department of Information University of via Gradenigo,</affiliation>
<address confidence="0.990638">I-35131 Padova,</address>
<email confidence="0.999401">satta@dei.unipd.it</email>
<abstract confidence="0.998304705882353">We investigate the problem of training probabilistic context-free grammars on the basis of a distribution defined over an infinite set of trees, by minimizing the cross-entropy. This problem can be seen as a generalization of the well-known maximum likelihood estimator on (finite) tree banks. We prove an unexpected theoretical property of grammars that are trained in this way, namely, we show that the derivational entropy of the grammar takes the same value as the crossentropy between the input distribution and the grammar itself. We show that the result also holds for the widely applied maximum likelihood estimator on tree banks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T L Booth</author>
<author>R A Thompson</author>
</authors>
<title>Applying probabilistic measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="4999" citStr="Booth and Thompson, 1973" startWordPosition="750" endWordPosition="753">nder of this paper is structured as follows. Section 2 introduces the basic notation and definitions and Section 3 discusses our new estimation method. Section 4 presents our main result, which is transferred in Section 5 to the method of maximum likelihood estimation. Section 6 discusses some simple examples, and Section 7 closes with some further discussion. 2 Preliminaries Throughout this paper we use standard notation and definitions from the literature on formal languages and probabilistic grammars, which we briefly summarize below. We refer the reader to (Hopcroft and Ullman, 1979) and (Booth and Thompson, 1973) for a more precise presentation. A context-free grammar (CFG) is a tuple G = (N, Σ, R, S), where N is a finite set of nonterminal symbols, Σ is a finite set of terminal symbols disjoint from N, S E N is the start symbol and R is a finite set of rules. Each rule has the form A —* α, where A E N and α E (Σ U N)∗. We denote by L(G) and T(G) the set of all strings, resp., trees, generated by G. For t E T(G), the yield of t is denoted by y(t). For a nonterminal A and a string α, we write f(A, α) to denote the number of occurrences of A in α. For a rule (A —* α) E R and a tree t E T(G), f(A —* α, t</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>T.L. Booth and R.A. Thompson. 1973. Applying probabilistic measures to abstract languages. IEEE Transactions on Computers, C-22(5):442–450, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In 39th Annual Meeting and 10th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>116--123</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="1480" citStr="Charniak, 2001" startWordPosition="215" endWordPosition="216">w that the result also holds for the widely applied maximum likelihood estimator on tree banks. 1 Introduction Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical natural language processing; see for instance (Collins, 2003) and references therein. Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001). Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, that is, finite samples of parse trees, through the maximization of the likelihood of the sample itself. It is well-known that this method also minimizes the cross-entropy between the probability distribution induced by the tree bank, also called the empirical distribution, and the tree probability distribution induced by the estimated grammar. In this paper we generalize the maximum likelihood method, proposing an estimation technique that works on any unrestricted tree distri</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. In 39th Annual Meeting and 10th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, pages 116–123, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chaudhuri</author>
<author>S Pham</author>
<author>O N Garcia</author>
</authors>
<title>Solution of an open problem on probabilistic grammars.</title>
<date>1983</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>32</volume>
<issue>8</issue>
<contexts>
<context position="17798" citStr="Chaudhuri et al., 1983" startWordPosition="3490" endWordPosition="3493">EpT f(A → α, t) f(A,T ) . (29) This is precisely the estimator in (28). From relation (29) we conclude that the MLE method can be seen as a special case of the general estimator in Section 3, with the input distribution defined over a finite set of trees. We can also derive the well-known fact that, in the finite case, the maximization of the likelihood pG(T) corresponds to the minimization of the cross-entropy H(pT ||pG). Let now G = (G, pG) be a PCFG trained on T using the MLE method. Again from relation (29) and Section 3 we have that G is consistent. This result has been firstly shown in (Chaudhuri et al., 1983) and later, with a different proof technique, in (Chi and Geman, 1998). We can then transfer the results of Section 4 to the supervised MLE method, showing the equality Hd(pG) = H(pT ||pG). (30) This result was not previously known in the literature on statistical parsing of natural language. Some examples will be discussed in Section 6. 6 Some examples In this section we discuss a simple example with the aim of clarifying the theoretical results in the previous sections. For a real number q with 0 &lt; q &lt; 1, f(A → α,T ) = 11 = f(t, T) · f(A → α, t), (25) t∈T EpT f(A, t) � f(t,T ) |T |· f(A → α,</context>
</contexts>
<marker>Chaudhuri, Pham, Garcia, 1983</marker>
<rawString>R. Chaudhuri, S. Pham, and O. N. Garcia. 1983. Solution of an open problem on probabilistic grammars. IEEE Transactions on Computers, 32(8):748–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>225--231</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="1462" citStr="Chelba and Jelinek, 1998" startWordPosition="211" endWordPosition="214">d the grammar itself. We show that the result also holds for the widely applied maximum likelihood estimator on tree banks. 1 Introduction Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical natural language processing; see for instance (Collins, 2003) and references therein. Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001). Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, that is, finite samples of parse trees, through the maximization of the likelihood of the sample itself. It is well-known that this method also minimizes the cross-entropy between the probability distribution induced by the tree bank, also called the empirical distribution, and the tree probability distribution induced by the estimated grammar. In this paper we generalize the maximum likelihood method, proposing an estimation technique that works on any unrest</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>C. Chelba and F. Jelinek. 1998. Exploiting syntactic structure for language modeling. In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, volume 1, pages 225–231, Montreal, Quebec, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
<author>S Geman</author>
</authors>
<title>Estimation of probabilistic context-free grammars.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="10335" citStr="Chi and Geman, 1998" startWordPosition="1965" endWordPosition="1968">le A → α a probability specified as the ratio between the expected number of A → α and the expected number of A, under the distribution pT. We remark here that the minimization of the cross-entropy above is equivalent to the minimization of the Kullback-Leibler distance between pT and pG, viewed as tree distributions. Also, note that the likelihood of an infinite set of derivations would always be zero and therefore cannot be considered here. To be used in the next section, we now show that the PCFG G obtained as above is consistent. The line of our argument below follows a proof provided in (Chi and Geman, 1998) for the maximum likelihood estimator based on finite tree distributions. Without loss of generality, we assume that in G the start symbol S is never used in the right-hand side of a rule. For each A ∈ N, let qA be the probability that a derivation in G rooted in A fails to terminate. We can then write qA ≤ X XqB · pG(A → α)f(B, α).(11) BEN α The inequality follows from the fact that the events considered in the right-hand side of (11) are not mutually exclusive. Combining (10) and (11) we obtain qA · EpT f(A, t) ≤ ≤ X XqB · EpT f(A → α, t)f(B, α). BEN α Summing over all nonterminals we have w</context>
<context position="17009" citStr="Chi and Geman, 1998" startWordPosition="3336" endWordPosition="3339">istribution pT is called the empirical distribution of T . Assume that the trees in T have internal nodes labeled by symbols in N, root nodes labeled by 5 and leaf nodes labeled by symbols in Σ. Let also R be the finite set of rules that are observed in T . We define the skeleton CFG underlying T as G = (N, Σ, R, 5). In the MLE method we probabilistically extend the skeleton CFG G by means of a function pG that maximizes the likelihood of T , defined as pG(T) = H pG(t)f(t,T), (27) t∈T subject to the usual properness conditions on pG. Such maximization provides the estimator (see for instance (Chi and Geman, 1998)) pG(A → α) = f(A → α, T) (28) f(A,T) Let us consider the estimator in (10). If we replace distribution pT with our empirical distribution pT , we derive pG(A → α) = EpT f(A → α, t) f(A,T ) . (29) This is precisely the estimator in (28). From relation (29) we conclude that the MLE method can be seen as a special case of the general estimator in Section 3, with the input distribution defined over a finite set of trees. We can also derive the well-known fact that, in the finite case, the maximization of the likelihood pG(T) corresponds to the minimization of the cross-entropy H(pT ||pG). Let now</context>
</contexts>
<marker>Chi, Geman, 1998</marker>
<rawString>Z. Chi and S. Geman. 1998. Estimation of probabilistic context-free grammars. Computational Linguistics, 24(2):299–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="4021" citStr="Chi, 1999" startWordPosition="594" endWordPosition="595">es, 1992). Not much is found in the literature about the estimation of probabilistic grammars from infinite distributions. This line of research was started in (Nederhof, 2005), investigating the problem of training an input probabilistic finite automaton from an infinite tree distribution specified by means of an input probabilistic context-free grammar. The problem we consider in this paper can then be seen as a generalization of the above problem, where the input model to be trained is a probabilistic context-free grammar and the input distribution is an unrestricted tree distribution. In (Chi, 1999) an estimator that maximizes the likelihood of a probability distribution defined over a finite set of trees is introduced, as a generalization of the maximum likelihood estimator. Again, the problems we consider here can be thought of as generalizations of such estimator to the case of distributions over infinite sets of trees or sentences. The remainder of this paper is structured as follows. Section 2 introduces the basic notation and definitions and Section 3 discusses our new estimation method. Section 4 presents our main result, which is transferred in Section 5 to the method of maximum </context>
<context position="21535" citStr="Chi, 1999" startWordPosition="4307" endWordPosition="4308">−(1 − q) +∞E qi log qi + i=0 −(1 − q) · log(1 − q) · +∞E qi i=0 = −(1 − q) · log q · +∞E i=0 q · log q − log(1 − q). (33) i · qi − log(1 − q) 1 − q d E i=0 341 7 Conclusions We have shown in this paper that, when a PCFG is estimated from some tree distribution by minimizing the cross-entropy, then the cross-entropy takes the same value as the derivational entropy of the PCFG itself. As a special case, this result holds for the maximum likelihood estimator, widely applied in statistical natural language parsing. The result also holds for the relative weighted frequency estimator introduced in (Chi, 1999) as a generalization of the maximum likelihood estimator, and for the estimator introduced in (Nederhof, 2005) already discussed in the introduction. In a journal version of the present paper, which is under submission, we have also extended the results of Section 4 to the unsupervised estimation of a PCFG from a distribution defined over an infinite set of (unannotated) sentences and, as a particular case, to the well-knonw insideoutside algorithm (Manning and Sch¨utze, 1999). In practical applications, the results of Section 4 can be exploited in the computation of model tightness. In fact, </context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Z. Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing. Computational Linguistics,</title>
<date>2003</date>
<pages>589--638</pages>
<contexts>
<context position="1188" citStr="Collins, 2003" startWordPosition="172" endWordPosition="173">elihood estimator on (finite) tree banks. We prove an unexpected theoretical property of grammars that are trained in this way, namely, we show that the derivational entropy of the grammar takes the same value as the crossentropy between the input distribution and the grammar itself. We show that the result also holds for the widely applied maximum likelihood estimator on tree banks. 1 Introduction Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical natural language processing; see for instance (Collins, 2003) and references therein. Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001). Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, that is, finite samples of parse trees, through the maximization of the likelihood of the sample itself. It is well-known that this method also minimizes the cross-entropy between the probab</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, pages 589–638.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hopcroft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="4968" citStr="Hopcroft and Ullman, 1979" startWordPosition="745" endWordPosition="748">of trees or sentences. The remainder of this paper is structured as follows. Section 2 introduces the basic notation and definitions and Section 3 discusses our new estimation method. Section 4 presents our main result, which is transferred in Section 5 to the method of maximum likelihood estimation. Section 6 discusses some simple examples, and Section 7 closes with some further discussion. 2 Preliminaries Throughout this paper we use standard notation and definitions from the literature on formal languages and probabilistic grammars, which we briefly summarize below. We refer the reader to (Hopcroft and Ullman, 1979) and (Booth and Thompson, 1973) for a more precise presentation. A context-free grammar (CFG) is a tuple G = (N, Σ, R, S), where N is a finite set of nonterminal symbols, Σ is a finite set of terminal symbols disjoint from N, S E N is the start symbol and R is a finite set of rules. Each rule has the form A —* α, where A E N and α E (Σ U N)∗. We denote by L(G) and T(G) the set of all strings, resp., trees, generated by G. For t E T(G), the yield of t is denoted by y(t). For a nonterminal A and a string α, we write f(A, α) to denote the number of occurrences of A in α. For a rule (A —* α) E R a</context>
</contexts>
<marker>Hopcroft, Ullman, 1979</marker>
<rawString>J.E. Hopcroft and J.D. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<journal>Foundations of Statistical Natural Language</journal>
<institution>Processing. Massachusetts Institute of Technology.</institution>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C.D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Kullback-Leibler distance between probabilistic context-free grammars and probabilistic finite automata.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th COLING,</booktitle>
<volume>1</volume>
<pages>71--77</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="11759" citStr="Nederhof and Satta, 2004" startWordPosition="2254" endWordPosition="2257">only appears at the root of the trees in T(G). Then it is easy to see that, for every A =6 S, we have EpT f,(A, t) = EpT f(A, t), while EpT f,(S, t) = 0 and EpT f(S, t) = 1. Using these relations in (12) we obtain qS · EpT f(S, T) ≤ qS · EpT f,(S, T), from which we conclude qS = 0, thus implying the consistency of G. 4 Cross-entropy and derivational entropy In this section we present the main result of the paper. We show that, when G = (G, pG) is estimated by minimizing the cross-entropy in (5), then such cross-entropy takes the same value as the derivational entropy of G, defined in (3). In (Nederhof and Satta, 2004) relations are derived for the exact computation of Hd(pG). For later use, we report these relations below, under the assumption that G is consistent (see Section 3). We have Hd(pG) = X outq(A) · HA(pG). (13) AEN Quantities HA(pG), A ∈ N, have been defined in (4). For each A ∈ N, quantity outq(A) is the sum of the probabilities of all trees generated by G, having root labeled by S and having a yield composed of terminal symbols with an unexpanded occurrence of nonterminal A. Again, we assume that symbol S does not appear in any of the right-hand sides of the rules in R. This means that S only </context>
</contexts>
<marker>Nederhof, Satta, 2004</marker>
<rawString>M.-J. Nederhof and G. Satta. 2004. Kullback-Leibler distance between probabilistic context-free grammars and probabilistic finite automata. In Proc. of the 20th COLING, volume 1, pages 71–77, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>A general technique to train language models on language models.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="3587" citStr="Nederhof, 2005" startWordPosition="526" endWordPosition="527">ihood estimation. Our general estimation method also has practical applications in cases one uses a probabilistic context-free grammar to approximate strictly more powerful rewriting systems, 335 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 335–342, New York, June 2006. c�2006 Association for Computational Linguistics as for instance probabilistic tree adjoining grammars (Schabes, 1992). Not much is found in the literature about the estimation of probabilistic grammars from infinite distributions. This line of research was started in (Nederhof, 2005), investigating the problem of training an input probabilistic finite automaton from an infinite tree distribution specified by means of an input probabilistic context-free grammar. The problem we consider in this paper can then be seen as a generalization of the above problem, where the input model to be trained is a probabilistic context-free grammar and the input distribution is an unrestricted tree distribution. In (Chi, 1999) an estimator that maximizes the likelihood of a probability distribution defined over a finite set of trees is introduced, as a generalization of the maximum likelih</context>
<context position="12544" citStr="Nederhof, 2005" startWordPosition="2401" endWordPosition="2402">have Hd(pG) = X outq(A) · HA(pG). (13) AEN Quantities HA(pG), A ∈ N, have been defined in (4). For each A ∈ N, quantity outq(A) is the sum of the probabilities of all trees generated by G, having root labeled by S and having a yield composed of terminal symbols with an unexpanded occurrence of nonterminal A. Again, we assume that symbol S does not appear in any of the right-hand sides of the rules in R. This means that S only appears at the root of the trees in T(G). Under this condition, quantities outq(A) can be exactly computed by solving the following system of linear equations (see also (Nederhof, 2005)) for each A =6 S outq(A) X = = B→β X qA · EpT f(A, t) ≤ AEN ≤ X XqB · X EpT f(A → α, t)f(B, α) BEN AEN α X= qB · EpT f,(B, t), (12) BEN outq(S) = 1; (14) outq(B) · f(A, β) · pG(B → β).(15) 338 We can now prove the equality Hd(pG) = H(pT ||pG), (16) where G is the PCFG estimated by minimizing the cross-entropy in (5), as described in Section 3. We start from the definition of cross-entropy H(pT ||pG) = X= − pT(t) · log pG(t) tET X= − pT(t) · log Y pG(A → α)f(A→α,t) tET A→α X= − pT(t) · tET X· f(A → α, t) · log pG(A → α) A→α X= − log pG(A → α) · 5, we have EpT f(5, t) = 1. (21) We now observe t</context>
<context position="21645" citStr="Nederhof, 2005" startWordPosition="4324" endWordPosition="4325">log(1 − q). (33) i · qi − log(1 − q) 1 − q d E i=0 341 7 Conclusions We have shown in this paper that, when a PCFG is estimated from some tree distribution by minimizing the cross-entropy, then the cross-entropy takes the same value as the derivational entropy of the PCFG itself. As a special case, this result holds for the maximum likelihood estimator, widely applied in statistical natural language parsing. The result also holds for the relative weighted frequency estimator introduced in (Chi, 1999) as a generalization of the maximum likelihood estimator, and for the estimator introduced in (Nederhof, 2005) already discussed in the introduction. In a journal version of the present paper, which is under submission, we have also extended the results of Section 4 to the unsupervised estimation of a PCFG from a distribution defined over an infinite set of (unannotated) sentences and, as a particular case, to the well-knonw insideoutside algorithm (Manning and Sch¨utze, 1999). In practical applications, the results of Section 4 can be exploited in the computation of model tightness. In fact, cross-entropy indicates how much the estimated model fits the observed data, and is commonly exploited in comp</context>
</contexts>
<marker>Nederhof, 2005</marker>
<rawString>M.-J. Nederhof. 2005. A general technique to train language models on language models. Computational Linguistics, 31(2):173–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="1498" citStr="Roark, 2001" startWordPosition="218" endWordPosition="219">o holds for the widely applied maximum likelihood estimator on tree banks. 1 Introduction Probabilistic context-free grammars are able to describe hierarchical, tree-shaped structures underlying sentences, and are widely used in statistical natural language processing; see for instance (Collins, 2003) and references therein. Probabilistic contextfree grammars seem also more suitable than finitestate devices for language modeling, and several language models based on these grammars have been recently proposed in the literature; see for instance (Chelba and Jelinek, 1998), (Charniak, 2001) and (Roark, 2001). Empirical estimation of probabilistic context-free grammars is usually carried out on tree banks, that is, finite samples of parse trees, through the maximization of the likelihood of the sample itself. It is well-known that this method also minimizes the cross-entropy between the probability distribution induced by the tree bank, also called the empirical distribution, and the tree probability distribution induced by the estimated grammar. In this paper we generalize the maximum likelihood method, proposing an estimation technique that works on any unrestricted tree distribution defined ove</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic lexicalized tree-adjoining grammars.</title>
<date>1992</date>
<booktitle>In Proc. of the fifteenth International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>426--432</pages>
<location>Nantes,</location>
<contexts>
<context position="3420" citStr="Schabes, 1992" startWordPosition="501" endWordPosition="502"> when the probabilistic context-free grammar is trained using the minimal cross-entropy criterion. We then translate back this property to the method of maximum likelihood estimation. Our general estimation method also has practical applications in cases one uses a probabilistic context-free grammar to approximate strictly more powerful rewriting systems, 335 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 335–342, New York, June 2006. c�2006 Association for Computational Linguistics as for instance probabilistic tree adjoining grammars (Schabes, 1992). Not much is found in the literature about the estimation of probabilistic grammars from infinite distributions. This line of research was started in (Nederhof, 2005), investigating the problem of training an input probabilistic finite automaton from an infinite tree distribution specified by means of an input probabilistic context-free grammar. The problem we consider in this paper can then be seen as a generalization of the above problem, where the input model to be trained is a probabilistic context-free grammar and the input distribution is an unrestricted tree distribution. In (Chi, 1999</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Y. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proc. of the fifteenth International Conference on Computational Linguistics, volume 2, pages 426–432, Nantes, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>