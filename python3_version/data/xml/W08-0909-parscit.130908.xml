<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.060820">
<title confidence="0.998088">
An Analysis of Statistical Models and Features for Reading Difficulty
Prediction
</title>
<author confidence="0.99796">
Michael Heilman, Kevyn Collins-Thompson and Maxine Eskenazi
</author>
<affiliation confidence="0.877497333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.999545">
{mheilman,kct,max}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999138214285714">
A reading difficulty measure can be described
as a function or model that maps a text to a
numerical value corresponding to a difficulty
or grade level. We describe a measure of read-
ability that uses a combination of lexical fea-
tures and grammatical features that are derived
from subtrees of syntactic parses. We also
tested statistical models for nominal, ordinal,
and interval scales of measurement. The re-
sults indicate that a model for ordinal regres-
sion, such as the proportional odds model, us-
ing a combination of grammatical and lexical
features is most effective at predicting reading
difficulty.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999993019607843">
A reading difficulty, or readability, measure can be
described as a function or model that maps a text
to a numerical value corresponding to a difficulty or
grade level. Inputs to this function are usually statis-
tics for various lexical and grammatical features of
the text. The output is one of a set of ordered dif-
ficulty levels, usually corresponding to grade levels
for elementary school through high school. As such,
reading difficulty prediction can be viewed as a re-
gression of grade level on a set of textual features.
Early work on readability measures employed
simple proxies for grammatical and lexical complex-
ity, including sentence length and the number of syl-
lables in a word. Fairly simple features were often
employed because of a lack of computational power.
Such features exhibit high bias because they rely on
strong assumptions about what makes a text difficult
to read. For example, the use of sentence length as a
measure of grammatical complexity assumes that a
longer sentence is more grammatically complex than
a shorter one, which is often but not always the case.
In one early model, the Dale-Chall model (Dale and
Chall, 1948; Chall and Dale, 1995), reading diffi-
culty is a linear function of the mean sentence length
and the percentage of rare words, as defined by a list
of 3,000 words commonly known by 4th grade. In
this paper, sentence length is defined as the mean
number of words in the sentences of a text.
Many early measures did not employ direct esti-
mates of word frequency due to computational lim-
itations (e.g., (Gunning, 1952; McLaughlin, 1969;
Kincaid et al., 1975)). Instead, these measures relied
on the strong relationship between the frequency of
and the number of syllables in a word. More fre-
quent words are more likely to have fewer syllables
(e.g., “the”) than less frequent words (e.g., “vocab-
ulary”), an association that is related to Zipf’s Law
(Zipf, 1935). The Flesch-Kincaid measure (Kincaid
et al., 1975) is probably the most common reading
difficulty measure in use. It is implemented in com-
mon word processing programs. This measure is a
linear function of the mean number of syllables per
word and the mean number of words per sentence.
Klare (1974) provides a summary of other early
work on readability.
More recent approaches to reading difficulty em-
ploy more sophisticated models that make use of the
growth in computational power. The Lexile Frame-
work (e.g., (Stenner, 1996)) uses individual word
frequency estimates as a measure of lexical diffi-
culty. The word frequency estimates are derived
</bodyText>
<page confidence="0.979262">
71
</page>
<note confidence="0.739082">
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71–79,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999638158536586">
from a large, varied corpus of text. Lexile uses a
Rasch model (Rasch, 1980) with the mean log word
frequency as a lexical feature and the log of the mean
sentence length as a grammatical feature. The Rasch
model, related to logistic regression, is used to esti-
mate the level of a student that would comprehend
75% of a given text. The converted log odds ratio
called a “Lexile” that is used as part of this measure
can be easily mapped to grade school levels.
A reading difficulty measure developed by
Collins-Thompson and Callan (2005) uses
smoothed unigram language modeling to capture
the predictive ability of individual words based
on their frequency at each reading difficulty level.
Collins-Thompson and Callan found that certain
words were very predictive of certain levels. For
example, “grownup” was very predictive of grade
1, and “essay” was very predictive of grade 12. For
a given text, this measure estimates the likelihood
that the text was generated by each level’s language
model. The prediction is the level of the model with
the highest likelihood of generating the text. There
are no grammatical features.
Natural language processing techniques enable
more sophisticated grammatical analysis for read-
ing difficulty measures. Rather than using sentence
length as a proxy, measures can employ tools for au-
tomatic analysis of the syntactic structure of texts
(e.g., (Charniak, 2000)). A measure by Schwarm
and Ostendorf (2005) incorporates syntactic analy-
ses, among a variety of other types of features. It in-
cludes four grammatical features derived from syn-
tactic parses of text: the mean parse tree height, the
mean number of noun phrases, mean number of verb
phrases, and mean number of “SBARs.” “SBARs”
are non-terminal nodes that are associated with sub-
ordinate clauses. Their system led to better pre-
dictions than the Flesch-Kincaid and Lexile mea-
sures, but the predictive value of the grammatical
features is not entirely clear. In initial experiments
using such course-grain grammatical features alone,
rather than in conjunction with language modeling
and other features as in Schwarm and Ostendorf’s
system, we found relatively poor prediction perfor-
mance. Our final approach using subtrees of syn-
tactic parses allows for a finer level of discrimina-
tion that may support the detection of differences in
grade levels between texts that exhibit the same high
level features.
A reading difficulty measure developed by Heil-
man, Collins-Thompson, Callan, and Eskenazi
(2007) uses the frequency of grammatical construc-
tions as a measure of grammatical difficulty. A set
of approximately twenty constructions were selected
from English as a Second Language grammar text-
books. This set includes grammatical constructions
such as the passive voice, relative clauses, and vari-
ous verb tenses. The frequencies are used as features
for a nearest neighbor classification algorithm. The
unigram language modeling approach of Collins-
Thompson and Callan (2005) is used to estimate
lexical difficulty in this measure. The final predic-
tion is a linear function of the lexical and grammat-
ical components. That model assumes that gram-
matical difficulty is adequately captured by a small
number of constructions chosen according to de-
tailed knowledge of English grammar. In that work,
the constructions were selected from an English as
a Second Language grammar textbook, a labor- and
knowledge-intensive task that may be less practical
for other languages.
We aim to identify the appropriate scale of mea-
surement for reading difficulty–nominal, ordinal, or
interval–by comparing the effectiveness of statistical
models for each type of data. We also extend pre-
vious work combining lexical and grammatical fea-
tures (Heilman et al., 2007) by making it possible
to include a large number of grammatical features
derived from syntactic structures without requiring
significant linguistic or pedagogical content knowl-
edge, such as a reference guide for the grammar of
the language of interest.
</bodyText>
<sectionHeader confidence="0.68937" genericHeader="method">
2 Types of Features
</sectionHeader>
<subsectionHeader confidence="0.997052">
2.1 Lexical Features
</subsectionHeader>
<bodyText confidence="0.9999715">
This section and the following section describe the
lexical and grammatical features used in our read-
ing difficulty models. The lexical features are the
relative frequencies of word unigrams. The use of
word unigrams is a standard approach in text clas-
sification (Yang and Pedersen, 1997), and has also
been successfully used to predict reading difficulty
(Collins-Thompson and Callan, 2005). Higher order
n-grams such as bigrams and trigrams were not used
as features because they did not improve predictions
</bodyText>
<page confidence="0.996795">
72
</page>
<bodyText confidence="0.999875">
in preliminary tests. The specific set of lexical fea-
tures was chosen based on the frequencies of words
in the training corpus. The system performs mor-
phological stemming and stopword removal. The
remaining 5000 most common words comprised the
lexical feature set.
</bodyText>
<subsectionHeader confidence="0.999579">
2.2 Grammatical Features
</subsectionHeader>
<bodyText confidence="0.9999919">
Grammatical features are extracted from automatic
context-free grammar parses of sentences. The sys-
tem computes relative frequencies of partial syn-
tactic derivations, which will be called ’subtrees’
hereafter. The approach extends (Heilman et al.,
2007), where frequencies of manually defined syn-
tactic patterns were extracted from syntactic struc-
tures. In that approach, the features are defined man-
ually using linguistic knowledge of the target lan-
guage to implement tree search patterns, a labor- and
knowledge-intensive process. The approach advo-
cated in this paper, however, extracts frequencies for
an automatically defined set of subtree patterns. The
system considers all subtrees up to a given depth that
occur in the training corpus. Examples of grammati-
cal features at levels 0 through 2 are shown in Figure
1. The sentence for the parse tree shown was taken
from a third grade text.
For depth 0, the system includes all subtrees con-
sisting of just nonterminal nodes. This includes all
parts of speech, as well as non-terminal nodes for
noun phrases, adjective phrases, clauses, etc. For
depth 1, the system includes subtrees corresponding
to the application of a single context free grammar
rule in the derivation of the tree. An example of a
feature at this level would be a sentence node that
dominates nodes for noun phrases and verb phrases.
For deeper levels, the system includes subtrees cor-
responding to the successive application of rules on
non-terminals symbols until either a terminal sym-
bol is reached or the given depth is reached. An
example feature for level 2 is a subtree in which
a prepositional phrase node dominates a preposi-
tion node and noun phrase node, and the preposition
node in turn dominates a preposition, and the noun
phrase dominates determiner, adjective, and noun
nodes.
We used a maximum depth of 3 in our exper-
iments. Features of deeper levels occur less fre-
quently in general, and deeper levels were avoided
due to data sparseness. A depth first search algo-
rithm extracts candidate grammatical features from
the training corpus. First, a context-free grammar
parser (Klein and Manning, 2003) derives parse
trees for all texts in the training corpus. The algo-
rithm traverses these parses, at each node counting
all subtree features up to the given depth that are
rooted at that node. The subtree features are sorted
by their overall counts in the corpus. In our ex-
periments, frequencies of the most common 1000
subtrees were chosen as the final features. These
included 64 level 0 features corresponding to non-
terminal symbols, 334 level 1 features, 461 level 2
features, and 141 level 3 features. Deeper levels
have more possible features, but sparsity at level 3
resulted in fewer level 3 features being selected.
In our experiments, the subtrees included terminal
symbols for stopwords. However, the system effec-
tively removed content word terminals from parses
before extracting features. The system could be
modified to include terminal symbols for content
words, or even to ignore all nodes for terminal sym-
bols. Subtree features including terminal symbols
for content words would, of course, occur with low
frequency and not likely be included in the final fea-
ture set. Terminal symbols for content words were
omitted so that lexical information was not included
in the set of grammatical features. Similar to leaving
higher order n-grams out of the lexical feature set,
omitting terminal symbols for content words avoids
confounding grammatical and lexical information in
the grammatical feature set. Subtree counts are nor-
malized by the number of words in a text to compute
the relative frequencies. Normalization by the num-
ber of sentences in a text is also possible, but did not
perform as well in preliminary tests. The Stanford
Parser (Klein and Manning, 2003) version 1.5.1 was
used to derive tree structures for sentences. We used
the unlexicalized model included in the distribution
which was trained on Wall Street Journal texts.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="method">
3 Statistical Models
</sectionHeader>
<subsectionHeader confidence="0.992964">
3.1 Scales of Measurement for Reading
Difficulty
</subsectionHeader>
<bodyText confidence="0.999467333333333">
Several statistical models were tested for effective-
ness at predicting reading difficulty. The appropri-
ateness of these models depends on the nature of
</bodyText>
<page confidence="0.998156">
73
</page>
<figureCaption confidence="0.999806">
Figure 1: Parse Tree for Sentence from Third Grade Text with Example Subtree Features.
</figureCaption>
<bodyText confidence="0.999990696969697">
reading difficulty data, particularly the scale of mea-
surement. The standard unit for reading difficulty is
the grade level. First through twelfth grade levels in
American schools have been used in previous work
(e.g., (Heilman et al., 2007; Collins-Thompson and
Callan, 2005)). English as a Second Language lev-
els have also been used (Heilman et al., 2007),
as well as grade levels for other languages such
as French (Collins-Thompson and Callan, 2005).
While these grades are assigned evenly spaced inte-
gers, the ranges of reading difficulty corresponding
to these grades are not necessarily evenly spaced. It
is possible, of course, that assuming even spacing
between levels might produce more parsimonious
and accurate statistical models. A more reasonable
assumption is that the grade numbers assigned to
each difficulty level denote an ordering: for exam-
ple, that grade 1 is in some sense less than grade 2,
which is less than grade 3, etc. Different statistical
models handle this assumption more or less well.
Statistics generally distinguish four scales of mea-
surement, which are, ordered by increasing assump-
tions about the relationships between values: nomi-
nal, ordinal, interval, and ratio (Stevens, 1946; Co-
hen et al., 2003). Nominal data involve no relation-
ships between the labels or classes of the data. An
example would be types of fruits, where a model
might be used to make decisions between apples and
oranges. This type of prediction is generally called
classification in machine learning and related fields.
Ordinal data have a natural ordering, but the val-
ues are not necessarily evenly spaced. For exam-
ple, data about the severity of illnesses might have
labels such as mild, moderate, severe, deceased, in
which the transitions between consecutive classes
all have the same direction but not the same mag-
nitude. Making predictions about such data is gen-
erally called ordinal regression (McCullagh, 1980).
Interval data, however, are both ordered and evenly
spaced. An example would be temperature as mea-
sured in Fahrenheit degrees. Such data have an ar-
bitrary zero point, and negative values may occur.
Ratio data, of which annual income is an example,
do have a meaningful zero point. We will not dis-
cuss ratio data further since its distinction from in-
terval data is not relevant to this paper. It is not clear
to which scale reading difficulty corresponds. The
assumption of an interval scale allows for simpler
models with fewer parameters. However, models for
ordinal or even nominal data might be more appro-
priate if the strong assumption of an interval scale
does not hold.
We experimented with three linear and log-linear
models corresponding to interval, ordinal, and nom-
inal data. Parameters were estimated using L2 reg-
ularization, which corresponds to a Gaussian prior
distribution with zero mean and a user-specified
variance over the parameters. We chose these mod-
els because they are commonly used in the statis-
tics, machine learning, and behavioral science com-
munities, and aimed to set up meaningful compar-
isons among the scales of measurement. Other ma-
chine learning algorithms might also be employed.
In fact, we briefly tested the maximum margin (Vap-
nik, 1995) approach, which led to comparable re-
sults and might be worth exploring in future work.
</bodyText>
<page confidence="0.992236">
74
</page>
<subsectionHeader confidence="0.992982">
3.2 Linear Regression
</subsectionHeader>
<bodyText confidence="0.9999408">
Linear Regression (LIN) produces a linear model in
which the dependent, or outcome, variable is a lin-
ear function of the values for predictor variables,
or features. A prediction for a given text is the
inner product of a vector of feature values for the
text and a vector of regression coefficients estimated
from training data. For the case of reading difficulty,
the grade level is a linear combination of the lexi-
cal and/or grammatical feature values. LIN provides
continuous estimates of reading difficulty, such that
a prediction might fall between grade levels. The
estimates were not rounded to whole numbers in the
experiments. For rare cases of an LIN prediction
falling outside the appropriate range of grade levels,
the value was set to the maximum or minimum grade
level. LIN implicitly assumes that the data fall on
an interval scale, meaning that the levels are evenly
spaced. The LIN model has relatively few parame-
ters but makes strong assumptions about the scale of
measurement. For details, see (Hastie et al., 2001).
</bodyText>
<subsectionHeader confidence="0.988185">
3.3 Proportional Odds Model
</subsectionHeader>
<bodyText confidence="0.999716083333333">
The Proportional Odds (PO) model, also called the
parallel regression model and the cumulative logit
model, is a form of log-linear, or exponential, model
for ordinal data (McCullagh, 1980). Given a new
unlabeled instance as input, the model provides es-
timates of the probability that the instance belongs
to a class at or above a particular level. In Equation
(1), P(y &gt; j) is this estimated probability, αj is an
intercept parameter for the given level j, 0 is vector
of regression coefficients, Xi is the vector of feature
values for instance i, and yi is the predicted reading
difficulty level.
</bodyText>
<equation confidence="0.988332">
exp(αj + 0TXi)
P(yi &gt;_ j) = 1 + exp(αj + 0TXi) (1)
P(yi &gt;_ j)
ln 1 − P(yi &gt;_ j)
</equation>
<bodyText confidence="0.999986090909091">
The PO model has a parameter αj for the thresh-
old, or intercept, at each level j, but only a single set
0 of parameters for the features. These two types of
parameters correspond to an implicit assumption of
ordinality. Having a single set of parameters for fea-
tures across the levels means that changes in feature
values proportionally affect the odds of transitioning
from any one class to another.
The estimated probability of an instance belong-
ing to a particular class is the difference between es-
timates for that class and the next highest class. For
example, the estimated probability of a text being
at the eighth grade level would be the estimate for
being at or above eighth grade minus the estimate
for being at or above ninth grade. As in binary lo-
gistic regression, the PO model estimates log odds
ratios based on the values of features or predictor
variables. The numerator of the odds ratio is the
probability of being at or above a level, and the de-
nominator is the probability of being below a level.
Equation (2) shows the form of the model that is
linear in the parameters.
</bodyText>
<subsectionHeader confidence="0.99655">
3.4 Multi-class Logistic Regression
</subsectionHeader>
<bodyText confidence="0.999101166666667">
Multi-class Logistic Regression (LOG), or multino-
mial logit regression, is a log-linear model for nom-
inal data. In contrast to the simpler PO model, the
model maintains parameters for all of the features
for every class except one category, which is used
for comparison. Thus, for reading difficulty, there
are about 11 times as many parameters to estimate
compared to LIN and PO. The increased difficulty
of parameter estimation for this model is offset for
domains in which assumptions of ordinality or lin-
earity do not hold. For more details, see (Hastie et
al., 2001).
</bodyText>
<sectionHeader confidence="0.999016" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.994851">
4.1 Web Corpus
</subsectionHeader>
<bodyText confidence="0.999991142857143">
The corpus of materials used for training and test-
ing the models consists of the content text extracted
from Web pages with reading difficulty level labels.
Web pages were used because the system for pre-
dicting reading difficulty is being used as part of the
REAP tutoring system, which finds authentic and
appropriate Web pages for English vocabulary prac-
tice (Brown and Eskenazi, 2004; Heilman et al.,
2006). Approximately half of these texts were au-
thored by students at the particular grade level, and
half were authored by teachers or writers and aimed
at readers at a particular grade level. Texts were
found for grade levels 1 through 12. The twelfth
grade level also included some post-secondary level
</bodyText>
<equation confidence="0.994458">
= αj + 0T Xi (2)
</equation>
<page confidence="0.98884">
75
</page>
<bodyText confidence="0.999986516129032">
texts. Various genres and subjects were represented.
In all cases, either the text itself or a link to it iden-
tified it as having a certain level. The content text
was manually extracted from these Web pages so
that noisy information such as navigation menus and
advertisements were not included. Automatic con-
tent extraction may, however, be able to remove such
noisy information without human intervention (e.g.,
(Gupta et al., 2003)). This Web corpus is adapted
from the corpora used in prior work on reading dif-
ficulty predication (Collins-Thompson and Callan,
2005; Heilman et al., 2007). We modified that cor-
pus because it contained a number of documents
pertaining to mathematics and vocabulary practice.
The majority of tokens in these texts were not part
of well-formed, grammatical sentences suitable for
reading practice. Since our goal is to measure the
difficulty of reading passages, we removed these
documents and added additional texts consisting of
more suitable reading material. The corpus con-
sisted of approximately 150,000 words, distributed
among 289 texts. The number of texts for each grade
level was approximately the same, with at least 28
texts at each level. The mean length in words of
the texts was approximately 500 words, which corre-
sponds to about a page. Texts for lower grades were
necessarily shorter. We extracted excerpts for higher
level texts so that texts were otherwise roughly equal
in length across levels. For these excerpts, the first
500 or so words of text were extracted, while re-
specting sentence and paragraph boundaries.
</bodyText>
<subsectionHeader confidence="0.981294">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999971290322581">
Root mean square error (RMSE), Pearson’s correla-
tion coefficient, and accuracy within 1 grade level
served as metrics for evaluating the performance
of reading difficulty predictions. Multiple statistics
were used because it is not entirely clear what the
best measure of prediction quality is for reading dif-
ficulty. RMSE is the square root of the empirical
mean of the squared error of predictions. It more
strongly penalizes those errors that are further away
from the true value. It can be interpreted as the aver-
age number of grade levels that predictions measure
deviate from human-assigned labels.
Pearson’s correlation coefficient measures the
strength of the linear relationship, or similarity of
trends, between two random variables. A high corre-
lation would indicate that difficult texts would more
likely receive high predicted difficulty values, and
easier texts would be more likely to receive low pre-
dicted difficulty values. Correlations do not, how-
ever, measure the degree to which values match in
absolute terms.
Adjacent accuracy is the proportion of predic-
tions that were within one grade level of the human-
assigned label for the given text. Exact accuracy is
too stringent a measure because the human-assigned
reading levels are not always perfect and consis-
tent. For example, one school might read “Romeo
and Juliet” in 9th grade while another school might
read it in 10th grade. The drawback of this accuracy
metric is that predictions that are two levels off are
treated the same as predictions that are ten levels off.
</bodyText>
<subsectionHeader confidence="0.99943">
4.3 Baselines
</subsectionHeader>
<bodyText confidence="0.9999784">
The performance of other algorithms for estimat-
ing reading difficulty was estimated using the same
data. These comparison include Collins-Thompson
and Callan’s implementation of their language mod-
eling approach (2005), an implementation of the
Flesch-Kincaid reading level measure (Kincaid et
al., 1975), and a measure using word frequency and
sentence length similar to Lexile (Stenner et al.,
1983). We did not directly test the approach de-
scribed by (Heilman et al., 2007). We observe
that its reported results for first language texts were
not significantly different in terms of correlation and
only slightly better in terms of mean squared er-
ror than the language modeling approach. Finally,
a simple uniform baseline, which always chose the
middle value of 6.5, was tested.
The Lexile-like measure (LX) used the same two
features as the Lexile measure: mean log frequency
or words and log mean sentence length. Instead of
using a Rasch model and converting scores to “Lex-
iles,” however, the PO model was used to directly
predict grade levels. The log frequency values for
words were estimated from the second release of the
American National Corpus (Reppen et al., 2005),
a 20 million word corpus with texts in American
English from different genres on a variety of sub-
jects. Using the proportional odds models is effec-
tively equivalent to using Lexile’s Rasch model and
mapping its output to grade levels. The major differ-
ence between the Lexile measure and the implemen-
</bodyText>
<page confidence="0.962417">
76
</page>
<bodyText confidence="0.997784666666667">
tation used in these experiments is the training data
sets used to estimated word frequencies and model
parameters.
</bodyText>
<subsectionHeader confidence="0.998853">
4.4 Procedure
</subsectionHeader>
<bodyText confidence="0.999997060606061">
The Web Corpus was randomly split into training
and test sets. The test set consisted of 25% of the
individual texts at each level, a total of 84 texts.
Ten-fold stratified cross-validation on the training
set was employed to estimate the prediction per-
formance according to the evaluation metrics. In
cross-validation, data are partitioned randomly into
a given number of folds, and each fold is used for
testing while all others are used for training. For
more details and a discussion of validation meth-
ods, see (Hastie et al., 2001). The regularization
hyper-parameters were tuned on the training set dur-
ing cross-validation by a simple grid search. After
cross-validation, models were trained on the entire
training set, and then evaluated using the held-out
test data.
We tested whether each feature-set, algorithm pair
or baseline performed significantly differently than
our hypothesized best model, the PO model with
the combined feature set. We employed the bias-
corrected and accelerated (BC,,) Bootstrap (Efron
and Tibshirani, 1993) with 50,000 replications of the
held-out test data to generate confidence intervals
for differences in evaluation results. If the (1− α)%
confidence intervals for the difference do not con-
tain zero, which is the value corresponding to the
null hypothesis, then that difference is significant at
the α level. For example, the 99% confidence inter-
val for the difference in adjacent accuracy between
the language modeling baseline and the PO model
with the combined feature set was (-1.86, -0.336),
indicating that this difference is significant at the .01
level since it does not contain zero.
</bodyText>
<sectionHeader confidence="0.999908" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999831489361702">
Table 1 presents correlation coefficients, RMSE val-
ues, and accuracy values for cross-validation and
held-out test data. Statistical significance was tested
only for the held-out test data since the hyper-
parameters were tuned during cross-validation. Our
discussion of the results pertains mostly to the eval-
uation on the test-set.
Of the various statistical models, the PO model
for ordinal data appears to provide superior perfor-
mance over the LIN and LOG models. Compared
to the LOG model, the PO model performs sig-
nificantly better in terms of correlation and RMSE
and comparably well in terms of adjacent accuracy.
Compared to the LIN model, the PO model performs
almost as well in terms of correlation, comparably
well in terms of RMSE, and far better in terms of
accuracy.
The performance of the methods when using dif-
ferent feature sets does not clearly indicate a best set
of features to use for predicting reading difficulty.
For the PO model, none of the feature sets lead to
significant gains over the others in terms of any of
the metrics. However, the combined feature set led
to the best performance in terms of correlation and
adjacent accuracy during cross-validation as well as
RMSE on the test set, suggesting at the very least
that including the extra features does not degrade
performance.
The PO model with the combined feature set out-
performed most of the baseline measures. LX had
the same accuracy value on the test set. The LX
method appears to perform the best in general of
the baselines models. Interestingly, LX uses pro-
portional odds logistic regression like PO, and thus
assumes an ordinal but not interval scale of measure-
ment. RMSE values were significantly lower for the
PO model than for LX and the language modeling
approach.
No statistically significant advantages are seen
for PO model when compared to Flesch-Kincaid.
We observe however, that for the sample of web
pages which constitutes the evaluation corpus the
PO model produced superior results across evalua-
tion metrics. That is, PO performed better in terms
of adjacent accuracy, RMSE, and correlation coeffi-
cients, both in cross-validation and testing with held-
out data.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999629">
In our tests, the PO model, which assumes ordinal
data, lead to the most effective predictions of read-
ing difficulty in general. This result indicates that the
reading difficulty of texts, according to grade level,
lies on an ordinal scale of measurement. That is,
</bodyText>
<page confidence="0.99471">
77
</page>
<table confidence="0.999870125">
Method Features Correl. Cross Adj. Acc. Held -Out Test Set
-Validation Correl. RMSE Adj. Acc.
RMSE
LIN Lexical .629 2.73 .242 .779 2.42 .167**
Grammatical .767 2.26 .294 .753 2.33 .274*
Combined .679 2.57 .284 .819** 2.21 .226**
PO Lexical .713 2.57 .498 .780 2.29 .464
Grammatical .762 2.22 .505 .734 2.42 .560
Combined .773 2.24 .519 .767 2.23 .440
LOG Lexical .517 3.24 .443 .619* 2.83* .548
Grammatical .632 2.87 .443 .506** 3.38** .464
Combined .582 2.94 .446 .652* 2.71* .556
LX - .659 2.77 .467 .731 2.67* .464
Lang. Modeling - .590 2.74 .370 .630 2.70** .381
Flesch-Kincaid - .697 2.66 .388 .718 2.54 .369
Uniform - .000 3.39 .170 .000** 3.45** .167**
</table>
<tableCaption confidence="0.74370825">
Table 1: Results from Cross-Validation and Test Set Evaluations, as measured by Correlation Coefficients (Correl.),
Root Mean Square Error (RMSE), and Adjacent Accuracy. The best result for each metric for each evaluation is
given in bold. Asterisks indicate significant differences compared to the PO model with a Combined Feature Set. * =
p &lt; .05, ** = p &lt; .01.
</tableCaption>
<bodyText confidence="0.999908533333333">
reading difficulty appears to increase steadily but not
linearly with grade level. As such, the LIN approach
that produces linear models was less effective, par-
ticularly in terms of adjacent accuracy. The LOG
model, for nominal data, also led to inferior perfor-
mance compared to the PO model, which can be at-
tributed to the difficulty of accurately estimating a
more complex model with many parameters for each
level.
Our tests found that grammatical features alone
can be effective predictors of readability. This find-
ing disagrees with a previous result that found that a
model using a combination of lexical and manually
defined grammatical features (Heilman et al., 2007)
outperformed a model using grammatical features
alone. The superior predictive ability of the mod-
els we describe that use grammatical features can be
attributed to the automatic derivation of a grammat-
ical feature set that is more than an order of magni-
tude larger than in the previous approach. Our ap-
proach enables the use of much larger grammatical
feature sets because it does not require the extensive
linguistic knowledge and effort to manually define
the grammatical features. The automatic approach
also enables an easier transition to other languages,
assuming a parser is available. Using the combined
feature set did not hurt performance, however, and
since regularized statistical models can avoid over-
fitting large numbers of parameters, a combined fea-
ture set still seems appropriate.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999788692307692">
We thank Jamie Callan for his comments and sug-
gestions. This research was supported in part by
the Institute of Education Sciences, U.S. Depart-
ment of Education, through Grant R305B040063 to
Carnegie Mellon University; Dept. of Education
grant R305G03123; the Pittsburgh Science of Learn-
ing Center which is funded by the National Sci-
ence Foundation, award number SBE-0354420; and
a National Science Foundation Graduate Research
Fellowship awarded to the first author. Any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are the authors, and do not
necessarily reflect those of the sponsors.
</bodyText>
<sectionHeader confidence="0.9996" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988779333333333">
Jon Brown and Maxine Eskenazi. 2004. Retrieval of au-
thentic documents for reader-specific lexical practice.
Proceedings of InSTIL/ICALL Symposium 2004.
</reference>
<page confidence="0.978379">
78
</page>
<reference confidence="0.996569928571428">
J. S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brookline
Books. Cambridge, MA.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. Proceedings of the NAACL.
J. Cohen, P. Cohen, S. G. West, and L. S. Aiken. 2003.
Applied Multiple Regression/Correlation Analysis for
the Behavioral Sciences, 3rd Edition. Lawrence Erl-
baum Associates, Inc.
Michael Collins and Nigel Duffy. 2002. Convolution
Kernels for Natural Language. Advances in Neural In-
formation Processing Systems..
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science and Technology, 56(13). pp. 1448-1462..
E. Dale and J. S. Chall. 1948. A Formula for Predicting
Readability. Educational Research Bulletin Vol. 27,
No. 1.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York.
R. Gunning. 1952. The technique of clear writing..
McGraw-Hill, New York.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press, New York.
Trevor Hastie, Robert Tibshirani, Jerome Friedman.
2003. The Elements of Statistical Learning:Data Min-
ing, Inference, and Prediction. Springer.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining Lex-
ical and Grammatical Features to Improve Readability
Measures for First and Second Language Texts. Pro-
ceedings of the Human Language Technology Confer-
ence. Rochester, NY.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical prac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
J. Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.
1975. Derivation of new readability formulas for navy
enlisted personnel. Branch Report 8-75. Chief of
Naval Training, Millington, TN.
G. R. Klare. 1974. Assessing Readability. Reading Re-
search Quarterly, Vol. 10, No. 1. pp. 62-102..
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics,
pp. 423-430.
G. H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading.
P. McCullagh. 1980. Regression Models for Ordinal
Data. Journal of the Royal Statistical Society. Series
B (Methodological), Vol. 42, No. 2. pp. 109-142.
G. Rasch. 1980. Probabilistic Models for Some Intelli-
gence and Attainment Tests. MESA Press, Chicago,
IL.
G. Rasch. 2005. American National Corpus (ANC) Sec-
ond Release.. Linguistic Data Consortium. Philadel-
phia, PA.
Sarah Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics.
A. J. Stenner, M. Smith, and D. S. Burdick. 1983. To-
ward a Theory of Construct Definition. Journal of Ed-
ucational Measurement, Vol. 20, No. 4. pp. 305-316.
A. J. Stenner. 1996. Measuring reading comprehension
with the Lexile framework. Fourth North American
Conference on Adolescent/Adult Literacy.
S. S. Stevens. 1946. On the theory of scales of measure-
ment. Science, 103, pp. 677-680.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Y. Yang and J. P. Pedersen. 1997. A Comparative Study
on Feature Selection in Text Categorization. Proceed-
ings of the Fourteenth International Conference on
Machine Learning (ICML’97), pp. 412-420.
G. K. Zipf. 1935. The Psychobiology off Language.
Houghton Mifflin, Boston, MA.
</reference>
<page confidence="0.999049">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.474923">
<title confidence="0.797546">An Analysis of Statistical Models and Features for Reading Difficulty Prediction Heilman, Kevyn Collins-Thompson Language Technologies</title>
<affiliation confidence="0.974525">Carnegie Mellon</affiliation>
<address confidence="0.991735">Pittsburgh, PA 15213,</address>
<abstract confidence="0.995196266666667">A reading difficulty measure can be described as a function or model that maps a text to a numerical value corresponding to a difficulty or grade level. We describe a measure of readability that uses a combination of lexical features and grammatical features that are derived from subtrees of syntactic parses. We also tested statistical models for nominal, ordinal, and interval scales of measurement. The results indicate that a model for ordinal regression, such as the proportional odds model, using a combination of grammatical and lexical features is most effective at predicting reading difficulty.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jon Brown</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Retrieval of authentic documents for reader-specific lexical practice.</title>
<date>2004</date>
<booktitle>Proceedings of InSTIL/ICALL Symposium</booktitle>
<contexts>
<context position="19969" citStr="Brown and Eskenazi, 2004" startWordPosition="3232" endWordPosition="3235">ed to LIN and PO. The increased difficulty of parameter estimation for this model is offset for domains in which assumptions of ordinality or linearity do not hold. For more details, see (Hastie et al., 2001). 4 Evaluation 4.1 Web Corpus The corpus of materials used for training and testing the models consists of the content text extracted from Web pages with reading difficulty level labels. Web pages were used because the system for predicting reading difficulty is being used as part of the REAP tutoring system, which finds authentic and appropriate Web pages for English vocabulary practice (Brown and Eskenazi, 2004; Heilman et al., 2006). Approximately half of these texts were authored by students at the particular grade level, and half were authored by teachers or writers and aimed at readers at a particular grade level. Texts were found for grade levels 1 through 12. The twelfth grade level also included some post-secondary level = αj + 0T Xi (2) 75 texts. Various genres and subjects were represented. In all cases, either the text itself or a link to it identified it as having a certain level. The content text was manually extracted from these Web pages so that noisy information such as navigation men</context>
</contexts>
<marker>Brown, Eskenazi, 2004</marker>
<rawString>Jon Brown and Maxine Eskenazi. 2004. Retrieval of authentic documents for reader-specific lexical practice. Proceedings of InSTIL/ICALL Symposium 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Chall</author>
<author>E Dale</author>
</authors>
<title>Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books.</title>
<date>1995</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="2062" citStr="Chall and Dale, 1995" startWordPosition="324" endWordPosition="327">asures employed simple proxies for grammatical and lexical complexity, including sentence length and the number of syllables in a word. Fairly simple features were often employed because of a lack of computational power. Such features exhibit high bias because they rely on strong assumptions about what makes a text difficult to read. For example, the use of sentence length as a measure of grammatical complexity assumes that a longer sentence is more grammatically complex than a shorter one, which is often but not always the case. In one early model, the Dale-Chall model (Dale and Chall, 1948; Chall and Dale, 1995), reading difficulty is a linear function of the mean sentence length and the percentage of rare words, as defined by a list of 3,000 words commonly known by 4th grade. In this paper, sentence length is defined as the mean number of words in the sentences of a text. Many early measures did not employ direct estimates of word frequency due to computational limitations (e.g., (Gunning, 1952; McLaughlin, 1969; Kincaid et al., 1975)). Instead, these measures relied on the strong relationship between the frequency of and the number of syllables in a word. More frequent words are more likely to have</context>
</contexts>
<marker>Chall, Dale, 1995</marker>
<rawString>J. S. Chall and E. Dale. 1995. Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>Proceedings of the NAACL.</booktitle>
<contexts>
<context position="5045" citStr="Charniak, 2000" startWordPosition="810" endWordPosition="811">r example, “grownup” was very predictive of grade 1, and “essay” was very predictive of grade 12. For a given text, this measure estimates the likelihood that the text was generated by each level’s language model. The prediction is the level of the model with the highest likelihood of generating the text. There are no grammatical features. Natural language processing techniques enable more sophisticated grammatical analysis for reading difficulty measures. Rather than using sentence length as a proxy, measures can employ tools for automatic analysis of the syntactic structure of texts (e.g., (Charniak, 2000)). A measure by Schwarm and Ostendorf (2005) incorporates syntactic analyses, among a variety of other types of features. It includes four grammatical features derived from syntactic parses of text: the mean parse tree height, the mean number of noun phrases, mean number of verb phrases, and mean number of “SBARs.” “SBARs” are non-terminal nodes that are associated with subordinate clauses. Their system led to better predictions than the Flesch-Kincaid and Lexile measures, but the predictive value of the grammatical features is not entirely clear. In initial experiments using such course-grain</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. Proceedings of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
<author>P Cohen</author>
<author>S G West</author>
<author>L S Aiken</author>
</authors>
<title>Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences, 3rd Edition. Lawrence Erlbaum Associates,</title>
<date>2003</date>
<publisher>Inc.</publisher>
<contexts>
<context position="14029" citStr="Cohen et al., 2003" startWordPosition="2228" endWordPosition="2232">sible, of course, that assuming even spacing between levels might produce more parsimonious and accurate statistical models. A more reasonable assumption is that the grade numbers assigned to each difficulty level denote an ordering: for example, that grade 1 is in some sense less than grade 2, which is less than grade 3, etc. Different statistical models handle this assumption more or less well. Statistics generally distinguish four scales of measurement, which are, ordered by increasing assumptions about the relationships between values: nominal, ordinal, interval, and ratio (Stevens, 1946; Cohen et al., 2003). Nominal data involve no relationships between the labels or classes of the data. An example would be types of fruits, where a model might be used to make decisions between apples and oranges. This type of prediction is generally called classification in machine learning and related fields. Ordinal data have a natural ordering, but the values are not necessarily evenly spaced. For example, data about the severity of illnesses might have labels such as mild, moderate, severe, deceased, in which the transitions between consecutive classes all have the same direction but not the same magnitude. </context>
</contexts>
<marker>Cohen, Cohen, West, Aiken, 2003</marker>
<rawString>J. Cohen, P. Cohen, S. G. West, and L. S. Aiken. 2003. Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences, 3rd Edition. Lawrence Erlbaum Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems..</booktitle>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. Convolution Kernels for Natural Language. Advances in Neural Information Processing Systems..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>Predicting reading difficulty with statistical language models.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>56</volume>
<issue>13</issue>
<pages>1448--1462</pages>
<contexts>
<context position="4181" citStr="Collins-Thompson and Callan (2005)" startWordPosition="677" endWordPosition="680">s, pages 71–79, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics from a large, varied corpus of text. Lexile uses a Rasch model (Rasch, 1980) with the mean log word frequency as a lexical feature and the log of the mean sentence length as a grammatical feature. The Rasch model, related to logistic regression, is used to estimate the level of a student that would comprehend 75% of a given text. The converted log odds ratio called a “Lexile” that is used as part of this measure can be easily mapped to grade school levels. A reading difficulty measure developed by Collins-Thompson and Callan (2005) uses smoothed unigram language modeling to capture the predictive ability of individual words based on their frequency at each reading difficulty level. Collins-Thompson and Callan found that certain words were very predictive of certain levels. For example, “grownup” was very predictive of grade 1, and “essay” was very predictive of grade 12. For a given text, this measure estimates the likelihood that the text was generated by each level’s language model. The prediction is the level of the model with the highest likelihood of generating the text. There are no grammatical features. Natural l</context>
<context position="8074" citStr="Collins-Thompson and Callan, 2005" startWordPosition="1275" endWordPosition="1278"> grammatical features derived from syntactic structures without requiring significant linguistic or pedagogical content knowledge, such as a reference guide for the grammar of the language of interest. 2 Types of Features 2.1 Lexical Features This section and the following section describe the lexical and grammatical features used in our reading difficulty models. The lexical features are the relative frequencies of word unigrams. The use of word unigrams is a standard approach in text classification (Yang and Pedersen, 1997), and has also been successfully used to predict reading difficulty (Collins-Thompson and Callan, 2005). Higher order n-grams such as bigrams and trigrams were not used as features because they did not improve predictions 72 in preliminary tests. The specific set of lexical features was chosen based on the frequencies of words in the training corpus. The system performs morphological stemming and stopword removal. The remaining 5000 most common words comprised the lexical feature set. 2.2 Grammatical Features Grammatical features are extracted from automatic context-free grammar parses of sentences. The system computes relative frequencies of partial syntactic derivations, which will be called </context>
<context position="13068" citStr="Collins-Thompson and Callan, 2005" startWordPosition="2076" endWordPosition="2079">ich was trained on Wall Street Journal texts. 3 Statistical Models 3.1 Scales of Measurement for Reading Difficulty Several statistical models were tested for effectiveness at predicting reading difficulty. The appropriateness of these models depends on the nature of 73 Figure 1: Parse Tree for Sentence from Third Grade Text with Example Subtree Features. reading difficulty data, particularly the scale of measurement. The standard unit for reading difficulty is the grade level. First through twelfth grade levels in American schools have been used in previous work (e.g., (Heilman et al., 2007; Collins-Thompson and Callan, 2005)). English as a Second Language levels have also been used (Heilman et al., 2007), as well as grade levels for other languages such as French (Collins-Thompson and Callan, 2005). While these grades are assigned evenly spaced integers, the ranges of reading difficulty corresponding to these grades are not necessarily evenly spaced. It is possible, of course, that assuming even spacing between levels might produce more parsimonious and accurate statistical models. A more reasonable assumption is that the grade numbers assigned to each difficulty level denote an ordering: for example, that grade </context>
<context position="20882" citStr="Collins-Thompson and Callan, 2005" startWordPosition="3385" endWordPosition="3388">vel also included some post-secondary level = αj + 0T Xi (2) 75 texts. Various genres and subjects were represented. In all cases, either the text itself or a link to it identified it as having a certain level. The content text was manually extracted from these Web pages so that noisy information such as navigation menus and advertisements were not included. Automatic content extraction may, however, be able to remove such noisy information without human intervention (e.g., (Gupta et al., 2003)). This Web corpus is adapted from the corpora used in prior work on reading difficulty predication (Collins-Thompson and Callan, 2005; Heilman et al., 2007). We modified that corpus because it contained a number of documents pertaining to mathematics and vocabulary practice. The majority of tokens in these texts were not part of well-formed, grammatical sentences suitable for reading practice. Since our goal is to measure the difficulty of reading passages, we removed these documents and added additional texts consisting of more suitable reading material. The corpus consisted of approximately 150,000 words, distributed among 289 texts. The number of texts for each grade level was approximately the same, with at least 28 tex</context>
</contexts>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>Kevyn Collins-Thompson and Jamie Callan. 2005. Predicting reading difficulty with statistical language models. Journal of the American Society for Information Science and Technology, 56(13). pp. 1448-1462..</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dale</author>
<author>J S Chall</author>
</authors>
<title>A Formula for Predicting Readability.</title>
<date>1948</date>
<journal>Educational Research Bulletin</journal>
<volume>27</volume>
<contexts>
<context position="2039" citStr="Dale and Chall, 1948" startWordPosition="320" endWordPosition="323">work on readability measures employed simple proxies for grammatical and lexical complexity, including sentence length and the number of syllables in a word. Fairly simple features were often employed because of a lack of computational power. Such features exhibit high bias because they rely on strong assumptions about what makes a text difficult to read. For example, the use of sentence length as a measure of grammatical complexity assumes that a longer sentence is more grammatically complex than a shorter one, which is often but not always the case. In one early model, the Dale-Chall model (Dale and Chall, 1948; Chall and Dale, 1995), reading difficulty is a linear function of the mean sentence length and the percentage of rare words, as defined by a list of 3,000 words commonly known by 4th grade. In this paper, sentence length is defined as the mean number of words in the sentences of a text. Many early measures did not employ direct estimates of word frequency due to computational limitations (e.g., (Gunning, 1952; McLaughlin, 1969; Kincaid et al., 1975)). Instead, these measures relied on the strong relationship between the frequency of and the number of syllables in a word. More frequent words </context>
</contexts>
<marker>Dale, Chall, 1948</marker>
<rawString>E. Dale and J. S. Chall. 1948. A Formula for Predicting Readability. Educational Research Bulletin Vol. 27, No. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman and Hall,</title>
<date>1993</date>
<location>New York.</location>
<contexts>
<context position="26121" citStr="Efron and Tibshirani, 1993" startWordPosition="4222" endWordPosition="4225">ile all others are used for training. For more details and a discussion of validation methods, see (Hastie et al., 2001). The regularization hyper-parameters were tuned on the training set during cross-validation by a simple grid search. After cross-validation, models were trained on the entire training set, and then evaluated using the held-out test data. We tested whether each feature-set, algorithm pair or baseline performed significantly differently than our hypothesized best model, the PO model with the combined feature set. We employed the biascorrected and accelerated (BC,,) Bootstrap (Efron and Tibshirani, 1993) with 50,000 replications of the held-out test data to generate confidence intervals for differences in evaluation results. If the (1− α)% confidence intervals for the difference do not contain zero, which is the value corresponding to the null hypothesis, then that difference is significant at the α level. For example, the 99% confidence interval for the difference in adjacent accuracy between the language modeling baseline and the PO model with the combined feature set was (-1.86, -0.336), indicating that this difference is significant at the .01 level since it does not contain zero. 5 Resul</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gunning</author>
</authors>
<title>The technique of clear writing..</title>
<date>1952</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="2453" citStr="Gunning, 1952" startWordPosition="396" endWordPosition="397">ical complexity assumes that a longer sentence is more grammatically complex than a shorter one, which is often but not always the case. In one early model, the Dale-Chall model (Dale and Chall, 1948; Chall and Dale, 1995), reading difficulty is a linear function of the mean sentence length and the percentage of rare words, as defined by a list of 3,000 words commonly known by 4th grade. In this paper, sentence length is defined as the mean number of words in the sentences of a text. Many early measures did not employ direct estimates of word frequency due to computational limitations (e.g., (Gunning, 1952; McLaughlin, 1969; Kincaid et al., 1975)). Instead, these measures relied on the strong relationship between the frequency of and the number of syllables in a word. More frequent words are more likely to have fewer syllables (e.g., “the”) than less frequent words (e.g., “vocabulary”), an association that is related to Zipf’s Law (Zipf, 1935). The Flesch-Kincaid measure (Kincaid et al., 1975) is probably the most common reading difficulty measure in use. It is implemented in common word processing programs. This measure is a linear function of the mean number of syllables per word and the mean</context>
</contexts>
<marker>Gunning, 1952</marker>
<rawString>R. Gunning. 1952. The technique of clear writing.. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>G Kaiser</author>
<author>D Neistadt</author>
<author>P Grimm</author>
</authors>
<title>DOM-based content extraction of HTML documents.</title>
<date>2003</date>
<publisher>ACM Press,</publisher>
<location>New York.</location>
<contexts>
<context position="20748" citStr="Gupta et al., 2003" startWordPosition="3364" endWordPosition="3367">s and aimed at readers at a particular grade level. Texts were found for grade levels 1 through 12. The twelfth grade level also included some post-secondary level = αj + 0T Xi (2) 75 texts. Various genres and subjects were represented. In all cases, either the text itself or a link to it identified it as having a certain level. The content text was manually extracted from these Web pages so that noisy information such as navigation menus and advertisements were not included. Automatic content extraction may, however, be able to remove such noisy information without human intervention (e.g., (Gupta et al., 2003)). This Web corpus is adapted from the corpora used in prior work on reading difficulty predication (Collins-Thompson and Callan, 2005; Heilman et al., 2007). We modified that corpus because it contained a number of documents pertaining to mathematics and vocabulary practice. The majority of tokens in these texts were not part of well-formed, grammatical sentences suitable for reading practice. Since our goal is to measure the difficulty of reading passages, we removed these documents and added additional texts consisting of more suitable reading material. The corpus consisted of approximately</context>
</contexts>
<marker>Gupta, Kaiser, Neistadt, Grimm, 2003</marker>
<rawString>S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003. DOM-based content extraction of HTML documents. ACM Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome Friedman</author>
</authors>
<date>2003</date>
<booktitle>The Elements of Statistical Learning:Data Mining, Inference, and Prediction.</booktitle>
<publisher>Springer.</publisher>
<marker>Hastie, Tibshirani, Friedman, 2003</marker>
<rawString>Trevor Hastie, Robert Tibshirani, Jerome Friedman. 2003. The Elements of Statistical Learning:Data Mining, Inference, and Prediction. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts.</title>
<date>2007</date>
<booktitle>Proceedings of the Human Language Technology Conference.</booktitle>
<location>Rochester, NY.</location>
<contexts>
<context position="7389" citStr="Heilman et al., 2007" startWordPosition="1170" endWordPosition="1173">l assumes that grammatical difficulty is adequately captured by a small number of constructions chosen according to detailed knowledge of English grammar. In that work, the constructions were selected from an English as a Second Language grammar textbook, a labor- and knowledge-intensive task that may be less practical for other languages. We aim to identify the appropriate scale of measurement for reading difficulty–nominal, ordinal, or interval–by comparing the effectiveness of statistical models for each type of data. We also extend previous work combining lexical and grammatical features (Heilman et al., 2007) by making it possible to include a large number of grammatical features derived from syntactic structures without requiring significant linguistic or pedagogical content knowledge, such as a reference guide for the grammar of the language of interest. 2 Types of Features 2.1 Lexical Features This section and the following section describe the lexical and grammatical features used in our reading difficulty models. The lexical features are the relative frequencies of word unigrams. The use of word unigrams is a standard approach in text classification (Yang and Pedersen, 1997), and has also bee</context>
<context position="8739" citStr="Heilman et al., 2007" startWordPosition="1375" endWordPosition="1378">rigrams were not used as features because they did not improve predictions 72 in preliminary tests. The specific set of lexical features was chosen based on the frequencies of words in the training corpus. The system performs morphological stemming and stopword removal. The remaining 5000 most common words comprised the lexical feature set. 2.2 Grammatical Features Grammatical features are extracted from automatic context-free grammar parses of sentences. The system computes relative frequencies of partial syntactic derivations, which will be called ’subtrees’ hereafter. The approach extends (Heilman et al., 2007), where frequencies of manually defined syntactic patterns were extracted from syntactic structures. In that approach, the features are defined manually using linguistic knowledge of the target language to implement tree search patterns, a labor- and knowledge-intensive process. The approach advocated in this paper, however, extracts frequencies for an automatically defined set of subtree patterns. The system considers all subtrees up to a given depth that occur in the training corpus. Examples of grammatical features at levels 0 through 2 are shown in Figure 1. The sentence for the parse tree</context>
<context position="13032" citStr="Heilman et al., 2007" startWordPosition="2072" endWordPosition="2075">in the distribution which was trained on Wall Street Journal texts. 3 Statistical Models 3.1 Scales of Measurement for Reading Difficulty Several statistical models were tested for effectiveness at predicting reading difficulty. The appropriateness of these models depends on the nature of 73 Figure 1: Parse Tree for Sentence from Third Grade Text with Example Subtree Features. reading difficulty data, particularly the scale of measurement. The standard unit for reading difficulty is the grade level. First through twelfth grade levels in American schools have been used in previous work (e.g., (Heilman et al., 2007; Collins-Thompson and Callan, 2005)). English as a Second Language levels have also been used (Heilman et al., 2007), as well as grade levels for other languages such as French (Collins-Thompson and Callan, 2005). While these grades are assigned evenly spaced integers, the ranges of reading difficulty corresponding to these grades are not necessarily evenly spaced. It is possible, of course, that assuming even spacing between levels might produce more parsimonious and accurate statistical models. A more reasonable assumption is that the grade numbers assigned to each difficulty level denote a</context>
<context position="20905" citStr="Heilman et al., 2007" startWordPosition="3389" endWordPosition="3392">ry level = αj + 0T Xi (2) 75 texts. Various genres and subjects were represented. In all cases, either the text itself or a link to it identified it as having a certain level. The content text was manually extracted from these Web pages so that noisy information such as navigation menus and advertisements were not included. Automatic content extraction may, however, be able to remove such noisy information without human intervention (e.g., (Gupta et al., 2003)). This Web corpus is adapted from the corpora used in prior work on reading difficulty predication (Collins-Thompson and Callan, 2005; Heilman et al., 2007). We modified that corpus because it contained a number of documents pertaining to mathematics and vocabulary practice. The majority of tokens in these texts were not part of well-formed, grammatical sentences suitable for reading practice. Since our goal is to measure the difficulty of reading passages, we removed these documents and added additional texts consisting of more suitable reading material. The corpus consisted of approximately 150,000 words, distributed among 289 texts. The number of texts for each grade level was approximately the same, with at least 28 texts at each level. The m</context>
<context position="23942" citStr="Heilman et al., 2007" startWordPosition="3871" endWordPosition="3874">ck of this accuracy metric is that predictions that are two levels off are treated the same as predictions that are ten levels off. 4.3 Baselines The performance of other algorithms for estimating reading difficulty was estimated using the same data. These comparison include Collins-Thompson and Callan’s implementation of their language modeling approach (2005), an implementation of the Flesch-Kincaid reading level measure (Kincaid et al., 1975), and a measure using word frequency and sentence length similar to Lexile (Stenner et al., 1983). We did not directly test the approach described by (Heilman et al., 2007). We observe that its reported results for first language texts were not significantly different in terms of correlation and only slightly better in terms of mean squared error than the language modeling approach. Finally, a simple uniform baseline, which always chose the middle value of 6.5, was tested. The Lexile-like measure (LX) used the same two features as the Lexile measure: mean log frequency or words and log mean sentence length. Instead of using a Rasch model and converting scores to “Lexiles,” however, the PO model was used to directly predict grade levels. The log frequency values </context>
<context position="30873" citStr="Heilman et al., 2007" startWordPosition="5007" endWordPosition="5010">t linearly with grade level. As such, the LIN approach that produces linear models was less effective, particularly in terms of adjacent accuracy. The LOG model, for nominal data, also led to inferior performance compared to the PO model, which can be attributed to the difficulty of accurately estimating a more complex model with many parameters for each level. Our tests found that grammatical features alone can be effective predictors of readability. This finding disagrees with a previous result that found that a model using a combination of lexical and manually defined grammatical features (Heilman et al., 2007) outperformed a model using grammatical features alone. The superior predictive ability of the models we describe that use grammatical features can be attributed to the automatic derivation of a grammatical feature set that is more than an order of magnitude larger than in the previous approach. Our approach enables the use of much larger grammatical feature sets because it does not require the extensive linguistic knowledge and effort to manually define the grammatical features. The automatic approach also enables an easier transition to other languages, assuming a parser is available. Using </context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2007</marker>
<rawString>Michael Heilman, Kevyn Collins-Thompson, Jamie Callan, and Maxine Eskenazi. 2007. Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts. Proceedings of the Human Language Technology Conference. Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Classroom success of an Intelligent Tutoring System for lexical practice and reading comprehension.</title>
<date>2006</date>
<booktitle>Proceedings of the Ninth International Conference on Spoken Language Processing.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="19992" citStr="Heilman et al., 2006" startWordPosition="3236" endWordPosition="3239">eased difficulty of parameter estimation for this model is offset for domains in which assumptions of ordinality or linearity do not hold. For more details, see (Hastie et al., 2001). 4 Evaluation 4.1 Web Corpus The corpus of materials used for training and testing the models consists of the content text extracted from Web pages with reading difficulty level labels. Web pages were used because the system for predicting reading difficulty is being used as part of the REAP tutoring system, which finds authentic and appropriate Web pages for English vocabulary practice (Brown and Eskenazi, 2004; Heilman et al., 2006). Approximately half of these texts were authored by students at the particular grade level, and half were authored by teachers or writers and aimed at readers at a particular grade level. Texts were found for grade levels 1 through 12. The twelfth grade level also included some post-secondary level = αj + 0T Xi (2) 75 texts. Various genres and subjects were represented. In all cases, either the text itself or a link to it identified it as having a certain level. The content text was manually extracted from these Web pages so that noisy information such as navigation menus and advertisements w</context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2006</marker>
<rawString>Michael Heilman, Kevyn Collins-Thompson, Jamie Callan, and Maxine Eskenazi. 2006. Classroom success of an Intelligent Tutoring System for lexical practice and reading comprehension. Proceedings of the Ninth International Conference on Spoken Language Processing. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kincaid</author>
<author>R Fishburne</author>
<author>R Rodgers</author>
<author>B Chissom</author>
</authors>
<title>Derivation of new readability formulas for navy enlisted personnel. Branch Report 8-75. Chief of Naval Training,</title>
<date>1975</date>
<location>Millington, TN.</location>
<contexts>
<context position="2494" citStr="Kincaid et al., 1975" startWordPosition="400" endWordPosition="403">nger sentence is more grammatically complex than a shorter one, which is often but not always the case. In one early model, the Dale-Chall model (Dale and Chall, 1948; Chall and Dale, 1995), reading difficulty is a linear function of the mean sentence length and the percentage of rare words, as defined by a list of 3,000 words commonly known by 4th grade. In this paper, sentence length is defined as the mean number of words in the sentences of a text. Many early measures did not employ direct estimates of word frequency due to computational limitations (e.g., (Gunning, 1952; McLaughlin, 1969; Kincaid et al., 1975)). Instead, these measures relied on the strong relationship between the frequency of and the number of syllables in a word. More frequent words are more likely to have fewer syllables (e.g., “the”) than less frequent words (e.g., “vocabulary”), an association that is related to Zipf’s Law (Zipf, 1935). The Flesch-Kincaid measure (Kincaid et al., 1975) is probably the most common reading difficulty measure in use. It is implemented in common word processing programs. This measure is a linear function of the mean number of syllables per word and the mean number of words per sentence. Klare (197</context>
<context position="23770" citStr="Kincaid et al., 1975" startWordPosition="3841" endWordPosition="3844">evels are not always perfect and consistent. For example, one school might read “Romeo and Juliet” in 9th grade while another school might read it in 10th grade. The drawback of this accuracy metric is that predictions that are two levels off are treated the same as predictions that are ten levels off. 4.3 Baselines The performance of other algorithms for estimating reading difficulty was estimated using the same data. These comparison include Collins-Thompson and Callan’s implementation of their language modeling approach (2005), an implementation of the Flesch-Kincaid reading level measure (Kincaid et al., 1975), and a measure using word frequency and sentence length similar to Lexile (Stenner et al., 1983). We did not directly test the approach described by (Heilman et al., 2007). We observe that its reported results for first language texts were not significantly different in terms of correlation and only slightly better in terms of mean squared error than the language modeling approach. Finally, a simple uniform baseline, which always chose the middle value of 6.5, was tested. The Lexile-like measure (LX) used the same two features as the Lexile measure: mean log frequency or words and log mean se</context>
</contexts>
<marker>Kincaid, Fishburne, Rodgers, Chissom, 1975</marker>
<rawString>J. Kincaid, R. Fishburne, R. Rodgers, and B. Chissom. 1975. Derivation of new readability formulas for navy enlisted personnel. Branch Report 8-75. Chief of Naval Training, Millington, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Klare</author>
</authors>
<title>Assessing Readability.</title>
<date>1974</date>
<journal>Reading Research Quarterly,</journal>
<volume>10</volume>
<pages>62--102</pages>
<contexts>
<context position="3096" citStr="Klare (1974)" startWordPosition="502" endWordPosition="503">al., 1975)). Instead, these measures relied on the strong relationship between the frequency of and the number of syllables in a word. More frequent words are more likely to have fewer syllables (e.g., “the”) than less frequent words (e.g., “vocabulary”), an association that is related to Zipf’s Law (Zipf, 1935). The Flesch-Kincaid measure (Kincaid et al., 1975) is probably the most common reading difficulty measure in use. It is implemented in common word processing programs. This measure is a linear function of the mean number of syllables per word and the mean number of words per sentence. Klare (1974) provides a summary of other early work on readability. More recent approaches to reading difficulty employ more sophisticated models that make use of the growth in computational power. The Lexile Framework (e.g., (Stenner, 1996)) uses individual word frequency estimates as a measure of lexical difficulty. The word frequency estimates are derived 71 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71–79, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics from a large, varied corpus of text. Lexile uses a R</context>
</contexts>
<marker>Klare, 1974</marker>
<rawString>G. R. Klare. 1974. Assessing Readability. Reading Research Quarterly, Vol. 10, No. 1. pp. 62-102..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="10625" citStr="Klein and Manning, 2003" startWordPosition="1685" endWordPosition="1688">is reached or the given depth is reached. An example feature for level 2 is a subtree in which a prepositional phrase node dominates a preposition node and noun phrase node, and the preposition node in turn dominates a preposition, and the noun phrase dominates determiner, adjective, and noun nodes. We used a maximum depth of 3 in our experiments. Features of deeper levels occur less frequently in general, and deeper levels were avoided due to data sparseness. A depth first search algorithm extracts candidate grammatical features from the training corpus. First, a context-free grammar parser (Klein and Manning, 2003) derives parse trees for all texts in the training corpus. The algorithm traverses these parses, at each node counting all subtree features up to the given depth that are rooted at that node. The subtree features are sorted by their overall counts in the corpus. In our experiments, frequencies of the most common 1000 subtrees were chosen as the final features. These included 64 level 0 features corresponding to nonterminal symbols, 334 level 1 features, 461 level 2 features, and 141 level 3 features. Deeper levels have more possible features, but sparsity at level 3 resulted in fewer level 3 f</context>
<context position="12306" citStr="Klein and Manning, 2003" startWordPosition="1959" endWordPosition="1962">d in the final feature set. Terminal symbols for content words were omitted so that lexical information was not included in the set of grammatical features. Similar to leaving higher order n-grams out of the lexical feature set, omitting terminal symbols for content words avoids confounding grammatical and lexical information in the grammatical feature set. Subtree counts are normalized by the number of words in a text to compute the relative frequencies. Normalization by the number of sentences in a text is also possible, but did not perform as well in preliminary tests. The Stanford Parser (Klein and Manning, 2003) version 1.5.1 was used to derive tree structures for sentences. We used the unlexicalized model included in the distribution which was trained on Wall Street Journal texts. 3 Statistical Models 3.1 Scales of Measurement for Reading Difficulty Several statistical models were tested for effectiveness at predicting reading difficulty. The appropriateness of these models depends on the nature of 73 Figure 1: Parse Tree for Sentence from Third Grade Text with Example Subtree Features. reading difficulty data, particularly the scale of measurement. The standard unit for reading difficulty is the gr</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pp. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H McLaughlin</author>
</authors>
<title>SMOG grading: A new readability formula.</title>
<date>1969</date>
<journal>Journal of Reading.</journal>
<contexts>
<context position="2471" citStr="McLaughlin, 1969" startWordPosition="398" endWordPosition="399"> assumes that a longer sentence is more grammatically complex than a shorter one, which is often but not always the case. In one early model, the Dale-Chall model (Dale and Chall, 1948; Chall and Dale, 1995), reading difficulty is a linear function of the mean sentence length and the percentage of rare words, as defined by a list of 3,000 words commonly known by 4th grade. In this paper, sentence length is defined as the mean number of words in the sentences of a text. Many early measures did not employ direct estimates of word frequency due to computational limitations (e.g., (Gunning, 1952; McLaughlin, 1969; Kincaid et al., 1975)). Instead, these measures relied on the strong relationship between the frequency of and the number of syllables in a word. More frequent words are more likely to have fewer syllables (e.g., “the”) than less frequent words (e.g., “vocabulary”), an association that is related to Zipf’s Law (Zipf, 1935). The Flesch-Kincaid measure (Kincaid et al., 1975) is probably the most common reading difficulty measure in use. It is implemented in common word processing programs. This measure is a linear function of the mean number of syllables per word and the mean number of words p</context>
</contexts>
<marker>McLaughlin, 1969</marker>
<rawString>G. H. McLaughlin. 1969. SMOG grading: A new readability formula. Journal of Reading.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McCullagh</author>
</authors>
<title>Regression Models for Ordinal Data.</title>
<date>1980</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>42</volume>
<pages>109--142</pages>
<contexts>
<context position="14720" citStr="McCullagh, 1980" startWordPosition="2343" endWordPosition="2344">e data. An example would be types of fruits, where a model might be used to make decisions between apples and oranges. This type of prediction is generally called classification in machine learning and related fields. Ordinal data have a natural ordering, but the values are not necessarily evenly spaced. For example, data about the severity of illnesses might have labels such as mild, moderate, severe, deceased, in which the transitions between consecutive classes all have the same direction but not the same magnitude. Making predictions about such data is generally called ordinal regression (McCullagh, 1980). Interval data, however, are both ordered and evenly spaced. An example would be temperature as measured in Fahrenheit degrees. Such data have an arbitrary zero point, and negative values may occur. Ratio data, of which annual income is an example, do have a meaningful zero point. We will not discuss ratio data further since its distinction from interval data is not relevant to this paper. It is not clear to which scale reading difficulty corresponds. The assumption of an interval scale allows for simpler models with fewer parameters. However, models for ordinal or even nominal data might be </context>
<context position="17363" citStr="McCullagh, 1980" startWordPosition="2777" endWordPosition="2778">or rare cases of an LIN prediction falling outside the appropriate range of grade levels, the value was set to the maximum or minimum grade level. LIN implicitly assumes that the data fall on an interval scale, meaning that the levels are evenly spaced. The LIN model has relatively few parameters but makes strong assumptions about the scale of measurement. For details, see (Hastie et al., 2001). 3.3 Proportional Odds Model The Proportional Odds (PO) model, also called the parallel regression model and the cumulative logit model, is a form of log-linear, or exponential, model for ordinal data (McCullagh, 1980). Given a new unlabeled instance as input, the model provides estimates of the probability that the instance belongs to a class at or above a particular level. In Equation (1), P(y &gt; j) is this estimated probability, αj is an intercept parameter for the given level j, 0 is vector of regression coefficients, Xi is the vector of feature values for instance i, and yi is the predicted reading difficulty level. exp(αj + 0TXi) P(yi &gt;_ j) = 1 + exp(αj + 0TXi) (1) P(yi &gt;_ j) ln 1 − P(yi &gt;_ j) The PO model has a parameter αj for the threshold, or intercept, at each level j, but only a single set 0 of p</context>
</contexts>
<marker>McCullagh, 1980</marker>
<rawString>P. McCullagh. 1980. Regression Models for Ordinal Data. Journal of the Royal Statistical Society. Series B (Methodological), Vol. 42, No. 2. pp. 109-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rasch</author>
</authors>
<title>Probabilistic Models for Some Intelligence and Attainment Tests.</title>
<date>1980</date>
<publisher>MESA Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="3720" citStr="Rasch, 1980" startWordPosition="598" endWordPosition="599">summary of other early work on readability. More recent approaches to reading difficulty employ more sophisticated models that make use of the growth in computational power. The Lexile Framework (e.g., (Stenner, 1996)) uses individual word frequency estimates as a measure of lexical difficulty. The word frequency estimates are derived 71 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71–79, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics from a large, varied corpus of text. Lexile uses a Rasch model (Rasch, 1980) with the mean log word frequency as a lexical feature and the log of the mean sentence length as a grammatical feature. The Rasch model, related to logistic regression, is used to estimate the level of a student that would comprehend 75% of a given text. The converted log odds ratio called a “Lexile” that is used as part of this measure can be easily mapped to grade school levels. A reading difficulty measure developed by Collins-Thompson and Callan (2005) uses smoothed unigram language modeling to capture the predictive ability of individual words based on their frequency at each reading dif</context>
</contexts>
<marker>Rasch, 1980</marker>
<rawString>G. Rasch. 1980. Probabilistic Models for Some Intelligence and Attainment Tests. MESA Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rasch</author>
</authors>
<title>American National Corpus (ANC) Second Release.. Linguistic Data Consortium.</title>
<date>2005</date>
<location>Philadelphia, PA.</location>
<marker>Rasch, 2005</marker>
<rawString>G. Rasch. 2005. American National Corpus (ANC) Second Release.. Linguistic Data Consortium. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Schwarm</author>
<author>Mari Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5089" citStr="Schwarm and Ostendorf (2005)" startWordPosition="815" endWordPosition="818">redictive of grade 1, and “essay” was very predictive of grade 12. For a given text, this measure estimates the likelihood that the text was generated by each level’s language model. The prediction is the level of the model with the highest likelihood of generating the text. There are no grammatical features. Natural language processing techniques enable more sophisticated grammatical analysis for reading difficulty measures. Rather than using sentence length as a proxy, measures can employ tools for automatic analysis of the syntactic structure of texts (e.g., (Charniak, 2000)). A measure by Schwarm and Ostendorf (2005) incorporates syntactic analyses, among a variety of other types of features. It includes four grammatical features derived from syntactic parses of text: the mean parse tree height, the mean number of noun phrases, mean number of verb phrases, and mean number of “SBARs.” “SBARs” are non-terminal nodes that are associated with subordinate clauses. Their system led to better predictions than the Flesch-Kincaid and Lexile measures, but the predictive value of the grammatical features is not entirely clear. In initial experiments using such course-grain grammatical features alone, rather than in </context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>Sarah Schwarm and Mari Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Stenner</author>
<author>M Smith</author>
<author>D S Burdick</author>
</authors>
<title>Toward a Theory of Construct Definition.</title>
<date>1983</date>
<journal>Journal of Educational Measurement,</journal>
<volume>20</volume>
<pages>305--316</pages>
<contexts>
<context position="23867" citStr="Stenner et al., 1983" startWordPosition="3857" endWordPosition="3860">” in 9th grade while another school might read it in 10th grade. The drawback of this accuracy metric is that predictions that are two levels off are treated the same as predictions that are ten levels off. 4.3 Baselines The performance of other algorithms for estimating reading difficulty was estimated using the same data. These comparison include Collins-Thompson and Callan’s implementation of their language modeling approach (2005), an implementation of the Flesch-Kincaid reading level measure (Kincaid et al., 1975), and a measure using word frequency and sentence length similar to Lexile (Stenner et al., 1983). We did not directly test the approach described by (Heilman et al., 2007). We observe that its reported results for first language texts were not significantly different in terms of correlation and only slightly better in terms of mean squared error than the language modeling approach. Finally, a simple uniform baseline, which always chose the middle value of 6.5, was tested. The Lexile-like measure (LX) used the same two features as the Lexile measure: mean log frequency or words and log mean sentence length. Instead of using a Rasch model and converting scores to “Lexiles,” however, the PO</context>
</contexts>
<marker>Stenner, Smith, Burdick, 1983</marker>
<rawString>A. J. Stenner, M. Smith, and D. S. Burdick. 1983. Toward a Theory of Construct Definition. Journal of Educational Measurement, Vol. 20, No. 4. pp. 305-316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Stenner</author>
</authors>
<title>Measuring reading comprehension with the Lexile framework.</title>
<date>1996</date>
<booktitle>Fourth North American Conference on Adolescent/Adult Literacy.</booktitle>
<contexts>
<context position="3325" citStr="Stenner, 1996" startWordPosition="538" endWordPosition="539"> words (e.g., “vocabulary”), an association that is related to Zipf’s Law (Zipf, 1935). The Flesch-Kincaid measure (Kincaid et al., 1975) is probably the most common reading difficulty measure in use. It is implemented in common word processing programs. This measure is a linear function of the mean number of syllables per word and the mean number of words per sentence. Klare (1974) provides a summary of other early work on readability. More recent approaches to reading difficulty employ more sophisticated models that make use of the growth in computational power. The Lexile Framework (e.g., (Stenner, 1996)) uses individual word frequency estimates as a measure of lexical difficulty. The word frequency estimates are derived 71 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71–79, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics from a large, varied corpus of text. Lexile uses a Rasch model (Rasch, 1980) with the mean log word frequency as a lexical feature and the log of the mean sentence length as a grammatical feature. The Rasch model, related to logistic regression, is used to estimate the level of a </context>
</contexts>
<marker>Stenner, 1996</marker>
<rawString>A. J. Stenner. 1996. Measuring reading comprehension with the Lexile framework. Fourth North American Conference on Adolescent/Adult Literacy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Stevens</author>
</authors>
<title>On the theory of scales of measurement.</title>
<date>1946</date>
<journal>Science,</journal>
<volume>103</volume>
<pages>677--680</pages>
<contexts>
<context position="14008" citStr="Stevens, 1946" startWordPosition="2226" endWordPosition="2227">aced. It is possible, of course, that assuming even spacing between levels might produce more parsimonious and accurate statistical models. A more reasonable assumption is that the grade numbers assigned to each difficulty level denote an ordering: for example, that grade 1 is in some sense less than grade 2, which is less than grade 3, etc. Different statistical models handle this assumption more or less well. Statistics generally distinguish four scales of measurement, which are, ordered by increasing assumptions about the relationships between values: nominal, ordinal, interval, and ratio (Stevens, 1946; Cohen et al., 2003). Nominal data involve no relationships between the labels or classes of the data. An example would be types of fruits, where a model might be used to make decisions between apples and oranges. This type of prediction is generally called classification in machine learning and related fields. Ordinal data have a natural ordering, but the values are not necessarily evenly spaced. For example, data about the severity of illnesses might have labels such as mild, moderate, severe, deceased, in which the transitions between consecutive classes all have the same direction but not</context>
</contexts>
<marker>Stevens, 1946</marker>
<rawString>S. S. Stevens. 1946. On the theory of scales of measurement. Science, 103, pp. 677-680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="15996" citStr="Vapnik, 1995" startWordPosition="2552" endWordPosition="2554">does not hold. We experimented with three linear and log-linear models corresponding to interval, ordinal, and nominal data. Parameters were estimated using L2 regularization, which corresponds to a Gaussian prior distribution with zero mean and a user-specified variance over the parameters. We chose these models because they are commonly used in the statistics, machine learning, and behavioral science communities, and aimed to set up meaningful comparisons among the scales of measurement. Other machine learning algorithms might also be employed. In fact, we briefly tested the maximum margin (Vapnik, 1995) approach, which led to comparable results and might be worth exploring in future work. 74 3.2 Linear Regression Linear Regression (LIN) produces a linear model in which the dependent, or outcome, variable is a linear function of the values for predictor variables, or features. A prediction for a given text is the inner product of a vector of feature values for the text and a vector of regression coefficients estimated from training data. For the case of reading difficulty, the grade level is a linear combination of the lexical and/or grammatical feature values. LIN provides continuous estimat</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J P Pedersen</author>
</authors>
<title>A Comparative Study on Feature Selection in Text Categorization.</title>
<date>1997</date>
<booktitle>Proceedings of the Fourteenth International Conference on Machine Learning (ICML’97),</booktitle>
<pages>412--420</pages>
<contexts>
<context position="7971" citStr="Yang and Pedersen, 1997" startWordPosition="1261" endWordPosition="1264">rammatical features (Heilman et al., 2007) by making it possible to include a large number of grammatical features derived from syntactic structures without requiring significant linguistic or pedagogical content knowledge, such as a reference guide for the grammar of the language of interest. 2 Types of Features 2.1 Lexical Features This section and the following section describe the lexical and grammatical features used in our reading difficulty models. The lexical features are the relative frequencies of word unigrams. The use of word unigrams is a standard approach in text classification (Yang and Pedersen, 1997), and has also been successfully used to predict reading difficulty (Collins-Thompson and Callan, 2005). Higher order n-grams such as bigrams and trigrams were not used as features because they did not improve predictions 72 in preliminary tests. The specific set of lexical features was chosen based on the frequencies of words in the training corpus. The system performs morphological stemming and stopword removal. The remaining 5000 most common words comprised the lexical feature set. 2.2 Grammatical Features Grammatical features are extracted from automatic context-free grammar parses of sent</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Y. Yang and J. P. Pedersen. 1997. A Comparative Study on Feature Selection in Text Categorization. Proceedings of the Fourteenth International Conference on Machine Learning (ICML’97), pp. 412-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<title>The Psychobiology off Language.</title>
<date>1935</date>
<location>Houghton Mifflin, Boston, MA.</location>
<contexts>
<context position="2797" citStr="Zipf, 1935" startWordPosition="452" endWordPosition="453"> of 3,000 words commonly known by 4th grade. In this paper, sentence length is defined as the mean number of words in the sentences of a text. Many early measures did not employ direct estimates of word frequency due to computational limitations (e.g., (Gunning, 1952; McLaughlin, 1969; Kincaid et al., 1975)). Instead, these measures relied on the strong relationship between the frequency of and the number of syllables in a word. More frequent words are more likely to have fewer syllables (e.g., “the”) than less frequent words (e.g., “vocabulary”), an association that is related to Zipf’s Law (Zipf, 1935). The Flesch-Kincaid measure (Kincaid et al., 1975) is probably the most common reading difficulty measure in use. It is implemented in common word processing programs. This measure is a linear function of the mean number of syllables per word and the mean number of words per sentence. Klare (1974) provides a summary of other early work on readability. More recent approaches to reading difficulty employ more sophisticated models that make use of the growth in computational power. The Lexile Framework (e.g., (Stenner, 1996)) uses individual word frequency estimates as a measure of lexical diffi</context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>G. K. Zipf. 1935. The Psychobiology off Language. Houghton Mifflin, Boston, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>