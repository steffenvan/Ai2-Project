<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000140">
<title confidence="0.993367">
Combining Heterogeneous Models for Measuring Relational Similarity
</title>
<author confidence="0.531555333333333">
Alisa Zhila*
Instituto Politecnico Nacional
Mexico City, Mexico
</author>
<email confidence="0.916369">
alisa.zhila@gmail.com
</email>
<author confidence="0.994062">
Geoffrey Zweig
</author>
<affiliation confidence="0.960805">
Microsoft Research
</affiliation>
<address confidence="0.940224">
Redmond, WA 98052, USA
</address>
<email confidence="0.998765">
gzweig@microsoft.com
</email>
<author confidence="0.970833">
Wen-tau Yih Christopher Meek
</author>
<affiliation confidence="0.944041">
Microsoft Research
</affiliation>
<address confidence="0.927014">
Redmond, WA 98052, USA
</address>
<email confidence="0.99487">
{scottyih,meek}@microsoft.com
</email>
<author confidence="0.99129">
Tomas Mikolov*
</author>
<affiliation confidence="0.828038">
BRNO University of Technology
BRNO, Czech Republic
</affiliation>
<email confidence="0.997418">
tmikolov@gmail.com
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874928571429">
In this work, we study the problem of mea-
suring relational similarity between two word
pairs (e.g., silverware:fork and clothing:shirt).
Due to the large number of possible relations,
we argue that it is important to combine mul-
tiple models based on heterogeneous informa-
tion sources. Our overall system consists of
two novel general-purpose relational similar-
ity models and three specific word relation
models. When evaluated in the setting of a
recently proposed SemEval-2012 task, our ap-
proach outperforms the previous best system
substantially, achieving a 54.1% relative in-
crease in Spearman’s rank correlation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999476745098039">
The problem of measuring relational similarity is
to determine the degree of correspondence between
two word pairs. For instance, the analogous word
pairs silverware:fork and clothing:shirt both exem-
plify well a Class-Inclusion:Singular Collective re-
lation and thus have high relational similarity. Un-
like the problem of attributional similarity, which
measures whether two words share similar attributes
and is addressed in extensive research work (Bu-
danitsky and Hirst, 2006; Reisinger and Mooney,
2010; Radinsky et al., 2011; Agirre et al., 2009; Yih
and Qazvinian, 2012), measuring relational similar-
ity is a relatively new research direction pioneered
by Turney (2006), but with many potential appli-
cations. For instance, problems of identifying spe-
cific relations between words, such as synonyms,
*Work conducted while interning at Microsoft Research.
antonyms or associations, can be reduced to mea-
suring relational similarity compared to prototypical
word pairs with the desired relation (Turney, 2008).
In scenarios like information extraction or question
answering, where identifying the existence of cer-
tain relations is often the core problem, measuring
relational similarity provides a more flexible solu-
tion rather than creating relational classifiers for pre-
defined or task-specific categories of relations (Tur-
ney, 2006; Jurgens et al., 2012).
In order to promote this research direction, Ju-
rgens et al. (2012) proposed a new shared task of
measuring relational similarity in SemEval-2012 re-
cently. In this task, each submitted system is re-
quired to judge the degree of a target word pair
having a particular relation, measured by its re-
lational similarity compared to a few prototypical
example word pairs. The system performance is
evaluated by its correlation with the human judg-
ments using two evaluation metrics, Spearman’s
rank correlation and MaxDiff accuracy (more de-
tails of the task and evaluation metrics will be given
in Sec. 3). Although participating systems incorpo-
rated substantial amounts of information from lex-
ical resources (e.g., WordNet) and contextual pat-
terns from large corpora, only one system (Rink and
Harabagiu, 2012) is able to outperform a simple
baseline that uses PMI (pointwise mutual informa-
tion) scoring, which demonstrates the difficulty of
this task.
In this paper, we explore the problem of mea-
suring relational similarity in the same task setting.
We argue that due to the large number of possible
relations, building an ensemble of relational simi-
</bodyText>
<page confidence="0.870423">
1000
</page>
<note confidence="0.46988">
Proceedings of NAACL-HLT 2013, pages 1000–1009,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999855371428571">
larity models based on heterogeneous information
sources is the key to advance the state-of-the-art on
this problem. By combining two general-purpose re-
lational similarity models with three specific word-
relation models covering relations like IsA and syn-
onymy/antonymy, we improve the previous state-
of-the-art substantially – having a relative gain of
54.1% in Spearman’s rank correlation and 14.7% in
the MaxDiff accuracy!
Our main contributions are threefold. First, we
propose a novel directional similarity method based
on the vector representation of words learned from
a recurrent neural network language model. The re-
lation of two words is captured by their vector off-
set in the latent semantic space. Similarity of rela-
tions can then be naturally measured by a distance
function in the vector space. This method alone
already performs better than all existing systems.
Second, unlike the previous finding, where SVMs
learn a much poorer model than naive Bayes (Rink
and Harabagiu, 2012), we show that using a highly-
regularized log-linear model on simple contextual
pattern features collected from a document collec-
tion of 20GB, a discriminative approach can learn a
strong model as well. Third, we demonstrate that by
augmenting existing word-relation models, which
cover only a small number of relations, the overall
system can be further improved.
The rest of this paper is organized as follows. We
first survey the related work in Sec. 2 and formally
define the problem in Sec. 3. We describe the indi-
vidual models in detail in Sec. 4. The combination
approach is depicted in Sec. 5, along with experi-
mental comparisons to individual models and exist-
ing systems. Finally, Sec. 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999877275862069">
Building a classifier to determine whether a relation-
ship holds between a pair of words is a natural ap-
proach to the task of measuring relational similarity.
While early work was mostly based on hand-crafted
rules (Finin, 1980; Vanderwende, 1994), Rosario
and Hearst (2001) introduced a machine learning ap-
proach to classify word pairs. They targeted clas-
sifying noun modifier pairs from the medical do-
main into 13 classes of semantic relations. Fea-
tures for each noun modifier pair were constructed
using large medical lexical resources and a multi-
class classifier was trained using a feed-forward neu-
ral network with one hidden layer. This work was
later extended by Nastase and Szpakowicz (2003)
to classify general domain noun-modifier pairs into
30 semantic relations. In addition to extracting fea-
tures using WordNet and Roget’s Thesaurus, they
also experimented with several different learners in-
cluding decision trees, memory-based learning and
inductive logic programming methods like RIPPER
and FOIL. Using the same dataset as in (Nastase
and Szpakowicz, 2003), Turney and Littman (2005)
created a 128-dimentional feature vector for each
word pair based on statistics of their co-occurrence
patterns in Web documents and applied the k-NN
method (k = 1 in their work).
Measuring relational similarity, which determines
whether two word pairs share the same relation, can
be viewed as an extension of classifying relations
between two words. Treating a relational similar-
ity measure as a distance metric, a testing pair of
words can be judged by whether they have a rela-
tion that is similar to some prototypical word pairs
having a particular relation. A multi-relation clas-
sifier can thus be built easily in this framework as
demonstrated in (Turney, 2008), where the prob-
lems of identifying synonyms, antonyms and asso-
ciated words are all reduced to finding good anal-
ogous word pairs. Measuring relational similarity
has been advocated and pioneered by Turney (2006),
who proposed a latent vector space model for an-
swering SAT analogy questions (e.g., mason:stone
vs. carpenter:wood). In contrast, we take a slightly
different view when building a relational similarity
measure. Existing classifiers for specific word re-
lations (e.g., synonyms or Is-A) are combined with
general relational similarity measures. Empirically,
mixing heterogeneous models tends to make the fi-
nal relational similarity measure more robust.
Although datasets for semantic relation classifica-
tion or SAT analogous questions can be used to eval-
uate a relational similarity model, their labels are ei-
ther binary or categorical, which makes the datasets
suboptimal for determining the quality of a model
when evaluated on instances of the same relation
class. As a result, Jurgens et al. (2012) proposed a
new task of “Measuring Degrees of Relational Simi-
larity” at SemEval-2012, which includes 79 relation
</bodyText>
<page confidence="0.984217">
1001
</page>
<bodyText confidence="0.999981441176471">
categories exemplified by three or four prototypical
word pairs and a schematic description. For exam-
ple, for the Class-Inclusion:Taxonomic relation, the
schematic description is “Y is a kind/type/instance
of X”. Using Amazon Mechanical Turk1, they col-
lected word pairs for each relation, as well as their
degrees of being a good representative of a partic-
ular relation when compared with defining exam-
ples. Participants of this shared task proposed var-
ious kinds of approaches that leverage both lexical
resources and general corpora. For instance, the
Duluth systems (Pedersen, 2012) created word vec-
tors based on WordNet and estimated the degree of
a relation using cosine similarity. The BUAP sys-
tem (Tovar et al., 2012) represented each word pair
as a whole by a vector of 4 different types of fea-
tures: context, WordNet, POS tags and the aver-
age number of words separating the two words in
text. The degree of relation was then determined
by the cosine distance of the target pair from the
prototypical examples of each relation. Although
their models incorporated a significant amount of
information of words or word pairs, unfortunately,
the performance were not much better than a ran-
dom baseline, which indicates the difficulty of this
task. In comparison, a supervised learning approach
seems more promising. The UTD system (Rink and
Harabagiu, 2012), which mined lexical patterns be-
tween co-occurring words in the corpus and then
used them as features to train a Naive Bayes classi-
fier, achieved the best results. However, potentially
due to the large feature space, this strategy did not
work as well when switching the learning algorithm
to SVMs.
</bodyText>
<sectionHeader confidence="0.982215" genericHeader="method">
3 Problem Definition &amp; Task Description
</sectionHeader>
<bodyText confidence="0.9997611">
Following the setting of SemEval-2012 Task 2 (Ju-
rgens et al., 2012), the problem of measur-
ing the degree of relational similarity is to rate
word pairs by the degree to which they are
prototypical members of a given relation class.
For instance, comparing to the prototypical word
pairs, {cutlery:spoon, clothing:shirt, vermin:rat} of
the Class-Inclusion:Singular Collective relation, we
would like to know among the input word pairs
{dish:bowl, book:novel, furniture:desk}, which one
</bodyText>
<footnote confidence="0.960353">
1http://www.mturk.com
</footnote>
<bodyText confidence="0.999904833333333">
best demonstrates the relation.
Because our approaches are evaluated using the
data provided in this SemEval-2012 task, we de-
scribe briefly below how the data was collected, as
well as the metrics used to evaluate system perfor-
mance. The dataset consists of 79 relation classes
that are chosen according to (Bejar et al., 1991)
and broadly fall into 10 main categories, includ-
ing Class-Inclusion, Part-Whole, Similar and more.
With the help of Amazon Mechanical Turk, Jurgens
et al. (2012) used a two-phase approach to collect
word pairs and their degrees. In the first phase,
a lexical schema, such as “a Y is one item in a
collection/group of X” for the aforementioned rela-
tion Class-Inclusion:Singular Collective, and a few
prototypical pairs for each class were given to the
workers, who were asked to provide approximately
a list of 40 word pairs representing the same rela-
tion class. Naturally, some of these pairs were bet-
ter examples than the others. Therefore, in the sec-
ond phase, the goal was to measure the degree of
their similarity to the corresponding relation. This
was done using the MaxDiff technique (Louviere
and Woodworth, 1991). For each relation, about one
hundred questions were first created. Each question
consists of four different word pairs randomly sam-
pled from the list. The worker was then asked to
choose the most and least representative word pairs
for the specific relation in each question.
The set of 79 word relations were randomly split
into training and testing sets. The former contains
10 relations and the latter has 69. Word pairs in all
79 relations were given to the task participants in ad-
vance, but only the human judgments of the training
set were available for system development. In this
work, we treat the training set as the validation set
– all the model exploration and refinement is done
using this set of data, as well as the hyper-parameter
tuning when learning the final model combination.
The quality of a relational similarity measure is
estimated by its correlation to human judgments.
This is evaluated using two metrics in the task: the
MaxDiff accuracy and Spearman’s rank correlation
coefficient (p). A system is first asked to pick the
most and least representative word pairs of each
question in the MaxDiff setting. The average accu-
racy of the predictions compared to the human an-
swers is then reported. In contrast, Spearman’s p
</bodyText>
<page confidence="0.982058">
1002
</page>
<bodyText confidence="0.998093">
measures the correlation between the total orderings
of all word pairs of a relation, where the total order-
ing is derived from the MaxDiff answers (see (Jur-
gens et al., 2012) for the exact procedure).
</bodyText>
<sectionHeader confidence="0.997471" genericHeader="method">
4 Models for Relational Similarity
</sectionHeader>
<bodyText confidence="0.999974545454545">
We investigate three types of models for relational
similarity. Operating in a word vector space, the di-
rectional similarity model compares the vector dif-
ferences of target and prototypical word pairs to es-
timate their relational similarity. The lexical pat-
tern method collects contextual information of pairs
of words when they co-occur in large corpora, and
learns a highly regularized log-linear model. Finally,
the word relation models incorporate existing, spe-
cific word relation measures for general relational
similarity.
</bodyText>
<subsectionHeader confidence="0.971385">
4.1 Directional Similarity Model
</subsectionHeader>
<bodyText confidence="0.999099535714286">
Our first model for relational similarity extends pre-
vious work on semantic word vector representa-
tions to a directional similarity model for pairs of
words. There are many different methods for cre-
ating real-valued semantic word vectors, such as
the distributed representation derived from a word
co-occurrence matrix and a low-rank approxima-
tion (Landauer et al., 1998), word clustering (Brown
et al., 1992) and neural-network language model-
ing (Bengio et al., 2003; Mikolov et al., 2010). Each
element in the vectors conceptually represents some
latent topicality information of the word. The goal
of these methods is that words with similar mean-
ings will tend to be close to each other in the vector
space.
Although the vector representation of single
words has been successfully applied to problems
like semantic word similarity and text classifica-
tion (Turian et al., 2010), the issue of how to repre-
sent and compare pairs of words in a vector space
remains unclear (Turney, 2012). In a companion
paper (Mikolov et al., 2013), we present a vector
offset method which performs consistently well in
identifying both syntactic and semantic regularities.
This method measures the degree of the analogy
“a is to b as c is to d” using the cosine score of
(~vb − ~va + ~v�,~vd), where a, b, c, d are the four given
words and ~va,~vb,~v,~vd are the corresponding vec-
</bodyText>
<figureCaption confidence="0.833789">
Figure 1: Directional vectors υ1 and υ2 capture the rela-
tions of clothing:shirt and furniture:desk respectively in
this semantic vector space. The relational similarity of
these two word pairs is estimated by the cosine of θ.
</figureCaption>
<bodyText confidence="0.999548727272727">
tors. In this paper, we propose a variant called the
directional similarity model, which performs bet-
ter for semantic relations. Let ωi = (wi1, wi2) and
ωj = (wj1, wj2) be the two word pairs being com-
pared. Suppose (~vi1,~vi2) and (~vj1,~vj2) are the cor-
responding vectors of these words. The directional
vectors of ωi and ωj are defined as ~υi - ~vi2 − ~vi1
and ~υj - ~vj2 − ~vj1, respectively. Relational simi-
larity of these two word pairs can be measured by
some distance function of υi and υj, such as the co-
sine function:
</bodyText>
<equation confidence="0.7678215">
~υi · ~υj
ll~υillll~υjll
</equation>
<bodyText confidence="0.999870333333333">
The rationale behind this variant is as follows. Be-
cause the difference of two word vectors reveals the
change from one word to the other in terms of mul-
tiple topicality dimensions in the vector space, two
word pairs having similar offsets (i.e., being rela-
tively parallel) can be interpreted as they have simi-
lar relations. Fig. 1 further illustrates this method.
Compared to the original method, this variant
places less emphasis on the similarity between
words wj1 and wj2. That similarity is necessary
for syntactic relations where the words are often re-
lated by morphology, but not for semantic relations.
On semantic relations studied in this paper, the di-
rectional similarity model performs about 18% rela-
tively better in Spearman’s ρ than the original one.
The quality of the directional similarity method
depends heavily on the underlying word vector
space model. We compared two choices with dif-
</bodyText>
<table confidence="0.829957">
shirt
desk
V2
furniture
V1
clothing
VP
2
1003
Word Embedding Spearman’s p MaxDiff Acc. (%)
LSA-80 0.055 34.6
LSA-320 0.066 34.4
LSA-640 0.102 35.7
RNNLM-80 0.168 37.5
RNNLM-320 0.214 39.1
RNNLM-640 0.221 39.2
RNNLM-1600 0.234 41.2
</table>
<tableCaption confidence="0.908743">
Table 1: Results of measuring relational similarity using
the directional similarity method, evaluated on the train-
ing set. The 1600-dimensional RNNLM vector space
achieves the highest Spearman’s p and MaxDiff accuracy.
</tableCaption>
<bodyText confidence="0.99997447368421">
ferent dimensionality settings: the word embedding
learned from the recurrent neural network language
model (RNNLM)2 and the LSA vectors, both were
trained using the same Broadcast News corpus of
320M words as described in (Mikolov et al., 2011).
All the word vectors were first normalized to unit
vectors before applying the directional similarity
method. Given a target word pair, we computed
its relational similarity compared with the prototyp-
ical word pairs of the same relation. The average
of these measurements was taken as the final model
score. Table 1 summarizes the results when evalu-
ated on the training set. As shown in the table, the
RNNLM vectors consistently outperform their LSA
counterparts with the same dimensionality. In addi-
tion, more dimensions seem to preserve more infor-
mation and lead to better performance. Therefore,
we take the 1600-dimensional RNNLM vectors to
construct our final directional similarity model.
</bodyText>
<subsectionHeader confidence="0.963248">
4.2 Lexical Pattern Model
</subsectionHeader>
<bodyText confidence="0.99995225">
Our second model for measuring relational similar-
ity is built based on lexical patterns. It is well-known
that contexts in which two words co-occur often pro-
vide useful cues for identifying the word relation.
For example, having observed frequent text frag-
ments like “X such as Y”, it is likely that there is a
Class-Inclusion:Taxonomic relation between X and
Y; namely, Y is a type of X. Indeed, by mining lexical
patterns from a large corpus, the UTD system (Rink
and Harabagiu, 2012) managed to outperform other
participants in the SemEval-2012 task of measuring
relational similarity.
</bodyText>
<footnote confidence="0.78379">
2http://www.fit.vutbr.cz/˜imikolov/rnnlm
</footnote>
<bodyText confidence="0.999982166666667">
In order to find more co-occurrences of each pair
of words, we used a large document set that con-
sists of the Gigaword corpus (Parker et al., 2009),
Wikipedia and LA Times articles3, summing up to
more than 20 Gigabytes of texts. For each word
pair (w1, w2) that co-occur in a sentence, we col-
lected the words in between as its context (or so-
called “raw pattern”). For instance, “such as” would
be the context extracted from “X such as Y” for
the word pair (X, Y). To reduce noise, contexts with
more than 9 words were dropped and 914,295 pat-
terns were collected in total.
Treating each raw pattern as a feature where the
value is the logarithm of the occurrence count, we
then built a probabilistic classifier to determine the
association of the context and relation. For each re-
lation, we treated all its word pairs as positive ex-
amples and all the word pairs in other relations as
negative examples4. 79 classifiers were trained in
total, where each one was trained using 3,218 ex-
amples. The degree of relational similarity of each
word pair can then be judged by the output of the
corresponding classifier5. Although this seems like a
standard supervised learning setting, the large num-
ber of features poses a challenge here. Using almost
1M features and 3,218 examples, the model could
easily overfit if not regularized properly, which may
explain why learning SVMs on pattern features per-
formed poorly (Rink and Harabagiu, 2012). In-
stead of employing explicit feature selection meth-
ods, we used an efficient L1 regularized log-linear
model learner (Andrew and Gao, 2007) and chose
the hyper-parameters based on model performance
on the training data. The final models we chose
were trained with L1 = 3, where 28,065 features
in average were selected automatically by the algo-
</bodyText>
<footnote confidence="0.945666692307692">
3We used a Nov-2010 dump of English Wikipedia, which
contains approximately 917M words after pre-processing. The
LA Times corpus consists of articles from 1985 to 2002 and has
about 1.1B words.
4Given that not all word pairs belonging to the same relation
category are equally good, removing those with low judgment
scores may help improve the quality of the labeled data. We
leave this study to future work.
5Training a separate classifier for each MaxDiff question us-
ing all words pairs except the four target pairs appears to be a
better setting, as it would avoid including the target pairs in the
training process. We did not use this setting because it is more
complicated and performed roughly the same empirically.
</footnote>
<page confidence="0.99752">
1004
</page>
<bodyText confidence="0.9943225">
rithm. The performance on the training data is 0.322
in Spearman’s p and 41.8% in MaxDiff accuracy.
</bodyText>
<subsectionHeader confidence="0.997383">
4.3 Word Relation Models
</subsectionHeader>
<bodyText confidence="0.999953916666667">
The directional similarity and lexical pattern mod-
els can be viewed as general purpose methods for
relational similarity as they do not differentiate the
specific relation categories. In contrast, for specific
word relations, there exist several high-quality meth-
ods. Although they are designed for detecting spe-
cific relations between words, incorporating them
could still improve the overall results. Next, we ex-
plore the use of some of these word relation mod-
els, including information encoded in the knowledge
base and a lexical semantic model for synonymy and
antonymy.
</bodyText>
<subsectionHeader confidence="0.486352">
4.3.1 Knowledge Bases
</subsectionHeader>
<bodyText confidence="0.999984172413793">
Predetermined types of relations can often be
found in existing lexical and knowledge databases,
such as WordNet’s Is-A taxonomy and the exten-
sive relations stored in the NELL (Carlson et al.,
2010) knowledge base. Although in theory, these
resources can be directly used to solve the problem
of relational similarity, such direct approaches often
suffer from two practical issues. First, the word cov-
erage of these databases is usually very limited and
it is common that the relation of a given word pair
is absent. Second, the degree of relation is often not
included, which makes the task of measuring the de-
gree of relational similarity difficult.
One counter example, however, is Probase (Wu
et al., 2012), which is a knowledge base that es-
tablishes connections between more than 2.5 mil-
lion concepts discovered automatically from the
Web. For the Is-A and Attribute relations it en-
codes, Probase also returns the probability that two
input words share the relation, based on the co-
occurrence frequency. We used some relations in
the training set to evaluate the quality of Probase.
For instance, its Is-A model performs exception-
ally well on the relation Class-Inclusion:Taxonomic,
reaching a high Spearman’s p = 0.642 and MaxD-
iff accuracy 55.8%. Similarly, its Attribute model
performs better than our lexical pattern model
on Attribute:Agent Attribute-State with Spearman’s
p = 0.290 and MaxDiff accuracy 32.7%.
</bodyText>
<subsectionHeader confidence="0.958207">
4.3.2 Lexical Semantics Measures
</subsectionHeader>
<bodyText confidence="0.999977631578947">
Most lexical semantics measures focus on the se-
mantic similarity or relatedness of two words. Since
our task focuses on distinguishing the difference be-
tween word pairs in the same relation category. The
crude relatedness model does not seem to help in our
preliminary experimental study. Instead, we lever-
age the recently proposed polarity-inducing latent
semantic analysis (PILSA) model (Yih et al., 2012),
which specifically estimates the degree of synonymy
and antonymy. This method first forms a signed co-
occurrence matrix using synonyms and antonyms in
a thesaurus and then generalizes it using a low-rank
approximation derived by SVD. Given two words,
the cosine score of their PILSA vectors tend to be
negative if they are antonymous and positive if syn-
onymous. When tested on the Similar:Synonymity
relation, it has a Spearman’s p = 0.242 and MaxD-
iff accuracy 42.1%, both are better than those of our
directional similarity and lexical pattern models.
</bodyText>
<sectionHeader confidence="0.986652" genericHeader="method">
5 Model Combination
</sectionHeader>
<bodyText confidence="0.99998925">
In order to fully leverage the diverse models pro-
posed in Sec. 4, we experiment with a model combi-
nation approach and conduct a model ablation study.
Performance of the combined and individual models
is evaluated using the test set and compared with ex-
isting systems.
We seek an optimal linear combination of all the
individual models by treating their output as fea-
tures and use a logistic regression learner to learn
the weights6. The training setting is essentially the
same as the one used to learn the lexical pattern
model (Sec. 4.2). For each relation, we treat all the
word pairs in this relation group as positive exam-
ples and all other word pairs as negative ones. Con-
sequently, 79 sets of weights for model combination
are learned in total. The average Spearman’s p of the
10 training relations is used for selecting the values
of the L1 and L2 regularizers7. Evaluated on the re-
maining 69 relations (i.e., the test set), the average
results of each main relation group and the overall
</bodyText>
<footnote confidence="0.995078">
6Nonlinear methods, such as MART (Friedman, 2001), do
not perform better in our experiments (not reported here).
7We tested 15 combinations, where L1 E {0, 0.01, 0.11 and
L2 E {0, 0.001, 0.01, 1, 101. The parameter setting that gave
the highest Spearman rank correlation coefficient score on the
training set was selected.
</footnote>
<page confidence="0.877128">
1005
</page>
<table confidence="0.999952916666667">
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 0.057 0.064 0.045 0.233 0.350 0.422 0.619 -0.137 0.029 0.519
Part-Whole 0.012 0.066 -0.061 0.252 0.317 0.244 -0.014 0.026 -0.010 0.329
Similar 0.026 -0.036 0.183 0.214 0.254 0.245 -0.020 0.133 0.058 0.303
Contrast -0.049 0.000 0.142 0.206 0.063 0.298 -0.012 -0.032 -0.079 0.268
Attribute 0.037 -0.095 0.044 0.158 0.431 0.198 -0.008 0.016 -0.052 0.406
Non-Attribute -0.070 0.009 0.079 0.098 0.195 0.117 0.036 0.078 -0.093 0.296
Case Relations 0.090 -0.037 -0.011 0.241 0.503 0.288 0.076 -0.075 0.059 0.473
Cause-Purpose -0.011 0.114 0.021 0.183 0.362 0.234 0.044 -0.059 0.038 0.296
Space-Time 0.013 0.035 0.055 0.375 0.439 0.248 0.064 -0.002 -0.018 0.443
Reference 0.142 -0.001 0.028 0.346 0.301 0.119 0.033 -0.123 0.021 0.208
Average 0.018 0.014 0.050 0.229 0.3241 0.235 0.058# -0.010# -0.009# 0.353#
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 30.1 29.0 26.7 39.1 46.7 43.4 59.6 24.7 32.3 51.2
Part-Whole 31.9 35.1 29.4 40.9 43.9 38.1 31.3 29.5 31.0 42.9
Similar 31.5 29.1 37.1 39.8 38.5 38.4 30.8 36.3 34.2 43.3
Contrast 30.4 32.4 38.3 40.9 33.6 42.2 32.3 31.8 30.1 42.8
Attribute 30.2 29.2 31.9 36.5 47.9 38.3 30.7 31.0 28.8 48.3
Non-Attribute 28.9 30.4 36.0 36.8 38.7 36.7 32.3 32.8 27.7 42.6
Case Relations 32.8 29.5 28.2 40.6 54.3 42.2 32.8 25.7 31.0 50.6
Cause-Purpose 30.8 35.4 29.5 36.3 45.3 38.0 30.3 28.1 32.0 41.7
Space-Time 30.6 32.5 31.9 43.2 50.0 39.2 33.2 29.3 30.6 47.7
Reference 35.1 30.0 31.9 41.2 45.7 36.9 30.4 27.2 30.2 42.5
Average 31.2 31.7 32.4 39.4 44.5# 39.2 33.3# 29.8# 30.7# 45.2#
</table>
<tableCaption confidence="0.993735">
Table 2: Average Spearman’s p (Top) and MaxDiff accuracy (%) (Bottom) of each major relation group and all 69
</tableCaption>
<bodyText confidence="0.962891263157895">
testing relations. The best result in each row is highlighted in boldface font. Statistical significance tests are conducted
by comparing each of our systems with the previous best performing system, UTDNB. t and t indicate the difference
in the average results is statistically significant with 95% or 99% confidence level, respectively.
results are presented in Table 2. For comparison, we
also show the performance of a random baseline and
the best performing system of each participant in the
SemEval-2012 task.
We draw two conclusions from this table. First,
both of our general relational similarity models, the
directional similarity (DS) and lexical pattern (Pat)
models are fairly strong. The former outperforms
the previous best system UTDNB in both Spear-
man’s p and MaxDiff accuracy, where the differ-
ences are statistically significant8; the latter has
comparable performance, where the differences are
not statistically significant. In contrast, while the
IsA relation from Probase is exceptionally good
in identifying Class-Inclusion relations, with high
Spearman’s p = 0.619 and MaxDiff accuracy
</bodyText>
<footnote confidence="0.498324">
8We conducted a paired-t test on the results of each of the
69 relation. The difference is considered statistically significant
if the p-value is less than 0.05.
</footnote>
<bodyText confidence="0.999573578947368">
59.6%, it does not have high correlations with hu-
man judgments in other relations. Like in the case of
Probase Attribute and PILSA, specific word-relation
models individually are not good measures for gen-
eral relational similarity. Second, as expected, com-
bining multiple diverse models (Com) is a robust
strategy, which provides the best overall perfor-
mance. It achieves superior results in both evalua-
tion metrics compared to UTDNB and only a lower
Spearman’s p value in one of the ten relation groups
(namely, Reference). The differences are statisti-
cally significant with p-value less than 10−3.
In order to understand the interaction among dif-
ferent component models, we conducted an ablation
study by iteratively removing one model from the fi-
nal combination. The weights are re-trained using
the same procedure that finds the best regularization
parameters with the help of training data. Table 3
summarizes the results and compares them with the
</bodyText>
<page confidence="0.93832">
1006
</page>
<table confidence="0.999792846153846">
Spearman’s p MaxDiff Accuracy (%)
Relation Group Com. -Attr -IsA -PILSA -DS -Pat Com. -Attr -IsA -PILSA -DS -Pat
Class-Inclusion 0.519 0.557 0.467 0.593 0.490 0.570 51.2 53.7 49.2 54.6 49.3 56.2
Part-Whole 0.329 0.326 0.335 0.331 0.277 0.285 42.9 42.1 42.6 41.8 38.5 42.9
Similar 0.303 0.269 0.302 0.281 0.256 0.144 43.3 41.2 42.7 40.5 40.2 38.9
Contrast 0.268 0.234 0.267 0.289 0.260 0.156 42.8 42.0 42.4 41.5 42.7 38.1
Attribute 0.406 0.409 0.405 0.433 0.164 0.447 48.3 47.8 48.2 49.1 36.9 49.0
Non-Attribute 0.296 0.287 0.296 0.276 0.123 0.283 42.6 42.9 42.6 41.8 36.0 43.0
Case Relations 0.473 0.497 0.470 0.484 0.309 0.498 50.6 52.5 50.2 50.9 42.9 53.2
Cause-Purpose 0.296 0.282 0.299 0.301 0.205 0.296 41.7 41.6 41.6 41.2 36.6 44.1
Space-Time 0.443 0.425 0.443 0.420 0.269 0.431 47.7 47.2 47.7 46.9 40.5 49.5
Reference 0.208 0.238 0.205 0.168 0.102 0.210 42.5 42.3 42.6 41.8 36.1 41.4
Average 0.353 0.348 0.350 0.354 0.238# 0.329 45.2 45.0 44.9# 44.7 39.6# 45.4
</table>
<tableCaption confidence="0.9894525">
Table 3: Average Spearman’s p and MaxDiff accuracy results of different model combinations. Com indicates combin-
ing all models, where other columns show the results when the specified model is removed. The best result in each row
is highlighted in boldface font. Statistical significance tests are conducted by comparing each ablation configuration
with Com. $ indicates the difference in the average results is statistically significant with 99% confidence level.
</tableCaption>
<bodyText confidence="0.991948485714286">
original combination model.
Overall, it is clear that the directional similarity
method based on RNNLM vectors is the most crit-
ical component model. Removing it from the fi-
nal combination decreases both the Spearman’s p
and MaxDiff accuracy by a large margin; both dif-
ferences (Com vs. -DS) are statistically significant.
The Probase IsA model also has an important im-
pact on the performance on the Class-Inclusion re-
lation group. Eliminating the IsA model makes
the overall MaxDiff accuracy statistically signifi-
cantly lower (Com vs. -IsA). Again, the benefits
of incorporating Probase Attribute and PILSA mod-
els are not clear. Removing them from the final
combination lowers the MaxDiff accuracy, but nei-
ther the difference in Spearman’s p nor MaxDiff
accuracy is statistically significant. Compared to
the RNNLM directional similarity model, the lex-
ical pattern model seems less critical. Removing
it lowers the Similar and Contrast relation groups,
but improves some other relation groups like Class-
Inclusion and Case Relations. The final MaxDiff ac-
curacy becomes slightly higher but the Spearman’s
p drops a little (Com vs. -Pat); neither is statistically
significant.
Notice that the main purpose of the ablation study
is to verify the importance of an individual compo-
nent model when a significant performance drop is
observed after removing it. However, occasionally
the overall performance may go up slightly. Typi-
cally this is due to the fact that some models do not
provide useful signals to a particular relation, but in-
stead introduce more noise. Such effects can often
be alleviated when there are enough quality training
data, which is unfortunately not the case here.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999965652173913">
In this paper, we presented a system that combines
heterogeneous models based on different informa-
tion sources for measuring relational similarity. Our
two individual general-purpose relational similarity
models, directional similarity and lexical pattern
methods, perform strongly when compared to ex-
isting systems. After incorporating specific word-
relation models, the final system sets a new state-of-
the-art on the SemEval-2012 task 2 test set, achiev-
ing Spearman’s p = 0.353 and MaxDiff accuracy
45.4% – resulting in 54.1% and 14.7% relative im-
provement in these two metrics, respectively.
Despite its simplicity, our directional similarity
approach provides a robust model for relational sim-
ilarity and is a critical component in the final sys-
tem. When the lexical pattern model is included, our
overall model combination method can be viewed
as a two-stage learning system. As demonstrated in
our work, with an appropriate regularization strat-
egy, high-quality models can be learned in both
stages. Finally, as we observe from the positive ef-
fect of adding the Probase IsA model, specific word-
relation models can further help improve the system
</bodyText>
<page confidence="0.98182">
1007
</page>
<bodyText confidence="0.999974703703704">
although they tend to cover only a small number of
relations. Incorporating more such models could be
a steady path to enhance the final system.
In the future, we plan to pursue several research
directions. First, as shown in our experimental re-
sults, the model combination approach does not al-
ways outperform individual models. Investigating
how to select models to combine for each specific re-
lation or relation group individually will be our next
step for improving this work. Second, because the
labeling process of relational similarity comparisons
is inherently noisy, it is unrealistic to request a sys-
tem to correlate human judgments perfectly. Con-
ducting some user study to estimate the performance
ceiling in each relation category may help us focus
on the weaknesses of the final system to enhance
it. Third, it is intriguing to see that the directional
similarity model based on the RNNLM vectors per-
forms strongly, even though the RNNLM training
process is not related to the task of relational sim-
ilarity. Investigating the effects of different vector
space models and proposing some theoretical jus-
tifications are certainly interesting research topics.
Finally, we would like to evaluate the utility our ap-
proach in other applications, such as the SAT anal-
ogy problems proposed by Turney (2006) and ques-
tion answering.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999372666666667">
We thank Richard Socher for valuable discussions,
Misha Bilenko for his technical advice and anony-
mous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.99859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999690537313433">
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas¸ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ’09, pages 19–27.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML ’07.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cog-
nitive and psychometric analysis of analogical prob-
lem solving. Recent research in psychology. Springer-
Verlag.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137–1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467–479.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32:13–47, March.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Timothy W. Finin. 1980. The Semantic Interpretation
of Compound Nominals. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
J.H. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist, 29(5):1189–
1232.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
degrees of relational similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 356–364, Montr´eal, Canada,
7-8 June. Association for Computational Linguistics.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259–284.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cer-
nock´y, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In INTER-
SPEECH, pages 1045–1048.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for train-
ing large scale neural network language models. In
ASRU.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013.
Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth edi-
tion. Technical report, Linguistic Data Consortium,
Philadelphia.
Ted Pedersen. 2012. Duluth: Measuring degrees of re-
lational similarity with the gloss vector measure of se-
mantic relatedness. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
</reference>
<page confidence="0.750302">
1008
</page>
<reference confidence="0.999885423728813">
2012), pages 497–501, Montr´eal, Canada, 7-8 June.
Association for Computational Linguistics.
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis. In
WWW ’11, pages 337–346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In NAACL ’10.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413–418,
Montr´eal, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP-01, pages 82–90.
Mireya Tovar, J. Alejandro Reyes, Azucena Montes,
Darnes Vilari˜no, David Pinto, and Saul Le´on. 2012.
BUAP: A first approximation to relational similarity
measuring. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 502–505, Montr´eal, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings ofAssociation for
Computational Linguistics (ACL 2010).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251–278.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Research
(JAIR), 44:533–585.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
COLING-94, pages 782–788.
Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q.
Zhu. 2012. Probase: a probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of
Data, pages 481–492, May.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616–
620, Montr´eal, Canada, June.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212–1222, Jeju Island,
Korea, July.
</reference>
<page confidence="0.99487">
1009
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196741">
<title confidence="0.700881">Combining Heterogeneous Models for Measuring Relational Similarity Instituto Politecnico</title>
<author confidence="0.770072">Mexico City</author>
<email confidence="0.999361">alisa.zhila@gmail.com</email>
<author confidence="0.969563">Geoffrey</author>
<affiliation confidence="0.975567">Microsoft</affiliation>
<address confidence="0.998567">Redmond, WA 98052,</address>
<email confidence="0.999705">gzweig@microsoft.com</email>
<author confidence="0.968802">Wen-tau Yih Christopher</author>
<affiliation confidence="0.979707">Microsoft</affiliation>
<address confidence="0.996806">Redmond, WA 98052,</address>
<affiliation confidence="0.8012455">BRNO University of BRNO, Czech</affiliation>
<email confidence="0.999319">tmikolov@gmail.com</email>
<abstract confidence="0.9930356">In this work, we study the problem of measuring relational similarity between two word (e.g., Due to the large number of possible relations, we argue that it is important to combine multiple models based on heterogeneous information sources. Our overall system consists of two novel general-purpose relational similarity models and three specific word relation models. When evaluated in the setting of a recently proposed SemEval-2012 task, our approach outperforms the previous best system substantially, achieving a 54.1% relative increase in Spearman’s rank correlation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pas¸ca</author>
<author>A Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL ’09,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas¸ca and A. Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In NAACL ’09, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In ICML ’07.</booktitle>
<contexts>
<context position="20402" citStr="Andrew and Gao, 2007" startWordPosition="3232" endWordPosition="3235">e was trained using 3,218 examples. The degree of relational similarity of each word pair can then be judged by the output of the corresponding classifier5. Although this seems like a standard supervised learning setting, the large number of features poses a challenge here. Using almost 1M features and 3,218 examples, the model could easily overfit if not regularized properly, which may explain why learning SVMs on pattern features performed poorly (Rink and Harabagiu, 2012). Instead of employing explicit feature selection methods, we used an efficient L1 regularized log-linear model learner (Andrew and Gao, 2007) and chose the hyper-parameters based on model performance on the training data. The final models we chose were trained with L1 = 3, where 28,065 features in average were selected automatically by the algo3We used a Nov-2010 dump of English Wikipedia, which contains approximately 917M words after pre-processing. The LA Times corpus consists of articles from 1985 to 2002 and has about 1.1B words. 4Given that not all word pairs belonging to the same relation category are equally good, removing those with low judgment scores may help improve the quality of the labeled data. We leave this study to</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of L1-regularized log-linear models. In ICML ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I I Bejar</author>
<author>R Chaffin</author>
<author>S E Embretson</author>
</authors>
<title>Cognitive and psychometric analysis of analogical problem solving. Recent research in psychology.</title>
<date>1991</date>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="10855" citStr="Bejar et al., 1991" startWordPosition="1664" endWordPosition="1667"> relation class. For instance, comparing to the prototypical word pairs, {cutlery:spoon, clothing:shirt, vermin:rat} of the Class-Inclusion:Singular Collective relation, we would like to know among the input word pairs {dish:bowl, book:novel, furniture:desk}, which one 1http://www.mturk.com best demonstrates the relation. Because our approaches are evaluated using the data provided in this SemEval-2012 task, we describe briefly below how the data was collected, as well as the metrics used to evaluate system performance. The dataset consists of 79 relation classes that are chosen according to (Bejar et al., 1991) and broadly fall into 10 main categories, including Class-Inclusion, Part-Whole, Similar and more. With the help of Amazon Mechanical Turk, Jurgens et al. (2012) used a two-phase approach to collect word pairs and their degrees. In the first phase, a lexical schema, such as “a Y is one item in a collection/group of X” for the aforementioned relation Class-Inclusion:Singular Collective, and a few prototypical pairs for each class were given to the workers, who were asked to provide approximately a list of 40 word pairs representing the same relation class. Naturally, some of these pairs were b</context>
</contexts>
<marker>Bejar, Chaffin, Embretson, 1991</marker>
<rawString>I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cognitive and psychometric analysis of analogical problem solving. Recent research in psychology. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="14197" citStr="Bengio et al., 2003" startWordPosition="2205" endWordPosition="2208">lly, the word relation models incorporate existing, specific word relation measures for general relational similarity. 4.1 Directional Similarity Model Our first model for relational similarity extends previous work on semantic word vector representations to a directional similarity model for pairs of words. There are many different methods for creating real-valued semantic word vectors, such as the distributed representation derived from a word co-occurrence matrix and a low-rank approximation (Landauer et al., 1998), word clustering (Brown et al., 1992) and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear (Turney, 2012). In a companion paper (Mikolov et al., 2013), we present a vector offset method </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="14139" citStr="Brown et al., 1992" startWordPosition="2196" endWordPosition="2199">ra, and learns a highly regularized log-linear model. Finally, the word relation models incorporate existing, specific word relation measures for general relational similarity. 4.1 Directional Similarity Model Our first model for relational similarity extends previous work on semantic word vector representations to a directional similarity model for pairs of words. There are many different methods for creating real-valued semantic word vectors, such as the distributed representation derived from a word co-occurrence matrix and a low-rank approximation (Landauer et al., 1998), word clustering (Brown et al., 1992) and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear (Turney, 2012). In a companion paper </context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<pages>32--13</pages>
<contexts>
<context position="1534" citStr="Budanitsky and Hirst, 2006" startWordPosition="205" endWordPosition="209">, our approach outperforms the previous best system substantially, achieving a 54.1% relative increase in Spearman’s rank correlation. 1 Introduction The problem of measuring relational similarity is to determine the degree of correspondence between two word pairs. For instance, the analogous word pairs silverware:fork and clothing:shirt both exemplify well a Class-Inclusion:Singular Collective relation and thus have high relational similarity. Unlike the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, *Work conducted while interning at Microsoft Research. antonyms or associations, can be reduced to measuring relational similarity compared to prototypical word pairs with the desired relation (Turney, 2008). In scenarios like information extraction or question answering, </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNetbased measures of lexical semantic relatedness. Computational Linguistics, 32:13–47, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI</booktitle>
<contexts>
<context position="22255" citStr="Carlson et al., 2010" startWordPosition="3532" endWordPosition="3535">tegories. In contrast, for specific word relations, there exist several high-quality methods. Although they are designed for detecting specific relations between words, incorporating them could still improve the overall results. Next, we explore the use of some of these word relation models, including information encoded in the knowledge base and a lexical semantic model for synonymy and antonymy. 4.3.1 Knowledge Bases Predetermined types of relations can often be found in existing lexical and knowledge databases, such as WordNet’s Is-A taxonomy and the extensive relations stored in the NELL (Carlson et al., 2010) knowledge base. Although in theory, these resources can be directly used to solve the problem of relational similarity, such direct approaches often suffer from two practical issues. First, the word coverage of these databases is usually very limited and it is common that the relation of a given word pair is absent. Second, the degree of relation is often not included, which makes the task of measuring the degree of relational similarity difficult. One counter example, however, is Probase (Wu et al., 2012), which is a knowledge base that establishes connections between more than 2.5 million c</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy W Finin</author>
</authors>
<title>The Semantic Interpretation of Compound Nominals.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="5650" citStr="Finin, 1980" startWordPosition="851" endWordPosition="852">urther improved. The rest of this paper is organized as follows. We first survey the related work in Sec. 2 and formally define the problem in Sec. 3. We describe the individual models in detail in Sec. 4. The combination approach is depicted in Sec. 5, along with experimental comparisons to individual models and existing systems. Finally, Sec. 6 concludes the paper. 2 Related Work Building a classifier to determine whether a relationship holds between a pair of words is a natural approach to the task of measuring relational similarity. While early work was mostly based on hand-crafted rules (Finin, 1980; Vanderwende, 1994), Rosario and Hearst (2001) introduced a machine learning approach to classify word pairs. They targeted classifying noun modifier pairs from the medical domain into 13 classes of semantic relations. Features for each noun modifier pair were constructed using large medical lexical resources and a multiclass classifier was trained using a feed-forward neural network with one hidden layer. This work was later extended by Nastase and Szpakowicz (2003) to classify general domain noun-modifier pairs into 30 semantic relations. In addition to extracting features using WordNet and</context>
</contexts>
<marker>Finin, 1980</marker>
<rawString>Timothy W. Finin. 1980. The Semantic Interpretation of Compound Nominals. Ph.D. thesis, University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Friedman</author>
</authors>
<title>Greedy function approximation: a gradient boosting machine.</title>
<date>2001</date>
<journal>Ann. Statist,</journal>
<volume>29</volume>
<issue>5</issue>
<pages>1232</pages>
<contexts>
<context position="25537" citStr="Friedman, 2001" startWordPosition="4070" endWordPosition="4071">. The training setting is essentially the same as the one used to learn the lexical pattern model (Sec. 4.2). For each relation, we treat all the word pairs in this relation group as positive examples and all other word pairs as negative ones. Consequently, 79 sets of weights for model combination are learned in total. The average Spearman’s p of the 10 training relations is used for selecting the values of the L1 and L2 regularizers7. Evaluated on the remaining 69 relations (i.e., the test set), the average results of each main relation group and the overall 6Nonlinear methods, such as MART (Friedman, 2001), do not perform better in our experiments (not reported here). 7We tested 15 combinations, where L1 E {0, 0.01, 0.11 and L2 E {0, 0.001, 0.01, 1, 101. The parameter setting that gave the highest Spearman rank correlation coefficient score on the training set was selected. 1005 Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com. Class-Inclusion 0.057 0.064 0.045 0.233 0.350 0.422 0.619 -0.137 0.029 0.519 Part-Whole 0.012 0.066 -0.061 0.252 0.317 0.244 -0.014 0.026 -0.010 0.329 Similar 0.026 -0.036 0.183 0.214 0.254 0.245 -0.020 0.133 0.058 0.303 Contrast -0.049 0.000 0.142 0</context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>J.H. Friedman. 2001. Greedy function approximation: a gradient boosting machine. Ann. Statist, 29(5):1189– 1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Saif Mohammad</author>
<author>Peter Turney</author>
<author>Keith Holyoak</author>
</authors>
<title>SemEval-2012 Task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>356--364</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="2416" citStr="Jurgens et al., 2012" startWordPosition="334" endWordPosition="337">f identifying specific relations between words, such as synonyms, *Work conducted while interning at Microsoft Research. antonyms or associations, can be reduced to measuring relational similarity compared to prototypical word pairs with the desired relation (Turney, 2008). In scenarios like information extraction or question answering, where identifying the existence of certain relations is often the core problem, measuring relational similarity provides a more flexible solution rather than creating relational classifiers for predefined or task-specific categories of relations (Turney, 2006; Jurgens et al., 2012). In order to promote this research direction, Jurgens et al. (2012) proposed a new shared task of measuring relational similarity in SemEval-2012 recently. In this task, each submitted system is required to judge the degree of a target word pair having a particular relation, measured by its relational similarity compared to a few prototypical example word pairs. The system performance is evaluated by its correlation with the human judgments using two evaluation metrics, Spearman’s rank correlation and MaxDiff accuracy (more details of the task and evaluation metrics will be given in Sec. 3). </context>
<context position="8202" citStr="Jurgens et al. (2012)" startWordPosition="1244" endWordPosition="1247"> relational similarity measure. Existing classifiers for specific word relations (e.g., synonyms or Is-A) are combined with general relational similarity measures. Empirically, mixing heterogeneous models tends to make the final relational similarity measure more robust. Although datasets for semantic relation classification or SAT analogous questions can be used to evaluate a relational similarity model, their labels are either binary or categorical, which makes the datasets suboptimal for determining the quality of a model when evaluated on instances of the same relation class. As a result, Jurgens et al. (2012) proposed a new task of “Measuring Degrees of Relational Similarity” at SemEval-2012, which includes 79 relation 1001 categories exemplified by three or four prototypical word pairs and a schematic description. For example, for the Class-Inclusion:Taxonomic relation, the schematic description is “Y is a kind/type/instance of X”. Using Amazon Mechanical Turk1, they collected word pairs for each relation, as well as their degrees of being a good representative of a particular relation when compared with defining examples. Participants of this shared task proposed various kinds of approaches that</context>
<context position="10088" citStr="Jurgens et al., 2012" startWordPosition="1548" endWordPosition="1552">he performance were not much better than a random baseline, which indicates the difficulty of this task. In comparison, a supervised learning approach seems more promising. The UTD system (Rink and Harabagiu, 2012), which mined lexical patterns between co-occurring words in the corpus and then used them as features to train a Naive Bayes classifier, achieved the best results. However, potentially due to the large feature space, this strategy did not work as well when switching the learning algorithm to SVMs. 3 Problem Definition &amp; Task Description Following the setting of SemEval-2012 Task 2 (Jurgens et al., 2012), the problem of measuring the degree of relational similarity is to rate word pairs by the degree to which they are prototypical members of a given relation class. For instance, comparing to the prototypical word pairs, {cutlery:spoon, clothing:shirt, vermin:rat} of the Class-Inclusion:Singular Collective relation, we would like to know among the input word pairs {dish:bowl, book:novel, furniture:desk}, which one 1http://www.mturk.com best demonstrates the relation. Because our approaches are evaluated using the data provided in this SemEval-2012 task, we describe briefly below how the data w</context>
<context position="13106" citStr="Jurgens et al., 2012" startWordPosition="2041" endWordPosition="2045">ity of a relational similarity measure is estimated by its correlation to human judgments. This is evaluated using two metrics in the task: the MaxDiff accuracy and Spearman’s rank correlation coefficient (p). A system is first asked to pick the most and least representative word pairs of each question in the MaxDiff setting. The average accuracy of the predictions compared to the human answers is then reported. In contrast, Spearman’s p 1002 measures the correlation between the total orderings of all word pairs of a relation, where the total ordering is derived from the MaxDiff answers (see (Jurgens et al., 2012) for the exact procedure). 4 Models for Relational Similarity We investigate three types of models for relational similarity. Operating in a word vector space, the directional similarity model compares the vector differences of target and prototypical word pairs to estimate their relational similarity. The lexical pattern method collects contextual information of pairs of words when they co-occur in large corpora, and learns a highly regularized log-linear model. Finally, the word relation models incorporate existing, specific word relation measures for general relational similarity. 4.1 Direc</context>
</contexts>
<marker>Jurgens, Mohammad, Turney, Holyoak, 2012</marker>
<rawString>David Jurgens, Saif Mohammad, Peter Turney, and Keith Holyoak. 2012. SemEval-2012 Task 2: Measuring degrees of relational similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 356–364, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<contexts>
<context position="14101" citStr="Landauer et al., 1998" startWordPosition="2190" endWordPosition="2193">f words when they co-occur in large corpora, and learns a highly regularized log-linear model. Finally, the word relation models incorporate existing, specific word relation measures for general relational similarity. 4.1 Directional Similarity Model Our first model for relational similarity extends previous work on semantic word vector representations to a directional similarity model for pairs of words. There are many different methods for creating real-valued semantic word vectors, such as the distributed representation derived from a word co-occurrence matrix and a low-rank approximation (Landauer et al., 1998), word clustering (Brown et al., 1992) and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25, pages 259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan J Louviere</author>
<author>G G Woodworth</author>
</authors>
<title>Bestworst scaling: A model for the largest difference judgments.</title>
<date>1991</date>
<tech>Technical report,</tech>
<institution>University of Alberta.</institution>
<contexts>
<context position="11677" citStr="Louviere and Woodworth, 1991" startWordPosition="1800" endWordPosition="1803">ollect word pairs and their degrees. In the first phase, a lexical schema, such as “a Y is one item in a collection/group of X” for the aforementioned relation Class-Inclusion:Singular Collective, and a few prototypical pairs for each class were given to the workers, who were asked to provide approximately a list of 40 word pairs representing the same relation class. Naturally, some of these pairs were better examples than the others. Therefore, in the second phase, the goal was to measure the degree of their similarity to the corresponding relation. This was done using the MaxDiff technique (Louviere and Woodworth, 1991). For each relation, about one hundred questions were first created. Each question consists of four different word pairs randomly sampled from the list. The worker was then asked to choose the most and least representative word pairs for the specific relation in each question. The set of 79 word relations were randomly split into training and testing sets. The former contains 10 relations and the latter has 69. Word pairs in all 79 relations were given to the task participants in advance, but only the human judgments of the training set were available for system development. In this work, we t</context>
</contexts>
<marker>Louviere, Woodworth, 1991</marker>
<rawString>Jordan J. Louviere and G. G. Woodworth. 1991. Bestworst scaling: A model for the largest difference judgments. Technical report, University of Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Anoop Deoras</author>
<author>Daniel Povey</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
</authors>
<title>Strategies for training large scale neural network language models.</title>
<date>2011</date>
<booktitle>In ASRU.</booktitle>
<contexts>
<context position="17473" citStr="Mikolov et al., 2011" startWordPosition="2751" endWordPosition="2754">axDiff Acc. (%) LSA-80 0.055 34.6 LSA-320 0.066 34.4 LSA-640 0.102 35.7 RNNLM-80 0.168 37.5 RNNLM-320 0.214 39.1 RNNLM-640 0.221 39.2 RNNLM-1600 0.234 41.2 Table 1: Results of measuring relational similarity using the directional similarity method, evaluated on the training set. The 1600-dimensional RNNLM vector space achieves the highest Spearman’s p and MaxDiff accuracy. ferent dimensionality settings: the word embedding learned from the recurrent neural network language model (RNNLM)2 and the LSA vectors, both were trained using the same Broadcast News corpus of 320M words as described in (Mikolov et al., 2011). All the word vectors were first normalized to unit vectors before applying the directional similarity method. Given a target word pair, we computed its relational similarity compared with the prototypical word pairs of the same relation. The average of these measurements was taken as the final model score. Table 1 summarizes the results when evaluated on the training set. As shown in the table, the RNNLM vectors consistently outperform their LSA counterparts with the same dimensionality. In addition, more dimensions seem to preserve more information and lead to better performance. Therefore,</context>
</contexts>
<marker>Mikolov, Deoras, Povey, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget, and Jan Cernocky. 2011. Strategies for training large scale neural network language models. In ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="14761" citStr="Mikolov et al., 2013" startWordPosition="2300" endWordPosition="2303">and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear (Turney, 2012). In a companion paper (Mikolov et al., 2013), we present a vector offset method which performs consistently well in identifying both syntactic and semantic regularities. This method measures the degree of the analogy “a is to b as c is to d” using the cosine score of (~vb − ~va + ~v�,~vd), where a, b, c, d are the four given words and ~va,~vb,~v,~vd are the corresponding vecFigure 1: Directional vectors υ1 and υ2 capture the relations of clothing:shirt and furniture:desk respectively in this semantic vector space. The relational similarity of these two word pairs is estimated by the cosine of θ. tors. In this paper, we propose a variant</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Exploring noun-modifier semantic relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the 5th International Workshop on Computational Semantics.</booktitle>
<contexts>
<context position="6122" citStr="Nastase and Szpakowicz (2003)" startWordPosition="924" endWordPosition="927">en a pair of words is a natural approach to the task of measuring relational similarity. While early work was mostly based on hand-crafted rules (Finin, 1980; Vanderwende, 1994), Rosario and Hearst (2001) introduced a machine learning approach to classify word pairs. They targeted classifying noun modifier pairs from the medical domain into 13 classes of semantic relations. Features for each noun modifier pair were constructed using large medical lexical resources and a multiclass classifier was trained using a feed-forward neural network with one hidden layer. This work was later extended by Nastase and Szpakowicz (2003) to classify general domain noun-modifier pairs into 30 semantic relations. In addition to extracting features using WordNet and Roget’s Thesaurus, they also experimented with several different learners including decision trees, memory-based learning and inductive logic programming methods like RIPPER and FOIL. Using the same dataset as in (Nastase and Szpakowicz, 2003), Turney and Littman (2005) created a 128-dimentional feature vector for each word pair based on statistics of their co-occurrence patterns in Web documents and applied the k-NN method (k = 1 in their work). Measuring relational</context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Vivi Nastase and Stan Szpakowicz. 2003. Exploring noun-modifier semantic relations. In Proceedings of the 5th International Workshop on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword fourth edition.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Linguistic Data Consortium,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="18973" citStr="Parker et al., 2009" startWordPosition="2988" endWordPosition="2991">vide useful cues for identifying the word relation. For example, having observed frequent text fragments like “X such as Y”, it is likely that there is a Class-Inclusion:Taxonomic relation between X and Y; namely, Y is a type of X. Indeed, by mining lexical patterns from a large corpus, the UTD system (Rink and Harabagiu, 2012) managed to outperform other participants in the SemEval-2012 task of measuring relational similarity. 2http://www.fit.vutbr.cz/˜imikolov/rnnlm In order to find more co-occurrences of each pair of words, we used a large document set that consists of the Gigaword corpus (Parker et al., 2009), Wikipedia and LA Times articles3, summing up to more than 20 Gigabytes of texts. For each word pair (w1, w2) that co-occur in a sentence, we collected the words in between as its context (or socalled “raw pattern”). For instance, “such as” would be the context extracted from “X such as Y” for the word pair (X, Y). To reduce noise, contexts with more than 9 words were dropped and 914,295 patterns were collected in total. Treating each raw pattern as a feature where the value is the logarithm of the occurrence count, we then built a probabilistic classifier to determine the association of the </context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2009</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2009. English Gigaword fourth edition. Technical report, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Duluth: Measuring degrees of relational similarity with the gloss vector measure of semantic relatedness.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>497--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="8905" citStr="Pedersen, 2012" startWordPosition="1353" endWordPosition="1354">ich includes 79 relation 1001 categories exemplified by three or four prototypical word pairs and a schematic description. For example, for the Class-Inclusion:Taxonomic relation, the schematic description is “Y is a kind/type/instance of X”. Using Amazon Mechanical Turk1, they collected word pairs for each relation, as well as their degrees of being a good representative of a particular relation when compared with defining examples. Participants of this shared task proposed various kinds of approaches that leverage both lexical resources and general corpora. For instance, the Duluth systems (Pedersen, 2012) created word vectors based on WordNet and estimated the degree of a relation using cosine similarity. The BUAP system (Tovar et al., 2012) represented each word pair as a whole by a vector of 4 different types of features: context, WordNet, POS tags and the average number of words separating the two words in text. The degree of relation was then determined by the cosine distance of the target pair from the prototypical examples of each relation. Although their models incorporated a significant amount of information of words or word pairs, unfortunately, the performance were not much better th</context>
</contexts>
<marker>Pedersen, 2012</marker>
<rawString>Ted Pedersen. 2012. Duluth: Measuring degrees of relational similarity with the gloss vector measure of semantic relatedness. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 497–501, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Radinsky</author>
<author>E Agichtein</author>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>A word at a time: computing word relatedness using temporal semantic analysis.</title>
<date>2011</date>
<booktitle>In WWW ’11,</booktitle>
<pages>337--346</pages>
<contexts>
<context position="1585" citStr="Radinsky et al., 2011" startWordPosition="214" endWordPosition="217">tantially, achieving a 54.1% relative increase in Spearman’s rank correlation. 1 Introduction The problem of measuring relational similarity is to determine the degree of correspondence between two word pairs. For instance, the analogous word pairs silverware:fork and clothing:shirt both exemplify well a Class-Inclusion:Singular Collective relation and thus have high relational similarity. Unlike the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, *Work conducted while interning at Microsoft Research. antonyms or associations, can be reduced to measuring relational similarity compared to prototypical word pairs with the desired relation (Turney, 2008). In scenarios like information extraction or question answering, where identifying the existence of certain relation</context>
</contexts>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>K. Radinsky, E. Agichtein, E. Gabrilovich, and S. Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In WWW ’11, pages 337–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>R Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In NAACL ’10.</booktitle>
<contexts>
<context position="1562" citStr="Reisinger and Mooney, 2010" startWordPosition="210" endWordPosition="213">he previous best system substantially, achieving a 54.1% relative increase in Spearman’s rank correlation. 1 Introduction The problem of measuring relational similarity is to determine the degree of correspondence between two word pairs. For instance, the analogous word pairs silverware:fork and clothing:shirt both exemplify well a Class-Inclusion:Singular Collective relation and thus have high relational similarity. Unlike the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, *Work conducted while interning at Microsoft Research. antonyms or associations, can be reduced to measuring relational similarity compared to prototypical word pairs with the desired relation (Turney, 2008). In scenarios like information extraction or question answering, where identifying the existe</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>J. Reisinger and R. Mooney. 2010. Multi-prototype vector-space models of word meaning. In NAACL ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Rink</author>
<author>Sanda Harabagiu</author>
</authors>
<title>UTD: Determining relational similarity using lexical patterns.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>413--418</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="3220" citStr="Rink and Harabagiu, 2012" startWordPosition="462" endWordPosition="465">mitted system is required to judge the degree of a target word pair having a particular relation, measured by its relational similarity compared to a few prototypical example word pairs. The system performance is evaluated by its correlation with the human judgments using two evaluation metrics, Spearman’s rank correlation and MaxDiff accuracy (more details of the task and evaluation metrics will be given in Sec. 3). Although participating systems incorporated substantial amounts of information from lexical resources (e.g., WordNet) and contextual patterns from large corpora, only one system (Rink and Harabagiu, 2012) is able to outperform a simple baseline that uses PMI (pointwise mutual information) scoring, which demonstrates the difficulty of this task. In this paper, we explore the problem of measuring relational similarity in the same task setting. We argue that due to the large number of possible relations, building an ensemble of relational simi1000 Proceedings of NAACL-HLT 2013, pages 1000–1009, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics larity models based on heterogeneous information sources is the key to advance the state-of-the-art on this problem. By co</context>
<context position="4692" citStr="Rink and Harabagiu, 2012" startWordPosition="689" endWordPosition="692">’s rank correlation and 14.7% in the MaxDiff accuracy! Our main contributions are threefold. First, we propose a novel directional similarity method based on the vector representation of words learned from a recurrent neural network language model. The relation of two words is captured by their vector offset in the latent semantic space. Similarity of relations can then be naturally measured by a distance function in the vector space. This method alone already performs better than all existing systems. Second, unlike the previous finding, where SVMs learn a much poorer model than naive Bayes (Rink and Harabagiu, 2012), we show that using a highlyregularized log-linear model on simple contextual pattern features collected from a document collection of 20GB, a discriminative approach can learn a strong model as well. Third, we demonstrate that by augmenting existing word-relation models, which cover only a small number of relations, the overall system can be further improved. The rest of this paper is organized as follows. We first survey the related work in Sec. 2 and formally define the problem in Sec. 3. We describe the individual models in detail in Sec. 4. The combination approach is depicted in Sec. 5,</context>
<context position="9681" citStr="Rink and Harabagiu, 2012" startWordPosition="1481" endWordPosition="1484"> each word pair as a whole by a vector of 4 different types of features: context, WordNet, POS tags and the average number of words separating the two words in text. The degree of relation was then determined by the cosine distance of the target pair from the prototypical examples of each relation. Although their models incorporated a significant amount of information of words or word pairs, unfortunately, the performance were not much better than a random baseline, which indicates the difficulty of this task. In comparison, a supervised learning approach seems more promising. The UTD system (Rink and Harabagiu, 2012), which mined lexical patterns between co-occurring words in the corpus and then used them as features to train a Naive Bayes classifier, achieved the best results. However, potentially due to the large feature space, this strategy did not work as well when switching the learning algorithm to SVMs. 3 Problem Definition &amp; Task Description Following the setting of SemEval-2012 Task 2 (Jurgens et al., 2012), the problem of measuring the degree of relational similarity is to rate word pairs by the degree to which they are prototypical members of a given relation class. For instance, comparing to t</context>
<context position="18682" citStr="Rink and Harabagiu, 2012" startWordPosition="2946" endWordPosition="2949">ance. Therefore, we take the 1600-dimensional RNNLM vectors to construct our final directional similarity model. 4.2 Lexical Pattern Model Our second model for measuring relational similarity is built based on lexical patterns. It is well-known that contexts in which two words co-occur often provide useful cues for identifying the word relation. For example, having observed frequent text fragments like “X such as Y”, it is likely that there is a Class-Inclusion:Taxonomic relation between X and Y; namely, Y is a type of X. Indeed, by mining lexical patterns from a large corpus, the UTD system (Rink and Harabagiu, 2012) managed to outperform other participants in the SemEval-2012 task of measuring relational similarity. 2http://www.fit.vutbr.cz/˜imikolov/rnnlm In order to find more co-occurrences of each pair of words, we used a large document set that consists of the Gigaword corpus (Parker et al., 2009), Wikipedia and LA Times articles3, summing up to more than 20 Gigabytes of texts. For each word pair (w1, w2) that co-occur in a sentence, we collected the words in between as its context (or socalled “raw pattern”). For instance, “such as” would be the context extracted from “X such as Y” for the word pair</context>
<context position="20260" citStr="Rink and Harabagiu, 2012" startWordPosition="3210" endWordPosition="3213">rd pairs as positive examples and all the word pairs in other relations as negative examples4. 79 classifiers were trained in total, where each one was trained using 3,218 examples. The degree of relational similarity of each word pair can then be judged by the output of the corresponding classifier5. Although this seems like a standard supervised learning setting, the large number of features poses a challenge here. Using almost 1M features and 3,218 examples, the model could easily overfit if not regularized properly, which may explain why learning SVMs on pattern features performed poorly (Rink and Harabagiu, 2012). Instead of employing explicit feature selection methods, we used an efficient L1 regularized log-linear model learner (Andrew and Gao, 2007) and chose the hyper-parameters based on model performance on the training data. The final models we chose were trained with L1 = 3, where 28,065 features in average were selected automatically by the algo3We used a Nov-2010 dump of English Wikipedia, which contains approximately 917M words after pre-processing. The LA Times corpus consists of articles from 1985 to 2002 and has about 1.1B words. 4Given that not all word pairs belonging to the same relati</context>
</contexts>
<marker>Rink, Harabagiu, 2012</marker>
<rawString>Bryan Rink and Sanda Harabagiu. 2012. UTD: Determining relational similarity using lexical patterns. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 413–418, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-01,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="5697" citStr="Rosario and Hearst (2001)" startWordPosition="855" endWordPosition="858"> paper is organized as follows. We first survey the related work in Sec. 2 and formally define the problem in Sec. 3. We describe the individual models in detail in Sec. 4. The combination approach is depicted in Sec. 5, along with experimental comparisons to individual models and existing systems. Finally, Sec. 6 concludes the paper. 2 Related Work Building a classifier to determine whether a relationship holds between a pair of words is a natural approach to the task of measuring relational similarity. While early work was mostly based on hand-crafted rules (Finin, 1980; Vanderwende, 1994), Rosario and Hearst (2001) introduced a machine learning approach to classify word pairs. They targeted classifying noun modifier pairs from the medical domain into 13 classes of semantic relations. Features for each noun modifier pair were constructed using large medical lexical resources and a multiclass classifier was trained using a feed-forward neural network with one hidden layer. This work was later extended by Nastase and Szpakowicz (2003) to classify general domain noun-modifier pairs into 30 semantic relations. In addition to extracting features using WordNet and Roget’s Thesaurus, they also experimented with</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Barbara Rosario and Marti Hearst. 2001. Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-01, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mireya Tovar</author>
<author>J Alejandro Reyes</author>
<author>Azucena Montes</author>
<author>Darnes Vilari˜no</author>
<author>David Pinto</author>
<author>Saul Le´on</author>
</authors>
<title>BUAP: A first approximation to relational similarity measuring.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>502--505</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<marker>Tovar, Reyes, Montes, Vilari˜no, Pinto, Le´on, 2012</marker>
<rawString>Mireya Tovar, J. Alejandro Reyes, Azucena Montes, Darnes Vilari˜no, David Pinto, and Saul Le´on. 2012. BUAP: A first approximation to relational similarity measuring. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 502–505, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings ofAssociation for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="14609" citStr="Turian et al., 2010" startWordPosition="2272" endWordPosition="2275">ted representation derived from a word co-occurrence matrix and a low-rank approximation (Landauer et al., 1998), word clustering (Brown et al., 1992) and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear (Turney, 2012). In a companion paper (Mikolov et al., 2013), we present a vector offset method which performs consistently well in identifying both syntactic and semantic regularities. This method measures the degree of the analogy “a is to b as c is to d” using the cosine score of (~vb − ~va + ~v�,~vd), where a, b, c, d are the four given words and ~va,~vb,~v,~vd are the corresponding vecFigure 1: Directional vectors υ1 and υ2 capture the relations of clothing:shirt and furniture:desk respectively in </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semisupervised learning. In Proceedings ofAssociation for Computational Linguistics (ACL 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Corpus-based learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<pages>1--3</pages>
<contexts>
<context position="6521" citStr="Turney and Littman (2005)" startWordPosition="981" endWordPosition="984">ifier pair were constructed using large medical lexical resources and a multiclass classifier was trained using a feed-forward neural network with one hidden layer. This work was later extended by Nastase and Szpakowicz (2003) to classify general domain noun-modifier pairs into 30 semantic relations. In addition to extracting features using WordNet and Roget’s Thesaurus, they also experimented with several different learners including decision trees, memory-based learning and inductive logic programming methods like RIPPER and FOIL. Using the same dataset as in (Nastase and Szpakowicz, 2003), Turney and Littman (2005) created a 128-dimentional feature vector for each word pair based on statistics of their co-occurrence patterns in Web documents and applied the k-NN method (k = 1 in their work). Measuring relational similarity, which determines whether two word pairs share the same relation, can be viewed as an extension of classifying relations between two words. Treating a relational similarity measure as a distance metric, a testing pair of words can be judged by whether they have a relation that is similar to some prototypical word pairs having a particular relation. A multi-relation classifier can thus</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter Turney and Michael Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning, 60 (1-3), pages 251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1731" citStr="Turney (2006)" startWordPosition="238" endWordPosition="239">ne the degree of correspondence between two word pairs. For instance, the analogous word pairs silverware:fork and clothing:shirt both exemplify well a Class-Inclusion:Singular Collective relation and thus have high relational similarity. Unlike the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, *Work conducted while interning at Microsoft Research. antonyms or associations, can be reduced to measuring relational similarity compared to prototypical word pairs with the desired relation (Turney, 2008). In scenarios like information extraction or question answering, where identifying the existence of certain relations is often the core problem, measuring relational similarity provides a more flexible solution rather than creating relational classifiers for pre</context>
<context position="7400" citStr="Turney (2006)" startWordPosition="1125" endWordPosition="1126">me relation, can be viewed as an extension of classifying relations between two words. Treating a relational similarity measure as a distance metric, a testing pair of words can be judged by whether they have a relation that is similar to some prototypical word pairs having a particular relation. A multi-relation classifier can thus be built easily in this framework as demonstrated in (Turney, 2008), where the problems of identifying synonyms, antonyms and associated words are all reduced to finding good analogous word pairs. Measuring relational similarity has been advocated and pioneered by Turney (2006), who proposed a latent vector space model for answering SAT analogy questions (e.g., mason:stone vs. carpenter:wood). In contrast, we take a slightly different view when building a relational similarity measure. Existing classifiers for specific word relations (e.g., synonyms or Is-A) are combined with general relational similarity measures. Empirically, mixing heterogeneous models tends to make the final relational similarity measure more robust. Although datasets for semantic relation classification or SAT analogous questions can be used to evaluate a relational similarity model, their labe</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>P. D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="2068" citStr="Turney, 2008" startWordPosition="286" endWordPosition="287">ributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, *Work conducted while interning at Microsoft Research. antonyms or associations, can be reduced to measuring relational similarity compared to prototypical word pairs with the desired relation (Turney, 2008). In scenarios like information extraction or question answering, where identifying the existence of certain relations is often the core problem, measuring relational similarity provides a more flexible solution rather than creating relational classifiers for predefined or task-specific categories of relations (Turney, 2006; Jurgens et al., 2012). In order to promote this research direction, Jurgens et al. (2012) proposed a new shared task of measuring relational similarity in SemEval-2012 recently. In this task, each submitted system is required to judge the degree of a target word pair havin</context>
<context position="7189" citStr="Turney, 2008" startWordPosition="1092" endWordPosition="1093"> pair based on statistics of their co-occurrence patterns in Web documents and applied the k-NN method (k = 1 in their work). Measuring relational similarity, which determines whether two word pairs share the same relation, can be viewed as an extension of classifying relations between two words. Treating a relational similarity measure as a distance metric, a testing pair of words can be judged by whether they have a relation that is similar to some prototypical word pairs having a particular relation. A multi-relation classifier can thus be built easily in this framework as demonstrated in (Turney, 2008), where the problems of identifying synonyms, antonyms and associated words are all reduced to finding good analogous word pairs. Measuring relational similarity has been advocated and pioneered by Turney (2006), who proposed a latent vector space model for answering SAT analogy questions (e.g., mason:stone vs. carpenter:wood). In contrast, we take a slightly different view when building a relational similarity measure. Existing classifiers for specific word relations (e.g., synonyms or Is-A) are combined with general relational similarity measures. Empirically, mixing heterogeneous models ten</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dual-space model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>44--533</pages>
<contexts>
<context position="14716" citStr="Turney, 2012" startWordPosition="2294" endWordPosition="2295">word clustering (Brown et al., 1992) and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear (Turney, 2012). In a companion paper (Mikolov et al., 2013), we present a vector offset method which performs consistently well in identifying both syntactic and semantic regularities. This method measures the degree of the analogy “a is to b as c is to d” using the cosine score of (~vb − ~va + ~v�,~vd), where a, b, c, d are the four given words and ~va,~vb,~v,~vd are the corresponding vecFigure 1: Directional vectors υ1 and υ2 capture the relations of clothing:shirt and furniture:desk respectively in this semantic vector space. The relational similarity of these two word pairs is estimated by the cosine of</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D. Turney. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research (JAIR), 44:533–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>Algorithm for automatic interpretation of noun sequences.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94,</booktitle>
<pages>782--788</pages>
<contexts>
<context position="5670" citStr="Vanderwende, 1994" startWordPosition="853" endWordPosition="854">ed. The rest of this paper is organized as follows. We first survey the related work in Sec. 2 and formally define the problem in Sec. 3. We describe the individual models in detail in Sec. 4. The combination approach is depicted in Sec. 5, along with experimental comparisons to individual models and existing systems. Finally, Sec. 6 concludes the paper. 2 Related Work Building a classifier to determine whether a relationship holds between a pair of words is a natural approach to the task of measuring relational similarity. While early work was mostly based on hand-crafted rules (Finin, 1980; Vanderwende, 1994), Rosario and Hearst (2001) introduced a machine learning approach to classify word pairs. They targeted classifying noun modifier pairs from the medical domain into 13 classes of semantic relations. Features for each noun modifier pair were constructed using large medical lexical resources and a multiclass classifier was trained using a feed-forward neural network with one hidden layer. This work was later extended by Nastase and Szpakowicz (2003) to classify general domain noun-modifier pairs into 30 semantic relations. In addition to extracting features using WordNet and Roget’s Thesaurus, </context>
</contexts>
<marker>Vanderwende, 1994</marker>
<rawString>Lucy Vanderwende. 1994. Algorithm for automatic interpretation of noun sequences. In Proceedings of COLING-94, pages 782–788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wentao Wu</author>
<author>Hongsong Li</author>
<author>Haixun Wang</author>
<author>Kenny Q Zhu</author>
</authors>
<title>Probase: a probabilistic taxonomy for text understanding.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>481--492</pages>
<contexts>
<context position="22767" citStr="Wu et al., 2012" startWordPosition="3617" endWordPosition="3620">s, such as WordNet’s Is-A taxonomy and the extensive relations stored in the NELL (Carlson et al., 2010) knowledge base. Although in theory, these resources can be directly used to solve the problem of relational similarity, such direct approaches often suffer from two practical issues. First, the word coverage of these databases is usually very limited and it is common that the relation of a given word pair is absent. Second, the degree of relation is often not included, which makes the task of measuring the degree of relational similarity difficult. One counter example, however, is Probase (Wu et al., 2012), which is a knowledge base that establishes connections between more than 2.5 million concepts discovered automatically from the Web. For the Is-A and Attribute relations it encodes, Probase also returns the probability that two input words share the relation, based on the cooccurrence frequency. We used some relations in the training set to evaluate the quality of Probase. For instance, its Is-A model performs exceptionally well on the relation Class-Inclusion:Taxonomic, reaching a high Spearman’s p = 0.642 and MaxDiff accuracy 55.8%. Similarly, its Attribute model performs better than our l</context>
</contexts>
<marker>Wu, Li, Wang, Zhu, 2012</marker>
<rawString>Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q. Zhu. 2012. Probase: a probabilistic taxonomy for text understanding. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, pages 481–492, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Vahed Qazvinian</author>
</authors>
<title>Measuring word relatedness using heterogeneous vector space models.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>616--620</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1632" citStr="Yih and Qazvinian, 2012" startWordPosition="222" endWordPosition="225">se in Spearman’s rank correlation. 1 Introduction The problem of measuring relational similarity is to determine the degree of correspondence between two word pairs. For instance, the analogous word pairs silverware:fork and clothing:shirt both exemplify well a Class-Inclusion:Singular Collective relation and thus have high relational similarity. Unlike the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, *Work conducted while interning at Microsoft Research. antonyms or associations, can be reduced to measuring relational similarity compared to prototypical word pairs with the desired relation (Turney, 2008). In scenarios like information extraction or question answering, where identifying the existence of certain relations is often the core problem, measuring relation</context>
</contexts>
<marker>Yih, Qazvinian, 2012</marker>
<rawString>Wen-tau Yih and Vahed Qazvinian. 2012. Measuring word relatedness using heterogeneous vector space models. In Proceedings of NAACL-HLT, pages 616– 620, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
<author>John Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1212--1222</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="23917" citStr="Yih et al., 2012" startWordPosition="3794" endWordPosition="3797">cy 55.8%. Similarly, its Attribute model performs better than our lexical pattern model on Attribute:Agent Attribute-State with Spearman’s p = 0.290 and MaxDiff accuracy 32.7%. 4.3.2 Lexical Semantics Measures Most lexical semantics measures focus on the semantic similarity or relatedness of two words. Since our task focuses on distinguishing the difference between word pairs in the same relation category. The crude relatedness model does not seem to help in our preliminary experimental study. Instead, we leverage the recently proposed polarity-inducing latent semantic analysis (PILSA) model (Yih et al., 2012), which specifically estimates the degree of synonymy and antonymy. This method first forms a signed cooccurrence matrix using synonyms and antonyms in a thesaurus and then generalizes it using a low-rank approximation derived by SVD. Given two words, the cosine score of their PILSA vectors tend to be negative if they are antonymous and positive if synonymous. When tested on the Similar:Synonymity relation, it has a Spearman’s p = 0.242 and MaxDiff accuracy 42.1%, both are better than those of our directional similarity and lexical pattern models. 5 Model Combination In order to fully leverage</context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of NAACL-HLT, pages 1212–1222, Jeju Island, Korea, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>