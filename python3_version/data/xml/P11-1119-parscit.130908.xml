<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001273">
<title confidence="0.9983225">
An Affect-Enriched Dialogue Act Classification Model
for Task-Oriented Dialogue
</title>
<note confidence="0.429826333333333">
Kristy Joseph F. Eun Young Robert James C.
Elizabeth Grafsgaard Ha Phillips* Lester
Boyer
</note>
<affiliation confidence="0.8400965">
Department of Computer Science
North Carolina State University
</affiliation>
<address confidence="0.936466333333333">
Raleigh, NC, USA
* Dual Affiliation with Applied Research Associates, Inc.
Raleigh, NC, USA
</address>
<email confidence="0.965766">
{keboyer, jfgrafsg, eha, rphilli, lester}@ncsu.edu
</email>
<sectionHeader confidence="0.995527" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997075">
Dialogue act classification is a central chal-
lenge for dialogue systems. Although the im-
portance of emotion in human dialogue is
widely recognized, most dialogue act classifi-
cation models make limited or no use of affec-
tive channels in dialogue act classification.
This paper presents a novel affect-enriched
dialogue act classifier for task-oriented dia-
logue that models facial expressions of users,
in particular, facial expressions related to con-
fusion. The findings indicate that the affect-
enriched classifiers perform significantly bet-
ter for distinguishing user requests for feed-
back and grounding dialogue acts within
textual dialogue. The results point to ways in
which dialogue systems can effectively lever-
age affective channels to improve dialogue act
classification.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999595547619048">
Dialogue systems aim to engage users in rich,
adaptive natural language conversation. For these
systems, understanding the role of a user’s utter-
ance in the broader context of the dialogue is a key
challenge (Sridhar, Bangalore, &amp; Narayanan,
2009). Central to this endeavor is dialogue act
classification, which categorizes the intention be-
hind the user’s move (e.g., asking a question,
providing declarative information). Automatic dia-
logue act classification has been the focus of a
large body of research, and a variety of approach-
es, including sequential models (Stolcke et al.,
2000), vector-based models (Sridhar, Bangalore, &amp;
Narayanan, 2009), and most recently, feature-
enhanced latent semantic analysis (Di Eugenio,
Xie, &amp; Serafin, 2010), have shown promise. These
models may be further improved by leveraging
regularities of the dialogue from both linguistic
and extra-linguistic sources. Users’ expressions of
emotion are one such source.
Human interaction has long been understood to
include rich phenomena consisting of verbal and
nonverbal cues, with facial expressions playing a
vital role (Knapp &amp; Hall, 2006; McNeill, 1992;
Mehrabian, 2007; Russell, Bachorowski, &amp;
Fernandez-Dols, 2003; Schmidt &amp; Cohn, 2001).
While the importance of emotional expressions in
dialogue is widely recognized, the majority of dia-
logue act classification projects have focused either
peripherally (or not at all) on emotion, such as by
leveraging acoustic and prosodic features of spo-
ken utterances to aid in online dialogue act classi-
fication (Sridhar, Bangalore, &amp; Narayanan, 2009).
Other research on emotion in dialogue has in-
volved detecting affect and adapting to it within a
dialogue system (Forbes-Riley, Rotaru, Litman, &amp;
Tetreault, 2009; L—pez-C—zar, Silovsky, &amp; Griol,
2010), but this work has not explored leveraging
affect information for automatic user dialogue act
classification. Outside of dialogue, sentiment anal-
ysis within discourse is an active area of research
(L—pez-C—zar et al., 2010), but it is generally lim-
</bodyText>
<page confidence="0.929621">
1190
</page>
<note confidence="0.979152">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1190–1199,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999825794871795">
ited to modeling textual features and not multi-
modal expressions of emotion such as facial ac-
tions. Such multimodal expressions have only just
begun to be explored within corpus-based dialogue
research (Calvo &amp; D&apos;Mello, 2010; Cavicchio,
2009).
This paper presents a novel affect-enriched dia-
logue act classification approach that leverages
knowledge of users’ facial expressions during
computer-mediated textual human-human dia-
logue. Intuitively, the user’s affective state is a
promising source of information that may help to
distinguish between particular dialogue acts (e.g., a
confused user may be more likely to ask a ques-
tion). We focus specifically on occurrences of stu-
dents’ confusion-related facial actions during task-
oriented tutorial dialogue.
Confusion was selected as the focus of this
work for several reasons. First, confusion is known
to be prevalent within tutoring, and its implications
for student learning are thought to run deep
(Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten,
2005). Second, while identifying the “ground
truth” of emotion based on any external display by
a user presents challenges, prior research has
demonstrated a correlation between particular faci-
al action units and confusion during learning
(Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser,
2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006;
McDaniel et al., 2007). Finally, automatic facial
action recognition technologies are developing rap-
idly, and confusion-related facial action events are
among those that can be reliably recognized auto-
matically (Bartlett et al., 2006; Cohn, Reed,
Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp;
Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang,
2009). This promising development bodes well for
the feasibility of automatic real-time confusion
detection within dialogue systems.
</bodyText>
<sectionHeader confidence="0.899356" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<subsectionHeader confidence="0.971312">
2.1 Dialogue Act Classification
</subsectionHeader>
<bodyText confidence="0.999985392857143">
Because of the importance of dialogue act classifi-
cation within dialogue systems, it has been an ac-
tive area of research for some time. Early work on
automatic dialogue act classification modeled dis-
course structure with hidden Markov models, ex-
perimenting with lexical and prosodic features, and
applying the dialogue act model as a constraint to
aid in automatic speech recognition (Stolcke et al.,
2000). In contrast to this sequential modeling ap-
proach, which is best suited to offline processing,
recent work has explored how lexical, syntactic,
and prosodic features perform for online dialogue
act tagging (when only partial dialogue sequences
are available) within a maximum entropy frame-
work (Sridhar, Bangalore, &amp; Narayanan, 2009). A
recently proposed alternative approach involves
treating dialogue utterances as documents within a
latent semantic analysis framework, and applying
feature enhancements that incorporate such infor-
mation as speaker and utterance duration (Di
Eugenio et al., 2010). Of the approaches noted
above, the modeling framework presented in this
paper is most similar to the vector-based maximum
entropy approach of Sridhar et al. (2009). Howev-
er, it takes a step beyond the previous work by in-
cluding multimodal affective displays, specifically
facial expressions, as features available to an af-
fect-enriched dialogue act classification model.
</bodyText>
<subsectionHeader confidence="0.999884">
2.2 Detecting Emotions in Dialogue
</subsectionHeader>
<bodyText confidence="0.999963296296296">
Detecting emotional states during spoken dialogue
is an active area of research, much of which focus-
es on detecting frustration so that a user can be
automatically transferred to a human dialogue
agent (L—pez-C—zar et al., 2010). Research on spo-
ken dialogue has leveraged lexical features along
with discourse cues and acoustic information to
classify user emotion, sometimes at a coarse grain
along a positive/negative axis (Lee &amp; Narayanan,
2005). Recent work on an affective companion
agent has examined user emotion classification
within conversational speech (Cavazza et al.,
2010). In contrast to that spoken dialogue research,
the work in this paper is situated within textual
dialogue, a widely used modality of communica-
tion for which a deeper understanding of user af-
fect may substantially improve system
performance.
While many projects have focused on linguistic
cues, recent work has begun to explore numerous
channels for affect detection including facial ac-
tions, electrocardiograms, skin conductance, and
posture sensors (Calvo &amp; D&apos;Mello, 2010). A recent
project in a map task domain investigates some of
these sources of affect data within task-oriented
dialogue (Cavicchio, 2009). Like that work, the
current project utilizes facial action tagging, for
</bodyText>
<page confidence="0.98699">
1191
</page>
<bodyText confidence="0.9998192">
which promising automatic technologies exist
(Bartlett et al., 2006; Pantic &amp; Bartlett, 2007;
Zeng, Pantic, Roisman, &amp; Huang, 2009). However,
we leverage the recognized expressions of emotion
for the task of dialogue act classification.
</bodyText>
<subsectionHeader confidence="0.998158">
2.3 Categorizing Emotions within Dialogue
and Discourse
</subsectionHeader>
<bodyText confidence="0.999658416666667">
Sets of emotion taxonomies for discourse and dia-
logue are often application-specific, for example,
focusing on the frustration of users who are inter-
acting with a spoken dialogue system (L—pez-
C—zar et al., 2010), or on uncertainty expressed by
students while interacting with a tutor (Forbes-
Riley, Rotaru, Litman, &amp; Tetreault, 2007). In con-
trast, the most widely utilized emotion frameworks
are not application-specific; for example, Ekman’s
Facial Action Coding System (FACS) has been
widely used as a rigorous technique for coding fa-
cial movements based on human facial anatomy
(Ekman &amp; Friesen, 1978). Within this framework,
facial movements are categorized into facial action
units, which represent discrete movements of mus-
cle groups. Additionally, facial action descriptors
(for movements not derived from facial muscles)
and movement and visibility codes are included.
Ekman’s basic emotions (Ekman, 1999) have been
used in recent work on classifying emotion ex-
pressed within blog text (Das &amp; Bandyopadhyay,
2009), while other recent work (Nguyen, 2010)
utilizes Russell’s core affect model (Russell, 2003)
for a similar task.
During tutorial dialogue, students may not fre-
quently experience Ekman’s basic emotions of
happiness, sadness, anger, fear, surprise, and dis-
gust. Instead, students appear to more frequently
experience cognitive-affective states such as flow
and confusion (Calvo &amp; D&apos;Mello, 2010). Our work
leverages Ekman’s facial tagging scheme to identi-
fy a particular facial action unit, Action Unit 4
(AU4), that has been observed to correlate with
confusion (Craig, D&apos;Mello, Witherspoon, Sullins,
&amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp;
Graesser, 2006; McDaniel et al., 2007).
</bodyText>
<subsectionHeader confidence="0.981963">
2.4 Importance of Confusion in Tutorial Dia-
logue
</subsectionHeader>
<bodyText confidence="0.999976583333334">
Among the affective states that students experience
during tutorial dialogue, confusion is prevalent,
and its implications for student learning are signif-
icant. Confusion is associated with cognitive dise-
quilibrium, a state in which students’ existing
knowledge is inconsistent with a novel learning
experience (Graesser, Lu, Olde, Cooper-Pye, &amp;
Whitten, 2005). Students may express such confu-
sion within dialogue as uncertainty, to which hu-
man tutors often adapt in a context-dependent
fashion (Forbes-Riley et al., 2007). Moreover, im-
plementing adaptations to student uncertainty with-
in a dialogue system can improve the effectiveness
of the system (Forbes-Riley et al., 2009).
For tutorial dialogue, the importance of under-
standing student utterances is paramount for a sys-
tem to positively impact student learning
(Dzikovska, Moore, Steinhauser, &amp; Campbell,
2010). The importance of frustration as a cogni-
tive-affective state during learning suggests that
the presence of student confusion may serve as a
useful constraining feature for dialogue act classi-
fication of student utterances. This paper explores
the use of facial expression features in this way.
</bodyText>
<sectionHeader confidence="0.985807" genericHeader="method">
3 Task-Oriented Dialogue Corpus
</sectionHeader>
<bodyText confidence="0.99999695">
The corpus was collected during a textual human-
human tutorial dialogue study in the domain of
introductory computer science (Boyer, Phillips, et
al., 2010). Students solved an introductory com-
puter programming problem and carried on textual
dialogue with tutors, who viewed a synchronized
version of the students’ problem-solving work-
space. The original corpus consists of 48 dia-
logues, one per student. Each student interacted
with one of two tutors. Facial videos of students
were collected using built-in webcams, but were
not shown to the tutors. Video quality was ranked
based on factors such as obscured foreheads due to
hats or hair, and improper camera position result-
ing in students’ faces not being fully captured on
the video. The highest-quality set contained 14
videos, and these videos were used in this analysis.
They have a total running time of 11 hours and 55
minutes, and include dialogues with three female
subjects and eleven male subjects.
</bodyText>
<subsectionHeader confidence="0.999234">
3.1 Dialogue act annotation
</subsectionHeader>
<bodyText confidence="0.99969175">
The dialogue act annotation scheme (Table 1) was
applied manually. The kappa statistic for inter-
annotator agreement on a 10% subset of the corpus
was κ=0.80, indicating good reliability.
</bodyText>
<page confidence="0.997965">
1192
</page>
<tableCaption confidence="0.995125">
Table 1. Dialogue act tags and relative frequencies
across fourteen dialogues in video corpus
</tableCaption>
<figure confidence="0.946576361111111">
Student Dialogue Rel.
Example
Act Freq.
EXTRA-DOMAIN Little sleep deprived .08
(EX) today
GROUNDING (G) Ok or Thanks .21
NEGATIVE I’m still confused on
FEEDBACK WITH what this next for loop .02
ELABORATION (NE) is doing.
NEGATIVE I don’t see the diff. .04
FEEDBACK (N)
It makes sense now
POSITIVE
FEEDBACK WITH that you explained it,
but I never used an .04
else if in any of my
ELABORATION (PE)
other programs
POSITIVE
Second part complete. .11
FEEDBACK (P)
Why couldn’t I have
QUESTION (Q) .11
said if (i&lt;5)
STATEMENT (S) i is my only index .07
So I need to create a
REQUEST FOR new method that sees .16
FEEDBACK (RF) how many elements
are in my array?
RESPONSE (RSP) You mean not the .14
length but the contents
UNCERTAIN I’m trying to remember .008
FEEDBACK WITH how to copy arrays
ELABORATION (UE)
UNCERTAIN .008
Not quite yetFEEDBACK (U)
</figure>
<subsectionHeader confidence="0.997635">
3.2 Task action annotation
</subsectionHeader>
<bodyText confidence="0.998753869565217">
The tutoring sessions were task-oriented, focusing
on a computer programming exercise. The task had
several subtasks consisting of programming mod-
ules to be implemented by the student. Each of
those subtasks also had numerous fine-grained
goals, and student task actions either contributed or
did not contribute to the goals. Therefore, to obtain
a rich representation of the task, a manual annota-
tion along two dimensions was conducted (Boyer,
Phillips, et al., 2010). First, the subtask structure
was annotated hierarchically, and then each task
action was labeled for correctness according to the
requirements of the assignment. Inter-annotator
agreement was computed on 20% of the corpus at
the leaves of the subtask tagging scheme, and re-
sulted in a simple kappa of κ=.56. However, the
leaves of the annotation scheme feature an implicit
ordering (subtasks were completed in order, and
adjacent subtasks are semantically more similar
than subtasks at a greater distance); therefore, a
weighted kappa is also meaningful to consider for
this annotation. The weighted kappa is κ,,ejghre,F=.80.
An annotated excerpt of the corpus is displayed in
</bodyText>
<tableCaption confidence="0.996473">
Table 2.
Table 2. Excerpt from corpus illustrating annota-
tions and interplay between dialogue and task
</tableCaption>
<table confidence="0.999166470588235">
13:38:09 Student: How do I know where to
end? [RF]
13:38:26 Tutor: Well you told me how to get
how many elements in an
array by using .length right?
13:38:26 Student: [Task action:
Subtask 1-a-iv, Buggy]
13:38:56 Tutor: Great
13:38:56 Student: [Task action:
Subtask 1-a-v, Correct]
13:39:35 Student: Well is it &amp;quot;array.length&amp;quot;?
[RF]
**Facial Expression: AU4
13:39:46 Tutor: You just need to use the
correct array name
13:39:46 Student: [Task action:
Subtask 1-a-iv, Buggy]
</table>
<subsectionHeader confidence="0.992209">
3.3 Lexical and Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999962375">
In addition to the manually annotated dialogue and
task features described above, syntactic features of
each utterance were automatically extracted using
the Stanford Parser (De Marneffe et al., 2006).
From the phrase structure trees, we extracted the
top-most syntactic node and its first two children.
In the case where an utterance consisted of more
than one sentence, only the phrase structure tree of
the first sentence was considered. Individual word
tokens in the utterances were further processed
with the Porter Stemmer (Porter, 1980) in the
NLTK package (Loper &amp; Bird, 2004). Our prior
work has shown that these lexical and syntactic
features are highly predictive of dialogue acts dur-
ing task-oriented tutorial dialogue (Boyer, Ha et al.
2010).
</bodyText>
<page confidence="0.980902">
1193
</page>
<sectionHeader confidence="0.995841" genericHeader="method">
4 Facial Action Tagging
</sectionHeader>
<bodyText confidence="0.998803113636364">
An annotator who was certified in the Facial Ac-
tion Coding System (FACS) (Ekman, Friesen, &amp;
Hager, 2002) tagged the video corpus consisting of
fourteen dialogues. The FACS certification process
requires annotators to pass a test designed to ana-
lyze their agreement with reference coders on a set
of spontaneous facial expressions (Ekman &amp;
Rosenberg, 2005). This annotator viewed the vide-
os continuously and paused the playback whenever
notable facial displays of Action Unit 4 (AU4:
Brow Lowerer) were seen. This action unit was
chosen for this study based on its correlations with
confusion in prior research (Craig, D&apos;Mello,
Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello,
Craig, Sullins, &amp; Graesser, 2006; McDaniel et al.,
2007).
To establish reliability of the annotation, a se-
cond FACS-certified annotator independently an-
notated 36% of the video corpus (5 of 14
dialogues), chosen randomly after stratification by
gender and tutor. This annotator followed the same
method as the first annotator, pausing the video at
any point to tag facial action events. At any given
time in the video, the coder was first identifying
whether an action unit event existed, and then de-
scribing the facial movements that were present.
The annotators also specified the beginning and
ending time of each event. In this way, the action
unit event tags spanned discrete durations of vary-
ing length, as specified by the coders. Because the
two coders were not required to tag at the same
point in time, but rather were permitted the free-
dom to stop the video at any point where they felt a
notable facial action event occurred, calculating
agreement between annotators required discretiz-
ing the continuous facial action time windows
across the tutoring sessions. This discretization
was performed at granularities of 1/4, 1/2, 3/4, and
1 second, and inter-rater reliability was calculated
at each level of granularity (Table 3). Windows in
which both annotators agreed that no facial action
event was present were tagged by default as neu-
tral. Figure 1 illustrates facial expressions that dis-
play facial Action Unit 4.
</bodyText>
<tableCaption confidence="0.855574">
Table 3. Kappa values for inter-annotator agree-
ment on facial action events
</tableCaption>
<figure confidence="0.9768168">
Granularity
1/4 sec 1/2 sec 3/4 sec 1 sec
Presence of AU4
84 .87 .86 .86
(Brow Lowerer)
</figure>
<figureCaption confidence="0.985524">
Figure 1. Facial expressions displaying AU4
(Brow Lowerer)
</figureCaption>
<bodyText confidence="0.99841425">
Despite the fact that promising automatic ap-
proaches exist to identifying many facial action
units (Bartlett et al., 2006; Cohn, Reed, Ambadar,
Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007;
Zeng, Pantic, Roisman, &amp; Huang, 2009), manual
annotation was selected for this project for two
reasons. First, manual annotation is more robust
than automatic recognition of facial action units,
and manual annotation facilitated an exploratory,
comprehensive view of student facial expressions
during learning through task-oriented dialogue.
Although a detailed discussion of the other emo-
tions present in the corpus is beyond the scope of
this paper, Figure 2 illustrates some other sponta-
neous student facial expressions that differ from
those associated with confusion.
</bodyText>
<page confidence="0.994731">
1194
</page>
<figureCaption confidence="0.998781">
Figure 2. Other facial expressions from the corpus
</figureCaption>
<sectionHeader confidence="0.995931" genericHeader="method">
5 Models
</sectionHeader>
<bodyText confidence="0.9998004">
The goal of the modeling experiment was to de-
termine whether the addition of confusion-related
facial expression features significantly boosts dia-
logue act classification accuracy for student utter-
ances.
</bodyText>
<subsectionHeader confidence="0.754062">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.9998095">
We take a vector-based approach, in which the fea-
tures consist of the following:
</bodyText>
<sectionHeader confidence="0.426968" genericHeader="method">
Utterance Features
</sectionHeader>
<bodyText confidence="0.653535666666667">
• Dialogue act features: Manually annotated
dialogue act for the past three utterances.
These features include tutor dialogue acts,
annotated with a scheme analogous to that
used to annotate student utterances (Boyer
et al., 2009).
</bodyText>
<listItem confidence="0.952546321428571">
• Speaker: Speaker for past three utterances
• Lexical features: Word unigrams
• Syntactic features: Top-most syntactic
node and its first two children
Task-based Features
• Subtask: Hierarchical subtask structure for
past three task actions (semantic pro-
gramming actions taken by student)
• Correctness: Correctness of past three task
actions taken by student
• Preceded by task: Indicator for whether the
most recent task action immediately pre-
ceded the target utterance, or whether it
was immediately preceded by the last dia-
logue move
Facial Expression Features
• AU4_1sec: Indicator for the display of the
brow lowerer within 1 second prior to this
utterance being sent, for the most recent
three utterances
• AU4_5sec: Indicator for the display of the
brow lowerer within 5 seconds prior to this
utterance being sent, for the most recent
three utterances
• AU4 _r10sec: Indicator for the display of
the bow lowerer within 10 seconds prior
to this utterance being sent, for the most
recent three utterances
</listItem>
<subsectionHeader confidence="0.999669">
5.2 Modeling Approach
</subsectionHeader>
<bodyText confidence="0.999982615384615">
A logistic regression approach was used to classify
the dialogue acts based on the above feature vec-
tors. The Weka machine learning toolkit (Hall et
al., 2009) was used to learn the models and to first
perform feature selection in a best-first search. Lo-
gistic regression is a generalized maximum likeli-
hood model that discriminates between pairs of
output values by calculating a feature weight vec-
tor over the predictors.
The goal of this work is to explore the utility of
confusion-related facial features in the context of
particular dialogue act types. For this reason, a
specialized classifier was learned by dialogue act.
</bodyText>
<subsectionHeader confidence="0.997515">
5.3 Classification Results
</subsectionHeader>
<bodyText confidence="0.9999878125">
The classification accuracy and kappa for each
specialized classifier is displayed in Table 4. Note
that kappa statistics adjust for the accuracy that
would be expected by majority-baseline chance; a
kappa statistic of zero indicates that the classifier
performed equal to chance, and a positive kappa
statistic indicates that the classifier performed bet-
ter than chance. A kappa of 1 constitutes perfect
agreement. As the table illustrates, the feature se-
lection chose to utilize the AU4 feature for every
dialogue act except STATEMENT (S). When consid-
ering the accuracy of the model across the ten
folds, two of the affect-enriched classifiers exhibit-
ed statistically significantly better performance.
For GROUNDING (G) and REQUEST FOR FEEDBACK
(RF), the facial expression features significantly
</bodyText>
<page confidence="0.982957">
1195
</page>
<bodyText confidence="0.9989505">
improved the classification accuracy compared to a
model that was learned without affective features.
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.998798">
Dialogue act classification is an essential task for
dialogue systems, and it has been addressed with a
variety of modeling approaches and feature sets.
We have presented a novel approach that treats
facial expressions of students as constraining fea-
tures for an affect-enriched dialogue act classifica-
tion model in task-oriented tutorial dialogue. The
results suggest that knowledge of the student’s
confusion-related facial expressions can signifi-
cantly enhance dialogue act classification for two
types of dialogue acts, GROUNDING and REQUEST
FOR FEEDBACK.
</bodyText>
<tableCaption confidence="0.892395">
Table 4. Classification accuracy and kappa for spe-
cialized DA classifiers. Statistically significant
differences (across ten folds, one-tailed t-test) are
shown in bold.
</tableCaption>
<table confidence="0.998960947368421">
Dialogue Classifier Classifier P-
Act with AU4 without value
% AU4
acc κ %
acc κ
EX 90.7 .62 89.0 .28 &gt;.05
G 92.6 .76 91 .71 .018
P 93 .49 92.2 .40 &gt;.05
Q 94.6 .72 94.2 .72 &gt;.05
S Not chosen 93 .22 n/a
in feat. sel.
RF 90.7 .62 88.3 .53 .003
RSP 93 .68 95 .75 &gt;.05
NE &amp;quot; &amp;quot;
N * &amp;quot;
PE &amp;quot; &amp;quot;
U * &amp;quot;
UE &amp;quot; &amp;quot;
*Too few instances for ten-fold cross-validation.
</table>
<subsectionHeader confidence="0.881431">
6.1 Features Selected for Classification
</subsectionHeader>
<bodyText confidence="0.999975">
Out of more than 1500 features available during
feature selection, each of the specialized dialogue
act classifiers selected between 30 and 50 features
in each condition (with and without affect fea-
tures). To gain insight into the specific features
that were useful for classifying these dialogue acts,
it is useful to examine which of the AU4 history
features were chosen during feature selection.
For GROUNDING, features that indicated the
presence of absence of AU4 in the immediately
preceding utterance, either at the 1 second or 5 se-
cond granularity, were selected. Absence of this
confusion-related facial action unit was associated
with a higher probability of a grounding act, such
as an acknowledgement. This finding is consistent
with our understanding of how students and tutors
interacted in this corpus; when a student experi-
enced confusion, she would be unlikely to then
make a simple grounding dialogue move, but in-
stead would tend to inspect her computer program,
ask a question, or wait for the tutor to explain
more.
For REQUEST FOR FEEDBACK, the predictive
features were presence or absence of AU4 within
ten seconds of the longest available history (three
turns in the past), as well as the presence of AU4
within five seconds of the current utterance (the
utterance whose dialogue act is being classified).
This finding suggests that there may be some lag
between the student experiencing confusion and
then choosing to make a request for feedback, and
that the confusion-related facial expressions may
re-emerge as the student is making a request for
feedback, since the five-second window prior to
the student sending the textual dialogue message
would overlap with the student’s construction of
the message itself.
Although the improvements seen with AU4 fea-
tures for QUESTION, POSITIVE FEEDBACK, and
EXTRA-DOMAIN acts were not statistically reliable,
examining the AU4 features that were selected for
classifying these moves points toward ways in
which facial expressions may influence classifica-
tion of these acts (Table 5).
</bodyText>
<page confidence="0.998109">
1196
</page>
<tableCaption confidence="0.9980835">
Table 5. Number of features, and AU4 features
selected, for specialized DA classifiers
</tableCaption>
<table confidence="0.919854142857143">
# fea-
Dialogue tures
Act selected AU4 features selected
G 43 One utterance ago:AU4_1sec, AU4_5sec
Three utterances ago:
AU4_10sec
Target utterance:
AU4_5sec
EX 50 Three utterances ago:
AU4_1sec
P 36 Current utterance:
AU4_10sec
Q 30 One utterance ago:
AU4_5sec
</table>
<subsectionHeader confidence="0.997853">
6.2 Implications
</subsectionHeader>
<bodyText confidence="0.999962315789474">
The results presented here demonstrate that lever-
aging knowledge of user affect, in particular of
spontaneous facial expressions, may improve the
performance of dialogue act classification models.
Perhaps most interestingly, displays of confusion-
related facial actions prior to a student dialogue
move enabled an affect-enriched classifier to rec-
ognize requests for feedback with significantly
greater accuracy than a classifier that did not have
access to the facial action features. Feedback is
known to be a key component of effective tutorial
dialogue, through which tutors provide adaptive
help (Shute, 2008). Requesting feedback also
seems to be an important behavior of students,
characteristically engaged in more frequently by
women than men, and more frequently by students
with lower incoming knowledge than by students
with higher incoming knowledge (Boyer, Vouk, &amp;
Lester, 2007).
</bodyText>
<subsectionHeader confidence="0.999361">
6.3 Limitations
</subsectionHeader>
<bodyText confidence="0.999987458333333">
The experiments reported here have several nota-
ble limitations. First, the time-consuming nature of
manual facial action tagging restricted the number
of dialogues that could be tagged. Although the
highest quality videos were selected for annotation,
other medium quality videos would have been suf-
ficiently clear to permit tagging, which would have
increased the sample size and likely revealed sta-
tistically significant trends. For example, the per-
formance of the affect-enriched classifier was bet-
ter for dialogue acts of interest such as positive
feedback and questions, but this difference was not
statistically reliable.
An additional limitation stems from the more
fundamental question of which affective states are
indicated by particular external displays. The field
is only just beginning to understand facial expres-
sions during learning and to correlate these facial
actions with emotions. Additional research into the
“ground truth” of emotion expression will shed
additional light on this area. Finally, the results of
manual facial action annotation may constitute up-
per-bound findings for applying automatic facial
expression analysis to dialogue act classification.
</bodyText>
<sectionHeader confidence="0.990012" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999901066666667">
Emotion plays a vital role in human interactions. In
particular, the role of facial expressions in human-
human dialogue is widely recognized. Facial ex-
pressions offer a promising channel for under-
standing the emotions experienced by users of
dialogue systems, particularly given the ubiquity of
webcam technologies and the increasing number of
dialogue systems that are deployed on webcam-
enabled devices. This paper has reported on a first
step toward using knowledge of user facial expres-
sions to improve a dialogue act classification mod-
el for tutorial dialogue, and the results demonstrate
that facial expressions hold great promise for dis-
tinguishing the pedagogically relevant dialogue act
REQUEST FOR FEEDBACK, and the conversational
moves of GROUNDING.
These early findings highlight the importance
of future work in this area. Dialogue act classifica-
tion models have not fully leveraged some of the
techniques emerging from work on sentiment anal-
ysis. These approaches may prove particularly use-
ful for identifying emotions in dialogue utterances.
Another important direction for future work in-
volves more fully exploring the ways in which af-
fect expression differs between textual and spoken
dialogue. Finally, as automatic facial tagging tech-
nologies mature, they may prove powerful enough
to enable broadly deployed dialogue systems to
feasibly leverage facial expression data in the near
future.
</bodyText>
<equation confidence="0.553427">
RF 37
</equation>
<page confidence="0.990828">
1197
</page>
<sectionHeader confidence="0.997993" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994641">
This work is supported in part by the North Caroli-
na State University Department of Computer Sci-
ence and by the National Science Foundation
through Grants REC-0632450, IIS-0812291, DRL-
1007962 and the STARS Alliance Grant CNS-
0739216. Any opinions, findings, conclusions, or
recommendations expressed in this report are those
of the participants, and do not necessarily represent
the official views, opinions, or policy of the Na-
tional Science Foundation.
</bodyText>
<sectionHeader confidence="0.998893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993234936170213">
A. Andreevskaia and S. Bergler. 2008. When specialists
and generalists work together: Overcoming do-
main dependence in sentiment tagging. Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics and Human Language
Technologies (ACL HLT), 290-298.
M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I.
Fasel, and J. Movellan. 2006. Fully Automatic
Facial Action Recognition in Spontaneous Behav-
ior. 7th International Conference on Automatic
Face and Gesture Recognition (FGR06), 223-230.
K.E. Boyer, M. Vouk, and J.C. Lester. 2007. The influ-
ence of learner characteristics on task-oriented tu-
torial dialogue. Proceedings of the International
Conference on Artificial Intelligence in Educa-
tion, 365–372.
K.E. Boyer, E.Y. Ha, R. Phillips, M.D. Wallis, M.
Vouk, and J.C. Lester. 2010. Dialogue act model-
ing in a complex task-oriented domain. Proceed-
ings of the 11th Annual Meeting of the Special
Interest Group on Discourse and Dialogue
(SIGDIAL), 297-305.
K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A.
Vouk, and J.C. Lester. 2009. Modeling dialogue
structure with adjacency pair analysis and hidden
Markov models. Proceedings of the Annual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Short
Papers, 49-52.
K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A.
Vouk, and J.C. Lester. 2010. Leveraging hidden
dialogue state to select tutorial moves. Proceed-
ings of the NAACL HLT 2010 Fifth Workshop on
Innovative Use of NLP for Building Educational
Applications, 66-73.
R.A. Calvo and S. D’Mello. 2010. Affect Detection: An
Interdisciplinary Review of Models, Methods, and
Their Applications. IEEE Transactions on Affec-
tive Computing, 1(1): 18-37.
M. Cavazza, R.S.D.L. C‡mara, M. Turunen, J. Gil, J.
Hakulinen, N. Crook, et al. 2010. How was your
day? An affective companion ECA prototype.
Proceedings of the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue
(SIGDIAL), 277-280.
F. Cavicchio. 2009. The modulation of cooperation and
emotion in dialogue: the REC Corpus. Proceed-
ings of the ACL-IJCNLP 2009 Student Research
Workshop, 43-48.
J.F. Cohn, L.I. Reed, Z. Ambadar, J. Xiao, and T. Mori-
yama. 2004. Automatic Analysis and Recognition
of Brow Actions and Head Motion in Spontaneous
Facial Behavior. IEEE International Conference
on Systems, Man and Cybernetics, 610-616.
S.D. Craig, S. D’Mello, A. Witherspoon, J. Sullins, and
A.C. Graesser. 2004. Emotions during learning:
The first steps toward an affect sensitive intelli-
gent tutoring system. In J. Nall and R. Robson
(Eds.), E-learn 2004: World conference on E-
learning in Corporate, Government, Healthcare, &amp;
Higher Education, 241-250.
D. Das and S. Bandyopadhyay. 2009. Word to sentence
level emotion tagging for Bengali blogs. Proceed-
ings of the ACL-IJCNLP Conference, Short Pa-
pers, 149-152.
S. Dasgupta and V. Ng. 2009. Mine the easy, classify
the hard: a semi-supervised approach to automatic
sentiment classification. Proceedings of the 46th
Annual Meeting of the ACL and the 4th IJCNLP,
701-709.
B. Di Eugenio, Z. Xie, and R. Serafin. 2010. Dialogue
Act Classification, Higher Order Dialogue Struc-
ture, and Instance-Based Learning. Dialogue &amp;
Discourse, 1(2): 1-24.
M. Dzikovska, J.D. Moore, N. Steinhauser, and G.
Campbell. 2010. The impact of interpretation
problems on tutorial dialogue. Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, Short Papers, 43-48.
S. D’Mello, S.D. Craig, J. Sullins, and A.C. Graesser.
2006. Predicting Affective States expressed
through an Emote-Aloud Procedure from AutoTu-
tor’s Mixed- Initiative Dialogue. International
Journal of Artificial Intelligence in Education,
16(1): 3-28.
P. Ekman. 1999. Basic Emotions. In T. Dalgleish and
M. J. Power (Eds.), Handbook of Cognition and
Emotion. New York: Wiley.
P. Ekman, W.V. Friesen. 1978. Facial Action Coding
System. Palo Alto, CA: Consulting Psychologists
Press.
P. Ekman, W.V. Friesen, and J.C. Hager. 2002. Facial
Action Coding System: Investigator’s Guide. Salt
Lake City, USA: A Human Face.
</reference>
<page confidence="0.867657">
1198
</page>
<reference confidence="0.99992796">
P. Ekman and E.L. Rosenberg (Eds.). 2005. What the
Face Reveals: Basic and Applied Studies of Spon-
taneous Expression Using the Facial Action Cod-
ing System (FACS) (2nd ed.). New York: Oxford
University Press.
K. Forbes-Riley, M. Rotaru, D.J. Litman, and J.
Tetreault. 2007. Exploring affect-context depend-
encies for adaptive system development. The Con-
ference of the North American Chapter of the
Association for Computational Linguistics and
Human Language Technologies (NAACL HLT),
Short Papers, 41-44.
K. Forbes-Riley, M. Rotaru, D.J. Litman, and J.
Tetreault. 2009. Adapting to student uncertainty
improves tutoring dialogues. Proceedings of the
14th International Conference on Artificial Intelli-
gence in Education (AIED), 33-40.
A.C. Graesser, S. Lu, B. Olde, E. Cooper-Pye, and S.
Whitten. 2005. Question asking and eye tracking
during cognitive disequilibrium: comprehending
illustrated texts on devices when the devices break
down. Memory &amp; Cognition, 33(7): 1235-1247.
S. Greene and P. Resnik. 2009. More than words: Syn-
tactic packaging and implicit sentiment. Proceed-
ings of the 2009 Annual Conference of the North
American Chapter of the ACL and Human Lan-
guage Technologies (NAACL HLT), 503-511.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explora-
tions, 11(1): 10–18.
R. Iida, S. Kobayashi, and T. Tokunaga. 2010. Incorpo-
rating extra-linguistic information into reference
resolution in collaborative task dialogue. Proceed-
ings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, 1259-1267.
M.L. Knapp and J.A. Hall. 2006. Nonverbal Communi-
cation in Human Interaction (6th ed.). Belmont,
CA: Wadsworth/Thomson Learning.
C.M. Lee, S.S. Narayanan. 2005. Toward detecting
emotions in spoken dialogs. IEEE Transactions on
Speech and Audio Processing, 13(2): 293-303.
R. L—pez-C—zar, J. Silovsky, and D. Griol. 2010. F2–
New Technique for Recognition of User Emotion-
al States in Spoken Dialogue Systems. Proceed-
ings of the 11th Annual Meeting of the Special
Interest Group on Discourse and Dialogue
(SIGDIAL), 281-288.
B.T. McDaniel, S. D’Mello, B.G. King, P. Chipman, K.
Tapp, and A.C. Graesser. 2007. Facial Features
for Affective State Detection in Learning Envi-
ronments. Proceedings of the 29th Annual Cogni-
tive Science Society, 467-472.
D. McNeill. 1992. Hand and mind: What gestures reveal
about thought. Chicago: University of Chicago
Press.
A. Mehrabian. 2007. Nonverbal Communication. New
Brunswick, NJ: Aldine Transaction.
T. Nguyen. 2010. Mood patterns and affective lexicon
access in weblogs. Proceedings of the ACL 2010
Student Research Workshop, 43-48.
M. Pantic and M.S. Bartlett. 2007. Machine Analysis of
Facial Expressions. In K. Delac and M. Grgic
(Eds.), Face Recognition, 377-416. Vienna, Aus-
tria: I-Tech Education and Publishing.
J.A. Russell. 2003. Core affect and the psychological
construction of emotion. Psychological Review,
110(1): 145-172.
J.A. Russell, J.A. Bachorowski, and J.M. Fernandez-
Dols. 2003. Facial and vocal expressions of emo-
tion. Annual Review of Psychology, 54, 329-49.
K.L. Schmidt and J.F. Cohn. 2001. Human Facial Ex-
pressions as Adaptations: Evolutionary Questions
in Facial Expression Research. Am J Phys An-
thropol, 33: 3-24.
V.J. Shute. 2008. Focus on Formative Feedback. Re-
view of Educational Research, 78(1): 153-189.
V.K.R Sridar, S. Bangalore, and S.S. Narayanan. 2009.
Combining lexical, syntactic and prosodic cues for
improved online dialog act tagging. Computer
Speech &amp; Language, 23(4): 407-422. Elsevier Ltd.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, et al. 2000. Dialogue Act Modeling
for Automatic Tagging and Recognition of Con-
versational Speech. Computational Linguistics,
26(3): 339-373.
C. Toprak, N. Jakob, and I. Gurevych. 2010. Sentence
and expression level annotation of opinions in us-
er-generated discourse. Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, 575-584.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: An Exploration of Fea-
tures for Phrase-Level Sentiment Analysis.
Computational Linguistics, 35(3): 399-433.
Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang.
2009. A Survey of Affect Recognition Methods:
Audio, Visual, and Spontaneous Expressions.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 31(1): 39-58.
</reference>
<page confidence="0.998095">
1199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.315971">
<title confidence="0.998827">An Affect-Enriched Dialogue Act Classification for Task-Oriented Dialogue</title>
<author confidence="0.918457">Boyer Joseph Eun James Grafsgaard Ha Lester</author>
<affiliation confidence="0.766324">Department of Computer North Carolina State</affiliation>
<address confidence="0.732883">Raleigh, NC, USA</address>
<affiliation confidence="0.959723">with Applied Research Associates,</affiliation>
<address confidence="0.999308">Raleigh, NC, USA</address>
<email confidence="0.999835">jfgrafsg,eha,rphilli,lester}@ncsu.edu</email>
<abstract confidence="0.995182736842105">Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Andreevskaia</author>
<author>S Bergler</author>
</authors>
<title>When specialists and generalists work together: Overcoming domain dependence in sentiment tagging.</title>
<date>2008</date>
<booktitle>Proceedings of the Annual Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL HLT),</booktitle>
<pages>290--298</pages>
<marker>Andreevskaia, Bergler, 2008</marker>
<rawString>A. Andreevskaia and S. Bergler. 2008. When specialists and generalists work together: Overcoming domain dependence in sentiment tagging. Proceedings of the Annual Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL HLT), 290-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Bartlett</author>
<author>G Littlewort</author>
<author>M Frank</author>
<author>C Lainscsek</author>
<author>I Fasel</author>
<author>J Movellan</author>
</authors>
<title>Fully Automatic Facial Action Recognition</title>
<date>2006</date>
<booktitle>in Spontaneous Behavior. 7th International Conference on Automatic Face and Gesture Recognition (FGR06),</booktitle>
<pages>223--230</pages>
<contexts>
<context position="4971" citStr="Bartlett et al., 2006" startWordPosition="721" endWordPosition="724">deep (Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten, 2005). Second, while identifying the “ground truth” of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular facial action units and confusion during learning (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). Finally, automatic facial action recognition technologies are developing rapidly, and confusion-related facial action events are among those that can be reliably recognized automatically (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems. 2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classification within dialogue systems, it has been an active area of research for some time. Early work on automatic dialogue act classification modeled discourse structure with hidden Markov models, experimenting with lexical and prosodic features, and appl</context>
<context position="8027" citStr="Bartlett et al., 2006" startWordPosition="1176" endWordPosition="1179">mmunication for which a deeper understanding of user affect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial actions, electrocardiograms, skin conductance, and posture sensors (Calvo &amp; D&apos;Mello, 2010). A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009). Like that work, the current project utilizes facial action tagging, for 1191 which promising automatic technologies exist (Bartlett et al., 2006; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009). However, we leverage the recognized expressions of emotion for the task of dialogue act classification. 2.3 Categorizing Emotions within Dialogue and Discourse Sets of emotion taxonomies for discourse and dialogue are often application-specific, for example, focusing on the frustration of users who are interacting with a spoken dialogue system (L—pezC—zar et al., 2010), or on uncertainty expressed by students while interacting with a tutor (ForbesRiley, Rotaru, Litman, &amp; Tetreault, 2007). In contrast, the most widely utilized em</context>
<context position="18340" citStr="Bartlett et al., 2006" startWordPosition="2789" endWordPosition="2792">nd, and inter-rater reliability was calculated at each level of granularity (Table 3). Windows in which both annotators agreed that no facial action event was present were tagged by default as neutral. Figure 1 illustrates facial expressions that display facial Action Unit 4. Table 3. Kappa values for inter-annotator agreement on facial action events Granularity 1/4 sec 1/2 sec 3/4 sec 1 sec Presence of AU4 84 .87 .86 .86 (Brow Lowerer) Figure 1. Facial expressions displaying AU4 (Brow Lowerer) Despite the fact that promising automatic approaches exist to identifying many facial action units (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009), manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emotions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other spontaneous student facial expressions that </context>
</contexts>
<marker>Bartlett, Littlewort, Frank, Lainscsek, Fasel, Movellan, 2006</marker>
<rawString>M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. 2006. Fully Automatic Facial Action Recognition in Spontaneous Behavior. 7th International Conference on Automatic Face and Gesture Recognition (FGR06), 223-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>M Vouk</author>
<author>J C Lester</author>
</authors>
<title>The influence of learner characteristics on task-oriented tutorial dialogue.</title>
<date>2007</date>
<booktitle>Proceedings of the International Conference on Artificial Intelligence in Education,</booktitle>
<pages>365--372</pages>
<contexts>
<context position="26635" citStr="Boyer, Vouk, &amp; Lester, 2007" startWordPosition="4083" endWordPosition="4087">tions prior to a student dialogue move enabled an affect-enriched classifier to recognize requests for feedback with significantly greater accuracy than a classifier that did not have access to the facial action features. Feedback is known to be a key component of effective tutorial dialogue, through which tutors provide adaptive help (Shute, 2008). Requesting feedback also seems to be an important behavior of students, characteristically engaged in more frequently by women than men, and more frequently by students with lower incoming knowledge than by students with higher incoming knowledge (Boyer, Vouk, &amp; Lester, 2007). 6.3 Limitations The experiments reported here have several notable limitations. First, the time-consuming nature of manual facial action tagging restricted the number of dialogues that could be tagged. Although the highest quality videos were selected for annotation, other medium quality videos would have been sufficiently clear to permit tagging, which would have increased the sample size and likely revealed statistically significant trends. For example, the performance of the affect-enriched classifier was better for dialogue acts of interest such as positive feedback and questions, but t</context>
</contexts>
<marker>Boyer, Vouk, Lester, 2007</marker>
<rawString>K.E. Boyer, M. Vouk, and J.C. Lester. 2007. The influence of learner characteristics on task-oriented tutorial dialogue. Proceedings of the International Conference on Artificial Intelligence in Education, 365–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>E Y Ha</author>
<author>R Phillips</author>
<author>M Vouk Wallis</author>
<author>J C Lester</author>
</authors>
<title>Dialogue act modeling in a complex task-oriented domain.</title>
<date>2010</date>
<booktitle>Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>297--305</pages>
<marker>Boyer, Ha, Phillips, Wallis, Lester, 2010</marker>
<rawString>K.E. Boyer, E.Y. Ha, R. Phillips, M.D. Wallis, M. Vouk, and J.C. Lester. 2010. Dialogue act modeling in a complex task-oriented domain. Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 297-305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>R Phillips</author>
<author>E Y Ha</author>
<author>M A Vouk Wallis</author>
<author>J C Lester</author>
</authors>
<title>Modeling dialogue structure with adjacency pair analysis and hidden Markov models.</title>
<date>2009</date>
<booktitle>Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Short Papers,</booktitle>
<pages>49--52</pages>
<contexts>
<context position="19597" citStr="Boyer et al., 2009" startWordPosition="2976" endWordPosition="2979">usion. 1194 Figure 2. Other facial expressions from the corpus 5 Models The goal of the modeling experiment was to determine whether the addition of confusion-related facial expression features significantly boosts dialogue act classification accuracy for student utterances. 5.1 Features We take a vector-based approach, in which the features consist of the following: Utterance Features • Dialogue act features: Manually annotated dialogue act for the past three utterances. These features include tutor dialogue acts, annotated with a scheme analogous to that used to annotate student utterances (Boyer et al., 2009). • Speaker: Speaker for past three utterances • Lexical features: Word unigrams • Syntactic features: Top-most syntactic node and its first two children Task-based Features • Subtask: Hierarchical subtask structure for past three task actions (semantic programming actions taken by student) • Correctness: Correctness of past three task actions taken by student • Preceded by task: Indicator for whether the most recent task action immediately preceded the target utterance, or whether it was immediately preceded by the last dialogue move Facial Expression Features • AU4_1sec: Indicator for the di</context>
</contexts>
<marker>Boyer, Phillips, Ha, Wallis, Lester, 2009</marker>
<rawString>K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2009. Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Short Papers, 49-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>R Phillips</author>
<author>E Y Ha</author>
<author>M A Vouk Wallis</author>
<author>J C Lester</author>
</authors>
<title>Leveraging hidden dialogue state to select tutorial moves.</title>
<date>2010</date>
<booktitle>Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>66--73</pages>
<marker>Boyer, Phillips, Ha, Wallis, Lester, 2010</marker>
<rawString>K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2010. Leveraging hidden dialogue state to select tutorial moves. Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, 66-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Calvo</author>
<author>S D’Mello</author>
</authors>
<title>Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications.</title>
<date>2010</date>
<journal>IEEE Transactions on Affective Computing,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>18--37</pages>
<marker>Calvo, D’Mello, 2010</marker>
<rawString>R.A. Calvo and S. D’Mello. 2010. Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications. IEEE Transactions on Affective Computing, 1(1): 18-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cavazza</author>
<author>R S D L C‡mara</author>
<author>M Turunen</author>
<author>J Gil</author>
<author>J Hakulinen</author>
<author>N Crook</author>
</authors>
<title>How was your day? An affective companion ECA prototype.</title>
<date>2010</date>
<booktitle>Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>277--280</pages>
<marker>Cavazza, C‡mara, Turunen, Gil, Hakulinen, Crook, 2010</marker>
<rawString>M. Cavazza, R.S.D.L. C‡mara, M. Turunen, J. Gil, J. Hakulinen, N. Crook, et al. 2010. How was your day? An affective companion ECA prototype. Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 277-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Cavicchio</author>
</authors>
<title>The modulation of cooperation and emotion in dialogue: the REC Corpus.</title>
<date>2009</date>
<booktitle>Proceedings of the ACL-IJCNLP 2009 Student Research Workshop,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="3645" citStr="Cavicchio, 2009" startWordPosition="531" endWordPosition="532">matic user dialogue act classification. Outside of dialogue, sentiment analysis within discourse is an active area of research (L—pez-C—zar et al., 2010), but it is generally lim1190 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1190–1199, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ited to modeling textual features and not multimodal expressions of emotion such as facial actions. Such multimodal expressions have only just begun to be explored within corpus-based dialogue research (Calvo &amp; D&apos;Mello, 2010; Cavicchio, 2009). This paper presents a novel affect-enriched dialogue act classification approach that leverages knowledge of users’ facial expressions during computer-mediated textual human-human dialogue. Intuitively, the user’s affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user may be more likely to ask a question). We focus specifically on occurrences of students’ confusion-related facial actions during taskoriented tutorial dialogue. Confusion was selected as the focus of this work for several reasons. First, confusio</context>
<context position="7881" citStr="Cavicchio, 2009" startWordPosition="1157" endWordPosition="1158">2010). In contrast to that spoken dialogue research, the work in this paper is situated within textual dialogue, a widely used modality of communication for which a deeper understanding of user affect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial actions, electrocardiograms, skin conductance, and posture sensors (Calvo &amp; D&apos;Mello, 2010). A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009). Like that work, the current project utilizes facial action tagging, for 1191 which promising automatic technologies exist (Bartlett et al., 2006; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009). However, we leverage the recognized expressions of emotion for the task of dialogue act classification. 2.3 Categorizing Emotions within Dialogue and Discourse Sets of emotion taxonomies for discourse and dialogue are often application-specific, for example, focusing on the frustration of users who are interacting with a spoken dialogue system (L—pezC—zar et al., 2010), or on uncertain</context>
</contexts>
<marker>Cavicchio, 2009</marker>
<rawString>F. Cavicchio. 2009. The modulation of cooperation and emotion in dialogue: the REC Corpus. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, 43-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Cohn</author>
<author>L I Reed</author>
<author>Z Ambadar</author>
<author>J Xiao</author>
<author>T Moriyama</author>
</authors>
<date>2004</date>
<booktitle>Automatic Analysis and Recognition of Brow Actions and Head Motion in Spontaneous Facial Behavior. IEEE International Conference on Systems, Man and Cybernetics,</booktitle>
<pages>610--616</pages>
<contexts>
<context position="5016" citStr="Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004" startWordPosition="725" endWordPosition="731">e, Cooper-Pye, &amp; Whitten, 2005). Second, while identifying the “ground truth” of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular facial action units and confusion during learning (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). Finally, automatic facial action recognition technologies are developing rapidly, and confusion-related facial action events are among those that can be reliably recognized automatically (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems. 2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classification within dialogue systems, it has been an active area of research for some time. Early work on automatic dialogue act classification modeled discourse structure with hidden Markov models, experimenting with lexical and prosodic features, and applying the dialogue act model as a constraint t</context>
<context position="18385" citStr="Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004" startWordPosition="2793" endWordPosition="2799">iability was calculated at each level of granularity (Table 3). Windows in which both annotators agreed that no facial action event was present were tagged by default as neutral. Figure 1 illustrates facial expressions that display facial Action Unit 4. Table 3. Kappa values for inter-annotator agreement on facial action events Granularity 1/4 sec 1/2 sec 3/4 sec 1 sec Presence of AU4 84 .87 .86 .86 (Brow Lowerer) Figure 1. Facial expressions displaying AU4 (Brow Lowerer) Despite the fact that promising automatic approaches exist to identifying many facial action units (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009), manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emotions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other spontaneous student facial expressions that differ from those associated with confusion. </context>
</contexts>
<marker>Cohn, Reed, Ambadar, Xiao, Moriyama, 2004</marker>
<rawString>J.F. Cohn, L.I. Reed, Z. Ambadar, J. Xiao, and T. Moriyama. 2004. Automatic Analysis and Recognition of Brow Actions and Head Motion in Spontaneous Facial Behavior. IEEE International Conference on Systems, Man and Cybernetics, 610-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Craig</author>
<author>S D’Mello</author>
<author>A Witherspoon</author>
<author>J Sullins</author>
<author>A C Graesser</author>
</authors>
<title>Emotions during learning: The first steps toward an affect sensitive intelligent tutoring system. In</title>
<date>2004</date>
<booktitle>E-learn 2004: World conference on Elearning in Corporate, Government, Healthcare, &amp; Higher Education,</booktitle>
<pages>241--250</pages>
<marker>Craig, D’Mello, Witherspoon, Sullins, Graesser, 2004</marker>
<rawString>S.D. Craig, S. D’Mello, A. Witherspoon, J. Sullins, and A.C. Graesser. 2004. Emotions during learning: The first steps toward an affect sensitive intelligent tutoring system. In J. Nall and R. Robson (Eds.), E-learn 2004: World conference on Elearning in Corporate, Government, Healthcare, &amp; Higher Education, 241-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>S Bandyopadhyay</author>
</authors>
<title>Word to sentence level emotion tagging for Bengali blogs.</title>
<date>2009</date>
<booktitle>Proceedings of the ACL-IJCNLP Conference, Short Papers,</booktitle>
<pages>149--152</pages>
<contexts>
<context position="9273" citStr="Das &amp; Bandyopadhyay, 2009" startWordPosition="1362" endWordPosition="1365"> not application-specific; for example, Ekman’s Facial Action Coding System (FACS) has been widely used as a rigorous technique for coding facial movements based on human facial anatomy (Ekman &amp; Friesen, 1978). Within this framework, facial movements are categorized into facial action units, which represent discrete movements of muscle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman’s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion expressed within blog text (Das &amp; Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell’s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not frequently experience Ekman’s basic emotions of happiness, sadness, anger, fear, surprise, and disgust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo &amp; D&apos;Mello, 2010). Our work leverages Ekman’s facial tagging scheme to identify a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D&apos;Mello, Witherspoon, Sullins, </context>
</contexts>
<marker>Das, Bandyopadhyay, 2009</marker>
<rawString>D. Das and S. Bandyopadhyay. 2009. Word to sentence level emotion tagging for Bengali blogs. Proceedings of the ACL-IJCNLP Conference, Short Papers, 149-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
<author>V Ng</author>
</authors>
<title>Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification.</title>
<date>2009</date>
<booktitle>Proceedings of the 46th Annual Meeting of the ACL and the 4th IJCNLP,</booktitle>
<pages>701--709</pages>
<marker>Dasgupta, Ng, 2009</marker>
<rawString>S. Dasgupta and V. Ng. 2009. Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification. Proceedings of the 46th Annual Meeting of the ACL and the 4th IJCNLP, 701-709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Di Eugenio</author>
<author>Z Xie</author>
<author>R Serafin</author>
</authors>
<date>2010</date>
<booktitle>Dialogue Act Classification, Higher Order Dialogue Structure, and Instance-Based Learning. Dialogue &amp; Discourse,</booktitle>
<volume>1</volume>
<issue>2</issue>
<pages>1--24</pages>
<marker>Di Eugenio, Xie, Serafin, 2010</marker>
<rawString>B. Di Eugenio, Z. Xie, and R. Serafin. 2010. Dialogue Act Classification, Higher Order Dialogue Structure, and Instance-Based Learning. Dialogue &amp; Discourse, 1(2): 1-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dzikovska</author>
<author>J D Moore</author>
<author>N Steinhauser</author>
<author>G Campbell</author>
</authors>
<title>The impact of interpretation problems on tutorial dialogue.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Short Papers,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="10873" citStr="Dzikovska, Moore, Steinhauser, &amp; Campbell, 2010" startWordPosition="1593" endWordPosition="1598">tate in which students’ existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten, 2005). Students may express such confusion within dialogue as uncertainty, to which human tutors often adapt in a context-dependent fashion (Forbes-Riley et al., 2007). Moreover, implementing adaptations to student uncertainty within a dialogue system can improve the effectiveness of the system (Forbes-Riley et al., 2009). For tutorial dialogue, the importance of understanding student utterances is paramount for a system to positively impact student learning (Dzikovska, Moore, Steinhauser, &amp; Campbell, 2010). The importance of frustration as a cognitive-affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act classification of student utterances. This paper explores the use of facial expression features in this way. 3 Task-Oriented Dialogue Corpus The corpus was collected during a textual humanhuman tutorial dialogue study in the domain of introductory computer science (Boyer, Phillips, et al., 2010). Students solved an introductory computer programming problem and carried on textual dialogue with tutors, who view</context>
</contexts>
<marker>Dzikovska, Moore, Steinhauser, Campbell, 2010</marker>
<rawString>M. Dzikovska, J.D. Moore, N. Steinhauser, and G. Campbell. 2010. The impact of interpretation problems on tutorial dialogue. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Short Papers, 43-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D’Mello</author>
<author>S D Craig</author>
<author>J Sullins</author>
<author>A C Graesser</author>
</authors>
<title>Predicting Affective States expressed through an Emote-Aloud Procedure from AutoTutor’s Mixed- Initiative Dialogue.</title>
<date>2006</date>
<journal>International Journal of Artificial Intelligence in Education,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>3--28</pages>
<marker>D’Mello, Craig, Sullins, Graesser, 2006</marker>
<rawString>S. D’Mello, S.D. Craig, J. Sullins, and A.C. Graesser. 2006. Predicting Affective States expressed through an Emote-Aloud Procedure from AutoTutor’s Mixed- Initiative Dialogue. International Journal of Artificial Intelligence in Education, 16(1): 3-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
</authors>
<title>Basic Emotions. In</title>
<date>1999</date>
<booktitle>Handbook of Cognition and Emotion.</booktitle>
<editor>Power (Eds.),</editor>
<publisher>Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="9165" citStr="Ekman, 1999" startWordPosition="1346" endWordPosition="1347">taru, Litman, &amp; Tetreault, 2007). In contrast, the most widely utilized emotion frameworks are not application-specific; for example, Ekman’s Facial Action Coding System (FACS) has been widely used as a rigorous technique for coding facial movements based on human facial anatomy (Ekman &amp; Friesen, 1978). Within this framework, facial movements are categorized into facial action units, which represent discrete movements of muscle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman’s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion expressed within blog text (Das &amp; Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell’s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not frequently experience Ekman’s basic emotions of happiness, sadness, anger, fear, surprise, and disgust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo &amp; D&apos;Mello, 2010). Our work leverages Ekman’s facial tagging scheme to identify a particular facial action unit, Act</context>
</contexts>
<marker>Ekman, 1999</marker>
<rawString>P. Ekman. 1999. Basic Emotions. In T. Dalgleish and M. J. Power (Eds.), Handbook of Cognition and Emotion. New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>W V Friesen</author>
</authors>
<title>Facial Action Coding System.</title>
<date>1978</date>
<publisher>Consulting Psychologists Press.</publisher>
<location>Palo Alto, CA:</location>
<contexts>
<context position="8856" citStr="Ekman &amp; Friesen, 1978" startWordPosition="1302" endWordPosition="1305">ogue and Discourse Sets of emotion taxonomies for discourse and dialogue are often application-specific, for example, focusing on the frustration of users who are interacting with a spoken dialogue system (L—pezC—zar et al., 2010), or on uncertainty expressed by students while interacting with a tutor (ForbesRiley, Rotaru, Litman, &amp; Tetreault, 2007). In contrast, the most widely utilized emotion frameworks are not application-specific; for example, Ekman’s Facial Action Coding System (FACS) has been widely used as a rigorous technique for coding facial movements based on human facial anatomy (Ekman &amp; Friesen, 1978). Within this framework, facial movements are categorized into facial action units, which represent discrete movements of muscle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman’s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion expressed within blog text (Das &amp; Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell’s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not frequently experience Ekma</context>
</contexts>
<marker>Ekman, Friesen, 1978</marker>
<rawString>P. Ekman, W.V. Friesen. 1978. Facial Action Coding System. Palo Alto, CA: Consulting Psychologists Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>W V Friesen</author>
<author>J C Hager</author>
</authors>
<title>Facial Action Coding System: Investigator’s Guide. Salt Lake City, USA: A Human Face.</title>
<date>2002</date>
<contexts>
<context position="15994" citStr="Ekman, Friesen, &amp; Hager, 2002" startWordPosition="2411" endWordPosition="2415">t syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper &amp; Bird, 2004). Our prior work has shown that these lexical and syntactic features are highly predictive of dialogue acts during task-oriented tutorial dialogue (Boyer, Ha et al. 2010). 1193 4 Facial Action Tagging An annotator who was certified in the Facial Action Coding System (FACS) (Ekman, Friesen, &amp; Hager, 2002) tagged the video corpus consisting of fourteen dialogues. The FACS certification process requires annotators to pass a test designed to analyze their agreement with reference coders on a set of spontaneous facial expressions (Ekman &amp; Rosenberg, 2005). This annotator viewed the videos continuously and paused the playback whenever notable facial displays of Action Unit 4 (AU4: Brow Lowerer) were seen. This action unit was chosen for this study based on its correlations with confusion in prior research (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser,</context>
</contexts>
<marker>Ekman, Friesen, Hager, 2002</marker>
<rawString>P. Ekman, W.V. Friesen, and J.C. Hager. 2002. Facial Action Coding System: Investigator’s Guide. Salt Lake City, USA: A Human Face.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>E L Rosenberg</author>
</authors>
<title>What the Face Reveals:</title>
<date>2005</date>
<booktitle>Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS) (2nd ed.).</booktitle>
<publisher>Oxford University Press.</publisher>
<location>New York:</location>
<contexts>
<context position="16246" citStr="Ekman &amp; Rosenberg, 2005" startWordPosition="2450" endWordPosition="2453">rter Stemmer (Porter, 1980) in the NLTK package (Loper &amp; Bird, 2004). Our prior work has shown that these lexical and syntactic features are highly predictive of dialogue acts during task-oriented tutorial dialogue (Boyer, Ha et al. 2010). 1193 4 Facial Action Tagging An annotator who was certified in the Facial Action Coding System (FACS) (Ekman, Friesen, &amp; Hager, 2002) tagged the video corpus consisting of fourteen dialogues. The FACS certification process requires annotators to pass a test designed to analyze their agreement with reference coders on a set of spontaneous facial expressions (Ekman &amp; Rosenberg, 2005). This annotator viewed the videos continuously and paused the playback whenever notable facial displays of Action Unit 4 (AU4: Brow Lowerer) were seen. This action unit was chosen for this study based on its correlations with confusion in prior research (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). To establish reliability of the annotation, a second FACS-certified annotator independently annotated 36% of the video corpus (5 of 14 dialogues), chosen randomly after stratification by gender and tutor. This annotator f</context>
</contexts>
<marker>Ekman, Rosenberg, 2005</marker>
<rawString>P. Ekman and E.L. Rosenberg (Eds.). 2005. What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS) (2nd ed.). New York: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>M Rotaru</author>
<author>D J Litman</author>
<author>J Tetreault</author>
</authors>
<title>Exploring affect-context dependencies for adaptive system development.</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL HLT), Short Papers,</booktitle>
<pages>41--44</pages>
<contexts>
<context position="10529" citStr="Forbes-Riley et al., 2007" startWordPosition="1545" endWordPosition="1548">ig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). 2.4 Importance of Confusion in Tutorial Dialogue Among the affective states that students experience during tutorial dialogue, confusion is prevalent, and its implications for student learning are significant. Confusion is associated with cognitive disequilibrium, a state in which students’ existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten, 2005). Students may express such confusion within dialogue as uncertainty, to which human tutors often adapt in a context-dependent fashion (Forbes-Riley et al., 2007). Moreover, implementing adaptations to student uncertainty within a dialogue system can improve the effectiveness of the system (Forbes-Riley et al., 2009). For tutorial dialogue, the importance of understanding student utterances is paramount for a system to positively impact student learning (Dzikovska, Moore, Steinhauser, &amp; Campbell, 2010). The importance of frustration as a cognitive-affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act classification of student utterances. This paper explores the use of</context>
</contexts>
<marker>Forbes-Riley, Rotaru, Litman, Tetreault, 2007</marker>
<rawString>K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2007. Exploring affect-context dependencies for adaptive system development. The Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL HLT), Short Papers, 41-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>M Rotaru</author>
<author>D J Litman</author>
<author>J Tetreault</author>
</authors>
<title>Adapting to student uncertainty improves tutoring dialogues.</title>
<date>2009</date>
<booktitle>Proceedings of the 14th International Conference on Artificial Intelligence in Education (AIED),</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2919" citStr="Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2009" startWordPosition="422" endWordPosition="427">vital role (Knapp &amp; Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, &amp; Fernandez-Dols, 2003; Schmidt &amp; Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dialogue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spoken utterances to aid in online dialogue act classification (Sridhar, Bangalore, &amp; Narayanan, 2009). Other research on emotion in dialogue has involved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2009; L—pez-C—zar, Silovsky, &amp; Griol, 2010), but this work has not explored leveraging affect information for automatic user dialogue act classification. Outside of dialogue, sentiment analysis within discourse is an active area of research (L—pez-C—zar et al., 2010), but it is generally lim1190 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1190–1199, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ited to modeling textual features and not multimodal expressions of emotion such as facial actions. Such multimodal </context>
<context position="10685" citStr="Forbes-Riley et al., 2009" startWordPosition="1568" endWordPosition="1571">during tutorial dialogue, confusion is prevalent, and its implications for student learning are significant. Confusion is associated with cognitive disequilibrium, a state in which students’ existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten, 2005). Students may express such confusion within dialogue as uncertainty, to which human tutors often adapt in a context-dependent fashion (Forbes-Riley et al., 2007). Moreover, implementing adaptations to student uncertainty within a dialogue system can improve the effectiveness of the system (Forbes-Riley et al., 2009). For tutorial dialogue, the importance of understanding student utterances is paramount for a system to positively impact student learning (Dzikovska, Moore, Steinhauser, &amp; Campbell, 2010). The importance of frustration as a cognitive-affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act classification of student utterances. This paper explores the use of facial expression features in this way. 3 Task-Oriented Dialogue Corpus The corpus was collected during a textual humanhuman tutorial dialogue study in the</context>
</contexts>
<marker>Forbes-Riley, Rotaru, Litman, Tetreault, 2009</marker>
<rawString>K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2009. Adapting to student uncertainty improves tutoring dialogues. Proceedings of the 14th International Conference on Artificial Intelligence in Education (AIED), 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Graesser</author>
<author>S Lu</author>
<author>B Olde</author>
<author>E Cooper-Pye</author>
<author>S Whitten</author>
</authors>
<title>Question asking and eye tracking during cognitive disequilibrium: comprehending illustrated texts on devices when the devices break down.</title>
<date>2005</date>
<journal>Memory &amp; Cognition,</journal>
<volume>33</volume>
<issue>7</issue>
<pages>1235--1247</pages>
<contexts>
<context position="4403" citStr="Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten, 2005" startWordPosition="638" endWordPosition="644">users’ facial expressions during computer-mediated textual human-human dialogue. Intuitively, the user’s affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user may be more likely to ask a question). We focus specifically on occurrences of students’ confusion-related facial actions during taskoriented tutorial dialogue. Confusion was selected as the focus of this work for several reasons. First, confusion is known to be prevalent within tutoring, and its implications for student learning are thought to run deep (Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten, 2005). Second, while identifying the “ground truth” of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular facial action units and confusion during learning (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). Finally, automatic facial action recognition technologies are developing rapidly, and confusion-related facial action events are among those that can be reliably recognized automatically (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; M</context>
<context position="10366" citStr="Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten, 2005" startWordPosition="1517" endWordPosition="1523"> identify a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). 2.4 Importance of Confusion in Tutorial Dialogue Among the affective states that students experience during tutorial dialogue, confusion is prevalent, and its implications for student learning are significant. Confusion is associated with cognitive disequilibrium, a state in which students’ existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, &amp; Whitten, 2005). Students may express such confusion within dialogue as uncertainty, to which human tutors often adapt in a context-dependent fashion (Forbes-Riley et al., 2007). Moreover, implementing adaptations to student uncertainty within a dialogue system can improve the effectiveness of the system (Forbes-Riley et al., 2009). For tutorial dialogue, the importance of understanding student utterances is paramount for a system to positively impact student learning (Dzikovska, Moore, Steinhauser, &amp; Campbell, 2010). The importance of frustration as a cognitive-affective state during learning suggests that</context>
</contexts>
<marker>Graesser, Lu, Olde, Cooper-Pye, Whitten, 2005</marker>
<rawString>A.C. Graesser, S. Lu, B. Olde, E. Cooper-Pye, and S. Whitten. 2005. Question asking and eye tracking during cognitive disequilibrium: comprehending illustrated texts on devices when the devices break down. Memory &amp; Cognition, 33(7): 1235-1247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greene</author>
<author>P Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Annual Conference of the North American Chapter of the ACL and Human Language Technologies (NAACL HLT),</booktitle>
<pages>503--511</pages>
<marker>Greene, Resnik, 2009</marker>
<rawString>S. Greene and P. Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. Proceedings of the 2009 Annual Conference of the North American Chapter of the ACL and Human Language Technologies (NAACL HLT), 503-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>10--18</pages>
<contexts>
<context position="20792" citStr="Hall et al., 2009" startWordPosition="3168" endWordPosition="3171">c: Indicator for the display of the brow lowerer within 1 second prior to this utterance being sent, for the most recent three utterances • AU4_5sec: Indicator for the display of the brow lowerer within 5 seconds prior to this utterance being sent, for the most recent three utterances • AU4 _r10sec: Indicator for the display of the bow lowerer within 10 seconds prior to this utterance being sent, for the most recent three utterances 5.2 Modeling Approach A logistic regression approach was used to classify the dialogue acts based on the above feature vectors. The Weka machine learning toolkit (Hall et al., 2009) was used to learn the models and to first perform feature selection in a best-first search. Logistic regression is a generalized maximum likelihood model that discriminates between pairs of output values by calculating a feature weight vector over the predictors. The goal of this work is to explore the utility of confusion-related facial features in the context of particular dialogue act types. For this reason, a specialized classifier was learned by dialogue act. 5.3 Classification Results The classification accuracy and kappa for each specialized classifier is displayed in Table 4. Note tha</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1): 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>S Kobayashi</author>
<author>T Tokunaga</author>
</authors>
<title>Incorporating extra-linguistic information into reference resolution in collaborative task dialogue.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1259--1267</pages>
<marker>Iida, Kobayashi, Tokunaga, 2010</marker>
<rawString>R. Iida, S. Kobayashi, and T. Tokunaga. 2010. Incorporating extra-linguistic information into reference resolution in collaborative task dialogue. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, 1259-1267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Knapp</author>
<author>J A Hall</author>
</authors>
<date>2006</date>
<booktitle>Nonverbal Communication in Human Interaction (6th ed.).</booktitle>
<publisher>Wadsworth/Thomson Learning.</publisher>
<location>Belmont, CA:</location>
<contexts>
<context position="2302" citStr="Knapp &amp; Hall, 2006" startWordPosition="332" endWordPosition="335">ch, and a variety of approaches, including sequential models (Stolcke et al., 2000), vector-based models (Sridhar, Bangalore, &amp; Narayanan, 2009), and most recently, featureenhanced latent semantic analysis (Di Eugenio, Xie, &amp; Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users’ expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp &amp; Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, &amp; Fernandez-Dols, 2003; Schmidt &amp; Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dialogue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spoken utterances to aid in online dialogue act classification (Sridhar, Bangalore, &amp; Narayanan, 2009). Other research on emotion in dialogue has involved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, </context>
</contexts>
<marker>Knapp, Hall, 2006</marker>
<rawString>M.L. Knapp and J.A. Hall. 2006. Nonverbal Communication in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S S Narayanan</author>
</authors>
<title>Toward detecting emotions in spoken dialogs.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>293--303</pages>
<contexts>
<context position="7132" citStr="Lee &amp; Narayanan, 2005" startWordPosition="1044" endWordPosition="1047">imodal affective displays, specifically facial expressions, as features available to an affect-enriched dialogue act classification model. 2.2 Detecting Emotions in Dialogue Detecting emotional states during spoken dialogue is an active area of research, much of which focuses on detecting frustration so that a user can be automatically transferred to a human dialogue agent (L—pez-C—zar et al., 2010). Research on spoken dialogue has leveraged lexical features along with discourse cues and acoustic information to classify user emotion, sometimes at a coarse grain along a positive/negative axis (Lee &amp; Narayanan, 2005). Recent work on an affective companion agent has examined user emotion classification within conversational speech (Cavazza et al., 2010). In contrast to that spoken dialogue research, the work in this paper is situated within textual dialogue, a widely used modality of communication for which a deeper understanding of user affect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial actions, electrocardiograms, skin conductance, and posture sensors (Calvo &amp; D&apos;</context>
</contexts>
<marker>Lee, Narayanan, 2005</marker>
<rawString>C.M. Lee, S.S. Narayanan. 2005. Toward detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2): 293-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L—pez-C—zar</author>
<author>J Silovsky</author>
<author>D Griol</author>
</authors>
<title>F2– New Technique for Recognition of User Emotional States in Spoken Dialogue Systems.</title>
<date>2010</date>
<booktitle>Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>281--288</pages>
<marker>L—pez-C—zar, Silovsky, Griol, 2010</marker>
<rawString>R. L—pez-C—zar, J. Silovsky, and D. Griol. 2010. F2– New Technique for Recognition of User Emotional States in Spoken Dialogue Systems. Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 281-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T McDaniel</author>
<author>S D’Mello</author>
<author>B G King</author>
<author>P Chipman</author>
<author>K Tapp</author>
<author>A C Graesser</author>
</authors>
<title>Facial Features for Affective State Detection in Learning Environments.</title>
<date>2007</date>
<booktitle>Proceedings of the 29th Annual Cognitive Science Society,</booktitle>
<pages>467--472</pages>
<marker>McDaniel, D’Mello, King, Chipman, Tapp, Graesser, 2007</marker>
<rawString>B.T. McDaniel, S. D’Mello, B.G. King, P. Chipman, K. Tapp, and A.C. Graesser. 2007. Facial Features for Affective State Detection in Learning Environments. Proceedings of the 29th Annual Cognitive Science Society, 467-472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Hand and mind: What gestures reveal about thought. Chicago:</title>
<date>1992</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="2317" citStr="McNeill, 1992" startWordPosition="336" endWordPosition="337"> approaches, including sequential models (Stolcke et al., 2000), vector-based models (Sridhar, Bangalore, &amp; Narayanan, 2009), and most recently, featureenhanced latent semantic analysis (Di Eugenio, Xie, &amp; Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users’ expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp &amp; Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, &amp; Fernandez-Dols, 2003; Schmidt &amp; Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dialogue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spoken utterances to aid in online dialogue act classification (Sridhar, Bangalore, &amp; Narayanan, 2009). Other research on emotion in dialogue has involved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 20</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>D. McNeill. 1992. Hand and mind: What gestures reveal about thought. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mehrabian</author>
</authors>
<title>Nonverbal Communication.</title>
<date>2007</date>
<location>New Brunswick, NJ: Aldine Transaction.</location>
<contexts>
<context position="2334" citStr="Mehrabian, 2007" startWordPosition="338" endWordPosition="339">cluding sequential models (Stolcke et al., 2000), vector-based models (Sridhar, Bangalore, &amp; Narayanan, 2009), and most recently, featureenhanced latent semantic analysis (Di Eugenio, Xie, &amp; Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users’ expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp &amp; Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, &amp; Fernandez-Dols, 2003; Schmidt &amp; Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dialogue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spoken utterances to aid in online dialogue act classification (Sridhar, Bangalore, &amp; Narayanan, 2009). Other research on emotion in dialogue has involved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2009; L—pez-C—zar, </context>
</contexts>
<marker>Mehrabian, 2007</marker>
<rawString>A. Mehrabian. 2007. Nonverbal Communication. New Brunswick, NJ: Aldine Transaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nguyen</author>
</authors>
<title>Mood patterns and affective lexicon access in weblogs.</title>
<date>2010</date>
<booktitle>Proceedings of the ACL 2010 Student Research Workshop,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="9313" citStr="Nguyen, 2010" startWordPosition="1370" endWordPosition="1371">l Action Coding System (FACS) has been widely used as a rigorous technique for coding facial movements based on human facial anatomy (Ekman &amp; Friesen, 1978). Within this framework, facial movements are categorized into facial action units, which represent discrete movements of muscle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman’s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion expressed within blog text (Das &amp; Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell’s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not frequently experience Ekman’s basic emotions of happiness, sadness, anger, fear, surprise, and disgust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo &amp; D&apos;Mello, 2010). Our work leverages Ekman’s facial tagging scheme to identify a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullin</context>
</contexts>
<marker>Nguyen, 2010</marker>
<rawString>T. Nguyen. 2010. Mood patterns and affective lexicon access in weblogs. Proceedings of the ACL 2010 Student Research Workshop, 43-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pantic</author>
<author>M S Bartlett</author>
</authors>
<title>Machine Analysis of Facial Expressions.</title>
<date>2007</date>
<booktitle>In K. Delac and M. Grgic (Eds.), Face Recognition,</booktitle>
<pages>377--416</pages>
<location>Vienna, Austria:</location>
<contexts>
<context position="5041" citStr="Pantic &amp; Bartlett, 2007" startWordPosition="732" endWordPosition="735">e identifying the “ground truth” of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular facial action units and confusion during learning (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). Finally, automatic facial action recognition technologies are developing rapidly, and confusion-related facial action events are among those that can be reliably recognized automatically (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems. 2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classification within dialogue systems, it has been an active area of research for some time. Early work on automatic dialogue act classification modeled discourse structure with hidden Markov models, experimenting with lexical and prosodic features, and applying the dialogue act model as a constraint to aid in automatic speech</context>
<context position="8052" citStr="Pantic &amp; Bartlett, 2007" startWordPosition="1180" endWordPosition="1183"> deeper understanding of user affect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial actions, electrocardiograms, skin conductance, and posture sensors (Calvo &amp; D&apos;Mello, 2010). A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009). Like that work, the current project utilizes facial action tagging, for 1191 which promising automatic technologies exist (Bartlett et al., 2006; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009). However, we leverage the recognized expressions of emotion for the task of dialogue act classification. 2.3 Categorizing Emotions within Dialogue and Discourse Sets of emotion taxonomies for discourse and dialogue are often application-specific, for example, focusing on the frustration of users who are interacting with a spoken dialogue system (L—pezC—zar et al., 2010), or on uncertainty expressed by students while interacting with a tutor (ForbesRiley, Rotaru, Litman, &amp; Tetreault, 2007). In contrast, the most widely utilized emotion frameworks are not </context>
<context position="18410" citStr="Pantic &amp; Bartlett, 2007" startWordPosition="2800" endWordPosition="2803">ularity (Table 3). Windows in which both annotators agreed that no facial action event was present were tagged by default as neutral. Figure 1 illustrates facial expressions that display facial Action Unit 4. Table 3. Kappa values for inter-annotator agreement on facial action events Granularity 1/4 sec 1/2 sec 3/4 sec 1 sec Presence of AU4 84 .87 .86 .86 (Brow Lowerer) Figure 1. Facial expressions displaying AU4 (Brow Lowerer) Despite the fact that promising automatic approaches exist to identifying many facial action units (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009), manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emotions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other spontaneous student facial expressions that differ from those associated with confusion. 1194 Figure 2. Other faci</context>
</contexts>
<marker>Pantic, Bartlett, 2007</marker>
<rawString>M. Pantic and M.S. Bartlett. 2007. Machine Analysis of Facial Expressions. In K. Delac and M. Grgic (Eds.), Face Recognition, 377-416. Vienna, Austria: I-Tech Education and Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Russell</author>
</authors>
<title>Core affect and the psychological construction of emotion.</title>
<date>2003</date>
<journal>Psychological Review,</journal>
<volume>110</volume>
<issue>1</issue>
<pages>145--172</pages>
<contexts>
<context position="9366" citStr="Russell, 2003" startWordPosition="1377" endWordPosition="1378">s a rigorous technique for coding facial movements based on human facial anatomy (Ekman &amp; Friesen, 1978). Within this framework, facial movements are categorized into facial action units, which represent discrete movements of muscle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman’s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion expressed within blog text (Das &amp; Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell’s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not frequently experience Ekman’s basic emotions of happiness, sadness, anger, fear, surprise, and disgust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo &amp; D&apos;Mello, 2010). Our work leverages Ekman’s facial tagging scheme to identify a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). 2.4 Impo</context>
</contexts>
<marker>Russell, 2003</marker>
<rawString>J.A. Russell. 2003. Core affect and the psychological construction of emotion. Psychological Review, 110(1): 145-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Russell</author>
<author>J A Bachorowski</author>
<author>J M FernandezDols</author>
</authors>
<title>Facial and vocal expressions of emotion.</title>
<date>2003</date>
<journal>Annual Review of Psychology,</journal>
<volume>54</volume>
<pages>329--49</pages>
<marker>Russell, Bachorowski, FernandezDols, 2003</marker>
<rawString>J.A. Russell, J.A. Bachorowski, and J.M. FernandezDols. 2003. Facial and vocal expressions of emotion. Annual Review of Psychology, 54, 329-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Schmidt</author>
<author>J F Cohn</author>
</authors>
<title>Human Facial Expressions as Adaptations: Evolutionary Questions in Facial Expression Research.</title>
<date>2001</date>
<journal>Am J Phys Anthropol,</journal>
<volume>33</volume>
<pages>3--24</pages>
<contexts>
<context position="2403" citStr="Schmidt &amp; Cohn, 2001" startWordPosition="345" endWordPosition="348">models (Sridhar, Bangalore, &amp; Narayanan, 2009), and most recently, featureenhanced latent semantic analysis (Di Eugenio, Xie, &amp; Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users’ expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp &amp; Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, &amp; Fernandez-Dols, 2003; Schmidt &amp; Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dialogue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spoken utterances to aid in online dialogue act classification (Sridhar, Bangalore, &amp; Narayanan, 2009). Other research on emotion in dialogue has involved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, &amp; Tetreault, 2009; L—pez-C—zar, Silovsky, &amp; Griol, 2010), but this work has not explored leveraging a</context>
</contexts>
<marker>Schmidt, Cohn, 2001</marker>
<rawString>K.L. Schmidt and J.F. Cohn. 2001. Human Facial Expressions as Adaptations: Evolutionary Questions in Facial Expression Research. Am J Phys Anthropol, 33: 3-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V J Shute</author>
</authors>
<title>Focus on Formative Feedback.</title>
<date>2008</date>
<journal>Review of Educational Research,</journal>
<volume>78</volume>
<issue>1</issue>
<pages>153--189</pages>
<contexts>
<context position="26358" citStr="Shute, 2008" startWordPosition="4045" endWordPosition="4046">he results presented here demonstrate that leveraging knowledge of user affect, in particular of spontaneous facial expressions, may improve the performance of dialogue act classification models. Perhaps most interestingly, displays of confusionrelated facial actions prior to a student dialogue move enabled an affect-enriched classifier to recognize requests for feedback with significantly greater accuracy than a classifier that did not have access to the facial action features. Feedback is known to be a key component of effective tutorial dialogue, through which tutors provide adaptive help (Shute, 2008). Requesting feedback also seems to be an important behavior of students, characteristically engaged in more frequently by women than men, and more frequently by students with lower incoming knowledge than by students with higher incoming knowledge (Boyer, Vouk, &amp; Lester, 2007). 6.3 Limitations The experiments reported here have several notable limitations. First, the time-consuming nature of manual facial action tagging restricted the number of dialogues that could be tagged. Although the highest quality videos were selected for annotation, other medium quality videos would have been sufficie</context>
</contexts>
<marker>Shute, 2008</marker>
<rawString>V.J. Shute. 2008. Focus on Formative Feedback. Review of Educational Research, 78(1): 153-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V K R Sridar</author>
<author>S Bangalore</author>
<author>S S Narayanan</author>
</authors>
<title>Combining lexical, syntactic and prosodic cues for improved online dialog act tagging.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<issue>4</issue>
<pages>407--422</pages>
<publisher>Elsevier Ltd.</publisher>
<marker>Sridar, Bangalore, Narayanan, 2009</marker>
<rawString>V.K.R Sridar, S. Bangalore, and S.S. Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech &amp; Language, 23(4): 407-422. Elsevier Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
</authors>
<title>Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>339--373</pages>
<contexts>
<context position="1767" citStr="Stolcke et al., 2000" startWordPosition="255" endWordPosition="258">t classification. 1 Introduction Dialogue systems aim to engage users in rich, adaptive natural language conversation. For these systems, understanding the role of a user’s utterance in the broader context of the dialogue is a key challenge (Sridhar, Bangalore, &amp; Narayanan, 2009). Central to this endeavor is dialogue act classification, which categorizes the intention behind the user’s move (e.g., asking a question, providing declarative information). Automatic dialogue act classification has been the focus of a large body of research, and a variety of approaches, including sequential models (Stolcke et al., 2000), vector-based models (Sridhar, Bangalore, &amp; Narayanan, 2009), and most recently, featureenhanced latent semantic analysis (Di Eugenio, Xie, &amp; Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users’ expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp &amp; Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, &amp; Fernand</context>
<context position="5676" citStr="Stolcke et al., 2000" startWordPosition="828" endWordPosition="831">, Roisman, &amp; Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems. 2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classification within dialogue systems, it has been an active area of research for some time. Early work on automatic dialogue act classification modeled discourse structure with hidden Markov models, experimenting with lexical and prosodic features, and applying the dialogue act model as a constraint to aid in automatic speech recognition (Stolcke et al., 2000). In contrast to this sequential modeling approach, which is best suited to offline processing, recent work has explored how lexical, syntactic, and prosodic features perform for online dialogue act tagging (when only partial dialogue sequences are available) within a maximum entropy framework (Sridhar, Bangalore, &amp; Narayanan, 2009). A recently proposed alternative approach involves treating dialogue utterances as documents within a latent semantic analysis framework, and applying feature enhancements that incorporate such information as speaker and utterance duration (Di Eugenio et al., 2010)</context>
</contexts>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, et al. 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics, 26(3): 339-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Toprak</author>
<author>N Jakob</author>
<author>I Gurevych</author>
</authors>
<title>Sentence and expression level annotation of opinions in user-generated discourse.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>575--584</pages>
<marker>Toprak, Jakob, Gurevych, 2010</marker>
<rawString>C. Toprak, N. Jakob, and I. Gurevych. 2010. Sentence and expression level annotation of opinions in user-generated discourse. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, 575-584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>399--433</pages>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis. Computational Linguistics, 35(3): 399-433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zeng</author>
<author>M Pantic</author>
<author>G I Roisman</author>
<author>T S Huang</author>
</authors>
<title>A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions.</title>
<date>2009</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>39--58</pages>
<contexts>
<context position="5079" citStr="Zeng, Pantic, Roisman, &amp; Huang, 2009" startWordPosition="736" endWordPosition="741"> truth” of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular facial action units and confusion during learning (Craig, D&apos;Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D&apos;Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007). Finally, automatic facial action recognition technologies are developing rapidly, and confusion-related facial action events are among those that can be reliably recognized automatically (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems. 2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classification within dialogue systems, it has been an active area of research for some time. Early work on automatic dialogue act classification modeled discourse structure with hidden Markov models, experimenting with lexical and prosodic features, and applying the dialogue act model as a constraint to aid in automatic speech recognition (Stolcke et al., 2000). I</context>
<context position="8090" citStr="Zeng, Pantic, Roisman, &amp; Huang, 2009" startWordPosition="1184" endWordPosition="1189">user affect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial actions, electrocardiograms, skin conductance, and posture sensors (Calvo &amp; D&apos;Mello, 2010). A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009). Like that work, the current project utilizes facial action tagging, for 1191 which promising automatic technologies exist (Bartlett et al., 2006; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009). However, we leverage the recognized expressions of emotion for the task of dialogue act classification. 2.3 Categorizing Emotions within Dialogue and Discourse Sets of emotion taxonomies for discourse and dialogue are often application-specific, for example, focusing on the frustration of users who are interacting with a spoken dialogue system (L—pezC—zar et al., 2010), or on uncertainty expressed by students while interacting with a tutor (ForbesRiley, Rotaru, Litman, &amp; Tetreault, 2007). In contrast, the most widely utilized emotion frameworks are not application-specific; for example, Ekm</context>
<context position="18448" citStr="Zeng, Pantic, Roisman, &amp; Huang, 2009" startWordPosition="2804" endWordPosition="2809">s in which both annotators agreed that no facial action event was present were tagged by default as neutral. Figure 1 illustrates facial expressions that display facial Action Unit 4. Table 3. Kappa values for inter-annotator agreement on facial action events Granularity 1/4 sec 1/2 sec 3/4 sec 1 sec Presence of AU4 84 .87 .86 .86 (Brow Lowerer) Figure 1. Facial expressions displaying AU4 (Brow Lowerer) Despite the fact that promising automatic approaches exist to identifying many facial action units (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009), manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emotions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other spontaneous student facial expressions that differ from those associated with confusion. 1194 Figure 2. Other facial expressions from the corpus 5 Model</context>
</contexts>
<marker>Zeng, Pantic, Roisman, Huang, 2009</marker>
<rawString>Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(1): 39-58.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>