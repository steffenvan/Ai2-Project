<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.9949325" genericHeader="abstract">
DISCOVERY PROCEDURES FOR SUBLANGUAGE SELECTIONAL PATTERNS:
INITIAL EXPERIMENTS
</sectionHeader>
<author confidence="0.644575">
Ralph Grishman
</author>
<affiliation confidence="0.7505295">
Computer Science Department
Courant Institute of Mathematical Sciences
</affiliation>
<address confidence="0.5823715">
New York University
New York, NY 10012
</address>
<title confidence="0.591736875">
Lynette Hirschman
Research and Development Division
System Development Corporation — A Burroughs Company
Paoli, PA 19301
Ngo Thanh Nhan
Computer Science Department
Courant Institute of Mathematical Sciences
New York University
</title>
<bodyText confidence="0.990281571428572">
Selectional constraints specify, for a particular domain, the combinations of semantic classes accepta-
ble in subject-verb-object relationships and other syntactic structures. These constraints are important
in blocking incorrect analyses in natural language processing systems. However, these constraints are
domain-specific and hence must be developed anew when a system is ported to a new domain. A
discovery procedure for selectional constraints is therefore essential in enhancing the portability of
such systems.
This paper describes a semi-automated procedure for collecting the co-occurrence patterns front a
sample of texts in a domain, and then using these patterns as the basis for selectional constraints in
analyzing further texts. We discuss some of the difficulties in automating the collection process, and
describe two experiments that measure the completeness of these patterns and their effectiveness
compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional
constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how
these procedures could be combined with a system that queries a domain expert, in order to produce a
more efficient discovery procedure.
</bodyText>
<sectionHeader confidence="0.9882485" genericHeader="introduction">
1 INTRODUCTION:
THE NEED FOR DISCOVERY PROCEDURES
</sectionHeader>
<bodyText confidence="0.9997173">
In order to analyze natural language texts reliably, a
computer system requires a great deal of information
about the syntax of the language, about the structure of
the discourse, and about the subject matter with which
the text deals. Because of the need for detailed know-
ledge of the subject matter, natural language systems at
present are limited to handling texts within very limited
domains of discourse. Once such a system has been
developed, the question of portability naturally arises:
can the system be readily moved to a new domain?
Portability involves two separate issues. The first
issue is whether a large portion of the natural language
system is domain-independent, so that this &amp;quot;core&amp;quot; can be
used in the new application without modification. The
second issue is whether the domain-dependent informa-
tion required by the system can be gathered in a method-
ical and efficient fashion. Our paper addresses the latter
Copyrightl 986 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that
the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy
otherwise, or to republish, requires a fee and/or specific permission.
</bodyText>
<page confidence="0.414914">
0362-613X/86/030205-215$03.00
</page>
<note confidence="0.5936775">
Computational Linguistics, Volume 12, Number 3, July-September 1986 205
Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns
</note>
<bodyText confidence="0.999029">
issue. Specifically, we report on some experiments aimed
at developing a semi-automated procedure for discover-
ing selectional patterns (the local semantic constraints of
language in a particular domain) from the analysis of a
sample of text in that domain.
</bodyText>
<sectionHeader confidence="0.759639" genericHeader="method">
2 SUBLANGUAGE AND SELECTION
2.1 SUBLANGUAGE
</sectionHeader>
<bodyText confidence="0.999902404761905">
A sublanguage is a specialized form of natural language
used to describe a limited subject matter, generally
employed by a group of specialists dealing with this
subject. Examples of sublanguages that have been
studied are weather reports (Chevalier et al. 1978),
aircraft maintenance manuals (Lehrberger 1983),
medical reports (Hirschman and Sager 1983), and equip-
ment failure reports (Marsh, Hamburger, and Grishman
1984). A sublanguage will generally be much more
constrained than the &amp;quot;standard language&amp;quot;, but it may
also include extensions to the standard language, such as
sentence fragments found in telegraphic-style message
text.
Zellig Harris, one of the first linguists to study
language use in restricted domains, defined sublanguages
in terms of one particular constraint: the constraint on
what words can co-occur within a particular syntactic
pattern, such as a subject-verb-object structure (Harris
1968). Just as speakers of the standard language distin-
guish between grammatical and ungrammatical
sentences, speakers of the sublanguage will distinguish
between acceptable and unacceptable (meaningless)
sentences, even though the unacceptable sentences may
be grammatical sentences of the standard language. For
example, in the sublanguage of medical records, a speak-
er would accept the sentence The X-ray revealed a tumor.
but not The tumor revealed an X-ray.
Harris hypothesized that, for any particular sublan-
guage, we can define sublanguage word classes — sets of
words that are acceptable in the same contexts (Harris
1968). For example, in the context revealed a tumor,
we might find words such as X-ray, film, and scan. Such
classes, even though defined on purely distributional
grounds, correspond closely to the natural semantic class-
es that might be identified by an expert in the domain.
Thus, we might label the group of words X-ray, film, and
scan as a TEST class, and similarly (for medical reports)
identify classes such as FINDING and MEDICATION.
See section 3.2 below for discussion of an experiment
verifying Harris&apos;s hypothesis. The sublanguage co-occur-
rence constraints, when stated between word classes, are
commonly called selectional constraints.
</bodyText>
<subsectionHeader confidence="0.941009">
2.2 IMPLEMENTING SELECTIONAL CONSTRAINTS
</subsectionHeader>
<bodyText confidence="0.99992098">
It is generally recognized that these selectional
constraints play an important role in distinguishing
between correct and incorrect sentence analyses. Conse-
quently, most natural language systems incorporate some
form of selectional constraints. We describe here, brief-
ly, how these constraints are implemented in the Linguis-
tic String Parser; more detailed descriptions are given in
Grishman, Hirschman, and Friedman (1982, 1983) .
The Linguistic String Parser English grammar (Sager
1981) is an augmented context-free grammar. Its princi-
pal components are a context-free grammar (stated in
terms of the grammatical categories of English), a set of
procedural restrictions (written in Restriction Language:
Sager and Grishman 1975), and a lexicon. Adding selec-
tional constraints to this grammar involved specifying the
sublanguage classifications of words, specifying the
allowed co-occurrence patterns in positive terms, and
providing restrictions that check the parse tree for these
patterns.
Each word in the domain vocabulary is assigned to
one or more sublanguage word classes; these class
assignments are recorded as part of the lexical entry for
each word. Thus a lexical entry consists of each major
syntactic class for a word followed by a list of attributes,
including its domain subclass. Words that have two or
more meanings and, as a result, occur in different
contexts will be assigned to more than one word class;
such words are referred to as homographs. For example,
in the medical domain, discharge refers both to a patient&apos;s
discharge from a hospital and to the excretion of some-
thing from the patient&apos;s body. It is classified as a noun
with attributes H-VMD (medical verb for &apos;discharge from
hospital&apos;) and H-BODYPART (for &apos;bloody discharge&apos;).
As a verb, however, discharge is classified only as
H-VMD. (Only words specific to the domain receive
sublanguage classes and participate in selection.)
The allowed co-occurrence patterns are specified by a
set of lists in the grammar. There is a separate list for
each major syntactic relation: clauses (subject-verb-ob-
ject structures), prepositional phrases, prenominal adjec-
tives, compound nouns. Each list element may have
associated sub-lists; this recursive structure provides a
reasonably compact specification of the allowable combi-
nation of sublanguage classes. For example, the V-S-0
list gives the positive co-occurrence patterns for the
VERB-SUBJECT-OBJECT relation, where the list associ-
ated with each verb class (e.g., H-SHOW below) has a
sub-list of associated subject classes (e.g., H-BODYPART
and H-TEST), each of which have associated lists of
object types (H-INDIC, H-RESULT, H-DIAG).
</bodyText>
<equation confidence="0.851472">
LIST V-S-0 =
H-SHOW: (H-BODYPART: (H-INDIC, H-RESULT,
H-DIAG),
H-TEST: (H-INDIC, H-RESULT,
</equation>
<bodyText confidence="0.7112565">
H-DIAG),
This is interpreted as follows:
</bodyText>
<listItem confidence="0.9751565">
• H-SHOW verbs with H-BODYPART (&amp;quot;body part&amp;quot; class)
as subject take objects of classes H-INDIC
(&amp;quot;indicators&amp;quot;), H-RESULT or H-DIAG (&amp;quot;diagnosis&amp;quot;),
as in liver showed no abnormalities;
</listItem>
<page confidence="0.829166">
206 Computational Linguistics, Volume 12, Number 3, July-September 1986
</page>
<author confidence="0.427802">
Ralph Grislunan, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns
</author>
<listItem confidence="0.836868">
• with H-TEST (&amp;quot;test&amp;quot;) as subject, H-SHOW verbs also
take the objects H-INDIC, H-DIAG, and H-RESULT, as
in test showed metastasis.
</listItem>
<bodyText confidence="0.982921966666666">
The list imposes selection only for listed verbs, and not
all verbs appear on the list. In particular, be and related
verbs do not, since they obey a different kind of selection
(between subject and object). Similarly, not all prep-
ositions participate in selection for prepositional phrases;
specifically, of has too broad a distribution for the state-
ment of selectional patterns. In this way, selection is
applied only to sublanguage-specific constructs, where it
is possible to describe the allowed patterns with reason-
able conciseness.
The selectional constraints are enforced by a set of
restrictions that use the lists of co-occurrence patterns.
Whenever a structure involved in selection (e.g., clause,
noun phrase, prepositional phrase) is completed during a
parse, a restriction is executed that compares the classes
assigned to the words in the parse tree with the allowed
selectional patterns for this structure. If the word partic-
ipates in selection, but its associated arguments do not
match the patterns on the list, then the analysis is
rejected and the parser backs up to seek an alternative
analysis. Because it operates on surface structure, the
restriction that tests for subject-verb-object selection
must take into account all the transforms of this basic
structure. For clauses, the restriction checks selection for
both active and passive sentences, sentences with inter-
vening aspectuals (as in patient continued to receive medi-
cation), and relative and reduced relative clauses. It does
this by identifying the &amp;quot;transformed&amp;quot; subject, verb, and
object; it can then use a single canonical set of subject-
verb-object patterns for selection.
</bodyText>
<subsectionHeader confidence="0.994644">
2.3 THE VALUE OF SELECTION
</subsectionHeader>
<bodyText confidence="0.999960739130435">
Although it is generally agreed that selectional
constraints are important in separating correct and incor-
rect analyses, we are not aware of any measurements of
the impact of selectional constraints, particularly in text
analysis (as contrasted with the analysis of natural
language database queries, for example). In order to
obtain a more objective measure of the importance of
these constraints, we conducted an experiment compar-
ing the effectiveness of grammars with and without selec-
tional constraints.
The test corpus was a set of hospital discharge
summaries containing 407 sentences and sentence frag-
ments. We used the NYU Linguistic String Project
medical grammar, which is a modification of the Linguis-
tic String Project English grammar including the sentence
fragments and other constructs (such as descriptions of
medication dosages) that appear in medical reports but
not in standard English (Marsh 1983). Each sentence
was analyzed twice, once without any selectional
constraints and once with selectional constraints (the
selectional patterns were developed manually at NYU by
linguists from a study of this test corpus and other similar
medical reports). The results of each analysis were clas-
</bodyText>
<subsectionHeader confidence="0.874611">
Computational Linguistics, Volume 12, Number 3, July-September 1986
</subsectionHeader>
<bodyText confidence="0.9969545">
sified into one of three categories: no parses obtained;
one or more parses obtained, good first parse;1 one or
more parses obtained, bad first parse. These results are
summarized in Table 1.
</bodyText>
<table confidence="0.9851365">
with selection without selection
good parses 308 (76%) 306 (75%)
bad parses 30 (7%) 68 (17%)
no parses 69 (17%) 33 (8%)
</table>
<tableCaption confidence="0.9792845">
Table 1. Parsing results for 407 sentences, run with and
without selectional constraints.
</tableCaption>
<bodyText confidence="0.999857181818182">
We found, in brief, that adding selectional constraints
had only a marginal effect on the number of good parses.
However, it greatly reduced the number of bad parses.
Sentences that previously got bad parses now got no
parse at all. This somewhat surprising result can be
explained by noting that a certain number of sentences
that had previously parsed correctly were blocked by
over-constraining due to selection. For example, the
phrase herpes type lesion was parsed successfully without
selection, but failed to parse with selection, because there
were no selection patterns for allowing a compound noun
of the form herpes type (H-DIAG + H-TYPE) + lesion
(H-INDIC). On the other hand, some sentences that
received an incorrect first parse without selection
received a correct parse with selection because the incor-
rect parse is blocked by selection. For example, the
patient had no JVD and no increase in thyroid size parsed
incorrectly without selection due to incorrect distribution
of the adjunct in thyroid size, but correctly with selection.
Overall, 21 sentences (out of a corpus of 400) changed
from good to no parse and 23 from bad parse to good or
acceptable parse.
Despite the fact that the number of correct parses did
not show any significant increase, the use of selection
produced a very substantial improvement in reliability of
parses. We consider this an important benefit, for two
reasons. First, in critical applications, an undetected
error (bad parse) may lead to erroneous data in the data
base; this is much worse than an error detected by the
system (no parse). Second, if the analysis failure can be
detected, it is possible to try various recovery techniques,
such as employing a different analysis technique or
asking the user for additional information.
</bodyText>
<subsectionHeader confidence="0.996612">
2.3 THE ROLE OF SELECTION
</subsectionHeader>
<bodyText confidence="0.999991111111111">
We recognize that selectional constraints may only be the
tip of the iceberg in terms of domain-specific informa-
tion. Much more detailed knowledge of the domain and
the structure of discourse in the sublanguage will doubt-
less be needed for a high-quality text analysis system.
Nonetheless, we believe selection has an important role
to play. As shown by the experiment just described,
more than half of the analysis errors resulting from
syntactic analysis can be detected using selectional
</bodyText>
<page confidence="0.940734">
207
</page>
<author confidence="0.390698">
Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns
</author>
<bodyText confidence="0.99988925">
constraints alone. In addition, selectional constraints are
simple in structure and have been more intensively
studied than most other domain knowledge; in particular,
their relationship to distributional information in the
sublanguage is better understood. It therefore seemed
appropriate to focus on selectional constraints in our
studies of discovery procedures for domain-specific
knowledge.
</bodyText>
<sectionHeader confidence="0.997589" genericHeader="method">
3 DISCOVERY PROCEDURES
</sectionHeader>
<subsectionHeader confidence="0.999386">
3.1 EXPERT VS. TEXT-BASED PROCEDURES
</subsectionHeader>
<bodyText confidence="0.999962657894737">
Two basic approaches have been proposed for mechaniz-
ing (or partially mechanizing) the acquisition of domain-
specific information for natural language systems. One
of these is based on the systematic interviewing of a
domain expert, who provides information on the basic
semantic classes and relations of the domain and their
linguistic forms and properties. Such an approach has
been incorporated into some natural language interfaces
for database retrieval, such as TEAM (Grosz 1983) and
LDC (Ballard, Lusth, and Tinkham 1984). This
approach assumes that the domain expert has some
model of the relations in the domain, and a knowledge of
the different ways in which these relations can be refer-
enced. This is not unreasonable in the database context,
since the database schema can serve as a domain model
(divining all the ways in which a relationship can be
referenced may still be difficult, however). This
approach is more difficult, however, in text analysis
applications, particularly because the user may not have
such a clear model of the domain semantics from which
to work.
An alternative approach is to acquire some of the
domain-specific information from the text itself. To the
extent that this information is reflected in distributional
relationships in the text, we can hope to extract this
information by automatically analyzing a sample of text
in a new domain. We have been pursuing this approach
for a number of years, and describe some of our earlier
efforts in the next subsection.
Although we present the expert and text-based
approaches as alternatives, we do not believe them to be
mutually exclusive. It may turn out that the most effec-
tive approach is a combination of these two, in which
information gleaned from the text &amp;quot;fills in&amp;quot; the skeletal
information provided by an expert, and the expert
provides generalizations that could not easily be derived
directly from the text. We shall return to this point in our
concluding section.
</bodyText>
<subsectionHeader confidence="0.998134">
3.2 OUR PRIOR WORK
</subsectionHeader>
<bodyText confidence="0.999821025">
Our previous work on discovery procedures has aimed at
automating the characterization of syntactic usage and
the identification of the principal semantic classes in
sublanguages. Both of these procedures, as well as the
procedure to be described below, start from a set of
parse trees (prepared automatically or manually) for a
sample text in the domain. The procedures for determin-
ing syntactic usage process the file of parse trees to
extract frequency data on the use of various productions
from the context-free grammar. Recent tests of this
procedure on both medical records and equipment failure
records indicate that accurate characterizations can be
obtained from a sample of a few hundred sentences, and
that (for both sublanguages) the size of the grammar
used was roughly one-third the size of the full Linguistic
String Parser English grammar (Grishman, Nhan, and
Marsh 1984; Grishman, Nhan, Marsh, and Hirschman
1984).
The procedure for discovering sublanguage classes is
based on identifying words that occur in the text in simi-
lar syntactic contexts, e.g., as subject of a given verb or
as object of a given verb or as adjective modifying a
given head, etc. We defined a similarity coefficient for
pairs of words, based on the number of contexts the
words shared. Then, using a statistical clustering proce-
dure, we grouped together words of high mutual similari-
ty. This procedure was successful in identifying classes
containing the high frequency words of the domain
(Hirschman, Grishman, and Sager 1975); the procedure
was not effective for words that occurred only a few
times in the sample corpus. Also, a number of false clus-
ters were generated, due to linguistic phenomena we
were able to identify, such as the omission of the head in
a noun phrase. This produced anomalies in the classifica-
tion, since the text contained occurrences such as chest
normal (understood as &apos;chest X-ray normal&apos;) as well as
X-ray normal. The result was a high similarity between
chest and X-ray and a resulting false cluster containing
chest with various test words such as X-ray, film, and
mammogram.
</bodyText>
<sectionHeader confidence="0.9849665" genericHeader="method">
4 AUTOMATIC GENERATION OF SELECTIONAL
PATTERNS
</sectionHeader>
<bodyText confidence="0.99990665">
Determining the selectional constraints for a new sublan-
guage involves both determining the sublanguage word
classes and determining the allowed co-occurrence
patterns among those classes. In principle, both can be
determined by a distributional analysis of a sample
corpus. In practice, this is a labor-intensive procedure
involving iteration between setting up sublanguage class-
es and identifying sublanguage patterns. However, in
order to simplify the work during our initial experiments,
we chose to separate these two tasks. We mentioned just
above the experiments we had conducted earlier on
discovering sublanguage classes. We describe here a
complementary set of experiments to demonstrate our
ability to generate co-occurrence patterns from a text
sample. These experiments assume a (manual) assign-
ment of words to sublanguage classes and aim at collect-
ing the co-occurrence patterns and evaluating their
completeness. These complementary experiments are
needed to validate our techniques before we address the
more difficult problem of building the selectional patterns
</bodyText>
<page confidence="0.926452">
208 Computational Linguistics, Volume 12, Number 3, July-September 1986
</page>
<note confidence="0.718759">
Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns
</note>
<bodyText confidence="0.999888692307692">
for a new domain (see section 7 for a discussion of this
issue).
Given our goal of evaluating the completeness of auto-
matically generated patterns, our initial experiments drew
on a domain where sublanguage vocabulary had already
been classified. Our test corpus consisted of eleven
medical reports. Six were patient documents that
included patient history, examination, and plan of treat-
ment; five were hospital &amp;quot;discharge summaries&amp;quot; which
included patient history, examination, and summaries of
the course of treatment in the hospital. The corpus
contained about 750 sentences and sentence fragments.
In analyzing these sentences, we used the Linguistic
String Project medical grammar, a modification of the
LSP English grammar that had been used for processing a
number of medical documents (Hirschman et al. 1981),
including the discharge summaries in our corpus. The
sublanguage word classes, which are recorded in our lexi-
con, had been developed based on the discharge summa-
ries and other similar medical records. However, neither
the grammar nor the word classes had been revised to
reflect any new syntactic forms or semantic patterns that
appeared in the other six patient documents; these docu-
ments were being analyzed for the first time.
The discovery procedure for selectional patterns has
five principal steps:
</bodyText>
<listItem confidence="0.997967875">
1. generating a set of correct parses;
2. resolving homographs;
3. generating instances of selectional patterns from the
parses;
4. collecting the patterns into lists sorted by syntactic
construct (e.g., a list for subject-verb-object patterns,
a list for head-prepositional modifier patterns, etc.);
5. final review of patterns for correctness.
</listItem>
<bodyText confidence="0.999979173913044">
We began by parsing the entire text with the Linguis-
tic String Parser and the Linguistic String Project medical
grammar, and collecting the resulting parse trees. Gener-
ation of correct patterns depends critically on having
correct parses; therefore, the automatically generated
parse trees had to be manually screened to select only
correct parses. One possibility would have been to
generate (without relying on selection) all parses for each
sentence and then to choose the correct parse by hand.
This would have required a great deal of work, since
without selection there may be many parses for each
sentence. Since we were focusing on evaluating the
completeness of the set of generated patterns, rather than
on the feasibility of acquiring the selectional patterns in a
new domain, we chose to use the existing selection mech-
anism as a short-cut to getting the correct parse. This
reduced drastically the number of parses that had to be
screened; it did not affect correctness of the chosen
parse, since the parse is or is not correct, regardless of
how it is generated. However, for sentences blocked by
selection, we did parse these sentences without selection
and did go through the manual procedure to select the
correct parse. For a number of sentences, we failed to
</bodyText>
<subsectionHeader confidence="0.583503">
Computational Linguistics, Volume 12, Number 3, July-September 1986
</subsectionHeader>
<bodyText confidence="0.999987122807018">
obtain an automatically generated correct parse by either
parsing method. These sentences were not included in
the corpus. This procedure furnished us with good parses
for about 520 sentences and sentence fragments (about
2/3 of the initial corpus).
The next step was to resolve homographs. As we
noted above, some words may have more than one mean-
ing or be used in more than one way, and thus be
assigned to more than one sublanguage class. Within any
particular sentence, the word was used in one of these
senses and should therefore have been identified with the
corresponding sublanguage class in order to produce the
correct sublanguage co-occurrence patterns. Much of the
homograph resolution was done automatically, by the
selection mechanism. However, in certain cases, a word
emerged from the processing with multiple sublanguase
classes. In some cases, this was due to insufficient
context resulting from omission of implicit (zeroed)
information, e.g., discharge on 1 / 12 would probably refer
to discharge of patient from the hospital, but selection
could not rule out the SYMPTOM reading of discharge
from this limited context. A second source of unresolved
homographs came from parses generated without
selection, in which case there was no automatic mech-
anism for homograph resolution. Whatever the source,
words listed as having multiple subclasses were screened
and disambiguated manually: we scanned the parse trees
for sentences containing multiply-classified words, and,
in each case, selected manually the sublanguage class
relevant to its use in that sentence.
We then proceeded to the task of extracting the
sublanguage class co-occurrence patterns from the file of
correct surface parse trees. Since co-occurrence patterns
reflect a regularized or canonical structure (e.g., verb-
subject-object relations), it was necessary to map surface
structure into the normalized set of relations required for
co-occurrence patterns. This involved locating the
&amp;quot;logical&amp;quot; subject and object in passive sentences, relative
clauses, reduced relatives, and clauses with aspectual
verbs. For example, in patient continued to receive medi-
cation, the verb/subject/object co-occurrence pattern of
interest is &amp;quot;receive/patient/medication&amp;quot;.
The computation of co-occurrence patterns was done
by a set of restrictions that borrowed code from those
used for the selection mechanism itself. (This was possi-
ble because the selection mechanism also has to find the
logical elements involved in co-occurrence, including the
cases where these differ from the surface structure). An
appropriate restriction (e.g., a subject-verb-object or an
adjective-noun or a noun-preposition-noun restriction)
identified the structures that participate in selection
(subject-verb-object, adjective-noun, or noun-preposi-
tion-noun). For each such structure, the restriction
located the words participating in the co-occurrence
relation and retrieved the sublanguage classes associated
with these words. The restriction then wrote the sublan-
guage class pattern onto a file. The pattern consisted of
</bodyText>
<page confidence="0.970209">
209
</page>
<bodyText confidence="0.8160347">
Ralph Grishman, Lynette Hirsclunan, Ngo Thanh Nhan
Discovery Procedures for Sublanguage Selectional Patterns
a tag identifying the pattern type (e.g., PRED-
ARG1-ARG2 for verb-subject-object or NVAR-APOS for
noun-adjective) and the actual words in that instance of
the pattern, followed by a line for each member of that
pattern, containing the major class (e.g., noun = N or
past participle = VEN), followed by the word, followed
by the subclass.
* 81A 1C. 1.11
</bodyText>
<equation confidence="0.812289">
PRED-ARG1-ARG2 EXAMINED 0 JOINTS
VEN EXAMINED (H-VMD)
0 (NIL)
JOINTS (H-AREA)
*81A 1C.1.11
</equation>
<sectionHeader confidence="0.797476" genericHeader="method">
NVAR-APOS JOINTS OTHER
JOINTS (H-AREA)
ADJ OTHER OTHER
</sectionHeader>
<bodyText confidence="0.99997544">
The final stage involved collecting, counting and refor-
matting the set of co-occurrence patterns into the selec-
tional lists required by the grammar. This permitted us to
use the automatically generated sets of co-occurrence
patterns as input to the selectional constraints of the
grammar.
Prior to running any parsing experiments, we
compared the automatically generated selection lists to
the lists produced manually by a linguist. Our expecta-
tion was that the automatically generated lists would be a
subset of the manually prepared set. It turned out that
this was not the case, primarily because a number of
human errors had allowed erroneous patterns to enter the
file: errors in assigning sublanguage classes to words,
errors in resolving homographs, oversights in weeding out
bad parses. We therefore found it necessary in practice
to make a final manual pass over the file of patterns,
discarding bad patterns that had crept in in one way or
another. Only then were the patterns suitable for use as
data to the selectional restrictions.
Although most of the data manipulation (generation of
parse trees, generation and collection of selectional
patterns) was automated, considerable manual inter-
vention was still needed to verify the processing at each
stage. We shall return to this issue below.
</bodyText>
<sectionHeader confidence="0.996953" genericHeader="method">
5 EVALUATION
</sectionHeader>
<bodyText confidence="0.999957666666667">
We have evaluated the selectional patterns obtained by
the procedure just described in two ways. First, we have
tried to estimate how complete the set of patterns is.
Second, we have compared the effectiveness of these
patterns in parsing new material with that of selectional
patterns generated by hand.
</bodyText>
<subsectionHeader confidence="0.958988">
5.1 GROWTH CURVES
</subsectionHeader>
<bodyText confidence="0.9971237">
A crucial question we wanted to answer with our exper-
iment was whether the size of our text sample was
adequate to obtain a reasonably complete set of selec-
tional patterns. To answer this question, we plotted the
growth in our sets of selectional patterns as a function of
the size of the sample we have processed (i.e., the
number of different patterns encountered in the first X
sentences). Figures 1, 2, and 3 show the growth curves
for the subject-verb-object, prepositional phrase, and
adjective-noun selectional patterns.2
</bodyText>
<figure confidence="0.946655388888889">
NUMBER OF PATTERNS
140
130
.120
110
100
90
80
70
60
50
40
30
20
10
SUBJECT-VERB-OBJECT PRTTERNS
„11,1111111111 111
--/
</figure>
<figureCaption confidence="0.8201094">
50 100 150 200 250 300 350 400 450 SOO 550
SIZE OF TEXT SAMPLE (SENTENCES)
Figure 1. The growth in the number of subject-verb-object selectional patterns as a function of the size of the text
sample (in sentences). The solid curve is the actual data, the dashed line the least-squares fit to a function
of the form A*(1—exp(—B*x)).
</figureCaption>
<page confidence="0.972457">
210
</page>
<figure confidence="0.982701033333333">
Computational Linguistics, Volume 12, Number 3, July-September 1986
Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns
PREPOSITIONRL PHRRSE PRTTERNS
120 _ ...., -
- ..,&amp;quot; _
111.111171111.11 , 111111111/1111
_
_
160
140
NUMBER OF PATTERNS
100
80
60
_
-
-
-
_
-
_
_
40
20
_
_
_
1 tlit,11111.111111.11111■1,1111.1111111■1■11111111■11
50 100 150 200 250 300 350 400 450 500 550
SIZE OF TEXT SRMPLE (SENTENCES)
</figure>
<figureCaption confidence="0.557021333333333">
Figure 2. The growth in the number of prepositional phrase selectional patterns as a function of the size of the text
sample (in sentences). The solid curve is the actual data, the dashed line the least-squares fit to a function of
the form A*(1—exp(—B*x)).
</figureCaption>
<figure confidence="0.948251259259259">
ROJECTIVE-NOUN PRTTERNS
120 VIIIIIT
-
110 —
I I 1111111f1111111,1111 1
_
100 —
_
90 —
-
80 —
_
NUMBER OF PATTERNS - //
70 —
60 ,— _
_
50 —
-
40 &amp;quot; -
30 —
20 — / -
10 — -
-
-
titillaillteitlin t■I,■1.1111■1111111).■ [tit glallilltil
50 100 150 200 250 300 350 400 450 500 550
SIZE OF TEXT SRMPLE (SENTENCES)
</figure>
<figureCaption confidence="0.963221">
Figure 3. The growth in the number of adjective-noun selectional patterns as a function of the size of the text sample
(in sentences). The solid curve is the actual data, the dashed line the least-squares fit to a function of the
form A*(1 —exp(—B*x)).
</figureCaption>
<footnote confidence="0.317691">
Computational Linguistics, Volume 12, Number 3, July-September 1986 211
</footnote>
<note confidence="0.587632">
Ralph Grishman, Lynette Hirschman, Ngo Thatth Nhan Discovery Procedures for Sublanguage Selectional Patterns
</note>
<bodyText confidence="0.999939923076923">
If our corpus had yielded a reasonably complete set of
patterns, we would expect the growth curves to flatten
out by the end (indicating that very few new patterns
were being encountered in the text). In our earlier study
of syntactic patterns in sublanguages, we had found just
such an effect after 200-250 sentences. Unfortunately —
as is evident in the figures — this is not the case here even
after 500 sentences; the slope of the curve has clearly
decreased, but it is by no means flat.
A pessimistic reader might suggest at this point that
the set of selectional patterns is not closed, and that the
curve will continue rising at a substantial rate until nearly
all possible patterns are present. Our experience with
sublanguage selection — and that of other linguists —
suggests, however, that, to a first approximation, the set
of patterns is closed and that, with a text sample several
times larger than the present one, the curve will flatten
out. In order to get a more quantitative estimate of the
size of corpus that will be needed, we can use the follow-
ing crude model. The successive patterns encountered in
processing the sentences represent a random selection
(with replacement) from a finite population (the
complete set of patterns for the sublanguage). We there-
fore expect the growth curve to have the form
Y=A*(1—exp( —Bs)), where s is the number of
sentences processed, A is the size of the complete set of
patterns, and B is a parameter related to the rate of
growth of the set of patterns. A least-squares fit of this
function to the growth curve yields the following values:
for subject-verb-object patterns, A=180, B=0.00264;
for prepositional phrase patterns, A=276, B=0.00162;
for adjective-noun patterns, A=126, B=0.00416. The
fitted exponential curves are shown as dashed lines in
Figures 1-3. These results can be more meaningfully
viewed in terms of the size of the corpus we would need
to get 90% complete patterns: for subject-verb-object
patterns, about 900 sentences; for prepositional phrase
patterns, about 1400 sentences; for adjective-noun
patterns, about 550 sentences.
</bodyText>
<subsectionHeader confidence="0.999303">
5.2 PARSING TESTS
</subsectionHeader>
<bodyText confidence="0.944734157142857">
The primary objective of our discovery procedure is to
produce a set of selectional patterns that can be used in
parsing further texts in the sublanguage; the ultimate test
of the patterns we generate, therefore, is to use them in
parsing new text and see how they affect the parsing
rates. The prospects for such a&apos; test are clouded by the
results of the previous subsection, which showed that the
set of patterns we had collected was still quite incom-
plete. Nonetheless we thought it worthwhile to proceed
with this second stage of evaluation.
In order to avoid the substantial labor associated with
processing a new text (entering the text, entering defi-
nitions for the new words, etc.), we proceeded as follows.
We took two medical records from our corpus of 11
records (about 20% of the corpus) and treated them as
&amp;quot;new text&amp;quot;. We reran the programs for collecting selec-
tional patterns, using only the remaining nine records
(the &amp;quot;old text&amp;quot;). We then parsed the new text using the
selectional patterns thus generated, and classified the
results for each sentence: good parse (first parse correct),
bad parse (first parse incorrect), or no parse for the
sentence.
We compared these results with the results of parsing
the text using manually prepared selectional patterns.
These patterns had been prepared by a computational
linguist, based on a study of various medical records
(including five of the records in our current corpus),
generalizing from observed patterns where it seemed
reasonable.
The results of the comparison are shown in Table 2.
The rate of successful analyses was substantially lower
with the automatically generated selectional patterns.
This is not surprising given our observations in the earlier
section about the incompleteness of these patterns.
Where a selectional pattern is missing, an analysis will be
blocked, thus generally producing no parse for the
sentence.
Selectional patterns generated:
manually automatically
good parses 54 43
bad parses 25 22
no parses 27 41
Table 2. Parsing results for 106 &amp;quot;new&amp;quot; sentences,
comparing manually and automatically gener-
ated selectional patterns.
The parsing rates shown here are relatively low (about
50% good parses) when compared with the data of
Table 1 (about 75% success). This reflects the fact that
the six patient summaries in our corpus, including the two
treated here as new text, were analyzed &amp;quot;cold&amp;quot;: words
were added to the lexicon as needed, but otherwise no
adjustments were made to the grammar or lexicon. The
documents are unedited medical reports with many
sentence fragments; they contain a substantial number of
sublanguage-specific constructs not previously encoun-
tered in processing other types of reports, (for example,
prepositions were sometimes omitted before body parts:
Synovial thickening both wrists bilaterally.). In addition,
the experiments revealed a substantial number of errors
in the lexicon. Of the 63 failures (bad or no parse) using
the automatically generated patterns, 20 were due to
syntactic constructs not present in the grammar, 3 to
other grammar or parser bugs, 11 to errors in the diction-
ary, and 23 to missing selectional patterns; 4 sentences
got a bad parse on the first parse and a good parse on the
second parse; 2 more were considered unanalyzable
sentence fragments. The syntactic gaps and lexical errors
uniformly depress the success rates for these exper-
iments, but we feel that the data is still valid for compar-
ing different sets of selectional constraints.
</bodyText>
<page confidence="0.929285">
212 Computational Linguistics, Volume 12, Number 3, July-September 1986
</page>
<note confidence="0.840195">
Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns
</note>
<subsectionHeader confidence="0.99588">
5.3 RESTRICTION RELAXATION
</subsectionHeader>
<bodyText confidence="0.999963272727273">
The incompleteness of semantic information is a serious
and general problem that transcends our particular work
on discovery procedures. As the domains with which
natural language systems deal become more complex, it
becomes more difficult to acquire a complete set of selec-
tional patterns. Furthermore, in many sublanguage texts
there are passages that fall outside the sublanguage; for
example, in one medical record domain, there is a
mention of a vacation a patient took, during which he got
sick. These passages will not satisfy the selectional
constraints of the sublanguage.
In the manual preparation of selectional patterns,
some small measures were taken to compensate for this
incompleteness. In preparing the patterns, the linguist
generalized from the patterns observed in the text,
adding new patterns that seemed equally reasonable
based on a knowledge of the domain. For certain very
common prepositions (e.g., of) for which it would be
difficult to collect all the selectional patterns, selection
was disabled. Similarly the linguist chose to omit some
verbs from the selectional patterns; in these cases,
subject-verb-object selection was not applied.
In the automatically generated patterns, no similar
measures were taken. This, combined with the limited
corpus used to gather the patterns, accentuated the effect
of the incompleteness of the patterns. Because absence
of a co-occurrence pattern can be interpreted as either
negative information (a particular pattern is not allowed
in the sublanguage) or as incomplete information (this
pattern has not yet been seen), any automatically gener-
ated set of patterns will over-constrain the parsing. We
therefore sought some way of automatically compensat-
ing for this incompleteness.
The approach we chose to try was restriction relaxa-
tion. If no parse can be obtained satisfying all selectional
constraints, the parser tries for an analysis that will satis-
fy all but one of the selectional constraints.3 In effect, the
parser is willing to relax any one of the selectional
constraints in order to get an analysis. Such an approach
has been suggested before by several computational
linguists (for example, Weischedel and Sondheimer
1983), although primarily to account for ungrammatical
input rather than for incompleteness of semantic know-
ledge.
We originally applied this technique in connection
with the manually generated selectional patterns. These
results were not very encouraging: about 5% of the
sentences in the sample that had previously gotten no
parse now got a correct parse, but another 5% got a bad
parse. This was not too surprising in retrospect; the
various measures mentioned above to compensate for the
incompleteness of the patterns resulted in a set of rela-
tively &amp;quot;loose&amp;quot; constraints, and any further loosening
(such as restriction relaxation) would let quite a few bad
parses through.
</bodyText>
<subsectionHeader confidence="0.559214">
Computational Linguistics, Vohune 12, Number 3, July-September 1986
</subsectionHeader>
<bodyText confidence="0.984472">
Our results using this technique in connection with the
automatically generated patterns, which are tighter and
less complete, have been more positive (although based
to date on an extremely small sample). Within our two-
record sample, there were 14 sentences that had previ-
ously not gotten a parse and now got one with restriction
relaxation. Of these, 10 were correct and 4 were incor-
rect. The automatically generated patterns, when
coupled with the mechanism for restriction relaxation,
did about as well as the manually generated patterns
(Table 3). Given the small text sample, and the
acknowledged incompleteness of the set of patterns, we
found this somewhat encouraging. Of course, these
experiments are still too small to reach any definite
conclusions.
Selectional patterns generated:
manually automatically
good parses 54 53
bad parses 25 26
no parses 27 27
Table 3. Parsing results for 106 &amp;quot;new&amp;quot; sentences,
comparing manually and automatically gener-
ated selectional patterns, and using restriction
relaxation when parsing with automatically
generated patterns.
</bodyText>
<sectionHeader confidence="0.953414" genericHeader="method">
6 WHY IS IT SO HARD?
</sectionHeader>
<bodyText confidence="0.999978592592592">
When it is first described, the discovery procedure —
parse the text, extract certain syntactic structures, collect
the sublanguage class patterns — may seem quite simple
and straightforward. It has, however, taken us several
iterations to achieve even the small success described
here. It is worthwhile to reflect briefly on why this is so.
First, there are several sources of human error, each of
which contributes some errors to the final set of patterns.
There are errors of word classification, where the wrong
sublanguage class is recorded in the lexicon. There are
errors in weeding out bad parses: a small defect (e.g.,
incorrect conjunction scope) is easily overlooked. Final-
ly, there are errors due to selecting the wrong subclass
for a homograph. We have tried to cope with these
errors by repeatedly reviewing the generated set of
sublanguage patterns, going back each time to find the
source of any unexpected patterns. However, as our text
samples grow from thousands of words to tens of thou-
sands (as they must to get a better set of patterns), more
systematic control will be needed to minimize such
errors.
Second, there are a number of linguistic phenomena
that complicate the extraction of the selectional patterns.
Specifically, there are cases in which the sublanguage
class of a noun phrase cannot be determined from the
class of the head alone. In some constructs of the form
Ni preposition N2, the head Ni is &amp;quot;transparent&amp;quot;, and the
</bodyText>
<page confidence="0.994958">
213
</page>
<note confidence="0.498168">
Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns
</note>
<bodyText confidence="0.999970777777778">
phrase has the class of (has the distribution of) N2.
Examples are history of ..., increase in .... In other
cases, the class of the phrase depends on both the head
and the modifier; thus throat has the class BODYPART
but sore throat the class SYMPTOM. We have incorpo-
rated the patterns and procedures for computing such
phrasal attributes for the medical domain into our selec-
tional restrictions. In moving to a new domain, we would
have to acquire new sets of phrasal attribute patterns as
well as selectional patterns. To limit our current exper-
iment, however, our procedure for generating selectional
patterns used the phrasal attribute patterns that had been
previously developed manually.
None of these difficulties pose insurmountable road-
blocks to our goal. Rather they point out that, as in any
experiment where a large body of reliable data must be
collected, the procedures may be complex and special
measures must be taken to assure accuracy.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="conclusions">
7 CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999930941176471">
Overall, the experiments we have conducted using our
discovery procedure are encouraging but not conclusive.
The selectional patterns gathered from a limited text
sample — when coupled with a procedure for restriction
relaxation — do about as well as manually prepared selec-
tional patterns. Furthermore, the growth curves for the
selectional patterns suggest that a corpus several times
larger would yield a more complete set of patterns and
thus better performance in parsing.
We have learned that such a procedure requires
substantial human interaction and we intend, before
advancing to a larger corpus, to restructure the system to
facilitate this interaction. The present system is basically
organized for batch processing; interaction takes place
by editing intermediate files. Our next step will be to
move to an interactive environment that supports the
following capabilities:
</bodyText>
<listItem confidence="0.909986">
• isolating parse ambiguities and homographs and
prompting the user to choose the appropriate
reading/meaning;
• displaying new selectional patterns the first time they
are encountered;
• supporting simultaneous inspection and manipulation
of text, parse tree, and selectional patterns.
</listItem>
<bodyText confidence="0.999903925925926">
In all of this interaction, however, the user is still
acting only as a monitor of the patterns generated. We
are still faced with the difficult issue of how to bootstrap
the system into a new domain. In the absence of selec-
tional patterns, choosing the correct parse can become a
tedious and time-consuming procedure, requiring exten-
sive interaction with both a domain expert and a linguist.
It is clearly not a realistic method of building up a ,set of
patterns sufficient for semi-automated processing of the
type described above.
Combining the text-based approach with elicitation
procedures offers a more practical method of acquiring
an initial set of domain knowledge. An expert could
provide some initial word classes and a partial set of
relationships, from which to generate selectional patterns.
A sample of text would then provide additional examples,
with the expert available to elaborate on further patterns.
For example, a system being developed at BBN4 uses a
hierarchy of sublanguage classes; given a selectional
pattern, it asks the user to generalize it by replacing
classes with superclasses where possible. This initial peri-
od of intensive interaction with an expert would provide
a sufficient pattern base so that the text-driven tools
would become effective in filling in the knowledge base.
Such an approach would offer the assurance of good
coverage provided by a text-based system while requiring
a smaller text sample than a purely text-based procedure.
</bodyText>
<sectionHeader confidence="0.999212" genericHeader="acknowledgments">
ACKNOWLEDGEMENT
</sectionHeader>
<reference confidence="0.891759">
This material is based upon work supported by the
National Science Foundation under grants MCS-80-02453,
DCR-82-02373, and DCR-82-02397.
</reference>
<sectionHeader confidence="0.696846" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999893906976744">
Ballard, B.; Lusth, J.; and Tinkham, N. 1984 LDC-1: A Transportable
Knowledge-Based Natural Language Processor for Office Environ-
ments. ACM Transactions on Office Information Systems 2: 1-25.
Chevalier, M. et al. 1978 TAUM-METEO: Description du Systeme.
Groupe de recherche en traduction automatique, Universite de
Montreal.
Grishman, R. and Hirschman, L. 1982 Natural Language Interfaces
Using Limited Semantic Information. NSF Final Report, New York
University, New York, New York.
Grishman, R.; Hirschman, L.; and Friedman, C. 1982 Natural
Language Interfaces using Limited Semantic Information.
Proceedings of the 9th International Conference on Computational
Linguistics. Prague, Czechoslovakia: 89-94.
Grishman, R.; Hirschman, L.; and Friedman, C. 1983 Isolating
Domain Dependencies in Natural Language Interfaces. Proceedings
of the Conference on Applied Natural Language Processing. Santa
Monica, California: 46-53.
Grishman, R.; Nhan, N. T.; and Marsh, E. 1984 Tuning Natural
Language Grammars for New Domains. Proceedings of the Confer-
ence on Intelligent Systems and Machines. Rochester, Minnesota:
342-346.
Grishman, R.; Nhan, N. T.; Marsh, E.; and Hirschman, L. 1984 Auto-
mated Determination of Sublanguage Syntactic Usage. Proceedings
of COLING84 (Tenth International Conference on Computational
Linguistics). Stanford, California: 96-100.
Grosz, B. 1983 TEAM: A Transportable Natural-Language Interface
System. Proceedings of Conference on Applied Natural Language Proc-
essing. Santa Monica, California: 39-45.
Harris, Z. 1968 Mathematical Structures of Language. Wiley Intersci-
ence, New York, New York.
Hirschman, L.; Grishman, R.; and Sager, N. 1975 Grammatically-
based Automatic Word Class Formation. Information Processing and
Management 11: 39-57.
Hirschman, L.; Story, G.; Marsh, E.; Lyman, M.; and N. Sager. 1981
An Experiment in Automated Health Care Evaluation of Narrative
Medical Records. Computers and Biomedical Research 14: 447-463.
Hirschman, L. and Sager, N. 1983 Automatic Information Formatting
of a Medical Sublanguage. In Kittredge and Lehrberger: 27-80.
Kittredge, R. and Lehrberger, J., Eds. 1983 Sublanguage: Studies of
Language in Restricted Semantic Domains. Series of Foundations of
Communications, Walter de Gruyter, Berlin: 27-80.
Lehrberger, J. 1983 Automatic Translation and the Concept of
Sublanguage. In Kittredge and Lehrberger.
</reference>
<page confidence="0.756889">
214 Computational Linguistics, Volume 12, Number 3, July-September 1986
</page>
<note confidence="0.430875">
Ralph Gtislunan, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns
</note>
<reference confidence="0.988126096774194">
Marsh, E. 1983 Utilizing Domain-Specific Information for Processing
Compact Text,. Proceedings of Conference on Applied Natural
Language Processing. Santa Monica, California: 99-103.
Marsh, E.; Hamburger, H.; and Grishman, R. 1984 A Production Rule
System for Message Summarization. Proceedings of the 1984
National Conference on Artificial Intelligence. Austin, Texas.
Sager, N. 1981 Natural Language Information Processing: A Computer
Grammar of English and its Applications. Addison-Wesley, Reading,
Massachusetts.
Sager, N. and Grishman, R. 1975 The Restriction Language for
Computer Grammars of Natural Language. Communications ACM
18: 390-400.
Weischedel, R. M. and Sondheimer, N. K. 1983 Meta-rules as a Basis
for Processing Ill-Formed Input. Journal of Computational Linguis-
tics 9: 161-177.
NOTES
1. &amp;quot;Good parses&amp;quot; included some parses that were not entirely correct
but that were good enough so they did not cause errors in the
process that converted the parsed trees into information formats
(a structured data base).
2. The curves are rather jagged because the reports are divided into
sections containing different types of information; when we begin
processing a new section, new patterns are encountered, and there
is therefore a sharp rise in the growth curves.
3. If no analysis can be obtained by relaxing one restriction, the
parser is able to try for an analysis that relaxes two, three, or more
restrictions. Our experiments have indicated, however, that relax-
ing more than one restriction produced bad parses more often than
good ones.
4. Private communication with M. Bates.
Computational Linguistics, Volume 12, Number 3, July-September 1986 215
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.153073">
<title confidence="0.815861">DISCOVERY PROCEDURES FOR SUBLANGUAGE SELECTIONAL</title>
<affiliation confidence="0.858173333333333">INITIAL EXPERIMENTS Computer Science Courant Institute of Mathematical</affiliation>
<address confidence="0.994351">New York New York, NY 10012</address>
<affiliation confidence="0.5614895">Research and Development Division System Development Corporation — A Burroughs</affiliation>
<address confidence="0.967942">Paoli, PA 19301</address>
<author confidence="0.908294">Ngo Thanh Nhan</author>
<affiliation confidence="0.997327666666667">Computer Science Department Courant Institute of Mathematical New York University</affiliation>
<abstract confidence="0.995996714285714">Selectional constraints specify, for a particular domain, the combinations of semantic classes acceptable in subject-verb-object relationships and other syntactic structures. These constraints are important in blocking incorrect analyses in natural language processing systems. However, these constraints are domain-specific and hence must be developed anew when a system is ported to a new domain. A discovery procedure for selectional constraints is therefore essential in enhancing the portability of such systems. This paper describes a semi-automated procedure for collecting the co-occurrence patterns front a sample of texts in a domain, and then using these patterns as the basis for selectional constraints in analyzing further texts. We discuss some of the difficulties in automating the collection process, and describe two experiments that measure the completeness of these patterns and their effectiveness compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how these procedures could be combined with a system that queries a domain expert, in order to produce a more efficient discovery procedure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This material is based upon work supported by the National Science Foundation under grants</title>
<pages>80--02453</pages>
<marker></marker>
<rawString>This material is based upon work supported by the National Science Foundation under grants MCS-80-02453, DCR-82-02373, and DCR-82-02397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Ballard</author>
<author>J Lusth</author>
<author>N Tinkham</author>
</authors>
<title>LDC-1: A Transportable Knowledge-Based Natural Language Processor for Office Environments.</title>
<date>1984</date>
<journal>ACM Transactions on Office Information Systems</journal>
<volume>2</volume>
<pages>1--25</pages>
<marker>Ballard, Lusth, Tinkham, 1984</marker>
<rawString>Ballard, B.; Lusth, J.; and Tinkham, N. 1984 LDC-1: A Transportable Knowledge-Based Natural Language Processor for Office Environments. ACM Transactions on Office Information Systems 2: 1-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chevalier</author>
</authors>
<title>TAUM-METEO: Description du Systeme. Groupe de recherche en traduction automatique,</title>
<date>1978</date>
<institution>Universite de Montreal.</institution>
<marker>Chevalier, 1978</marker>
<rawString>Chevalier, M. et al. 1978 TAUM-METEO: Description du Systeme. Groupe de recherche en traduction automatique, Universite de Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>L Hirschman</author>
</authors>
<title>Natural Language Interfaces Using Limited Semantic Information. NSF Final Report,</title>
<date>1982</date>
<location>New York University, New York, New York.</location>
<marker>Grishman, Hirschman, 1982</marker>
<rawString>Grishman, R. and Hirschman, L. 1982 Natural Language Interfaces Using Limited Semantic Information. NSF Final Report, New York University, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>L Hirschman</author>
<author>C Friedman</author>
</authors>
<title>Natural Language Interfaces using Limited Semantic Information.</title>
<date>1982</date>
<booktitle>Proceedings of the 9th International Conference on Computational Linguistics.</booktitle>
<pages>89--94</pages>
<location>Prague, Czechoslovakia:</location>
<marker>Grishman, Hirschman, Friedman, 1982</marker>
<rawString>Grishman, R.; Hirschman, L.; and Friedman, C. 1982 Natural Language Interfaces using Limited Semantic Information. Proceedings of the 9th International Conference on Computational Linguistics. Prague, Czechoslovakia: 89-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>L Hirschman</author>
<author>C Friedman</author>
</authors>
<title>Isolating Domain Dependencies in Natural Language Interfaces.</title>
<date>1983</date>
<booktitle>Proceedings of the Conference on Applied Natural Language Processing.</booktitle>
<pages>46--53</pages>
<location>Santa Monica, California:</location>
<marker>Grishman, Hirschman, Friedman, 1983</marker>
<rawString>Grishman, R.; Hirschman, L.; and Friedman, C. 1983 Isolating Domain Dependencies in Natural Language Interfaces. Proceedings of the Conference on Applied Natural Language Processing. Santa Monica, California: 46-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>N T Nhan</author>
<author>E Marsh</author>
</authors>
<title>Tuning Natural Language Grammars for New Domains.</title>
<date>1984</date>
<booktitle>Proceedings of the Conference on Intelligent Systems and Machines.</booktitle>
<pages>342--346</pages>
<location>Rochester, Minnesota:</location>
<marker>Grishman, Nhan, Marsh, 1984</marker>
<rawString>Grishman, R.; Nhan, N. T.; and Marsh, E. 1984 Tuning Natural Language Grammars for New Domains. Proceedings of the Conference on Intelligent Systems and Machines. Rochester, Minnesota: 342-346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>N T Nhan</author>
<author>E Marsh</author>
<author>L Hirschman</author>
</authors>
<title>Automated Determination of Sublanguage Syntactic Usage.</title>
<date>1984</date>
<booktitle>Proceedings of COLING84 (Tenth International Conference on Computational Linguistics).</booktitle>
<pages>96--100</pages>
<location>Stanford, California:</location>
<marker>Grishman, Nhan, Marsh, Hirschman, 1984</marker>
<rawString>Grishman, R.; Nhan, N. T.; Marsh, E.; and Hirschman, L. 1984 Automated Determination of Sublanguage Syntactic Usage. Proceedings of COLING84 (Tenth International Conference on Computational Linguistics). Stanford, California: 96-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
</authors>
<title>TEAM: A Transportable Natural-Language Interface System.</title>
<date>1983</date>
<booktitle>Proceedings of Conference on Applied Natural Language Processing.</booktitle>
<pages>39--45</pages>
<location>Santa Monica, California:</location>
<contexts>
<context position="15787" citStr="Grosz 1983" startWordPosition="2373" endWordPosition="2374">our studies of discovery procedures for domain-specific knowledge. 3 DISCOVERY PROCEDURES 3.1 EXPERT VS. TEXT-BASED PROCEDURES Two basic approaches have been proposed for mechanizing (or partially mechanizing) the acquisition of domainspecific information for natural language systems. One of these is based on the systematic interviewing of a domain expert, who provides information on the basic semantic classes and relations of the domain and their linguistic forms and properties. Such an approach has been incorporated into some natural language interfaces for database retrieval, such as TEAM (Grosz 1983) and LDC (Ballard, Lusth, and Tinkham 1984). This approach assumes that the domain expert has some model of the relations in the domain, and a knowledge of the different ways in which these relations can be referenced. This is not unreasonable in the database context, since the database schema can serve as a domain model (divining all the ways in which a relationship can be referenced may still be difficult, however). This approach is more difficult, however, in text analysis applications, particularly because the user may not have such a clear model of the domain semantics from which to work.</context>
</contexts>
<marker>Grosz, 1983</marker>
<rawString>Grosz, B. 1983 TEAM: A Transportable Natural-Language Interface System. Proceedings of Conference on Applied Natural Language Processing. Santa Monica, California: 39-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Mathematical Structures of Language.</title>
<date>1968</date>
<publisher>Wiley Interscience,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="4464" citStr="Harris 1968" startWordPosition="651" endWordPosition="652">83), medical reports (Hirschman and Sager 1983), and equipment failure reports (Marsh, Hamburger, and Grishman 1984). A sublanguage will generally be much more constrained than the &amp;quot;standard language&amp;quot;, but it may also include extensions to the standard language, such as sentence fragments found in telegraphic-style message text. Zellig Harris, one of the first linguists to study language use in restricted domains, defined sublanguages in terms of one particular constraint: the constraint on what words can co-occur within a particular syntactic pattern, such as a subject-verb-object structure (Harris 1968). Just as speakers of the standard language distinguish between grammatical and ungrammatical sentences, speakers of the sublanguage will distinguish between acceptable and unacceptable (meaningless) sentences, even though the unacceptable sentences may be grammatical sentences of the standard language. For example, in the sublanguage of medical records, a speaker would accept the sentence The X-ray revealed a tumor. but not The tumor revealed an X-ray. Harris hypothesized that, for any particular sublanguage, we can define sublanguage word classes — sets of words that are acceptable in the sa</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Z. 1968 Mathematical Structures of Language. Wiley Interscience, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>R Grishman</author>
<author>N Sager</author>
</authors>
<title>Grammaticallybased Automatic Word Class Formation.</title>
<date>1975</date>
<journal>Information Processing and Management</journal>
<volume>11</volume>
<pages>39--57</pages>
<marker>Hirschman, Grishman, Sager, 1975</marker>
<rawString>Hirschman, L.; Grishman, R.; and Sager, N. 1975 Grammaticallybased Automatic Word Class Formation. Information Processing and Management 11: 39-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>G Story</author>
<author>E Marsh</author>
<author>M Lyman</author>
<author>N Sager</author>
</authors>
<title>An Experiment in Automated Health Care Evaluation of Narrative Medical Records.</title>
<date>1981</date>
<journal>Computers and Biomedical Research</journal>
<volume>14</volume>
<pages>447--463</pages>
<contexts>
<context position="21512" citStr="Hirschman et al. 1981" startWordPosition="3270" endWordPosition="3273">sublanguage vocabulary had already been classified. Our test corpus consisted of eleven medical reports. Six were patient documents that included patient history, examination, and plan of treatment; five were hospital &amp;quot;discharge summaries&amp;quot; which included patient history, examination, and summaries of the course of treatment in the hospital. The corpus contained about 750 sentences and sentence fragments. In analyzing these sentences, we used the Linguistic String Project medical grammar, a modification of the LSP English grammar that had been used for processing a number of medical documents (Hirschman et al. 1981), including the discharge summaries in our corpus. The sublanguage word classes, which are recorded in our lexicon, had been developed based on the discharge summaries and other similar medical records. However, neither the grammar nor the word classes had been revised to reflect any new syntactic forms or semantic patterns that appeared in the other six patient documents; these documents were being analyzed for the first time. The discovery procedure for selectional patterns has five principal steps: 1. generating a set of correct parses; 2. resolving homographs; 3. generating instances of se</context>
</contexts>
<marker>Hirschman, Story, Marsh, Lyman, Sager, 1981</marker>
<rawString>Hirschman, L.; Story, G.; Marsh, E.; Lyman, M.; and N. Sager. 1981 An Experiment in Automated Health Care Evaluation of Narrative Medical Records. Computers and Biomedical Research 14: 447-463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>N Sager</author>
</authors>
<title>Automatic Information Formatting of a Medical Sublanguage.</title>
<date>1983</date>
<booktitle>In Kittredge and Lehrberger:</booktitle>
<pages>27--80</pages>
<contexts>
<context position="3899" citStr="Hirschman and Sager 1983" startWordPosition="567" endWordPosition="570"> on some experiments aimed at developing a semi-automated procedure for discovering selectional patterns (the local semantic constraints of language in a particular domain) from the analysis of a sample of text in that domain. 2 SUBLANGUAGE AND SELECTION 2.1 SUBLANGUAGE A sublanguage is a specialized form of natural language used to describe a limited subject matter, generally employed by a group of specialists dealing with this subject. Examples of sublanguages that have been studied are weather reports (Chevalier et al. 1978), aircraft maintenance manuals (Lehrberger 1983), medical reports (Hirschman and Sager 1983), and equipment failure reports (Marsh, Hamburger, and Grishman 1984). A sublanguage will generally be much more constrained than the &amp;quot;standard language&amp;quot;, but it may also include extensions to the standard language, such as sentence fragments found in telegraphic-style message text. Zellig Harris, one of the first linguists to study language use in restricted domains, defined sublanguages in terms of one particular constraint: the constraint on what words can co-occur within a particular syntactic pattern, such as a subject-verb-object structure (Harris 1968). Just as speakers of the standard </context>
</contexts>
<marker>Hirschman, Sager, 1983</marker>
<rawString>Hirschman, L. and Sager, N. 1983 Automatic Information Formatting of a Medical Sublanguage. In Kittredge and Lehrberger: 27-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kittredge</author>
<author>J Lehrberger</author>
<author>Eds</author>
</authors>
<date>1983</date>
<booktitle>Sublanguage: Studies of Language in Restricted Semantic Domains. Series of Foundations of Communications, Walter de Gruyter,</booktitle>
<pages>27--80</pages>
<location>Berlin:</location>
<marker>Kittredge, Lehrberger, Eds, 1983</marker>
<rawString>Kittredge, R. and Lehrberger, J., Eds. 1983 Sublanguage: Studies of Language in Restricted Semantic Domains. Series of Foundations of Communications, Walter de Gruyter, Berlin: 27-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lehrberger</author>
</authors>
<title>Automatic Translation and the Concept of Sublanguage.</title>
<date>1983</date>
<booktitle>In Kittredge and Lehrberger.</booktitle>
<contexts>
<context position="3855" citStr="Lehrberger 1983" startWordPosition="563" endWordPosition="564">erns issue. Specifically, we report on some experiments aimed at developing a semi-automated procedure for discovering selectional patterns (the local semantic constraints of language in a particular domain) from the analysis of a sample of text in that domain. 2 SUBLANGUAGE AND SELECTION 2.1 SUBLANGUAGE A sublanguage is a specialized form of natural language used to describe a limited subject matter, generally employed by a group of specialists dealing with this subject. Examples of sublanguages that have been studied are weather reports (Chevalier et al. 1978), aircraft maintenance manuals (Lehrberger 1983), medical reports (Hirschman and Sager 1983), and equipment failure reports (Marsh, Hamburger, and Grishman 1984). A sublanguage will generally be much more constrained than the &amp;quot;standard language&amp;quot;, but it may also include extensions to the standard language, such as sentence fragments found in telegraphic-style message text. Zellig Harris, one of the first linguists to study language use in restricted domains, defined sublanguages in terms of one particular constraint: the constraint on what words can co-occur within a particular syntactic pattern, such as a subject-verb-object structure (Har</context>
</contexts>
<marker>Lehrberger, 1983</marker>
<rawString>Lehrberger, J. 1983 Automatic Translation and the Concept of Sublanguage. In Kittredge and Lehrberger.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsh</author>
</authors>
<title>Utilizing Domain-Specific Information for Processing Compact Text,.</title>
<date>1983</date>
<booktitle>Proceedings of Conference on Applied Natural Language Processing.</booktitle>
<pages>99--103</pages>
<location>Santa Monica, California:</location>
<contexts>
<context position="11670" citStr="Marsh 1983" startWordPosition="1734" endWordPosition="1735">xample). In order to obtain a more objective measure of the importance of these constraints, we conducted an experiment comparing the effectiveness of grammars with and without selectional constraints. The test corpus was a set of hospital discharge summaries containing 407 sentences and sentence fragments. We used the NYU Linguistic String Project medical grammar, which is a modification of the Linguistic String Project English grammar including the sentence fragments and other constructs (such as descriptions of medication dosages) that appear in medical reports but not in standard English (Marsh 1983). Each sentence was analyzed twice, once without any selectional constraints and once with selectional constraints (the selectional patterns were developed manually at NYU by linguists from a study of this test corpus and other similar medical reports). The results of each analysis were clasComputational Linguistics, Volume 12, Number 3, July-September 1986 sified into one of three categories: no parses obtained; one or more parses obtained, good first parse;1 one or more parses obtained, bad first parse. These results are summarized in Table 1. with selection without selection good parses 308</context>
</contexts>
<marker>Marsh, 1983</marker>
<rawString>Marsh, E. 1983 Utilizing Domain-Specific Information for Processing Compact Text,. Proceedings of Conference on Applied Natural Language Processing. Santa Monica, California: 99-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsh</author>
<author>H Hamburger</author>
<author>R Grishman</author>
</authors>
<title>A Production Rule System for Message Summarization.</title>
<date>1984</date>
<booktitle>Proceedings of the 1984 National Conference on Artificial Intelligence.</booktitle>
<location>Austin, Texas.</location>
<marker>Marsh, Hamburger, Grishman, 1984</marker>
<rawString>Marsh, E.; Hamburger, H.; and Grishman, R. 1984 A Production Rule System for Message Summarization. Proceedings of the 1984 National Conference on Artificial Intelligence. Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sager</author>
</authors>
<title>Natural Language Information Processing: A Computer Grammar of English and its Applications.</title>
<date>1981</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="6262" citStr="Sager 1981" startWordPosition="917" endWordPosition="918"> when stated between word classes, are commonly called selectional constraints. 2.2 IMPLEMENTING SELECTIONAL CONSTRAINTS It is generally recognized that these selectional constraints play an important role in distinguishing between correct and incorrect sentence analyses. Consequently, most natural language systems incorporate some form of selectional constraints. We describe here, briefly, how these constraints are implemented in the Linguistic String Parser; more detailed descriptions are given in Grishman, Hirschman, and Friedman (1982, 1983) . The Linguistic String Parser English grammar (Sager 1981) is an augmented context-free grammar. Its principal components are a context-free grammar (stated in terms of the grammatical categories of English), a set of procedural restrictions (written in Restriction Language: Sager and Grishman 1975), and a lexicon. Adding selectional constraints to this grammar involved specifying the sublanguage classifications of words, specifying the allowed co-occurrence patterns in positive terms, and providing restrictions that check the parse tree for these patterns. Each word in the domain vocabulary is assigned to one or more sublanguage word classes; these </context>
</contexts>
<marker>Sager, 1981</marker>
<rawString>Sager, N. 1981 Natural Language Information Processing: A Computer Grammar of English and its Applications. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sager</author>
<author>R Grishman</author>
</authors>
<title>The Restriction Language for Computer Grammars of Natural Language.</title>
<date>1975</date>
<journal>Communications ACM</journal>
<volume>18</volume>
<pages>390--400</pages>
<contexts>
<context position="6504" citStr="Sager and Grishman 1975" startWordPosition="950" endWordPosition="953">en correct and incorrect sentence analyses. Consequently, most natural language systems incorporate some form of selectional constraints. We describe here, briefly, how these constraints are implemented in the Linguistic String Parser; more detailed descriptions are given in Grishman, Hirschman, and Friedman (1982, 1983) . The Linguistic String Parser English grammar (Sager 1981) is an augmented context-free grammar. Its principal components are a context-free grammar (stated in terms of the grammatical categories of English), a set of procedural restrictions (written in Restriction Language: Sager and Grishman 1975), and a lexicon. Adding selectional constraints to this grammar involved specifying the sublanguage classifications of words, specifying the allowed co-occurrence patterns in positive terms, and providing restrictions that check the parse tree for these patterns. Each word in the domain vocabulary is assigned to one or more sublanguage word classes; these class assignments are recorded as part of the lexical entry for each word. Thus a lexical entry consists of each major syntactic class for a word followed by a list of attributes, including its domain subclass. Words that have two or more mea</context>
</contexts>
<marker>Sager, Grishman, 1975</marker>
<rawString>Sager, N. and Grishman, R. 1975 The Restriction Language for Computer Grammars of Natural Language. Communications ACM 18: 390-400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
<author>N K Sondheimer</author>
</authors>
<title>Meta-rules as a Basis for Processing Ill-Formed Input.</title>
<date>1983</date>
<journal>Journal of Computational Linguistics</journal>
<volume>9</volume>
<pages>161--177</pages>
<contexts>
<context position="39519" citStr="Weischedel and Sondheimer 1983" startWordPosition="6120" endWordPosition="6123">yet been seen), any automatically generated set of patterns will over-constrain the parsing. We therefore sought some way of automatically compensating for this incompleteness. The approach we chose to try was restriction relaxation. If no parse can be obtained satisfying all selectional constraints, the parser tries for an analysis that will satisfy all but one of the selectional constraints.3 In effect, the parser is willing to relax any one of the selectional constraints in order to get an analysis. Such an approach has been suggested before by several computational linguists (for example, Weischedel and Sondheimer 1983), although primarily to account for ungrammatical input rather than for incompleteness of semantic knowledge. We originally applied this technique in connection with the manually generated selectional patterns. These results were not very encouraging: about 5% of the sentences in the sample that had previously gotten no parse now got a correct parse, but another 5% got a bad parse. This was not too surprising in retrospect; the various measures mentioned above to compensate for the incompleteness of the patterns resulted in a set of relatively &amp;quot;loose&amp;quot; constraints, and any further loosening (su</context>
</contexts>
<marker>Weischedel, Sondheimer, 1983</marker>
<rawString>Weischedel, R. M. and Sondheimer, N. K. 1983 Meta-rules as a Basis for Processing Ill-Formed Input. Journal of Computational Linguistics 9: 161-177.</rawString>
</citation>
<citation valid="false">
<title>Good parses&amp;quot; included some parses that were not entirely correct but that were good enough so they did not cause errors in the process that converted the parsed trees into information formats (a structured data base).</title>
<marker></marker>
<rawString>1. &amp;quot;Good parses&amp;quot; included some parses that were not entirely correct but that were good enough so they did not cause errors in the process that converted the parsed trees into information formats (a structured data base).</rawString>
</citation>
<citation valid="false">
<title>The curves are rather jagged because the reports are divided into sections containing different types of information; when we begin processing a new section, new patterns are encountered, and there is therefore a sharp rise in the growth curves.</title>
<marker></marker>
<rawString>2. The curves are rather jagged because the reports are divided into sections containing different types of information; when we begin processing a new section, new patterns are encountered, and there is therefore a sharp rise in the growth curves.</rawString>
</citation>
<citation valid="false">
<title>If no analysis can be obtained by relaxing one restriction, the parser is able to try for an analysis that relaxes two, three, or more restrictions. Our experiments have indicated, however, that relaxing more than one restriction produced bad parses more often than good ones. 4. Private communication with</title>
<marker></marker>
<rawString>3. If no analysis can be obtained by relaxing one restriction, the parser is able to try for an analysis that relaxes two, three, or more restrictions. Our experiments have indicated, however, that relaxing more than one restriction produced bad parses more often than good ones. 4. Private communication with M. Bates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Computational Linguistics</author>
</authors>
<date>1986</date>
<volume>12</volume>
<pages>215</pages>
<marker>Linguistics, 1986</marker>
<rawString>Computational Linguistics, Volume 12, Number 3, July-September 1986 215</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>