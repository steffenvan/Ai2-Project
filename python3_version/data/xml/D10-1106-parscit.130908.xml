<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.981206">
Learning First-Order Horn Clauses from Web Text
</title>
<author confidence="0.991746">
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld
</author>
<affiliation confidence="0.968823333333333">
Turing Center
University of Washington
Computer Science and Engineering
</affiliation>
<address confidence="0.9812085">
Box 352350
Seattle, WA 98125, USA
</address>
<email confidence="0.999317">
stef,etzioni,weld@cs.washington.edu
</email>
<author confidence="0.968972">
Jesse Davis
</author>
<affiliation confidence="0.9756955">
Katholieke Universiteit Leuven
Department of Computer Science
</affiliation>
<address confidence="0.823163">
POBox 02402 Celestijnenlaan 200a
B-3001 Heverlee, Belgium
</address>
<email confidence="0.998325">
jesse.davis@cs.kuleuven.be
</email>
<sectionHeader confidence="0.993867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99975452631579">
Even the entire Web corpus does not explic-
itly answer all questions, yet inference can un-
cover many implicit answers. But where do
inference rules come from?
This paper investigates the problem of learn-
ing inference rules from Web text in an un-
supervised, domain-independent manner. The
SHERLOCK system, described herein, is a
first-order learner that acquires over 30,000
Horn clauses from Web text. SHERLOCK em-
bodies several innovations, including a novel
rule scoring function based on Statistical Rel-
evance (Salmon et al., 1971) which is effec-
tive on ambiguous, noisy and incomplete Web
extractions. Our experiments show that in-
ference over the learned rules discovers three
times as many facts (at precision 0.8) as the
TEXTRUNNER system which merely extracts
facts explicitly stated in Web text.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999757266666667">
Today’s Web search engines locate pages that match
keyword queries. Even sophisticated Web-based
Q/A systems merely locate pages that contain an ex-
plicit answer to a question. These systems are help-
less if the answer has to be inferred from multiple
sentences, possibly on different pages. To solve this
problem, Schoenmackers et al.(2008) introduced the
HOLMES system, which infers answers from tuples
extracted from text.
HOLMES’s distinction is that it is domain inde-
pendent and that its inference time is linear in the
size of its input corpus, which enables it to scale to
the Web. However, HOLMES’s Achilles heel is that
it requires hand-coded, first-order, Horn clauses as
input. Thus, while HOLMES’s inference run time
is highly scalable, it requires substantial labor and
expertise to hand-craft the appropriate set of Horn
clauses for each new domain.
Is it possible to learn effective first-order Horn
clauses automatically from Web text in a domain-
independent and scalable manner? We refer to the
set of ground facts derived from Web text as open-
domain theories. Learning Horn clauses has been
studied extensively in the Inductive Logic Program-
ming (ILP) literature (Quinlan, 1990; Muggleton,
1995). However, learning Horn clauses from open-
domain theories is particularly challenging for sev-
eral reasons. First, the theories denote instances of
an unbounded and unknown set of relations. Sec-
ond, the ground facts in the theories are noisy, and
incomplete. Negative examples are mostly absent,
and certainly we cannot make the closed-world as-
sumption typically made by ILP systems. Finally,
the names used to denote both entities and relations
are rife with both synonyms and polysymes making
their referents ambiguous and resulting in a particu-
larly noisy and ambiguous set of ground facts.
This paper presents a new ILP method, which is
optimized to operate on open-domain theories de-
rived from massive and diverse corpora such as the
Web, and experimentally confirms both its effective-
ness and superiority over traditional ILP algorithms
in this context. Table 1 shows some example rules
that were learned by SHERLOCK.
This work makes the following contributions:
</bodyText>
<footnote confidence="0.60432825">
1. We describe the design and implementation of
the SHERLOCK system, which utilizes a novel,
unsupervised ILP method to learn first-order
Horn clauses from open-domain Web text.
</footnote>
<page confidence="0.946049">
1088
</page>
<note confidence="0.8799705">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1088–1098,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.604097375">
IsHeadquarteredIn(Company, State) .-
IsBasedIn(Company, City) ∧ IsLocatedIn(City, State);
Contains(Food, Chemical) .-
IsMadeFrom(Food, Ingredient) ∧ Contains(Ingredient, Chemical);
Reduce(Medication, Factor) .-
KnownGenericallyAs(Medication, Drug) ∧ Reduce(Drug, Factor);
ReturnTo(Writer, Place) .- BornIn(Writer, City) ∧ CapitalOf(City, Place);
Make(Company1, Device) .- Buy(Company1, Company2) ∧ Make(Company2, Device);
</figure>
<tableCaption confidence="0.99613">
Table 1: Example rules learned by SHERLOCK from Web extractions. Note that the italicized rules are unsound.
</tableCaption>
<bodyText confidence="0.862769176470588">
2. We derive an innovative scoring function that is
particularly well-suited to unsupervised learn-
ing from noisy text. For Web text, the scoring
function yields more accurate rules than several
functions from the ILP literature.
3. We demonstrate the utility of SHERLOCK’s
automatically learned inference rules. Infer-
ence using SHERLOCK’s learned rules identi-
fies three times as many high quality facts (e.g.,
precision &gt; 0.8) as were originally extracted
from the Web text corpus.
The remainder of this paper is organized as fol-
lows. We start by describing previous work. Sec-
tion 3 introduces the SHERLOCK rule learning sys-
tem, with Section 3.4 describing how it estimates
rule quality. We empirically evaluate SHERLOCK in
Section 4, and conclude.
</bodyText>
<sectionHeader confidence="0.996191" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999874692307692">
SHERLOCK is one of the first systems to learn first-
order Horn clauses from open-domain Web extrac-
tions. The learning method in SHERLOCK belongs
to the Inductive logic programming (ILP) subfield
of machine learning (Lavrac and Dzeroski, 2001).
However, classical ILP systems (e.g., FOIL (Quin-
lan, 1990) and Progol (Muggleton, 1995)) make
strong assumptions that are inappropriate for open
domains. First, ILP systems assume high-quality,
hand-labeled training examples for each relation of
interest. Second, ILP systems assume that constants
uniquely denote individuals; however, in Web text
strings such as “dad” or “John Smith” are highly
ambiguous. Third, ILP system typically assume
complete, largely noise-free data whereas tuples ex-
tracted from Web text are both noisy and radically
incomplete. Finally, ILP systems typically utilize
negative examples, which are not available when
learning from open-domain facts. One system that
does not require negative examples is LIME (Mc-
Creath and Sharma, 1997); We compare SHERLOCK
with LIME’s methods in Section 4.3. Most prior ILP
and Markov logic structure learning systems (e.g.,
(Kok and Domingos, 2005)) are not designed to han-
dle the noise and incompleteness of open-domain,
extracted facts.
NELL (Carlson et al., 2010) performs coupled
semi-supervised learning to extract a large knowl-
edge base of instances, relations, and inference
rules, bootstrapping from a few seed examples of
each class and relation of interest and a few con-
straints among them. In contrast, SHERLOCK fo-
cuses mainly on learning inference rules, but does so
without any manually specified seeds or constraints.
Craven etal.(1998) also used ILP to help infor-
mation extraction on the Web, but required training
examples and focused on a single domain.
Two other notable systems that learn inference
rules from text are DIRT (Lin and Pantel, 2001)
and RESOLVER (Yates and Etzioni, 2007). How-
ever, both DIRT and RESOLVER learn only a lim-
ited set of rules capturing synonyms, paraphrases,
and simple entailments, not more expressive multi-
part Horn clauses. For example, these systems may
learn the rule X acquired Y ==&gt;. X bought Y ,
which captures different ways of describing a pur-
chase. Applications of these rules often depend on
context (e.g., if a person acquires a skill, that does
not mean they bought the skill). To add the neces-
sary context, ISP (Pantel et al., 2007) learned selec-
tional preferences (Resnik, 1997) for DIRT’s rules.
The selectional preferences act as type restrictions
</bodyText>
<page confidence="0.998827">
1089
</page>
<figureCaption confidence="0.996476333333333">
Figure 1: Architecture of SHERLOCK. SHERLOCK learns
inference rules offline and provides them to the HOLMES
inference engine, which uses the rules to answer queries.
</figureCaption>
<bodyText confidence="0.999924529411764">
on the arguments, and attempt to filter out incorrect
inferences. While these approaches are useful, they
are strictly more limited than the rules learned by
SHERLOCK.
The Recognizing Textual Entailment (RTE)
task (Dagan et al., 2005) is to determine whether
one sentence entails another. Approaches to RTE
include those of Tatu and Moldovan (2007), which
generates inference rules from WordNet lexical
chains and a set of axiom templates, and Pennac-
chiotti and Zanzotto (2007), which learns inference
rules based on similarity across entailment pairs. In
contrast with this work, RTE systems reason over
full sentences, but benefit by being given the sen-
tences and training data. SHERLOCK operates over
simpler Web extractions, but is not given guidance
about which facts may interact.
</bodyText>
<sectionHeader confidence="0.989457" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.848735142857143">
SHERLOCK takes as input a large set of open domain
facts, and returns a set of weighted Horn-clause in-
ference rules. Other systems (e.g., HOLMES) use the
rules to answer questions, infer additional facts, etc.
SHERLOCK’s basic architecture is depicted in
Figure 1. To learn inference rules, SHERLOCK per-
forms the following steps:
</bodyText>
<listItem confidence="0.9888806">
1. Identify a “productive” set of classes and in-
stances of those classes
2. Discover relations between classes
3. Learn inference rules using the discovered rela-
tions and determine the confidence in each rule
</listItem>
<bodyText confidence="0.999917571428571">
The first two steps help deal with the synonyms,
homonyms, and noise present in open-domain the-
ories by identifying a smaller, cleaner, and more co-
hesive set of facts to learn rules over.
SHERLOCK learns inference rules from a collec-
tion of open-domain extractions produced by TEX-
TRUNNER (Banko et al., 2007). The rules learned
by SHERLOCK are input to an inference engine and
used to find answers to a user’s query. In this paper,
SHERLOCK utilizes HOLMES as its inference engine
when answering queries, and uses extracted facts
of the form R(arg1, arg2) provided by the authors
of TEXTRUNNER, but the techniques presented are
more broadly applicable.
</bodyText>
<subsectionHeader confidence="0.999915">
3.1 Finding Classes and Instances
</subsectionHeader>
<bodyText confidence="0.999978387096774">
SHERLOCK first searches for a set of well-defined
classes and class instances. Instances of the same
class tend to behave similarly, so identifying a good
set of instances will make it easier to discover the
general properties of the entire class.
Options for identifying interesting classes include
manually created methods (WordNet (Miller et al.,
1990)), textual patterns (Hearst, 1992), automated
clustering (Lin and Pantel, 2002), and combina-
tions (Snow et al., 2006). We use Hearst patterns
because they are simple, capture how classes and in-
stances are mentioned in Web text, and yield intu-
itive, explainable groups.
Hearst (1992) identified a set of textual patterns
which indicate hyponymy (e.g., ‘Class such as In-
stance’). Using these patterns, we extracted 29 mil-
lion (instance, class) pairs from a large Web crawl.
We then cleaned them using word stemming, nor-
malization, and by dropping modifiers.
Unfortunately, the patterns make systematic er-
rors (e.g., extracting Canada as the name of a city
from the phrase ‘Toronto, Canada and other cities.’)
To address this issue, we discard the low frequency
classes of each instance. This heuristic reduces the
noise due to systematic error while still capturing the
important senses of each word. Additionally, we use
the extraction frequency to estimate the probability
that a particular mention of an instance refers to each
of its potential classes (e.g., New York appears as a
city 40% of the time, a state 35% of the time, and a
place, area, or center the rest of the time).
</bodyText>
<page confidence="0.957478">
1090
</page>
<bodyText confidence="0.999965652173913">
Ambiguity presents a significant obstacle when
learning inference rules. For example, the corpus
contains the sentences ‘broccoli contains this vita-
min’ and ‘this vitamin prevents scurvy,’ but it is un-
clear if the sentences refer to the same vitamin. The
two main sources of ambiguity we observed are ref-
erences to a more general class instead of a specific
instance (e.g., ‘vitamin’), and references to a person
by only their first or last name. We eliminate the
first by removing terms that frequently appear as the
class name with other instances, and the second by
removing common first and last names.
The 250 most frequently mentioned class names
include a large number of interesting classes (e.g.,
companies, cities, foods, nutrients, locations) as
well as ambiguous concepts (e.g., ideas, things). We
focus on the less ambiguous classes by eliminating
any class not appearing as a descendant of physical
entity, social group, physical condition, or event in
WordNet. Beyond this filtering we make no use of a
type hierarchy and treat classes independently.
In our corpus, we identify 1.1 million distinct,
cleaned (instance, class) pairs for 156 classes.
</bodyText>
<subsectionHeader confidence="0.999837">
3.2 Discovering Relations between Classes
</subsectionHeader>
<bodyText confidence="0.999663428571428">
Next, SHERLOCK discovers how classes relate to
and interact with each other. Prior work in relation
discovery (Shinyama and Sekine, 2006) has investi-
gated the problem of finding relationships between
different classes. However, the goal of this work is
to learn rules on top of the discovered typed rela-
tions. We use a few simple heuristics to automati-
cally identify interesting relations.
For every pair of classes (C1, C2), we find a set
of typed, candidate relations from the 100 most fre-
quent relations in the corpus where the first argu-
ment is an instance of C1 and the second argument
is an instance of C2. For extraction terms with mul-
tiple senses (e.g., New York), we split their weight
based on how frequently they appear with each class
in the Hearst patterns.
However, many discovered relations are rare and
meaningless, arising from either an extraction error
or word-sense ambiguity. For example, the extrac-
tion ‘Apple is based in Cupertino’ gives some evi-
dence that a fruit may possibly be based in a city.
We attempt to filter out incorrectly-typed relations
using two heuristics. We first discard any relation
whose weighted frequency falls below a threshold,
since rare relations are more likely to arise due to
extraction errors or word-sense ambiguity. We also
remove relations whose pointwise mutual informa-
tion (PMI) is below a threshold T=exp(2) Pz� 7.4:
</bodyText>
<equation confidence="0.999274333333333">
p(R, C1, C2)
PMI(R(C1, C2)) =
p(R, ·, ·) * p(·, C1, ·) * p(·, ·, C2)
</equation>
<bodyText confidence="0.999997133333334">
where p(R, ·,·) is the probability a random extrac-
tion has relation R, p(·, C1, ·) is the probability a
random extraction has an instance of C1 as its first
argument, p(·, ·, C2) is similar for the second argu-
ment, and p(R, C1, C2) is the probability that a ran-
dom extraction has relation R and instances of C1
and C2 as its first and second arguments, respec-
tively. A low PMI indicates the relation occurred by
random chance, which is typically due to ambiguous
terms or extraction errors.
Finally, we use two TEXTRUNNER specific clean-
ing heuristics: we ignore a small set of stop-relations
(‘be’, ‘have’, and ‘be preposition’) and extractions
whose arguments are more than four tokens apart.
This process identifies 10,000 typed relations.
</bodyText>
<subsectionHeader confidence="0.999788">
3.3 Learning Inference Rules
</subsectionHeader>
<bodyText confidence="0.999167125">
SHERLOCK attempts to learn inference rules for
each typed relation in turn. SHERLOCK receives a
target relation, R, a set of observed examples of the
relation, E+, a maximum clause length k, a mini-
mum support, s, and an acceptance threshold, t, as
input. SHERLOCK generates all first-order, definite
clauses up to length k, where R appears as the head
of the clause. It retains each clause that:
</bodyText>
<listItem confidence="0.999762">
1. Contains no unbound variables
2. Infers at least s examples from E+
3. Scores at least t according to the score function
</listItem>
<bodyText confidence="0.99942">
We now propose a novel score function, and empir-
ically validate our choice in Sections 4.3 and 4.4.
</bodyText>
<subsectionHeader confidence="0.993684">
3.4 Evaluating Rules by Statistical Relevance
</subsectionHeader>
<bodyText confidence="0.999993166666667">
The problem of evaluating candidate rules has been
studied by many researchers, but typically in either a
supervised or propositional context whereas we are
learning first-order Horn-clauses from a noisy set of
positive examples. Moreover, due to the incomplete
nature of the input corpus and the imperfect yield of
</bodyText>
<page confidence="0.96325">
1091
</page>
<bodyText confidence="0.999863796875">
extraction—many true facts are not stated explicitly
in the set of ground assertions used by the learner to
evaluate rules.
The absence of negative examples, coupled with
noise, means that standard ILP evaluation functions
(e.g., (Quinlan, 1990) and (Dzeroski and Bratko,
1992)) are not appropriate. Furthermore, when eval-
uating a particular rule with consequent C and an-
tecedent A, it is natural to consider p(C|A) but, due
to missing data, this absolute probability estimate is
often misleading: in many cases C will hold given
A but the fact C is not mentioned in the corpus.
Thus to evaluate rules over extractions, we need
to consider relative probability estimates. I.e., is
p(C|A) » p(C)? If so, then A is said to be sta-
tistically relevant to C (Salmon et al., 1971).
Statistical relevance tries to infer the simplest set
of factors which explain an observation. It can be
viewed as searching for the simplest propositional
Horn-clause which increases the likelihood of a goal
proposition g. The two key ideas in determining sta-
tistical relevance are discovering factors which sub-
stantially increase the likelihood of g (even if the
probabilities are small in an absolute sense), and dis-
missing irrelevant factors.
To illustrate these concepts, consider the follow-
ing example. Suppose our goal is to predict if New
York City will have a storm (S). On an arbitrary
day, the probability of having a storm is fairly low
(p(S) « 1). However, if we know that the atmo-
spheric pressure on that day is low, this substantially
increases the probability of having a storm (although
that absolute probability may still be small). Ac-
cording to the principle of statistical relevance, low
atmospheric pressure (LP) is a factor which predicts
storms (S :- LP), since p(S|LP) » p(S) .
The principle of statistical relevance also identi-
fies and removes irrelevant factors. For example, let
M denote the gender of New York’s mayor. Since
p(S|LP, M) » p(S), it naively appears that storms
in New York depend on the gender of the mayor in
addition to the air pressure. The statistical relevance
principle sidesteps this trap by removing any fac-
tors which are conditionally independent of the goal,
given the remaining factors. For example, we ob-
serve p(S|LP)=p(S|LP, M), and so we say that M
is not statistically relevant to S. This test applies Oc-
cam’s razor by searching for the simplest rule which
explains the goal.
Statistical relevance appears useful in the open-
domain context, since all the necessary probabilities
can be estimated from only positive examples. Fur-
thermore, approximating relative probabilities in the
presence of missing data is much more reliable than
determining absolute probabilities.
Unfortunately, Salmon defined statistical rele-
vance in a propositional context. One technical
contribution of our work is to lift statistical rele-
vance to first order Horn-clauses as follows. For
the Horn-clause Head(v1, ..., vn):-Body(v1, ..., v.)
(where Body(v1, ..., v.) is a conjunction of function-
free, non-negated, first-order relations, and vi E V
is the set of typed variables used in the rule), we say
the body helps explain the head if:
</bodyText>
<listItem confidence="0.9285895">
1. Observing an instance of the body substantially
increases the probability of observing the head.
2. The body contains no irrelevant (conditionally
independent) terms.
</listItem>
<bodyText confidence="0.95303608">
We evaluate conditional independence of terms
using ILP’s technique of O-subsumption, ensuring
there is no more general clause that is similarly
predictive of the head. Formally, clause C1 O-
subsumes clause C2 if and only if there exists a sub-
stitution O such that C1O C_ C2 where each clause is
treated as the set of its literals. For example, R(x, y)
O-subsumes R(x, x), since {R(x, y)}O C_ {R(x, x)}
when O={y/x}. Intuitively, if C1 O-subsumes C2,
it means that C1 is more general than C2.
Definition 1 A first-order Horn-clause
Head(...):-Body(...) is statistically relevant if
p(Head(...)|Body(...)) » p(Head(...)) and if there
is no clause body B&apos;(...)O C_ Body(...) such that
p(Head(...)|Body(...)) Pz� p(Head(...)|B&apos;(...))
In practice it is difficult to determine the proba-
bilities exactly, so when checking for statistical rele-
vance we ensure that the probability of the rule is at
least a factor t greater than the probability of any
subsuming rule, that is, p(Head(...)|Body(...)) &gt;
t * p(Head(...)|B&apos;(...))
We estimate p(Head(...)|B(...)) from the observed
facts by assuming values of Head(...) are generated
by sampling values of B(...) as follows: for variables
vs shared between Head(...) and B(...), we sample
</bodyText>
<page confidence="0.982219">
1092
</page>
<bodyText confidence="0.999807290322581">
values of vs uniformly from all observed ground-
ings of B(...). For variables vi, if any, that appear
in Head(...) but not in B(...), we sample their values
according to a distribution p(vi|classi). We estimate
p(vi|classi) based on the relative frequency that vi
was extracted using a Hearst pattern with classi.
Finally, we ensure the differences are statistically
significant using the likelihood ratio statistic:
where p(-Head(...)|B(...)) = 1−p(Head(...)|B(...))
and Nr is the number of results inferred by the
rule Head(...):-Body(...). This test is distributed ap-
proximately as x2 with one degree of freedom. It
is similar to the statistical significance test used in
mFOIL (Dzeroski and Bratko, 1992), but has two
modifications since SHERLOCK doesn’t have train-
ing data. In lieu of positive and negative examples,
we use whether or not the inferred head value was
observed, and compare against the distribution of a
subsuming clause B&apos;(...) rather than a known prior.
This method of evaluating rules has two impor-
tant differences from ILP under a closed world as-
sumption. First, our probability estimates consider
the fact that examples provide varying amounts of
information. Second, statistical relevance finds rules
with large increases in relative probability, not nec-
essarily a large absolute probability. This is crucial
in an open domain setting where most facts are false,
which means the trivial rule that everything is false
will have high accuracy. Even for true rules, the ob-
served estimates p(Head(...)|Body(...)) « 1 due to
missing data and noise.
</bodyText>
<subsectionHeader confidence="0.972942">
3.5 Making Inferences
</subsectionHeader>
<bodyText confidence="0.936329034482759">
In order to benefit from learned rules, we need
an inference engine; with its linear-time scalabil-
ity, HOLMES is a natural choice (Schoenmackers
et al., 2008). As input HOLMES requires a target
atom H(...), an evidence set E and weighted rule
set R as input. It performs a form of knowledge
based model construction (Wellman et al., 1992),
first finding facts using logical inference, then esti-
mating the confidence of each using a Markov Logic
Network (Richardson and Domingos, 2006).
Prior to running inference, it is necessary to assign
a weight to each rule learned by SHERLOCK. Since
the rules and inferences are based on a set of noisy
and incomplete extractions, the algorithms used for
both weight learning and inference should capture
the following characteristics of our problem:
C1. Any arbitrary unknown fact is highly unlikely
to be true.
C2. The more frequently a fact is extracted from the
Web, the more likely it is to be true. However,
facts in E should have a confidence bounded
by a threshold pma, &lt; 1. E contains system-
atic extraction errors, so we want uncertainty in
even the most frequently extracted facts.
C3. An inference that combines uncertain facts
should be less likely than each fact it uses.
Next, we describe the needed modifications to the
weight learning and inference algorithm to achieve
the desired behavior.
</bodyText>
<subsectionHeader confidence="0.861238">
3.5.1 Weight Learning
</subsectionHeader>
<bodyText confidence="0.999987125">
We use the discriminative weight learning proce-
dure described by Huynh and Mooney (2008). Set-
ting the weights involves counting the number of
true groundings for each rule in the data (Richard-
son and Domingos, 2006). However, the noisy na-
ture of Web extractions will make this an overesti-
mate. Consequently, we compute ni(E), the number
of true groundings of rule i, as follows:
</bodyText>
<equation confidence="0.967897">
�ni(E) =
j
</equation>
<bodyText confidence="0.9999758">
where E is the evidence, j ranges over heads of the
rule, Bodyijk is the body of the kth grounding for
jth head of rule i, and p(B(...)) is approximated us-
ing a logistic function of the number of times B(...)
was extracted,1 scaled to be in the range [0,0.75].
This models C2 by giving increasing but bounded
confidence for more frequently extracted facts. In
practice, this also helps address C3 by giving longer
rules smaller values of ni, which reflects that infer-
ences arrived at through a combination of multiple,
noisy facts should have lower confidence. Longer
rules are also more likely to have multiple ground-
ings that infer a particular head, so keeping only the
most likely grounding prevents a head from receiv-
ing undue weight from any single rule.
</bodyText>
<footnote confidence="0.980323">
1We note that this approximation is equivalent to an MLN
which uses only the two rules defined in 3.5.2
</footnote>
<figure confidence="0.873996125">
�
2N�
H(...)E
{Head(...), Head(...)}
p(H(...)|Body(...)) � log p(H(...)|Body(...))
p(H(...)|B�(...))
max ri p(B(...)) (1)
k B(...)EBodyzjk
</figure>
<page confidence="0.97095">
1093
</page>
<bodyText confidence="0.999997">
Finally, we place a very strong Gaussian prior
(i.e., L2 penalty) on the weights. Longer rules have a
higher prior to capture the notion that they are more
likely to make incorrect inferences. Without a high
prior, each rule would receive an unduly high weight
as we have no negative examples.
</bodyText>
<subsectionHeader confidence="0.488706">
3.5.2 Probabilistic Inference
</subsectionHeader>
<bodyText confidence="0.999841">
After learning the weights, we add the following
two rules to our rule set:
</bodyText>
<listItem confidence="0.921112333333333">
1. H(...) with negative weight wprior
2. H(...):-ExtractedFrom(H(...),sentencei)
with weight 1
</listItem>
<bodyText confidence="0.99937275">
The first rule models C1, by saying that most facts
are false. The second rule models C2, by stating the
probability of fact depends on the number of times it
was extracted. The weights of these rules are fixed.
We do not include these rules during weight learning
as doing so swamps the effects of the other inference
rules (i.e., forces them to zero).
HOLMES attempts to infer the truth value of each
ground atom H(...) in turn by treating all other ex-
tractions E in our corpus as evidence. Inference also
requires computing ni(E) which we do according to
Equation 1 as in weight learning.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999411125">
One can attempt to evaluate a rule learner by esti-
mating the quality of learned rules, or by measuring
their impact on a system that uses the learned rules.
Since the notion of ‘rule quality’ is vague except
in the context of an application, we evaluate SHER-
LOCK in the context of the HOLMES inference-based
question answering system.
Our evaluation focuses on three main questions:
</bodyText>
<listItem confidence="0.993984">
1. Does inference utilizing learned Horn rules im-
prove the precision/recall of question answer-
ing and by how much?
2. How do different rule-scoring functions affect
the performance of learning?
3. What role does each of SHERLOCK’s compo-
nents have in the resulting performance?
</listItem>
<subsectionHeader confidence="0.971454">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.996576214285714">
Our objective with rule learning was to improve the
system’s ability to answer questions such as ‘What
foods prevent disease?’ So we focus our evaluation
on the task of computing as many instances as pos-
sible of an atomic pattern Rel(x, y). In this exam-
ple, Rel would be bound to ‘Prevents’, x would have
type ‘Food’ and y would have type ‘Disease.’
But which relations should be used in the test?
There is a large variance in behavior across relations,
so examining any particular relation may give mis-
leading results. Instead, we examine the global per-
formance of the system by querying HOLMES for
all open-domain relations identified in Section 3.2
as follows:
</bodyText>
<listItem confidence="0.981865375">
1. Score all candidate rules according to the rule
scoring metric M, accept all rules with a score
at least tM (tuned on a small development set of
rules), and learn weights for all accepted rules.
2. Find all facts inferred by the rules and use the
rule weights to estimate the fact probabilities.
3. Reduce type information. For each fact, (e.g.,
BasedIn(Diebold, Ohio)) which has been de-
duced with multiple type signatures (e.g., Ohio
is both a state and a geographic location), keep
only the one with maximum probability (i.e.,
conservatively assuming dependence).
4. Place all results into bins based on their proba-
bilities, and estimate the precision and the num-
ber of correct facts in the bin using a random
sample.
</listItem>
<bodyText confidence="0.999802916666667">
In these experiments we consider rules with up to
k = 2 relations in the body. We use a corpus of
1 million raw extractions, corresponding to 250,000
distinct facts. SHERLOCK found 5 million candidate
rules that infer at least two of the observed facts. Un-
less otherwise noted, we use SHERLOCK’s rule scor-
ing function to evaluate the rules (Section 3.4).
The results represent a wide variety of domains,
covering a total of 10,672 typed relations. We ob-
serve between a dozen and 2,375 distinct, ground
facts for each relation. SHERLOCK learned a total
of 31,000 inference rules.2 Learning all rules, rule
</bodyText>
<footnote confidence="0.991916">
2The learned rules are available at:
http://www.cs.washington.edu/research/sherlock-hornclauses/
</footnote>
<page confidence="0.995163">
1094
</page>
<figure confidence="0.983801666666667">
Benefits of Inference using Learned Rules
0 350000 700000 1050000 1400000
Estimated Number of Correct Facts
</figure>
<figureCaption confidence="0.995488428571428">
Figure 2: Inference discovers many facts which are not
explicitly extracted, identifying 3x as many high quality
facts (precision 0.8) and more than 5x as many facts over-
all. Horn-clauses with multiple relations in the body in-
fer 30% more correct facts than are identified by simpler
entailment rules, inferring many facts not present in the
corpus in any form.
</figureCaption>
<bodyText confidence="0.999947">
weights, and performing the inference took 50 min-
utes on a 72 core cluster. However, we note that for
half of the relations SHERLOCK accepts no inference
rules, and remind the reader that the performance on
any particular relation may be substantially differ-
ent, and depends on the facts observed in the corpus
and on the rules learned.
</bodyText>
<subsectionHeader confidence="0.999751">
4.2 Benefits of Inference
</subsectionHeader>
<bodyText confidence="0.9959955">
We first evaluate the utility of the learned Horn rules
by contrasting the precision and number of correct
and incorrect facts identified with and without infer-
ence over learned rules. We compare against two
simpler variants of SHERLOCK. The first is a no-
inference baseline that uses no rules, returning only
facts that are explicitly extracted. The second base-
line only accepts rules of length k = 1, allowing it to
make simple entailments but not more complicated
inferences using multiple facts.
Figure 2 compares the precision and estimated
number of correct facts with and without inference.
As is apparent, the learned inference rules substan-
tially increase the number of known facts, quadru-
pling the number of correct facts beyond what are
explicitly extracted.
The Horn rules having a body-length of two iden-
tify 30% more facts than the simpler length-one
rules. Furthermore, we find the Horn rules yield
slightly increased precision at comparable levels of
recall, although the increase is not statistically sig-
nificant. This behavior can be attributed to learn-
ing smaller weights for the length-two rules than
the length-one rules, allowing the length-two rules
provide a small amount of additional evidence as
to which facts are true, but typically not enough to
overcome the confidence of a more reliable length-
one rule.
Analyzing the errors, we found that about
one third of SHERLOCK’s mistakes are due
to metonymy and word sense ambiguity (e.g.,
confusing Vancouver, British Columbia with
Vancouver, Washington), one third are due to
inferences based on incorrectly-extracted facts
(e.g., inferences based on the incorrect fact
IsLocatedIn(New York, Suffolk County),
which was extracted from sentences like ‘Deer
Park, New York is located in Suffolk County’),
and the rest are due to unsound or incorrect
inference rules (e.g., BasedIn(Company, City):-
BasedIn(Company, Country) n CapitalOf(City,
Country)). Without negative examples it is difficult
to distinguish correct rules from these unsound
rules, since the unsound rules are correct more often
than expected by chance.
Finally, we note that although simple, length-one
rules capture many of the results, in some respects
they are just rephrasing facts that are extracted in
another form. However, the more complex, length-
two rules synthesize facts extracted from multiple
pages, and infer results that are not stated anywhere
in the corpus.
</bodyText>
<subsectionHeader confidence="0.999917">
4.3 Effect of Scoring Function
</subsectionHeader>
<bodyText confidence="0.999949416666667">
We now examine how SHERLOCK’s rule scoring
function affects its results, by comparing it with
three rule scoring functions used in prior work:
LIME. The LIME ILP system (McCreath and
Sharma, 1997) proposed a metric that generalized
Muggleton’s (1997) positive-only score function
by modeling noise and limited sample sizes.
M-Estimate of rule precision. This is a common
approach for handling noise in ILP (Dzeroski and
Bratko, 1992). It requires negative examples,
which we generated by randomly swapping argu-
ments between positive examples.
</bodyText>
<figure confidence="0.985647">
Precision of Inferred Facts
0.8
0.6
0.4
0.2
0
1
Extracted
Facts
Inferred by Simple
Entailment Rules
Inferred by
Multi-Part
Horn Rules
Sherlock With Complex Rules
Sherlock With Only Simple Entailments
No Inference
1095
Comparison of Rule Scoring Functions
0 500000 1000000 1500000 2000000 2500000
Estimated Number of Correct Facts
</figure>
<figureCaption confidence="0.914175666666667">
Figure 3: SHERLOCK identifies rules that lead to more
accurate inferences over a large set of open-domain ex-
tracted facts, deducing 2x as many facts at precision 0.8.
</figureCaption>
<bodyText confidence="0.966237588235294">
L1 Regularization. As proposed in (Huynh and
Mooney, 2008), this learns weights for all can-
didate rules using L1-regularization (encouraging
sparsity) instead of L2-regularization, and retains
only those with non-zero weight.
Figure 3 compares the precision and estimated
number of correct facts inferred by the rules of
each scoring function. SHERLOCK has consistently
higher precision, and finds twice as many correct
facts at precision 0.8.
M-Estimate accepted eight times as many rules as
SHERLOCK, increasing the number of inferred facts
at the cost of precision and longer inference times.
Most of the errors in M-Estimate and L1 Regulariza-
tion come from incorrect or unsound rules, whereas
most of the errors for LIME stem from systematic
extraction errors.
</bodyText>
<subsectionHeader confidence="0.998698">
4.4 Scoring Function Design Decisions
</subsectionHeader>
<bodyText confidence="0.984542">
SHERLOCK requires a rule to have statistical rele-
vance and statistical significance. We perform an
ablation study to understand how each of these con-
tribute to SHERLOCK’s results.
Figure 4 compares the precision and estimated
number of correct facts obtained when requiring
rules to be only statistically relevant, only statisti-
cally significant, or both. As is expected, there is
a precision/recall tradeoff. SHERLOCK has higher
precision, finding more than twice as many results at
precision 0.8 and reducing the error by 39% at a re-
call of 1 million correct facts. Statistical significance
finds twice as many correct facts as SHERLOCK, but
the extra facts it discovers have precision &lt; 0.4.
</bodyText>
<figure confidence="0.850092333333333">
Design Decisions of Sherlock’s Scoring Function
0 500000 1000000 1500000 2000000 2500000 3000000
Estimated Number of Correct Facts
</figure>
<figureCaption confidence="0.668533">
Figure 4: By requiring rules to have both statistical rel-
evance and statistical significance, SHERLOCK rejects
many error-prone rules that are accepted by the metrics
individually. The better rule set yields more accurate in-
ferences, but identifies fewer correct facts.
</figureCaption>
<bodyText confidence="0.999987153846154">
Comparing the rules accepted in each case, we
found that statistical relevance and statistical signifi-
cance each accepted about 180,000 rules, compared
to about 31,000 for SHERLOCK. The smaller set
of rules accepted by SHERLOCK not only leads to
higher precision inferences, but also speeds up in-
ference time by a factor of seven.
In a qualitative analysis, we found the statisti-
cal relevance metric overestimates probabilities for
sparse rules, which leads to a number of very high
scoring but meaningless rules. The statistical signif-
icance metric handles sparse rules better, but is still
overconfident in the case of many unsound rules.
</bodyText>
<subsectionHeader confidence="0.999621">
4.5 Analysis of Weight Learning
</subsectionHeader>
<bodyText confidence="0.9997733">
Finally, we empirically validate the modifications of
the weight learning algorithm from Section 3.5.1.
The learned-rule weights only affect the probabil-
ities of the inferred facts, not the inferred facts them-
selves, so to measure the influence of the weight
learning algorithm we examine the recall at preci-
sion 0.8 and the area under the precision-recall curve
(AuC). We build a test set by holding SHERLOCK’s
inference rules constant and randomly sampling 700
inferred facts. We test the effects of:
</bodyText>
<listItem confidence="0.819801">
• Fixed vs. Variable Penalty - Do we use the
same L2 penalty on the weights for all rules or
a stronger L2 penalty for longer rules?
• Full vs. Weighted Grounding Counts - Do we
count all unweighted rule groundings (as in
(Huynh and Mooney, 2008)), or only the best
weighted one (as in Equation 1)?
</listItem>
<figure confidence="0.997715380952381">
1
Precision of Inferred Facts
0.8
0.6
0.4
0.2
0
Sherlock
LIME
M-Estimate
L1 Reg.
Precision of Inferred Facts
0.8
0.6
0.4
0.2
1
Sherlock
Statistical Relevance
Statistical Significance
0
</figure>
<page confidence="0.953886">
1096
</page>
<table confidence="0.997214142857143">
Recall AuC
(p=0.8)
Variable Penalty, Weighted 0.35 0.735
Counts (used by SHERLOCK)
Variable Penalty, Full Counts 0.28 0.726
Fixed Penalty, Weighted Counts 0.27 0.675
Fixed Penalty, Full Counts 0.17 0.488
</table>
<tableCaption confidence="0.608514">
Table 2: SHERLOCK’s modified weight learning algo-
rithm gives better probability estimates over noisy and in-
complete Web extractions. Most of the gains come from
penalizing longer rules more, but using weighted ground-
ing counts further improves recall by 0.07, which corre-
sponds to almost 100,000 additional facts at precision 0.8.
</tableCaption>
<bodyText confidence="0.9999753">
We vary each of these independently, and give the
performance of all 4 combinations in Table 2.
The modifications from Section 3.5.1 improve
both the AuC and the recall at precision 0.8. Most
of the improvement is due to using stronger penal-
ties on longer rules, but using the weighted counts
in Equation 1 improves recall by a factor of 1.25 at
precision 0.8. While this may not seem like much,
the scale is such that it leads to almost 100,000 ad-
ditional correct facts at precision 0.8.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999944310344828">
This paper addressed the problem of learning first-
order Horn clauses from the noisy and heteroge-
neous corpus of open-domain facts extracted from
Web text. We showed that SHERLOCK is able
to learn Horn clauses in a large-scale, domain-
independent manner. Furthermore, the learned rules
are valuable, because they infer a substantial number
of facts which were not extracted from the corpus.
While SHERLOCK belongs to the broad category
of ILP learners, it has a number of novel features that
enable it to succeed in the challenging, open-domain
context. First, SHERLOCK automatically identifies
a set of high-quality extracted facts, using several
simple but effective heuristics to defeat noise and
ambiguity. Second, SHERLOCK is unsupervised and
does not require negative examples; this enables it to
scale to an unbounded number of relations. Third, it
utilizes a novel rule-scoring function, which is toler-
ant of the noise, ambiguity, and missing data issues
prevalent in facts extracted from Web text. The ex-
periments in Figure 3 show that, for open-domain
facts, SHERLOCK’s method represents a substantial
improvement over traditional ILP scoring functions.
Directions for future work include inducing
longer inference rules, investigating better methods
for combining the rules, allowing deeper inferences
across multiple rules, evaluating our system on other
corpora and devising better techniques for handling
word sense ambiguity.
</bodyText>
<sectionHeader confidence="0.996364" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999778">
We thank Sonal Gupta and the anonymous review-
ers for their helpful comments. This research was
supported in part by NSF grant IIS-0803481, ONR
grant N00014-08-1-0431, the WRF / TJ Cable Pro-
fessorship and carried out at the University of Wash-
ington’s Turing Center. The University of Washing-
ton gratefully acknowledges the support of Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract nos. FA8750-
09-C-0179 and FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991619047619">
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
M. Craven, D. DiPasquo, D. Freitag, A.K. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to Extract Symbolic Knowledge from the World
Wide Web. In Procs. of the 15th Conference of the
American Association for Artificial Intelligence, pages
509–516, Madison, US. AAAI Press, Menlo Park, US.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1–8.
S. Dzeroski and I. Bratko. 1992. Handling noise in in-
ductive logic programming. In Proceedings of the 2nd
</reference>
<page confidence="0.801993">
1097
</page>
<reference confidence="0.999300457142857">
International Workshop on Inductive Logic Program-
ming.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539–545, Nantes, France.
T.N. Huynh and R.J. Mooney. 2008. Discriminative
structure and parameter learning for Markov logic net-
works. In Proceedings of the 25th international con-
ference on Machine learning, pages 416–423. ACM.
Stanley Kok and Pedro Domingos. 2005. Learning the
structure of markov logic networks. In ICML ’05:
Proceedings of the 22nd international conference on
Machine learning, pages 441–448, New York, NY,
USA. ACM.
N. Lavrac and S. Dzeroski, editors. 2001. Relational
Data Mining. Springer-Verlag, Berlin, September.
D. Lin and P. Pantel. 2001. DIRT – Discovery of Infer-
ence Rules from Text. In KDD.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1–
7.
E. McCreath and A. Sharma. 1997. ILP with noise
and fixed example size: a Bayesian approach. In Pro-
ceedings of the Fifteenth international joint conference
on Artifical intelligence-Volume 2, pages 1310–1315.
Morgan Kaufmann Publishers Inc.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235–312.
S. Muggleton. 1995. Inverse entailment and Progol.
New Generation Computing, 13:245–286.
S. Muggleton. 1997. Learning from positive data. Lec-
ture Notes in Computer Science, 1314:358–376.
P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and
E. Hovy. 2007. ISP: Learning inferential selectional
preferences. In Proceedings of NAACL HLT, vol-
ume 7, pages 564–571.
M. Pennacchiotti and F.M. Zanzotto. 2007. Learning
Shallow Semantic Rules for Textual Entailment. Pro-
ceedings of RANLP 2007.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5:239–2666.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proc. of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why,
What, and How?
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107–136.
W.C. Salmon, R.C. Jeffrey, and J.G. Greeno. 1971. Sta-
tistical explanation &amp; statistical relevance. Univ of
Pittsburgh Pr.
S. Schoenmackers, O. Etzioni, and D. Weld. 2008. Scal-
ing Textual Inference to the Web. In Procs. of EMNLP.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Procs. of HLT/NAACL.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL 2006.
M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing, pages 22–27.
M.P. Wellman, J.S. Breese, and R.P. Goldman. 1992.
From knowledge bases to decision models. The
Knowledge Engineering Review, 7(1):35–53.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
</reference>
<page confidence="0.993338">
1098
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.180739">
<title confidence="0.999929">Learning First-Order Horn Clauses from Web Text</title>
<author confidence="0.971989">Stefan Schoenmackers</author>
<author confidence="0.971989">Oren Etzioni</author>
<author confidence="0.971989">S Daniel</author>
<affiliation confidence="0.919738333333333">Turing University of Computer Science and</affiliation>
<address confidence="0.928557">Box Seattle, WA 98125,</address>
<email confidence="0.999644">stef,etzioni,weld@cs.washington.edu</email>
<author confidence="0.825211">Jesse Katholieke Universiteit</author>
<affiliation confidence="0.993622">Department of Computer</affiliation>
<address confidence="0.579854">POBox 02402 Celestijnenlaan B-3001 Heverlee,</address>
<email confidence="0.965961">jesse.davis@cs.kuleuven.be</email>
<abstract confidence="0.99751125">the entire Web corpus does not explicall questions, yet inference can unmany But where do inference rules come from? paper investigates the problem of learnrules from Web text in an unsupervised, domain-independent manner. The described herein, is a first-order learner that acquires over 30,000 clauses from Web text. embodies several innovations, including a novel rule scoring function based on Statistical Relevance (Salmon et al., 1971) which is effective on ambiguous, noisy and incomplete Web extractions. Our experiments show that inference over the learned rules discovers three times as many facts (at precision 0.8) as the which merely extracts facts explicitly stated in Web text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the Web. In Procs. of IJCAI.</title>
<date>2007</date>
<contexts>
<context position="9425" citStr="Banko et al., 2007" startWordPosition="1433" endWordPosition="1436">rchitecture is depicted in Figure 1. To learn inference rules, SHERLOCK performs the following steps: 1. Identify a “productive” set of classes and instances of those classes 2. Discover relations between classes 3. Learn inference rules using the discovered relations and determine the confidence in each rule The first two steps help deal with the synonyms, homonyms, and noise present in open-domain theories by identifying a smaller, cleaner, and more cohesive set of facts to learn rules over. SHERLOCK learns inference rules from a collection of open-domain extractions produced by TEXTRUNNER (Banko et al., 2007). The rules learned by SHERLOCK are input to an inference engine and used to find answers to a user’s query. In this paper, SHERLOCK utilizes HOLMES as its inference engine when answering queries, and uses extracted facts of the form R(arg1, arg2) provided by the authors of TEXTRUNNER, but the techniques presented are more broadly applicable. 3.1 Finding Classes and Instances SHERLOCK first searches for a set of well-defined classes and class instances. Instances of the same class tend to behave similarly, so identifying a good set of instances will make it easier to discover the general prope</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the Web. In Procs. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI</booktitle>
<contexts>
<context position="6353" citStr="Carlson et al., 2010" startWordPosition="939" endWordPosition="942">Third, ILP system typically assume complete, largely noise-free data whereas tuples extracted from Web text are both noisy and radically incomplete. Finally, ILP systems typically utilize negative examples, which are not available when learning from open-domain facts. One system that does not require negative examples is LIME (McCreath and Sharma, 1997); We compare SHERLOCK with LIME’s methods in Section 4.3. Most prior ILP and Markov logic structure learning systems (e.g., (Kok and Domingos, 2005)) are not designed to handle the noise and incompleteness of open-domain, extracted facts. NELL (Carlson et al., 2010) performs coupled semi-supervised learning to extract a large knowledge base of instances, relations, and inference rules, bootstrapping from a few seed examples of each class and relation of interest and a few constraints among them. In contrast, SHERLOCK focuses mainly on learning inference rules, but does so without any manually specified seeds or constraints. Craven etal.(1998) also used ILP to help information extraction on the Web, but required training examples and focused on a single domain. Two other notable systems that learn inference rules from text are DIRT (Lin and Pantel, 2001) </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>D DiPasquo</author>
<author>D Freitag</author>
<author>A K McCallum</author>
<author>T Mitchell</author>
<author>K Nigam</author>
<author>S Slattery</author>
</authors>
<title>Learning to Extract Symbolic Knowledge from the World Wide Web.</title>
<date>1998</date>
<booktitle>In Procs. of the 15th Conference of the American Association for Artificial Intelligence,</booktitle>
<pages>509--516</pages>
<publisher>AAAI Press,</publisher>
<location>Madison, US.</location>
<marker>Craven, DiPasquo, Freitag, McCallum, Mitchell, Nigam, Slattery, 1998</marker>
<rawString>M. Craven, D. DiPasquo, D. Freitag, A.K. McCallum, T. Mitchell, K. Nigam, and S. Slattery. 1998. Learning to Extract Symbolic Knowledge from the World Wide Web. In Procs. of the 15th Conference of the American Association for Artificial Intelligence, pages 509–516, Madison, US. AAAI Press, Menlo Park, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8004" citStr="Dagan et al., 2005" startWordPosition="1205" endWordPosition="1208">t does not mean they bought the skill). To add the necessary context, ISP (Pantel et al., 2007) learned selectional preferences (Resnik, 1997) for DIRT’s rules. The selectional preferences act as type restrictions 1089 Figure 1: Architecture of SHERLOCK. SHERLOCK learns inference rules offline and provides them to the HOLMES inference engine, which uses the rules to answer queries. on the arguments, and attempt to filter out incorrect inferences. While these approaches are useful, they are strictly more limited than the rules learned by SHERLOCK. The Recognizing Textual Entailment (RTE) task (Dagan et al., 2005) is to determine whether one sentence entails another. Approaches to RTE include those of Tatu and Moldovan (2007), which generates inference rules from WordNet lexical chains and a set of axiom templates, and Pennacchiotti and Zanzotto (2007), which learns inference rules based on similarity across entailment pairs. In contrast with this work, RTE systems reason over full sentences, but benefit by being given the sentences and training data. SHERLOCK operates over simpler Web extractions, but is not given guidance about which facts may interact. 3 System Description SHERLOCK takes as input a </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL Recognising Textual Entailment Challenge. Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dzeroski</author>
<author>I Bratko</author>
</authors>
<title>Handling noise in inductive logic programming.</title>
<date>1992</date>
<booktitle>In Proceedings of the 2nd International Workshop on Inductive Logic Programming.</booktitle>
<contexts>
<context position="16033" citStr="Dzeroski and Bratko, 1992" startWordPosition="2519" endWordPosition="2522">es by Statistical Relevance The problem of evaluating candidate rules has been studied by many researchers, but typically in either a supervised or propositional context whereas we are learning first-order Horn-clauses from a noisy set of positive examples. Moreover, due to the incomplete nature of the input corpus and the imperfect yield of 1091 extraction—many true facts are not stated explicitly in the set of ground assertions used by the learner to evaluate rules. The absence of negative examples, coupled with noise, means that standard ILP evaluation functions (e.g., (Quinlan, 1990) and (Dzeroski and Bratko, 1992)) are not appropriate. Furthermore, when evaluating a particular rule with consequent C and antecedent A, it is natural to consider p(C|A) but, due to missing data, this absolute probability estimate is often misleading: in many cases C will hold given A but the fact C is not mentioned in the corpus. Thus to evaluate rules over extractions, we need to consider relative probability estimates. I.e., is p(C|A) » p(C)? If so, then A is said to be statistically relevant to C (Salmon et al., 1971). Statistical relevance tries to infer the simplest set of factors which explain an observation. It can </context>
<context position="21011" citStr="Dzeroski and Bratko, 1992" startWordPosition="3318" endWordPosition="3321">if any, that appear in Head(...) but not in B(...), we sample their values according to a distribution p(vi|classi). We estimate p(vi|classi) based on the relative frequency that vi was extracted using a Hearst pattern with classi. Finally, we ensure the differences are statistically significant using the likelihood ratio statistic: where p(-Head(...)|B(...)) = 1−p(Head(...)|B(...)) and Nr is the number of results inferred by the rule Head(...):-Body(...). This test is distributed approximately as x2 with one degree of freedom. It is similar to the statistical significance test used in mFOIL (Dzeroski and Bratko, 1992), but has two modifications since SHERLOCK doesn’t have training data. In lieu of positive and negative examples, we use whether or not the inferred head value was observed, and compare against the distribution of a subsuming clause B&apos;(...) rather than a known prior. This method of evaluating rules has two important differences from ILP under a closed world assumption. First, our probability estimates consider the fact that examples provide varying amounts of information. Second, statistical relevance finds rules with large increases in relative probability, not necessarily a large absolute pr</context>
<context position="32244" citStr="Dzeroski and Bratko, 1992" startWordPosition="5152" endWordPosition="5155">er form. However, the more complex, lengthtwo rules synthesize facts extracted from multiple pages, and infer results that are not stated anywhere in the corpus. 4.3 Effect of Scoring Function We now examine how SHERLOCK’s rule scoring function affects its results, by comparing it with three rule scoring functions used in prior work: LIME. The LIME ILP system (McCreath and Sharma, 1997) proposed a metric that generalized Muggleton’s (1997) positive-only score function by modeling noise and limited sample sizes. M-Estimate of rule precision. This is a common approach for handling noise in ILP (Dzeroski and Bratko, 1992). It requires negative examples, which we generated by randomly swapping arguments between positive examples. Precision of Inferred Facts 0.8 0.6 0.4 0.2 0 1 Extracted Facts Inferred by Simple Entailment Rules Inferred by Multi-Part Horn Rules Sherlock With Complex Rules Sherlock With Only Simple Entailments No Inference 1095 Comparison of Rule Scoring Functions 0 500000 1000000 1500000 2000000 2500000 Estimated Number of Correct Facts Figure 3: SHERLOCK identifies rules that lead to more accurate inferences over a large set of open-domain extracted facts, deducing 2x as many facts at precisio</context>
</contexts>
<marker>Dzeroski, Bratko, 1992</marker>
<rawString>S. Dzeroski and I. Bratko. 1992. Handling noise in inductive logic programming. In Proceedings of the 2nd International Workshop on Inductive Logic Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Procs. of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="10193" citStr="Hearst, 1992" startWordPosition="1555" endWordPosition="1556"> inference engine when answering queries, and uses extracted facts of the form R(arg1, arg2) provided by the authors of TEXTRUNNER, but the techniques presented are more broadly applicable. 3.1 Finding Classes and Instances SHERLOCK first searches for a set of well-defined classes and class instances. Instances of the same class tend to behave similarly, so identifying a good set of instances will make it easier to discover the general properties of the entire class. Options for identifying interesting classes include manually created methods (WordNet (Miller et al., 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al., 2006). We use Hearst patterns because they are simple, capture how classes and instances are mentioned in Web text, and yield intuitive, explainable groups. Hearst (1992) identified a set of textual patterns which indicate hyponymy (e.g., ‘Class such as Instance’). Using these patterns, we extracted 29 million (instance, class) pairs from a large Web crawl. We then cleaned them using word stemming, normalization, and by dropping modifiers. Unfortunately, the patterns make systematic errors (e.g., extracting Canada as</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Procs. of the 14th International Conference on Computational Linguistics, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T N Huynh</author>
<author>R J Mooney</author>
</authors>
<title>Discriminative structure and parameter learning for Markov logic networks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>416--423</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="23352" citStr="Huynh and Mooney (2008)" startWordPosition="3700" endWordPosition="3703">ikely to be true. C2. The more frequently a fact is extracted from the Web, the more likely it is to be true. However, facts in E should have a confidence bounded by a threshold pma, &lt; 1. E contains systematic extraction errors, so we want uncertainty in even the most frequently extracted facts. C3. An inference that combines uncertain facts should be less likely than each fact it uses. Next, we describe the needed modifications to the weight learning and inference algorithm to achieve the desired behavior. 3.5.1 Weight Learning We use the discriminative weight learning procedure described by Huynh and Mooney (2008). Setting the weights involves counting the number of true groundings for each rule in the data (Richardson and Domingos, 2006). However, the noisy nature of Web extractions will make this an overestimate. Consequently, we compute ni(E), the number of true groundings of rule i, as follows: �ni(E) = j where E is the evidence, j ranges over heads of the rule, Bodyijk is the body of the kth grounding for jth head of rule i, and p(B(...)) is approximated using a logistic function of the number of times B(...) was extracted,1 scaled to be in the range [0,0.75]. This models C2 by giving increasing b</context>
<context position="32909" citStr="Huynh and Mooney, 2008" startWordPosition="5254" endWordPosition="5257">generated by randomly swapping arguments between positive examples. Precision of Inferred Facts 0.8 0.6 0.4 0.2 0 1 Extracted Facts Inferred by Simple Entailment Rules Inferred by Multi-Part Horn Rules Sherlock With Complex Rules Sherlock With Only Simple Entailments No Inference 1095 Comparison of Rule Scoring Functions 0 500000 1000000 1500000 2000000 2500000 Estimated Number of Correct Facts Figure 3: SHERLOCK identifies rules that lead to more accurate inferences over a large set of open-domain extracted facts, deducing 2x as many facts at precision 0.8. L1 Regularization. As proposed in (Huynh and Mooney, 2008), this learns weights for all candidate rules using L1-regularization (encouraging sparsity) instead of L2-regularization, and retains only those with non-zero weight. Figure 3 compares the precision and estimated number of correct facts inferred by the rules of each scoring function. SHERLOCK has consistently higher precision, and finds twice as many correct facts at precision 0.8. M-Estimate accepted eight times as many rules as SHERLOCK, increasing the number of inferred facts at the cost of precision and longer inference times. Most of the errors in M-Estimate and L1 Regularization come fr</context>
<context position="36172" citStr="Huynh and Mooney, 2008" startWordPosition="5770" endWordPosition="5773">ts only affect the probabilities of the inferred facts, not the inferred facts themselves, so to measure the influence of the weight learning algorithm we examine the recall at precision 0.8 and the area under the precision-recall curve (AuC). We build a test set by holding SHERLOCK’s inference rules constant and randomly sampling 700 inferred facts. We test the effects of: • Fixed vs. Variable Penalty - Do we use the same L2 penalty on the weights for all rules or a stronger L2 penalty for longer rules? • Full vs. Weighted Grounding Counts - Do we count all unweighted rule groundings (as in (Huynh and Mooney, 2008)), or only the best weighted one (as in Equation 1)? 1 Precision of Inferred Facts 0.8 0.6 0.4 0.2 0 Sherlock LIME M-Estimate L1 Reg. Precision of Inferred Facts 0.8 0.6 0.4 0.2 1 Sherlock Statistical Relevance Statistical Significance 0 1096 Recall AuC (p=0.8) Variable Penalty, Weighted 0.35 0.735 Counts (used by SHERLOCK) Variable Penalty, Full Counts 0.28 0.726 Fixed Penalty, Weighted Counts 0.27 0.675 Fixed Penalty, Full Counts 0.17 0.488 Table 2: SHERLOCK’s modified weight learning algorithm gives better probability estimates over noisy and incomplete Web extractions. Most of the gains co</context>
</contexts>
<marker>Huynh, Mooney, 2008</marker>
<rawString>T.N. Huynh and R.J. Mooney. 2008. Discriminative structure and parameter learning for Markov logic networks. In Proceedings of the 25th international conference on Machine learning, pages 416–423. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Pedro Domingos</author>
</authors>
<title>Learning the structure of markov logic networks.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>441--448</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6235" citStr="Kok and Domingos, 2005" startWordPosition="920" endWordPosition="923">constants uniquely denote individuals; however, in Web text strings such as “dad” or “John Smith” are highly ambiguous. Third, ILP system typically assume complete, largely noise-free data whereas tuples extracted from Web text are both noisy and radically incomplete. Finally, ILP systems typically utilize negative examples, which are not available when learning from open-domain facts. One system that does not require negative examples is LIME (McCreath and Sharma, 1997); We compare SHERLOCK with LIME’s methods in Section 4.3. Most prior ILP and Markov logic structure learning systems (e.g., (Kok and Domingos, 2005)) are not designed to handle the noise and incompleteness of open-domain, extracted facts. NELL (Carlson et al., 2010) performs coupled semi-supervised learning to extract a large knowledge base of instances, relations, and inference rules, bootstrapping from a few seed examples of each class and relation of interest and a few constraints among them. In contrast, SHERLOCK focuses mainly on learning inference rules, but does so without any manually specified seeds or constraints. Craven etal.(1998) also used ILP to help information extraction on the Web, but required training examples and focus</context>
</contexts>
<marker>Kok, Domingos, 2005</marker>
<rawString>Stanley Kok and Pedro Domingos. 2005. Learning the structure of markov logic networks. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 441–448, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>Relational Data Mining.</booktitle>
<editor>N. Lavrac and S. Dzeroski, editors.</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<marker>2001</marker>
<rawString>N. Lavrac and S. Dzeroski, editors. 2001. Relational Data Mining. Springer-Verlag, Berlin, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<date>2001</date>
<booktitle>DIRT – Discovery of Inference Rules from Text. In KDD.</booktitle>
<contexts>
<context position="6952" citStr="Lin and Pantel, 2001" startWordPosition="1035" endWordPosition="1038">(Carlson et al., 2010) performs coupled semi-supervised learning to extract a large knowledge base of instances, relations, and inference rules, bootstrapping from a few seed examples of each class and relation of interest and a few constraints among them. In contrast, SHERLOCK focuses mainly on learning inference rules, but does so without any manually specified seeds or constraints. Craven etal.(1998) also used ILP to help information extraction on the Web, but required training examples and focused on a single domain. Two other notable systems that learn inference rules from text are DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007). However, both DIRT and RESOLVER learn only a limited set of rules capturing synonyms, paraphrases, and simple entailments, not more expressive multipart Horn clauses. For example, these systems may learn the rule X acquired Y ==&gt;. X bought Y , which captures different ways of describing a purchase. Applications of these rules often depend on context (e.g., if a person acquires a skill, that does not mean they bought the skill). To add the necessary context, ISP (Pantel et al., 2007) learned selectional preferences (Resnik, 1997) for DIRT’s rules. The se</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>D. Lin and P. Pantel. 2001. DIRT – Discovery of Inference Rules from Text. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational linguistics (COLING-02),</booktitle>
<pages>1--7</pages>
<contexts>
<context position="10238" citStr="Lin and Pantel, 2002" startWordPosition="1559" endWordPosition="1562">ies, and uses extracted facts of the form R(arg1, arg2) provided by the authors of TEXTRUNNER, but the techniques presented are more broadly applicable. 3.1 Finding Classes and Instances SHERLOCK first searches for a set of well-defined classes and class instances. Instances of the same class tend to behave similarly, so identifying a good set of instances will make it easier to discover the general properties of the entire class. Options for identifying interesting classes include manually created methods (WordNet (Miller et al., 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al., 2006). We use Hearst patterns because they are simple, capture how classes and instances are mentioned in Web text, and yield intuitive, explainable groups. Hearst (1992) identified a set of textual patterns which indicate hyponymy (e.g., ‘Class such as Instance’). Using these patterns, we extracted 29 million (instance, class) pairs from a large Web crawl. We then cleaned them using word stemming, normalization, and by dropping modifiers. Unfortunately, the patterns make systematic errors (e.g., extracting Canada as the name of a city from the phrase ‘Toronto,</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>D. Lin and P. Pantel. 2002. Concept discovery from text. In Proceedings of the 19th International Conference on Computational linguistics (COLING-02), pages 1– 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E McCreath</author>
<author>A Sharma</author>
</authors>
<title>ILP with noise and fixed example size: a Bayesian approach.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifteenth international joint conference on Artifical intelligence-Volume 2,</booktitle>
<pages>1310--1315</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="6087" citStr="McCreath and Sharma, 1997" startWordPosition="896" endWordPosition="900">or open domains. First, ILP systems assume high-quality, hand-labeled training examples for each relation of interest. Second, ILP systems assume that constants uniquely denote individuals; however, in Web text strings such as “dad” or “John Smith” are highly ambiguous. Third, ILP system typically assume complete, largely noise-free data whereas tuples extracted from Web text are both noisy and radically incomplete. Finally, ILP systems typically utilize negative examples, which are not available when learning from open-domain facts. One system that does not require negative examples is LIME (McCreath and Sharma, 1997); We compare SHERLOCK with LIME’s methods in Section 4.3. Most prior ILP and Markov logic structure learning systems (e.g., (Kok and Domingos, 2005)) are not designed to handle the noise and incompleteness of open-domain, extracted facts. NELL (Carlson et al., 2010) performs coupled semi-supervised learning to extract a large knowledge base of instances, relations, and inference rules, bootstrapping from a few seed examples of each class and relation of interest and a few constraints among them. In contrast, SHERLOCK focuses mainly on learning inference rules, but does so without any manually </context>
<context position="32007" citStr="McCreath and Sharma, 1997" startWordPosition="5117" endWordPosition="5120">les, since the unsound rules are correct more often than expected by chance. Finally, we note that although simple, length-one rules capture many of the results, in some respects they are just rephrasing facts that are extracted in another form. However, the more complex, lengthtwo rules synthesize facts extracted from multiple pages, and infer results that are not stated anywhere in the corpus. 4.3 Effect of Scoring Function We now examine how SHERLOCK’s rule scoring function affects its results, by comparing it with three rule scoring functions used in prior work: LIME. The LIME ILP system (McCreath and Sharma, 1997) proposed a metric that generalized Muggleton’s (1997) positive-only score function by modeling noise and limited sample sizes. M-Estimate of rule precision. This is a common approach for handling noise in ILP (Dzeroski and Bratko, 1992). It requires negative examples, which we generated by randomly swapping arguments between positive examples. Precision of Inferred Facts 0.8 0.6 0.4 0.2 0 1 Extracted Facts Inferred by Simple Entailment Rules Inferred by Multi-Part Horn Rules Sherlock With Complex Rules Sherlock With Only Simple Entailments No Inference 1095 Comparison of Rule Scoring Function</context>
</contexts>
<marker>McCreath, Sharma, 1997</marker>
<rawString>E. McCreath and A. Sharma. 1997. ILP with noise and fixed example size: a Bayesian approach. In Proceedings of the Fifteenth international joint conference on Artifical intelligence-Volume 2, pages 1310–1315. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="10159" citStr="Miller et al., 1990" startWordPosition="1549" endWordPosition="1552">is paper, SHERLOCK utilizes HOLMES as its inference engine when answering queries, and uses extracted facts of the form R(arg1, arg2) provided by the authors of TEXTRUNNER, but the techniques presented are more broadly applicable. 3.1 Finding Classes and Instances SHERLOCK first searches for a set of well-defined classes and class instances. Instances of the same class tend to behave similarly, so identifying a good set of instances will make it easier to discover the general properties of the entire class. Options for identifying interesting classes include manually created methods (WordNet (Miller et al., 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al., 2006). We use Hearst patterns because they are simple, capture how classes and instances are mentioned in Web text, and yield intuitive, explainable groups. Hearst (1992) identified a set of textual patterns which indicate hyponymy (e.g., ‘Class such as Instance’). Using these patterns, we extracted 29 million (instance, class) pairs from a large Web crawl. We then cleaned them using word stemming, normalization, and by dropping modifiers. Unfortunately, the patterns make systematic </context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muggleton</author>
</authors>
<date>1995</date>
<booktitle>Inverse entailment and Progol. New Generation Computing,</booktitle>
<pages>13--245</pages>
<contexts>
<context position="2436" citStr="Muggleton, 1995" startWordPosition="367" endWordPosition="368">, HOLMES’s Achilles heel is that it requires hand-coded, first-order, Horn clauses as input. Thus, while HOLMES’s inference run time is highly scalable, it requires substantial labor and expertise to hand-craft the appropriate set of Horn clauses for each new domain. Is it possible to learn effective first-order Horn clauses automatically from Web text in a domainindependent and scalable manner? We refer to the set of ground facts derived from Web text as opendomain theories. Learning Horn clauses has been studied extensively in the Inductive Logic Programming (ILP) literature (Quinlan, 1990; Muggleton, 1995). However, learning Horn clauses from opendomain theories is particularly challenging for several reasons. First, the theories denote instances of an unbounded and unknown set of relations. Second, the ground facts in the theories are noisy, and incomplete. Negative examples are mostly absent, and certainly we cannot make the closed-world assumption typically made by ILP systems. Finally, the names used to denote both entities and relations are rife with both synonyms and polysymes making their referents ambiguous and resulting in a particularly noisy and ambiguous set of ground facts. This pa</context>
<context position="5411" citStr="Muggleton, 1995" startWordPosition="801" endWordPosition="802">s. The remainder of this paper is organized as follows. We start by describing previous work. Section 3 introduces the SHERLOCK rule learning system, with Section 3.4 describing how it estimates rule quality. We empirically evaluate SHERLOCK in Section 4, and conclude. 2 Previous Work SHERLOCK is one of the first systems to learn firstorder Horn clauses from open-domain Web extractions. The learning method in SHERLOCK belongs to the Inductive logic programming (ILP) subfield of machine learning (Lavrac and Dzeroski, 2001). However, classical ILP systems (e.g., FOIL (Quinlan, 1990) and Progol (Muggleton, 1995)) make strong assumptions that are inappropriate for open domains. First, ILP systems assume high-quality, hand-labeled training examples for each relation of interest. Second, ILP systems assume that constants uniquely denote individuals; however, in Web text strings such as “dad” or “John Smith” are highly ambiguous. Third, ILP system typically assume complete, largely noise-free data whereas tuples extracted from Web text are both noisy and radically incomplete. Finally, ILP systems typically utilize negative examples, which are not available when learning from open-domain facts. One system</context>
</contexts>
<marker>Muggleton, 1995</marker>
<rawString>S. Muggleton. 1995. Inverse entailment and Progol. New Generation Computing, 13:245–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muggleton</author>
</authors>
<title>Learning from positive data.</title>
<date>1997</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>1314--358</pages>
<marker>Muggleton, 1997</marker>
<rawString>S. Muggleton. 1997. Learning from positive data. Lecture Notes in Computer Science, 1314:358–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>R Bhagat</author>
<author>B Coppola</author>
<author>T Chklovski</author>
<author>E Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<volume>7</volume>
<pages>564--571</pages>
<contexts>
<context position="7480" citStr="Pantel et al., 2007" startWordPosition="1127" endWordPosition="1130">o other notable systems that learn inference rules from text are DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007). However, both DIRT and RESOLVER learn only a limited set of rules capturing synonyms, paraphrases, and simple entailments, not more expressive multipart Horn clauses. For example, these systems may learn the rule X acquired Y ==&gt;. X bought Y , which captures different ways of describing a purchase. Applications of these rules often depend on context (e.g., if a person acquires a skill, that does not mean they bought the skill). To add the necessary context, ISP (Pantel et al., 2007) learned selectional preferences (Resnik, 1997) for DIRT’s rules. The selectional preferences act as type restrictions 1089 Figure 1: Architecture of SHERLOCK. SHERLOCK learns inference rules offline and provides them to the HOLMES inference engine, which uses the rules to answer queries. on the arguments, and attempt to filter out incorrect inferences. While these approaches are useful, they are strictly more limited than the rules learned by SHERLOCK. The Recognizing Textual Entailment (RTE) task (Dagan et al., 2005) is to determine whether one sentence entails another. Approaches to RTE inc</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and E. Hovy. 2007. ISP: Learning inferential selectional preferences. In Proceedings of NAACL HLT, volume 7, pages 564–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pennacchiotti</author>
<author>F M Zanzotto</author>
</authors>
<title>Learning Shallow Semantic Rules for Textual Entailment.</title>
<date>2007</date>
<booktitle>Proceedings of RANLP</booktitle>
<contexts>
<context position="8247" citStr="Pennacchiotti and Zanzotto (2007)" startWordPosition="1242" endWordPosition="1246">Architecture of SHERLOCK. SHERLOCK learns inference rules offline and provides them to the HOLMES inference engine, which uses the rules to answer queries. on the arguments, and attempt to filter out incorrect inferences. While these approaches are useful, they are strictly more limited than the rules learned by SHERLOCK. The Recognizing Textual Entailment (RTE) task (Dagan et al., 2005) is to determine whether one sentence entails another. Approaches to RTE include those of Tatu and Moldovan (2007), which generates inference rules from WordNet lexical chains and a set of axiom templates, and Pennacchiotti and Zanzotto (2007), which learns inference rules based on similarity across entailment pairs. In contrast with this work, RTE systems reason over full sentences, but benefit by being given the sentences and training data. SHERLOCK operates over simpler Web extractions, but is not given guidance about which facts may interact. 3 System Description SHERLOCK takes as input a large set of open domain facts, and returns a set of weighted Horn-clause inference rules. Other systems (e.g., HOLMES) use the rules to answer questions, infer additional facts, etc. SHERLOCK’s basic architecture is depicted in Figure 1. To l</context>
</contexts>
<marker>Pennacchiotti, Zanzotto, 2007</marker>
<rawString>M. Pennacchiotti and F.M. Zanzotto. 2007. Learning Shallow Semantic Rules for Textual Entailment. Proceedings of RANLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Learning logical definitions from relations.</title>
<date>1990</date>
<booktitle>Machine Learning,</booktitle>
<pages>5--239</pages>
<contexts>
<context position="2418" citStr="Quinlan, 1990" startWordPosition="365" endWordPosition="366">he Web. However, HOLMES’s Achilles heel is that it requires hand-coded, first-order, Horn clauses as input. Thus, while HOLMES’s inference run time is highly scalable, it requires substantial labor and expertise to hand-craft the appropriate set of Horn clauses for each new domain. Is it possible to learn effective first-order Horn clauses automatically from Web text in a domainindependent and scalable manner? We refer to the set of ground facts derived from Web text as opendomain theories. Learning Horn clauses has been studied extensively in the Inductive Logic Programming (ILP) literature (Quinlan, 1990; Muggleton, 1995). However, learning Horn clauses from opendomain theories is particularly challenging for several reasons. First, the theories denote instances of an unbounded and unknown set of relations. Second, the ground facts in the theories are noisy, and incomplete. Negative examples are mostly absent, and certainly we cannot make the closed-world assumption typically made by ILP systems. Finally, the names used to denote both entities and relations are rife with both synonyms and polysymes making their referents ambiguous and resulting in a particularly noisy and ambiguous set of gro</context>
<context position="5382" citStr="Quinlan, 1990" startWordPosition="796" endWordPosition="798">ted from the Web text corpus. The remainder of this paper is organized as follows. We start by describing previous work. Section 3 introduces the SHERLOCK rule learning system, with Section 3.4 describing how it estimates rule quality. We empirically evaluate SHERLOCK in Section 4, and conclude. 2 Previous Work SHERLOCK is one of the first systems to learn firstorder Horn clauses from open-domain Web extractions. The learning method in SHERLOCK belongs to the Inductive logic programming (ILP) subfield of machine learning (Lavrac and Dzeroski, 2001). However, classical ILP systems (e.g., FOIL (Quinlan, 1990) and Progol (Muggleton, 1995)) make strong assumptions that are inappropriate for open domains. First, ILP systems assume high-quality, hand-labeled training examples for each relation of interest. Second, ILP systems assume that constants uniquely denote individuals; however, in Web text strings such as “dad” or “John Smith” are highly ambiguous. Third, ILP system typically assume complete, largely noise-free data whereas tuples extracted from Web text are both noisy and radically incomplete. Finally, ILP systems typically utilize negative examples, which are not available when learning from </context>
<context position="16001" citStr="Quinlan, 1990" startWordPosition="2516" endWordPosition="2517">. 3.4 Evaluating Rules by Statistical Relevance The problem of evaluating candidate rules has been studied by many researchers, but typically in either a supervised or propositional context whereas we are learning first-order Horn-clauses from a noisy set of positive examples. Moreover, due to the incomplete nature of the input corpus and the imperfect yield of 1091 extraction—many true facts are not stated explicitly in the set of ground assertions used by the learner to evaluate rules. The absence of negative examples, coupled with noise, means that standard ILP evaluation functions (e.g., (Quinlan, 1990) and (Dzeroski and Bratko, 1992)) are not appropriate. Furthermore, when evaluating a particular rule with consequent C and antecedent A, it is natural to consider p(C|A) but, due to missing data, this absolute probability estimate is often misleading: in many cases C will hold given A but the fact C is not mentioned in the corpus. Thus to evaluate rules over extractions, we need to consider relative probability estimates. I.e., is p(C|A) » p(C)? If so, then A is said to be statistically relevant to C (Salmon et al., 1971). Statistical relevance tries to infer the simplest set of factors which</context>
</contexts>
<marker>Quinlan, 1990</marker>
<rawString>J. R. Quinlan. 1990. Learning logical definitions from relations. Machine Learning, 5:239–2666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proc. of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why,</booktitle>
<location>What, and How?</location>
<contexts>
<context position="7527" citStr="Resnik, 1997" startWordPosition="1135" endWordPosition="1136">m text are DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007). However, both DIRT and RESOLVER learn only a limited set of rules capturing synonyms, paraphrases, and simple entailments, not more expressive multipart Horn clauses. For example, these systems may learn the rule X acquired Y ==&gt;. X bought Y , which captures different ways of describing a purchase. Applications of these rules often depend on context (e.g., if a person acquires a skill, that does not mean they bought the skill). To add the necessary context, ISP (Pantel et al., 2007) learned selectional preferences (Resnik, 1997) for DIRT’s rules. The selectional preferences act as type restrictions 1089 Figure 1: Architecture of SHERLOCK. SHERLOCK learns inference rules offline and provides them to the HOLMES inference engine, which uses the rules to answer queries. on the arguments, and attempt to filter out incorrect inferences. While these approaches are useful, they are strictly more limited than the rules learned by SHERLOCK. The Recognizing Textual Entailment (RTE) task (Dagan et al., 2005) is to determine whether one sentence entails another. Approaches to RTE include those of Tatu and Moldovan (2007), which g</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proc. of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov Logic Networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="22379" citStr="Richardson and Domingos, 2006" startWordPosition="3538" endWordPosition="3541">l have high accuracy. Even for true rules, the observed estimates p(Head(...)|Body(...)) « 1 due to missing data and noise. 3.5 Making Inferences In order to benefit from learned rules, we need an inference engine; with its linear-time scalability, HOLMES is a natural choice (Schoenmackers et al., 2008). As input HOLMES requires a target atom H(...), an evidence set E and weighted rule set R as input. It performs a form of knowledge based model construction (Wellman et al., 1992), first finding facts using logical inference, then estimating the confidence of each using a Markov Logic Network (Richardson and Domingos, 2006). Prior to running inference, it is necessary to assign a weight to each rule learned by SHERLOCK. Since the rules and inferences are based on a set of noisy and incomplete extractions, the algorithms used for both weight learning and inference should capture the following characteristics of our problem: C1. Any arbitrary unknown fact is highly unlikely to be true. C2. The more frequently a fact is extracted from the Web, the more likely it is to be true. However, facts in E should have a confidence bounded by a threshold pma, &lt; 1. E contains systematic extraction errors, so we want uncertaint</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov Logic Networks. Machine Learning, 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Salmon</author>
<author>R C Jeffrey</author>
<author>J G Greeno</author>
</authors>
<title>Statistical explanation &amp; statistical relevance.</title>
<date>1971</date>
<institution>Univ of Pittsburgh Pr.</institution>
<contexts>
<context position="941" citStr="Salmon et al., 1971" startWordPosition="128" endWordPosition="131">02402 Celestijnenlaan 200a B-3001 Heverlee, Belgium jesse.davis@cs.kuleuven.be Abstract Even the entire Web corpus does not explicitly answer all questions, yet inference can uncover many implicit answers. But where do inference rules come from? This paper investigates the problem of learning inference rules from Web text in an unsupervised, domain-independent manner. The SHERLOCK system, described herein, is a first-order learner that acquires over 30,000 Horn clauses from Web text. SHERLOCK embodies several innovations, including a novel rule scoring function based on Statistical Relevance (Salmon et al., 1971) which is effective on ambiguous, noisy and incomplete Web extractions. Our experiments show that inference over the learned rules discovers three times as many facts (at precision 0.8) as the TEXTRUNNER system which merely extracts facts explicitly stated in Web text. 1 Introduction Today’s Web search engines locate pages that match keyword queries. Even sophisticated Web-based Q/A systems merely locate pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, Sc</context>
<context position="16529" citStr="Salmon et al., 1971" startWordPosition="2607" endWordPosition="2610">les, coupled with noise, means that standard ILP evaluation functions (e.g., (Quinlan, 1990) and (Dzeroski and Bratko, 1992)) are not appropriate. Furthermore, when evaluating a particular rule with consequent C and antecedent A, it is natural to consider p(C|A) but, due to missing data, this absolute probability estimate is often misleading: in many cases C will hold given A but the fact C is not mentioned in the corpus. Thus to evaluate rules over extractions, we need to consider relative probability estimates. I.e., is p(C|A) » p(C)? If so, then A is said to be statistically relevant to C (Salmon et al., 1971). Statistical relevance tries to infer the simplest set of factors which explain an observation. It can be viewed as searching for the simplest propositional Horn-clause which increases the likelihood of a goal proposition g. The two key ideas in determining statistical relevance are discovering factors which substantially increase the likelihood of g (even if the probabilities are small in an absolute sense), and dismissing irrelevant factors. To illustrate these concepts, consider the following example. Suppose our goal is to predict if New York City will have a storm (S). On an arbitrary da</context>
</contexts>
<marker>Salmon, Jeffrey, Greeno, 1971</marker>
<rawString>W.C. Salmon, R.C. Jeffrey, and J.G. Greeno. 1971. Statistical explanation &amp; statistical relevance. Univ of Pittsburgh Pr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schoenmackers</author>
<author>O Etzioni</author>
<author>D Weld</author>
</authors>
<title>Scaling Textual Inference to the Web. In Procs. of EMNLP.</title>
<date>2008</date>
<contexts>
<context position="22053" citStr="Schoenmackers et al., 2008" startWordPosition="3484" endWordPosition="3487">he fact that examples provide varying amounts of information. Second, statistical relevance finds rules with large increases in relative probability, not necessarily a large absolute probability. This is crucial in an open domain setting where most facts are false, which means the trivial rule that everything is false will have high accuracy. Even for true rules, the observed estimates p(Head(...)|Body(...)) « 1 due to missing data and noise. 3.5 Making Inferences In order to benefit from learned rules, we need an inference engine; with its linear-time scalability, HOLMES is a natural choice (Schoenmackers et al., 2008). As input HOLMES requires a target atom H(...), an evidence set E and weighted rule set R as input. It performs a form of knowledge based model construction (Wellman et al., 1992), first finding facts using logical inference, then estimating the confidence of each using a Markov Logic Network (Richardson and Domingos, 2006). Prior to running inference, it is necessary to assign a weight to each rule learned by SHERLOCK. Since the rules and inferences are based on a set of noisy and incomplete extractions, the algorithms used for both weight learning and inference should capture the following </context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, 2008</marker>
<rawString>S. Schoenmackers, O. Etzioni, and D. Weld. 2008. Scaling Textual Inference to the Web. In Procs. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Shinyama</author>
<author>S Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Procs. of HLT/NAACL.</booktitle>
<contexts>
<context position="12689" citStr="Shinyama and Sekine, 2006" startWordPosition="1957" endWordPosition="1960">cities, foods, nutrients, locations) as well as ambiguous concepts (e.g., ideas, things). We focus on the less ambiguous classes by eliminating any class not appearing as a descendant of physical entity, social group, physical condition, or event in WordNet. Beyond this filtering we make no use of a type hierarchy and treat classes independently. In our corpus, we identify 1.1 million distinct, cleaned (instance, class) pairs for 156 classes. 3.2 Discovering Relations between Classes Next, SHERLOCK discovers how classes relate to and interact with each other. Prior work in relation discovery (Shinyama and Sekine, 2006) has investigated the problem of finding relationships between different classes. However, the goal of this work is to learn rules on top of the discovered typed relations. We use a few simple heuristics to automatically identify interesting relations. For every pair of classes (C1, C2), we find a set of typed, candidate relations from the 100 most frequent relations in the corpus where the first argument is an instance of C1 and the second argument is an instance of C2. For extraction terms with multiple senses (e.g., New York), we split their weight based on how frequently they appear with e</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Y. Shinyama and S. Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Procs. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In COLING/ACL</booktitle>
<contexts>
<context position="10276" citStr="Snow et al., 2006" startWordPosition="1566" endWordPosition="1569"> R(arg1, arg2) provided by the authors of TEXTRUNNER, but the techniques presented are more broadly applicable. 3.1 Finding Classes and Instances SHERLOCK first searches for a set of well-defined classes and class instances. Instances of the same class tend to behave similarly, so identifying a good set of instances will make it easier to discover the general properties of the entire class. Options for identifying interesting classes include manually created methods (WordNet (Miller et al., 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al., 2006). We use Hearst patterns because they are simple, capture how classes and instances are mentioned in Web text, and yield intuitive, explainable groups. Hearst (1992) identified a set of textual patterns which indicate hyponymy (e.g., ‘Class such as Instance’). Using these patterns, we extracted 29 million (instance, class) pairs from a large Web crawl. We then cleaned them using word stemming, normalization, and by dropping modifiers. Unfortunately, the patterns make systematic errors (e.g., extracting Canada as the name of a city from the phrase ‘Toronto, Canada and other cities.’) To address</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tatu</author>
<author>D Moldovan</author>
</authors>
<title>COGEX at RTE3.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>22--27</pages>
<contexts>
<context position="8118" citStr="Tatu and Moldovan (2007)" startWordPosition="1223" endWordPosition="1226">tional preferences (Resnik, 1997) for DIRT’s rules. The selectional preferences act as type restrictions 1089 Figure 1: Architecture of SHERLOCK. SHERLOCK learns inference rules offline and provides them to the HOLMES inference engine, which uses the rules to answer queries. on the arguments, and attempt to filter out incorrect inferences. While these approaches are useful, they are strictly more limited than the rules learned by SHERLOCK. The Recognizing Textual Entailment (RTE) task (Dagan et al., 2005) is to determine whether one sentence entails another. Approaches to RTE include those of Tatu and Moldovan (2007), which generates inference rules from WordNet lexical chains and a set of axiom templates, and Pennacchiotti and Zanzotto (2007), which learns inference rules based on similarity across entailment pairs. In contrast with this work, RTE systems reason over full sentences, but benefit by being given the sentences and training data. SHERLOCK operates over simpler Web extractions, but is not given guidance about which facts may interact. 3 System Description SHERLOCK takes as input a large set of open domain facts, and returns a set of weighted Horn-clause inference rules. Other systems (e.g., HO</context>
</contexts>
<marker>Tatu, Moldovan, 2007</marker>
<rawString>M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 22–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Wellman</author>
<author>J S Breese</author>
<author>R P Goldman</author>
</authors>
<title>From knowledge bases to decision models.</title>
<date>1992</date>
<journal>The Knowledge Engineering Review,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="22233" citStr="Wellman et al., 1992" startWordPosition="3516" endWordPosition="3519">bability. This is crucial in an open domain setting where most facts are false, which means the trivial rule that everything is false will have high accuracy. Even for true rules, the observed estimates p(Head(...)|Body(...)) « 1 due to missing data and noise. 3.5 Making Inferences In order to benefit from learned rules, we need an inference engine; with its linear-time scalability, HOLMES is a natural choice (Schoenmackers et al., 2008). As input HOLMES requires a target atom H(...), an evidence set E and weighted rule set R as input. It performs a form of knowledge based model construction (Wellman et al., 1992), first finding facts using logical inference, then estimating the confidence of each using a Markov Logic Network (Richardson and Domingos, 2006). Prior to running inference, it is necessary to assign a weight to each rule learned by SHERLOCK. Since the rules and inferences are based on a set of noisy and incomplete extractions, the algorithms used for both weight learning and inference should capture the following characteristics of our problem: C1. Any arbitrary unknown fact is highly unlikely to be true. C2. The more frequently a fact is extracted from the Web, the more likely it is to be </context>
</contexts>
<marker>Wellman, Breese, Goldman, 1992</marker>
<rawString>M.P. Wellman, J.S. Breese, and R.P. Goldman. 1992. From knowledge bases to decision models. The Knowledge Engineering Review, 7(1):35–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yates</author>
<author>O Etzioni</author>
</authors>
<title>Unsupervised resolution of objects and relations on the Web. In Procs. of HLT.</title>
<date>2007</date>
<contexts>
<context position="6991" citStr="Yates and Etzioni, 2007" startWordPosition="1041" endWordPosition="1044">led semi-supervised learning to extract a large knowledge base of instances, relations, and inference rules, bootstrapping from a few seed examples of each class and relation of interest and a few constraints among them. In contrast, SHERLOCK focuses mainly on learning inference rules, but does so without any manually specified seeds or constraints. Craven etal.(1998) also used ILP to help information extraction on the Web, but required training examples and focused on a single domain. Two other notable systems that learn inference rules from text are DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007). However, both DIRT and RESOLVER learn only a limited set of rules capturing synonyms, paraphrases, and simple entailments, not more expressive multipart Horn clauses. For example, these systems may learn the rule X acquired Y ==&gt;. X bought Y , which captures different ways of describing a purchase. Applications of these rules often depend on context (e.g., if a person acquires a skill, that does not mean they bought the skill). To add the necessary context, ISP (Pantel et al., 2007) learned selectional preferences (Resnik, 1997) for DIRT’s rules. The selectional preferences act as type restr</context>
</contexts>
<marker>Yates, Etzioni, 2007</marker>
<rawString>A. Yates and O. Etzioni. 2007. Unsupervised resolution of objects and relations on the Web. In Procs. of HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>