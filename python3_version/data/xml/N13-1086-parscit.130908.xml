<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001589">
<title confidence="0.9215665">
Using Document Summarization Techniques for Speech Data Subset
Selection
</title>
<author confidence="0.999308">
Kai Wei*, Yuzong Liu*, Katrin Kirchhoff , Jeff Bilmes
</author>
<affiliation confidence="0.998228">
Department of Eletrical Engineering
University of Washington
</affiliation>
<address confidence="0.783707">
Seattle, WA 98195, USA
</address>
<email confidence="0.999559">
{kaiwei,yzliu,katrin,bilmes}@ee.washington.edu
</email>
<sectionHeader confidence="0.99683" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992418666666667">
In this paper we leverage methods from sub-
modular function optimization developed for
document summarization and apply them to
the problem of subselecting acoustic data. We
evaluate our results on data subset selection
for a phone recognition task. Our framework
shows significant improvements over random
selection and previously proposed methods us-
ing a similar amount of resources.
</bodyText>
<sectionHeader confidence="0.998394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998914317073171">
Present-day applications in spoken language technol-
ogy (speech recognizers, keyword spotters, etc.) can
draw on an unprecedented amount of training data.
However, larger data sets come with increased de-
mands on computational resources; moreover, they
tend to include redundant information as their size
increases. Therefore, the performance gain curves
of large-scale systems with respect to the amount of
training data often show “diminishing returns”: new
data is often less valuable (in terms of performance
gain) when added to a larger pre-existing data set than
when added to a smaller pre-existing set (e.g.,(Moore,
2003)). Therefore it is of prime importance to de-
velop methods for data subset selection. We distin-
guish two data subselection scenarios: (a) a priori
selection of a data set before (re-)training a system;
in this case the goal is to subselect the existing data
set as well as possible, eliminating redundant infor-
mation; (b) selection for adaptation, where the goal
*These authors are joint first authors with equal contribu-
tions.
is to tune a system to a known development or test
set. While many studies have addressed the second
scenario, this paper investigates the first: our goal is
to select a smaller subset of the data that fits a given
’budget’ (e.g. maximum number of hours of data) but
provides, to the extent possible, as much information
as the complete data set. Additionally, our selection
method should be a low-resource method that does
not require an already-trained complex system such
as an existing word recognizer.
This problem is akin to unsupervised data ’sum-
marization’. In (Lin and Bilmes, 2009) a novel class
of summarization techniques based on submodular
function optimization were proposed for extractive
document summarization. Interestingly, these meth-
ods can also be applied to speech data ’summariza-
tion’ with only small modifications. In the following
sections we develop a submodular framework for
speech data summarization and evaluate it on a proof-
of-concept phone recognition task.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999718916666667">
Most approaches to data subset selection in speech
have relied on “rank-and-select” approaches that de-
termine the utility of each sample in the data set,
rank all samples according to their utility scores, and
then select the top N samples. In weakly supervised
approaches (e.g.,(Kemp and Waibel, 1998; Lamel
et al., 2002; Hakkani-Tur et al., 2002), utility is re-
lated to the confidence of an existing word recognizer
on new data samples: untranscribed training data is
automatically transcribed using an existing baseline
speech recognizer, and individual utterances are se-
lected as additional training data if they have low
</bodyText>
<page confidence="0.975448">
721
</page>
<subsectionHeader confidence="0.294046">
Proceedings of NAACL-HLT 2013, pages 721–726,
</subsectionHeader>
<bodyText confidence="0.984898625">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
confidence. These are active learning approaches
suitable for a scenario where a well-trained speech
recognizer is already available and additional data
for retraining needs to be selected. However, we
would like to reduce available training data ahead of
time with a low-resource approach. In (Chen et al.,
2009) individual samples are selected for the purpose
of discriminative training by considering phone ac-
curacy and the frame-level entropy of the Gaussian
posteriors. (Itoh et al., 2012) use a utility function
consisting of the entropy of word hypothesis N-best
lists and the representativeness of the sample using a
phone-based TF-IDF measure. The latter is compa-
rable to methods used in this paper, though the first
term in their objective function still requires a word
recognizer. In (Wu et al., 2007) acoustic training data
associated with transcriptions is subselected to max-
imize the entropy of the distribution over linguistic
units (phones or words). Most importantly, all these
methods select samples in a greedy fashion without
optimality guarantees. As we will explain in the next
section, greedy selection is near-optimal only when
applied to monotone submodular functions.
</bodyText>
<sectionHeader confidence="0.987595" genericHeader="method">
3 Submodular Functions
</sectionHeader>
<bodyText confidence="0.99987780952381">
Submodular functions (Edmonds, 1970) have been
widely studied in mathematics, economics, and op-
erations research and have recently attracted interest
in machine learning (Krause and Guestrin, 2011). A
submodular function is defined as follows: Given a fi-
nite ground set of objects (samples) V = {v1, ..., v.,,}
and a function f : 2V - 4R+ that returns a real value
for any subset S C_ V , f is submodular if VA C_ B,
and v V B, f(A + v) − f(A) &gt; f(B + v) − f(B).
That is, the incremental “value” of v decreases when
the set in which v is considered grows from A to B.
Powerful optimization guarantees exist for certain
subtypes of submodular functions. If, for example,
the function is monotone submodular, i.e. VA C_
B, f(A) &lt; f(B), then it can be maximized, under
a cardinality constraint, by a greedy algorithm that
scales to extremely large data sets, and finds a solu-
tion guaranteed to approximate the optimal solution
to within a constant factor 1 − 1/e (Nemhauser et al.,
1978). Submodular functions can be considered the
discrete analog of convexity.
</bodyText>
<subsectionHeader confidence="0.997705">
3.1 Submodular Document Summarization
</subsectionHeader>
<bodyText confidence="0.9997326">
In (Lin and Bilmes, 2011) submodular functions were
recently applied to extractive document summariza-
tion. The problem was formulated as a monotone
submodular function that had to be maximized sub-
ject to cardinality or knapsack constraints:
</bodyText>
<equation confidence="0.977446">
argmaxScV {f(S) : c(S) &lt; K} (1)
</equation>
<bodyText confidence="0.99983625">
where V is the set of sentences to be summarized, K
is the maximum number of sentences to be selected,
and c(·) &gt; 0 is sentence cost. f(S) was instantiated
by a form of saturated coverage:
</bodyText>
<equation confidence="0.995608">
fSC(S) = � min{Ci(S),αCi(V )} (2)
iEV
</equation>
<bodyText confidence="0.999868888888889">
where Ci(S) = EjES wij, and where wij &gt; 0 in-
dicates the similarity between sentences i and j —
Ci : 2V - 4R is itself monotone submodular (modu-
lar in fact) and 0 &lt; α &lt; 1 is a saturation coefficient.
fSC(S) is monotone submodular and therefore has
the previously mentioned performance guarantees.
The weighting function w was implemented as the
cosine similarity between TF-IDF weighted n-gram
count vectors for the sentences in the dataset.
</bodyText>
<subsectionHeader confidence="0.999795">
3.2 Submodular Speech Summarization
</subsectionHeader>
<bodyText confidence="0.999971181818182">
Similar to the procedure described above we can treat
the task of subselecting an acoustic data set as an
extractive summarization problem. For our a priori
data selection scenario we would like to extract those
training samples that jointly are representative of
the total data set. Initial explorations of submodular
functions for speech data can be found in (Lin and
Bilmes, 2009), where submodular functions were
used in combination with a purely acoustic similarity
measure (Fisher kernel). In addition Equation 2 the
facility location function was used:
</bodyText>
<equation confidence="0.969281333333333">
�
ff��(S) =
iEV
</equation>
<bodyText confidence="0.999913">
Here our focus is on utilizing methods that move
beyond purely acoustic similarity measures and con-
sider kernels derived from discrete representations
of the acoustic signal. To this end we first run a to-
kenizer over the acoustic signal that converts it into
a sequence of discrete labels. In our case we use a
</bodyText>
<equation confidence="0.953754">
wij (3)
max
jES
</equation>
<page confidence="0.972922">
722
</page>
<bodyText confidence="0.992921083333333">
simple bottom-up monophone recognizer (without
higher-level constraints such as a phone language
model) that produces phone labels. We then use the
hypothesized sequence of phonetic labels to compute
two different sentence similarity measures: (a) co-
sine similarity using TF-IDF weighted phone n-gram
counts, and (b) string kernels. We compare their
performance to that of the Fisher kernel as a purely
acoustic similarity measure.
TF-EDF weighted cosine similarity
The cosine similarity between phone sequences si
and sj is computed as
</bodyText>
<equation confidence="0.9128728">
simij =
qP qP
w∈si tf2 w,si idf2 w∈sj tf2 w,sj idf2
w w
(4)
</equation>
<bodyText confidence="0.967759285714286">
where tfw,si is the count of n-gram w in si and idfw
is the inverse document count of w (each sentence is
a “document”). We use n = 1, 2, 3.
String kernel
The particular string kernel we use is a gapped,
weighted subsequence kernel of the type described in
(Rousu and Shawe-Taylor, 2005). Formally, we de-
fine a sentence s as a concatenation of symbols from
a finite alphabet Σ (here the inventory of phones) and
an embedding function from strings to feature vec-
tors, φ : Σ∗ - 4H. The string kernel function K(s, t)
computes the distance between the resulting vectors
for two sentences si and sj. The embedding function
is defined as
</bodyText>
<equation confidence="0.9786375">
Xφku(s) := λ|i |u E Σk (5)
i:u=s(i)
</equation>
<bodyText confidence="0.999679">
where k is the maximum length of subsequences,
|i |is the length of i, and λ is a penalty parameter
for each gap encountered in the subsequence. K is
defined as
</bodyText>
<equation confidence="0.9647385">
XK(si, sj) = (φu(si), φu(sj))wu (6)
u
</equation>
<bodyText confidence="0.9971585">
where w is a weight dependent on the length of
u, l(u). Finally, the kernel score is normalized by
pK(si, si) · K(sj, sj) to discourage long sentences
from being favored.
</bodyText>
<subsectionHeader confidence="0.618945">
Fisher kernel
</subsectionHeader>
<bodyText confidence="0.98483875">
The Fisher kernel is based on the vector of derivatives
UX of the log-likelihood of the acoustic data (X)
with respect to the parameters in the phone HMMs
θ1, ..., θm for m models, having similarity score:
</bodyText>
<equation confidence="0.9874915">
simij = (max
i&apos;,j&apos; di&apos;j&apos;) − dij, where dij = ||U0i − U0 j||1,
UθX = Vθ log P(X|θ), and U0X = Uθ�X o Uθ2
x , ..., oUθmX .
</equation>
<sectionHeader confidence="0.934657" genericHeader="method">
4 Data and Systems
</sectionHeader>
<bodyText confidence="0.999948414634146">
We evaluate our approach on subselecting training
data from the TIMIT corpus for training a phone rec-
ognizer. Although this not a large-scale data task, it
is an appropriate proof-of-concept task for rapidly
testing different combinations of submodular func-
tions and similarity measures. Our goal is to focus
on acoustic modeling only; we thus look at phone
recognition performance and do not have to take into
account potential interactions with a language model.
We also chose a simple acoustic model, a monophone
HMM recognizer, rather than a more powerful but
computationally complex model in order to ensure
quick experimental turnaround time. Note that the
goal of this study is not to obtain the highest phone
accuracy possible; what is important is the relative
performance of the different subset selection meth-
ods, especially on small data subsets.
The sizes of the training, development and test data
are 4620, 200 and 192 utterances, respectively. Pre-
processing was done by extracting 39-dimensional
MFCC feature vectors every 10 ms, with a window
of 25.6ms. Speaker mean and variance normaliza-
tion was applied. A 16-component Gaussian mixture
monophone HMM system was trained on the full data
set to generate parameters for the Fisher kernel and
phone sequences for the string kernel and TF-IDF
based similarity measures.
Following the selection of subsets (2.5%, 5%, 10%,
20%, 30%, 40%, 50%, 60%, 70% and 80% of the
data, measured as percentage of non-silence speech
frames), we train a 3-state HMM monophone recog-
nizer for all 48 TIMIT phone classes on the result-
ing sets and evaluate the performance on the core
test set of 192 utterances, collapsing the 48 classes
into 39 in line with standard practice (Lee and Hon,
1989). The HMM state output distributions are mod-
eled by diagonal-covariance Gaussian mixtures with
the number of Gaussians ranging between 4 and 64,
depending on the data size.
As a baseline we perform 100 random draws of
the specified subset sizes and average the results.
</bodyText>
<equation confidence="0.814592">
�` 2
/�wEsi tfw,si X tfw,s� X idfw
</equation>
<page confidence="0.992155">
723
</page>
<bodyText confidence="0.999337">
The second baseline consists of the method in (Wu et
al., 2007), where utterances are selected to maximize
the entropy of the distribution over phones in the
selected subset.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999985333333333">
We tested the three different similarity measures de-
scribed above in combination with the submodular
functions in Equations 2 and 3. The parameters of
the gapped string kernel (i.e. the kernel order (k), the
gap penalty (A), and the contiguous substring length
l) were optimized on the development set. The best
values were A = 0.1, k = 4,l = 3. We found that
facility location was superior to saturated cover func-
tion across the board.
</bodyText>
<subsectionHeader confidence="0.660248">
Comparison of different data subset selection methods
</subsectionHeader>
<bodyText confidence="0.667027">
Comparison of different submodular functions
</bodyText>
<figureCaption confidence="0.995949">
Figure 2: Phone accuracy obtained by random selection,
facility location function, and saturated coverage function
(string kernel similarity measure).
</figureCaption>
<figure confidence="0.997357555555555">
Phone Accuracy (%)
Percentage of Speech in Selected Subset
45
65
60
55
50
2.5 5 10 20 30 40 60 60 70 80
Phone Accuracy (%)
</figure>
<figureCaption confidence="0.99809525">
Figure 1: Phone accuracy for different subset sizes; each
block of bars lists, from bottom to top: random baseline,
entropy baseline, Fisher kernel, TF-IDF (unigram), TF-
IDF (bigram), TF-IDF (trigram), string kernel.
</figureCaption>
<bodyText confidence="0.993095675">
Figure 1 shows the performance of the random and
entropy-based baselines as well as the performance
of the facility location function with different sim-
ilarity measures. The entropy-based baseline beats
the random baseline for most percentage cases but
is otherwise the lowest-performing method overall.
Note that this baseline uses the true transcriptions in
line with (Wu et al., 2007) rather than the hypothe-
sized phone labels output by our recognizer. The low
performance and the fact that it is even outperformed
by the random baseline in the 2.5% and 70% cases
may be because the selection method encourages
highly diverse but not very representative subsets.
Furthermore, the entropy-based baseline utilizes a
non-submodular objective function with a heuristic
greedy search method. No theoretical guarantee of
optimality can be made for the subset found by this
method.
Among the different similarity measures the Fisher
kernel outperforms the baseline methods but has
lower performance than the TF-IDF kernel and the
string kernel. The best performance is obtained with
the string kernel, especially when using small train-
ing data sets (2.5%-10%). The submodular selection
methods yield significant improvements (p &lt; 0.05)
over both the random baseline and over the entropy-
based method.
We also investigated using different submodular
functions, i.e. the facility location function and the
saturated coverage function. Figure 2 shows the per-
formance of the facility location (ffa,) and saturated
coverage (fSC) functions in combination with the
string kernel similarity measure. The reason ffa,
outperforms fSC is that fSC primarily controls for
over-coverage of any element not in the subset via the
α saturation hyper-parameter. However, it does not
ensure that every non-selected element has good rep-
resentation in the subset. fSC measures the quality of
the subset by how well each individual element out-
side the subset has a surrogate within the subset (via
</bodyText>
<figure confidence="0.974552676470588">
Percentage of Speech in Selected Subset
2.5
20
30
40
50
60
70
80
10
5
40 45 50 55 60 65
string kernel
TF7IDF trigram
TF7IDF bigram
TF7IDF unigram
Fisher kernel
entropy
random
724
2.5p 5p 10p 20p 30p 2.5p 5p 10p 20p 30p
TF.IDF unigram TF.IDF bigram
65
60
55
50
45
40
65
60
55
50
45
40
</figure>
<bodyText confidence="0.9991425">
produce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. The views and conclusions contained herein
are those of the authors and should not be interpreted
as necessarily representing the official policies or
endorsements, either expressed or implied, of Intelli-
gence Advanced Research Projects Activity (IARPA)
or the U.S. Government.
</bodyText>
<figure confidence="0.925472">
65 65
TF.IDF trigram string kernel
</figure>
<figureCaption confidence="0.9950975">
Figure 3: Phone accuracy for true vs. hypothesized phone
labels, for string-based similarity measures.
</figureCaption>
<bodyText confidence="0.992299769230769">
the max function) and hence tends to model complete
coverage better, leading to better results.
Finally we examined whether using hypothesized
phone sequences vs. the true transcriptions has nega-
tive effects. Figure 3 shows that this is not the case:
interestingly, the hypothesized labels even result in
slightly better results. This may be because the rec-
ognized phone sequences are a function of both the
underlying phonetic sequences that were spoken and
the acoustic signal characteristics, such as the speaker
and channel. The true transcriptions, on the other
hand, are able to provide information only about pho-
netic as opposed to acoustic characteristics.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99997225">
We have presented a low-resource framework for
acoustic data subset selection based on submodular
function optimization, which was previously devel-
oped for document summarization. Evaluation on a
proof-of-concept task has shown that the method is
successful at selecting data subsets that outperform
subsets selected randomly or by a previously pro-
posed low-resource method. We note that the best
selection strategies for the experimental conditions
tested here involve similarity measures based on a
discrete tokenization of the speech signal rather than
direct acoustic similarity measures.
</bodyText>
<sectionHeader confidence="0.998247" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.90722625">
This material is based on research sponsored by
Intelligence Advanced Research Projects Activity
(IARPA) under agreement number FA8650-12-2-
7263. The U.S. Government is authorized to re-
</bodyText>
<sectionHeader confidence="0.996866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999742095238095">
B. Chen, S.H Liu, and F.H. Chu. 2009. Training data se-
lection for improving discriminative training of acoustic
models. Pattern Recognition Letters, 30:1228–1235.
J. Edmonds, 1970. Combinatorial Structures and their Ap-
plications, chapter Submodular functions, matroids and
certain polyhedra, pages 69–87. Gordon and Breach.
G. Hakkani-Tur, G. Riccardi, and A. Gorin. 2002. Active
learning for automatic speech recognition. In Proc. of
ICASSP, pages 3904–3907.
N. Itoh, T.N. Sainath, D.N. Jiang, J. Zhou, and B. Ramab-
hadran. 2012. N-best entropy based data selection for
acoustic modeling. In Proceedings of ICASSP.
Thomas Kemp and Alex Waibel. 1998. Unsupervised
training of a speech recognizer using TV broadcasts.
In in Proceedings of the International Conference on
Spoken Language Processing (ICSLP-98), pages 2207–
2210.
A. Krause and C. Guestrin. 2011. Submodularity and its
applications in optimized information gathering. ACM
Transactions on Intelligent Systems and Technology,
2(4).
L. Lamel, J.L. Gauvain, and G. Adda. 2002. Lightly
supervised and unsupervised acoustic model training.
Computer, Speech and Language, 16:116 – 125.
K.F. Lee and H.W. Hon. 1989. Speaker-independent
phone recognition using Hidden Markov Models. IEEE
Trans. ASSP, 37:1641–1648.
Hui Lin and Jeff A. Bilmes. 2009. How to select a good
training-data subset for transcription: Submodular ac-
tive selection for sequences. In Proc. Annual Confer-
ence of the International Speech Communication Asso-
ciation (INTERSPEECH), Brighton, UK, September.
H. Lin and J. Bilmes. 2011. A class of submodular
functions for document summarization. In Proceedings
of ACL.
R.K. Moore. 2003. A comparison of the data require-
ments of automatic speech recognition systems and
human listeners. In Proceedings of Eurospeech, pages
2581–2584.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978.
An analysis of approximations for maximizing submod-
ular functions-I. Math. Program., 14:265–294.
</reference>
<figure confidence="0.985391666666667">
60
55
50
45
40
2.5p 5p 10p 20p 30p
60
55
50
45
40
2.5p 5p 10p 20p 30p
</figure>
<page confidence="0.97983">
725
</page>
<reference confidence="0.9968686">
J. Rousu and J. Shawe-Taylor. 2005. Efficien computa-
tion of of gapped substring kernels for large alphabets.
Journal of Machine Leaning Research, 6:13231344.
Y. Wu, R. Zhang, and A. Rudnicky. 2007. Data selection
for speech recognition. In Proceedings of ASRU.
</reference>
<page confidence="0.998409">
726
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.992545">
<title confidence="0.9998065">Using Document Summarization Techniques for Speech Data Subset Selection</title>
<author confidence="0.999568">Yuzong Katrin Kirchhoff</author>
<affiliation confidence="0.9998065">Department of Eletrical University of</affiliation>
<address confidence="0.999545">Seattle, WA 98195,</address>
<abstract confidence="0.9994107">In this paper we leverage methods from submodular function optimization developed for document summarization and apply them to the problem of subselecting acoustic data. We evaluate our results on data subset selection for a phone recognition task. Our framework shows significant improvements over random selection and previously proposed methods using a similar amount of resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>S H Liu</author>
<author>F H Chu</author>
</authors>
<title>Training data selection for improving discriminative training of acoustic models.</title>
<date>2009</date>
<journal>Pattern Recognition Letters,</journal>
<pages>30--1228</pages>
<contexts>
<context position="3799" citStr="Chen et al., 2009" startWordPosition="571" endWordPosition="574">raining data is automatically transcribed using an existing baseline speech recognizer, and individual utterances are selected as additional training data if they have low 721 Proceedings of NAACL-HLT 2013, pages 721–726, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics confidence. These are active learning approaches suitable for a scenario where a well-trained speech recognizer is already available and additional data for retraining needs to be selected. However, we would like to reduce available training data ahead of time with a low-resource approach. In (Chen et al., 2009) individual samples are selected for the purpose of discriminative training by considering phone accuracy and the frame-level entropy of the Gaussian posteriors. (Itoh et al., 2012) use a utility function consisting of the entropy of word hypothesis N-best lists and the representativeness of the sample using a phone-based TF-IDF measure. The latter is comparable to methods used in this paper, though the first term in their objective function still requires a word recognizer. In (Wu et al., 2007) acoustic training data associated with transcriptions is subselected to maximize the entropy of the</context>
</contexts>
<marker>Chen, Liu, Chu, 2009</marker>
<rawString>B. Chen, S.H Liu, and F.H. Chu. 2009. Training data selection for improving discriminative training of acoustic models. Pattern Recognition Letters, 30:1228–1235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Combinatorial Structures and their Applications, chapter Submodular functions, matroids and certain polyhedra,</title>
<date>1970</date>
<pages>69--87</pages>
<publisher>Gordon and Breach.</publisher>
<contexts>
<context position="4740" citStr="Edmonds, 1970" startWordPosition="715" endWordPosition="716">IDF measure. The latter is comparable to methods used in this paper, though the first term in their objective function still requires a word recognizer. In (Wu et al., 2007) acoustic training data associated with transcriptions is subselected to maximize the entropy of the distribution over linguistic units (phones or words). Most importantly, all these methods select samples in a greedy fashion without optimality guarantees. As we will explain in the next section, greedy selection is near-optimal only when applied to monotone submodular functions. 3 Submodular Functions Submodular functions (Edmonds, 1970) have been widely studied in mathematics, economics, and operations research and have recently attracted interest in machine learning (Krause and Guestrin, 2011). A submodular function is defined as follows: Given a finite ground set of objects (samples) V = {v1, ..., v.,,} and a function f : 2V - 4R+ that returns a real value for any subset S C_ V , f is submodular if VA C_ B, and v V B, f(A + v) − f(A) &gt; f(B + v) − f(B). That is, the incremental “value” of v decreases when the set in which v is considered grows from A to B. Powerful optimization guarantees exist for certain subtypes of submo</context>
</contexts>
<marker>Edmonds, 1970</marker>
<rawString>J. Edmonds, 1970. Combinatorial Structures and their Applications, chapter Submodular functions, matroids and certain polyhedra, pages 69–87. Gordon and Breach.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hakkani-Tur</author>
<author>G Riccardi</author>
<author>A Gorin</author>
</authors>
<title>Active learning for automatic speech recognition.</title>
<date>2002</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>3904--3907</pages>
<contexts>
<context position="3075" citStr="Hakkani-Tur et al., 2002" startWordPosition="464" endWordPosition="467">Interestingly, these methods can also be applied to speech data ’summarization’ with only small modifications. In the following sections we develop a submodular framework for speech data summarization and evaluate it on a proofof-concept phone recognition task. 2 Related Work Most approaches to data subset selection in speech have relied on “rank-and-select” approaches that determine the utility of each sample in the data set, rank all samples according to their utility scores, and then select the top N samples. In weakly supervised approaches (e.g.,(Kemp and Waibel, 1998; Lamel et al., 2002; Hakkani-Tur et al., 2002), utility is related to the confidence of an existing word recognizer on new data samples: untranscribed training data is automatically transcribed using an existing baseline speech recognizer, and individual utterances are selected as additional training data if they have low 721 Proceedings of NAACL-HLT 2013, pages 721–726, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics confidence. These are active learning approaches suitable for a scenario where a well-trained speech recognizer is already available and additional data for retraining needs to be selected.</context>
</contexts>
<marker>Hakkani-Tur, Riccardi, Gorin, 2002</marker>
<rawString>G. Hakkani-Tur, G. Riccardi, and A. Gorin. 2002. Active learning for automatic speech recognition. In Proc. of ICASSP, pages 3904–3907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Itoh</author>
<author>T N Sainath</author>
<author>D N Jiang</author>
<author>J Zhou</author>
<author>B Ramabhadran</author>
</authors>
<title>N-best entropy based data selection for acoustic modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="3980" citStr="Itoh et al., 2012" startWordPosition="598" endWordPosition="601">edings of NAACL-HLT 2013, pages 721–726, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics confidence. These are active learning approaches suitable for a scenario where a well-trained speech recognizer is already available and additional data for retraining needs to be selected. However, we would like to reduce available training data ahead of time with a low-resource approach. In (Chen et al., 2009) individual samples are selected for the purpose of discriminative training by considering phone accuracy and the frame-level entropy of the Gaussian posteriors. (Itoh et al., 2012) use a utility function consisting of the entropy of word hypothesis N-best lists and the representativeness of the sample using a phone-based TF-IDF measure. The latter is comparable to methods used in this paper, though the first term in their objective function still requires a word recognizer. In (Wu et al., 2007) acoustic training data associated with transcriptions is subselected to maximize the entropy of the distribution over linguistic units (phones or words). Most importantly, all these methods select samples in a greedy fashion without optimality guarantees. As we will explain in th</context>
</contexts>
<marker>Itoh, Sainath, Jiang, Zhou, Ramabhadran, 2012</marker>
<rawString>N. Itoh, T.N. Sainath, D.N. Jiang, J. Zhou, and B. Ramabhadran. 2012. N-best entropy based data selection for acoustic modeling. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Kemp</author>
<author>Alex Waibel</author>
</authors>
<title>Unsupervised training of a speech recognizer using TV broadcasts. In</title>
<date>1998</date>
<booktitle>in Proceedings of the International Conference on Spoken Language Processing (ICSLP-98),</booktitle>
<pages>2207--2210</pages>
<contexts>
<context position="3028" citStr="Kemp and Waibel, 1998" startWordPosition="456" endWordPosition="459">sed for extractive document summarization. Interestingly, these methods can also be applied to speech data ’summarization’ with only small modifications. In the following sections we develop a submodular framework for speech data summarization and evaluate it on a proofof-concept phone recognition task. 2 Related Work Most approaches to data subset selection in speech have relied on “rank-and-select” approaches that determine the utility of each sample in the data set, rank all samples according to their utility scores, and then select the top N samples. In weakly supervised approaches (e.g.,(Kemp and Waibel, 1998; Lamel et al., 2002; Hakkani-Tur et al., 2002), utility is related to the confidence of an existing word recognizer on new data samples: untranscribed training data is automatically transcribed using an existing baseline speech recognizer, and individual utterances are selected as additional training data if they have low 721 Proceedings of NAACL-HLT 2013, pages 721–726, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics confidence. These are active learning approaches suitable for a scenario where a well-trained speech recognizer is already available and addit</context>
</contexts>
<marker>Kemp, Waibel, 1998</marker>
<rawString>Thomas Kemp and Alex Waibel. 1998. Unsupervised training of a speech recognizer using TV broadcasts. In in Proceedings of the International Conference on Spoken Language Processing (ICSLP-98), pages 2207– 2210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krause</author>
<author>C Guestrin</author>
</authors>
<title>Submodularity and its applications in optimized information gathering.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="4901" citStr="Krause and Guestrin, 2011" startWordPosition="736" endWordPosition="739">er. In (Wu et al., 2007) acoustic training data associated with transcriptions is subselected to maximize the entropy of the distribution over linguistic units (phones or words). Most importantly, all these methods select samples in a greedy fashion without optimality guarantees. As we will explain in the next section, greedy selection is near-optimal only when applied to monotone submodular functions. 3 Submodular Functions Submodular functions (Edmonds, 1970) have been widely studied in mathematics, economics, and operations research and have recently attracted interest in machine learning (Krause and Guestrin, 2011). A submodular function is defined as follows: Given a finite ground set of objects (samples) V = {v1, ..., v.,,} and a function f : 2V - 4R+ that returns a real value for any subset S C_ V , f is submodular if VA C_ B, and v V B, f(A + v) − f(A) &gt; f(B + v) − f(B). That is, the incremental “value” of v decreases when the set in which v is considered grows from A to B. Powerful optimization guarantees exist for certain subtypes of submodular functions. If, for example, the function is monotone submodular, i.e. VA C_ B, f(A) &lt; f(B), then it can be maximized, under a cardinality constraint, by a </context>
</contexts>
<marker>Krause, Guestrin, 2011</marker>
<rawString>A. Krause and C. Guestrin. 2011. Submodularity and its applications in optimized information gathering. ACM Transactions on Intelligent Systems and Technology, 2(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>J L Gauvain</author>
<author>G Adda</author>
</authors>
<title>Lightly supervised and unsupervised acoustic model training.</title>
<date>2002</date>
<journal>Computer, Speech and Language,</journal>
<pages>16--116</pages>
<contexts>
<context position="3048" citStr="Lamel et al., 2002" startWordPosition="460" endWordPosition="463">ment summarization. Interestingly, these methods can also be applied to speech data ’summarization’ with only small modifications. In the following sections we develop a submodular framework for speech data summarization and evaluate it on a proofof-concept phone recognition task. 2 Related Work Most approaches to data subset selection in speech have relied on “rank-and-select” approaches that determine the utility of each sample in the data set, rank all samples according to their utility scores, and then select the top N samples. In weakly supervised approaches (e.g.,(Kemp and Waibel, 1998; Lamel et al., 2002; Hakkani-Tur et al., 2002), utility is related to the confidence of an existing word recognizer on new data samples: untranscribed training data is automatically transcribed using an existing baseline speech recognizer, and individual utterances are selected as additional training data if they have low 721 Proceedings of NAACL-HLT 2013, pages 721–726, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics confidence. These are active learning approaches suitable for a scenario where a well-trained speech recognizer is already available and additional data for retra</context>
</contexts>
<marker>Lamel, Gauvain, Adda, 2002</marker>
<rawString>L. Lamel, J.L. Gauvain, and G. Adda. 2002. Lightly supervised and unsupervised acoustic model training. Computer, Speech and Language, 16:116 – 125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K F Lee</author>
<author>H W Hon</author>
</authors>
<title>Speaker-independent phone recognition using Hidden Markov Models.</title>
<date>1989</date>
<journal>IEEE Trans. ASSP,</journal>
<pages>37--1641</pages>
<contexts>
<context position="11421" citStr="Lee and Hon, 1989" startWordPosition="1862" endWordPosition="1865">. A 16-component Gaussian mixture monophone HMM system was trained on the full data set to generate parameters for the Fisher kernel and phone sequences for the string kernel and TF-IDF based similarity measures. Following the selection of subsets (2.5%, 5%, 10%, 20%, 30%, 40%, 50%, 60%, 70% and 80% of the data, measured as percentage of non-silence speech frames), we train a 3-state HMM monophone recognizer for all 48 TIMIT phone classes on the resulting sets and evaluate the performance on the core test set of 192 utterances, collapsing the 48 classes into 39 in line with standard practice (Lee and Hon, 1989). The HMM state output distributions are modeled by diagonal-covariance Gaussian mixtures with the number of Gaussians ranging between 4 and 64, depending on the data size. As a baseline we perform 100 random draws of the specified subset sizes and average the results. �` 2 /�wEsi tfw,si X tfw,s� X idfw 723 The second baseline consists of the method in (Wu et al., 2007), where utterances are selected to maximize the entropy of the distribution over phones in the selected subset. 5 Experiments We tested the three different similarity measures described above in combination with the submodular f</context>
</contexts>
<marker>Lee, Hon, 1989</marker>
<rawString>K.F. Lee and H.W. Hon. 1989. Speaker-independent phone recognition using Hidden Markov Models. IEEE Trans. ASSP, 37:1641–1648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff A Bilmes</author>
</authors>
<title>How to select a good training-data subset for transcription: Submodular active selection for sequences.</title>
<date>2009</date>
<booktitle>In Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH),</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="2312" citStr="Lin and Bilmes, 2009" startWordPosition="348" endWordPosition="351">ors with equal contributions. is to tune a system to a known development or test set. While many studies have addressed the second scenario, this paper investigates the first: our goal is to select a smaller subset of the data that fits a given ’budget’ (e.g. maximum number of hours of data) but provides, to the extent possible, as much information as the complete data set. Additionally, our selection method should be a low-resource method that does not require an already-trained complex system such as an existing word recognizer. This problem is akin to unsupervised data ’summarization’. In (Lin and Bilmes, 2009) a novel class of summarization techniques based on submodular function optimization were proposed for extractive document summarization. Interestingly, these methods can also be applied to speech data ’summarization’ with only small modifications. In the following sections we develop a submodular framework for speech data summarization and evaluate it on a proofof-concept phone recognition task. 2 Related Work Most approaches to data subset selection in speech have relied on “rank-and-select” approaches that determine the utility of each sample in the data set, rank all samples according to t</context>
<context position="7161" citStr="Lin and Bilmes, 2009" startWordPosition="1131" endWordPosition="1134">ore has the previously mentioned performance guarantees. The weighting function w was implemented as the cosine similarity between TF-IDF weighted n-gram count vectors for the sentences in the dataset. 3.2 Submodular Speech Summarization Similar to the procedure described above we can treat the task of subselecting an acoustic data set as an extractive summarization problem. For our a priori data selection scenario we would like to extract those training samples that jointly are representative of the total data set. Initial explorations of submodular functions for speech data can be found in (Lin and Bilmes, 2009), where submodular functions were used in combination with a purely acoustic similarity measure (Fisher kernel). In addition Equation 2 the facility location function was used: � ff��(S) = iEV Here our focus is on utilizing methods that move beyond purely acoustic similarity measures and consider kernels derived from discrete representations of the acoustic signal. To this end we first run a tokenizer over the acoustic signal that converts it into a sequence of discrete labels. In our case we use a wij (3) max jES 722 simple bottom-up monophone recognizer (without higher-level constraints such</context>
</contexts>
<marker>Lin, Bilmes, 2009</marker>
<rawString>Hui Lin and Jeff A. Bilmes. 2009. How to select a good training-data subset for transcription: Submodular active selection for sequences. In Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), Brighton, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5826" citStr="Lin and Bilmes, 2011" startWordPosition="907" endWordPosition="910">decreases when the set in which v is considered grows from A to B. Powerful optimization guarantees exist for certain subtypes of submodular functions. If, for example, the function is monotone submodular, i.e. VA C_ B, f(A) &lt; f(B), then it can be maximized, under a cardinality constraint, by a greedy algorithm that scales to extremely large data sets, and finds a solution guaranteed to approximate the optimal solution to within a constant factor 1 − 1/e (Nemhauser et al., 1978). Submodular functions can be considered the discrete analog of convexity. 3.1 Submodular Document Summarization In (Lin and Bilmes, 2011) submodular functions were recently applied to extractive document summarization. The problem was formulated as a monotone submodular function that had to be maximized subject to cardinality or knapsack constraints: argmaxScV {f(S) : c(S) &lt; K} (1) where V is the set of sentences to be summarized, K is the maximum number of sentences to be selected, and c(·) &gt; 0 is sentence cost. f(S) was instantiated by a form of saturated coverage: fSC(S) = � min{Ci(S),αCi(V )} (2) iEV where Ci(S) = EjES wij, and where wij &gt; 0 indicates the similarity between sentences i and j — Ci : 2V - 4R is itself monoton</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>H. Lin and J. Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R K Moore</author>
</authors>
<title>A comparison of the data requirements of automatic speech recognition systems and human listeners.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>2581--2584</pages>
<contexts>
<context position="1294" citStr="Moore, 2003" startWordPosition="181" endWordPosition="182">lications in spoken language technology (speech recognizers, keyword spotters, etc.) can draw on an unprecedented amount of training data. However, larger data sets come with increased demands on computational resources; moreover, they tend to include redundant information as their size increases. Therefore, the performance gain curves of large-scale systems with respect to the amount of training data often show “diminishing returns”: new data is often less valuable (in terms of performance gain) when added to a larger pre-existing data set than when added to a smaller pre-existing set (e.g.,(Moore, 2003)). Therefore it is of prime importance to develop methods for data subset selection. We distinguish two data subselection scenarios: (a) a priori selection of a data set before (re-)training a system; in this case the goal is to subselect the existing data set as well as possible, eliminating redundant information; (b) selection for adaptation, where the goal *These authors are joint first authors with equal contributions. is to tune a system to a known development or test set. While many studies have addressed the second scenario, this paper investigates the first: our goal is to select a sma</context>
</contexts>
<marker>Moore, 2003</marker>
<rawString>R.K. Moore. 2003. A comparison of the data requirements of automatic speech recognition systems and human listeners. In Proceedings of Eurospeech, pages 2581–2584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G L Nemhauser</author>
<author>L A Wolsey</author>
<author>M L Fisher</author>
</authors>
<title>An analysis of approximations for maximizing submodular functions-I.</title>
<date>1978</date>
<journal>Math. Program.,</journal>
<pages>14--265</pages>
<contexts>
<context position="5688" citStr="Nemhauser et al., 1978" startWordPosition="888" endWordPosition="891">lue for any subset S C_ V , f is submodular if VA C_ B, and v V B, f(A + v) − f(A) &gt; f(B + v) − f(B). That is, the incremental “value” of v decreases when the set in which v is considered grows from A to B. Powerful optimization guarantees exist for certain subtypes of submodular functions. If, for example, the function is monotone submodular, i.e. VA C_ B, f(A) &lt; f(B), then it can be maximized, under a cardinality constraint, by a greedy algorithm that scales to extremely large data sets, and finds a solution guaranteed to approximate the optimal solution to within a constant factor 1 − 1/e (Nemhauser et al., 1978). Submodular functions can be considered the discrete analog of convexity. 3.1 Submodular Document Summarization In (Lin and Bilmes, 2011) submodular functions were recently applied to extractive document summarization. The problem was formulated as a monotone submodular function that had to be maximized subject to cardinality or knapsack constraints: argmaxScV {f(S) : c(S) &lt; K} (1) where V is the set of sentences to be summarized, K is the maximum number of sentences to be selected, and c(·) &gt; 0 is sentence cost. f(S) was instantiated by a form of saturated coverage: fSC(S) = � min{Ci(S),αCi(</context>
</contexts>
<marker>Nemhauser, Wolsey, Fisher, 1978</marker>
<rawString>G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978. An analysis of approximations for maximizing submodular functions-I. Math. Program., 14:265–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rousu</author>
<author>J Shawe-Taylor</author>
</authors>
<title>Efficien computation of of gapped substring kernels for large alphabets.</title>
<date>2005</date>
<journal>Journal of Machine Leaning Research,</journal>
<pages>6--13231344</pages>
<contexts>
<context position="8569" citStr="Rousu and Shawe-Taylor, 2005" startWordPosition="1367" endWordPosition="1370">similarity using TF-IDF weighted phone n-gram counts, and (b) string kernels. We compare their performance to that of the Fisher kernel as a purely acoustic similarity measure. TF-EDF weighted cosine similarity The cosine similarity between phone sequences si and sj is computed as simij = qP qP w∈si tf2 w,si idf2 w∈sj tf2 w,sj idf2 w w (4) where tfw,si is the count of n-gram w in si and idfw is the inverse document count of w (each sentence is a “document”). We use n = 1, 2, 3. String kernel The particular string kernel we use is a gapped, weighted subsequence kernel of the type described in (Rousu and Shawe-Taylor, 2005). Formally, we define a sentence s as a concatenation of symbols from a finite alphabet Σ (here the inventory of phones) and an embedding function from strings to feature vectors, φ : Σ∗ - 4H. The string kernel function K(s, t) computes the distance between the resulting vectors for two sentences si and sj. The embedding function is defined as Xφku(s) := λ|i |u E Σk (5) i:u=s(i) where k is the maximum length of subsequences, |i |is the length of i, and λ is a penalty parameter for each gap encountered in the subsequence. K is defined as XK(si, sj) = (φu(si), φu(sj))wu (6) u where w is a weight</context>
</contexts>
<marker>Rousu, Shawe-Taylor, 2005</marker>
<rawString>J. Rousu and J. Shawe-Taylor. 2005. Efficien computation of of gapped substring kernels for large alphabets. Journal of Machine Leaning Research, 6:13231344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wu</author>
<author>R Zhang</author>
<author>A Rudnicky</author>
</authors>
<title>Data selection for speech recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of ASRU.</booktitle>
<contexts>
<context position="4299" citStr="Wu et al., 2007" startWordPosition="651" endWordPosition="654">, we would like to reduce available training data ahead of time with a low-resource approach. In (Chen et al., 2009) individual samples are selected for the purpose of discriminative training by considering phone accuracy and the frame-level entropy of the Gaussian posteriors. (Itoh et al., 2012) use a utility function consisting of the entropy of word hypothesis N-best lists and the representativeness of the sample using a phone-based TF-IDF measure. The latter is comparable to methods used in this paper, though the first term in their objective function still requires a word recognizer. In (Wu et al., 2007) acoustic training data associated with transcriptions is subselected to maximize the entropy of the distribution over linguistic units (phones or words). Most importantly, all these methods select samples in a greedy fashion without optimality guarantees. As we will explain in the next section, greedy selection is near-optimal only when applied to monotone submodular functions. 3 Submodular Functions Submodular functions (Edmonds, 1970) have been widely studied in mathematics, economics, and operations research and have recently attracted interest in machine learning (Krause and Guestrin, 201</context>
<context position="11793" citStr="Wu et al., 2007" startWordPosition="1927" endWordPosition="1930">in a 3-state HMM monophone recognizer for all 48 TIMIT phone classes on the resulting sets and evaluate the performance on the core test set of 192 utterances, collapsing the 48 classes into 39 in line with standard practice (Lee and Hon, 1989). The HMM state output distributions are modeled by diagonal-covariance Gaussian mixtures with the number of Gaussians ranging between 4 and 64, depending on the data size. As a baseline we perform 100 random draws of the specified subset sizes and average the results. �` 2 /�wEsi tfw,si X tfw,s� X idfw 723 The second baseline consists of the method in (Wu et al., 2007), where utterances are selected to maximize the entropy of the distribution over phones in the selected subset. 5 Experiments We tested the three different similarity measures described above in combination with the submodular functions in Equations 2 and 3. The parameters of the gapped string kernel (i.e. the kernel order (k), the gap penalty (A), and the contiguous substring length l) were optimized on the development set. The best values were A = 0.1, k = 4,l = 3. We found that facility location was superior to saturated cover function across the board. Comparison of different data subset s</context>
<context position="13332" citStr="Wu et al., 2007" startWordPosition="2171" endWordPosition="2174">Accuracy (%) Figure 1: Phone accuracy for different subset sizes; each block of bars lists, from bottom to top: random baseline, entropy baseline, Fisher kernel, TF-IDF (unigram), TFIDF (bigram), TF-IDF (trigram), string kernel. Figure 1 shows the performance of the random and entropy-based baselines as well as the performance of the facility location function with different similarity measures. The entropy-based baseline beats the random baseline for most percentage cases but is otherwise the lowest-performing method overall. Note that this baseline uses the true transcriptions in line with (Wu et al., 2007) rather than the hypothesized phone labels output by our recognizer. The low performance and the fact that it is even outperformed by the random baseline in the 2.5% and 70% cases may be because the selection method encourages highly diverse but not very representative subsets. Furthermore, the entropy-based baseline utilizes a non-submodular objective function with a heuristic greedy search method. No theoretical guarantee of optimality can be made for the subset found by this method. Among the different similarity measures the Fisher kernel outperforms the baseline methods but has lower perf</context>
</contexts>
<marker>Wu, Zhang, Rudnicky, 2007</marker>
<rawString>Y. Wu, R. Zhang, and A. Rudnicky. 2007. Data selection for speech recognition. In Proceedings of ASRU.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>