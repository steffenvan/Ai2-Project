<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.761832">
Meaningful Clustering of Senses
Helps Boost Word Sense Disambiguation Performance
</title>
<author confidence="0.767848">
Roberto Navigli
</author>
<affiliation confidence="0.467401">
Dipartimento di Informatica
Universit`a di Roma “La Sapienza”
Roma, Italy
</affiliation>
<email confidence="0.9864">
navigli@di.uniroma1.it
</email>
<sectionHeader confidence="0.994482" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999743615384616">
Fine-grained sense distinctions are one of
the major obstacles to successful Word
Sense Disambiguation. In this paper,
we present a method for reducing the
granularity of the WordNet sense inven-
tory based on the mapping to a manually
crafted dictionary encoding sense hierar-
chies, namely the Oxford Dictionary of
English. We assess the quality of the map-
ping and the induced clustering, and eval-
uate the performance of coarse WSD sys-
tems in the Senseval-3 English all-words
task.
</bodyText>
<sectionHeader confidence="0.99861" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998958964912281">
Word Sense Disambiguation (WSD) is undoubt-
edly one of the hardest tasks in the field of Nat-
ural Language Processing. Even though some re-
cent studies report benefits in the use of WSD in
specific applications (e.g. Vickrey et al. (2005)
and Stokoe (2005)), the present performance of
the best ranking WSD systems does not provide a
sufficient degree of accuracy to enable real-world,
language-aware applications.
Most of the disambiguation approaches adopt
the WordNet dictionary (Fellbaum, 1998) as a
sense inventory, thanks to its free availability, wide
coverage, and existence of a number of standard
test sets based on it. Unfortunately, WordNet is a
fine-grained resource, encoding sense distinctions
that are often difficult to recognize even for human
annotators (Edmonds and Kilgariff, 1998).
Recent estimations of the inter-annotator agree-
ment when using the WordNet inventory report
figures of 72.5% agreement in the preparation of
the English all-words test set at Senseval-3 (Sny-
der and Palmer, 2004) and 67.3% on the Open
Mind Word Expert annotation exercise (Chklovski
and Mihalcea, 2002). These numbers lead us to
believe that a credible upper bound for unrestricted
fine-grained WSD is around 70%, a figure that
state-of-the-art automatic systems find it difficult
to outperform. Furthermore, even if a system were
able to exceed such an upper bound, it would be
unclear how to interpret such a result.
It seems therefore that the major obstacle to ef-
fective WSD is the fine granularity of the Word-
Net sense inventory, rather than the performance
of the best disambiguation systems. Interestingly,
Ng et al. (1999) show that, when a coarse-grained
sense inventory is adopted, the increase in inter-
annotator agreement is much higher than the re-
duction of the polysemy degree.
Following these observations, the main ques-
tion that we tackle in this paper is: can we pro-
duce and evaluate coarse-grained sense distinc-
tions and show that they help boost disambigua-
tion on standard test sets? We believe that this is
a crucial research topic in the field of WSD, that
could potentially benefit several application areas.
The contribution of this paper is two-fold. First,
we provide a wide-coverage method for clustering
WordNet senses via a mapping to a coarse-grained
sense inventory, namely the Oxford Dictionary of
English (Soanes and Stevenson, 2003) (Section 2).
We show that this method is well-founded and ac-
curate with respect to manually-made clusterings
(Section 3). Second, we evaluate the performance
of WSD systems when using coarse-grained sense
inventories (Section 4). We conclude the paper
with an account of related work (Section 5), and
some final remarks (Section 6).
</bodyText>
<page confidence="0.988789">
105
</page>
<note confidence="0.866803">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 105–112,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.713847" genericHeader="method">
2 Producing a Coarse-Grained Sense
Inventory
</sectionHeader>
<bodyText confidence="0.9999244">
In this section, we present an approach to the au-
tomatic construction of a coarse-grained sense in-
ventory based on the mapping of WordNet senses
to coarse senses in the Oxford Dictionary of Eng-
lish. In section 2.1, we introduce the two dictio-
naries, in Section 2.2 we illustrate the creation of
sense descriptions from both resources, while in
Section 2.3 we describe a lexical and a semantic
method for mapping sense descriptions of Word-
Net senses to ODE coarse entries.
</bodyText>
<subsectionHeader confidence="0.961336">
2.1 The Dictionaries
</subsectionHeader>
<bodyText confidence="0.998882225806451">
WordNet (Fellbaum, 1998) is a computational lex-
icon of English which encodes concepts as syn-
onym sets (synsets), according to psycholinguistic
principles. For each word sense, WordNet pro-
vides a gloss (i.e. a textual definition) and a set
of relations such as hypernymy (e.g. apple kind-of
edible fruit), meronymy (e.g. computer has-part
CPU), etc.
The Oxford Dictionary of English (ODE)
(Soanes and Stevenson, 2003)1 provides a hierar-
chical structure of senses, distinguishing between
homonymy (i.e. completely distinct senses, like
race as a competition and race as a taxonomic
group) and polysemy (e.g. race as a channel and
as a current). Each polysemous sense is further di-
vided into a core sense and a set of subsenses. For
each sense (both core and subsenses), the ODE
provides a textual definition, and possibly hyper-
nyms and domain labels. Excluding monosemous
senses, the ODE has an average number of 2.56
senses per word compared to the average poly-
semy of 3.21 in WordNet on the same words (with
peaks for verbs of 2.73 and 3.75 senses, respec-
tively).
In Table 1 we show an excerpt of the sense in-
ventories of the noun race as provided by both
dictionaries2. The ODE identifies 3 homonyms
and 3 polysemous senses for the first homonym,
while WordNet encodes a flat list of 6 senses,
some of which strongly related (e.g. race#1 and
race#3). Also, the ODE provides a sense (ginger
</bodyText>
<footnote confidence="0.998811714285714">
1The ODE was kindly made available by Ken Litkowski
(CL Research) in the context of a license agreement.
2In the following, we denote a WordNet sense with the
convention w#p#i where w is a word, p a part of speech and i
is a sense number; analogously, we denote an ODE sense with
the convention w#p#h.k where h is the homonym number
and k is the k-th polysemous entry under homonym h.
</footnote>
<bodyText confidence="0.990621642857143">
root) which is not taken into account in WordNet.
The structure of the ODE senses is clearly hier-
archical: if we were able to map with a high accu-
racy WordNet senses to ODE entries, then a sense
clustering could be trivially induced from the map-
ping. As a result, the granularity of the WordNet
inventory would be drastically reduced. Further-
more, disregarding errors, the clustering would be
well-founded, as the ODE sense groupings were
manually crafted by expert lexicographers. In the
next section we illustrate a general way of con-
structing sense descriptions that we use for deter-
mining a complete, automatic mapping between
the two dictionaries.
</bodyText>
<subsectionHeader confidence="0.999192">
2.2 Constructing Sense Descriptions
</subsectionHeader>
<bodyText confidence="0.998168666666667">
For each word w, and for each sense S of w in a
given dictionary D E {WORDNET, ODE}, we con-
struct a sense description dD(S) as a bag of words:
</bodyText>
<listItem confidence="0.895800153846154">
dD(S) = def D(S) U hyperD(S) U domainsD(S)
where:
• def D(S) is the set of words in the tex-
tual definition of S (excluding usage ex-
amples), automatically lemmatized and part-
of-speech tagged with the RASP statistical
parser (Briscoe and Carroll, 2002);
• hyperD(S) is the set of direct hypernyms of
S in the taxonomy hierarchy of D (0 if hy-
pernymy is not available);
• domainsD(S) includes the set of domain la-
bels possibly assigned to sense S (0 when no
domain is assigned).
</listItem>
<bodyText confidence="0.9996453">
Specifically, in the case of WordNet, we
generate def WN(S) from the gloss of S,
hyperWN(S) from the noun and verb taxonomy,
and domainsWN(S) from the subject field codes,
i.e. domain labels produced semi-automatically
by Magnini and Cavagli`a (2000) for each Word-
Net synset (we exclude the general-purpose label,
called FACTOTUM).
For example, for the first WordNet sense of
race#n we obtain the following description:
</bodyText>
<equation confidence="0.984711">
dWN(race#n#1) = {competition#n} U
{contest#n} U {POLITICS#N, SPORT#N}
</equation>
<bodyText confidence="0.9988584">
In the case of the ODE, def ODE(S) is gener-
ated from the definitions of the core sense and
the subsenses of the entry S. Hypernymy (for
nouns only) and domain labels, when available,
are included in the respective sets hyperODE(S)
</bodyText>
<page confidence="0.999522">
106
</page>
<tableCaption confidence="0.9972685">
Table 1: The sense inventory of race#n in WordNet and ODE (definitions are abridged, bullets (•)
indicate a subsense in the ODE, arrows (→) indicate hypernymy, DOMAIN LABELS are in small caps).
</tableCaption>
<table confidence="0.924755409090909">
race#n (WordNet)
#1 Any competition (→ contest).
#2 People who are believed to be-
long to the same genetic stock
(→ group).
#3 A contest of speed (→ contest).
#4 The flow of air that is driven
backwards by an aircraft pro-
peller (→ flow).
#5 A taxonomic group that is a
division of a species; usually
arises as a consequence of ge-
ographical isolation within a
species (→ taxonomic group).
#6 A canal for a current of water
(→ canal).
race#n (ODE)
#1.1 Core: SPORT A competition between runners, horses, vehicles, etc.
• RACING A series of such competitions for horses or dogs • A sit-
uation in which individuals or groups compete (→ contest) • AS-
TRONOMY The course of the sun or moon through the heavens (→
trajectory).
</table>
<listItem confidence="0.9683858">
#1.2 Core: NAUTICAL A strong or rapid current (→ flow).
#1.3 Core: A groove, channel, or passage.
• MECHANICS A water channel • Smooth groove or guide for balls (→
indentation, conduit) • FARMING Fenced passageway in a stockyard
(→ route) • TEXTILES The channel along which the shuttle moves.
#2.1 Core: ANTHROPOLOGY Division of humankind (→ ethnic group).
• The condition of belonging to a racial division or group • A group
of people sharing the same culture, history, language • BIOLOGY A
group of people descended from a common ancestor.
#3.1 Core: BOTANY, FOOD A ginger root (→ plant part).
</listItem>
<bodyText confidence="0.899495">
and domainsODE(S). For example, the first ODE
sense of race#n is described as follows:
</bodyText>
<equation confidence="0.82428875">
dODE(race#n#1.1) = {competition#n,
runner#n, horse#n, vehicle#n, ... ,
heavens#n} U {contest#n, trajectory#n} U
{SPORT#N, RACING#N, ASTRONOMY#N}
</equation>
<bodyText confidence="0.999499714285714">
Notice that, for every S, dD(S) is non-empty as
a definition is always provided by both dictionar-
ies. This approach to sense descriptions is gen-
eral enough to be applicable to any other dictio-
nary with similar characteristics (e.g. the Long-
man Dictionary of Contemporary English in place
of ODE).
</bodyText>
<subsectionHeader confidence="0.999892">
2.3 Mapping Word Senses
</subsectionHeader>
<bodyText confidence="0.997983833333333">
In order to produce a coarse-grained version of the
WordNet inventory, we aim at defining an auto-
matic mapping between WordNet and ODE, i.e.
a function µ : SensesWN → SensesODE U {E},
where SensesD is the set of senses in the dictio-
nary D and E is a special element assigned when
no plausible option is available for mapping (e.g.
when the ODE encodes no entry corresponding to
a WordNet sense).
Given a WordNet sense S E SensesWN(w) we
define ˆm(S), the best matching sense in the ODE,
as:
</bodyText>
<equation confidence="0.992539857142857">
c(w)
:
E
={µ−1(S&apos;)
S&apos;
SensesODE(w),µ−1(S&apos;)=�∅}
U {{S} : S E SensesWN(w), µ(S) = E}
</equation>
<bodyText confidence="0.927420166666667">
where µ−1(S&apos;) is the group of WordNet senses
mapped to the same sense S&apos; of the ODE, while
the second set includes singletons of WordNet
107 where B is a threshold below which a matching
between sense descriptions is considered unreli-
able. Finally, we define the clusteri
</bodyText>
<equation confidence="0.86032875">
c(race#n)
{{race#n#1,
{race#n#2,
{race#n#4},
</equation>
<bodyText confidence="0.999239">
In Sections 2.3.1 and 2.3.2 we describe two
different choices for the match function, respec-
tively based on the use of lexical and seman
</bodyText>
<equation confidence="0.937807375">
=
race#n#3},
race#n#5},
{race#n#6}}
tic in-
formation.
ˆm(S) = arg max match(S, S&apos;)
S&apos;∈SensesODE(w)
</equation>
<bodyText confidence="0.99837625">
where match : SensesWNxSensesODE → [0, 1]
is a function that measures the degree of matching
between the sense descriptions of S and S&apos;. We
define the mapping µ as:
</bodyText>
<equation confidence="0.87694075">
�
ˆm(S) if match(S, ˆm(S)) ≥ B
µ(S) =
ption lengths:
</equation>
<bodyText confidence="0.8702705">
ng of senses
c(w) of a word w as:
senses for which no mapping can be provided ac-
cording to the definition of µ.
For example, an ideal mapping between entries
in Table 1 would be as follows:
</bodyText>
<equation confidence="0.999992">
µ(race#n#1) = race#n#1.1, µ(race#n#2) = race#n#2.1,
µ(race#n#3) = race#n#1.1, µ(race#n#5) = race#n#2.1,
µ(race#n#4) = race#n#1.2, µ(race#n#6) = race#n#1.3,
</equation>
<bodyText confidence="0.998438">
resulting in the following clustering:
</bodyText>
<subsectionHeader confidence="0.843153">
2.3.1 Lexical matching
</subsectionHeader>
<bodyText confidence="0.999983285714286">
As a first approach, we adopted a purely lexi-
cal matching function based on the notion of lex-
ical overlap (Lesk, 1986). The function counts
the number of lemmas that two sense descriptions
of a word have in common (we neglect parts of
speech), and is normalized by the minimum of the
two descri
</bodyText>
<equation confidence="0.954770666666667">
matchLESK(S, S&apos;) = |dWN(S)∩dODE(S&apos;)|
min{|dWN(S)|,|dODE(S&apos;)|}
E otherwise
where S E SensesWN(w) and S&apos; E
SensesODE(w). For instance:
matchLESK(race#n#1, race#n#1.1) =
min{4,201
3 4 = 0.75
3
matchLESK(race#n#2, race#n#1.1) =
8 = 0.125
1
</equation>
<bodyText confidence="0.9971574">
Notice that unrelated senses can get a positive
score because of an overlap of the sense descrip-
tions. In the example, group#n, the hypernym
of race#n#2, is also present in the definition of
race#n#1.1.
</bodyText>
<subsectionHeader confidence="0.76009">
2.3.2 Semantic matching
</subsectionHeader>
<bodyText confidence="0.994411421052632">
Unfortunately, the very same concept can be
defined with entirely different words. To match
definitions in a semantic manner we adopted
a knowledge-based Word Sense Disambiguation
algorithm, Structural Semantic Interconnections
(SSI, Navigli and Velardi (2004)).
SSI3 exploits an extensive lexical knowledge
base, built upon the WordNet lexicon and enriched
with collocation information representing seman-
tic relatedness between sense pairs. Collocations
are acquired from existing resources (like the Ox-
ford Collocations, the Longman Language Acti-
vator, collocation web sites, etc.). Each colloca-
tion is mapped to the WordNet sense inventory in
a semi-automatic manner and transformed into a
relatedness edge (Navigli and Velardi, 2005).
Given a word context C = {w1, ..., wn}, SSI
builds a graph G = (V, E) such that V =
n
</bodyText>
<equation confidence="0.809014">
U SensesWN(wi) and (S, S&apos;) E E if there is
i=1
</equation>
<bodyText confidence="0.9972454375">
at least one semantic interconnection between S
and S&apos; in the lexical knowledge base. A seman-
tic interconnection pattern is a relevant sequence
of edges selected according to a manually-created
context-free grammar, i.e. a path connecting a pair
of word senses, possibly including a number of in-
termediate concepts. The grammar consists of a
small number of rules, inspired by the notion of
lexical chains (Morris and Hirst, 1991).
SSI performs disambiguation in an iterative
fashion, by maintaining a set C of senses as a se-
mantic context. Initially, C = V (the entire set
of senses of words in C). At each step, for each
sense S in C, the algorithm calculates a score of
the degree of connectivity between S and the other
senses in C:
</bodyText>
<footnote confidence="0.643756">
3Available online from: http://lcl.di.uniroma1.it/ssi
</footnote>
<equation confidence="0.8267605">
E |IC(S,S&apos;)|
S&apos;EC\{S}
</equation>
<bodyText confidence="0.9697465">
where IC(S, S&apos;) is the set of interconnections be-
tween senses S and S&apos;. The contribution of a sin-
gle interconnection is given by the reciprocal of its
length, calculated as the number of edges connect-
ing its ends. The overall degree of connectivity is
then normalized by the number of contributing in-
terconnections. The highest ranking sense S of
word w is chosen and the senses of w are removed
from the semantic context C. The algorithm termi-
nates when either C = ∅ or there is no sense such
that its score exceeds a fixed threshold.
Given a word w, semantic matching is per-
formed in two steps. First, for each dictionary
D E {WORDNET, ODE}, and for each sense S E
SensesD(w), the sense description of S is dis-
ambiguated by applying SSI to dD(S). As a re-
sult, we obtain a semantic description as a bag of
concepts dsem
D (S). Notice that sense descriptions
from both dictionaries are disambiguated with re-
spect to the WordNet sense inventory.
Second, given a WordNet sense
S E SensesWN(w) and an ODE sense
S&apos; E SensesODE(w), we define matchSSI(S, S&apos;)
as a function of the direct relations connecting
senses in dsem WN(S) and dsem
</bodyText>
<equation confidence="0.9897344">
ODE (S&apos;):
matchSSI 5, 5&apos; = |c-+c&apos;:cEdsemWN(S),c&apos;EdoDE (S&apos;)|
( ) |dsem
WN (S)|·|dsem
ODE (S&apos;)|
</equation>
<bodyText confidence="0.9964035">
where c → c&apos; denotes the existence of a relation
edge in the lexical knowledge base between a con-
cept c in the description of S and a concept c&apos; in
the description of S&apos;. Edges include the WordNet
relation set (synonymy, hypernymy, meronymy,
antonymy, similarity, nominalization, etc.) and the
relatedness edge mentioned above (we adopt only
direct relations to maintain a high precision).
For example, some of the relations found
between concepts in dsem
</bodyText>
<equation confidence="0.997249545454546">
WN (race#n#3) and
dsem
ODE (race#n#1.1) are:
race#n#3 relation race#n#1.1
speed#n#1 related−to −→vehicle#n#1
related−to
race#n#3 −→ compete#v#1
kind−of
racing#n#1 −→ sport#n#1
kind−of
race#n#3 −→ contest#n#1
</equation>
<bodyText confidence="0.9985525">
contributing to the final value of the function on
the two senses:
</bodyText>
<equation confidence="0.679474">
matchSSI(race#n#3, race#n#1.1) = 0.41
</equation>
<bodyText confidence="0.9973745">
Due to the normalization factor in the denomi-
nator, these values are generally low, but unrelated
</bodyText>
<equation confidence="0.65003675">
E E 1
S&apos;EC\{S} iEIC(S,S&apos;)
length(i)
ScoreSSI(S,C) =
</equation>
<page confidence="0.987336">
108
</page>
<tableCaption confidence="0.9849095">
Table 2: Performance of the lexical and semantic
mapping functions.
</tableCaption>
<table confidence="0.996109333333333">
Func. Prec. Recall F1 Acc.
Lesk 84.74% 65.43% 73.84% 66.08%
SSI 86.87% 79.67% 83.11% 77.94%
</table>
<bodyText confidence="0.96432475">
senses have values much closer to 0. We chose
SSI for the semantic matching function as it has
the best performance among untrained systems on
unconstrained WSD (cf. Section 4.1).
</bodyText>
<sectionHeader confidence="0.983136" genericHeader="method">
3 Evaluating the Clustering
</sectionHeader>
<bodyText confidence="0.99999296">
We evaluated the accuracy of the mapping pro-
duced with the lexical and semantic methods de-
scribed in Sections 2.3.1 and 2.3.2, respectively.
We produced a gold-standard data set by manually
mapping 5,077 WordNet senses of 763 randomly-
selected words to the respective ODE entries (dis-
tributed as follows: 466 nouns, 231 verbs, 50 ad-
jectives, 16 adverbs). The data set was created
by two annotators and included only polysemous
words. These words had 2,600 senses in the ODE.
Overall, 4,599 out of the 5,077 WordNet senses
had a corresponding sense in ODE (i.e. the ODE
covered 90.58% of the WordNet senses in the data
set), while 2,053 out of the 2,600 ODE senses had
an analogous entry in WordNet (i.e. WordNet cov-
ered 78.69% of the ODE senses). The WordNet
clustering induced by the manual mapping was
49.85% of the original size and the average degree
of polysemy decreased from 6.65 to 3.32.
The reliability of our data set is substantiated by
a quantitative assessment: 548 WordNet senses of
60 words were mapped to ODE entries by both
annotators, with a pairwise mapping agreement
of 92.7%. The average Cohen’s κ agreement be-
tween the two annotators was 0.874.
In Table 2 we report the precision and recall of
the lexical and semantic functions in providing the
appropriate association for the set of senses having
a corresponding entry in ODE (i.e. excluding the
cases where a sense 2 was assigned by the manual
annotators, cf. Section 2.3). We also report in the
Table the accuracy of the two functions when we
view the problem as a classification task: an auto-
matic association is correct if it corresponds to the
manual association provided by the annotators or
if both assign no answer (equivalently, if both pro-
vide an 2 label). All the differences between Lesk
and SSI are statistically significant (p &lt; 0.01).
As a second experiment, we used two
information-theoretic measures, namely entropy
and purity (Zhao and Karypis, 2004), to compare
an automatic clustering c(w) (i.e. the sense groups
acquired for word w) with a manual clustering
ˆc(w). The entropy quantifies the distribution of the
senses of a group over manually-defined groups,
while the purity measures the extent to which a
group contains senses primarily from one manual
group.
Given a word w, and a sense group G E c(w),
the entropy of G is defined as:
</bodyText>
<equation confidence="0.99400975">
�
H(G) = −1
log |ˆc(w)|
ˆGEˆc(w)
</equation>
<bodyText confidence="0.9971265">
i.e., the entropy4 of the distribution of senses of
group G over the groups of the manual clustering
ˆc(w). The entropy of an entire clustering c(w) is
defined as:
</bodyText>
<equation confidence="0.9977">
Entropy(c(w)) = E
GEc(w)
</equation>
<bodyText confidence="0.961809666666667">
that is, the entropy of each group weighted by
its size. The purity of a sense group G E c(w) is
defined as:
</bodyText>
<equation confidence="0.998480666666667">
Pu(G) =1
|G |max  |Gˆ n G|
ˆGEˆc(w)
</equation>
<bodyText confidence="0.9930592">
i.e., the normalized size of the largest subset of
G contained in a single group Gˆ of the manual
clustering. The overall purity of a clustering is ob-
tained as a weighted sum of the individual cluster
purities:
</bodyText>
<equation confidence="0.9741865">
|G|
|SensesWN(w)|Pu(G)
</equation>
<bodyText confidence="0.999971866666667">
We calculated the entropy and purity of the
clustering produced automatically with the lexical
and the semantic method, when compared to the
grouping induced by our manual mapping (ODE),
and to the grouping manually produced for the
English all-words task at Senseval-2 (3,499 senses
of 403 nouns). We excluded from both gold stan-
dards words having a single cluster. The figures
are shown in Table 3 (good entropy and purity val-
ues should be close to 0 and 1 respectively).
Table 3 shows that the quality of the cluster-
ing induced with a semantic function outperforms
both lexical overlap and a random baseline. The
baseline was computed averaging among 200 ran-
dom clustering solutions for each word. Random
</bodyText>
<footnote confidence="0.83981225">
4Notice that we are comparing clusterings against the
manual clustering (rather than viceversa), as otherwise a
completely unclustered solution would result in 1.0 entropy
and 0.0 purity.
</footnote>
<equation confidence="0.987216166666666">
 |ˆGnG ||ˆGnG|
 |ˆG |log( |ˆG |)
|G|H(G)
|SensesWN(w)|
Purity(c(w)) = E
GEc(w)
</equation>
<page confidence="0.999024">
109
</page>
<tableCaption confidence="0.999985">
Table 3: Comparison with gold standards.
</tableCaption>
<table confidence="0.999263714285714">
Gold standard Method Entropy Purity
ODE Lesk 0.15 0.87
SSI 0.11 0.87
Baseline 0.28 0.67
Senseval Lesk 0.17 0.71
SSI 0.16 0.69
Baseline 0.27 0.57
</table>
<bodyText confidence="0.9998881">
clusterings were the result of a random mapping
function between WordNet and ODE senses. As
expected, the automatic clusterings have a lower
purity when compared to the Senseval-2 noun
grouping as the granularity of the latter is much
finer than ODE (entropy is only partially affected
by this difference, indicating that we are producing
larger groups). Indeed, our gold standard (ODE),
when compared to the Senseval groupings, obtains
a low purity as well (0.75) and an entropy of 0.13.
</bodyText>
<sectionHeader confidence="0.905102" genericHeader="method">
4 Evaluating Coarse-Grained WSD
</sectionHeader>
<bodyText confidence="0.999982176470588">
The main reason for building a clustering of Word-
Net senses is to make Word Sense Disambigua-
tion a feasible task, thus overcoming the obstacles
that even humans encounter when annotating sen-
tences with excessively fine-grained word senses.
As the semantic method outperformed the lex-
ical overlap in the evaluations of previous Sec-
tion, we decided to acquire a clustering on the
entire WordNet sense inventory using this ap-
proach. As a result, we obtained a reduction of
33.54% in the number of entries (from 60,302 to
40,079 senses) and a decrease of the polysemy
degree from 3.14 to 2.09. These figures exclude
monosemous senses and derivatives in WordNet.
As we are experimenting on an automatically-
acquired clustering, all the figures are affected by
the 22.06% error rate resulting from Table 2.
</bodyText>
<subsectionHeader confidence="0.998023">
4.1 Experiments on Senseval-3
</subsectionHeader>
<bodyText confidence="0.9986379">
As a first experiment, we assessed the effect of
the automatic sense clustering on the English all-
words task at Senseval-3 (Snyder and Palmer,
2004). This task required WSD systems to pro-
vide a sense choice for 2,081 content words in a
set of 301 sentences from the fiction, news story,
and editorial domains.
We considered the three best-ranking WSD sys-
tems – GAMBL (Decadt et al., 2004), Sense-
Learner (Mihalcea and Faruque, 2004), and Koc
</bodyText>
<tableCaption confidence="0.8729765">
Table 4: Performance of WSD systems at
Senseval-3 on coarse-grained sense inventories.
</tableCaption>
<table confidence="0.999875">
System Prec. Rec. F1 F1fine
Gambl 0.779 0.779 0.779 0.652
SenseLearner 0.769 0.769 0.769 0.646
KOC Univ. 0.768 0.768 0.768 0.641
SSI 0.758 0.758 0.758 0.612
IRST-DDD 0.721 0.719 0.720 0.583
FS baseline 0.769 0.769 0.769 0.624
Random BL 0.497 0.497 0.497 0.340
</table>
<bodyText confidence="0.995508230769231">
University (Yuret, 2004) – and the best unsuper-
vised system, namely IRST-DDD (Strapparava et
al., 2004). We also included SSI as it outper-
forms all the untrained systems (Navigli and Ve-
lardi, 2005). To evaluate the performance of the
five systems on our coarse clustering, we consid-
ered a fine-grained answer to be correct if it be-
longs to the same cluster as that of the correct an-
swer. Table 4 reports the performance of the sys-
tems, together with the first sense and the random
baseline (in the last column we report the perfor-
mance on the original fine-grained test set).
The best system, Gambl, obtains almost 78%
precision and recall, an interesting figure com-
pared to 65% performance in the fine-grained
WSD task. An interesting aspect is that the rank-
ing across systems was maintained when mov-
ing from a fine-grained to a coarse-grained sense
inventory, although two systems (SSI and IRST-
DDD) show the best improvement.
In order to show that the general improvement
is the result of an appropriate clustering, we as-
sessed the performance of Gambl by averaging its
results when using 100 randomly-generated differ-
ent clusterings. We excluded monosemous clus-
ters from the test set (i.e. words with all the senses
mapped to the same ODE entry), so as to clar-
ify the real impact of properly grouped clusters.
As a result, the random setting obtained 64.56%
average accuracy, while the performance when
adopting our automatic clustering was 70.84%
(1,025/1,447 items).
To make it clear that the performance improve-
ment is not only due to polysemy reduction, we
considered a subset of the Senseval-3 test set in-
cluding only the incorrect answers given by the
fine-grained version of Gambl (623 items). In
other words, on this data set Gambl performs with
0% accuracy. We compared the performance of
</bodyText>
<page confidence="0.999211">
110
</page>
<tableCaption confidence="0.890158">
Table 5: Performance of SSI on coarse inventories
(SSI* uses a coarse-grained knowledge base).
</tableCaption>
<table confidence="0.9921985">
System Prec. Recall F1
SSI + baseline 0.758 0.758 0.758
SSI 0.717 0.576 0.639
SSI* 0.748 0.674 0.709
</table>
<bodyText confidence="0.999089925925926">
Gambl when adopting our automatic clustering
with the accuracy of the random baseline. The re-
sults were respectively 34% and 15.32% accuracy.
These experiments prove that the performance
in Table 4 is not due to chance, but to an effec-
tive way of clustering word senses. Furthermore,
the systems in the Table are not taking advantage
of the information given by the clustering (trained
systems could be retrained on the coarse cluster-
ing). To assess this aspect, we performed a fur-
ther experiment. We modified the sense inventory
of the SSI lexical knowledge base by adopting the
coarse inventory acquired automatically. To this
end, we merged the semantic interconnections be-
longing to the same cluster. We also disabled the
first sense baseline heuristic, that most of the sys-
tems use as a back-off when they have no infor-
mation about the word at hand. We call this new
setting SSI* (as opposed to SSI used in Table 4).
In Table 5 we report the results. The algorithm
obtains an improvement of 9.8% recall and 3.1%
precision (both statistically significant, p &lt; 0.05).
The increase in recall is mostly due to the fact
that different senses belonging to the same clus-
ter now contribute together to the choice of that
cluster (rather than individually to the choice of a
fine-grained sense).
</bodyText>
<sectionHeader confidence="0.999545" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999668">
WordNet for the identification of sense regular-
ities: to this end, they provide a set of seman-
tic and probabilistic rules. An evaluation of the
heuristics provided leads to a polysemy reduc-
tion of 39% and an error rate of 5.6%. A differ-
ent principle for clustering WordNet senses, based
on the Minimum Description Length, is described
by Tomuro (2001). The clustering is evaluated
against WordNet cousins and used for the study of
inter-annotator disagreement. Another approach
exploits the (dis)agreements of human annotators
to derive coarse-grained sense clusters (Chklovski
and Mihalcea, 2003), where sense similarity is
computed from confusion matrices.
Agirre and Lopez (2003) analyze a set of meth-
ods to cluster WordNet senses based on the use
of confusion matrices from the results of WSD
systems, translation equivalences, and topic sig-
natures (word co-occurrences extracted from the
web). They assess the acquired clusterings against
20 words from the Senseval-2 sense groupings.
Finally, McCarthy (2006) proposes the use
of ranked lists, based on distributionally nearest
neighbours, to relate word senses. This softer no-
tion of sense relatedness allows to adopt the most
appropriate granularity for a specific application.
Compared to our approach, most of these meth-
ods do not evaluate the clustering produced with
respect to a gold-standard clustering. Indeed,
such an evaluation would be difficult and time-
consuming without a coarse sense inventory like
that of ODE. A limited assessment of coarse WSD
is performed by Fellbaum et al. (2001), who ob-
tain a large improvement in the accuracy of a
maximum-entropy system on clustered verbs.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999887896551724">
Dolan (1994) describes a method for clustering
word senses with the use of information provided
in the electronic version of LDOCE (textual de-
finitions, semantic relations, domain labels, etc.).
Unfortunately, the approach is not described in de-
tail and no evaluation is provided.
Most of the approaches in the literature make
use of the WordNet structure to cluster its senses.
Peters et al. (1998) exploit specific patterns in the
WordNet hierarchy (e.g. sisters, autohyponymy,
twins, etc.) to group word senses. They study
semantic regularities or generalizations obtained
and analyze the effect of clustering on the com-
patibility of language-specific wordnets. Mihal-
cea and Moldovan (2001) study the structure of
In this paper, we presented a study on the construc-
tion of a coarse sense inventory for the WordNet
lexicon and its effects on unrestricted WSD.
A key feature in our approach is the use of a
well-established dictionary encoding sense hierar-
chies. As remarked in Section 2.2, the method can
employ any dictionary with a sufficiently struc-
tured inventory of senses, and can thus be applied
to reduce the granularity of, e.g., wordnets of other
languages. One could argue that the adoption of
the ODE as a sense inventory for WSD would be a
better solution. While we are not against this pos-
sibility, there are problems that cannot be solved
at present: the ODE does not encode semantic re-
</bodyText>
<page confidence="0.996207">
111
</page>
<bodyText confidence="0.999910809523809">
lations and is not freely available. Also, most of
the present research and standard data sets focus
on WordNet.
The fine granularity of the WordNet sense in-
ventory is unsuitable for most applications, thus
constituting an obstacle that must be overcome.
We believe that the research topic analyzed in this
paper is a first step towards making WSD a fea-
sible task and enabling language-aware applica-
tions, like information retrieval, question answer-
ing, machine translation, etc. In a future work, we
plan to investigate the contribution of coarse dis-
ambiguation to such real-world applications. To
this end, we aim to set up an Open Mind-like ex-
periment for the validation of the entire mapping
from WordNet to ODE, so that only a minimal er-
ror rate would affect the experiments to come.
Finally, the method presented here could be use-
ful for lexicographers in the comparison of the
quality of dictionaries, and in the detection of
missing word senses.
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.494798">
This work is partially funded by the Interop NoE (508011),
6th European Union FP. We wish to thank Paola Velardi,
Mirella Lapata and Samuel Brody for their useful comments.
</reference>
<sectionHeader confidence="0.95629" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999788322916667">
Eneko Agirre and Oier Lopez. 2003. Clustering wordnet
word senses. In Proc. of Conf. on Recent Advances on
Natural Language (RANLP). Borovets, Bulgary.
Ted Briscoe and John Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proc. of 3Td Confer-
ence on Language Resources and Evaluation. Las Palmas,
Gran Canaria.
Tim Chklovski and Rada Mihalcea. 2002. Building a sense
tagged corpus with open mind word expert. In Proc. of
ACL 2002 Workshop on WSD: Recent Successes and Fu-
ture Directions. Philadelphia, PA.
Tim Chklovski and Rada Mihalcea. 2003. Exploiting agree-
ment and disagreement of human annotators for word
sense disambiguation. In Proc. of Recent Advances In
NLP (RANLP 2003). Borovetz, Bulgaria.
Bart Decadt, V´eronique Hoste, Walter Daelemans, and Antal
van den Bosch. 2004. Gambl, genetic algorithm opti-
mization of memory-based wsd. In Proc. ofACL/SIGLEX
Senseval-3. Barcelona, Spain.
William B. Dolan. 1994. Word sense ambiguation: Cluster-
ing related senses. In Proc. of 15th Conference on Com-
putational Linguistics (COLING). Morristown, N.J.
Philip Edmonds and Adam Kilgariff. 1998. Introduction to
the special issue on evaluating word sense disambiguation
systems. Journal ofNatural Language Engineering, 8(4).
Christiane Fellbaum, Martha Palmer, Hoa Trang Dang, Lau-
ren Delfs, and Susanne Wolf. 2001. Manual and au-
tomatic semantic annotation with wordnet. In Proc. of
NAACL Workshop on WordNet and Other Lexical Re-
sources. Pittsburgh, PA.
Christiane Fellbaum, editor. 1998. WordNet: an Electronic
Lexical Database. MIT Press.
Michael Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine code
from an ice cream cone. In Proc. of 5th Conf. on Systems
Documentation. ACM Press.
Bernardo Magnini and Gabriela Cavagli`a. 2000. Integrating
subject field codes into wordnet. In Proc. of the 2nd Con-
ference on Language Resources and Evaluation (LREC).
Athens, Greece.
Diana McCarthy. 2006. Relating wordnet senses for word
sense disambiguation. In Proc. ofACL Workshop on Mak-
ing Sense of Sense. Trento, Italy.
Rada Mihalcea and Ehsanul Faruque. 2004. Senselearner:
Minimally supervised word sense disambiguation for all
words in open text. In Proc. of ACL/SIGLEX Senseval-3.
Barcelona, Spain.
Rada Mihalcea and Dan Moldovan. 2001. Automatic
generation of a coarse grained wordnet. In Proc. of
NAACL Workshop on WordNet and Other Lexical Re-
sources. Pittsburgh, PA.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion com-
puted by thesaural relations as an indicator of the structure
of text. Computational Linguistics, 17(1).
Roberto Navigli and Paola Velardi. 2004. Learning domain
ontologies from document warehouses and dedicated web-
sites. Computational Linguistics, 30(2).
Roberto Navigli and Paola Velardi. 2005. Structural se-
mantic interconnections: a knowledge-based approach to
word sense disambiguation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence (PAMI), 27(7).
Hwee T. Ng, Chung Y. Lim, and Shou K. Foo. 1999. A case
study on the inter-annotator agreement for word sense dis-
ambiguation. In Proc. of ACL Workshop: Standardizing
Lexical Resources. College Park, Maryland.
Wim Peters, Ivonne Peters, and Piek Vossen. 1998. Au-
tomatic sense clustering in eurowordnet. In Proc. of the
1st Conference on Language Resources and Evaluation
(LREC). Granada, Spain.
Benjamin Snyder and Martha Palmer. 2004. The english
all-words task. In Proc. ofACL 2004 SENSEVAL-3 Work-
shop. Barcelona, Spain.
Catherine Soanes and Angus Stevenson, editors. 2003. Ox-
ford Dictionary of English. Oxford University Press.
Christopher Stokoe. 2005. Differentiating homonymy and
polysemy in information retrieval. In Proc. of the Confer-
ence on Empirical Methods in Natural Language Process-
ing. Vancouver, Canada.
Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano.
2004. Pattern abstraction and term similarity for word
sense disambiguation. In Proc. ofACL/SIGLEX Senseval-
3. Barcelona, Spain.
Noriko Tomuro. 2001. Tree-cut and a lexicon based on sys-
tematic polysemy. In Proc. of the Meeting of the NAACL.
Pittsburgh, USA.
David Vickrey, Luke Biewald, Marc Teyssier, and Daphne
Koller. 2005. Word sense disambiguation vs. statistical
machine translation. In Proc. of Conference on Empiri-
cal Methods in Natural Language Processing. Vancouver,
Canada.
Deniz Yuret. 2004. Some experiments with a naive
bayes wsd system. In Proc. of ACL/SIGLEX Senseval-3.
Barcelona, Spain.
Ying Zhao and George Karypis. 2004. Empirical and theo-
retical comparisons of selected criterion functions for doc-
ument clustering. Machine Learning, 55(3).
</reference>
<page confidence="0.998288">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.769235">
<title confidence="0.9415875">Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance</title>
<author confidence="0.999884">Roberto Navigli</author>
<affiliation confidence="0.992976">Dipartimento di Informatica Universit`a di Roma “La Sapienza”</affiliation>
<address confidence="0.989736">Roma, Italy</address>
<email confidence="0.990799">navigli@di.uniroma1.it</email>
<abstract confidence="0.991662785714286">Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work is partially funded by the Interop NoE (508011), 6th European Union FP. We wish to thank Paola Velardi, Mirella Lapata and Samuel Brody for their useful comments.</title>
<marker></marker>
<rawString>This work is partially funded by the Interop NoE (508011), 6th European Union FP. We wish to thank Paola Velardi, Mirella Lapata and Samuel Brody for their useful comments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez</author>
</authors>
<title>Clustering wordnet word senses.</title>
<date>2003</date>
<booktitle>In Proc. of Conf. on Recent Advances on Natural Language (RANLP). Borovets,</booktitle>
<location>Bulgary.</location>
<contexts>
<context position="26941" citStr="Agirre and Lopez (2003)" startWordPosition="4468" endWordPosition="4471">is end, they provide a set of semantic and probabilistic rules. An evaluation of the heuristics provided leads to a polysemy reduction of 39% and an error rate of 5.6%. A different principle for clustering WordNet senses, based on the Minimum Description Length, is described by Tomuro (2001). The clustering is evaluated against WordNet cousins and used for the study of inter-annotator disagreement. Another approach exploits the (dis)agreements of human annotators to derive coarse-grained sense clusters (Chklovski and Mihalcea, 2003), where sense similarity is computed from confusion matrices. Agirre and Lopez (2003) analyze a set of methods to cluster WordNet senses based on the use of confusion matrices from the results of WSD systems, translation equivalences, and topic signatures (word co-occurrences extracted from the web). They assess the acquired clusterings against 20 words from the Senseval-2 sense groupings. Finally, McCarthy (2006) proposes the use of ranked lists, based on distributionally nearest neighbours, to relate word senses. This softer notion of sense relatedness allows to adopt the most appropriate granularity for a specific application. Compared to our approach, most of these methods</context>
</contexts>
<marker>Agirre, Lopez, 2003</marker>
<rawString>Eneko Agirre and Oier Lopez. 2003. Clustering wordnet word senses. In Proc. of Conf. on Recent Advances on Natural Language (RANLP). Borovets, Bulgary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proc. of 3Td Conference on Language Resources and Evaluation. Las</booktitle>
<location>Palmas, Gran Canaria.</location>
<contexts>
<context position="6994" citStr="Briscoe and Carroll, 2002" startWordPosition="1136" endWordPosition="1139">by expert lexicographers. In the next section we illustrate a general way of constructing sense descriptions that we use for determining a complete, automatic mapping between the two dictionaries. 2.2 Constructing Sense Descriptions For each word w, and for each sense S of w in a given dictionary D E {WORDNET, ODE}, we construct a sense description dD(S) as a bag of words: dD(S) = def D(S) U hyperD(S) U domainsD(S) where: • def D(S) is the set of words in the textual definition of S (excluding usage examples), automatically lemmatized and partof-speech tagged with the RASP statistical parser (Briscoe and Carroll, 2002); • hyperD(S) is the set of direct hypernyms of S in the taxonomy hierarchy of D (0 if hypernymy is not available); • domainsD(S) includes the set of domain labels possibly assigned to sense S (0 when no domain is assigned). Specifically, in the case of WordNet, we generate def WN(S) from the gloss of S, hyperWN(S) from the noun and verb taxonomy, and domainsWN(S) from the subject field codes, i.e. domain labels produced semi-automatically by Magnini and Cavagli`a (2000) for each WordNet synset (we exclude the general-purpose label, called FACTOTUM). For example, for the first WordNet sense of</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proc. of 3Td Conference on Language Resources and Evaluation. Las Palmas, Gran Canaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with open mind word expert.</title>
<date>2002</date>
<booktitle>In Proc. of ACL 2002 Workshop on WSD: Recent Successes and Future Directions.</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1801" citStr="Chklovski and Mihalcea, 2002" startWordPosition="270" endWordPosition="273">onary (Fellbaum, 1998) as a sense inventory, thanks to its free availability, wide coverage, and existence of a number of standard test sets based on it. Unfortunately, WordNet is a fine-grained resource, encoding sense distinctions that are often difficult to recognize even for human annotators (Edmonds and Kilgariff, 1998). Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer, 2004) and 67.3% on the Open Mind Word Expert annotation exercise (Chklovski and Mihalcea, 2002). These numbers lead us to believe that a credible upper bound for unrestricted fine-grained WSD is around 70%, a figure that state-of-the-art automatic systems find it difficult to outperform. Furthermore, even if a system were able to exceed such an upper bound, it would be unclear how to interpret such a result. It seems therefore that the major obstacle to effective WSD is the fine granularity of the WordNet sense inventory, rather than the performance of the best disambiguation systems. Interestingly, Ng et al. (1999) show that, when a coarse-grained sense inventory is adopted, the increa</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Tim Chklovski and Rada Mihalcea. 2002. Building a sense tagged corpus with open mind word expert. In Proc. of ACL 2002 Workshop on WSD: Recent Successes and Future Directions. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Exploiting agreement and disagreement of human annotators for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proc. of Recent Advances In NLP (RANLP</booktitle>
<location>Borovetz, Bulgaria.</location>
<contexts>
<context position="26856" citStr="Chklovski and Mihalcea, 2003" startWordPosition="4456" endWordPosition="4459">-grained sense). 5 Related Work WordNet for the identification of sense regularities: to this end, they provide a set of semantic and probabilistic rules. An evaluation of the heuristics provided leads to a polysemy reduction of 39% and an error rate of 5.6%. A different principle for clustering WordNet senses, based on the Minimum Description Length, is described by Tomuro (2001). The clustering is evaluated against WordNet cousins and used for the study of inter-annotator disagreement. Another approach exploits the (dis)agreements of human annotators to derive coarse-grained sense clusters (Chklovski and Mihalcea, 2003), where sense similarity is computed from confusion matrices. Agirre and Lopez (2003) analyze a set of methods to cluster WordNet senses based on the use of confusion matrices from the results of WSD systems, translation equivalences, and topic signatures (word co-occurrences extracted from the web). They assess the acquired clusterings against 20 words from the Senseval-2 sense groupings. Finally, McCarthy (2006) proposes the use of ranked lists, based on distributionally nearest neighbours, to relate word senses. This softer notion of sense relatedness allows to adopt the most appropriate gr</context>
</contexts>
<marker>Chklovski, Mihalcea, 2003</marker>
<rawString>Tim Chklovski and Rada Mihalcea. 2003. Exploiting agreement and disagreement of human annotators for word sense disambiguation. In Proc. of Recent Advances In NLP (RANLP 2003). Borovetz, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Decadt</author>
<author>V´eronique Hoste</author>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>Gambl, genetic algorithm optimization of memory-based wsd.</title>
<date>2004</date>
<booktitle>In Proc. ofACL/SIGLEX Senseval-3.</booktitle>
<location>Barcelona,</location>
<marker>Decadt, Hoste, Daelemans, van den Bosch, 2004</marker>
<rawString>Bart Decadt, V´eronique Hoste, Walter Daelemans, and Antal van den Bosch. 2004. Gambl, genetic algorithm optimization of memory-based wsd. In Proc. ofACL/SIGLEX Senseval-3. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
</authors>
<title>Word sense ambiguation: Clustering related senses.</title>
<date>1994</date>
<booktitle>In Proc. of 15th Conference on Computational Linguistics (COLING).</booktitle>
<location>Morristown, N.J.</location>
<contexts>
<context position="27937" citStr="Dolan (1994)" startWordPosition="4627" endWordPosition="4628">tionally nearest neighbours, to relate word senses. This softer notion of sense relatedness allows to adopt the most appropriate granularity for a specific application. Compared to our approach, most of these methods do not evaluate the clustering produced with respect to a gold-standard clustering. Indeed, such an evaluation would be difficult and timeconsuming without a coarse sense inventory like that of ODE. A limited assessment of coarse WSD is performed by Fellbaum et al. (2001), who obtain a large improvement in the accuracy of a maximum-entropy system on clustered verbs. 6 Conclusions Dolan (1994) describes a method for clustering word senses with the use of information provided in the electronic version of LDOCE (textual definitions, semantic relations, domain labels, etc.). Unfortunately, the approach is not described in detail and no evaluation is provided. Most of the approaches in the literature make use of the WordNet structure to cluster its senses. Peters et al. (1998) exploit specific patterns in the WordNet hierarchy (e.g. sisters, autohyponymy, twins, etc.) to group word senses. They study semantic regularities or generalizations obtained and analyze the effect of clustering</context>
</contexts>
<marker>Dolan, 1994</marker>
<rawString>William B. Dolan. 1994. Word sense ambiguation: Clustering related senses. In Proc. of 15th Conference on Computational Linguistics (COLING). Morristown, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Adam Kilgariff</author>
</authors>
<title>Introduction to the special issue on evaluating word sense disambiguation systems.</title>
<date>1998</date>
<journal>Journal ofNatural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="1498" citStr="Edmonds and Kilgariff, 1998" startWordPosition="223" endWordPosition="226"> use of WSD in specific applications (e.g. Vickrey et al. (2005) and Stokoe (2005)), the present performance of the best ranking WSD systems does not provide a sufficient degree of accuracy to enable real-world, language-aware applications. Most of the disambiguation approaches adopt the WordNet dictionary (Fellbaum, 1998) as a sense inventory, thanks to its free availability, wide coverage, and existence of a number of standard test sets based on it. Unfortunately, WordNet is a fine-grained resource, encoding sense distinctions that are often difficult to recognize even for human annotators (Edmonds and Kilgariff, 1998). Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer, 2004) and 67.3% on the Open Mind Word Expert annotation exercise (Chklovski and Mihalcea, 2002). These numbers lead us to believe that a credible upper bound for unrestricted fine-grained WSD is around 70%, a figure that state-of-the-art automatic systems find it difficult to outperform. Furthermore, even if a system were able to exceed such an upper bound, it would be unclear how to inter</context>
</contexts>
<marker>Edmonds, Kilgariff, 1998</marker>
<rawString>Philip Edmonds and Adam Kilgariff. 1998. Introduction to the special issue on evaluating word sense disambiguation systems. Journal ofNatural Language Engineering, 8(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Lauren Delfs</author>
<author>Susanne Wolf</author>
</authors>
<title>Manual and automatic semantic annotation with wordnet.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL Workshop on WordNet and Other Lexical Resources.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="27814" citStr="Fellbaum et al. (2001)" startWordPosition="4605" endWordPosition="4608">gs against 20 words from the Senseval-2 sense groupings. Finally, McCarthy (2006) proposes the use of ranked lists, based on distributionally nearest neighbours, to relate word senses. This softer notion of sense relatedness allows to adopt the most appropriate granularity for a specific application. Compared to our approach, most of these methods do not evaluate the clustering produced with respect to a gold-standard clustering. Indeed, such an evaluation would be difficult and timeconsuming without a coarse sense inventory like that of ODE. A limited assessment of coarse WSD is performed by Fellbaum et al. (2001), who obtain a large improvement in the accuracy of a maximum-entropy system on clustered verbs. 6 Conclusions Dolan (1994) describes a method for clustering word senses with the use of information provided in the electronic version of LDOCE (textual definitions, semantic relations, domain labels, etc.). Unfortunately, the approach is not described in detail and no evaluation is provided. Most of the approaches in the literature make use of the WordNet structure to cluster its senses. Peters et al. (1998) exploit specific patterns in the WordNet hierarchy (e.g. sisters, autohyponymy, twins, et</context>
</contexts>
<marker>Fellbaum, Palmer, Dang, Delfs, Wolf, 2001</marker>
<rawString>Christiane Fellbaum, Martha Palmer, Hoa Trang Dang, Lauren Delfs, and Susanne Wolf. 2001. Manual and automatic semantic annotation with wordnet. In Proc. of NAACL Workshop on WordNet and Other Lexical Resources. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="28324" citStr="(1998)" startWordPosition="4690" endWordPosition="4690">tory like that of ODE. A limited assessment of coarse WSD is performed by Fellbaum et al. (2001), who obtain a large improvement in the accuracy of a maximum-entropy system on clustered verbs. 6 Conclusions Dolan (1994) describes a method for clustering word senses with the use of information provided in the electronic version of LDOCE (textual definitions, semantic relations, domain labels, etc.). Unfortunately, the approach is not described in detail and no evaluation is provided. Most of the approaches in the literature make use of the WordNet structure to cluster its senses. Peters et al. (1998) exploit specific patterns in the WordNet hierarchy (e.g. sisters, autohyponymy, twins, etc.) to group word senses. They study semantic regularities or generalizations obtained and analyze the effect of clustering on the compatibility of language-specific wordnets. Mihalcea and Moldovan (2001) study the structure of In this paper, we presented a study on the construction of a coarse sense inventory for the WordNet lexicon and its effects on unrestricted WSD. A key feature in our approach is the use of a well-established dictionary encoding sense hierarchies. As remarked in Section 2.2, the met</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine code from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proc. of 5th Conf. on Systems Documentation.</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="11874" citStr="Lesk, 1986" startWordPosition="1966" endWordPosition="1967">S&apos;. We define the mapping µ as: � ˆm(S) if match(S, ˆm(S)) ≥ B µ(S) = ption lengths: ng of senses c(w) of a word w as: senses for which no mapping can be provided according to the definition of µ. For example, an ideal mapping between entries in Table 1 would be as follows: µ(race#n#1) = race#n#1.1, µ(race#n#2) = race#n#2.1, µ(race#n#3) = race#n#1.1, µ(race#n#5) = race#n#2.1, µ(race#n#4) = race#n#1.2, µ(race#n#6) = race#n#1.3, resulting in the following clustering: 2.3.1 Lexical matching As a first approach, we adopted a purely lexical matching function based on the notion of lexical overlap (Lesk, 1986). The function counts the number of lemmas that two sense descriptions of a word have in common (we neglect parts of speech), and is normalized by the minimum of the two descri matchLESK(S, S&apos;) = |dWN(S)∩dODE(S&apos;)| min{|dWN(S)|,|dODE(S&apos;)|} E otherwise where S E SensesWN(w) and S&apos; E SensesODE(w). For instance: matchLESK(race#n#1, race#n#1.1) = min{4,201 3 4 = 0.75 3 matchLESK(race#n#2, race#n#1.1) = 8 = 0.125 1 Notice that unrelated senses can get a positive score because of an overlap of the sense descriptions. In the example, group#n, the hypernym of race#n#2, is also present in the definition</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine code from an ice cream cone. In Proc. of 5th Conf. on Systems Documentation. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavagli`a</author>
</authors>
<title>Integrating subject field codes into wordnet.</title>
<date>2000</date>
<booktitle>In Proc. of the 2nd Conference on Language Resources and Evaluation (LREC).</booktitle>
<location>Athens, Greece.</location>
<marker>Magnini, Cavagli`a, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavagli`a. 2000. Integrating subject field codes into wordnet. In Proc. of the 2nd Conference on Language Resources and Evaluation (LREC). Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Relating wordnet senses for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proc. ofACL Workshop on Making Sense of Sense.</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="27273" citStr="McCarthy (2006)" startWordPosition="4521" endWordPosition="4522">usins and used for the study of inter-annotator disagreement. Another approach exploits the (dis)agreements of human annotators to derive coarse-grained sense clusters (Chklovski and Mihalcea, 2003), where sense similarity is computed from confusion matrices. Agirre and Lopez (2003) analyze a set of methods to cluster WordNet senses based on the use of confusion matrices from the results of WSD systems, translation equivalences, and topic signatures (word co-occurrences extracted from the web). They assess the acquired clusterings against 20 words from the Senseval-2 sense groupings. Finally, McCarthy (2006) proposes the use of ranked lists, based on distributionally nearest neighbours, to relate word senses. This softer notion of sense relatedness allows to adopt the most appropriate granularity for a specific application. Compared to our approach, most of these methods do not evaluate the clustering produced with respect to a gold-standard clustering. Indeed, such an evaluation would be difficult and timeconsuming without a coarse sense inventory like that of ODE. A limited assessment of coarse WSD is performed by Fellbaum et al. (2001), who obtain a large improvement in the accuracy of a maxim</context>
</contexts>
<marker>McCarthy, 2006</marker>
<rawString>Diana McCarthy. 2006. Relating wordnet senses for word sense disambiguation. In Proc. ofACL Workshop on Making Sense of Sense. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ehsanul Faruque</author>
</authors>
<title>Senselearner: Minimally supervised word sense disambiguation for all words in open text.</title>
<date>2004</date>
<booktitle>In Proc. of ACL/SIGLEX Senseval-3.</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="22594" citStr="Mihalcea and Faruque, 2004" startWordPosition="3747" endWordPosition="3750">derivatives in WordNet. As we are experimenting on an automaticallyacquired clustering, all the figures are affected by the 22.06% error rate resulting from Table 2. 4.1 Experiments on Senseval-3 As a first experiment, we assessed the effect of the automatic sense clustering on the English allwords task at Senseval-3 (Snyder and Palmer, 2004). This task required WSD systems to provide a sense choice for 2,081 content words in a set of 301 sentences from the fiction, news story, and editorial domains. We considered the three best-ranking WSD systems – GAMBL (Decadt et al., 2004), SenseLearner (Mihalcea and Faruque, 2004), and Koc Table 4: Performance of WSD systems at Senseval-3 on coarse-grained sense inventories. System Prec. Rec. F1 F1fine Gambl 0.779 0.779 0.779 0.652 SenseLearner 0.769 0.769 0.769 0.646 KOC Univ. 0.768 0.768 0.768 0.641 SSI 0.758 0.758 0.758 0.612 IRST-DDD 0.721 0.719 0.720 0.583 FS baseline 0.769 0.769 0.769 0.624 Random BL 0.497 0.497 0.497 0.340 University (Yuret, 2004) – and the best unsupervised system, namely IRST-DDD (Strapparava et al., 2004). We also included SSI as it outperforms all the untrained systems (Navigli and Velardi, 2005). To evaluate the performance of the five syst</context>
</contexts>
<marker>Mihalcea, Faruque, 2004</marker>
<rawString>Rada Mihalcea and Ehsanul Faruque. 2004. Senselearner: Minimally supervised word sense disambiguation for all words in open text. In Proc. of ACL/SIGLEX Senseval-3. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic generation of a coarse grained wordnet.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL Workshop on WordNet and Other Lexical Resources.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="28618" citStr="Mihalcea and Moldovan (2001)" startWordPosition="4727" endWordPosition="4731">he use of information provided in the electronic version of LDOCE (textual definitions, semantic relations, domain labels, etc.). Unfortunately, the approach is not described in detail and no evaluation is provided. Most of the approaches in the literature make use of the WordNet structure to cluster its senses. Peters et al. (1998) exploit specific patterns in the WordNet hierarchy (e.g. sisters, autohyponymy, twins, etc.) to group word senses. They study semantic regularities or generalizations obtained and analyze the effect of clustering on the compatibility of language-specific wordnets. Mihalcea and Moldovan (2001) study the structure of In this paper, we presented a study on the construction of a coarse sense inventory for the WordNet lexicon and its effects on unrestricted WSD. A key feature in our approach is the use of a well-established dictionary encoding sense hierarchies. As remarked in Section 2.2, the method can employ any dictionary with a sufficiently structured inventory of senses, and can thus be applied to reduce the granularity of, e.g., wordnets of other languages. One could argue that the adoption of the ODE as a sense inventory for WSD would be a better solution. While we are not agai</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 2001. Automatic generation of a coarse grained wordnet. In Proc. of NAACL Workshop on WordNet and Other Lexical Resources. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="13816" citStr="Morris and Hirst, 1991" startWordPosition="2271" endWordPosition="2274">formed into a relatedness edge (Navigli and Velardi, 2005). Given a word context C = {w1, ..., wn}, SSI builds a graph G = (V, E) such that V = n U SensesWN(wi) and (S, S&apos;) E E if there is i=1 at least one semantic interconnection between S and S&apos; in the lexical knowledge base. A semantic interconnection pattern is a relevant sequence of edges selected according to a manually-created context-free grammar, i.e. a path connecting a pair of word senses, possibly including a number of intermediate concepts. The grammar consists of a small number of rules, inspired by the notion of lexical chains (Morris and Hirst, 1991). SSI performs disambiguation in an iterative fashion, by maintaining a set C of senses as a semantic context. Initially, C = V (the entire set of senses of words in C). At each step, for each sense S in C, the algorithm calculates a score of the degree of connectivity between S and the other senses in C: 3Available online from: http://lcl.di.uniroma1.it/ssi E |IC(S,S&apos;)| S&apos;EC\{S} where IC(S, S&apos;) is the set of interconnections between senses S and S&apos;. The contribution of a single interconnection is given by the reciprocal of its length, calculated as the number of edges connecting its ends. The</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning domain ontologies from document warehouses and dedicated websites.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="12774" citStr="Navigli and Velardi (2004)" startWordPosition="2101" endWordPosition="2104">nd S&apos; E SensesODE(w). For instance: matchLESK(race#n#1, race#n#1.1) = min{4,201 3 4 = 0.75 3 matchLESK(race#n#2, race#n#1.1) = 8 = 0.125 1 Notice that unrelated senses can get a positive score because of an overlap of the sense descriptions. In the example, group#n, the hypernym of race#n#2, is also present in the definition of race#n#1.1. 2.3.2 Semantic matching Unfortunately, the very same concept can be defined with entirely different words. To match definitions in a semantic manner we adopted a knowledge-based Word Sense Disambiguation algorithm, Structural Semantic Interconnections (SSI, Navigli and Velardi (2004)). SSI3 exploits an extensive lexical knowledge base, built upon the WordNet lexicon and enriched with collocation information representing semantic relatedness between sense pairs. Collocations are acquired from existing resources (like the Oxford Collocations, the Longman Language Activator, collocation web sites, etc.). Each collocation is mapped to the WordNet sense inventory in a semi-automatic manner and transformed into a relatedness edge (Navigli and Velardi, 2005). Given a word context C = {w1, ..., wn}, SSI builds a graph G = (V, E) such that V = n U SensesWN(wi) and (S, S&apos;) E E if t</context>
</contexts>
<marker>Navigli, Velardi, 2004</marker>
<rawString>Roberto Navigli and Paola Velardi. 2004. Learning domain ontologies from document warehouses and dedicated websites. Computational Linguistics, 30(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: a knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="13251" citStr="Navigli and Velardi, 2005" startWordPosition="2169" endWordPosition="2172">mantic manner we adopted a knowledge-based Word Sense Disambiguation algorithm, Structural Semantic Interconnections (SSI, Navigli and Velardi (2004)). SSI3 exploits an extensive lexical knowledge base, built upon the WordNet lexicon and enriched with collocation information representing semantic relatedness between sense pairs. Collocations are acquired from existing resources (like the Oxford Collocations, the Longman Language Activator, collocation web sites, etc.). Each collocation is mapped to the WordNet sense inventory in a semi-automatic manner and transformed into a relatedness edge (Navigli and Velardi, 2005). Given a word context C = {w1, ..., wn}, SSI builds a graph G = (V, E) such that V = n U SensesWN(wi) and (S, S&apos;) E E if there is i=1 at least one semantic interconnection between S and S&apos; in the lexical knowledge base. A semantic interconnection pattern is a relevant sequence of edges selected according to a manually-created context-free grammar, i.e. a path connecting a pair of word senses, possibly including a number of intermediate concepts. The grammar consists of a small number of rules, inspired by the notion of lexical chains (Morris and Hirst, 1991). SSI performs disambiguation in an</context>
<context position="23148" citStr="Navigli and Velardi, 2005" startWordPosition="3836" endWordPosition="3840">– GAMBL (Decadt et al., 2004), SenseLearner (Mihalcea and Faruque, 2004), and Koc Table 4: Performance of WSD systems at Senseval-3 on coarse-grained sense inventories. System Prec. Rec. F1 F1fine Gambl 0.779 0.779 0.779 0.652 SenseLearner 0.769 0.769 0.769 0.646 KOC Univ. 0.768 0.768 0.768 0.641 SSI 0.758 0.758 0.758 0.612 IRST-DDD 0.721 0.719 0.720 0.583 FS baseline 0.769 0.769 0.769 0.624 Random BL 0.497 0.497 0.497 0.340 University (Yuret, 2004) – and the best unsupervised system, namely IRST-DDD (Strapparava et al., 2004). We also included SSI as it outperforms all the untrained systems (Navigli and Velardi, 2005). To evaluate the performance of the five systems on our coarse clustering, we considered a fine-grained answer to be correct if it belongs to the same cluster as that of the correct answer. Table 4 reports the performance of the systems, together with the first sense and the random baseline (in the last column we report the performance on the original fine-grained test set). The best system, Gambl, obtains almost 78% precision and recall, an interesting figure compared to 65% performance in the fine-grained WSD task. An interesting aspect is that the ranking across systems was maintained when</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli and Paola Velardi. 2005. Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 27(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee T Ng</author>
<author>Chung Y Lim</author>
<author>Shou K Foo</author>
</authors>
<title>A case study on the inter-annotator agreement for word sense disambiguation.</title>
<date>1999</date>
<booktitle>In Proc. of ACL Workshop: Standardizing Lexical Resources. College Park,</booktitle>
<location>Maryland.</location>
<contexts>
<context position="2329" citStr="Ng et al. (1999)" startWordPosition="357" endWordPosition="360">nd 67.3% on the Open Mind Word Expert annotation exercise (Chklovski and Mihalcea, 2002). These numbers lead us to believe that a credible upper bound for unrestricted fine-grained WSD is around 70%, a figure that state-of-the-art automatic systems find it difficult to outperform. Furthermore, even if a system were able to exceed such an upper bound, it would be unclear how to interpret such a result. It seems therefore that the major obstacle to effective WSD is the fine granularity of the WordNet sense inventory, rather than the performance of the best disambiguation systems. Interestingly, Ng et al. (1999) show that, when a coarse-grained sense inventory is adopted, the increase in interannotator agreement is much higher than the reduction of the polysemy degree. Following these observations, the main question that we tackle in this paper is: can we produce and evaluate coarse-grained sense distinctions and show that they help boost disambiguation on standard test sets? We believe that this is a crucial research topic in the field of WSD, that could potentially benefit several application areas. The contribution of this paper is two-fold. First, we provide a wide-coverage method for clustering </context>
</contexts>
<marker>Ng, Lim, Foo, 1999</marker>
<rawString>Hwee T. Ng, Chung Y. Lim, and Shou K. Foo. 1999. A case study on the inter-annotator agreement for word sense disambiguation. In Proc. of ACL Workshop: Standardizing Lexical Resources. College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wim Peters</author>
<author>Ivonne Peters</author>
<author>Piek Vossen</author>
</authors>
<title>Automatic sense clustering in eurowordnet.</title>
<date>1998</date>
<booktitle>In Proc. of the 1st Conference on Language Resources and Evaluation (LREC).</booktitle>
<location>Granada,</location>
<contexts>
<context position="28324" citStr="Peters et al. (1998)" startWordPosition="4687" endWordPosition="4690">se sense inventory like that of ODE. A limited assessment of coarse WSD is performed by Fellbaum et al. (2001), who obtain a large improvement in the accuracy of a maximum-entropy system on clustered verbs. 6 Conclusions Dolan (1994) describes a method for clustering word senses with the use of information provided in the electronic version of LDOCE (textual definitions, semantic relations, domain labels, etc.). Unfortunately, the approach is not described in detail and no evaluation is provided. Most of the approaches in the literature make use of the WordNet structure to cluster its senses. Peters et al. (1998) exploit specific patterns in the WordNet hierarchy (e.g. sisters, autohyponymy, twins, etc.) to group word senses. They study semantic regularities or generalizations obtained and analyze the effect of clustering on the compatibility of language-specific wordnets. Mihalcea and Moldovan (2001) study the structure of In this paper, we presented a study on the construction of a coarse sense inventory for the WordNet lexicon and its effects on unrestricted WSD. A key feature in our approach is the use of a well-established dictionary encoding sense hierarchies. As remarked in Section 2.2, the met</context>
</contexts>
<marker>Peters, Peters, Vossen, 1998</marker>
<rawString>Wim Peters, Ivonne Peters, and Piek Vossen. 1998. Automatic sense clustering in eurowordnet. In Proc. of the 1st Conference on Language Resources and Evaluation (LREC). Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The english all-words task.</title>
<date>2004</date>
<booktitle>In Proc. ofACL 2004 SENSEVAL-3 Workshop.</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1711" citStr="Snyder and Palmer, 2004" startWordPosition="255" endWordPosition="259">age-aware applications. Most of the disambiguation approaches adopt the WordNet dictionary (Fellbaum, 1998) as a sense inventory, thanks to its free availability, wide coverage, and existence of a number of standard test sets based on it. Unfortunately, WordNet is a fine-grained resource, encoding sense distinctions that are often difficult to recognize even for human annotators (Edmonds and Kilgariff, 1998). Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer, 2004) and 67.3% on the Open Mind Word Expert annotation exercise (Chklovski and Mihalcea, 2002). These numbers lead us to believe that a credible upper bound for unrestricted fine-grained WSD is around 70%, a figure that state-of-the-art automatic systems find it difficult to outperform. Furthermore, even if a system were able to exceed such an upper bound, it would be unclear how to interpret such a result. It seems therefore that the major obstacle to effective WSD is the fine granularity of the WordNet sense inventory, rather than the performance of the best disambiguation systems. Interestingly</context>
<context position="22311" citStr="Snyder and Palmer, 2004" startWordPosition="3698" endWordPosition="3701">ire a clustering on the entire WordNet sense inventory using this approach. As a result, we obtained a reduction of 33.54% in the number of entries (from 60,302 to 40,079 senses) and a decrease of the polysemy degree from 3.14 to 2.09. These figures exclude monosemous senses and derivatives in WordNet. As we are experimenting on an automaticallyacquired clustering, all the figures are affected by the 22.06% error rate resulting from Table 2. 4.1 Experiments on Senseval-3 As a first experiment, we assessed the effect of the automatic sense clustering on the English allwords task at Senseval-3 (Snyder and Palmer, 2004). This task required WSD systems to provide a sense choice for 2,081 content words in a set of 301 sentences from the fiction, news story, and editorial domains. We considered the three best-ranking WSD systems – GAMBL (Decadt et al., 2004), SenseLearner (Mihalcea and Faruque, 2004), and Koc Table 4: Performance of WSD systems at Senseval-3 on coarse-grained sense inventories. System Prec. Rec. F1 F1fine Gambl 0.779 0.779 0.779 0.652 SenseLearner 0.769 0.769 0.769 0.646 KOC Univ. 0.768 0.768 0.768 0.641 SSI 0.758 0.758 0.758 0.612 IRST-DDD 0.721 0.719 0.720 0.583 FS baseline 0.769 0.769 0.769 </context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The english all-words task. In Proc. ofACL 2004 SENSEVAL-3 Workshop. Barcelona, Spain.</rawString>
</citation>
<citation valid="false">
<booktitle>2003. Oxford Dictionary of English.</booktitle>
<editor>Catherine Soanes and Angus Stevenson, editors.</editor>
<publisher>Oxford University Press.</publisher>
<marker></marker>
<rawString>Catherine Soanes and Angus Stevenson, editors. 2003. Oxford Dictionary of English. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Stokoe</author>
</authors>
<title>Differentiating homonymy and polysemy in information retrieval.</title>
<date>2005</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="952" citStr="Stokoe (2005)" startWordPosition="146" endWordPosition="147">hod for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. 1 Introduction Word Sense Disambiguation (WSD) is undoubtedly one of the hardest tasks in the field of Natural Language Processing. Even though some recent studies report benefits in the use of WSD in specific applications (e.g. Vickrey et al. (2005) and Stokoe (2005)), the present performance of the best ranking WSD systems does not provide a sufficient degree of accuracy to enable real-world, language-aware applications. Most of the disambiguation approaches adopt the WordNet dictionary (Fellbaum, 1998) as a sense inventory, thanks to its free availability, wide coverage, and existence of a number of standard test sets based on it. Unfortunately, WordNet is a fine-grained resource, encoding sense distinctions that are often difficult to recognize even for human annotators (Edmonds and Kilgariff, 1998). Recent estimations of the inter-annotator agreement </context>
</contexts>
<marker>Stokoe, 2005</marker>
<rawString>Christopher Stokoe. 2005. Differentiating homonymy and polysemy in information retrieval. In Proc. of the Conference on Empirical Methods in Natural Language Processing. Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proc. ofACL/SIGLEX Senseval3.</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="23054" citStr="Strapparava et al., 2004" startWordPosition="3820" endWordPosition="3823">fiction, news story, and editorial domains. We considered the three best-ranking WSD systems – GAMBL (Decadt et al., 2004), SenseLearner (Mihalcea and Faruque, 2004), and Koc Table 4: Performance of WSD systems at Senseval-3 on coarse-grained sense inventories. System Prec. Rec. F1 F1fine Gambl 0.779 0.779 0.779 0.652 SenseLearner 0.769 0.769 0.769 0.646 KOC Univ. 0.768 0.768 0.768 0.641 SSI 0.758 0.758 0.758 0.612 IRST-DDD 0.721 0.719 0.720 0.583 FS baseline 0.769 0.769 0.769 0.624 Random BL 0.497 0.497 0.497 0.340 University (Yuret, 2004) – and the best unsupervised system, namely IRST-DDD (Strapparava et al., 2004). We also included SSI as it outperforms all the untrained systems (Navigli and Velardi, 2005). To evaluate the performance of the five systems on our coarse clustering, we considered a fine-grained answer to be correct if it belongs to the same cluster as that of the correct answer. Table 4 reports the performance of the systems, together with the first sense and the random baseline (in the last column we report the performance on the original fine-grained test set). The best system, Gambl, obtains almost 78% precision and recall, an interesting figure compared to 65% performance in the fine-</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation. In Proc. ofACL/SIGLEX Senseval3. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Tomuro</author>
</authors>
<title>Tree-cut and a lexicon based on systematic polysemy.</title>
<date>2001</date>
<booktitle>In Proc. of the Meeting of the NAACL.</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="26610" citStr="Tomuro (2001)" startWordPosition="4426" endWordPosition="4427">y significant, p &lt; 0.05). The increase in recall is mostly due to the fact that different senses belonging to the same cluster now contribute together to the choice of that cluster (rather than individually to the choice of a fine-grained sense). 5 Related Work WordNet for the identification of sense regularities: to this end, they provide a set of semantic and probabilistic rules. An evaluation of the heuristics provided leads to a polysemy reduction of 39% and an error rate of 5.6%. A different principle for clustering WordNet senses, based on the Minimum Description Length, is described by Tomuro (2001). The clustering is evaluated against WordNet cousins and used for the study of inter-annotator disagreement. Another approach exploits the (dis)agreements of human annotators to derive coarse-grained sense clusters (Chklovski and Mihalcea, 2003), where sense similarity is computed from confusion matrices. Agirre and Lopez (2003) analyze a set of methods to cluster WordNet senses based on the use of confusion matrices from the results of WSD systems, translation equivalences, and topic signatures (word co-occurrences extracted from the web). They assess the acquired clusterings against 20 word</context>
</contexts>
<marker>Tomuro, 2001</marker>
<rawString>Noriko Tomuro. 2001. Tree-cut and a lexicon based on systematic polysemy. In Proc. of the Meeting of the NAACL. Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Luke Biewald</author>
<author>Marc Teyssier</author>
<author>Daphne Koller</author>
</authors>
<title>Word sense disambiguation vs. statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of Conference on Empirical Methods in Natural Language Processing.</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="934" citStr="Vickrey et al. (2005)" startWordPosition="141" endWordPosition="144">is paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. 1 Introduction Word Sense Disambiguation (WSD) is undoubtedly one of the hardest tasks in the field of Natural Language Processing. Even though some recent studies report benefits in the use of WSD in specific applications (e.g. Vickrey et al. (2005) and Stokoe (2005)), the present performance of the best ranking WSD systems does not provide a sufficient degree of accuracy to enable real-world, language-aware applications. Most of the disambiguation approaches adopt the WordNet dictionary (Fellbaum, 1998) as a sense inventory, thanks to its free availability, wide coverage, and existence of a number of standard test sets based on it. Unfortunately, WordNet is a fine-grained resource, encoding sense distinctions that are often difficult to recognize even for human annotators (Edmonds and Kilgariff, 1998). Recent estimations of the inter-an</context>
</contexts>
<marker>Vickrey, Biewald, Teyssier, Koller, 2005</marker>
<rawString>David Vickrey, Luke Biewald, Marc Teyssier, and Daphne Koller. 2005. Word sense disambiguation vs. statistical machine translation. In Proc. of Conference on Empirical Methods in Natural Language Processing. Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Some experiments with a naive bayes wsd system.</title>
<date>2004</date>
<booktitle>In Proc. of ACL/SIGLEX Senseval-3.</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="22975" citStr="Yuret, 2004" startWordPosition="3809" endWordPosition="3810">choice for 2,081 content words in a set of 301 sentences from the fiction, news story, and editorial domains. We considered the three best-ranking WSD systems – GAMBL (Decadt et al., 2004), SenseLearner (Mihalcea and Faruque, 2004), and Koc Table 4: Performance of WSD systems at Senseval-3 on coarse-grained sense inventories. System Prec. Rec. F1 F1fine Gambl 0.779 0.779 0.779 0.652 SenseLearner 0.769 0.769 0.769 0.646 KOC Univ. 0.768 0.768 0.768 0.641 SSI 0.758 0.758 0.758 0.612 IRST-DDD 0.721 0.719 0.720 0.583 FS baseline 0.769 0.769 0.769 0.624 Random BL 0.497 0.497 0.497 0.340 University (Yuret, 2004) – and the best unsupervised system, namely IRST-DDD (Strapparava et al., 2004). We also included SSI as it outperforms all the untrained systems (Navigli and Velardi, 2005). To evaluate the performance of the five systems on our coarse clustering, we considered a fine-grained answer to be correct if it belongs to the same cluster as that of the correct answer. Table 4 reports the performance of the systems, together with the first sense and the random baseline (in the last column we report the performance on the original fine-grained test set). The best system, Gambl, obtains almost 78% preci</context>
</contexts>
<marker>Yuret, 2004</marker>
<rawString>Deniz Yuret. 2004. Some experiments with a naive bayes wsd system. In Proc. of ACL/SIGLEX Senseval-3. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
</authors>
<title>Empirical and theoretical comparisons of selected criterion functions for document clustering.</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<volume>55</volume>
<issue>3</issue>
<contexts>
<context position="18657" citStr="Zhao and Karypis, 2004" startWordPosition="3086" endWordPosition="3089">g a corresponding entry in ODE (i.e. excluding the cases where a sense 2 was assigned by the manual annotators, cf. Section 2.3). We also report in the Table the accuracy of the two functions when we view the problem as a classification task: an automatic association is correct if it corresponds to the manual association provided by the annotators or if both assign no answer (equivalently, if both provide an 2 label). All the differences between Lesk and SSI are statistically significant (p &lt; 0.01). As a second experiment, we used two information-theoretic measures, namely entropy and purity (Zhao and Karypis, 2004), to compare an automatic clustering c(w) (i.e. the sense groups acquired for word w) with a manual clustering ˆc(w). The entropy quantifies the distribution of the senses of a group over manually-defined groups, while the purity measures the extent to which a group contains senses primarily from one manual group. Given a word w, and a sense group G E c(w), the entropy of G is defined as: � H(G) = −1 log |ˆc(w)| ˆGEˆc(w) i.e., the entropy4 of the distribution of senses of group G over the groups of the manual clustering ˆc(w). The entropy of an entire clustering c(w) is defined as: Entropy(c(w</context>
</contexts>
<marker>Zhao, Karypis, 2004</marker>
<rawString>Ying Zhao and George Karypis. 2004. Empirical and theoretical comparisons of selected criterion functions for document clustering. Machine Learning, 55(3).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>