<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.293619">
Human Evaluation of Machine Translation Through Binary System
Comparisons
</title>
<author confidence="0.548228">
David Vilar, Gregor Leusch
and Hermann Ney
</author>
<affiliation confidence="0.774064">
Lehrstuhl f¨ur Informatik 6
RWTH Aachen University
</affiliation>
<address confidence="0.652607">
D-52056 Aachen, Germany
</address>
<email confidence="0.997715">
{vilar,leusch,ney}@cs.rwth-aachen
</email>
<author confidence="0.595368">
Rafael E. Banchs
</author>
<affiliation confidence="0.622825">
D. of Signal Theory and Communications
</affiliation>
<address confidence="0.8756885">
Universitat Polit`ecnica de Catalunya
08034 Barcelona, Spain
</address>
<email confidence="0.951261">
rbanchs@gps.tsc.upc.edu
</email>
<bodyText confidence="0.544094">
.de
</bodyText>
<sectionHeader confidence="0.981548" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887541666667">
We introduce a novel evaluation scheme for
the human evaluation of different machine
translation systems. Our method is based
on direct comparison of two sentences at a
time by human judges. These binary judg-
ments are then used to decide between all
possible rankings of the systems. The ad-
vantages of this new method are the lower
dependency on extensive evaluation guide-
lines, and a tighter focus on a typical eval-
uation task, namely the ranking of systems.
Furthermore we argue that machine transla-
tion evaluations should be regarded as sta-
tistical processes, both for human and au-
tomatic evaluation. We show how confi-
dence ranges for state-of-the-art evaluation
measures such as WER and TER can be
computed accurately and efficiently without
having to resort to Monte Carlo estimates.
We give an example of our new evaluation
scheme, as well as a comparison with classi-
cal automatic and human evaluation on data
from a recent international evaluation cam-
paign.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970073170732">
Evaluation of machine translation (MT) output is a
difficult and still open problem. As in other natu-
ral language processing tasks, automatic measures
which try to asses the quality of the translation
can be computed. The most widely known are the
Word Error Rate (WER), the Position independent
word Error Rate (PER), the NIST score (Dodding-
ton, 2002) and, especially in recent years, the BLEU
score (Papineni et al., 2002) and the Translation Er-
ror Rate (TER) (Snover et al., 2005). All of the-
ses measures compare the system output with one
or more gold standard references and produce a nu-
merical value (score or error rate) which measures
the similarity between the machine translation and a
human produced one. Once such reference transla-
tions are available, the evaluation can be carried out
in a quick, efficient and reproducible manner.
However, automatic measures also have big dis-
advantages; (Callison-Burch et al., 2006) describes
some of them. A major problem is that a given sen-
tence in one language can have several correct trans-
lations in another language and thus, the measure of
similarity with one or even a small amount of ref-
erence translations will never be flexible enough to
truly reflect the wide range of correct possibilities of
a translation. 1 This holds in particular for long sen-
tences and wide- or open-domain tasks like the ones
dealt with in current MT projects and evaluations.
If the actual quality of a translation in terms of
usefulness for human users is to be evaluated, human
evaluation needs to be carried out. This is however
a costly and very time-consuming process. In this
work we present a novel approach to human evalu-
ation that simplifies the task for human judges. In-
stead of having to assign numerical scores to each
sentence to be evaluated, as is done in current evalu-
ation procedures, human judges choose the best one
out of two candidate translations. We show how this
method can be used to rank an arbitrary number of
systems and present a detailed analysis of the statis-
tical significance of the method.
</bodyText>
<footnote confidence="0.9883785">
1Compare this with speech recognition, where apart from
orthographic variance there is only one correct reference.
</footnote>
<page confidence="0.95428">
96
</page>
<note confidence="0.961407">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 96–103,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.700338" genericHeader="introduction">
2 State-of-the-art
</sectionHeader>
<bodyText confidence="0.999978975609756">
The standard procedure for carrying out a human
evaluation of machine translation output is based on
the manual scoring of each sentence with two nu-
merical values between 1 and 5. The first one mea-
sures the fluency of the sentence, that is its readabil-
ity and understandability. This is a monolingual fea-
ture which does not take the source sentence into
account. The second one reflects the adequacy, that
is whether the translated sentence is a correct trans-
lation of the original sentence in the sense that the
meaning is transferred. Since humans will be the
end users of the generated output,2 it can be ex-
pected that these human-produced measures will re-
flect the usability and appropriateness of MT output
better than any automatic measure.
This kind of human evaluation has however addi-
tional problems. It is much more time consuming
than the automatic evaluation, and because it is sub-
jective, results are not reproducible, even from the
same group of evaluators. Furthermore, there can
be biases among the human judges. Large amounts
of sentences must therefore be evaluated and proce-
dures like evaluation normalization must be carried
out before significant conclusions from the evalua-
tion can be drawn. Another important drawback,
which is also one of the causes of the aforemen-
tioned problems, is that it is very difficult to define
the meaning of the numerical scores precisely. Even
if human judges have explicit evaluation guidelines
at hand, they still find it difficult to assign a numeri-
cal value which represents the quality of the transla-
tion for many sentences (Koehn and Monz, 2006).
In this paper we present an alternative to this eval-
uation scheme. Our method starts from the obser-
vation that normally the final objective of a human
evaluation is to find a “ranking” of different systems,
and the absolute score for each system is not relevant
(and it can even not be comparable between differ-
ent evaluations). We focus on a method that aims to
simplify the task of the judges and allows to rank the
systems according to their translation quality.
</bodyText>
<sectionHeader confidence="0.970617" genericHeader="method">
3 Binary System Comparisons
</sectionHeader>
<bodyText confidence="0.9987995">
The main idea of our method relies in the fact
that a human evaluator, when presented two differ-
ent translations of the same sentence, can normally
choose the best one out of them in a more or less
</bodyText>
<footnote confidence="0.9226765">
2With the exception of cross-language information retrieval
and similar tasks.
</footnote>
<bodyText confidence="0.962575">
definite way. In social sciences, a similar method
has been proposed by (Thurstone, 1927).
</bodyText>
<subsectionHeader confidence="0.999922">
3.1 Comparison of Two Systems
</subsectionHeader>
<bodyText confidence="0.999937386363636">
For the comparison of two MT systems, a set of
translated sentence pairs is selected. Each of these
pairs consists of the translations of a particular
source sentence from the two systems. The human
judge is then asked to select the “best” translation of
these two, or to mark the translations to be equally
good. We are aware that the definition of “best” here
is fuzzy. In our experiments, we made a point of not
giving the evaluators explicit guidelines on how to
decide between both translations. As a consequence,
the judges were not to make a distinction between
fluency and adequacy of the translation. This has a
two-fold purpose: on the one hand it simplifies the
decision procedure for the judges, as in most of the
cases the decision is quite natural and they do not
need to think explicitly in terms of fluency and ade-
quacy. On the other hand, one should keep in mind
that the final goal of an MT system is its usefulness
for a human user, which is why we do not want to
impose artificial constraints on the evaluation proce-
dure. If only certain quality aspects of the systems
are relevant for the ranking, for example if we want
to focus on the fluency of the translations, explicit
guidelines can be given to the judges. If the evalua-
tors are bilingual they can use the original sentences
to judge whether the information was preserved in
the translation.
After our experiment, the human judges provided
feedback on the evaluation process. We learned
that the evaluators normally selected the translation
which preserved most of the information from the
original sentence. Thus, we expect to have a slight
preference for adequacy over fluency in this evalu-
ation process. Note however that adequacy and flu-
ency have shown a high correlation3 in previous ex-
periments. This can be explained by noting that a
low fluency renders the text incomprehensible and
thus the adequacy score will also be low.
The difference in the amount of selected sen-
tences of each system is an indicator of the differ-
ence in quality between the systems. Statistics can
be carried out in order to decide whether this differ-
ence is statistically significant; we will describe this
in more detail in Section 3.4.
</bodyText>
<footnote confidence="0.995575">
3At least for “sensible” translation systems. Academic
counter-examples could easily be constructed.
</footnote>
<page confidence="0.999551">
97
</page>
<subsectionHeader confidence="0.999741">
3.2 Evaluation of Multiple Systems
</subsectionHeader>
<bodyText confidence="0.999330722222222">
We can generalize our method to find a ranking of
several systems as follows: In this setting, we have
a set of n systems. Furthermore, we have defined an
order relationship “is better than” between pairs of
these systems. Our goal now is to find an ordering
of the systems, such that each system is better than
its predecessor. In other words, this is just a sorting
problem – as widely known in computer science.
Several efficient sorting algorithms can be found
in the literature. Generally, the efficiency of sort-
ing algorithms is measured in terms of the number
of comparisons carried out. State-of-the-art sort-
ing algorithms have a worst-case running time of
0(n log n), where n is the number of elements to
sort. In our case, because such binary comparisons
are very time consuming, we want to minimize the
absolute number of comparisons needed. This mini-
mization should be carried out in the strict sense, not
just in an asymptotic manner.
(Knuth, 1973) discusses this issue in detail. It is
relatively straightforward to show that, in the worst
case, the minimum number of comparisons to be
carried out to sort n elements is at least Flog n!]
(for which n log n is an approximation). It is not
always possible to reach this minimum, however, as
was proven e.g. for the case n = 12 in (Wells, 1971)
and for n = 13 in (Peczarski, 2002). (Ford Jr and
Johnson, 1959) propose an algorithm called merge
insertion which comes very close to the theoretical
limit. This algorithm is sketched in Figure 1. There
are also algorithms with a better asymptotic runtime
(Bui and Thanh, 1985), but they only take effect for
values of n too large for our purposes (e.g., more
than 100). Thus, using the algorithm from Figure 1
we can obtain the ordering of the systems with a
(nearly) optimal number of comparisons.
</bodyText>
<subsectionHeader confidence="0.996548">
3.3 Further Considerations
</subsectionHeader>
<bodyText confidence="0.999966555555556">
In Section 3.1 we described how to carry out the
comparison between two systems when there is only
one human judge carrying out this comparison. The
comparison of systems is a very time consuming
task. Therefore it is hardly possible for one judge
to carry out the evaluation on a whole test corpus.
Usually, subsets of these test corpora are selected
for human evaluations instead. In order to obtain
a better coverage of the test corpus, but also to try
to alleviate the possible bias of a single evaluator, it
is advantageous to have several evaluators carrying
out the comparison between two systems. However,
there are two points that must be considered.
The first one is the selection of sentences each hu-
man judge should evaluate. Assume that we have al-
ready decided the amount of sentences m each eval-
uator has to work with (in our case m = 100). One
possibility is that all human judges evaluate the same
set of sentences, which presumably will cancel pos-
sible biases of the evaluators. A second possibility is
to give each judge a disjunct set of sentences. In this
way we benefit from a higher coverage of the corpus,
but do not have an explicit bias compensation.
In our experiments, we decided for a middle
course: Each evaluator receives a randomly selected
set of sentences. There are no restrictions on the se-
lection process. This implicitly produces some over-
lap while at the same time allowing for a larger set
of sentences to be evaluated. To maintain the same
conditions for each comparison, we also decided
that each human judge should evaluate the same set
of sentences for each system pair.
The other point to consider is how the evaluation
results of each of the human judges should be com-
bined into a decision for the whole system. One
possibility would be to take only a “majority vote”
among the evaluators to decide which system is the
best. By doing this, however, possible quantitative
information on the quality difference of the systems
is not taken into account. Consequently, the output is
strongly influenced by statistical fluctuations of the
data and/or of the selected set of sentences to eval-
uate. Thus, in order to combine the evaluations we
just summed over all decisions to get a total count of
sentences for each system.
</bodyText>
<subsectionHeader confidence="0.997462">
3.4 Statistical Significance
</subsectionHeader>
<bodyText confidence="0.99992075">
The evaluation of MT systems by evaluating trans-
lations of test sentences – be it automatic evaluation
or human evaluation – must always be regarded as
a statistical process: Whereas the outcome, or score
R, of an evaluation is considered to hold for “all”
possible sentences from a given domain, a test cor-
pus naturally consists of only a sample from all these
sentences. Consequently, R depends on that sam-
ple of test sentences. Furthermore, both a human
evaluation score and an automatic evaluation score
for a hypothesis sentence are by itself noisy: Hu-
man evaluation is subjective, and as such is subject
to “human noise”, as described in Section 2. Each
automatic score, on the other hand, depends heavily
on the ambiguous selection of reference translations.
Accordingly, evaluation scores underly a probability
</bodyText>
<page confidence="0.990089">
98
</page>
<listItem confidence="0.879794142857143">
1. Make pairwise comparisons of Ln/2] disjoint pairs of elements. (If n is odd, leave one element out).
2. Sort the Ln/2] larger elements found in step 1, recursively by merge insertion.
3. Name the Ln/2] elements found in step 2 a1, a2, ... , aLn/2j and the rest b1, b2, ... , brn/2], such that
a1 G a2 G · · · G aLn/2] and bi G ai for 1 G i G Ln/2]. Call b1 and the a’s the “main chain”.
4. Insert the remaining b’s into the main chain, using binary insertion, in the following order (ignore the
2k+1+(−1)k
bj such that j &gt; Ln/2]): b3, b2; b5, b4; b11, ... , b6; ... ; btk ... , btk_1+1; ... with tk = .
</listItem>
<page confidence="0.326174">
3
</page>
<figureCaption confidence="0.999968">
Figure 1: The merge insertion algorithm as presented in (Knuth, 1973).
</figureCaption>
<bodyText confidence="0.999989740740741">
distribution, and each evaluation result we achieve
must be considered as a sample from that distribu-
tion. Consequently, both human and automatic eval-
uation results must undergo statistical analysis be-
fore conclusions can be drawn from them.
A typical application of MT evaluation – for ex-
ample in the method described in this paper – is to
decide whether a given MT system X, represented
by a set of translated sentences, is significantly better
than another system Y with respect to a given eval-
uation measure. This outcome is traditionally called
the alternative hypothesis. The opposite outcome,
namely that the two systems are equal, is known
as the null hypothesis. We say that certain values
of RX, RY confirm the alternative hypothesis if the
null hypothesis can be rejected with a given level
of certainty, e.g. 95%. In the case of comparing
two MT systems, the null hypothesis would be “both
systems are equal with regard to the evaluation mea-
sure; that is, both evaluation scores R, R&apos; come from
the same distribution R0”.
As R is randomly distributed, it has an expecta-
tion E[R] and a standard error se[R]. Assuming a
normal distribution for R, we can reject the null hy-
pothesis with a confidence of 95% if the sampled
score R is more than 1.96 times the standard error
away from the null hypothesis expectation:
</bodyText>
<equation confidence="0.863317">
R significant &lt;---&gt; |E[R0] − R |&gt; 1.96 se[R0] (1)
</equation>
<bodyText confidence="0.999762538461539">
The question we have to solve is: How can we es-
timate E[R0] and se[R0]? The first step is that we
consider R and R0 to share the same standard error
se[R0] = se[R]. This value can then be estimated
from the test data. In a second step, we give an es-
timate for E[R0], either inherent in the evaluation
measure (see below), or from the estimate for the
comparison system R&apos;.
A universal estimation method is the bootstrap
estimate: The core idea is to create replications of
R by random sampling from the data set (Bisani
and Ney, 2004). Bootstrapping is generally possi-
ble for all evaluation measures. With a high number
of replicates, se[R] and E[R0] can be estimated with
satisfactory precision.
For a certain class of evaluation measures, these
parameters can be estimated more accurately and ef-
ficiently from the evaluation data without resorting
to Monte Carlo estimates. This is the class of er-
rors based on the arithmetic mean over a sentence-
wise score: In our binary comparison experiments,
each judge was given hypothesis translations ei,X,
ei,Y . She could then judge ei,X to be better than,
equal to, or worse than ei,Y . All these judgments
were counted over the systems. We define a sentence
score ri,X,Y for this evaluation method as follows:
</bodyText>
<equation confidence="0.994976333333333">
{ +1 ei,X is better than ei,Y
0 ei,X is equal to ei,Y
−1 ei,X is worse than ei,Y
</equation>
<bodyText confidence="0.998808">
Then, the total evaluation score for a binary com-
parison of systems X and Y is
</bodyText>
<equation confidence="0.9967945">
1 M
RX,Y := m i=1 ri,X,Y , (3)
</equation>
<bodyText confidence="0.999395666666667">
with m the number of evaluated sentences.
For this case, namely R being an arithmetic mean,
(Efron and Tibshirani, 1993) gives an explicit for-
mula for the estimated standard error of the score
RX,Y . To simplify the notation, we will use R in-
stead of RX,Y from now on, and ri instead of ri,X,Y .
</bodyText>
<equation confidence="0.972667">
1
se[R] =
m − 1
</equation>
<bodyText confidence="0.6681765">
With x denoting the number of sentences where
ri = 1, and y denoting the number of sentences
</bodyText>
<equation confidence="0.9661234">
ri,X,Y :=
. (2)
(ri − R)2 . (4)
d M
i=1
</equation>
<page confidence="0.940715">
99
</page>
<bodyText confidence="0.950107">
where rz = −1,
</bodyText>
<equation confidence="0.933980166666667">
x − y
R =
m
and with basic algebra
se[R] = m 1 1rx + y − (x − y)2 (6)
m
</equation>
<bodyText confidence="0.9564925">
The null hypothesis is that both systems are
Then, we should expect as many
</bodyText>
<equation confidence="0.853781666666667">
sentences where X is better than Y as vice versa,
i.e. x = y. Thus,
= 0.
</equation>
<bodyText confidence="0.8292425">
Using Equation 4, we calculate se[R] and thus a
significance range for adequacy and fluency judg-
ments. When comparing two systems X and Y ,
we assume for the null hypothesis that
se[RX] and
=
(or vice versa).
A very useful (and to our knowledge new) result
for MT evaluation is that se[R] can also be explic-
itly estimated for weighted means
as WER,
PER, and TER. These measures are defined as fol-
</bodyText>
<equation confidence="0.942037176470588">
lows: Let
i = 1, ... , m denote the number of
(edit operations) of the translation candidate ez
with regard to a reference tran
E[R0]:
“equallygood”.
E[R0]
se[R0]=
E[R0]
E[RY ]
– such
dz,
“er-
rors”
slation with length lz.
Then, the total error rate will be computed as
dz (7)
</equation>
<bodyText confidence="0.806212">
where
</bodyText>
<equation confidence="0.869395333333333">
L := XM lz (8)
z=1
– the
</equation>
<bodyText confidence="0.9831025">
dard error of R:
with weight lz
effect of leaving out a sen-
tence with length 40 is four times higher than that
of leaving out one with length 10. Consequently,
these weights must be considered when estimating
</bodyText>
<equation confidence="0.97657">
1 MOiz RJ) - \2
X • lz
(m − 1)(L − 1)
z=1
</equation>
<table confidence="0.9777654">
Vocabulary 159K 111K
Singletons 63K 46K
Test Sentences 1117
Words 26K
OOV Words 72
</table>
<tableCaption confidence="0.999613">
Table 1: Statistics of the EPPS Corpus.
</tableCaption>
<bodyText confidence="0.961262583333333">
Spanish English
Train Sentences 1.2M
Words 32M 31M
The corpus for the
language pair
consists of the official version of the speeches held in
the European Parliament Plenary Sessions (EPPS),
as available on the web page of the European Parlia-
ment. Amore detailed description of the EPPS data
can be found in (Vilar et al., 2005). Table 1 shows
the statistics of the corpus.
A total of 9 different MT systems participated in
this condition in the evaluation campaign that took
place in February 2006. We selected five representa-
tive systems for our study. Henceforth we shall refer
to these systems as System A through System E. We
restricted the number of systems in order to keep the
evaluation effort manageable for a first experimental
setup to test the feasibility of our method. The rank-
ing of 5 systems can
Spanish–English
be carried out with as few as 7
comparisons, but the ranking of 9 systems requires
19 comparisons.
</bodyText>
<sectionHeader confidence="0.886117" genericHeader="method">
5 Evaluation Results
</sectionHeader>
<equation confidence="0.465456428571429">
(5)
R :=
1
L
XM
z=1
se[R] = tu u v
</equation>
<bodyText confidence="0.936164">
d Vogel, 2004). 4http://www.tc-star.org/
Moreover, we can explicitly give an estimate for
As a result, each sentence ez affects the overall score
the stan
(9)
With this Equation, Monte-Carlo-estimates are no
longer necessary for examining the significance of
WER, PER, TER, etc. Unfortunately, we do not ex-
pect such a short explicit formula to exist for the
standard BLEU score. Still, a confidence range
for BLEU can be estimated by bootstrapping (Och,
2003; Zhang an
</bodyText>
<sectionHeader confidence="0.994721" genericHeader="method">
4 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.993492625">
The evaluation procedure was carried out on the data
generated in the second evaluation campaign of the
TC-STAR project4. The goal of this project is to
build aspeech-to-speech translation system that can
deal with real life data. Three translation directions
are dealt with in the project: Spanish to English, En-
glish to Spanish and Chinese to English. For the sys-
tem comparison we concentrated only in the English
to Spanish direction.
Seven human bilingual evaluators (6 native speakers
and one near-native speaker of Spanish) carried out
the evaluation. 100 sentences were randomly cho-
sen and assigned to each of the evaluators for every
system comparison, as discussed in Section 3.3. The
results can be seen in Table 2 an
d Figure 2. Counts
</bodyText>
<page confidence="0.903356">
100
</page>
<figure confidence="0.998834965517241">
# &amp;quot;Second system better&amp;quot;
0 10 20 30 40 50 60 70
0 10 20 30 40 50 60 70
# &amp;quot;First system better&amp;quot;
(a) Each judge.
# &amp;quot;First system better&amp;quot;
(b) All judges.
B−A
● D−C
A−C
E−A
E−B
B−D
D−A
●
●
●
●
●
●
●
0 100 200 300 400
# &amp;quot;Second system better&amp;quot;
0 100 200 300 400
E−B
A−C E−A
C
BA
B−D
</figure>
<figureCaption confidence="0.827256333333333">
Figure 2: Results of the binary comparisons. Number of times the winning system was really judged “better”
vs. number of times it was judged “worse”. Results in hatched area can not reject null hypothesis, i.e. would
be considered insignificant.
</figureCaption>
<bodyText confidence="0.998866625">
missing to 100 and 700 respectively denote “same
quality” decisions.
As can be seen from the results, in most of the
cases the judges clearly favor one of the systems.
The most notable exception is found when compar-
ing systems A and C, where a difference of only 3
sentences is clearly not enough to decide between
the two. Thus, the two bottom positions in the final
ranking could be swapped.
Figure 2(a) shows the outcome for the binary
comparisons separately for each judge, together with
an analysis of the statistical significance of the re-
sults. As can be seen, the number of samples (100)
would have been too low to show significant re-
sults in many experiments (data points in the hatched
area). In some cases, the evaluator even judged bet-
ter the system which was scored to be worse by the
majority of the other evaluators (data points above
the bisector). As Figure 2(b) shows, “the only thing
better than data is more data”: When we summarize
R over all judges, we see a significant difference
(with a confidence of 95%) at all comparisons but
two (A vs. C, and E vs. B). It is interesting to note
that exactly these two pairs do not show a significant
difference when using a majority vote strategy.
Table 3 shows also the standard evaluation met-
rics. Three BLEU scores are given in this table, the
one computed on the whole corpus, the one com-
puted on the set used for standard adequacy and flu-
ency computations and the ones on the set we se-
lected for this task5. It can be seen that the BLEU
scores are consistent across all data subsets. In this
case the ranking according to this automatic measure
matches exactly the ranking found by our method.
When comparing with the adequacy and fluency
scores, however, the ranking of the systems changes
considerably: B D E C A. However, the difference
between the three top systems is quite small. This
can be seen in Figure 3, which shows some auto-
matic and human scores for the five systems in our
experiments, along with the estimated 95% confi-
dence range. The bigger difference is found when
comparing the bottom systems, namely System A
and System C. While our method produces nearly
no difference the adequacy and fluency scores indi-
cate System C as clearly superior to System A. It is
worth noting that the both groups use quite different
translation approaches (statistical vs. rule-based).
</bodyText>
<footnote confidence="0.97786475">
5Regretfully these two last sets were not the same. This is
due to the fact that the “AF Test Set” was further used for eval-
uating Text-to-Speech systems, and thus a targeted subset of
sentences was selected.
</footnote>
<page confidence="0.993647">
101
</page>
<figure confidence="0.943986651162791">
Sys E1 E2 E3 E4 E5 E6 E7 E
A 29 19 38 17 32 29 41 205
B 40 59 48 53 63 64 45 372
C 32 22 29 23 32 34 42 214
D 39 61 59 50 64 58 46 377
A 32 31 31 31 47 38 40 250
C 37 29 32 22 39 45 43 247
A 36 28 17 28 34 37 31 211
E 41 47 44 43 53 45 58 331
B 26 29 18 24 43 36 33 209
E 34 33 28 27 32 29 43 226
B 34 28 30 31 40 41 48 252
D 23 17 23 17 24 28 38 170
A 36 14 27 9 31 30 34 181
D 34 50 40 50 57 61 57 349
Final ranking (best—worst): E B D A C
Fluency
E ●
B ●
D ●
A ●
C ●
Adequacy
E ●
B ●
D ●
A ●
C ●
1−WER
E ●
B ●
D ●
A ●
C ●
BLEU
E ●
B ●
D ●
A ●
C ●
0.3 0.4 0.5 0.6 0.7
worse &lt;− normalized score −&gt; better
Measure Ali System
</figure>
<tableCaption confidence="0.890172333333333">
Table 2: Result of the binary system comparison.
Numbers of sentences for which each system was
judged better by each evaluator (E1-E7).
</tableCaption>
<table confidence="0.999866285714286">
Subset: Whole A+F Binary
Sys BLEU BLEU A F BLEU
A 36.3 36.2 2.93 2.46 36.3
B 49.4 49.3 3.74 3.58 49.2
C 36.3 36.2 3.53 3.31 36.1
D 48.2 46.8 3.68 3.48 47.7
E 49.8 49.6 3.67 3.46 49.4
</table>
<tableCaption confidence="0.997862">
Table 3: BLEU scores and Adequacy and Fluency
</tableCaption>
<bodyText confidence="0.517215666666667">
scores for the different systems and subsets of the
whole test set. BLEU values in %, Adequacy (A)
and Fluency (F) from 1 (worst) to 5 (best).
</bodyText>
<sectionHeader confidence="0.999569" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.992452630434783">
In this section we will review the main drawbacks of
the human evaluation listed in Section 2 and analyze
how our approach deals with them. The first one
was the use of explicit numerical scores, which are
difficult to define exactly. Our system was mainly
designed for the elimination of this issue.
Our evaluation continues to be time consuming.
Even more, the number of individual comparisons
needed is in the order of log(n!), in contrast with the
standard adequacy-fluency evaluation which needs
2n individual evaluations (two evaluations per sys-
tem, one for fluency, another one for adequacy). For
n in the range of 1 up to 20 (a realistic number of
systems for current evaluation campaigns) these two
quantities are comparable. And actually each of our
Figure 3: Normalized evaluation scores. Higher
scores are better. Solid lines show the 95% con-
fidence range. Automatic scores calculated on the
whole test set, human scores on the A+F subset.
evaluations should be simpler than the standard ad-
equacy and fluency ones. Therefore the time needed
for both evaluation procedures is probably similar.
Reproducibility of the evaluation is also an impor-
tant concern. We computed the number of “errors”
in the evaluation process, i.e. the number of sen-
tences evaluated by two or more evaluators where
the evaluators’ judgement was different. Only in
10% of the cases the evaluation was contradictory,
in the sense that one evaluator chose one sentence as
better than the other, while the other evaluator chose
the other one. In 30% of the cases, however, one
evaluator estimated both sentences to be of the same
quality while the other judged one sentence as supe-
rior to the other one. As comparison, for the fluency-
adequacy judgement nearly one third of the com-
mon evaluations have a difference in score greater or
equal than two (where the maximum would be four),
and another third a score difference of one point6.
With respect to biases, we feel that it is almost im-
possible to eliminate them if humans are involved. If
one of the judges prefers one kind of structure, there
will a bias for a system producing such output, in-
dependently of the evaluation procedure. However,
the suppression of explicit numerical scores elimi-
nates an additional bias of evaluators. It has been
observed that human judges often give scores within
</bodyText>
<footnote confidence="0.9538945">
6Note however that possible evaluator biases can have a
great influence in these statistics.
</footnote>
<page confidence="0.997315">
102
</page>
<bodyText confidence="0.999720083333333">
a certain range (e.g. in the mid-range or only ex-
treme values), which constitute an additional diffi-
culty when carrying out the evaluation (Leusch et
al., 2005). Our method suppresses this kind of bias.
Another advantage of our method is the possibil-
ity of assessing improvements within one system.
With one evaluation we can decide if some modi-
fications actually improve performance. This eval-
uation even gives us a confidence interval to weight
the significance of an improvement. Carrying out
a full adequacy-fluency analysis would require a lot
more effort, without giving more useful results.
</bodyText>
<sectionHeader confidence="0.998837" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999989434782609">
We presented a novel human evaluation technique
that simplifies the task of the evaluators. Our method
relies on two basic observations. The first one is that
in most evaluations the final goal is to find a ranking
of different systems, the absolute scores are usually
not so relevant. Especially when considering human
evaluation, the scores are not even comparable be-
tween two evaluation campaigns. The second one
is the fact that a human judge can normally choose
the best one out of two translations, and this is a
much easier process than the assessment of numeri-
cal scores whose definition is not at all clear. Taking
this into consideration we suggested a method that
aims at finding a ranking of different MT systems
based on the comparison of pairs of translation can-
didates for a set of sentences to be evaluated.
A detailed analysis of the statistical significance
of the method is presented and also applied to some
wide-spread automatic measures. The evaluation
methodology was applied for the ranking of 5 sys-
tems that participated in the second evaluation cam-
paign of the TC-STAR project and comparison with
standard evaluation measures was performed.
</bodyText>
<sectionHeader confidence="0.998313" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999232">
We would like to thank the human judges who par-
ticipated in the evaluation. This work has been
funded by the integrated project TC-STAR– Tech-
nology and Corpora for Speech-to-Speech Transla-
tion – (IST-2002-FP6-506738).
</bodyText>
<sectionHeader confidence="0.999274" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887174603175">
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluationx.
IEEE ICASSP, pages 409–412, Montreal, Canada,
May.
T. Bui and M. Thanh. 1985. Significant improvements to
the Ford-Johnson algorithm for sorting. BIT Numeri-
cal Mathematics, 25(1):70–75.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. Proceeding of the 11th Conference of the Eu-
ropean Chapter of the ACL: EACL 2006, pages 249–
256, Trento, Italy, Apr.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. Proc. ARPA Workshop on Human Language
Technology.
B. Efron and R. J. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman &amp; Hall, New York and
London.
L. Ford Jr and S. Johnson. 1959. A Tournament Problem.
The American Mathematical Monthly, 66(5):387–389.
D. E. Knuth. 1973. The Art of Computer Programming,
volume 3. Addison-Wesley, 1st edition. Sorting and
Searching.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. Proceedings of the Workshop on Statisti-
cal Machine Translation, pages 102–121, New York
City, Jun.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005.
Preprocessing and normalization for automatic evalu-
ation of machine translation. 43rd ACL: Proc. Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization, pages 17–24, Ann Ar-
bor, Michigan, Jun.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. Proc. of the 41st ACL, pages
160–167, Sapporo, Japan, Jul.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. Proc. of the 40th ACL, pages 311–318,
Philadelphia, PA, Jul.
M. Peczarski. 2002. Sorting 13 elements requires 34
comparisons. LNCS, 2461/2002:785–794, Sep.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-
ulla, and R. Weischedel. 2005. A study of translation
error rate with targeted human annotation. Technical
Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-
2005-58, University of Maryland, College Park, MD.
L. Thurstone. 1927. The method of paired comparisons
for social values. Journal ofAbnormal and Social Psy-
chology, 21:384–400.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical Machine Translation of European
Parliamentary Speeches. Proceedings of MT Summit
X, pages 259–266, Phuket, Thailand, Sep.
M. Wells. 1971. Elements of combinatorial computing.
Pergamon Press.
Y. Zhang and S. Vogel. 2004. Measuring confidence
intervals for the machine translation evaluation met-
rics. Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation, pages 4–6, Baltimore, MD.
</reference>
<page confidence="0.999298">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270300">
<title confidence="0.9996565">Human Evaluation of Machine Translation Through Binary System Comparisons</title>
<author confidence="0.997343">David Vilar</author>
<author confidence="0.997343">Gregor</author>
<author confidence="0.997343">Hermann Ney</author>
<affiliation confidence="0.754766">Lehrstuhl f¨ur Informatik 6 RWTH Aachen University</affiliation>
<address confidence="0.999788">D-52056 Aachen, Germany</address>
<author confidence="0.998828">Rafael E Banchs</author>
<affiliation confidence="0.997127">D. of Signal Theory and Communications Universitat Polit`ecnica de Catalunya</affiliation>
<address confidence="0.999578">08034 Barcelona, Spain</address>
<email confidence="0.910382">rbanchs@gps.tsc.upc.edu.de</email>
<abstract confidence="0.9861402">We introduce a novel evaluation scheme for the human evaluation of different machine translation systems. Our method is based on direct comparison of two sentences at a time by human judges. These binary judgments are then used to decide between all possible rankings of the systems. The advantages of this new method are the lower dependency on extensive evaluation guidelines, and a tighter focus on a typical evaluation task, namely the ranking of systems. Furthermore we argue that machine translation evaluations should be regarded as statistical processes, both for human and automatic evaluation. We show how confidence ranges for state-of-the-art evaluation measures such as WER and TER can be computed accurately and efficiently without having to resort to Monte Carlo estimates. We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bisani</author>
<author>H Ney</author>
</authors>
<title>Bootstrap estimates for confidence intervals in ASR performance evaluationx.</title>
<date>2004</date>
<journal>IEEE ICASSP,</journal>
<pages>409--412</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="16084" citStr="Bisani and Ney, 2004" startWordPosition="2735" endWordPosition="2738">rom the null hypothesis expectation: R significant &lt;---&gt; |E[R0] − R |&gt; 1.96 se[R0] (1) The question we have to solve is: How can we estimate E[R0] and se[R0]? The first step is that we consider R and R0 to share the same standard error se[R0] = se[R]. This value can then be estimated from the test data. In a second step, we give an estimate for E[R0], either inherent in the evaluation measure (see below), or from the estimate for the comparison system R&apos;. A universal estimation method is the bootstrap estimate: The core idea is to create replications of R by random sampling from the data set (Bisani and Ney, 2004). Bootstrapping is generally possible for all evaluation measures. With a high number of replicates, se[R] and E[R0] can be estimated with satisfactory precision. For a certain class of evaluation measures, these parameters can be estimated more accurately and efficiently from the evaluation data without resorting to Monte Carlo estimates. This is the class of errors based on the arithmetic mean over a sentencewise score: In our binary comparison experiments, each judge was given hypothesis translations ei,X, ei,Y . She could then judge ei,X to be better than, equal to, or worse than ei,Y . Al</context>
</contexts>
<marker>Bisani, Ney, 2004</marker>
<rawString>M. Bisani and H. Ney. 2004. Bootstrap estimates for confidence intervals in ASR performance evaluationx. IEEE ICASSP, pages 409–412, Montreal, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bui</author>
<author>M Thanh</author>
</authors>
<title>Significant improvements to the Ford-Johnson algorithm for sorting.</title>
<date>1985</date>
<journal>BIT Numerical Mathematics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="10131" citStr="Bui and Thanh, 1985" startWordPosition="1687" endWordPosition="1690">cusses this issue in detail. It is relatively straightforward to show that, in the worst case, the minimum number of comparisons to be carried out to sort n elements is at least Flog n!] (for which n log n is an approximation). It is not always possible to reach this minimum, however, as was proven e.g. for the case n = 12 in (Wells, 1971) and for n = 13 in (Peczarski, 2002). (Ford Jr and Johnson, 1959) propose an algorithm called merge insertion which comes very close to the theoretical limit. This algorithm is sketched in Figure 1. There are also algorithms with a better asymptotic runtime (Bui and Thanh, 1985), but they only take effect for values of n too large for our purposes (e.g., more than 100). Thus, using the algorithm from Figure 1 we can obtain the ordering of the systems with a (nearly) optimal number of comparisons. 3.3 Further Considerations In Section 3.1 we described how to carry out the comparison between two systems when there is only one human judge carrying out this comparison. The comparison of systems is a very time consuming task. Therefore it is hardly possible for one judge to carry out the evaluation on a whole test corpus. Usually, subsets of these test corpora are selecte</context>
</contexts>
<marker>Bui, Thanh, 1985</marker>
<rawString>T. Bui and M. Thanh. 1985. Significant improvements to the Ford-Johnson algorithm for sorting. BIT Numerical Mathematics, 25(1):70–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Reevaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>Proceeding of the 11th Conference of the European Chapter of the ACL: EACL</booktitle>
<pages>249--256</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2291" citStr="Callison-Burch et al., 2006" startWordPosition="355" endWordPosition="358">ror Rate (PER), the NIST score (Doddington, 2002) and, especially in recent years, the BLEU score (Papineni et al., 2002) and the Translation Error Rate (TER) (Snover et al., 2005). All of theses measures compare the system output with one or more gold standard references and produce a numerical value (score or error rate) which measures the similarity between the machine translation and a human produced one. Once such reference translations are available, the evaluation can be carried out in a quick, efficient and reproducible manner. However, automatic measures also have big disadvantages; (Callison-Burch et al., 2006) describes some of them. A major problem is that a given sentence in one language can have several correct translations in another language and thus, the measure of similarity with one or even a small amount of reference translations will never be flexible enough to truly reflect the wide range of correct possibilities of a translation. 1 This holds in particular for long sentences and wide- or open-domain tasks like the ones dealt with in current MT projects and evaluations. If the actual quality of a translation in terms of usefulness for human users is to be evaluated, human evaluation need</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Reevaluating the role of BLEU in machine translation research. Proceeding of the 11th Conference of the European Chapter of the ACL: EACL 2006, pages 249– 256, Trento, Italy, Apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>Proc. ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="1712" citStr="Doddington, 2002" startWordPosition="262" endWordPosition="264">rately and efficiently without having to resort to Monte Carlo estimates. We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign. 1 Introduction Evaluation of machine translation (MT) output is a difficult and still open problem. As in other natural language processing tasks, automatic measures which try to asses the quality of the translation can be computed. The most widely known are the Word Error Rate (WER), the Position independent word Error Rate (PER), the NIST score (Doddington, 2002) and, especially in recent years, the BLEU score (Papineni et al., 2002) and the Translation Error Rate (TER) (Snover et al., 2005). All of theses measures compare the system output with one or more gold standard references and produce a numerical value (score or error rate) which measures the similarity between the machine translation and a human produced one. Once such reference translations are available, the evaluation can be carried out in a quick, efficient and reproducible manner. However, automatic measures also have big disadvantages; (Callison-Burch et al., 2006) describes some of th</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. Proc. ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall,</publisher>
<location>New York and London.</location>
<contexts>
<context position="17117" citStr="Efron and Tibshirani, 1993" startWordPosition="2917" endWordPosition="2920">sentencewise score: In our binary comparison experiments, each judge was given hypothesis translations ei,X, ei,Y . She could then judge ei,X to be better than, equal to, or worse than ei,Y . All these judgments were counted over the systems. We define a sentence score ri,X,Y for this evaluation method as follows: { +1 ei,X is better than ei,Y 0 ei,X is equal to ei,Y −1 ei,X is worse than ei,Y Then, the total evaluation score for a binary comparison of systems X and Y is 1 M RX,Y := m i=1 ri,X,Y , (3) with m the number of evaluated sentences. For this case, namely R being an arithmetic mean, (Efron and Tibshirani, 1993) gives an explicit formula for the estimated standard error of the score RX,Y . To simplify the notation, we will use R instead of RX,Y from now on, and ri instead of ri,X,Y . 1 se[R] = m − 1 With x denoting the number of sentences where ri = 1, and y denoting the number of sentences ri,X,Y := . (2) (ri − R)2 . (4) d M i=1 99 where rz = −1, x − y R = m and with basic algebra se[R] = m 1 1rx + y − (x − y)2 (6) m The null hypothesis is that both systems are Then, we should expect as many sentences where X is better than Y as vice versa, i.e. x = y. Thus, = 0. Using Equation 4, we calculate se[R]</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman &amp; Hall, New York and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ford Jr</author>
<author>S Johnson</author>
</authors>
<title>A Tournament Problem.</title>
<date>1959</date>
<journal>The American Mathematical Monthly,</journal>
<volume>66</volume>
<issue>5</issue>
<contexts>
<context position="9917" citStr="Jr and Johnson, 1959" startWordPosition="1653" endWordPosition="1656">ry comparisons are very time consuming, we want to minimize the absolute number of comparisons needed. This minimization should be carried out in the strict sense, not just in an asymptotic manner. (Knuth, 1973) discusses this issue in detail. It is relatively straightforward to show that, in the worst case, the minimum number of comparisons to be carried out to sort n elements is at least Flog n!] (for which n log n is an approximation). It is not always possible to reach this minimum, however, as was proven e.g. for the case n = 12 in (Wells, 1971) and for n = 13 in (Peczarski, 2002). (Ford Jr and Johnson, 1959) propose an algorithm called merge insertion which comes very close to the theoretical limit. This algorithm is sketched in Figure 1. There are also algorithms with a better asymptotic runtime (Bui and Thanh, 1985), but they only take effect for values of n too large for our purposes (e.g., more than 100). Thus, using the algorithm from Figure 1 we can obtain the ordering of the systems with a (nearly) optimal number of comparisons. 3.3 Further Considerations In Section 3.1 we described how to carry out the comparison between two systems when there is only one human judge carrying out this com</context>
</contexts>
<marker>Jr, Johnson, 1959</marker>
<rawString>L. Ford Jr and S. Johnson. 1959. A Tournament Problem. The American Mathematical Monthly, 66(5):387–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>The Art of Computer Programming,</title>
<date>1973</date>
<volume>3</volume>
<publisher>Addison-Wesley,</publisher>
<contexts>
<context position="9507" citStr="Knuth, 1973" startWordPosition="1577" endWordPosition="1578"> is just a sorting problem – as widely known in computer science. Several efficient sorting algorithms can be found in the literature. Generally, the efficiency of sorting algorithms is measured in terms of the number of comparisons carried out. State-of-the-art sorting algorithms have a worst-case running time of 0(n log n), where n is the number of elements to sort. In our case, because such binary comparisons are very time consuming, we want to minimize the absolute number of comparisons needed. This minimization should be carried out in the strict sense, not just in an asymptotic manner. (Knuth, 1973) discusses this issue in detail. It is relatively straightforward to show that, in the worst case, the minimum number of comparisons to be carried out to sort n elements is at least Flog n!] (for which n log n is an approximation). It is not always possible to reach this minimum, however, as was proven e.g. for the case n = 12 in (Wells, 1971) and for n = 13 in (Peczarski, 2002). (Ford Jr and Johnson, 1959) propose an algorithm called merge insertion which comes very close to the theoretical limit. This algorithm is sketched in Figure 1. There are also algorithms with a better asymptotic runti</context>
<context position="14173" citStr="Knuth, 1973" startWordPosition="2400" endWordPosition="2401"> (If n is odd, leave one element out). 2. Sort the Ln/2] larger elements found in step 1, recursively by merge insertion. 3. Name the Ln/2] elements found in step 2 a1, a2, ... , aLn/2j and the rest b1, b2, ... , brn/2], such that a1 G a2 G · · · G aLn/2] and bi G ai for 1 G i G Ln/2]. Call b1 and the a’s the “main chain”. 4. Insert the remaining b’s into the main chain, using binary insertion, in the following order (ignore the 2k+1+(−1)k bj such that j &gt; Ln/2]): b3, b2; b5, b4; b11, ... , b6; ... ; btk ... , btk_1+1; ... with tk = . 3 Figure 1: The merge insertion algorithm as presented in (Knuth, 1973). distribution, and each evaluation result we achieve must be considered as a sample from that distribution. Consequently, both human and automatic evaluation results must undergo statistical analysis before conclusions can be drawn from them. A typical application of MT evaluation – for example in the method described in this paper – is to decide whether a given MT system X, represented by a set of translated sentences, is significantly better than another system Y with respect to a given evaluation measure. This outcome is traditionally called the alternative hypothesis. The opposite outcome</context>
</contexts>
<marker>Knuth, 1973</marker>
<rawString>D. E. Knuth. 1973. The Art of Computer Programming, volume 3. Addison-Wesley, 1st edition. Sorting and Searching.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<location>New York City,</location>
<contexts>
<context position="5314" citStr="Koehn and Monz, 2006" startWordPosition="858" endWordPosition="861">, there can be biases among the human judges. Large amounts of sentences must therefore be evaluated and procedures like evaluation normalization must be carried out before significant conclusions from the evaluation can be drawn. Another important drawback, which is also one of the causes of the aforementioned problems, is that it is very difficult to define the meaning of the numerical scores precisely. Even if human judges have explicit evaluation guidelines at hand, they still find it difficult to assign a numerical value which represents the quality of the translation for many sentences (Koehn and Monz, 2006). In this paper we present an alternative to this evaluation scheme. Our method starts from the observation that normally the final objective of a human evaluation is to find a “ranking” of different systems, and the absolute score for each system is not relevant (and it can even not be comparable between different evaluations). We focus on a method that aims to simplify the task of the judges and allows to rank the systems according to their translation quality. 3 Binary System Comparisons The main idea of our method relies in the fact that a human evaluator, when presented two different tran</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>P. Koehn and C. Monz. 2006. Manual and automatic evaluation of machine translation between european languages. Proceedings of the Workshop on Statistical Machine Translation, pages 102–121, New York City, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>N Ueffing</author>
<author>D Vilar</author>
<author>H Ney</author>
</authors>
<title>Preprocessing and normalization for automatic evaluation of machine translation. 43rd ACL:</title>
<date>2005</date>
<booktitle>Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>17--24</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="27808" citStr="Leusch et al., 2005" startWordPosition="4925" endWordPosition="4928">possible to eliminate them if humans are involved. If one of the judges prefers one kind of structure, there will a bias for a system producing such output, independently of the evaluation procedure. However, the suppression of explicit numerical scores eliminates an additional bias of evaluators. It has been observed that human judges often give scores within 6Note however that possible evaluator biases can have a great influence in these statistics. 102 a certain range (e.g. in the mid-range or only extreme values), which constitute an additional difficulty when carrying out the evaluation (Leusch et al., 2005). Our method suppresses this kind of bias. Another advantage of our method is the possibility of assessing improvements within one system. With one evaluation we can decide if some modifications actually improve performance. This evaluation even gives us a confidence interval to weight the significance of an improvement. Carrying out a full adequacy-fluency analysis would require a lot more effort, without giving more useful results. 7 Conclusion We presented a novel human evaluation technique that simplifies the task of the evaluators. Our method relies on two basic observations. The first on</context>
</contexts>
<marker>Leusch, Ueffing, Vilar, Ney, 2005</marker>
<rawString>G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005. Preprocessing and normalization for automatic evaluation of machine translation. 43rd ACL: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 17–24, Ann Arbor, Michigan, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>Proc. of the 41st ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="20198" citStr="Och, 2003" startWordPosition="3504" endWordPosition="3505">arried out with as few as 7 comparisons, but the ranking of 9 systems requires 19 comparisons. 5 Evaluation Results (5) R := 1 L XM z=1 se[R] = tu u v d Vogel, 2004). 4http://www.tc-star.org/ Moreover, we can explicitly give an estimate for As a result, each sentence ez affects the overall score the stan (9) With this Equation, Monte-Carlo-estimates are no longer necessary for examining the significance of WER, PER, TER, etc. Unfortunately, we do not expect such a short explicit formula to exist for the standard BLEU score. Still, a confidence range for BLEU can be estimated by bootstrapping (Och, 2003; Zhang an 4 Evaluation Setup The evaluation procedure was carried out on the data generated in the second evaluation campaign of the TC-STAR project4. The goal of this project is to build aspeech-to-speech translation system that can deal with real life data. Three translation directions are dealt with in the project: Spanish to English, English to Spanish and Chinese to English. For the system comparison we concentrated only in the English to Spanish direction. Seven human bilingual evaluators (6 native speakers and one near-native speaker of Spanish) carried out the evaluation. 100 sentence</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. Proc. of the 41st ACL, pages 160–167, Sapporo, Japan, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>Proc. of the 40th ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1784" citStr="Papineni et al., 2002" startWordPosition="273" endWordPosition="276">mates. We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign. 1 Introduction Evaluation of machine translation (MT) output is a difficult and still open problem. As in other natural language processing tasks, automatic measures which try to asses the quality of the translation can be computed. The most widely known are the Word Error Rate (WER), the Position independent word Error Rate (PER), the NIST score (Doddington, 2002) and, especially in recent years, the BLEU score (Papineni et al., 2002) and the Translation Error Rate (TER) (Snover et al., 2005). All of theses measures compare the system output with one or more gold standard references and produce a numerical value (score or error rate) which measures the similarity between the machine translation and a human produced one. Once such reference translations are available, the evaluation can be carried out in a quick, efficient and reproducible manner. However, automatic measures also have big disadvantages; (Callison-Burch et al., 2006) describes some of them. A major problem is that a given sentence in one language can have se</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. Proc. of the 40th ACL, pages 311–318, Philadelphia, PA, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Peczarski</author>
</authors>
<title>Sorting 13 elements requires 34 comparisons.</title>
<date>2002</date>
<journal>LNCS,</journal>
<pages>2461--2002</pages>
<contexts>
<context position="9888" citStr="Peczarski, 2002" startWordPosition="1650" endWordPosition="1651"> case, because such binary comparisons are very time consuming, we want to minimize the absolute number of comparisons needed. This minimization should be carried out in the strict sense, not just in an asymptotic manner. (Knuth, 1973) discusses this issue in detail. It is relatively straightforward to show that, in the worst case, the minimum number of comparisons to be carried out to sort n elements is at least Flog n!] (for which n log n is an approximation). It is not always possible to reach this minimum, however, as was proven e.g. for the case n = 12 in (Wells, 1971) and for n = 13 in (Peczarski, 2002). (Ford Jr and Johnson, 1959) propose an algorithm called merge insertion which comes very close to the theoretical limit. This algorithm is sketched in Figure 1. There are also algorithms with a better asymptotic runtime (Bui and Thanh, 1985), but they only take effect for values of n too large for our purposes (e.g., more than 100). Thus, using the algorithm from Figure 1 we can obtain the ordering of the systems with a (nearly) optimal number of comparisons. 3.3 Further Considerations In Section 3.1 we described how to carry out the comparison between two systems when there is only one huma</context>
</contexts>
<marker>Peczarski, 2002</marker>
<rawString>M. Peczarski. 2002. Sorting 13 elements requires 34 comparisons. LNCS, 2461/2002:785–794, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B J Dorr</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
<author>L Micciulla</author>
<author>R Weischedel</author>
</authors>
<title>A study of translation error rate with targeted human annotation.</title>
<date>2005</date>
<tech>Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR2005-58,</tech>
<institution>University of Maryland, College Park, MD.</institution>
<contexts>
<context position="1843" citStr="Snover et al., 2005" startWordPosition="284" endWordPosition="287">ll as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign. 1 Introduction Evaluation of machine translation (MT) output is a difficult and still open problem. As in other natural language processing tasks, automatic measures which try to asses the quality of the translation can be computed. The most widely known are the Word Error Rate (WER), the Position independent word Error Rate (PER), the NIST score (Doddington, 2002) and, especially in recent years, the BLEU score (Papineni et al., 2002) and the Translation Error Rate (TER) (Snover et al., 2005). All of theses measures compare the system output with one or more gold standard references and produce a numerical value (score or error rate) which measures the similarity between the machine translation and a human produced one. Once such reference translations are available, the evaluation can be carried out in a quick, efficient and reproducible manner. However, automatic measures also have big disadvantages; (Callison-Burch et al., 2006) describes some of them. A major problem is that a given sentence in one language can have several correct translations in another language and thus, th</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Makhoul, Micciulla, Weischedel, 2005</marker>
<rawString>M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micciulla, and R. Weischedel. 2005. A study of translation error rate with targeted human annotation. Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR2005-58, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Thurstone</author>
</authors>
<title>The method of paired comparisons for social values.</title>
<date>1927</date>
<journal>Journal ofAbnormal and Social Psychology,</journal>
<pages>21--384</pages>
<contexts>
<context position="6176" citStr="Thurstone, 1927" startWordPosition="1008" endWordPosition="1009">m is not relevant (and it can even not be comparable between different evaluations). We focus on a method that aims to simplify the task of the judges and allows to rank the systems according to their translation quality. 3 Binary System Comparisons The main idea of our method relies in the fact that a human evaluator, when presented two different translations of the same sentence, can normally choose the best one out of them in a more or less 2With the exception of cross-language information retrieval and similar tasks. definite way. In social sciences, a similar method has been proposed by (Thurstone, 1927). 3.1 Comparison of Two Systems For the comparison of two MT systems, a set of translated sentence pairs is selected. Each of these pairs consists of the translations of a particular source sentence from the two systems. The human judge is then asked to select the “best” translation of these two, or to mark the translations to be equally good. We are aware that the definition of “best” here is fuzzy. In our experiments, we made a point of not giving the evaluators explicit guidelines on how to decide between both translations. As a consequence, the judges were not to make a distinction between</context>
</contexts>
<marker>Thurstone, 1927</marker>
<rawString>L. Thurstone. 1927. The method of paired comparisons for social values. Journal ofAbnormal and Social Psychology, 21:384–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vilar</author>
<author>E Matusov</author>
<author>S Hasan</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Statistical Machine Translation of European Parliamentary Speeches.</title>
<date>2005</date>
<booktitle>Proceedings of MT Summit X,</booktitle>
<pages>259--266</pages>
<publisher>Pergamon Press.</publisher>
<location>Phuket, Thailand,</location>
<contexts>
<context position="19084" citStr="Vilar et al., 2005" startWordPosition="3311" endWordPosition="3314"> four times higher than that of leaving out one with length 10. Consequently, these weights must be considered when estimating 1 MOiz RJ) - \2 X • lz (m − 1)(L − 1) z=1 Vocabulary 159K 111K Singletons 63K 46K Test Sentences 1117 Words 26K OOV Words 72 Table 1: Statistics of the EPPS Corpus. Spanish English Train Sentences 1.2M Words 32M 31M The corpus for the language pair consists of the official version of the speeches held in the European Parliament Plenary Sessions (EPPS), as available on the web page of the European Parliament. Amore detailed description of the EPPS data can be found in (Vilar et al., 2005). Table 1 shows the statistics of the corpus. A total of 9 different MT systems participated in this condition in the evaluation campaign that took place in February 2006. We selected five representative systems for our study. Henceforth we shall refer to these systems as System A through System E. We restricted the number of systems in order to keep the evaluation effort manageable for a first experimental setup to test the feasibility of our method. The ranking of 5 systems can Spanish–English be carried out with as few as 7 comparisons, but the ranking of 9 systems requires 19 comparisons. </context>
</contexts>
<marker>Vilar, Matusov, Hasan, Zens, Ney, 2005</marker>
<rawString>D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney. 2005. Statistical Machine Translation of European Parliamentary Speeches. Proceedings of MT Summit X, pages 259–266, Phuket, Thailand, Sep. M. Wells. 1971. Elements of combinatorial computing. Pergamon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
</authors>
<title>Measuring confidence intervals for the machine translation evaluation metrics.</title>
<date>2004</date>
<booktitle>Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>4--6</pages>
<location>Baltimore, MD.</location>
<marker>Zhang, Vogel, 2004</marker>
<rawString>Y. Zhang and S. Vogel. 2004. Measuring confidence intervals for the machine translation evaluation metrics. Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 4–6, Baltimore, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>