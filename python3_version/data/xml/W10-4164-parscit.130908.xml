<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000405">
<title confidence="0.990807">
Chinese Word Sense Induction with Basic Clustering Algorithms
</title>
<author confidence="0.999823">
Yuxiang Jia1,2, Shiwen Yu1, Zhengyan Chen3
</author>
<affiliation confidence="0.999708">
1Key Laboratory of Computational Linguistics, Ministry of Education, China
2College of Information and Engineering, Zhengzhou University, Zhengzhou, China
3Department of Information Technology, Henan Institute of Education, Zhengzhou, China
</affiliation>
<email confidence="0.998317">
{yxjia,yusw}@pku.edu.cn chenzhengyan1981@163.com
</email>
<sectionHeader confidence="0.998552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996016833333333">
Word Sense Induction (WSI) is an
important topic in natural langage
processing area. For the bakeoff task
Chinese Word Sense Induction (CWSI),
this paper proposes two systems using
basic clustering algorithms, k-means and
agglomerative clustering. Experimental
results show that k-means achieves a
better performance. Based only on the
data provided by the task organizers, the
two systems get FScores of 0.7812 and
0.7651 respectively.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999326877192983">
Word Sense Induction (WSI) or Word Sense
Discrimination is a task of automatically discov-
ering word senses from un-annotated text. It is
distinct from Word Sense Disambiguation
(WSD) where the senses are assumed to be
known and the aim is to decide the right mean-
ing of the target word in context. WSD generally
requires the use of large-scale manually anno-
tated lexical resources, while WSI can overcome
this limitation. Furthermore, automatically in-
duced word senses can improve performance on
many natural language processing tasks such as
information retrieval (Uzuner et al., 1999), in-
formation extraction (Chai and Biermann, 1999)
and machine translation (Vickrey et al., 2005).
WSI is typically treated as a clustering prob-
lem. The input is instances of the ambiguous
word with their accompanying contexts and the
output is a grouping of these instances into
classes corresponding to the induced senses. In
other words, contexts that are grouped together
in the same class represent a specific word sense.
The task can be formally defined as a two
stage process, feature selection and word cluster-
ing. The first stage determines which context
features to consider when comparing similarity
between words, while the second stage apply
some process that clusters similar words using
the selected features. So the simplest approaches
to WSI involve the use of basic word co-
occurrence features and application of classical
clustering algorithms, more sophisticated tech-
niques improve performance by introducing new
context features, novel clustering algorithms, or
both. (Denkowski, 2009) makes a comprehen-
sive survey of techniques for unsupervised word
sense induction.
Two tasks on English Word Sense Induction
were held on SemEval2007 (Agirre and Soroa,
2007) and SemEval2010 (Manandhar and Kla-
paftis, 2010) respectively, which greatly pro-
mote the research of English WSI.
However, the study on Chinese Word Sense
Induction (CWSI) is inadequate (Zhu, 2009),
and Chinese word senses have their own charac-
teristics. The methods that work well in English
may not work well in Chinese. So, as an explo-
ration, this paper proposes simple approaches
utilizing basic features and basic clustering algo-
rithms, such as partitional method k-means and
hierarchical agglomerative method.
The rest of this paper is organized as follows.
Section 2 briefly introduces the basic clustering
algorithms. Section 3 describes the feature set.
Section 4 gives experimental details and analysis.
Conclusions and future work are given in Sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.965949" genericHeader="method">
2 Clustering Algorithms
</sectionHeader>
<bodyText confidence="0.999756705882353">
Partitional clustering and hierarchical clustering
are the two basic types of clustering algorithms.
Partitional clustering partitions a given dataset
into a set of clusters without any explicit
structure, while hierarchical clustering creates a
hierarchy of clusters.
The k-means algorithm is the most notable
partitional clustering method. It takes a simple
two step iterative process, data assignment and
relocation of means, to divide the dataset into a
specified number of clusters, k.
Hierarchical clustering algorithms are either
top-down or bottom-up. Bottom-up algorithms
treat each instance as a singleton cluster at the
beginning and then successively merge pairs of
clusters until all clusters have been merged into
a single cluster. Bottom-up clustering is also
called hierarchical agglomerative clustering,
which is more popular than top-down clustering.
We use k-means and agglomerative algo-
rithms for the CWSI task, and compare the per-
formances of the two algorithms.
Estimating the number of the induced clusters,
k, is difficult for general clustering problems.
But in CWSI, it is simplified because the sense
number of the target word is given beforehand.
CLUTO (Karypis, 2003), a clustering toolkit,
is used for implementation. The similarity be-
tween objects is computed using cosine function.
The criterion functions for k-means and agglom-
erative algorithms are I2 and UPGMA respec-
tively. Biased agglomerative approach is chosen
in stead of the traditional agglomerative ap-
proach.
</bodyText>
<sectionHeader confidence="0.986192" genericHeader="method">
3 Feature Set
</sectionHeader>
<bodyText confidence="0.998811">
For each target word, instances are extracted
from the XML data file. Then the encoding of
the instance file is transformed from UTF-8 to
GB2312. Word segmentation and part-of-speech
tagging is finished with the tool ICTCLAS 1.
Then the following three types of features are
extracted:
</bodyText>
<listItem confidence="0.970249">
1. The part-of-speech of the target word
2. Words before and after the target word
within window of size 3 with position informa-
tion
3. Unordered single words in all the contex-
tual sentences without the target word, punctua-
tions and symbols of the part-of-speech “nx”
(Each word is only counted once, which is dif-
</listItem>
<footnote confidence="0.90874">
1http://ictclas.org/
</footnote>
<bodyText confidence="0.935355411764706">
ferent from the word frequency in the bag-of-
words model)
The target word is not necessarily a seg-
mented word. Their relations are as follows:
1. The target word is a segmented word.
E.g. 别/d 打/v 我/r 电话/n
Don’t dial my phone.
The target word is “打” (dial) and the seg-
mented word is also “打” (dial). So they match.
2. The target word is inside of a segmented
word.
E.g.同/p 媒体/n 打交道/v
deal with media
The target word is “打” (deal), but the seg-
mented word is “打交道” (deal with). Then we
split the segmented word and specify the part-of-
speech of the target word as “1”.
</bodyText>
<listItem confidence="0.540359">
3. The target word is the combination of two
segmented words.
</listItem>
<bodyText confidence="0.992251368421053">
E.g. 发/v 动/v “/w 文化大革命/nz ”/w
launching the “Culture Revolution”
The target word is “发动” (launching), but it
is split into two segmented words “发” (start)
and “动” (move). Then we combine the two
segmented words and specify the part-of-speech
of the target word as “2”.
4. The target word is split into two segmented
words.
E.g. 刮/v 起/v 了/u 东/j 北风/n
blow up northeast wind
The target word is “东北”, but it is segmented
into two words “东” (east) and “北风” (north
wind). In this case, we specify the postion of
first segmented word as the position of the target
word and the part-of-speech of the target word
as “3”.
If the target word occurs more than once in an
instance, we consider the first occurrence.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.979273">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9976964">
Two data sets are provided. The trial set contains
50 target words and 50 examples for each target
word. The test set consists of 100 new target
word and 50 examples for each target word.
Both data sets are collected from the internet.
Table 1 shows the distribution of sense num-
bers of the target words in the two data sets. We
can see that two sense words dominate and three
sense words are the second majority. The word
“打” (beat) in the trial set has 21 senses.
</bodyText>
<tableCaption confidence="0.997162">
Table 1. Distribution of sense numbers
</tableCaption>
<table confidence="0.999919">
sense number 2 3 4 6 7 8 21
trial set 39 9 1 0 0 0 1
test set 77 10 7 4 1 1 0
</table>
<tableCaption confidence="0.978779">
Table 2. Distribution of relations between target
words and segmented words
</tableCaption>
<table confidence="0.742360666666667">
relation type 1 2 3 4 Total
trial set 2314 105 68 12 2499
test set 4031 710 212 47 5000
</table>
<bodyText confidence="0.999930714285714">
As is shown in table 2, the total instance
number in the trial set is 2499 because there is a
target word has only 49 instances. About 7.4%
of the instances in the trial set and 19.38% of the
instances in the test set have mismatched target
words and segmented words (with relation types
2, 3 and 4).
</bodyText>
<subsectionHeader confidence="0.954192">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.98435625">
The official performance metric for the CWSI
task is FScore (Zhao and Karypis, 2005). Given
a particular class Ci of size ni and a cluster Sr of
size nr, suppose i
nr examples in the class Ci be-
long to Sr. The F value of this class and cluster is
defined to be:
where P(Ci, S)r = r is the precision value
</bodyText>
<equation confidence="0.9073358">
ni
nr
and R(C , S) = is the recall value defined
i r n i r
ni
</equation>
<bodyText confidence="0.840764666666666">
for class Ci and cluster Sr. The FScore of class Ci
is the maximum F value attained at any cluster,
that is
</bodyText>
<equation confidence="0.5334655">
FScore Ci
( ) max ( i , r
= F C S
Sr )
</equation>
<bodyText confidence="0.746875">
and the FScore of the entire clustering solution
is
</bodyText>
<equation confidence="0.952926">
ni FScore C
( i )
n
</equation>
<bodyText confidence="0.9991278">
where c is the number of classes and n is the size
of the clustering solution.
Another two metrics, Entropy and Purity
(Zhao and Karypis, 2001), are also employed in
this paper to measure our system performance.
Entropy measures how the various classes of
word senses are distributed within each cluster,
while Purity measures the extent to which each
cluster contained word senses from primarily
one class. The entropy of cluster Sr is defined as
</bodyText>
<equation confidence="0.9915915">
c1 n
E(Sr) = − Eri log
logc n
i 1 r
</equation>
<bodyText confidence="0.9985965">
The entropy of the entire clustering solution is
then defined to be the sum of the individual clus-
ter entropies weighted according to the cluster
size. That is
</bodyText>
<equation confidence="0.919957333333333">
Entropy = k nr E S )
∑= ( r
r 1 n
The purity of a cluster is defined to be
P(Sr) = 1 max (n&apos;
nr i
</equation>
<bodyText confidence="0.921110125">
which is the fraction of the overall cluster size
that the largest class of examples assigned to that
cluster represents. The overall purity of the clus-
tering solution is obtained as a weighted sum of
the individual cluster purities and is given by
Purity = k nr P S )
∑= ( r
r 1 n
In general, the larger the values of FScore and
Purity, the better the clustering solution is. The
smaller the Entropy values, the better the cluster-
ing solution is.
The above three metrics are defined to evalu-
ate the result of a single target word. Macro av-
erage metrics are used to evaluate the overall
performance of all the target words.
</bodyText>
<subsectionHeader confidence="0.949198">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999833666666667">
The overall performance on the trial data is
shown in table 3. From the Macro Average En-
tropy and Macro Average Purity, we can see that
k-means works better than agglomerative
method. The detailed results of the k-means sys-
tem are shown in table 4.
</bodyText>
<tableCaption confidence="0.994014">
Table 3. Result comparison on the trial data
</tableCaption>
<table confidence="0.72213975">
Entropy Purity
k-means 0.4858 0.8288
agglomerative 0.5328 0.8020
F C S
</table>
<equation confidence="0.9107262">
( , ) =
i r
P(Ci, Sr) + R(Ci, Sr
2 * P(Ci, Sr) * R(Ci, Sr)
) ,
c
=
FScore
E=
i 1
ni
r
nr
,
)
</equation>
<tableCaption confidence="0.994016">
Table 4. Detailed results of k-means system
</tableCaption>
<table confidence="0.99934537254902">
TargetWord SenseNum Entropy Purity
反射 2 0.855 0.72
翻身 2 0.692 0.78
发展 2 0.377 0.92
发4 3 0.207 0.94
扼杀 2 0.833 0.7
C气 2 0 1
CZ 2 0.592 0.82
杜鹃 2 0.245 0.959
4力 2 0.116 0.98
东A 3 0.396 0.82
东方 2 0.201 0.96
东北 2 0.201 0.96
调4 3 0.181 0.9
导Ir 2 0.122 0.98
单纯 2 0.327 0.92
大人 2 0.653 0.82
大气 2 0 1
大陆 2 0.855 0.72
大军 2 0.5 0.8
7气 2 0.312 0.92
7破 2 0.519 0.86
7开 3 0.534 0.72
7C 2 0.846 0.7
7 21 0.264 0.48
戳穿 2 0.521 0.88
春秋 3 0 1
初二 2 0.76 0.78
出口 3 0.205 0.92
冲撞 2 0.854 0.72
冲洗 2 0.449 0.9
充电 2 0.467 0.9
吃饭 2 0.881 0.7
澄清 2 0.402 0.92
程序 2 0.39 0.92
V包 2 0.793 0.76
参hn 2 0.904 0.68
采购 2 0.943 0.64
材料 3 0.548 0.74
哺育 2 0.583 0.86
补h 2 0.999 0.52
病毒 2 0.242 0.96
标兵 2 0.75 0.74
便宜 3 0.464 0.84
比重 2 0.181 0.96
背离 2 0.672 0.78
报销 2 0.471 0.82
保管 3 0.543 0.7
保安 2 0.347 0.9
把握 4 0.508 0.66
暗淡 2 0.583 0.86
</table>
<tableCaption confidence="0.8669962">
The official results on the test set are shown in
table 5. Our k-means system and agglomerative
system rank 5 and 8 respectively among all the
18 systems.
Table 5. System rankin
</tableCaption>
<table confidence="0.999645">
Rank FScore Rank FScore
1 0.7933 6 0.7788
2 0.7895 7 0.7729
3 0.7855 8* 0.7651
4 0.7849 9 0.7598
5* 0.7812 18 0.5789
</table>
<sectionHeader confidence="0.998266" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979727272727">
This paper tries to build basic systems for Chi-
nese Word Sense Induction (CWSI) task. Basic
clustering algorithms including k-means and
agglomerative methods are studied. No extra
language resources are used except the data
given by the task organizers.
To improve the performance of CWSI sys-
tems, we will introduce new features and study
novel clustering algorithms. We will also inves-
tigate the bakeoff data sets to find some more
characteristics of Chinese word senses.
</bodyText>
<sectionHeader confidence="0.998026" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993508833333333">
The authors are grateful to the organizers of the
Word Sense Induction task for their hard work to
provide such a good research platform. The
work in this paper is supported by grants from
the National Natural Science Foundation of
China (No.60773173, No.60970083).
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999665891891892">
D. Vickrey, L. Biewald, M. Teyssler, and D. Koller.
2005. Word sense disambiguation for machine
translation. In Proceedings of HLT/EMNLP2005,
pp. 771-778.
E. Agirre and A. Soroa. 2007. Semeval-2007 task 02:
Evaluating word sense induction and discrimina-
tion systems. In Proceedings of SemEval2007, pp.
7-12.
G. Karypis. 2002. CLUTO - a clustering toolkit.
Technical Report 02-017, Dept. of Computer Sci-
ence, University of Minnesota. Available at
http://www.cs.umn.edu˜cluto.
H. Zhu. 2009. Research into Automatic Word Sense
Discrimination on Chinese. PhD Dissertation of
Peking University.
J. Y. Chai and A. W. Biermann. 1999. The use of
word sense disambiguation in an information ex-
traction system. In Proceedings of AAAI/IAAI1999,
pp. 850-855.
M. Denkowski. 2009. A Survey of Techniques for
Unsupervised Word Sense Induction. Language &amp;
Statistics II Literature Review.
O. Uzuner, B. Katz, and D. Yuret. 1999. Word sense
disambiguation for information retrieval. In Pro-
ceedings of AAAI/IAAI1999, pp.985.
S. Manandhar and I. P. Klapaftis. 2010. SemEval-
2010 Task 14: Evaluation Setting forWord Sense
Induction &amp;Disambiguation Systems. In Proceed-
ings of SemEval2010, pp. 117-122.
Y. Zhao and G. Karypis. 2005. Hierarchical cluster-
ing algorithms for document datasets. Data Mining
and Knowledge Discovery, 10(2):141–168.
Y. Zhao and G. Karypis. 2001. Criterion functions for
document clustering: Experiments and analysis.
Technical Report 01–40, Dept. of Computer Sci-
ence, University of Minnesota. Available at
http://cs.umn.edu/˜karypis/publications.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728580">
<title confidence="0.999906">Chinese Word Sense Induction with Basic Clustering Algorithms</title>
<author confidence="0.998876">Shiwen Zhengyan</author>
<affiliation confidence="0.950813">Laboratory of Computational Linguistics, Ministry of Education, of Information and Engineering, Zhengzhou University, Zhengzhou,</affiliation>
<address confidence="0.971657">of Information Technology, Henan Institute of Education, Zhengzhou, China</address>
<email confidence="0.978834">yxjia@pku.edu.cnchenzhengyan1981@163.com</email>
<email confidence="0.978834">yusw@pku.edu.cnchenzhengyan1981@163.com</email>
<abstract confidence="0.988035">Word Sense Induction (WSI) is an important topic in natural langage processing area. For the bakeoff task Chinese Word Sense Induction (CWSI), this paper proposes two systems using basic clustering algorithms, k-means and agglomerative clustering. Experimental results show that k-means achieves a better performance. Based only on the data provided by the task organizers, the two systems get FScores of 0.7812 and 0.7651 respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Vickrey</author>
<author>L Biewald</author>
<author>M Teyssler</author>
<author>D Koller</author>
</authors>
<title>Word sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP2005,</booktitle>
<pages>771--778</pages>
<contexts>
<context position="1539" citStr="Vickrey et al., 2005" startWordPosition="214" endWordPosition="217">ask of automatically discovering word senses from un-annotated text. It is distinct from Word Sense Disambiguation (WSD) where the senses are assumed to be known and the aim is to decide the right meaning of the target word in context. WSD generally requires the use of large-scale manually annotated lexical resources, while WSI can overcome this limitation. Furthermore, automatically induced word senses can improve performance on many natural language processing tasks such as information retrieval (Uzuner et al., 1999), information extraction (Chai and Biermann, 1999) and machine translation (Vickrey et al., 2005). WSI is typically treated as a clustering problem. The input is instances of the ambiguous word with their accompanying contexts and the output is a grouping of these instances into classes corresponding to the induced senses. In other words, contexts that are grouped together in the same class represent a specific word sense. The task can be formally defined as a two stage process, feature selection and word clustering. The first stage determines which context features to consider when comparing similarity between words, while the second stage apply some process that clusters similar words u</context>
</contexts>
<marker>Vickrey, Biewald, Teyssler, Koller, 2005</marker>
<rawString>D. Vickrey, L. Biewald, M. Teyssler, and D. Koller. 2005. Word sense disambiguation for machine translation. In Proceedings of HLT/EMNLP2005, pp. 771-778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval2007,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="2621" citStr="Agirre and Soroa, 2007" startWordPosition="380" endWordPosition="383">context features to consider when comparing similarity between words, while the second stage apply some process that clusters similar words using the selected features. So the simplest approaches to WSI involve the use of basic word cooccurrence features and application of classical clustering algorithms, more sophisticated techniques improve performance by introducing new context features, novel clustering algorithms, or both. (Denkowski, 2009) makes a comprehensive survey of techniques for unsupervised word sense induction. Two tasks on English Word Sense Induction were held on SemEval2007 (Agirre and Soroa, 2007) and SemEval2010 (Manandhar and Klapaftis, 2010) respectively, which greatly promote the research of English WSI. However, the study on Chinese Word Sense Induction (CWSI) is inadequate (Zhu, 2009), and Chinese word senses have their own characteristics. The methods that work well in English may not work well in Chinese. So, as an exploration, this paper proposes simple approaches utilizing basic features and basic clustering algorithms, such as partitional method k-means and hierarchical agglomerative method. The rest of this paper is organized as follows. Section 2 briefly introduces the bas</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>E. Agirre and A. Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of SemEval2007, pp. 7-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Karypis</author>
</authors>
<title>CLUTO - a clustering toolkit.</title>
<date>2002</date>
<tech>Technical Report 02-017,</tech>
<institution>Dept. of Computer Science, University of Minnesota.</institution>
<note>Available at http://www.cs.umn.edu˜cluto.</note>
<marker>Karypis, 2002</marker>
<rawString>G. Karypis. 2002. CLUTO - a clustering toolkit. Technical Report 02-017, Dept. of Computer Science, University of Minnesota. Available at http://www.cs.umn.edu˜cluto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhu</author>
</authors>
<title>Research into Automatic Word Sense Discrimination on Chinese.</title>
<date>2009</date>
<tech>PhD</tech>
<institution>Dissertation of Peking University.</institution>
<contexts>
<context position="2818" citStr="Zhu, 2009" startWordPosition="412" endWordPosition="413"> the use of basic word cooccurrence features and application of classical clustering algorithms, more sophisticated techniques improve performance by introducing new context features, novel clustering algorithms, or both. (Denkowski, 2009) makes a comprehensive survey of techniques for unsupervised word sense induction. Two tasks on English Word Sense Induction were held on SemEval2007 (Agirre and Soroa, 2007) and SemEval2010 (Manandhar and Klapaftis, 2010) respectively, which greatly promote the research of English WSI. However, the study on Chinese Word Sense Induction (CWSI) is inadequate (Zhu, 2009), and Chinese word senses have their own characteristics. The methods that work well in English may not work well in Chinese. So, as an exploration, this paper proposes simple approaches utilizing basic features and basic clustering algorithms, such as partitional method k-means and hierarchical agglomerative method. The rest of this paper is organized as follows. Section 2 briefly introduces the basic clustering algorithms. Section 3 describes the feature set. Section 4 gives experimental details and analysis. Conclusions and future work are given in Section 5. 2 Clustering Algorithms Partiti</context>
</contexts>
<marker>Zhu, 2009</marker>
<rawString>H. Zhu. 2009. Research into Automatic Word Sense Discrimination on Chinese. PhD Dissertation of Peking University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y Chai</author>
<author>A W Biermann</author>
</authors>
<title>The use of word sense disambiguation in an information extraction system.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI/IAAI1999,</booktitle>
<pages>850--855</pages>
<contexts>
<context position="1492" citStr="Chai and Biermann, 1999" startWordPosition="207" endWordPosition="210">nduction (WSI) or Word Sense Discrimination is a task of automatically discovering word senses from un-annotated text. It is distinct from Word Sense Disambiguation (WSD) where the senses are assumed to be known and the aim is to decide the right meaning of the target word in context. WSD generally requires the use of large-scale manually annotated lexical resources, while WSI can overcome this limitation. Furthermore, automatically induced word senses can improve performance on many natural language processing tasks such as information retrieval (Uzuner et al., 1999), information extraction (Chai and Biermann, 1999) and machine translation (Vickrey et al., 2005). WSI is typically treated as a clustering problem. The input is instances of the ambiguous word with their accompanying contexts and the output is a grouping of these instances into classes corresponding to the induced senses. In other words, contexts that are grouped together in the same class represent a specific word sense. The task can be formally defined as a two stage process, feature selection and word clustering. The first stage determines which context features to consider when comparing similarity between words, while the second stage a</context>
</contexts>
<marker>Chai, Biermann, 1999</marker>
<rawString>J. Y. Chai and A. W. Biermann. 1999. The use of word sense disambiguation in an information extraction system. In Proceedings of AAAI/IAAI1999, pp. 850-855.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Denkowski</author>
</authors>
<title>A Survey of Techniques for Unsupervised Word Sense Induction. Language &amp; Statistics II Literature Review.</title>
<date>2009</date>
<contexts>
<context position="2447" citStr="Denkowski, 2009" startWordPosition="355" endWordPosition="356">lass represent a specific word sense. The task can be formally defined as a two stage process, feature selection and word clustering. The first stage determines which context features to consider when comparing similarity between words, while the second stage apply some process that clusters similar words using the selected features. So the simplest approaches to WSI involve the use of basic word cooccurrence features and application of classical clustering algorithms, more sophisticated techniques improve performance by introducing new context features, novel clustering algorithms, or both. (Denkowski, 2009) makes a comprehensive survey of techniques for unsupervised word sense induction. Two tasks on English Word Sense Induction were held on SemEval2007 (Agirre and Soroa, 2007) and SemEval2010 (Manandhar and Klapaftis, 2010) respectively, which greatly promote the research of English WSI. However, the study on Chinese Word Sense Induction (CWSI) is inadequate (Zhu, 2009), and Chinese word senses have their own characteristics. The methods that work well in English may not work well in Chinese. So, as an exploration, this paper proposes simple approaches utilizing basic features and basic cluster</context>
</contexts>
<marker>Denkowski, 2009</marker>
<rawString>M. Denkowski. 2009. A Survey of Techniques for Unsupervised Word Sense Induction. Language &amp; Statistics II Literature Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Uzuner</author>
<author>B Katz</author>
<author>D Yuret</author>
</authors>
<title>Word sense disambiguation for information retrieval.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI/IAAI1999,</booktitle>
<pages>985</pages>
<contexts>
<context position="1442" citStr="Uzuner et al., 1999" startWordPosition="200" endWordPosition="203">7651 respectively. 1 Introduction Word Sense Induction (WSI) or Word Sense Discrimination is a task of automatically discovering word senses from un-annotated text. It is distinct from Word Sense Disambiguation (WSD) where the senses are assumed to be known and the aim is to decide the right meaning of the target word in context. WSD generally requires the use of large-scale manually annotated lexical resources, while WSI can overcome this limitation. Furthermore, automatically induced word senses can improve performance on many natural language processing tasks such as information retrieval (Uzuner et al., 1999), information extraction (Chai and Biermann, 1999) and machine translation (Vickrey et al., 2005). WSI is typically treated as a clustering problem. The input is instances of the ambiguous word with their accompanying contexts and the output is a grouping of these instances into classes corresponding to the induced senses. In other words, contexts that are grouped together in the same class represent a specific word sense. The task can be formally defined as a two stage process, feature selection and word clustering. The first stage determines which context features to consider when comparing </context>
</contexts>
<marker>Uzuner, Katz, Yuret, 1999</marker>
<rawString>O. Uzuner, B. Katz, and D. Yuret. 1999. Word sense disambiguation for information retrieval. In Proceedings of AAAI/IAAI1999, pp.985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Manandhar</author>
<author>I P Klapaftis</author>
</authors>
<title>SemEval2010 Task 14: Evaluation Setting forWord Sense Induction &amp;Disambiguation Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval2010,</booktitle>
<pages>117--122</pages>
<contexts>
<context position="2669" citStr="Manandhar and Klapaftis, 2010" startWordPosition="386" endWordPosition="390">ng similarity between words, while the second stage apply some process that clusters similar words using the selected features. So the simplest approaches to WSI involve the use of basic word cooccurrence features and application of classical clustering algorithms, more sophisticated techniques improve performance by introducing new context features, novel clustering algorithms, or both. (Denkowski, 2009) makes a comprehensive survey of techniques for unsupervised word sense induction. Two tasks on English Word Sense Induction were held on SemEval2007 (Agirre and Soroa, 2007) and SemEval2010 (Manandhar and Klapaftis, 2010) respectively, which greatly promote the research of English WSI. However, the study on Chinese Word Sense Induction (CWSI) is inadequate (Zhu, 2009), and Chinese word senses have their own characteristics. The methods that work well in English may not work well in Chinese. So, as an exploration, this paper proposes simple approaches utilizing basic features and basic clustering algorithms, such as partitional method k-means and hierarchical agglomerative method. The rest of this paper is organized as follows. Section 2 briefly introduces the basic clustering algorithms. Section 3 describes th</context>
</contexts>
<marker>Manandhar, Klapaftis, 2010</marker>
<rawString>S. Manandhar and I. P. Klapaftis. 2010. SemEval2010 Task 14: Evaluation Setting forWord Sense Induction &amp;Disambiguation Systems. In Proceedings of SemEval2010, pp. 117-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>G Karypis</author>
</authors>
<title>Hierarchical clustering algorithms for document datasets.</title>
<date>2005</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="8059" citStr="Zhao and Karypis, 2005" startWordPosition="1308" endWordPosition="1311"> trial set 39 9 1 0 0 0 1 test set 77 10 7 4 1 1 0 Table 2. Distribution of relations between target words and segmented words relation type 1 2 3 4 Total trial set 2314 105 68 12 2499 test set 4031 710 212 47 5000 As is shown in table 2, the total instance number in the trial set is 2499 because there is a target word has only 49 instances. About 7.4% of the instances in the trial set and 19.38% of the instances in the test set have mismatched target words and segmented words (with relation types 2, 3 and 4). 4.2 Evaluation Metrics The official performance metric for the CWSI task is FScore (Zhao and Karypis, 2005). Given a particular class Ci of size ni and a cluster Sr of size nr, suppose i nr examples in the class Ci belong to Sr. The F value of this class and cluster is defined to be: where P(Ci, S)r = r is the precision value ni nr and R(C , S) = is the recall value defined i r n i r ni for class Ci and cluster Sr. The FScore of class Ci is the maximum F value attained at any cluster, that is FScore Ci ( ) max ( i , r = F C S Sr ) and the FScore of the entire clustering solution is ni FScore C ( i ) n where c is the number of classes and n is the size of the clustering solution. Another two metrics</context>
</contexts>
<marker>Zhao, Karypis, 2005</marker>
<rawString>Y. Zhao and G. Karypis. 2005. Hierarchical clustering algorithms for document datasets. Data Mining and Knowledge Discovery, 10(2):141–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>G Karypis</author>
</authors>
<title>Criterion functions for document clustering: Experiments and analysis.</title>
<date>2001</date>
<tech>Technical Report 01–40,</tech>
<institution>Dept. of Computer Science, University of Minnesota.</institution>
<note>Available at http://cs.umn.edu/˜karypis/publications.</note>
<contexts>
<context position="8704" citStr="Zhao and Karypis, 2001" startWordPosition="1453" endWordPosition="1456">lass Ci of size ni and a cluster Sr of size nr, suppose i nr examples in the class Ci belong to Sr. The F value of this class and cluster is defined to be: where P(Ci, S)r = r is the precision value ni nr and R(C , S) = is the recall value defined i r n i r ni for class Ci and cluster Sr. The FScore of class Ci is the maximum F value attained at any cluster, that is FScore Ci ( ) max ( i , r = F C S Sr ) and the FScore of the entire clustering solution is ni FScore C ( i ) n where c is the number of classes and n is the size of the clustering solution. Another two metrics, Entropy and Purity (Zhao and Karypis, 2001), are also employed in this paper to measure our system performance. Entropy measures how the various classes of word senses are distributed within each cluster, while Purity measures the extent to which each cluster contained word senses from primarily one class. The entropy of cluster Sr is defined as c1 n E(Sr) = − Eri log logc n i 1 r The entropy of the entire clustering solution is then defined to be the sum of the individual cluster entropies weighted according to the cluster size. That is Entropy = k nr E S ) ∑= ( r r 1 n The purity of a cluster is defined to be P(Sr) = 1 max (n&apos; nr i w</context>
</contexts>
<marker>Zhao, Karypis, 2001</marker>
<rawString>Y. Zhao and G. Karypis. 2001. Criterion functions for document clustering: Experiments and analysis. Technical Report 01–40, Dept. of Computer Science, University of Minnesota. Available at http://cs.umn.edu/˜karypis/publications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>