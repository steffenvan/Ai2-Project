<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000060">
<title confidence="0.993131">
Improved Transliteration Mining Using Graph Reinforcement
</title>
<author confidence="0.998053">
Ali El-Kahky1, Kareem Darwish1, Ahmed Saad Aldein2, Mohamed Abd El-Wahab3,
Ahmed Hefny2, Waleed Ammar4
</author>
<affiliation confidence="0.977271">
1 Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
</affiliation>
<address confidence="0.630449333333333">
2 Computer Engineering Department, Cairo University, Cairo, Egypt
3 Microsoft Research, Microsoft, Cairo, Egypt
4 Microsoft Research, Microsoft, Redmond, WA, US
</address>
<email confidence="0.903234">
{aelkahky,kdarwish}@qf.org.qa1, asaadaldien@hotmail.com2,
ahmed.s.hefny@gmail.com2, t-momah@microsoft.com3,
i-waamma@microsoft.com4
</email>
<bodyText confidence="0.9975135">
points for the four language pairs
respectively.
</bodyText>
<sectionHeader confidence="0.987055" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999953681818182">
Transliteration Mining (TM) is the process of
finding transliterated word pairs in parallel or
comparable corpora. TM has many potential
applications such as mining training data for
transliteration, improving lexical coverage for
machine translation, and cross language retrieval
via translation resource expansion. TM has been
gaining some attention lately with a shared task in
the ACL 2010 NEWS workshop (Kumaran, et al.
2010).
One popular statistical TM approach is performed
in two stages. First, a generative model is trained
by performing automatic character level alignment
of parallel transliterated word pairs to find
character segment mappings between source and
target languages. Second, given comparable or
parallel text, the trained generative model is used
to generate possible transliterations of a word in
the source language while constraining the
transliterations to words that exist in the target
language.
However, two problems arise in this approach:
</bodyText>
<listItem confidence="0.965100285714286">
1. Many possible character sequence mappings
between source and target languages may not be
observed in training data, particularly when limited
training data is available – hurting recall.
2. Conditional probability estimates of obtained
mappings may be inaccurate, because some
mappings and some character sequences may not
</listItem>
<sectionHeader confidence="0.725787" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.998648870967742">
Mining of transliterations from comparable
or parallel text can enhance natural
language processing applications such as
machine translation and cross language
information retrieval. This paper presents
an enhanced transliteration mining
technique that uses a generative graph
reinforcement model to infer mappings
between source and target character
sequences. An initial set of mappings are
learned through automatic alignment of
transliteration pairs at character sequence
level. Then, these mappings are modeled
using a bipartite graph. A graph
reinforcement algorithm is then used to
enrich the graph by inferring additional
mappings. During graph reinforcement,
appropriate link reweighting is used to
promote good mappings and to demote bad
ones. The enhanced transliteration mining
technique is tested in the context of mining
transliterations from parallel Wikipedia
titles in 4 alphabet-based languages pairs,
namely English-Arabic, English-Russian,
English-Hindi, and English-Tamil. The
improvements in F1-measure over the
baseline system were 18.7, 1.0, 4.5, and
32.5 basis points for the four language
pairs respectively. The results herein
outperform the best reported results in the
literature by 2.6, 4.8, 0.8, and 4.1 basis
</bodyText>
<page confidence="0.991757">
1384
</page>
<bodyText confidence="0.99835932">
appear a sufficient number of times in training to
properly estimate their probabilities – hurting
precision.
In this paper we focus on overcoming these two
problems to improve overall TM. To address the
first problem, we modeled the automatically
obtained character sequence mappings (from
alignment) as a bipartite graph and then we
performed graph reinforcement to enrich the graph
and predict possible mappings that were not
directly obtained from training data. The example
in Figure 1 motivates graph reinforcement. In the
example, the Arabic letter &amp;quot;ق&amp;quot; (pronounced as
&amp;quot;qa&amp;quot;) was not aligned to the English letter &amp;quot;c&amp;quot; in
training data. Such a mapping seems probable
given that another Arabic letter, &amp;quot;ك&amp;quot; (pronounced
as &amp;quot;ka&amp;quot;), maps to two English letters, &amp;quot;q&amp;quot; and &amp;quot;k&amp;quot;,
to which &amp;quot;ق&amp;quot; also maps. In this case, there are
multiple paths that would lead to a mapping
between the Arabic letter &amp;quot;ق&amp;quot; and the English letter
&amp;quot;c&amp;quot;, namely ق  q  ك  c and ق  k  ك 
c. By using multiple paths as sources of evidence,
we can infer the new mapping and estimate its
probability.
Another method for overcoming the missing
mappings problem entails assigning small
smoothing probabilities to unseen mappings.
However, from looking at the graph, it is evident
that some mappings could be inferred and should
be assigned probabilities that are higher than a
small smoothing probability.
The second problem has to do primarily with some
characters in one language, typically vowels,
mapping to many character sequences in the other
language, with some of these mappings assuming
very high probabilities (due to limited training
data). To overcome this problem, we used link
reweighting in graph reinforcement to scale down
the likelihood of mappings to target character
sequences in proportion to how many source
sequences map to them.
We tested the proposed method using the ACL
2010 NEWS workshop data for English-Arabic,
English-Russian, English-Hindi, and English-
Tamil (Kumaran et al., 2010). For each language
pair, the standard ACL 2010 NEWS workshop data
contained a base set of 1,000 transliteration pairs
for training, and set of 1,000 parallel Wikipedia
titles for testing.
The contributions of the paper are:
</bodyText>
<listItem confidence="0.9903966">
1. Employing graph reinforcement to improve the
coverage of automatically aligned data – as they
apply to transliteration mining. This positively
affects recall.
2. Applying link reweighting to overcome
</listItem>
<bodyText confidence="0.947319181818182">
situations where certain tokens – character
sequences in the case of transliteration – tend to
have many mappings, which are often erroneous.
This positively affects precision.
The rest of the paper is organized as follows:
Section 2 surveys prior work on transliteration
mining; Section 3 describes the baseline TM
approach and reports on its effectiveness; Section 4
describes the proposed graph reinforcement along
with link reweighting and reports on the observed
improvements; and Section 5 concludes the paper.
</bodyText>
<figureCaption confidence="0.995528">
Figure 1: Example mappings seen in training
</figureCaption>
<subsectionHeader confidence="0.648768">
Background
</subsectionHeader>
<bodyText confidence="0.98320875">
Much work has been done on TM for different
language pairs such as English-Chinese (Kuo et al.,
2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al.
2008;), English-Tamil (Saravanan and Kumaran,
2008; Udupa and Khapra, 2010), English-Korean
(Oh and Isahara, 2006; Oh and Choi, 2006),
English-Japanese (Qu et al., 2000; Brill et al.,
2001; Oh and Isahara, 2006), English-Hindi (Fei et
al., 2003; Mahesh and Sinha, 2009), and English-
Russian (Klementiev and Roth, 2006).
TM typically involves two main tasks, namely:
finding character mappings between two
languages, and given the mappings ascertaining
whether two words are transliterations or not.
When training with a limited number of
transliteration pairs, two additional problems
appear: many possible character sequence
mappings between source and target languages
may not be observed in training data, and
conditional probability estimates of obtained
</bodyText>
<page confidence="0.984943">
1385
</page>
<bodyText confidence="0.970289">
mappings may be inaccurate. These two problems
affect recall and precision respectively.
</bodyText>
<subsectionHeader confidence="0.999872">
1.1 Finding Character Mappings
</subsectionHeader>
<bodyText confidence="0.99967525">
To find character sequence mappings between two
languages, the most common approach entails
using automatic letter alignment of transliteration
pairs. Akin to phrasal alignment in machine
translation, character sequence alignment is treated
as a word alignment problem between parallel
sentences, where transliteration pairs are treated as
if they are parallel sentences and the characters
from which they are composed are treated as if
they are words. Automatic alignment can be
performed using different algorithms such as the
EM algorithm (Kuo et al., 2008; Lee and Chang,
2003) or HMM based alignment (Udupa et al.,
2009a; Udupa et al., 2009b). In this paper, we use
automatic character alignment between
transliteration pairs using an HMM aligner.
Another method is to use automatic speech
recognition confusion tables to extract phonetically
equivalent character sequences to discover
monolingual and cross lingual pronunciation
variations (Kuo and Yang, 2005). Alternatively,
letters can be mapped into a common character set
using a predefined transliteration scheme (Oh and
Choi, 2006).
</bodyText>
<subsectionHeader confidence="0.99105">
1.2 Transliteration Mining
</subsectionHeader>
<bodyText confidence="0.9999826">
For the problem of ascertaining if two words can
be transliterations of each other, a common
approach involves using a generative model that
attempts to generate all possible transliterations of
a source word, given the character mappings
between two languages, and restricting the output
to words in the target language (Fei et al., 2003;
Lee and Chang, 2003, Udupa et al., 2009a). This is
similar to the baseline approach that we used in
this paper. Noeman and Madkour (2010)
implemented this technique using a finite state
automaton by generating all possible
transliterations along with weighted edit distance
and then filtered them using appropriate thresholds
and target language words. They reported the best
TM results between English and Arabic with F1-
measure of 0.915 on the ACL-2010 NEWS
workshop standard TM dataset. A related
alternative is to use back-transliteration to
determine if one sequence could have been
generated by successively mapping character
sequences from one language into another (Brill et
al., 2001; Bilac and Tanaka, 2005; Oh and Isahara,
2006).
Udupa and Khapra (2010) proposed a method in
which transliteration candidates are mapped into a
―low-dimensional common representation space‖.
Then, similarity between the resultant feature
vectors for both candidates can be computed.
Udupa and Kumar (2010) suggested that mapping
to a common space can be performed using context
sensitive hashing. They applied their technique to
find variant spellings of names.
Jiampojamarn et al. (2010) used classification to
determine if a source language word and target
language word are valid transliterations. They used
a variety of features including edit distance
between an English token and the Romanized
versions of the foreign token, forward and
backward transliteration probabilities, and
character n-gram similarity. They reported the best
results for Russian, Tamil, and Hindi with F1-
measure of 0.875, 0.924, and 0.914 respectively on
the ACL-2010 NEWS workshop standard TM
datasets.
</bodyText>
<subsectionHeader confidence="0.996867">
1.3 Training with Limited Training Data
</subsectionHeader>
<bodyText confidence="0.999958916666667">
When only limited training data is available to
train a character mapping model, the resultant
mappings are typically incomplete (due to
sparseness in the training data). Further, resultant
mappings may not be observed a sufficient of
times and hence their mapping probabilities may
be inaccurate.
Different methods were proposed to solve these
two problems. These methods focused on making
training data less sparse by performing some kind
of letter conflation. Oh and Choi (2006) used a
SOUNDEX like scheme. SOUNDEX is used to
convert English words into a simplified phonetic
representation, in which vowels are removed and
phonetically similar characters are conflated. A
variant of SOUNDEX along with iterative training
was proposed by Darwish (2010). Darwish (2010)
reported significant improvements in TM recall at
the cost of limited drop in precision. Another
method involved expanding character sequence
maps by automatically mining transliteration pairs
and then aligning these pairs to generate an
expanded set of character sequence maps (Fei et
al., 2003). In this work we proposed graph
</bodyText>
<page confidence="0.956752">
1386
</page>
<bodyText confidence="0.999931428571428">
reinforcement with link reweighting to address this
problem. Graph reinforcement was used in the
context of different problems such as mining
paraphrases (Zhao et al., 2008; Kok and Brockett,
2010; Bannard and Callison-Burch 2005) and
named entity translation extraction (You et al.,
2010).
</bodyText>
<subsectionHeader confidence="0.9931665">
Baseline Transliteration Mining
1.4 Description of Baseline System
</subsectionHeader>
<bodyText confidence="0.999093363636364">
The basic TM setup that we employed in this
work used a generative transliteration model,
which was trained on a set of transliteration pairs.
The training involved automatically aligning
character sequences. The alignment was performed
using a Bayesian learner that was trained on word
dependent transition models for HMM based word
alignment (He, 2007). Alignment produced
mappings of source character sequences to target
character sequences along with the probability of
source given target and vice versa. Source
character sequences were restricted to be 1 to 3
characters long.
For all the work reported herein, given an
English-foreign language transliteration candidate
pair, English was treated as the target language and
the foreign language as the source. Given a
foreign source language word sequence Fl and an
English target word sequence El&amp;quot;, Fl E Fl could
be a potential transliteration of Ej E El&amp;quot; . An
example of word sequences pair is the Tamil-
English pair: (முதலாம் ஹைலி செலாெி,
Haile Selassie I of Ethiopia), where &amp;quot;முதலாம்”
could be transliteration for any or none of the
English words {&amp;quot;Haile&amp;quot;, &amp;quot;Selassie&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;of&amp;quot;,
&amp;quot;Ethiopia&amp;quot;}. The pseudo code below describes
how transliteration mining generates candidates.
Basically, given a source language word, all
possible segmentations, where each segment has a
maximum length of 3 characters, are produced
along with their associated mappings into the
target language. Given all mapping combinations,
combinations producing valid target words are
retained and sorted according to the product of
their mapping probabilities. If the product of the
mapping probabilities for the top combination is
above a certain threshold, then it is chosen as the
transliteration candidate. Otherwise, no candidate
is chosen. To illustrate how TM works, consider
the following example: Given the Arabic word
&amp;quot;نم&amp;quot;, all possible segmentations are (ن ، م) and (نم).
Given the target words {the, best, man} and the
possible mappings for the segments and their
probabilities:
</bodyText>
<equation confidence="0.754308">
ـم _ {(m, 0.7), (me, 0.25), (ma, 0.05)}
ن _
{n, 0.7), (nu, 0.2), (an, 0.1)}
نم _ {(men, 0.4), (man, 0.3), (mn, 0.3)}
</equation>
<bodyText confidence="0.976001">
The only combinations leading valid target
words would be:
</bodyText>
<equation confidence="0.9426975">
(نم) 4 {(man: 0.3)}
(ن ، ـم) 4 {(m,an: 0.07), (ma, n: 0.035)}
</equation>
<bodyText confidence="0.999921222222222">
Consequently, the algorithm would produce the
tuple with the highest probability: (نم , man, 0.3).
As the pseudo code suggests, the actual
implementation is optimized via: incremental left
to right processing of source words; the use of a
Patricia trie to prune mapping combinations that
don’t lead to valid words; and the use of a priority
queue to insure that the best candidate is always at
the top of the queue.
</bodyText>
<subsectionHeader confidence="0.996748">
1.5 Smoothing and Thresholding
</subsectionHeader>
<bodyText confidence="0.999851923076923">
We implemented the baseline system with and
without assigning small smoothing probabilities
for unseen source character to target character
mappings. Subsequent to training, the smoothing
probability was selected as the smallest observed
mapping probability in training.
We used a threshold on the minimum acceptable
transliteration score to filter out unreliable
transliterations. We couldn’t fix a minimum score
for reliable transliterations to a uniform value for
all words, because this would have caused the
model to filter out long transliterations. Thus, we
tied the threshold to the length of transliterated
words. We assumed a threshold d for single
character mappings and the transliteration
threshold for a target word of length l was
computed as dl. We selected d by sorting the
mapping probabilities, removing the lowest 10% of
mapping probabilities (which we assumed to be
outliers), and then selecting the smallest observed
probability to be the character threshold d. The
choice of removing the lowest ranking 10% of
mapping probabilities was based on intuition,
because we did not have a validation set. The
threshold was then applied to filter out
transliteration with TransliterationScore &lt; dl.
</bodyText>
<page confidence="0.960427">
1387
</page>
<listItem confidence="0.9760025">
1: Input: Mappings, set of source given target mappings with associated Prob.
2: Input: SourceWord ( Fj E Fi ), Source language word
3: Input: TargetWords, Patricia trie containing all target language words (El&amp;quot;)
4: Data Structures: DFS, Priority queue to store candidate transliterations pair ordered by their transliteration
score – Each candidate transliteration tuple = (SourceFragment, TargetTransliteration, TransliterationScore).
5: StartSymbol = (“”, “”, 1.0)
6: DFS={StartSymbol}
7: While(DFS is not empty)
</listItem>
<figure confidence="0.562361833333333">
8: SourceFragment= DFS.Top().SourceFragment
9: TargetFragment= DFS.Top().TargetTransliteration
10: FragmentProb=DFS.Top().TransliterationScore
11: If (SourceWord == SourceFragment )
12: If(FragmentScore &gt; Threshold)
22: DFS.Remove(SourceFragment)
</figure>
<figureCaption confidence="0.939865">
Figure 2: Pseudo code for transliteration mining
</figureCaption>
<table confidence="0.733304818181818">
13: Return (SourceWord, TargetTransliteration, TransliterationScore)
14: Else
15: Return Null
16: DFS.RemoveTop()
17: For SubFragmentLength=1 to 3
18: SourceSubString= SubString( SourceWord, SourceFragment.Length , SubFragmentLength)
19: Foreach mapping in Mappings[SourceSubString]
20: If( (TargetFragment + mapping) is a sub-string in TargetWords)
21: DFS.Add(SourceFragment + SourceSubString, Mapping.Score * FragmentScore)
23: End While
24: Return Null
</table>
<subsectionHeader confidence="0.997098">
1.6 Effectiveness of Baseline System
</subsectionHeader>
<bodyText confidence="0.99991175">
To test the effectiveness of the baseline system, we
used the standard TM training and test datasets
from the ACL-2010 NEWS workshop shared task.
The datasets are henceforth collectively referred to
as the NEWS dataset. The dataset included 4
alphabet-based language pairs, namely English-
Arabic, English-Russian, English-Hindi, and
English-Tamil. For each pair, a dataset included a
list of 1,000 parallel transliteration word pairs to
train a transliteration model, and a list of 1,000
parallel word sequences to test TM. The parallel
sequences in the test sets were extracted titles from
Wikipedia article for which cross language links
exist between both languages.
We preprocessed the different languages as
follows:
</bodyText>
<listItem confidence="0.9870105">
• Russian: characters were case-folded
• Arabic: the different forms of alef (alef, alef
maad, alef with hamza on top, and alef with
hamza below it) were normalized to alef, ya
and alef maqsoura were normalized to ya, and
ta marbouta was mapped to ha.
• English: letters were case-folded and the
following letter conflations were performed:
</listItem>
<equation confidence="0.880558">
ž, ż 4 z á, â, ä, à, ã, ā, ą, æ 4 a
é, ę, è 4 e ć, č, g 4c
ł 4l ï, í, ì, î 4 i
ó, ō, ö, õ 4 o ń, ñ, ṅ 4 n
ş, ś, ß, š 4 s ř 4 r
ý 4 y ū, ü, ú, û 4 u
</equation>
<listItem confidence="0.991829">
• Tamil and Hindi: no preprocessing was
performed.
</listItem>
<table confidence="0.9996868">
English/ P R F
Arabic 0.988 0.983 0.583 0.603 0.733 0.748
Russian 0.975 0.967 0.831 0.862 0.897 0.912
Hindi 0.986 0.981 0.693 0.796 0.814 0.879
Tamil 0.984 0.981 0.274 0.460 0.429 0.626
</table>
<tableCaption confidence="0.99544">
Table 1: Baseline results for all language pairs.
Results with smoothing are shaded.
</tableCaption>
<bodyText confidence="0.9619692">
Table 1 reports the precision, recall, and F1-
measure results for using the baseline system in
TM between English and each of the 4 other
languages in the NEWS dataset with and without
smoothing. As is apparent in the results, without
smoothing, precision is consistently high for all
languages, but recall is generally poor, particularly
for Tamil. When smoothing is applied, we
observed a slight drop in precision for Arabic,
Hindi, and Tamil and a significant drop of 5.6
</bodyText>
<page confidence="0.969204">
1388
</page>
<bodyText confidence="0.9997702">
basis points for Russian. However, the application
of smoothing increased recall dramatically for all
languages, particularly Tamil. For the remainder of
the paper, the results with smoothing are used as
the baseline results.
</bodyText>
<sectionHeader confidence="0.719852" genericHeader="method">
Background
</sectionHeader>
<subsectionHeader confidence="0.999346">
1.7 Description of Graph Reinforcement
</subsectionHeader>
<bodyText confidence="0.999347">
In graph reinforcement, the mappings deduced
from the alignment process were represented using
a bipartite graph G = (S, T, M), where S was the
set of source language character sequences, T was
the set of target language character sequences, and
M was the set of mappings (links or edges)
between S and T. The score of each mapping
m(v1|v2), where m(v1|v2) E M, was initially set to
the conditional probability of target given source
p(v1|v2). Graph reinforcement was performed by
traversing the graph from S 4 T 4 S 4 T in
order to deduce new mappings. Given a source
sequence s&apos; E S and a target sequence t&apos; E T, the
deduced mapping probabilities were computed as
follows (Eq.1):
</bodyText>
<equation confidence="0.843659">
𝑚(  |) ∏ ( 𝑚(  |)𝑚(  |)𝑚(  |))
</equation>
<bodyText confidence="0.9997545">
where the term ( 𝑚(  |)𝑚(  |)𝑚(  |))
computed the probability that a mapping is not
correct. Hence, the probability of an inferred
mapping would be boosted if it was obtained from
multiple paths. Given the example in Figure 1,
m(c|ق) would be computed as follows:
</bodyText>
<equation confidence="0.9653505">
( 𝑚( |ك)𝑚(ك |)𝑚( |ق))
( 𝑚( |ك)𝑚(ك |)𝑚( |ق))
</equation>
<bodyText confidence="0.999955">
We were able to apply reinforcement iteratively on
all mappings from S to T to deduce previously
unseen mappings (graph edges) and to update the
probabilities of existing mappings.
</bodyText>
<subsectionHeader confidence="0.998047">
1.8 Link Reweighting
</subsectionHeader>
<bodyText confidence="0.998461371428571">
The basic graph reinforcement algorithm is prone
to producing irrelevant mappings by using
character sequences with many different possible
mappings as a bridge. Vowels were the most
obvious examples of such character sequences. For
example, automatic alignment produced 26 Hindi
character sequences that map to the English letter
&amp;quot;a&amp;quot;, most of which were erroneous such as the
mapping between &amp;quot;a&amp;quot; and &amp;quot;व” (pronounced va).
Graph reinforcement resulted in many more such
mappings. After successive iterations, such
character sequences would cause the graph to be
fully connected and eventually the link weights
will tend to be uniform in their values. To illustrate
this effect, we experimented with basic graph
reinforcement on the NEWS dataset. The figures of
merit were precision, recall, and F1-measure.
Figures 3, 4, 5, and 6 show reinforcement results
for Arabic, Russian, Hindi, and Tamil respectively.
The figures show that: recall increased quickly and
nearly saturated after several iterations; precision
continued to drop with more iterations; and F1-
measure peaked after a few iterations and began to
drop afterwards. This behavior was undesirable
because overall F1-measure values did not
converge with iterations, necessitating the need to
find clear stopping conditions.
To avoid this effect and to improve precision, we
applied link reweighting after each iteration. Link
reweighting had the effect of decreasing the
weights of target character sequences that have
many source character sequences mapping to them
and hence reducing the effect of incorrectly
inducing mappings. Link reweighting was
performed as follows (Eq. 2):
</bodyText>
<equation confidence="0.998531">
𝑚 (  |) (  |)
∑ (  |)
</equation>
<bodyText confidence="0.998514666666667">
Where si E S is a source character sequence that
maps to t. So in the case of &amp;quot;a&amp;quot; mapping to the &amp;quot;व&amp;quot;
character in Hindi, the link weight from &amp;quot;a&amp;quot; to &amp;quot;व&amp;quot;
is divided by the sum of link weights from &amp;quot;a&amp;quot; to
all 26 characters to which &amp;quot;a&amp;quot; maps.
We performed multiple experiments on the NEWS
dataset to test the effect of graph reinforcement
with link reweighting with varying number of
reinforcement iterations. Figures 7, 8, 9, and 10
compare baseline results with smoothing to results
with graph reinforcement at different iterations.
As can be seen in the figures, the F1-measure
values stabilized as we performed multiple graph
reinforcement iterations. Except for Russian, the
results across different languages behaved in a
similar manner.
For Russian, graph reinforcement marginally
affected TM F1-measure, as precision and recall
</bodyText>
<page confidence="0.990538">
1389
</page>
<bodyText confidence="0.999754">
marginally changed. The net improvement was 1.1
basis points. English and Russian do not share the
same alphabet, and the number of initial mappings
was bigger compared to the other language pairs.
Careful inspection of the English-Russian test set,
with the help of a Russian speaker, suggests that:
</bodyText>
<listItem confidence="0.994360272727273">
1) the test set reference contained many false
negatives;
2) Russian names often have multiple phonetic
forms (or spellings) in Russian with a single
standard transliteration in English. For example,
the Russian name &amp;quot;Olga&amp;quot; is often written and
pronounced as &amp;quot;Ola&amp;quot; and &amp;quot;Olga&amp;quot; in Russian; and
3) certain English phones do not exist in Russian,
leading to inconsistent character mappings in
Russian. For example, the English phone for &amp;quot;g&amp;quot;,
as in &amp;quot;George&amp;quot;, does not exist in Russian.
</listItem>
<bodyText confidence="0.999954074074074">
For the other languages, graph reinforcement
yielded steadily improving recall and consequently
steadily improving F1-measure. Most
improvements were achieved within the first 5
iterations, and improvements beyond 10 iterations
were generally small (less than 0.5 basis points in
F1-measure). After 15 iterations, the improvements
in overall F1-measure above the baseline with
smoothing were 19.3, 5.3, and 32.8 basis points for
Arabic, Tamil, and Hindi respectively. The F1-
measure values seemed to stabilize with successive
iterations. The least improvements were observed
for Hindi. This could be attributed to the fact that
Hindi spelling is largely phonetic, making letters in
words pronounceable in only one way. This fact
makes transliteration between Hindi and English
easier than Arabic and Tamil. In the case of Tamil,
the phonetics of letters change depending on the
position of letters in words. As for Arabic, multiple
letters sequences in English can map to single
letters in Arabic and vice versa. Also, Arabic has
diacritics which are typically omitted, but
commonly transliterate to English vowels. Thus,
the greater the difference in phonetics between two
languages and the greater the phonetic complexity
of either, the more TM can gain from the proposed
technique.
</bodyText>
<subsectionHeader confidence="0.976152">
1.9 When Graph Reinforcement Worked
</subsectionHeader>
<bodyText confidence="0.999958575757576">
An example where reinforcement worked entails
the English-Arabic transliteration pair (Seljuq,
هقجلاس). In the baseline runs with 1,000 training
examples, both were not mapped to each other
because there were no mappings between the letter
&amp;quot;q&amp;quot; and the Arabic letter sequence &amp;quot;هق&amp;quot;
(pronounced as &amp;quot;qah&amp;quot;). The only mappings that
were available for &amp;quot;q&amp;quot; were &amp;quot;هك&amp;quot; (pronounced as
&amp;quot;kah&amp;quot;), &amp;quot;ق&amp;quot; (pronounced as &amp;quot;q&amp;quot;), and &amp;quot;ك&amp;quot;
(pronounced as &amp;quot;k&amp;quot;) with probabilities 54.0, 0.10,
and 5452 respectively. Intuitively, the third
mapping is more likely than the second. After 3
graph reinforcement iterations, the top 5 mappings
for &amp;quot;q&amp;quot; were &amp;quot;ق&amp;quot; (pronounced as &amp;quot;q&amp;quot;), &amp;quot;هق&amp;quot;
(pronounced as &amp;quot;qah&amp;quot;), &amp;quot;هك&amp;quot; (pronounced as
&amp;quot;kah&amp;quot;), &amp;quot;ك&amp;quot; (pronounced as &amp;quot;k&amp;quot;), and &amp;quot;قلا&amp;quot;
(pronounced as &amp;quot;alq&amp;quot;) with mapping probabilities
of 0.22, 0.19, 0.15, 0.05, and 0.05 respectively. In
this case, graph reinforcement was able to find the
missing mapping and properly reorder the
mappings. Performing 10 iterations with link
reweighting for Arabic led to 17 false positives.
Upon examining them, we found that: 9 were
actually correct, but erroneously labeled as false in
the test set; 6 were phonetically similar like &amp;quot;اينابسا&amp;quot;
(pronounced espanya) and &amp;quot;Spain&amp;quot; and &amp;quot;ايجولونكتلا&amp;quot;
(pronounced alteknologya) and &amp;quot;technology&amp;quot;; and
the remaining 2 were actually wrong, which were
&amp;quot;يشتيب&amp;quot; (pronounced beatchi) and &amp;quot;medici&amp;quot; and
&amp;quot; يديس&amp;quot; (pronounced sidi) and &amp;quot;taya&amp;quot;. This seems to
indicate that graph reinforcement generally
introduced more proper mappings than improper
ones.
</bodyText>
<subsectionHeader confidence="0.670069">
1.10 Comparing to the State-of-the-Art
</subsectionHeader>
<bodyText confidence="0.999894785714286">
Table 2 compares the best reported results in ACL-
2010 NEWS TM shared task for Arabic (Noeman
and Madkour, 2010) and for the other languages
(Jiampojamarn et al. 2010) and the results obtained
by the proposed technique using 10 iterations, with
link reweighting. The comparison shows that the
proposed algorithm yielded better results than the
best reported results in the literature by 2.6, 4.8,
0.8 and 4.1 F1-measure points in Arabic, Russian,
Hindi and Tamil respectively. For Arabic, the
improvement over the previously reported result
was due to improvement in precision, while for the
other languages the improvements were due to
improvements in both recall and precision.
</bodyText>
<page confidence="0.946963">
1390
</page>
<figure confidence="0.999753208333333">
F
R
P
1.000
0.900
0.800
0.700
0.600
0.500
0.400
5
Iterations
1.000
5
Iterations
F
R
P
0.900
0.800
0.700
0.600
0.500
0.400
</figure>
<figureCaption confidence="0.981946">
Figure 3: Graph reinforcement w/o link reweighting
for Arabic
Figure 4: Graph reinforcement w/o link reweighting
</figureCaption>
<figure confidence="0.996078">
for Russian
F
R
P
1.000
0.900
0.800
0.700
0.600
0.500
0.400
5
Iterations
F
R
P
1.000
0.900
0.800
0.700
0.600
0.500
0.400
5
Iterations
</figure>
<figureCaption confidence="0.8987894">
Figure 5: Graph reinforcement w/o link reweighting
for Hindi
Figure 6: Graph reinforcement w/o link reweighting
for Tamil
Figure 7: Graph reinforcement results for Arabic Figure 8: Graph reinforcement results for Russian
</figureCaption>
<figure confidence="0.986534065789474">
F
R
P
baseline
F = 0.748
R = 0.603
P = 0.983
1.00
0.98
0.96
0.94
0.92
0.90
0.88
0.86
0.84
0.82
1 2 3 4 5 6 7 8 9 101112131415
Number of Iterations
baseline
F = 0.912
R = 0.862
P = 0.967
F
R
P
1 2 3 4 5 6 7 8 9 101112131415
Number of Iterations
0.88
0.86
0.84
0.82
1.00
0.98
0.96
0.94
0.92
0.90
F
R
P
baseline
F = 0.879
R = 0.796
P = 0.981
1.00
0.98
0.96
0.94
0.92
0.90
0.88
0.86
0.84
0.82
1 2 3 4 5 6 7 8 9 101112131415
Number of Iterations
1.00
0.98
0.96
0.94
0.92
0.90
0.88
0.86
0.84
0.82
1 2 3 4 5 6 7 8 9 101112131415
Number of Iterations
baseline
F = 0.626
R = 0.460
P = 0.981
F
R
P
</figure>
<page confidence="0.729341">
1391
</page>
<figureCaption confidence="0.955533">
Figure 9: Graph reinforcement results for Hindi Figure 10: Graph reinforcement results for Tamil
</figureCaption>
<table confidence="0.999003666666667">
Shared Task Proposed Algorithm
English/ P R F P R F
Arabic 0.887 0.945 0.915 0.979 0.905 0.941
Russian 0.880 0.869 0.875 0.921 0.925 0.923
Hindi 0.954 0.895 0.924 0.972 0.895 0.932
Tamil 0.923 0.906 0.914 0.964 0.945 0.955
</table>
<tableCaption confidence="0.949485333333333">
Table 2: Best results obtained in ACL-2010 NEWS TM
shared task compared to graph reinforcement with link
reweighting after 10 iterations
</tableCaption>
<sectionHeader confidence="0.937085" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.9999794">
In this paper, we presented a graph reinforcement
algorithm with link reweighting to improve
transliteration mining recall and precision by
systematically inferring mappings that were unseen
in training. We used the improved technique to
extract transliteration pairs from parallel Wikipedia
titles. The proposed technique solves two problems
in transliteration mining, namely: some mappings
may not be seen in training data – hurting recall,
and certain mappings may not be seen a sufficient
number of times to appropriate estimate mapping
probabilities – hurting precision. The results
showed that graph reinforcement yielded improved
transliteration mining from parallel Wikipedia
titles for all four languages on which the technique
was tested.
Generally iterative graph reinforcement was able to
induce unseen mappings in training data –
improving recall. Link reweighting favored
precision over recall counterbalancing the effect of
graph reinforcement. The proposed system
outperformed the best reported results in the
literature for the ACL-2010 NEWS workshop
shared task for Arabic, Russian, Hindi and Tamil.
To extend the work, we would like to try
transliteration mining from large comparable texts.
The test parts of the NEWS dataset only contained
short parallel fragments. For future work, graph
reinforcement could be extended to MT to improve
the coverage of aligned phrase tables. In doing so,
it is reasonable to assume that there are multiple
ways of expressing a singular concept and hence
multiple translations are possible. Using graph
reinforcement can help discover such translation
though they may never be seen in training data.
Using link reweighting in graph reinforcement can
help demote unlikely translations while promoting
likely ones. This could help clean MT phrase
tables. Further, when dealing with transliteration,
graph reinforcement can help find phonetic
variations within a single language, which can
have interesting applications in spelling correction
and information retrieval. Applying the same to
machine translation phrase tables can help identify
paraphrases automatically.
</bodyText>
<sectionHeader confidence="0.998967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991356097561">
Colin Bannard, Chris Callison-Burch. 2005.
Paraphrasing with Bilingual Parallel Corpora. ACL-
2005, pages 597—604.
Slaven Bilac, Hozumi Tanaka. 2005. Extracting
transliteration pairs from comparable corpora. NLP-
2005.
Eric Brill, Gary Kacmarcik, Chris Brockett. 2001.
Automatically harvesting Katakana-English term
pairs from search engine query logs. NLPRS 2001,
pages 393–399.
Kareem Darwish. 2010. Transliteration Mining with
Phonetic Conflation and Iterative Training. ACL
NEWS workshop 2010.
Huang Fei, Stephan Vogel, and Alex Waibel. 2003.
Extracting Named Entity Translingual Equivalence
with Limited Resources. TALIP, 2(2):124–129.
Xiaodong He. 2007. Using Word-Dependent Transition
Models in HMM based Word Alignment for
Statistical Machine Translation. ACL-07 2nd SMT
workshop.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane
Bergsma, Aditya Bhargava, Qing Dou, Mi-Young
Kim and Grzegorz Kondrak. 2010. Transliteration
Generation and Mining with Limited Training
Resources. ACL NEWS workshop 2010.
Chengguo Jin, Dong-Il Kim, Seung-Hoon Na, Jong-
Hyeok Lee. 2008. Automatic Extraction of English-
Chinese Transliteration Pairs using Dynamic
Window and Tokenizer. Sixth SIGHAN Workshop
on Chinese Language Processing, 2008.
Alexandre Klementiev and Dan Roth. 2006. Named
Entity Transliteration and Discovery from
Multilingual Comparable Corpora. HLT Conf. of the
North American Chapter of the ACL, pages 82–88.
Stanley Kok, Chris Brockett.. 2010. Hitting the Right
Paraphrases in Good Time. Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, June 2010
A. Kumaran, Mitesh M. Khapra, Haizhou Li. 2010.
Report of NEWS 2010 Transliteration Mining Shared
Task. Proceedings of the 2010 Named Entities
</reference>
<page confidence="0.860333">
1392
</page>
<reference confidence="0.999653932432433">
Workshop, ACL 2010, pages 21–28, Uppsala,
Sweden, 16 July 2010.
Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. 2006.
Learning Transliteration Lexicons from the Web.
COLING-ACL2006, Sydney, Australia, 1129 – 1136.
Jin-shea Kuo, Haizhou Li, Ying-kuei Yang. 2007. A
phonetic similarity model for automatic extraction of
transliteration pairs. TALIP, 2007
Jin-Shea Kuo, Haizhou Li, Chih-Lung Lin. 2008.
Mining Transliterations from Web Query Results: An
Incremental Approach. Sixth SIGHAN Workshop on
Chinese Language Processing, 2008.
Jin-shea Kuo, Ying-kuei Yang. 2005. Incorporating
Pronunciation Variation into Extraction of
Transliterated-term Pairs from Web Corpora. Journal
of Chinese Language and Computing, 15 (1): (33-
44).
Chun-Jen Lee, Jason S. Chang. 2003. Acquisition of
English-Chinese transliterated word pairs from
parallel-aligned texts using a statistical machine
transliteration model. Workshop on Building and
Using Parallel Texts, HLT-NAACL-2003, 2003.
Sara Noeman and Amgad Madkour. 2010. Language
Independent Transliteration Mining System Using
Finite State Automata Framework. ACL NEWS
workshop 2010.
R. Mahesh, K. Sinha. 2009. Automated Mining Of
Names Using Parallel Hindi-English Corpus. 7th
Workshop on Asian Language Resources, ACL-
IJCNLP 2009, pages 48–54, 2009.
Jong-Hoon Oh, Key-Sun Choi. 2006. Recognizing
transliteration equivalents for enriching domain
specific thesauri. 3rd Intl. WordNet Conf. (GWC-
06), pages 231–237, 2006.
Jong-Hoon Oh, Hitoshi Isahara. 2006. Mining the Web
for Transliteration Lexicons: Joint-Validation
Approach. pp.254-261, 2006 IEEE/WIC/ACM Intl.
Conf. on Web Intelligence (WI&apos;06), 2006.
Yan Qu, Gregory Grefenstette, David A. Evans. 2003.
Automatic transliteration for Japanese-to-English text
retrieval. SIGIR 2003:353-360
Robert Russell. 1918. Specifications of Letters. US
patent number 1,261,167.
K Saravanan, A Kumaran. 2008. Some Experiments in
Mining Named Entity Transliteration Pairs from
Comparable Corpora. The 2nd Intl. Workshop on
Cross Lingual Information Access: Addressing the
Need of Multilingual Societies, 2008.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
Abhijit Bhole. 2009a. &amp;quot;They Are Out There, If You
Know Where to Look&amp;quot;: Mining Transliterations of
OOV Query Terms for Cross-Language Information
Retrieval. ECIR-2009, Toulouse, France, 2009.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and
Jagadeesh Jagarlamudi. 2009b. MINT: A Method for
Effective and Scalable Mining of Named Entity
Transliterations from Large Comparable Corpora.
EACL 2009.
Raghavendra Udupa and Mitesh Khapra. 2010a.
Transliteration Equivalence using Canonical
Correlation Analysis. ECIR-2010, 2010.
Raghavendra Udupa, Shaishav Kumar. 2010b. Hashing-
based Approaches to Spelling Correction of Personal
Names. EMNLP 2010.
Gae-won You, Seung-won Hwang, Young-In Song,
Long Jiang, Zaiqing Nie. 2010. Mining Name
Translations from Entity Graph Mapping.
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
430–439.
Shiqi Zhao, Haifeng Wang, Ting Liu, Sheng Li. 2008.
Pivot Approach for Extracting Paraphrase Patterns
from Bilingual Corpora. Proceedings of ACL-08:
HLT, pages 780–788.
</reference>
<page confidence="0.978441">
1393
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.262789">
<title confidence="0.958418">Improved Transliteration Mining Using Graph Reinforcement</title>
<author confidence="0.883284">Kareem Ahmed Saad Mohamed Abd</author>
<note confidence="0.756840142857143">Waleed Computing Research Institute, Qatar Foundation, Doha, Qatar Engineering Department, Cairo University, Cairo, Egypt Research, Microsoft, Cairo, Egypt 4Microsoft Research, Microsoft, Redmond, WA, US points for the four language pairs respectively.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<date>2005</date>
<booktitle>Paraphrasing with Bilingual Parallel Corpora. ACL2005,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="11669" citStr="Bannard and Callison-Burch 2005" startWordPosition="1728" endWordPosition="1731">h iterative training was proposed by Darwish (2010). Darwish (2010) reported significant improvements in TM recall at the cost of limited drop in precision. Another method involved expanding character sequence maps by automatically mining transliteration pairs and then aligning these pairs to generate an expanded set of character sequence maps (Fei et al., 2003). In this work we proposed graph 1386 reinforcement with link reweighting to address this problem. Graph reinforcement was used in the context of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity translation extraction (You et al., 2010). Baseline Transliteration Mining 1.4 Description of Baseline System The basic TM setup that we employed in this work used a generative transliteration model, which was trained on a set of transliteration pairs. The training involved automatically aligning character sequences. The alignment was performed using a Bayesian learner that was trained on word dependent transition models for HMM based word alignment (He, 2007). Alignment produced mappings of source character sequences to target character sequences along with the probability o</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard, Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. ACL2005, pages 597—604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Extracting transliteration pairs from comparable corpora.</title>
<date>2005</date>
<pages>2005</pages>
<contexts>
<context position="9331" citStr="Bilac and Tanaka, 2005" startWordPosition="1379" endWordPosition="1382">d in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. They reported the best TM results between English and Arabic with F1- measure of 0.915 on the ACL-2010 NEWS workshop standard TM dataset. A related alternative is to use back-transliteration to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a ―low-dimensional common representation space‖. Then, similarity between the resultant feature vectors for both candidates can be computed. Udupa and Kumar (2010) suggested that mapping to a common space can be performed using context sensitive hashing. They applied their technique to find variant spellings of names. Jiampojamarn et al. (2010) used classification to determine if a source language word and target language word are valid transliterations. They used a variety of</context>
</contexts>
<marker>Bilac, Tanaka, 2005</marker>
<rawString>Slaven Bilac, Hozumi Tanaka. 2005. Extracting transliteration pairs from comparable corpora. NLP2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Gary Kacmarcik</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically harvesting Katakana-English term pairs from search engine query logs. NLPRS</title>
<date>2001</date>
<pages>393--399</pages>
<contexts>
<context position="6456" citStr="Brill et al., 2001" startWordPosition="952" endWordPosition="955">escribes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may not be observed in training data, and conditional probability estimates of obtained 1385 mappings may be inaccurat</context>
<context position="9307" citStr="Brill et al., 2001" startWordPosition="1375" endWordPosition="1378">approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. They reported the best TM results between English and Arabic with F1- measure of 0.915 on the ACL-2010 NEWS workshop standard TM dataset. A related alternative is to use back-transliteration to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a ―low-dimensional common representation space‖. Then, similarity between the resultant feature vectors for both candidates can be computed. Udupa and Kumar (2010) suggested that mapping to a common space can be performed using context sensitive hashing. They applied their technique to find variant spellings of names. Jiampojamarn et al. (2010) used classification to determine if a source language word and target language word are valid transliterations</context>
</contexts>
<marker>Brill, Kacmarcik, Brockett, 2001</marker>
<rawString>Eric Brill, Gary Kacmarcik, Chris Brockett. 2001. Automatically harvesting Katakana-English term pairs from search engine query logs. NLPRS 2001, pages 393–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
</authors>
<title>Transliteration Mining with Phonetic Conflation and Iterative Training.</title>
<date>2010</date>
<booktitle>ACL NEWS workshop</booktitle>
<contexts>
<context position="11088" citStr="Darwish (2010)" startWordPosition="1643" endWordPosition="1644">rseness in the training data). Further, resultant mappings may not be observed a sufficient of times and hence their mapping probabilities may be inaccurate. Different methods were proposed to solve these two problems. These methods focused on making training data less sparse by performing some kind of letter conflation. Oh and Choi (2006) used a SOUNDEX like scheme. SOUNDEX is used to convert English words into a simplified phonetic representation, in which vowels are removed and phonetically similar characters are conflated. A variant of SOUNDEX along with iterative training was proposed by Darwish (2010). Darwish (2010) reported significant improvements in TM recall at the cost of limited drop in precision. Another method involved expanding character sequence maps by automatically mining transliteration pairs and then aligning these pairs to generate an expanded set of character sequence maps (Fei et al., 2003). In this work we proposed graph 1386 reinforcement with link reweighting to address this problem. Graph reinforcement was used in the context of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity t</context>
</contexts>
<marker>Darwish, 2010</marker>
<rawString>Kareem Darwish. 2010. Transliteration Mining with Phonetic Conflation and Iterative Training. ACL NEWS workshop 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huang Fei</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Extracting Named Entity Translingual Equivalence with Limited Resources.</title>
<date>2003</date>
<journal>TALIP,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="6512" citStr="Fei et al., 2003" startWordPosition="961" endWordPosition="964">tiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may not be observed in training data, and conditional probability estimates of obtained 1385 mappings may be inaccurate. These two problems affect recall and precision respec</context>
<context position="8612" citStr="Fei et al., 2003" startWordPosition="1268" endWordPosition="1271">ly equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. They reported the best TM results between English and Arabic with F1- measure of 0.915 on the ACL-2010 NEWS workshop standard TM dataset. A related alternative is to use back-transliteration to determine if one sequence could have been generated</context>
<context position="11401" citStr="Fei et al., 2003" startWordPosition="1687" endWordPosition="1690"> conflation. Oh and Choi (2006) used a SOUNDEX like scheme. SOUNDEX is used to convert English words into a simplified phonetic representation, in which vowels are removed and phonetically similar characters are conflated. A variant of SOUNDEX along with iterative training was proposed by Darwish (2010). Darwish (2010) reported significant improvements in TM recall at the cost of limited drop in precision. Another method involved expanding character sequence maps by automatically mining transliteration pairs and then aligning these pairs to generate an expanded set of character sequence maps (Fei et al., 2003). In this work we proposed graph 1386 reinforcement with link reweighting to address this problem. Graph reinforcement was used in the context of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity translation extraction (You et al., 2010). Baseline Transliteration Mining 1.4 Description of Baseline System The basic TM setup that we employed in this work used a generative transliteration model, which was trained on a set of transliteration pairs. The training involved automatically aligning character sequen</context>
</contexts>
<marker>Fei, Vogel, Waibel, 2003</marker>
<rawString>Huang Fei, Stephan Vogel, and Alex Waibel. 2003. Extracting Named Entity Translingual Equivalence with Limited Resources. TALIP, 2(2):124–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
</authors>
<title>Using Word-Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>ACL-07 2nd SMT workshop.</booktitle>
<contexts>
<context position="12151" citStr="He, 2007" startWordPosition="1801" endWordPosition="1802">t of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity translation extraction (You et al., 2010). Baseline Transliteration Mining 1.4 Description of Baseline System The basic TM setup that we employed in this work used a generative transliteration model, which was trained on a set of transliteration pairs. The training involved automatically aligning character sequences. The alignment was performed using a Bayesian learner that was trained on word dependent transition models for HMM based word alignment (He, 2007). Alignment produced mappings of source character sequences to target character sequences along with the probability of source given target and vice versa. Source character sequences were restricted to be 1 to 3 characters long. For all the work reported herein, given an English-foreign language transliteration candidate pair, English was treated as the target language and the foreign language as the source. Given a foreign source language word sequence Fl and an English target word sequence El&amp;quot;, Fl E Fl could be a potential transliteration of Ej E El&amp;quot; . An example of word sequences pair is th</context>
</contexts>
<marker>He, 2007</marker>
<rawString>Xiaodong He. 2007. Using Word-Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation. ACL-07 2nd SMT workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Kenneth Dwyer</author>
<author>Shane Bergsma</author>
</authors>
<title>Aditya Bhargava, Qing Dou, Mi-Young Kim and Grzegorz Kondrak.</title>
<date>2010</date>
<booktitle>ACL NEWS workshop</booktitle>
<contexts>
<context position="9796" citStr="Jiampojamarn et al. (2010)" startWordPosition="1447" endWordPosition="1450"> if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a ―low-dimensional common representation space‖. Then, similarity between the resultant feature vectors for both candidates can be computed. Udupa and Kumar (2010) suggested that mapping to a common space can be performed using context sensitive hashing. They applied their technique to find variant spellings of names. Jiampojamarn et al. (2010) used classification to determine if a source language word and target language word are valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, and character n-gram similarity. They reported the best results for Russian, Tamil, and Hindi with F1- measure of 0.875, 0.924, and 0.914 respectively on the ACL-2010 NEWS workshop standard TM datasets. 1.3 Training with Limited Training Data When only limited training data is available to train a charac</context>
<context position="26866" citStr="Jiampojamarn et al. 2010" startWordPosition="4110" endWordPosition="4113">y labeled as false in the test set; 6 were phonetically similar like &amp;quot;اينابسا&amp;quot; (pronounced espanya) and &amp;quot;Spain&amp;quot; and &amp;quot;ايجولونكتلا&amp;quot; (pronounced alteknologya) and &amp;quot;technology&amp;quot;; and the remaining 2 were actually wrong, which were &amp;quot;يشتيب&amp;quot; (pronounced beatchi) and &amp;quot;medici&amp;quot; and &amp;quot; يديس&amp;quot; (pronounced sidi) and &amp;quot;taya&amp;quot;. This seems to indicate that graph reinforcement generally introduced more proper mappings than improper ones. 1.10 Comparing to the State-of-the-Art Table 2 compares the best reported results in ACL2010 NEWS TM shared task for Arabic (Noeman and Madkour, 2010) and for the other languages (Jiampojamarn et al. 2010) and the results obtained by the proposed technique using 10 iterations, with link reweighting. The comparison shows that the proposed algorithm yielded better results than the best reported results in the literature by 2.6, 4.8, 0.8 and 4.1 F1-measure points in Arabic, Russian, Hindi and Tamil respectively. For Arabic, the improvement over the previously reported result was due to improvement in precision, while for the other languages the improvements were due to improvements in both recall and precision. 1390 F R P 1.000 0.900 0.800 0.700 0.600 0.500 0.400 5 Iterations 1.000 5 Iterations F </context>
</contexts>
<marker>Jiampojamarn, Dwyer, Bergsma, 2010</marker>
<rawString>Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma, Aditya Bhargava, Qing Dou, Mi-Young Kim and Grzegorz Kondrak. 2010. Transliteration Generation and Mining with Limited Training Resources. ACL NEWS workshop 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengguo Jin</author>
<author>Dong-Il Kim</author>
<author>Seung-Hoon Na</author>
<author>JongHyeok Lee</author>
</authors>
<title>Automatic Extraction of EnglishChinese Transliteration Pairs using Dynamic Window and Tokenizer.</title>
<date>2008</date>
<booktitle>Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<contexts>
<context position="6272" citStr="Jin et al. 2008" startWordPosition="925" endWordPosition="928">ngs, which are often erroneous. This positively affects precision. The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possibl</context>
</contexts>
<marker>Jin, Kim, Na, Lee, 2008</marker>
<rawString>Chengguo Jin, Dong-Il Kim, Seung-Hoon Na, JongHyeok Lee. 2008. Automatic Extraction of EnglishChinese Transliteration Pairs using Dynamic Window and Tokenizer. Sixth SIGHAN Workshop on Chinese Language Processing, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Named Entity Transliteration and Discovery from Multilingual Comparable Corpora.</title>
<date>2006</date>
<booktitle>HLT Conf. of the North American Chapter of the ACL,</booktitle>
<pages>82--88</pages>
<contexts>
<context position="6585" citStr="Klementiev and Roth, 2006" startWordPosition="972" endWordPosition="975">along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may not be observed in training data, and conditional probability estimates of obtained 1385 mappings may be inaccurate. These two problems affect recall and precision respectively. 1.1 Finding Character Mappings To find character sequence mapping</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Named Entity Transliteration and Discovery from Multilingual Comparable Corpora. HLT Conf. of the North American Chapter of the ACL, pages 82–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Chris Brockett</author>
</authors>
<title>Hitting the Right Paraphrases in Good Time. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the ACL,</booktitle>
<contexts>
<context position="11635" citStr="Kok and Brockett, 2010" startWordPosition="1724" endWordPosition="1727">ant of SOUNDEX along with iterative training was proposed by Darwish (2010). Darwish (2010) reported significant improvements in TM recall at the cost of limited drop in precision. Another method involved expanding character sequence maps by automatically mining transliteration pairs and then aligning these pairs to generate an expanded set of character sequence maps (Fei et al., 2003). In this work we proposed graph 1386 reinforcement with link reweighting to address this problem. Graph reinforcement was used in the context of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity translation extraction (You et al., 2010). Baseline Transliteration Mining 1.4 Description of Baseline System The basic TM setup that we employed in this work used a generative transliteration model, which was trained on a set of transliteration pairs. The training involved automatically aligning character sequences. The alignment was performed using a Bayesian learner that was trained on word dependent transition models for HMM based word alignment (He, 2007). Alignment produced mappings of source character sequences to target character sequ</context>
</contexts>
<marker>Kok, Brockett, 2010</marker>
<rawString>Stanley Kok, Chris Brockett.. 2010. Hitting the Right Paraphrases in Good Time. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, June 2010</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Mitesh M Khapra</author>
<author>Haizhou Li</author>
</authors>
<date>2010</date>
<booktitle>Report of NEWS 2010 Transliteration Mining Shared Task. Proceedings of the 2010 Named Entities</booktitle>
<contexts>
<context position="1014" citStr="Kumaran, et al. 2010" startWordPosition="122" endWordPosition="125">lkahky,kdarwish}@qf.org.qa1, asaadaldien@hotmail.com2, ahmed.s.hefny@gmail.com2, t-momah@microsoft.com3, i-waamma@microsoft.com4 points for the four language pairs respectively. Introduction Transliteration Mining (TM) is the process of finding transliterated word pairs in parallel or comparable corpora. TM has many potential applications such as mining training data for transliteration, improving lexical coverage for machine translation, and cross language retrieval via translation resource expansion. TM has been gaining some attention lately with a shared task in the ACL 2010 NEWS workshop (Kumaran, et al. 2010). One popular statistical TM approach is performed in two stages. First, a generative model is trained by performing automatic character level alignment of parallel transliterated word pairs to find character segment mappings between source and target languages. Second, given comparable or parallel text, the trained generative model is used to generate possible transliterations of a word in the source language while constraining the transliterations to words that exist in the target language. However, two problems arise in this approach: 1. Many possible character sequence mappings between sou</context>
<context position="5119" citStr="Kumaran et al., 2010" startWordPosition="744" endWordPosition="747"> probability. The second problem has to do primarily with some characters in one language, typically vowels, mapping to many character sequences in the other language, with some of these mappings assuming very high probabilities (due to limited training data). To overcome this problem, we used link reweighting in graph reinforcement to scale down the likelihood of mappings to target character sequences in proportion to how many source sequences map to them. We tested the proposed method using the ACL 2010 NEWS workshop data for English-Arabic, English-Russian, English-Hindi, and EnglishTamil (Kumaran et al., 2010). For each language pair, the standard ACL 2010 NEWS workshop data contained a base set of 1,000 transliteration pairs for training, and set of 1,000 parallel Wikipedia titles for testing. The contributions of the paper are: 1. Employing graph reinforcement to improve the coverage of automatically aligned data – as they apply to transliteration mining. This positively affects recall. 2. Applying link reweighting to overcome situations where certain tokens – character sequences in the case of transliteration – tend to have many mappings, which are often erroneous. This positively affects precis</context>
</contexts>
<marker>Kumaran, Khapra, Li, 2010</marker>
<rawString>A. Kumaran, Mitesh M. Khapra, Haizhou Li. 2010. Report of NEWS 2010 Transliteration Mining Shared Task. Proceedings of the 2010 Named Entities</rawString>
</citation>
<citation valid="true">
<authors>
<author>ACL Workshop</author>
</authors>
<date>2010</date>
<pages>21--28</pages>
<location>Uppsala,</location>
<marker>Workshop, 2010</marker>
<rawString>Workshop, ACL 2010, pages 21–28, Uppsala, Sweden, 16 July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Shea Kuo</author>
<author>Haizhou Li</author>
<author>Ying-Kuei Yang</author>
</authors>
<date>2006</date>
<booktitle>Learning Transliteration Lexicons from the Web. COLING-ACL2006,</booktitle>
<pages>1136</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6219" citStr="Kuo et al., 2006" startWordPosition="913" endWordPosition="916"> the case of transliteration – tend to have many mappings, which are often erroneous. This positively affects precision. The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteratio</context>
</contexts>
<marker>Kuo, Li, Yang, 2006</marker>
<rawString>Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. 2006. Learning Transliteration Lexicons from the Web. COLING-ACL2006, Sydney, Australia, 1129 – 1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-shea Kuo</author>
<author>Haizhou Li</author>
<author>Ying-kuei Yang</author>
</authors>
<title>A phonetic similarity model for automatic extraction of transliteration pairs. TALIP,</title>
<date>2007</date>
<contexts>
<context position="6237" citStr="Kuo et al., 2007" startWordPosition="917" endWordPosition="920">literation – tend to have many mappings, which are often erroneous. This positively affects precision. The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two addit</context>
</contexts>
<marker>Kuo, Li, Yang, 2007</marker>
<rawString>Jin-shea Kuo, Haizhou Li, Ying-kuei Yang. 2007. A phonetic similarity model for automatic extraction of transliteration pairs. TALIP, 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Shea Kuo</author>
<author>Haizhou Li</author>
<author>Chih-Lung Lin</author>
</authors>
<title>Mining Transliterations from Web Query Results: An Incremental Approach.</title>
<date>2008</date>
<booktitle>Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<contexts>
<context position="6255" citStr="Kuo et al., 2008" startWordPosition="921" endWordPosition="924">to have many mappings, which are often erroneous. This positively affects precision. The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems app</context>
<context position="7710" citStr="Kuo et al., 2008" startWordPosition="1133" endWordPosition="1136">d precision respectively. 1.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Akin to phrasal alignment in machine translation, character sequence alignment is treated as a word alignment problem between parallel sentences, where transliteration pairs are treated as if they are parallel sentences and the characters from which they are composed are treated as if they are words. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or HMM based alignment (Udupa et al., 2009a; Udupa et al., 2009b). In this paper, we use automatic character alignment between transliteration pairs using an HMM aligner. Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if t</context>
</contexts>
<marker>Kuo, Li, Lin, 2008</marker>
<rawString>Jin-Shea Kuo, Haizhou Li, Chih-Lung Lin. 2008. Mining Transliterations from Web Query Results: An Incremental Approach. Sixth SIGHAN Workshop on Chinese Language Processing, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-shea Kuo</author>
<author>Ying-kuei Yang</author>
</authors>
<title>Incorporating Pronunciation Variation into Extraction of Transliterated-term Pairs from Web Corpora.</title>
<date>2005</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>15</volume>
<issue>1</issue>
<pages>33--44</pages>
<contexts>
<context position="8117" citStr="Kuo and Yang, 2005" startWordPosition="1192" endWordPosition="1195"> are parallel sentences and the characters from which they are composed are treated as if they are words. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or HMM based alignment (Udupa et al., 2009a; Udupa et al., 2009b). In this paper, we use automatic character alignment between transliteration pairs using an HMM aligner. Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in thi</context>
</contexts>
<marker>Kuo, Yang, 2005</marker>
<rawString>Jin-shea Kuo, Ying-kuei Yang. 2005. Incorporating Pronunciation Variation into Extraction of Transliterated-term Pairs from Web Corpora. Journal of Chinese Language and Computing, 15 (1): (33-44).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Jen Lee</author>
<author>Jason S Chang</author>
</authors>
<title>Acquisition of English-Chinese transliterated word pairs from parallel-aligned texts using a statistical machine transliteration model. Workshop on Building and Using Parallel Texts,</title>
<date>2003</date>
<contexts>
<context position="7732" citStr="Lee and Chang, 2003" startWordPosition="1137" endWordPosition="1140">tively. 1.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Akin to phrasal alignment in machine translation, character sequence alignment is treated as a word alignment problem between parallel sentences, where transliteration pairs are treated as if they are parallel sentences and the characters from which they are composed are treated as if they are words. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or HMM based alignment (Udupa et al., 2009a; Udupa et al., 2009b). In this paper, we use automatic character alignment between transliteration pairs using an HMM aligner. Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transl</context>
</contexts>
<marker>Lee, Chang, 2003</marker>
<rawString>Chun-Jen Lee, Jason S. Chang. 2003. Acquisition of English-Chinese transliterated word pairs from parallel-aligned texts using a statistical machine transliteration model. Workshop on Building and Using Parallel Texts, HLT-NAACL-2003, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Noeman</author>
<author>Amgad Madkour</author>
</authors>
<title>Language Independent Transliteration Mining System Using Finite State Automata Framework.</title>
<date>2010</date>
<booktitle>ACL NEWS workshop</booktitle>
<contexts>
<context position="8751" citStr="Noeman and Madkour (2010)" startWordPosition="1293" endWordPosition="1296">tively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. They reported the best TM results between English and Arabic with F1- measure of 0.915 on the ACL-2010 NEWS workshop standard TM dataset. A related alternative is to use back-transliteration to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 20</context>
<context position="26811" citStr="Noeman and Madkour, 2010" startWordPosition="4101" endWordPosition="4104"> we found that: 9 were actually correct, but erroneously labeled as false in the test set; 6 were phonetically similar like &amp;quot;اينابسا&amp;quot; (pronounced espanya) and &amp;quot;Spain&amp;quot; and &amp;quot;ايجولونكتلا&amp;quot; (pronounced alteknologya) and &amp;quot;technology&amp;quot;; and the remaining 2 were actually wrong, which were &amp;quot;يشتيب&amp;quot; (pronounced beatchi) and &amp;quot;medici&amp;quot; and &amp;quot; يديس&amp;quot; (pronounced sidi) and &amp;quot;taya&amp;quot;. This seems to indicate that graph reinforcement generally introduced more proper mappings than improper ones. 1.10 Comparing to the State-of-the-Art Table 2 compares the best reported results in ACL2010 NEWS TM shared task for Arabic (Noeman and Madkour, 2010) and for the other languages (Jiampojamarn et al. 2010) and the results obtained by the proposed technique using 10 iterations, with link reweighting. The comparison shows that the proposed algorithm yielded better results than the best reported results in the literature by 2.6, 4.8, 0.8 and 4.1 F1-measure points in Arabic, Russian, Hindi and Tamil respectively. For Arabic, the improvement over the previously reported result was due to improvement in precision, while for the other languages the improvements were due to improvements in both recall and precision. 1390 F R P 1.000 0.900 0.800 0.7</context>
</contexts>
<marker>Noeman, Madkour, 2010</marker>
<rawString>Sara Noeman and Amgad Madkour. 2010. Language Independent Transliteration Mining System Using Finite State Automata Framework. ACL NEWS workshop 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mahesh</author>
<author>K Sinha</author>
</authors>
<title>Automated Mining Of Names Using Parallel Hindi-English Corpus.</title>
<date>2009</date>
<booktitle>7th Workshop on Asian Language Resources, ACLIJCNLP</booktitle>
<pages>48--54</pages>
<contexts>
<context position="6537" citStr="Mahesh and Sinha, 2009" startWordPosition="965" endWordPosition="968">4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may not be observed in training data, and conditional probability estimates of obtained 1385 mappings may be inaccurate. These two problems affect recall and precision respectively. 1.1 Finding Chara</context>
</contexts>
<marker>Mahesh, Sinha, 2009</marker>
<rawString>R. Mahesh, K. Sinha. 2009. Automated Mining Of Names Using Parallel Hindi-English Corpus. 7th Workshop on Asian Language Resources, ACLIJCNLP 2009, pages 48–54, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>Recognizing transliteration equivalents for enriching domain specific thesauri. 3rd Intl. WordNet Conf.</title>
<date>2006</date>
<pages>231--237</pages>
<contexts>
<context position="6401" citStr="Oh and Choi, 2006" startWordPosition="943" endWordPosition="946">rveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may not be observed in training data, and conditional probabili</context>
<context position="8245" citStr="Oh and Choi, 2006" startWordPosition="1211" endWordPosition="1214"> be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or HMM based alignment (Udupa et al., 2009a; Udupa et al., 2009b). In this paper, we use automatic character alignment between transliteration pairs using an HMM aligner. Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible translit</context>
<context position="10815" citStr="Oh and Choi (2006)" startWordPosition="1600" endWordPosition="1603">sure of 0.875, 0.924, and 0.914 respectively on the ACL-2010 NEWS workshop standard TM datasets. 1.3 Training with Limited Training Data When only limited training data is available to train a character mapping model, the resultant mappings are typically incomplete (due to sparseness in the training data). Further, resultant mappings may not be observed a sufficient of times and hence their mapping probabilities may be inaccurate. Different methods were proposed to solve these two problems. These methods focused on making training data less sparse by performing some kind of letter conflation. Oh and Choi (2006) used a SOUNDEX like scheme. SOUNDEX is used to convert English words into a simplified phonetic representation, in which vowels are removed and phonetically similar characters are conflated. A variant of SOUNDEX along with iterative training was proposed by Darwish (2010). Darwish (2010) reported significant improvements in TM recall at the cost of limited drop in precision. Another method involved expanding character sequence maps by automatically mining transliteration pairs and then aligning these pairs to generate an expanded set of character sequence maps (Fei et al., 2003). In this work</context>
</contexts>
<marker>Oh, Choi, 2006</marker>
<rawString>Jong-Hoon Oh, Key-Sun Choi. 2006. Recognizing transliteration equivalents for enriching domain specific thesauri. 3rd Intl. WordNet Conf. (GWC06), pages 231–237, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Mining the Web for Transliteration Lexicons: Joint-Validation Approach.</title>
<date>2006</date>
<booktitle>IEEE/WIC/ACM Intl. Conf. on Web Intelligence (WI&apos;06),</booktitle>
<pages>254--261</pages>
<contexts>
<context position="6381" citStr="Oh and Isahara, 2006" startWordPosition="939" endWordPosition="942"> follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may not be observed in training data, and c</context>
<context position="9354" citStr="Oh and Isahara, 2006" startWordPosition="1383" endWordPosition="1386">and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. They reported the best TM results between English and Arabic with F1- measure of 0.915 on the ACL-2010 NEWS workshop standard TM dataset. A related alternative is to use back-transliteration to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a ―low-dimensional common representation space‖. Then, similarity between the resultant feature vectors for both candidates can be computed. Udupa and Kumar (2010) suggested that mapping to a common space can be performed using context sensitive hashing. They applied their technique to find variant spellings of names. Jiampojamarn et al. (2010) used classification to determine if a source language word and target language word are valid transliterations. They used a variety of features including edi</context>
</contexts>
<marker>Oh, Isahara, 2006</marker>
<rawString>Jong-Hoon Oh, Hitoshi Isahara. 2006. Mining the Web for Transliteration Lexicons: Joint-Validation Approach. pp.254-261, 2006 IEEE/WIC/ACM Intl. Conf. on Web Intelligence (WI&apos;06), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Qu</author>
<author>Gregory Grefenstette</author>
<author>David A Evans</author>
</authors>
<title>Automatic transliteration for Japanese-to-English text retrieval.</title>
<date>2003</date>
<journal>SIGIR</journal>
<pages>2003--353</pages>
<marker>Qu, Grefenstette, Evans, 2003</marker>
<rawString>Yan Qu, Gregory Grefenstette, David A. Evans. 2003. Automatic transliteration for Japanese-to-English text retrieval. SIGIR 2003:353-360</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Russell</author>
</authors>
<date>1918</date>
<booktitle>Specifications of Letters. US patent number</booktitle>
<pages>1--261</pages>
<marker>Russell, 1918</marker>
<rawString>Robert Russell. 1918. Specifications of Letters. US patent number 1,261,167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Saravanan</author>
<author>A Kumaran</author>
</authors>
<title>Some Experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora.</title>
<date>2008</date>
<booktitle>The 2nd Intl. Workshop on Cross Lingual Information Access: Addressing the Need of Multilingual Societies,</booktitle>
<contexts>
<context position="6318" citStr="Saravanan and Kumaran, 2008" startWordPosition="930" endWordPosition="933">is positively affects precision. The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source a</context>
</contexts>
<marker>Saravanan, Kumaran, 2008</marker>
<rawString>K Saravanan, A Kumaran. 2008. Some Experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora. The 2nd Intl. Workshop on Cross Lingual Information Access: Addressing the Need of Multilingual Societies, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>K Saravanan</author>
</authors>
<title>Anton Bakalov, Abhijit Bhole.</title>
<date>2009</date>
<booktitle>ECIR-2009,</booktitle>
<location>Toulouse, France,</location>
<marker>Udupa, Saravanan, 2009</marker>
<rawString>Raghavendra Udupa, K. Saravanan, Anton Bakalov, Abhijit Bhole. 2009a. &amp;quot;They Are Out There, If You Know Where to Look&amp;quot;: Mining Transliterations of OOV Query Terms for Cross-Language Information Retrieval. ECIR-2009, Toulouse, France, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>K Saravanan</author>
<author>A Kumaran</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>MINT: A Method for Effective and Scalable Mining of Named Entity Transliterations from Large Comparable Corpora. EACL</title>
<date>2009</date>
<contexts>
<context position="7775" citStr="Udupa et al., 2009" startWordPosition="1145" endWordPosition="1148">d character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Akin to phrasal alignment in machine translation, character sequence alignment is treated as a word alignment problem between parallel sentences, where transliteration pairs are treated as if they are parallel sentences and the characters from which they are composed are treated as if they are words. Automatic alignment can be performed using different algorithms such as the EM algorithm (Kuo et al., 2008; Lee and Chang, 2003) or HMM based alignment (Udupa et al., 2009a; Udupa et al., 2009b). In this paper, we use automatic character alignment between transliteration pairs using an HMM aligner. Another method is to use automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Oh and Choi, 2006). 1.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach</context>
</contexts>
<marker>Udupa, Saravanan, Kumaran, Jagarlamudi, 2009</marker>
<rawString>Raghavendra Udupa, K. Saravanan, A. Kumaran, and Jagadeesh Jagarlamudi. 2009b. MINT: A Method for Effective and Scalable Mining of Named Entity Transliterations from Large Comparable Corpora. EACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>Mitesh Khapra</author>
</authors>
<title>Transliteration Equivalence using Canonical Correlation Analysis.</title>
<date>2010</date>
<contexts>
<context position="6343" citStr="Udupa and Khapra, 2010" startWordPosition="934" endWordPosition="937">on. The rest of the paper is organized as follows: Section 2 surveys prior work on transliteration mining; Section 3 describes the baseline TM approach and reports on its effectiveness; Section 4 describes the proposed graph reinforcement along with link reweighting and reports on the observed improvements; and Section 5 concludes the paper. Figure 1: Example mappings seen in training Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves two main tasks, namely: finding character mappings between two languages, and given the mappings ascertaining whether two words are transliterations or not. When training with a limited number of transliteration pairs, two additional problems appear: many possible character sequence mappings between source and target languages may n</context>
<context position="9379" citStr="Udupa and Khapra (2010)" startWordPosition="1387" endWordPosition="1390">emented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. They reported the best TM results between English and Arabic with F1- measure of 0.915 on the ACL-2010 NEWS workshop standard TM dataset. A related alternative is to use back-transliteration to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a ―low-dimensional common representation space‖. Then, similarity between the resultant feature vectors for both candidates can be computed. Udupa and Kumar (2010) suggested that mapping to a common space can be performed using context sensitive hashing. They applied their technique to find variant spellings of names. Jiampojamarn et al. (2010) used classification to determine if a source language word and target language word are valid transliterations. They used a variety of features including edit distance between an Eng</context>
</contexts>
<marker>Udupa, Khapra, 2010</marker>
<rawString>Raghavendra Udupa and Mitesh Khapra. 2010a. Transliteration Equivalence using Canonical Correlation Analysis. ECIR-2010, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
</authors>
<title>Shaishav Kumar. 2010b. Hashingbased Approaches to Spelling Correction of Personal Names.</title>
<date>2010</date>
<publisher>EMNLP</publisher>
<marker>Udupa, 2010</marker>
<rawString>Raghavendra Udupa, Shaishav Kumar. 2010b. Hashingbased Approaches to Spelling Correction of Personal Names. EMNLP 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gae-won You</author>
</authors>
<title>Seung-won Hwang, Young-In Song, Long Jiang, Zaiqing Nie.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>430--439</pages>
<marker>You, 2010</marker>
<rawString>Gae-won You, Seung-won Hwang, Young-In Song, Long Jiang, Zaiqing Nie. 2010. Mining Name Translations from Entity Graph Mapping. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 430–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>780--788</pages>
<contexts>
<context position="11611" citStr="Zhao et al., 2008" startWordPosition="1720" endWordPosition="1723">e conflated. A variant of SOUNDEX along with iterative training was proposed by Darwish (2010). Darwish (2010) reported significant improvements in TM recall at the cost of limited drop in precision. Another method involved expanding character sequence maps by automatically mining transliteration pairs and then aligning these pairs to generate an expanded set of character sequence maps (Fei et al., 2003). In this work we proposed graph 1386 reinforcement with link reweighting to address this problem. Graph reinforcement was used in the context of different problems such as mining paraphrases (Zhao et al., 2008; Kok and Brockett, 2010; Bannard and Callison-Burch 2005) and named entity translation extraction (You et al., 2010). Baseline Transliteration Mining 1.4 Description of Baseline System The basic TM setup that we employed in this work used a generative transliteration model, which was trained on a set of transliteration pairs. The training involved automatically aligning character sequences. The alignment was performed using a Bayesian learner that was trained on word dependent transition models for HMM based word alignment (He, 2007). Alignment produced mappings of source character sequences </context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, Sheng Li. 2008. Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora. Proceedings of ACL-08: HLT, pages 780–788.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>