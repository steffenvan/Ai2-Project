<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001580">
<title confidence="0.998427">
Extracting Bilingual Dictionary from Comparable Corpora with
Dependency Heterogeneity
</title>
<author confidence="0.991246">
Kun Yu Junichi Tsujii
</author>
<affiliation confidence="0.9875665">
Graduate School of Information Science and Technology
The University of Tokyo
</affiliation>
<address confidence="0.776596">
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
</address>
<email confidence="0.989157">
{kunyu, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.994596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995199">
This paper proposes an approach for bilingual
dictionary extraction from comparable corpora.
The proposed approach is based on the obser-
vation that a word and its translation share
similar dependency relations. Experimental re-
sults using 250 randomly selected translation
pairs prove that the proposed approach signifi-
cantly outperforms the traditional context-
based approach that uses bag-of-words around
translation candidates.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998398">
Bilingual dictionary plays an important role in many
natural language processing tasks. For example, ma-
chine translation uses bilingual dictionary to reinforce
word and phrase alignment (Och and Ney, 2003), cross-
language information retrieval uses bilingual dictionary
for query translation (Grefenstette, 1998). The direct
way of bilingual dictionary acquisition is aligning trans-
lation candidates using parallel corpora (Wu, 1994). But
for some languages, collecting parallel corpora is not
easy. Therefore, many researchers paid attention to bi-
lingual dictionary extraction from comparable corpora
(Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and
Morin, 2008; Robitaille et al., 2006; Morin et al., 2007;
Otero, 2008), in which texts are not exact translation of
each other but share common features.
Context-based approach, which is based on the ob-
servation that a term and its translation appear in similar
lexical contexts (Daille and Morin, 2008), is the most
popular approach for extracting bilingual dictionary
from comparable corpora and has shown its effective-
ness in terminology extraction (Fung, 2000; Chiao and
Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al.,
2007). But it only concerns about the lexical context
around translation candidates in a restricted window.
Besides, in comparable corpora, some words may appear
in similar context even if they are not translation of each
other. For example, using a Chinese-English comparable
corpus from Wikipedia and following the definition in
(Fung, 1995), we get context heterogeneity vector of
three words (see Table 1). The Euclidean distance be-
tween the vector of ‘经济学(economics)’ and ‘econom-
ics’ is 0.084. But the Euclidean distance between the
vector of ‘经济学’ and ‘medicine’ is 0.075. In such
case, the incorrect dictionary entry ‘经济学/medicine’
will be extracted by context-based approach.
</bodyText>
<tableCaption confidence="0.999439">
Table 1. Context heterogeneity vector of words.
</tableCaption>
<table confidence="0.78467275">
Word Context Heterogeneity Vector
经济学(economics) (0.185, 0.006)
economics (0.101, 0.013)
medicine (0.113,0.028)
</table>
<bodyText confidence="0.999828190476191">
To solve this problem, we investigate a comparable
corpora from Wikipedia and find the following phe-
nomenon: if we preprocessed the corpora with a de-
pendency syntactic analyzer, a word in source language
shares similar head and modifiers with its translation in
target language, no matter whether they occur in similar
context or not. We call this phenomenon as dependency
heterogeneity. Based on this observation, we propose an
approach to extract bilingual dictionary from compara-
ble corpora. Not like only using bag-of-words around
translation candidates in context-based approach, the
proposed approach utilizes the syntactic analysis of
comparable corpora to recognize the meaning of transla-
tion candidates. Besides, the lexical information used in
the proposed approach does not restrict in a small win-
dow, but comes from the entire sentence.
We did experiments with 250 randomly selected
translation pairs. Results show that compared with the
approach based on context heterogeneity, the proposed
approach improves the accuracy of dictionary extraction
significantly.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998621461538462">
In previous work about dictionary extraction from com-
parable corpora, using context similarity is the most
popular one.
At first, Fung (1995) utilized context heterogeneity
for bilingual dictionary extraction. Our proposed ap-
proach borrows Fung’s idea but extends context hetero-
geneity to dependency heterogeneity, in order to utilize
rich syntactic information other than bag-of-words.
After that, researchers extended context heterogeneity
vector to context vector with the aid of an existing bilin-
gual dictionary (Fung, 2000; Chiao and Zweigenbaum,
2002; Robitaille et al., 2006; Morin et al., 2007; Daille
and Morin, 2008). In these works, dictionary extraction
</bodyText>
<page confidence="0.992865">
121
</page>
<note confidence="0.3587525">
Proceedings of NAACL HLT 2009: Short Papers, pages 121–124,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999974485714286">
is fulfilled by comparing the similarity between the con-
text vectors of words in target language and the context
vectors of words in source language using an external
dictionary. The main difference between these works
and our approach is still our usage of syntactic depend-
ency other than bag-of-words. In addition, except for a
morphological analyzer and a dependency parser, our
approach does not need other external resources, such as
the external dictionary. Because of the well-developed
morphological and syntactic analysis research in recent
years, the requirement of analyzers will not bring too
much burden to the proposed approach.
Besides of using window-based contexts, there were
also some works utilizing syntactic information for bi-
lingual dictionary extraction. Otero (2007) extracted
lexico-syntactic templates from parallel corpora first,
and then used them as seeds to calculate similarity be-
tween translation candidates. Otero (2008) defined syn-
tactic rules to get lexico-syntactic contexts of words, and
then used an external bilingual dictionary to fulfill simi-
larity calculation between the lexico-syntactic context
vectors of translation candidates. Our approach differs
from these works in two ways: (1) both the above works
defined syntactic rules or templates by hand to get syn-
tactic information. Our approach uses data-driven syn-
tactic analyzers for acquiring dependency relations
automatically. Therefore, it is easier to adapt our ap-
proach to other language pairs. (2) the types of depend-
encies used for similarity calculation in our approach are
different from Otero’s work. Otero (2007; 2008) only
considered about the modification dependency among
nouns, prepositions and verbs, such as the adjective
modifier of nouns and the object of verbs. But our ap-
proach not only uses modifiers of translation candidates,
but also considers about their heads.
</bodyText>
<sectionHeader confidence="0.775847" genericHeader="method">
3 Dependency Heterogeneity of Words in
Comparable Corpora
</sectionHeader>
<bodyText confidence="0.99915525">
Dependency heterogeneity means a word and its trans-
lation share similar modifiers and head in comparable
corpora. Namely, the modifiers and head of unrelated
words are different even if they occur in similar context.
</bodyText>
<tableCaption confidence="0.999662">
Table 2. Frequently used modifiers (words are not ranked).
</tableCaption>
<table confidence="0.989064181818182">
ff.VF1E(economics) economics medicine
�*U&amp;quot;/micro keynesian physiology
P4U&amp;quot;/macro new Chinese
iM/computation institutional traditional
Allnew positive biology
SIMM/politics classical internal
)C1E/university labor science
ti&apos;,*f/classicists development clinical
&amp;W/development engineering veterinary
ffiMheory finance western
933/demonstration international agriculture
</table>
<bodyText confidence="0.9994141">
For example, Table 2 collects the most frequently
used 10 modifiers of the words listed in Table 1. It
shows there are 3 similar modifiers (italic words) be-
tween ‘441E(economics)’ and ‘economics’. But there
is no similar word between the modifiers of ‘441E’
and that of ‘medicine’. Table 3 lists the most frequently
used 10 heads (when a candidate word acts as subject)
of the three words. If excluding copula, ‘441E’ and
‘economics’ share one similar head (italic words). But
‘0_41E’ and ’medicine’ shares no similar head.
</bodyText>
<tableCaption confidence="0.9968395">
Table 3. Frequently used heads
the predicate of subject, words are not ranked).
</tableCaption>
<table confidence="0.995940181818182">
ff.VF1E(economics) economics medicine
)/is is is
1iM/average has tends
*graduate was include
*-Lk/admit emphasizes moved
M/can non-rivaled means
ffft/split became requires
01-F/leave assume includes
Wcompare relies were
AVbecome can has
M-S/emphasize replaces may
</table>
<sectionHeader confidence="0.8650135" genericHeader="method">
4 Bilingual Dictionary Extraction with De-
pendency Heterogeneity
</sectionHeader>
<bodyText confidence="0.99958175">
Based on the observation of dependency heterogeneity
in comparable corpora, we propose an approach to ex-
tract bilingual dictionary using dependency heterogene-
ity similarity.
</bodyText>
<subsectionHeader confidence="0.989829">
4.1 Comparable Corpora Preprocessing
</subsectionHeader>
<bodyText confidence="0.999953571428571">
Before calculating dependency heterogeneity similarity,
we need to preprocess the comparable corpora. In this
work, we focus on Chinese-English bilingual dictionary
extraction for single-nouns. Therefore, we first use a
Chinese morphological analyzer (Nakagawa and Uchi-
moto, 2007) and an English pos-tagger (Tsuruoka et al.,
2005) to analyze the raw corpora. Then we use Malt-
Parser (Nivre et al., 2007) to get syntactic dependency of
both the Chinese corpus and the English corpus. The
dependency labels produced by MaltParser (e.g. SUB)
are used to decide the type of heads and modifiers.
After that, the analyzed corpora are refined through
following steps: (1) we use a stemmer1 to do stemming
for the English corpus. Considering that only nouns are
treated as translation candidates, we use stems for trans-
lation candidate but keep the original form of their heads
and modifiers in order to avoid excessive stemming. (2)
stop words are removed. For English, we use the stop
word list from (Fung, 1995). For Chinese, we remove
‘M(of)’ as stop word. (3) we remove the dependencies
including punctuations and remove the sentences with
</bodyText>
<footnote confidence="0.958122">
1 http://search.cpan.org/~snowhare/Lingua-Stem-0.83/
</footnote>
<page confidence="0.991989">
122
</page>
<bodyText confidence="0.999971">
more than k (set as 30 empirically) words from both
English corpus and Chinese corpus, in order to reduce
the effect of parsing error on dictionary extraction.
</bodyText>
<subsectionHeader confidence="0.981754">
4.2 Dependency Heterogeneity Vector Calculation
</subsectionHeader>
<bodyText confidence="0.960943461538462">
Equation 1 shows the definition of dependency hetero-
geneity vector of a word W. It includes four elements.
Each element represents the heterogeneity of a depend-
ency relation. ‘NMOD’ (noun modifier), ‘SUB’ (sub-
ject) and ‘OBJ’ (object) are the dependency labels
produced by MaltParser.
(HNMODHead ,HSUBHead ,HOBJHead ,HNMODMod ) (1)
number of different heads of W with NMOD label
HNMODHead (W ) = total number of heads of W with NMOD label
number of different heads of W with SUB label
HOBJHead(W ) = total number of heads of W with OBJ label
number of different modifiers of W with NMOD label
HNMODMod (W ) = total number of modifiers of W with NMOD label
</bodyText>
<subsectionHeader confidence="0.997732">
4.3 Bilingual Dictionary Extraction
</subsectionHeader>
<bodyText confidence="0.999974125">
After calculating dependency heterogeneity vector of
translation candidates, bilingual dictionary entries are
extracted according to the distance between the vector of
Ws in source language and the vector of Wt in target lan-
guage. We use Euclidean distance (see equation 2) for
distance computation. The smaller distance between the
dependency heterogeneity vectors of Ws and Wt, the
more likely they are translations of each other.
</bodyText>
<equation confidence="0.999972">
DH(Ws,Wt) = DNMODHead2 + DSUBHead2 + DOBJHead2 + DNMODMod2 (2)
DNMODHead = HNMODHead(Ws) − HNMODHead(Wt)
DSUBHead = HSUBHead (Ws) − HSUBHead (Wt)
DOBJHead = HOBJHead(Ws) − HOBJHead(Wt)
DNMODMod = HNMODMod(Ws) − HNMODMod(Wt)
</equation>
<bodyText confidence="0.999794888888889">
For example, following above definitions, we get de-
pendency heterogeneity vector of the words analyzed
before (see Table 4). The distances between these vec-
tors are DH(经济学, economics) = 0.222, DH(经济学,
medicine) = 0.496. It is clear that the distance between
the vector of ‘经济学(economics)’ and ‘economics’ is
much smaller than that between ‘经济学’ and ‘medi-
cine’. Thus, the pair ‘经济学/economics’ is extracted
successfully.
</bodyText>
<tableCaption confidence="0.99847">
Table 4. Dependency heterogeneity vector of words.
</tableCaption>
<table confidence="0.96030725">
Word Dependency Heterogeneity Vector
经济学(economics) (0.398, 0.677, 0.733, 0.471)
economics (0.466, 0.500, 0.625, 0.432)
medicine (0.748, 0.524, 0.542, 0.220)
</table>
<sectionHeader confidence="0.998419" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.941275">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999313666666667">
We collect Chinese and English pages from Wikipedia2
with inter-language link and use them as comparable
corpora. After corpora preprocessing, we get 1,132,492
</bodyText>
<footnote confidence="0.748129">
2 http://download.wikimedia.org
</footnote>
<bodyText confidence="0.999948222222222">
English sentences and 665,789 Chinese sentences for
dependency heterogeneity vector learning. To evaluate
the proposed approach, we randomly select 250 Chi-
nese/English single-noun pairs from the aligned titles of
the collected pages as testing data, and divide them into
5 folders. Accuracy (see equation 3) and MMR (Voor-
hees, 1999) (see equation 4) are used as evaluation met-
rics. The average scores of both accuracy and MMR
among 5 folders are also calculated.
</bodyText>
<equation confidence="0.990742727272727">
N
Accuracy = ti
∑ N
i=1
1, if there exists correct translation in top n ranking
0, otherwise
∑ , ranki = ri, if ri &lt; n
N 1 ⎧
⎨ (4)
i=1 ranki ⎩ 0, otherwise
n means top n evaluation,
</equation>
<bodyText confidence="0.99682">
ri means the rank of the correct translation in top n ranking
N means the total number of words for evaluation
</bodyText>
<subsectionHeader confidence="0.997738">
5.2 Results of Bilingual Dictionary Extraction
</subsectionHeader>
<bodyText confidence="0.999971916666667">
Two approaches were evaluated in this experiment. One
is the context heterogeneity approach proposed in (Fung,
1995) (context for short). The other is our proposed ap-
proach (dependency for short).
The average results of dictionary extraction are listed
in Table 5. It shows both the average accuracy and aver-
age MMR of extracted dictionary entries were improved
significantly (McNemar’s test, p&lt;0.05) by the proposed
approach. Besides, the increase of top5 evaluation was
much higher than that of top10 evaluation, which means
the proposed approach has more potential to extract pre-
cise bilingual dictionary entries.
</bodyText>
<subsectionHeader confidence="0.9960225">
5.3 Effect of Dependency Heterogeneity Vector
Definition
</subsectionHeader>
<bodyText confidence="0.999931">
In the proposed approach, a dependency heterogeneity
vector is defined as the combination of head and modi-
fier heterogeneities. To see the effects of different de-
pendency heterogeneity on dictionary extraction, we
evaluated the proposed approach with different vector
definitions, which are
</bodyText>
<tableCaption confidence="0.56704625">
only-head: (HNMODHead ,HSUBHead ,HOBJHead )
only-mod: (HNMODMod )
only-NMOD: (HNMODHead ,HNMODMod )
Table 6. Average results with different vector definitions.
</tableCaption>
<table confidence="0.682317714285714">
Top5 Top10
ave.accu ave.MMR ave.accu ave.MMR
context 0.132 0.064 0.296 0.086
dependency 0.208 0.104 0.380 0.128
only-mod 0.156 0.080 0.336 0.103
only-head 0.176 0.077 0.336 0.098
only-NMODs 0.200 0.094 0.364 0.115
</table>
<figure confidence="0.7889297">
HSUBHead (W ) =total number of heads of W with SUB label
number of different heads of W with OBJ label
(3)
⎧
⎨
⎩
ti =
1
MMR =
N
</figure>
<tableCaption confidence="0.968008">
Table 5. Average results of dictionary extraction.
</tableCaption>
<table confidence="0.97566175">
context dependency
ave.accu ave.MMR ave.accu ave.MMR
Top5 0.132 0.064 0.208(↑57.58%) 0.104(↑62.50%)
Top10 0.296 0.086 0.380(↑28.38%) 0.128(↑48.84%)
</table>
<page confidence="0.998281">
123
</page>
<bodyText confidence="0.9999696">
The results are listed in Table 6. It shows with any
types of vector definitions, the proposed approach out-
performed the context approach. Besides, if comparing
the results of dependency, only-mod, and only-head, a
conclusion can be drawn that head dependency hetero-
geneities and modifier dependency heterogeneities gave
similar contribution to the proposed approach. At last,
the difference between the results of dependency and
only-NMOD shows the head and modifier with NMOD
label contributed more to the proposed approach.
</bodyText>
<subsectionHeader confidence="0.865372">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999752181818182">
To do detailed analysis, we collect the dictionary entries
that are not extracted by context approach but extracted
by the proposed approach (good for short), and the en-
tries that are extracted by context approach but not ex-
tracted by the proposed approach (bad for short) from
top10 evaluation results with their occurrence time (see
Table 7). If neglecting the entries ‘Pflu,/passports’ and
‘_L*/shanghai’, we found that the proposed approach
tended to extract correct bilingual dictionary entries if
both the two words occurred frequently in the compara-
ble corpora, but failed if one of them seldom appeared.
</bodyText>
<tableCaption confidence="0.994783">
Table 7. Good and bad dictionary entries.
</tableCaption>
<table confidence="0.999253">
Good Bad
Chinese English Chinese English
RSCA/262 jew/122 +7-9/53 crucifixion/19
319[/568 velocity/175 7XX /6 aquarium/31
1)7 /2298 history/2376 NAW47 mixture/179
`,kV,/1775 organizations/2194 0/17 brick/66
jz 4/1534 movement/1541 Aft/23 quantification/31
Pffi/76 passports/80 _L*/843 shanghai/1247
</table>
<bodyText confidence="0.999942555555556">
But there are two exceptions: (1) although ‘_L*
(shanghai)’ and ‘shanghai’ appeared frequently, the pro-
posed approach did not extract them correctly; (2) both
‘PR(passport)’ and ‘passports’ occurred less than 100
times, but they were recognized successfully by the pro-
posed approach. Analysis shows the cleanliness of the
comparable corpora is the most possible reason. In the
English corpus we used for evaluation, many words are
incorrectly combined with ‘shanghai’ by ‘br’ (i.e. line
break), such as ‘airportbrshanghai’. These errors af-
fected the correctness of dependency heterogeneity vec-
tor of ‘shanghai’ greatly. Compared with the dirty
resource of ‘shanghai’, only base form and plural form
of ‘passport’ occur in the English corpus. Therefore, the
dependency heterogeneity vectors of ‘PRV and ‘pass-
ports’ were precise and result in the successful extrac-
tion of this dictionary entry. We will clean the corpora to
solve this problem in our future work.
</bodyText>
<sectionHeader confidence="0.997247" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99998085">
This paper proposes an approach, which not uses the
similarity of bag-of-words around translation candidates
but considers about the similarity of syntactic dependen-
cies, to extract bilingual dictionary from comparable
corpora. Experimental results show that the proposed
approach outperformed the context-based approach sig-
nificantly. It not only validates the feasibility of the pro-
posed approach, but also shows the effectiveness of
applying syntactic analysis in real application.
There are several future works under consideration
including corpora cleaning, extending the proposed ap-
proach from single-noun dictionary extraction to multi-
words, and adapting the proposed approach to other lan-
guage pairs. Besides, because the proposed approach is
based on the syntactic analysis of sentences with no
more than k words (see Section 4.1), the parsing accu-
racy and the setting of threshold k will affect the cor-
rectness of dependency heterogeneity vector learning.
We will try other thresholds and syntactic parsers to see
their effects on dictionary extraction in the future.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.906331">
This research is sponsored by Microsoft Research Asia
Web-scale Natural Language Processing Theme.
</bodyText>
<sectionHeader confidence="0.995808" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995245076923077">
Y.Chiao and P.Zweigenbaum. 2002. Looking for Candidate Transla-
tional Equivalents in Specialized, Comparable Corpora. Proceed-
ings of LREC 2002.
B.Daille and E.Morin. 2008. An Effective Compositional Model for
Lexical Alignment. Proceedings of IJCNLP-08.
P.Fung. 1995. Compiling Bilingual Lexicon Entries from a Non-
parallel English-Chinese Corpus. Proceedings of the 3rd Annual
Workshop on Very Large Corpora. pp. 173-183.
P.Fung. 2000. A Statistical View on Bilingual Lexicon Extraction
from Parallel Corpora to Non-parallel Corpora. Parallel Text Proc-
essing: Alignment and Use of Translation Corpora. Kluwer Aca-
demic Publishers.
G.Grefenstette. 1998. The Problem of Cross-language Information
Retrieval. Cross-language Information Retrieval. Kluwer Aca-
demic Publishers.
E.Morin et al.. 2007. Bilingual Terminology Mining – Using Brain,
not Brawn Comparable Corpora. Proceedings of ACL 2007.
T.Nakagawa and K.Uchimoto. 2007. A Hybrid Approach to Word
Segmentation and POS Tagging. Proceedings of ACL 2007.
J.Nivre et al.. 2007. MaltParser: A Language-independent System for
Data-driven Dependency Parsing. Natural Language Engineering.
13(2): 95-135.
F.Och and H.Ney. 2003. A Systematic Comparison of Various Statis-
tical Alignment Models. Computational Linguistics, 29(1): 19-51.
P.Otero. 2007. Learning Bilingual Lexicons from Comparable English
and Spanish Corpora. Proceedings of MT Summit XI. pp. 191-198.
P.Otero. 2008. Evaluating Two Different Methods for the Task of
Extracting Bilingual Lexicons from Comparable Corpora. Proceed-
ings of LREC 2008 Workshop on Comparable Corpora. pp. 19-26.
X.Robitaille et al.. 2006. Compiling French Japanese Terminologies
from the Web. Proceedings of EACL 2006.
Y.Tsuruoka et al.. 2005. Developing a Robust Part-of-speech Tagger
for Biomedical Text. Advances in Informatics – 10th Panhellenic
Conference on Informationcs. LNCS 3746. pp. 382-392.
E.M.Voorhees. 1999. The TREC-8 Question Answering Track Report.
Proceedings of the 8th Text Retrieval Conference.
D.Wu. 1994. Learning an English-Chinese Lexicon from a Parallel
Corpus. Proceedings of the 1st Conference of the Association for
Machine Translation in the Americas.
</reference>
<page confidence="0.997964">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912900">
<title confidence="0.989683">Extracting Bilingual Dictionary from Comparable Corpora with Dependency Heterogeneity</title>
<author confidence="0.998812">Kun Yu Junichi Tsujii</author>
<affiliation confidence="0.997135">Graduate School of Information Science and The University of</affiliation>
<address confidence="0.959271">Hongo 7-3-1, Bunkyo-ku, Tokyo,</address>
<email confidence="0.989793">kunyu@is.s.u-tokyo.ac.jp</email>
<email confidence="0.989793">tsujii@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.998689909090909">This paper proposes an approach for bilingual dictionary extraction from comparable corpora. The proposed approach is based on the observation that a word and its translation share similar dependency relations. Experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional contextbased approach that uses bag-of-words around translation candidates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Chiao</author>
<author>P Zweigenbaum</author>
</authors>
<title>Looking for Candidate Translational Equivalents in Specialized, Comparable Corpora.</title>
<date>2002</date>
<booktitle>Proceedings of LREC</booktitle>
<contexts>
<context position="1352" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="177" endWordPosition="180">ionary plays an important role in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features. Context-based approach, which is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from comparable corpora and has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). But it only concerns about the lexical context </context>
<context position="4388" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="627" endWordPosition="630">es the accuracy of dictionary extraction significantly. 2 Related Work In previous work about dictionary extraction from comparable corpora, using context similarity is the most popular one. At first, Fung (1995) utilized context heterogeneity for bilingual dictionary extraction. Our proposed approach borrows Fung’s idea but extends context heterogeneity to dependency heterogeneity, in order to utilize rich syntactic information other than bag-of-words. After that, researchers extended context heterogeneity vector to context vector with the aid of an existing bilingual dictionary (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008). In these works, dictionary extraction 121 Proceedings of NAACL HLT 2009: Short Papers, pages 121–124, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics is fulfilled by comparing the similarity between the context vectors of words in target language and the context vectors of words in source language using an external dictionary. The main difference between these works and our approach is still our usage of syntactic dependency other than bag-of-words. In addition, except for a morphological ana</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Y.Chiao and P.Zweigenbaum. 2002. Looking for Candidate Translational Equivalents in Specialized, Comparable Corpora. Proceedings of LREC 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
<author>E Morin</author>
</authors>
<title>An Effective Compositional Model for Lexical Alignment.</title>
<date>2008</date>
<booktitle>Proceedings of IJCNLP-08.</booktitle>
<contexts>
<context position="1376" citStr="Daille and Morin, 2008" startWordPosition="181" endWordPosition="184">e in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features. Context-based approach, which is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from comparable corpora and has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). But it only concerns about the lexical context around translation candi</context>
<context position="4458" citStr="Daille and Morin, 2008" startWordPosition="639" endWordPosition="642">previous work about dictionary extraction from comparable corpora, using context similarity is the most popular one. At first, Fung (1995) utilized context heterogeneity for bilingual dictionary extraction. Our proposed approach borrows Fung’s idea but extends context heterogeneity to dependency heterogeneity, in order to utilize rich syntactic information other than bag-of-words. After that, researchers extended context heterogeneity vector to context vector with the aid of an existing bilingual dictionary (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008). In these works, dictionary extraction 121 Proceedings of NAACL HLT 2009: Short Papers, pages 121–124, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics is fulfilled by comparing the similarity between the context vectors of words in target language and the context vectors of words in source language using an external dictionary. The main difference between these works and our approach is still our usage of syntactic dependency other than bag-of-words. In addition, except for a morphological analyzer and a dependency parser, our approach does not need other extern</context>
</contexts>
<marker>Daille, Morin, 2008</marker>
<rawString>B.Daille and E.Morin. 2008. An Effective Compositional Model for Lexical Alignment. Proceedings of IJCNLP-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>Compiling Bilingual Lexicon Entries from a Nonparallel English-Chinese Corpus.</title>
<date>1995</date>
<booktitle>Proceedings of the 3rd Annual Workshop on Very Large Corpora.</booktitle>
<pages>173--183</pages>
<contexts>
<context position="2241" citStr="Fung, 1995" startWordPosition="317" endWordPosition="318">ical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from comparable corpora and has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). But it only concerns about the lexical context around translation candidates in a restricted window. Besides, in comparable corpora, some words may appear in similar context even if they are not translation of each other. For example, using a Chinese-English comparable corpus from Wikipedia and following the definition in (Fung, 1995), we get context heterogeneity vector of three words (see Table 1). The Euclidean distance between the vector of ‘经济学(economics)’ and ‘economics’ is 0.084. But the Euclidean distance between the vector of ‘经济学’ and ‘medicine’ is 0.075. In such case, the incorrect dictionary entry ‘经济学/medicine’ will be extracted by context-based approach. Table 1. Context heterogeneity vector of words. Word Context Heterogeneity Vector 经济学(economics) (0.185, 0.006) economics (0.101, 0.013) medicine (0.113,0.028) To solve this problem, we investigate a comparable corpora from Wikipedia and find the following ph</context>
<context position="3973" citStr="Fung (1995)" startWordPosition="572" endWordPosition="573">syntactic analysis of comparable corpora to recognize the meaning of translation candidates. Besides, the lexical information used in the proposed approach does not restrict in a small window, but comes from the entire sentence. We did experiments with 250 randomly selected translation pairs. Results show that compared with the approach based on context heterogeneity, the proposed approach improves the accuracy of dictionary extraction significantly. 2 Related Work In previous work about dictionary extraction from comparable corpora, using context similarity is the most popular one. At first, Fung (1995) utilized context heterogeneity for bilingual dictionary extraction. Our proposed approach borrows Fung’s idea but extends context heterogeneity to dependency heterogeneity, in order to utilize rich syntactic information other than bag-of-words. After that, researchers extended context heterogeneity vector to context vector with the aid of an existing bilingual dictionary (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008). In these works, dictionary extraction 121 Proceedings of NAACL HLT 2009: Short Papers, pages 121–124, Boulder, Co</context>
<context position="9377" citStr="Fung, 1995" startWordPosition="1361" endWordPosition="1362">., 2007) to get syntactic dependency of both the Chinese corpus and the English corpus. The dependency labels produced by MaltParser (e.g. SUB) are used to decide the type of heads and modifiers. After that, the analyzed corpora are refined through following steps: (1) we use a stemmer1 to do stemming for the English corpus. Considering that only nouns are treated as translation candidates, we use stems for translation candidate but keep the original form of their heads and modifiers in order to avoid excessive stemming. (2) stop words are removed. For English, we use the stop word list from (Fung, 1995). For Chinese, we remove ‘M(of)’ as stop word. (3) we remove the dependencies including punctuations and remove the sentences with 1 http://search.cpan.org/~snowhare/Lingua-Stem-0.83/ 122 more than k (set as 30 empirically) words from both English corpus and Chinese corpus, in order to reduce the effect of parsing error on dictionary extraction. 4.2 Dependency Heterogeneity Vector Calculation Equation 1 shows the definition of dependency heterogeneity vector of a word W. It includes four elements. Each element represents the heterogeneity of a dependency relation. ‘NMOD’ (noun modifier), ‘SUB’</context>
<context position="12910" citStr="Fung, 1995" startWordPosition="1914" endWordPosition="1915">and MMR (Voorhees, 1999) (see equation 4) are used as evaluation metrics. The average scores of both accuracy and MMR among 5 folders are also calculated. N Accuracy = ti ∑ N i=1 1, if there exists correct translation in top n ranking 0, otherwise ∑ , ranki = ri, if ri &lt; n N 1 ⎧ ⎨ (4) i=1 ranki ⎩ 0, otherwise n means top n evaluation, ri means the rank of the correct translation in top n ranking N means the total number of words for evaluation 5.2 Results of Bilingual Dictionary Extraction Two approaches were evaluated in this experiment. One is the context heterogeneity approach proposed in (Fung, 1995) (context for short). The other is our proposed approach (dependency for short). The average results of dictionary extraction are listed in Table 5. It shows both the average accuracy and average MMR of extracted dictionary entries were improved significantly (McNemar’s test, p&lt;0.05) by the proposed approach. Besides, the increase of top5 evaluation was much higher than that of top10 evaluation, which means the proposed approach has more potential to extract precise bilingual dictionary entries. 5.3 Effect of Dependency Heterogeneity Vector Definition In the proposed approach, a dependency het</context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>P.Fung. 1995. Compiling Bilingual Lexicon Entries from a Nonparallel English-Chinese Corpus. Proceedings of the 3rd Annual Workshop on Very Large Corpora. pp. 173-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>A Statistical View on Bilingual Lexicon Extraction from Parallel Corpora to Non-parallel Corpora. Parallel Text Processing: Alignment and Use of Translation Corpora.</title>
<date>2000</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1323" citStr="Fung, 2000" startWordPosition="175" endWordPosition="176">lingual dictionary plays an important role in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features. Context-based approach, which is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from comparable corpora and has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). But it only concer</context>
<context position="4359" citStr="Fung, 2000" startWordPosition="625" endWordPosition="626">roach improves the accuracy of dictionary extraction significantly. 2 Related Work In previous work about dictionary extraction from comparable corpora, using context similarity is the most popular one. At first, Fung (1995) utilized context heterogeneity for bilingual dictionary extraction. Our proposed approach borrows Fung’s idea but extends context heterogeneity to dependency heterogeneity, in order to utilize rich syntactic information other than bag-of-words. After that, researchers extended context heterogeneity vector to context vector with the aid of an existing bilingual dictionary (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008). In these works, dictionary extraction 121 Proceedings of NAACL HLT 2009: Short Papers, pages 121–124, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics is fulfilled by comparing the similarity between the context vectors of words in target language and the context vectors of words in source language using an external dictionary. The main difference between these works and our approach is still our usage of syntactic dependency other than bag-of-words. In addition, e</context>
</contexts>
<marker>Fung, 2000</marker>
<rawString>P.Fung. 2000. A Statistical View on Bilingual Lexicon Extraction from Parallel Corpora to Non-parallel Corpora. Parallel Text Processing: Alignment and Use of Translation Corpora. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>The Problem of Cross-language Information Retrieval. Cross-language Information Retrieval.</title>
<date>1998</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1022" citStr="Grefenstette, 1998" startWordPosition="133" endWordPosition="134">vation that a word and its translation share similar dependency relations. Experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional contextbased approach that uses bag-of-words around translation candidates. 1 Introduction Bilingual dictionary plays an important role in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features. Context-based approach, which is based on the observation that a term and its translation appear in sim</context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>G.Grefenstette. 1998. The Problem of Cross-language Information Retrieval. Cross-language Information Retrieval. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Morin</author>
</authors>
<title>Bilingual Terminology Mining – Using Brain, not Brawn Comparable Corpora.</title>
<date>2007</date>
<booktitle>Proceedings of ACL</booktitle>
<marker>Morin, 2007</marker>
<rawString>E.Morin et al.. 2007. Bilingual Terminology Mining – Using Brain, not Brawn Comparable Corpora. Proceedings of ACL 2007. T.Nakagawa and K.Uchimoto. 2007. A Hybrid Approach to Word Segmentation and POS Tagging. Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>MaltParser: A Language-independent System for Data-driven Dependency Parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering.</journal>
<volume>13</volume>
<issue>2</issue>
<pages>95--135</pages>
<marker>Nivre, 2007</marker>
<rawString>J.Nivre et al.. 2007. MaltParser: A Language-independent System for Data-driven Dependency Parsing. Natural Language Engineering. 13(2): 95-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="916" citStr="Och and Ney, 2003" startWordPosition="119" endWordPosition="122"> for bilingual dictionary extraction from comparable corpora. The proposed approach is based on the observation that a word and its translation share similar dependency relations. Experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional contextbased approach that uses bag-of-words around translation candidates. 1 Introduction Bilingual dictionary plays an important role in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common feature</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.Och and H.Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1): 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Otero</author>
</authors>
<title>Learning Bilingual Lexicons from Comparable English and Spanish Corpora.</title>
<date>2007</date>
<booktitle>Proceedings of MT Summit XI.</booktitle>
<pages>191--198</pages>
<contexts>
<context position="5432" citStr="Otero (2007)" startWordPosition="784" endWordPosition="785">The main difference between these works and our approach is still our usage of syntactic dependency other than bag-of-words. In addition, except for a morphological analyzer and a dependency parser, our approach does not need other external resources, such as the external dictionary. Because of the well-developed morphological and syntactic analysis research in recent years, the requirement of analyzers will not bring too much burden to the proposed approach. Besides of using window-based contexts, there were also some works utilizing syntactic information for bilingual dictionary extraction. Otero (2007) extracted lexico-syntactic templates from parallel corpora first, and then used them as seeds to calculate similarity between translation candidates. Otero (2008) defined syntactic rules to get lexico-syntactic contexts of words, and then used an external bilingual dictionary to fulfill similarity calculation between the lexico-syntactic context vectors of translation candidates. Our approach differs from these works in two ways: (1) both the above works defined syntactic rules or templates by hand to get syntactic information. Our approach uses data-driven syntactic analyzers for acquiring d</context>
</contexts>
<marker>Otero, 2007</marker>
<rawString>P.Otero. 2007. Learning Bilingual Lexicons from Comparable English and Spanish Corpora. Proceedings of MT Summit XI. pp. 191-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Otero</author>
</authors>
<title>Evaluating Two Different Methods for the Task of Extracting Bilingual Lexicons from Comparable Corpora.</title>
<date>2008</date>
<booktitle>Proceedings of LREC 2008 Workshop on Comparable Corpora.</booktitle>
<pages>pp.</pages>
<contexts>
<context position="1435" citStr="Otero, 2008" startWordPosition="193" endWordPosition="194">nslation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features. Context-based approach, which is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from comparable corpora and has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). But it only concerns about the lexical context around translation candidates in a restricted window. Besides, in comparable corpor</context>
<context position="5595" citStr="Otero (2008)" startWordPosition="806" endWordPosition="807">l analyzer and a dependency parser, our approach does not need other external resources, such as the external dictionary. Because of the well-developed morphological and syntactic analysis research in recent years, the requirement of analyzers will not bring too much burden to the proposed approach. Besides of using window-based contexts, there were also some works utilizing syntactic information for bilingual dictionary extraction. Otero (2007) extracted lexico-syntactic templates from parallel corpora first, and then used them as seeds to calculate similarity between translation candidates. Otero (2008) defined syntactic rules to get lexico-syntactic contexts of words, and then used an external bilingual dictionary to fulfill similarity calculation between the lexico-syntactic context vectors of translation candidates. Our approach differs from these works in two ways: (1) both the above works defined syntactic rules or templates by hand to get syntactic information. Our approach uses data-driven syntactic analyzers for acquiring dependency relations automatically. Therefore, it is easier to adapt our approach to other language pairs. (2) the types of dependencies used for similarity calcula</context>
</contexts>
<marker>Otero, 2008</marker>
<rawString>P.Otero. 2008. Evaluating Two Different Methods for the Task of Extracting Bilingual Lexicons from Comparable Corpora. Proceedings of LREC 2008 Workshop on Comparable Corpora. pp. 19-26. X.Robitaille et al.. 2006. Compiling French Japanese Terminologies from the Web. Proceedings of EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
</authors>
<title>Developing a Robust Part-of-speech Tagger for Biomedical Text.</title>
<date>2005</date>
<booktitle>Advances in Informatics – 10th Panhellenic Conference on Informationcs. LNCS</booktitle>
<pages>3746--382</pages>
<marker>Tsuruoka, 2005</marker>
<rawString>Y.Tsuruoka et al.. 2005. Developing a Robust Part-of-speech Tagger for Biomedical Text. Advances in Informatics – 10th Panhellenic Conference on Informationcs. LNCS 3746. pp. 382-392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>The TREC-8 Question Answering Track Report.</title>
<date>1999</date>
<booktitle>Proceedings of the 8th Text Retrieval Conference. D.Wu.</booktitle>
<contexts>
<context position="12323" citStr="Voorhees, 1999" startWordPosition="1803" endWordPosition="1805">2) medicine (0.748, 0.524, 0.542, 0.220) 5 Results and Discussion 5.1 Experimental Setting We collect Chinese and English pages from Wikipedia2 with inter-language link and use them as comparable corpora. After corpora preprocessing, we get 1,132,492 2 http://download.wikimedia.org English sentences and 665,789 Chinese sentences for dependency heterogeneity vector learning. To evaluate the proposed approach, we randomly select 250 Chinese/English single-noun pairs from the aligned titles of the collected pages as testing data, and divide them into 5 folders. Accuracy (see equation 3) and MMR (Voorhees, 1999) (see equation 4) are used as evaluation metrics. The average scores of both accuracy and MMR among 5 folders are also calculated. N Accuracy = ti ∑ N i=1 1, if there exists correct translation in top n ranking 0, otherwise ∑ , ranki = ri, if ri &lt; n N 1 ⎧ ⎨ (4) i=1 ranki ⎩ 0, otherwise n means top n evaluation, ri means the rank of the correct translation in top n ranking N means the total number of words for evaluation 5.2 Results of Bilingual Dictionary Extraction Two approaches were evaluated in this experiment. One is the context heterogeneity approach proposed in (Fung, 1995) (context for</context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>E.M.Voorhees. 1999. The TREC-8 Question Answering Track Report. Proceedings of the 8th Text Retrieval Conference. D.Wu. 1994. Learning an English-Chinese Lexicon from a Parallel Corpus. Proceedings of the 1st Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>