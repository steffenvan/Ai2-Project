<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010969">
<title confidence="0.993201">
Bayesian Word Alignment for Massively Parallel Texts
</title>
<author confidence="0.998664">
Robert ¨Ostling
</author>
<affiliation confidence="0.9836345">
Department of Linguistics
Stockholm University
</affiliation>
<email confidence="0.980951">
robert@ling.su.se
</email>
<sectionHeader confidence="0.982125" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961571428572">
There has been a great amount of work
done in the field of bitext alignment, but
the problem of aligning words in mas-
sively parallel texts with hundreds or thou-
sands of languages is largely unexplored.
While the basic task is similar, there
are also important differences in purpose,
method and evaluation between the prob-
lems. In this work, I present a non-
parametric Bayesian model that can be
used for simultaneous word alignment in
massively parallel corpora. This method
is evaluated on a corpus containing 1144
translations of the New Testament.
</bodyText>
<sectionHeader confidence="0.995158" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999763232142857">
Bitext word alignment is the problem of finding
links between words given pairs of translated sen-
tences (Tiedemann, 2011). Initially, this was mo-
tivated by Statistical Machine Translation (SMT)
applications (Brown et al., 1993), but word-
aligned texts have also been used to transfer lin-
guistic annotation between languages (Yarowsky
et al., 2001; T¨ackstr¨om, 2013), for Word Sense
Disambiguation (WSD) (Diab and Resnik, 2002)
and lexicon extraction (Wu and Xia, 1994).
Massively parallel texts, in the sense used by
Cysouw and W¨alchli (2007), are essentially the
same as bitexts, only with hundreds or thousands
of languages rather than just two. Parallel corpora
used in SMT, for instance the Europarl Corpus
(Koehn, 2005), tend to contain few (up to tens of)
languages, but many (up to billions of) words in
each language. Massively parallel corpora, on the
other hand, contain many (hundreds of) languages,
but usually fewer (less than a million) words in
each language.
Additionally, aligned massively parallel corpora
have different applications than traditional paral-
lel corpora with pairwise alignments. Whereas the
latter tend to be used for the various NLP tasks
mentioned above, massively parallel corpora have
mostly been used for investigations in linguistic
typology (Cysouw and W¨alchli, 2007).
There has been surprisingly few studies on mul-
tilingual word alignment. Mayer and Cysouw
(2012) treat alignment as a clustering problem,
where the words in each sentence are clustered ac-
cording to some measure of co-occurrence. They
provide no evaluation, but alignment methods
based on co-occurrence statistics have been found
to have lower accuracy than even very simple gen-
erative models (Och and Ney, 2003), so this might
not be a promising direction as far as accuracy is
concerned.
A related line of research is due to Lardilleux
et al. (2011), who learn sets of multilingual trans-
lation equivalent phrases. Although later work
(Lardilleux et al., 2013) uses phrase pairs ex-
tracted with this method for (bitext) word align-
ment, their method solves a somewhat different
problem from what is considered here.
Some authors have studied how multilingual
parallel corpora can be used to improve bitext
alignment. Filali and Bilmes (2005) use (bitext)
alignments to addditional languages as features in
bitext alignment, while Kumar et al. (2007) in-
terpolate alignments through multiple bridge lan-
guages to produce a bitext alignment for another
language pair. Since the goal of this research is
not multilingual alignment, it will not be consid-
ered further here.
</bodyText>
<sectionHeader confidence="0.97187" genericHeader="method">
2 Multilingual Alignment
</sectionHeader>
<bodyText confidence="0.998833142857143">
In bitext alignment, the goal is to find links be-
tween individual word tokens in parallel sentence
pairs. The IBM models (Brown et al., 1993) for-
malize this in a directional fashion where each
word j in a source language is linked to word i
in the target language through alignment variables
i = aj, thus specifying a 1-to-n mapping from
</bodyText>
<page confidence="0.697381">
123
</page>
<note confidence="0.895273">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 123–127,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999887846153846">
source language words to target language words.
An intuitively appealing way to formalize the
multilingual alignment problem is through a com-
mon representation (or interlingua) to which each
individual language is aligned. If the common rep-
resentation is isomorphic to one of the languages
in the corpus, this is equivalent to using that lan-
guage as a bridge. However, since all languages
(and all translations) have their own idiosyncrasies
that make linking to other translations difficult, it
seems better to learn a common representation that
corresponds to information in a sentence that is
present in as many of the translations as possible.
</bodyText>
<sectionHeader confidence="0.977063" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9998923">
Recently, it has been shown that Bayesian meth-
ods that use priors to bias towards linguistically
more plausible solutions can improve bitext word
alignment (Mermer and Sarac¸lar, 2011; Riley and
Gildea, 2012; Gal and Blunsom, 2013). Given
these promising results and the fact that massively
parallel texts tend to be rather short, which makes
the role of realistic priors more important, I have
decided to use a Bayesian alignment model for this
work.
</bodyText>
<subsectionHeader confidence="0.999143">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.999968966666667">
The model used in this work uses a common rep-
resentation of concepts generated by a Chinese
Restaurant Process (CRP), which is aligned to
each of the languages in a corpus using the model
of Mermer and Sarac¸lar (2011).
Table 1 introduces the variables (observed and
latent) as well as the hyperparameters of the
model. Basically, the model consists of a common
representation c (where token i of sentence s is de-
noted csi), which is aligned to one or more words
wlsj (from language l, sentence s, token j) through
a set of alignment variables alsj which contain the
index within cs that wlsj is linked to.
The probability of an assignment c is:
where ne is the number of occurrences of concept
type e in the assignment c, and n = Ee ne is the
(fixed) total number of tokens in the common rep-
resentation.
For the translation probabilities, I follow
Mermer and Sarac¸lar (2011) in assuming that
p(fl|e) ∼ Dir(tl; θl), and that the priors θl are
symmetric (i.e. all values in these vectors are
equal, θlef = β). By specifying a low value for
β (a sparse prior), we can encode our prior knowl-
edge that translation probability functions p(fl|e)
tend to have a low entropy, or in other words,
that each concept is typically only translated into
a very small number of words in each language.
The joint probability of the common represen-
tation and the alignments is given by:
</bodyText>
<equation confidence="0.964789">
p(c, a, w, t; α, θ) = (1)
p(c; α) · p(w |c, a, t) · p(a |c) · p(t; θ)
</equation>
<bodyText confidence="0.999857166666667">
where p(c; α) = CRP(c; α) and the remaining
factors are the same as in Mermer and Sarac¸lar
(2011) with the common representation being the
“target language”, except that there is a product
across all languages l. Note that since word order
is not modeled, p(a|c) is constant.
</bodyText>
<subsectionHeader confidence="0.999751">
3.2 Learning
</subsectionHeader>
<bodyText confidence="0.999956">
The model is trained using a collapsed Gibbs sam-
pler. Due to space limitations, the full derivation
is omitted, but the sampling distribution turns out
to be as follows for the common representation:
</bodyText>
<equation confidence="0.973294875">
�
1 α if ne0 = 1
p(csi = e0) ∝ ·
n − 1 + α ne0 − 1 if ne0 &gt; 1
77-�-7� 7--�17tl
11 f EAlsi l 1k=1sif (nle0 f + θle0 f − k)
77Ef-lsif
l 1k=1 (Ef ∈Fl nle0f + θle0f − k)
</equation>
<bodyText confidence="0.995575071428571">
(2)
where Alsi is the set of word types f in language l
which are aligned to csi, and mlsif is the number
of times each such f is aligned to csi. In order to
speed up calculations, the product in Equation 2
can be approximated by letting l run over a small
random subset of languages. The experiments car-
ried out in this work only use this approximation
when the full corpus of 1144 translations is used,
then a subset of 24 languages is randomly selected
when each csi is sampled. An empirical evalua-
tion of the effects of this approximation is left for
future work.
The alignment sampling distribution is:
</bodyText>
<equation confidence="0.9965054">
nle0f0 + θle0f0 − 1
p(alsj = i) ∝ (3)
Γ (1 + α) fl
CRP(c;α) = Γ (n + α) · α|Ec|−1 ·
e∈Ec
(ne − 1)!
fl·
l
E �nle0f + θle0f � − 1
f
</equation>
<bodyText confidence="0.997432">
where e0 = csalsj is the concept type aligned to
word type f0 = wlsj.
Rather than sampling directly from the distribu-
tions above, one can sample from ˆp(csi = e0) ∝
</bodyText>
<page confidence="0.862573">
124
</page>
<tableCaption confidence="0.99977">
Table 1: Variables used in the model.
</tableCaption>
<table confidence="0.719139307692307">
Observed variables
Fl the set of word types in language l
wlsj E Fl word j of sentence s in language l
Is E N length of sentence s in the common representation
Jls E N length of sentence s in language l
Latent variables
Ec the set of concepts in the assignment c
csi E Ec concept i of sentence s in the common representation
alsj E {1..Is} alignment of wlsj to csi; i = alsj
tlef E R translation probability p(fl|e), where fl E Fl and e E Ec
Hyperparameters
α CRP hyperparameter, fixed to 1000 in the experiments
Q symmetric Dirichlet prior for translation distributions θl,
</table>
<bodyText confidence="0.9528258">
fixed to 0.001 in the experiments
p(csi = e&apos;)λ and ˆp(alsj = i) a p(alsj = i)λ.
The temperature parameter A can be varied dur-
ing training to change the amount of randomness
while sampling.
</bodyText>
<subsectionHeader confidence="0.98289">
3.3 Initialization
</subsectionHeader>
<bodyText confidence="0.999985777777778">
In order to obtain a reasonable initial state for the
Gibbs sampling, one can simply initialize the com-
mon representation to be identical to one of the
languages in the corpus. For this language one
then (trivially) has a perfect alignment, while the
remaining languages are initialized randomly and
their alignments are learned. Random initializa-
tion of the common representation is possible, but
turns out to perform poorly.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999976090909091">
The most basic question about the present model
is whether sampling the common representation is
helpful, compared to simply choosing a language
and aligning all other languages to that one.
In order to test this, I initialize the model as de-
scribed in section 3.3 and sample alignments (but
not the common representation) for 200 iterations
with A linearly increasing from 0 to 2, followed by
two iterations with A —* oc. This gives a strong
baseline, from which one can start learning the
joint model.
</bodyText>
<subsectionHeader confidence="0.968378">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9997955">
I use a corpus containing verse-aligned transla-
tions of the New Testament into a great number of
languages. After some exclusions due to e.g. non-
standard formatting or improperly segmented text,
the version used in this work contains 1144 trans-
lations in 986 different languages. The mean num-
ber of tokens among the translations is 236 000,
and the mean number of types is 9 500.
</bodyText>
<subsectionHeader confidence="0.962315">
4.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999974357142857">
Previous authors have tended to avoid multilingual
evaluation altogether. Mayer and Cysouw (2012)
do not evaluate their method, while Lardilleux et
al. (2011) only use bilingual evaluation.
Cysouw et al. (2007) use the fact that some
translations of the Bible have been annotated with
Strong’s Numbers, which map most word tokens
to the lemma of its translation equivalent in the
original language, to perform bilingual evaluation
of Bible corpus alignments.
Strong’s Numbers can be used in a different
way to evaluate the type of multilingual alignment
produced by the method in this work. Both the
Strong’s Numbers and the common representation
can be interpreted as clusterings of the word to-
kens in each language. Ideally one would want
these two clusterings to be identical, as they would
be if the original language had been perfectly con-
structed. Standard clustering evaluation measures
can be used for this task, and in this work I use
normalized mutual information (also reinvented as
V-measure by Rosenberg and Hirschberg (2007)).
The evaluation is limited to words which are as-
signed exactly one Strong’s Number, in an attempt
to avoid some of the problems with scope dis-
cussed by Cysouw et al. (2007). Note that even a
perfect alignment from one language to itself does
not achieve the maximum score using this mea-
</bodyText>
<page confidence="0.763845">
125
</page>
<table confidence="0.966530916666667">
English Mandarin
A A+J A A+J
deu 0.817 0.824 0.708 0.788
eng 0.854 0.851 0.714 0.800
eng2 0.834 0.833 0.708 0.790
fra 0.807 0.816 0.712 0.783
ind 0.774 0.785 0.710 0.770
ind2 0.791 0.803 0.721 0.786
nld 0.839 0.850 0.724 0.809
por 0.807 0.813 0.709 0.782
rus 0.792 0.800 0.699 0.772
Normalized Mutual Information
</table>
<figure confidence="0.997251444444444">
0.80
0.78
0.76
0.74
0.70
0.68
0.72
fra
nld
eng
eng
deu
ind
por
rus
ind
200 400 600 800 1000
Iterations
</figure>
<figureCaption confidence="0.991941">
Figure 1: Alignment quality of Mandarin-
initialized model.
</figureCaption>
<bodyText confidence="0.999910428571429">
sure, only a successful reconstruction of the origi-
nal text (minus inflections) would.
In the Bible corpus used here, nine translations
in seven languages contain Strong’s Numbers an-
notations: English and Indonesian (two transla-
tions each), as well as German, French, Dutch,
Portuguese and Russian (one translation each).
</bodyText>
<subsectionHeader confidence="0.964967">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.994396155555556">
Figure 1 shows alignment quality during training
in a model initialized using a translation in Man-
darin, which is not related to any of the languages
in the evaluation sample and was chosen to avoid
initialization bias. After an initial drop when noise
is introduced during the Gibbs sampling process,
alignment quality quickly increases as the com-
mon representation moves towards the versions in
the evaluation sample. The final two iterations
(with λ → ∞) remove the sampling noise and the
model rapidly converges to a local maximum, re-
sulting in a sharp increase in alignment quality at
the end. Further iterations only result in minor im-
provements.
Table 2 contains the baseline and joint model re-
sults for models initialized with either English or
Mandarin versions. The joint model outperforms
the baseline in all cases except when the initial-
ization language is the same as the evaluation lan-
guage (the two English translations in the left col-
umn), which is expected since it is easy to align a
text to itself or to a very similar version.
The two models described so far only use the
nine-translation evaluation sample to learn the
common representation, since using additional
languages would unfairly penalize the joint learn-
Table 2: Normalized mutual information with re-
spect to Strong’s Numbers, using alignment only
(A) or joint alignment + common representation
learning (A+J), for models initialized using En-
glish or Mandarin.
ing model. I have also tested the model on the
full corpus of 1144 translations with an English-
initialized model and the same training setup as
above (initialized from English). In this case,
alignment quality decreased somewhat for the lan-
guages most similar to English, which is to be ex-
pected since the majority of languages in the cor-
pus are unrelated to English and pull the common
representation away from the European languages
in the evaluation sample. Although it is not possi-
ble to directly evaluate alignment quality outside
the evaluation sample with Strong’s Numbers, the
log-probability of the entire data under the model
(Equation 1) increases as expected, by about 5%.
</bodyText>
<sectionHeader confidence="0.977679" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979846153846">
As the number of translations in a parallel cor-
pus increases, the problem of aligning them be-
comes a rather different one from aligning trans-
lation pairs. I have presented a Bayesian method
that jointly learns a common structure along with
alignments to each language in the corpus. In
an empirical evaluation, the joint method outper-
forms the baseline where the common structure is
one of the languages.
Currently the underlying alignment model is
quite simplistic, and preliminary results indicate
that including the HMM word order model of Vo-
gel et al. (1996) further improves alignments.
</bodyText>
<sectionHeader confidence="0.995441" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996769">
Thanks to J¨org Tiedemann, Mats Wir´en and the
anonymous reviewers for their comments.
</bodyText>
<page confidence="0.851955">
126
</page>
<sectionHeader confidence="0.95022" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996674972477064">
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311, June.
Michael Cysouw and Bernhard W¨alchli. 2007. Paral-
lel texts: Using translational equivalents in linguistic
typology. STUF - Language Typology and Univer-
sals, 60(2):95–99.
Michael Cysouw, Chris Biemann, and Matthias Ongy-
erth. 2007. Using Strong’s Numbers in the Bible to
test an automatic alignment of parallel texts. STUF -
Language Typology and Universals, 60(2):158–171.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
’02, pages 255–262, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Karim Filali and Jeff Bilmes. 2005. Leveraging multi-
ple languages to improve statistical MT word align-
ments. In IEEE Workshop on Automatic Speech
Recognition and Understanding, pages 92–97, San
Juan, November. IEEE.
Yarin Gal and Phil Blunsom. 2013. A systematic
bayesian treatment of the ibm alignment models. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit, Phuket, Thailand.
Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 42–50,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Adrien Lardilleux, Yves Lepage, and Franois Yvon.
2011. The contribution of low frequencies to multi-
lingual sub-sentential alignment: a differential asso-
ciative approach. International Journal ofAdvanced
Intelligence, 3(2):189–217.
Adrien Lardilleux, Francois Yvon, and Yves Lepage.
2013. Hierarchical sub-sentential alignment with
Anymalign. In Proceedings of the 16th EAMT Con-
ference, pages 279–286, Trento, Italy, 28-30 May
2012.
Thomas Mayer and Michael Cysouw. 2012. Lan-
guage comparison through sparse multilingual word
alignment. In Proceedings of the EACL 2012 Joint
Workshop of LINGVIS &amp; UNCLH, EACL 2012,
pages 54–62, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Cos¸kun Mermer and Murat Sarac¸lar. 2011. Bayesian
word alignment for statistical machine translation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ’11, pages 182–187, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51,
March.
Darcey Riley and Daniel Gildea. 2012. Improving the
IBM alignment models using variational Bayes. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short Papers
- Volume 2, ACL ’12, pages 306–310, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 410–
420, Prague, Czech Republic, June. Association for
Computational Linguistics.
Oscar T¨ackstr¨om. 2013. Predicting Linguistic Struc-
ture with Incomplete and Cross-Lingual Supervi-
sion. Ph.D. thesis, Uppsala University, Department
of Linguistics and Philology.
J¨org Tiedemann. 2011. Bitext Alignment. Synthesis
Lectures on Human Language Technologies. Mor-
gan &amp; Claypool Publishers.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2, COLING
’96, pages 836–841, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Dekai Wu and Xuanyin Xia. 1994. Learning an
English-Chinese lexicon from a parallel corpus. In
Proceedings of the First Conference of the Associa-
tion for Machine Translation in the Americas, pages
206–213.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of the First International Con-
ference on Human Language Technology Research,
HLT ’01, pages 1–8, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.86622">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.551043">
<title confidence="0.960891">Bayesian Word Alignment for Massively Parallel Texts</title>
<affiliation confidence="0.875122">Department of</affiliation>
<address confidence="0.746736">Stockholm</address>
<email confidence="0.987802">robert@ling.su.se</email>
<abstract confidence="0.983954066666667">There has been a great amount of work done in the field of bitext alignment, but the problem of aligning words in massively parallel texts with hundreds or thousands of languages is largely unexplored. While the basic task is similar, there are also important differences in purpose, method and evaluation between the problems. In this work, I present a nonparametric Bayesian model that can be used for simultaneous word alignment in massively parallel corpora. This method is evaluated on a corpus containing 1144 translations of the New Testament.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="936" citStr="Brown et al., 1993" startWordPosition="140" endWordPosition="143">ages is largely unexplored. While the basic task is similar, there are also important differences in purpose, method and evaluation between the problems. In this work, I present a nonparametric Bayesian model that can be used for simultaneous word alignment in massively parallel corpora. This method is evaluated on a corpus containing 1144 translations of the New Testament. 1 Introduction Bitext word alignment is the problem of finding links between words given pairs of translated sentences (Tiedemann, 2011). Initially, this was motivated by Statistical Machine Translation (SMT) applications (Brown et al., 1993), but wordaligned texts have also been used to transfer linguistic annotation between languages (Yarowsky et al., 2001; T¨ackstr¨om, 2013), for Word Sense Disambiguation (WSD) (Diab and Resnik, 2002) and lexicon extraction (Wu and Xia, 1994). Massively parallel texts, in the sense used by Cysouw and W¨alchli (2007), are essentially the same as bitexts, only with hundreds or thousands of languages rather than just two. Parallel corpora used in SMT, for instance the Europarl Corpus (Koehn, 2005), tend to contain few (up to tens of) languages, but many (up to billions of) words in each language. </context>
<context position="3450" citStr="Brown et al., 1993" startWordPosition="536" endWordPosition="539">. Some authors have studied how multilingual parallel corpora can be used to improve bitext alignment. Filali and Bilmes (2005) use (bitext) alignments to addditional languages as features in bitext alignment, while Kumar et al. (2007) interpolate alignments through multiple bridge languages to produce a bitext alignment for another language pair. Since the goal of this research is not multilingual alignment, it will not be considered further here. 2 Multilingual Alignment In bitext alignment, the goal is to find links between individual word tokens in parallel sentence pairs. The IBM models (Brown et al., 1993) formalize this in a directional fashion where each word j in a source language is linked to word i in the target language through alignment variables i = aj, thus specifying a 1-to-n mapping from 123 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 123–127, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics source language words to target language words. An intuitively appealing way to formalize the multilingual alignment problem is through a common representation (or interlingua) to which ea</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Cysouw</author>
<author>Bernhard W¨alchli</author>
</authors>
<title>Parallel texts: Using translational equivalents in linguistic typology.</title>
<date>2007</date>
<journal>STUF - Language Typology and Universals,</journal>
<volume>60</volume>
<issue>2</issue>
<marker>Cysouw, W¨alchli, 2007</marker>
<rawString>Michael Cysouw and Bernhard W¨alchli. 2007. Parallel texts: Using translational equivalents in linguistic typology. STUF - Language Typology and Universals, 60(2):95–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Cysouw</author>
<author>Chris Biemann</author>
<author>Matthias Ongyerth</author>
</authors>
<title>Using Strong’s Numbers in the Bible to test an automatic alignment of parallel texts.</title>
<date>2007</date>
<journal>STUF -Language Typology and Universals,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="10373" citStr="Cysouw et al. (2007)" startWordPosition="1770" endWordPosition="1773"> Data I use a corpus containing verse-aligned translations of the New Testament into a great number of languages. After some exclusions due to e.g. nonstandard formatting or improperly segmented text, the version used in this work contains 1144 translations in 986 different languages. The mean number of tokens among the translations is 236 000, and the mean number of types is 9 500. 4.2 Evaluation Measures Previous authors have tended to avoid multilingual evaluation altogether. Mayer and Cysouw (2012) do not evaluate their method, while Lardilleux et al. (2011) only use bilingual evaluation. Cysouw et al. (2007) use the fact that some translations of the Bible have been annotated with Strong’s Numbers, which map most word tokens to the lemma of its translation equivalent in the original language, to perform bilingual evaluation of Bible corpus alignments. Strong’s Numbers can be used in a different way to evaluate the type of multilingual alignment produced by the method in this work. Both the Strong’s Numbers and the common representation can be interpreted as clusterings of the word tokens in each language. Ideally one would want these two clusterings to be identical, as they would be if the origin</context>
</contexts>
<marker>Cysouw, Biemann, Ongyerth, 2007</marker>
<rawString>Michael Cysouw, Chris Biemann, and Matthias Ongyerth. 2007. Using Strong’s Numbers in the Bible to test an automatic alignment of parallel texts. STUF -Language Typology and Universals, 60(2):158–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>255--262</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1135" citStr="Diab and Resnik, 2002" startWordPosition="170" endWordPosition="173">yesian model that can be used for simultaneous word alignment in massively parallel corpora. This method is evaluated on a corpus containing 1144 translations of the New Testament. 1 Introduction Bitext word alignment is the problem of finding links between words given pairs of translated sentences (Tiedemann, 2011). Initially, this was motivated by Statistical Machine Translation (SMT) applications (Brown et al., 1993), but wordaligned texts have also been used to transfer linguistic annotation between languages (Yarowsky et al., 2001; T¨ackstr¨om, 2013), for Word Sense Disambiguation (WSD) (Diab and Resnik, 2002) and lexicon extraction (Wu and Xia, 1994). Massively parallel texts, in the sense used by Cysouw and W¨alchli (2007), are essentially the same as bitexts, only with hundreds or thousands of languages rather than just two. Parallel corpora used in SMT, for instance the Europarl Corpus (Koehn, 2005), tend to contain few (up to tens of) languages, but many (up to billions of) words in each language. Massively parallel corpora, on the other hand, contain many (hundreds of) languages, but usually fewer (less than a million) words in each language. Additionally, aligned massively parallel corpora h</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 255–262, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Filali</author>
<author>Jeff Bilmes</author>
</authors>
<title>Leveraging multiple languages to improve statistical MT word alignments.</title>
<date>2005</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>92--97</pages>
<publisher>IEEE.</publisher>
<location>San Juan,</location>
<contexts>
<context position="2958" citStr="Filali and Bilmes (2005)" startWordPosition="457" endWordPosition="460">ve been found to have lower accuracy than even very simple generative models (Och and Ney, 2003), so this might not be a promising direction as far as accuracy is concerned. A related line of research is due to Lardilleux et al. (2011), who learn sets of multilingual translation equivalent phrases. Although later work (Lardilleux et al., 2013) uses phrase pairs extracted with this method for (bitext) word alignment, their method solves a somewhat different problem from what is considered here. Some authors have studied how multilingual parallel corpora can be used to improve bitext alignment. Filali and Bilmes (2005) use (bitext) alignments to addditional languages as features in bitext alignment, while Kumar et al. (2007) interpolate alignments through multiple bridge languages to produce a bitext alignment for another language pair. Since the goal of this research is not multilingual alignment, it will not be considered further here. 2 Multilingual Alignment In bitext alignment, the goal is to find links between individual word tokens in parallel sentence pairs. The IBM models (Brown et al., 1993) formalize this in a directional fashion where each word j in a source language is linked to word i in the t</context>
</contexts>
<marker>Filali, Bilmes, 2005</marker>
<rawString>Karim Filali and Jeff Bilmes. 2005. Leveraging multiple languages to improve statistical MT word alignments. In IEEE Workshop on Automatic Speech Recognition and Understanding, pages 92–97, San Juan, November. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yarin Gal</author>
<author>Phil Blunsom</author>
</authors>
<title>A systematic bayesian treatment of the ibm alignment models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4750" citStr="Gal and Blunsom, 2013" startWordPosition="740" endWordPosition="743">to one of the languages in the corpus, this is equivalent to using that language as a bridge. However, since all languages (and all translations) have their own idiosyncrasies that make linking to other translations difficult, it seems better to learn a common representation that corresponds to information in a sentence that is present in as many of the translations as possible. 3 Method Recently, it has been shown that Bayesian methods that use priors to bias towards linguistically more plausible solutions can improve bitext word alignment (Mermer and Sarac¸lar, 2011; Riley and Gildea, 2012; Gal and Blunsom, 2013). Given these promising results and the fact that massively parallel texts tend to be rather short, which makes the role of realistic priors more important, I have decided to use a Bayesian alignment model for this work. 3.1 Model The model used in this work uses a common representation of concepts generated by a Chinese Restaurant Process (CRP), which is aligned to each of the languages in a corpus using the model of Mermer and Sarac¸lar (2011). Table 1 introduces the variables (observed and latent) as well as the hyperparameters of the model. Basically, the model consists of a common represe</context>
</contexts>
<marker>Gal, Blunsom, 2013</marker>
<rawString>Yarin Gal and Phil Blunsom. 2013. A systematic bayesian treatment of the ibm alignment models. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In The Tenth Machine Translation</booktitle>
<location>Summit, Phuket, Thailand.</location>
<contexts>
<context position="1434" citStr="Koehn, 2005" startWordPosition="220" endWordPosition="221">n, 2011). Initially, this was motivated by Statistical Machine Translation (SMT) applications (Brown et al., 1993), but wordaligned texts have also been used to transfer linguistic annotation between languages (Yarowsky et al., 2001; T¨ackstr¨om, 2013), for Word Sense Disambiguation (WSD) (Diab and Resnik, 2002) and lexicon extraction (Wu and Xia, 1994). Massively parallel texts, in the sense used by Cysouw and W¨alchli (2007), are essentially the same as bitexts, only with hundreds or thousands of languages rather than just two. Parallel corpora used in SMT, for instance the Europarl Corpus (Koehn, 2005), tend to contain few (up to tens of) languages, but many (up to billions of) words in each language. Massively parallel corpora, on the other hand, contain many (hundreds of) languages, but usually fewer (less than a million) words in each language. Additionally, aligned massively parallel corpora have different applications than traditional parallel corpora with pairwise alignments. Whereas the latter tend to be used for the various NLP tasks mentioned above, massively parallel corpora have mostly been used for investigations in linguistic typology (Cysouw and W¨alchli, 2007). There has been</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In The Tenth Machine Translation Summit, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Franz J Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Improving word alignment with bridge languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>42--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3066" citStr="Kumar et al. (2007)" startWordPosition="473" endWordPosition="476">ot be a promising direction as far as accuracy is concerned. A related line of research is due to Lardilleux et al. (2011), who learn sets of multilingual translation equivalent phrases. Although later work (Lardilleux et al., 2013) uses phrase pairs extracted with this method for (bitext) word alignment, their method solves a somewhat different problem from what is considered here. Some authors have studied how multilingual parallel corpora can be used to improve bitext alignment. Filali and Bilmes (2005) use (bitext) alignments to addditional languages as features in bitext alignment, while Kumar et al. (2007) interpolate alignments through multiple bridge languages to produce a bitext alignment for another language pair. Since the goal of this research is not multilingual alignment, it will not be considered further here. 2 Multilingual Alignment In bitext alignment, the goal is to find links between individual word tokens in parallel sentence pairs. The IBM models (Brown et al., 1993) formalize this in a directional fashion where each word j in a source language is linked to word i in the target language through alignment variables i = aj, thus specifying a 1-to-n mapping from 123 Proceedings of </context>
</contexts>
<marker>Kumar, Och, Macherey, 2007</marker>
<rawString>Shankar Kumar, Franz J. Och, and Wolfgang Macherey. 2007. Improving word alignment with bridge languages. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 42–50, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrien Lardilleux</author>
<author>Yves Lepage</author>
<author>Franois Yvon</author>
</authors>
<title>The contribution of low frequencies to multilingual sub-sentential alignment: a differential associative approach.</title>
<date>2011</date>
<journal>International Journal ofAdvanced Intelligence,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="2569" citStr="Lardilleux et al. (2011)" startWordPosition="397" endWordPosition="400">sed for investigations in linguistic typology (Cysouw and W¨alchli, 2007). There has been surprisingly few studies on multilingual word alignment. Mayer and Cysouw (2012) treat alignment as a clustering problem, where the words in each sentence are clustered according to some measure of co-occurrence. They provide no evaluation, but alignment methods based on co-occurrence statistics have been found to have lower accuracy than even very simple generative models (Och and Ney, 2003), so this might not be a promising direction as far as accuracy is concerned. A related line of research is due to Lardilleux et al. (2011), who learn sets of multilingual translation equivalent phrases. Although later work (Lardilleux et al., 2013) uses phrase pairs extracted with this method for (bitext) word alignment, their method solves a somewhat different problem from what is considered here. Some authors have studied how multilingual parallel corpora can be used to improve bitext alignment. Filali and Bilmes (2005) use (bitext) alignments to addditional languages as features in bitext alignment, while Kumar et al. (2007) interpolate alignments through multiple bridge languages to produce a bitext alignment for another lan</context>
<context position="10321" citStr="Lardilleux et al. (2011)" startWordPosition="1762" endWordPosition="1765">, from which one can start learning the joint model. 4.1 Data I use a corpus containing verse-aligned translations of the New Testament into a great number of languages. After some exclusions due to e.g. nonstandard formatting or improperly segmented text, the version used in this work contains 1144 translations in 986 different languages. The mean number of tokens among the translations is 236 000, and the mean number of types is 9 500. 4.2 Evaluation Measures Previous authors have tended to avoid multilingual evaluation altogether. Mayer and Cysouw (2012) do not evaluate their method, while Lardilleux et al. (2011) only use bilingual evaluation. Cysouw et al. (2007) use the fact that some translations of the Bible have been annotated with Strong’s Numbers, which map most word tokens to the lemma of its translation equivalent in the original language, to perform bilingual evaluation of Bible corpus alignments. Strong’s Numbers can be used in a different way to evaluate the type of multilingual alignment produced by the method in this work. Both the Strong’s Numbers and the common representation can be interpreted as clusterings of the word tokens in each language. Ideally one would want these two cluster</context>
</contexts>
<marker>Lardilleux, Lepage, Yvon, 2011</marker>
<rawString>Adrien Lardilleux, Yves Lepage, and Franois Yvon. 2011. The contribution of low frequencies to multilingual sub-sentential alignment: a differential associative approach. International Journal ofAdvanced Intelligence, 3(2):189–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrien Lardilleux</author>
<author>Francois Yvon</author>
<author>Yves Lepage</author>
</authors>
<title>Hierarchical sub-sentential alignment with Anymalign.</title>
<date>2013</date>
<booktitle>In Proceedings of the 16th EAMT Conference,</booktitle>
<pages>279--286</pages>
<location>Trento,</location>
<contexts>
<context position="2679" citStr="Lardilleux et al., 2013" startWordPosition="413" endWordPosition="416">dies on multilingual word alignment. Mayer and Cysouw (2012) treat alignment as a clustering problem, where the words in each sentence are clustered according to some measure of co-occurrence. They provide no evaluation, but alignment methods based on co-occurrence statistics have been found to have lower accuracy than even very simple generative models (Och and Ney, 2003), so this might not be a promising direction as far as accuracy is concerned. A related line of research is due to Lardilleux et al. (2011), who learn sets of multilingual translation equivalent phrases. Although later work (Lardilleux et al., 2013) uses phrase pairs extracted with this method for (bitext) word alignment, their method solves a somewhat different problem from what is considered here. Some authors have studied how multilingual parallel corpora can be used to improve bitext alignment. Filali and Bilmes (2005) use (bitext) alignments to addditional languages as features in bitext alignment, while Kumar et al. (2007) interpolate alignments through multiple bridge languages to produce a bitext alignment for another language pair. Since the goal of this research is not multilingual alignment, it will not be considered further h</context>
</contexts>
<marker>Lardilleux, Yvon, Lepage, 2013</marker>
<rawString>Adrien Lardilleux, Francois Yvon, and Yves Lepage. 2013. Hierarchical sub-sentential alignment with Anymalign. In Proceedings of the 16th EAMT Conference, pages 279–286, Trento, Italy, 28-30 May 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mayer</author>
<author>Michael Cysouw</author>
</authors>
<title>Language comparison through sparse multilingual word alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH, EACL 2012,</booktitle>
<pages>54--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2115" citStr="Mayer and Cysouw (2012)" startWordPosition="321" endWordPosition="324">(up to billions of) words in each language. Massively parallel corpora, on the other hand, contain many (hundreds of) languages, but usually fewer (less than a million) words in each language. Additionally, aligned massively parallel corpora have different applications than traditional parallel corpora with pairwise alignments. Whereas the latter tend to be used for the various NLP tasks mentioned above, massively parallel corpora have mostly been used for investigations in linguistic typology (Cysouw and W¨alchli, 2007). There has been surprisingly few studies on multilingual word alignment. Mayer and Cysouw (2012) treat alignment as a clustering problem, where the words in each sentence are clustered according to some measure of co-occurrence. They provide no evaluation, but alignment methods based on co-occurrence statistics have been found to have lower accuracy than even very simple generative models (Och and Ney, 2003), so this might not be a promising direction as far as accuracy is concerned. A related line of research is due to Lardilleux et al. (2011), who learn sets of multilingual translation equivalent phrases. Although later work (Lardilleux et al., 2013) uses phrase pairs extracted with th</context>
<context position="10260" citStr="Mayer and Cysouw (2012)" startWordPosition="1752" endWordPosition="1755">by two iterations with A —* oc. This gives a strong baseline, from which one can start learning the joint model. 4.1 Data I use a corpus containing verse-aligned translations of the New Testament into a great number of languages. After some exclusions due to e.g. nonstandard formatting or improperly segmented text, the version used in this work contains 1144 translations in 986 different languages. The mean number of tokens among the translations is 236 000, and the mean number of types is 9 500. 4.2 Evaluation Measures Previous authors have tended to avoid multilingual evaluation altogether. Mayer and Cysouw (2012) do not evaluate their method, while Lardilleux et al. (2011) only use bilingual evaluation. Cysouw et al. (2007) use the fact that some translations of the Bible have been annotated with Strong’s Numbers, which map most word tokens to the lemma of its translation equivalent in the original language, to perform bilingual evaluation of Bible corpus alignments. Strong’s Numbers can be used in a different way to evaluate the type of multilingual alignment produced by the method in this work. Both the Strong’s Numbers and the common representation can be interpreted as clusterings of the word toke</context>
</contexts>
<marker>Mayer, Cysouw, 2012</marker>
<rawString>Thomas Mayer and Michael Cysouw. 2012. Language comparison through sparse multilingual word alignment. In Proceedings of the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH, EACL 2012, pages 54–62, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cos¸kun Mermer</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Bayesian word alignment for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>182--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Mermer, Sarac¸lar, 2011</marker>
<rawString>Cos¸kun Mermer and Murat Sarac¸lar. 2011. Bayesian word alignment for statistical machine translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 182–187, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2430" citStr="Och and Ney, 2003" startWordPosition="371" endWordPosition="374">ignments. Whereas the latter tend to be used for the various NLP tasks mentioned above, massively parallel corpora have mostly been used for investigations in linguistic typology (Cysouw and W¨alchli, 2007). There has been surprisingly few studies on multilingual word alignment. Mayer and Cysouw (2012) treat alignment as a clustering problem, where the words in each sentence are clustered according to some measure of co-occurrence. They provide no evaluation, but alignment methods based on co-occurrence statistics have been found to have lower accuracy than even very simple generative models (Och and Ney, 2003), so this might not be a promising direction as far as accuracy is concerned. A related line of research is due to Lardilleux et al. (2011), who learn sets of multilingual translation equivalent phrases. Although later work (Lardilleux et al., 2013) uses phrase pairs extracted with this method for (bitext) word alignment, their method solves a somewhat different problem from what is considered here. Some authors have studied how multilingual parallel corpora can be used to improve bitext alignment. Filali and Bilmes (2005) use (bitext) alignments to addditional languages as features in bitext </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darcey Riley</author>
<author>Daniel Gildea</author>
</authors>
<title>Improving the IBM alignment models using variational Bayes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>306--310</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4726" citStr="Riley and Gildea, 2012" startWordPosition="736" endWordPosition="739">sentation is isomorphic to one of the languages in the corpus, this is equivalent to using that language as a bridge. However, since all languages (and all translations) have their own idiosyncrasies that make linking to other translations difficult, it seems better to learn a common representation that corresponds to information in a sentence that is present in as many of the translations as possible. 3 Method Recently, it has been shown that Bayesian methods that use priors to bias towards linguistically more plausible solutions can improve bitext word alignment (Mermer and Sarac¸lar, 2011; Riley and Gildea, 2012; Gal and Blunsom, 2013). Given these promising results and the fact that massively parallel texts tend to be rather short, which makes the role of realistic priors more important, I have decided to use a Bayesian alignment model for this work. 3.1 Model The model used in this work uses a common representation of concepts generated by a Chinese Restaurant Process (CRP), which is aligned to each of the languages in a corpus using the model of Mermer and Sarac¸lar (2011). Table 1 introduces the variables (observed and latent) as well as the hyperparameters of the model. Basically, the model cons</context>
</contexts>
<marker>Riley, Gildea, 2012</marker>
<rawString>Darcey Riley and Daniel Gildea. 2012. Improving the IBM alignment models using variational Bayes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 306–310, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>410--420</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11201" citStr="Rosenberg and Hirschberg (2007)" startWordPosition="1904" endWordPosition="1907">rm bilingual evaluation of Bible corpus alignments. Strong’s Numbers can be used in a different way to evaluate the type of multilingual alignment produced by the method in this work. Both the Strong’s Numbers and the common representation can be interpreted as clusterings of the word tokens in each language. Ideally one would want these two clusterings to be identical, as they would be if the original language had been perfectly constructed. Standard clustering evaluation measures can be used for this task, and in this work I use normalized mutual information (also reinvented as V-measure by Rosenberg and Hirschberg (2007)). The evaluation is limited to words which are assigned exactly one Strong’s Number, in an attempt to avoid some of the problems with scope discussed by Cysouw et al. (2007). Note that even a perfect alignment from one language to itself does not achieve the maximum score using this mea125 English Mandarin A A+J A A+J deu 0.817 0.824 0.708 0.788 eng 0.854 0.851 0.714 0.800 eng2 0.834 0.833 0.708 0.790 fra 0.807 0.816 0.712 0.783 ind 0.774 0.785 0.710 0.770 ind2 0.791 0.803 0.721 0.786 nld 0.839 0.850 0.724 0.809 por 0.807 0.813 0.709 0.782 rus 0.792 0.800 0.699 0.772 Normalized Mutual Informa</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410– 420, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
</authors>
<title>Predicting Linguistic Structure with Incomplete and Cross-Lingual Supervision.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>Uppsala University, Department of Linguistics and Philology.</institution>
<marker>T¨ackstr¨om, 2013</marker>
<rawString>Oscar T¨ackstr¨om. 2013. Predicting Linguistic Structure with Incomplete and Cross-Lingual Supervision. Ph.D. thesis, Uppsala University, Department of Linguistics and Philology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Bitext Alignment. Synthesis Lectures on Human Language Technologies.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="830" citStr="Tiedemann, 2011" startWordPosition="127" endWordPosition="128">ment, but the problem of aligning words in massively parallel texts with hundreds or thousands of languages is largely unexplored. While the basic task is similar, there are also important differences in purpose, method and evaluation between the problems. In this work, I present a nonparametric Bayesian model that can be used for simultaneous word alignment in massively parallel corpora. This method is evaluated on a corpus containing 1144 translations of the New Testament. 1 Introduction Bitext word alignment is the problem of finding links between words given pairs of translated sentences (Tiedemann, 2011). Initially, this was motivated by Statistical Machine Translation (SMT) applications (Brown et al., 1993), but wordaligned texts have also been used to transfer linguistic annotation between languages (Yarowsky et al., 2001; T¨ackstr¨om, 2013), for Word Sense Disambiguation (WSD) (Diab and Resnik, 2002) and lexicon extraction (Wu and Xia, 1994). Massively parallel texts, in the sense used by Cysouw and W¨alchli (2007), are essentially the same as bitexts, only with hundreds or thousands of languages rather than just two. Parallel corpora used in SMT, for instance the Europarl Corpus (Koehn, 2</context>
</contexts>
<marker>Tiedemann, 2011</marker>
<rawString>J¨org Tiedemann. 2011. Bitext Alignment. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics - Volume 2, COLING ’96,</booktitle>
<pages>836--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th Conference on Computational Linguistics - Volume 2, COLING ’96, pages 836–841, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Xuanyin Xia</author>
</authors>
<title>Learning an English-Chinese lexicon from a parallel corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>206--213</pages>
<contexts>
<context position="1177" citStr="Wu and Xia, 1994" startWordPosition="177" endWordPosition="180">word alignment in massively parallel corpora. This method is evaluated on a corpus containing 1144 translations of the New Testament. 1 Introduction Bitext word alignment is the problem of finding links between words given pairs of translated sentences (Tiedemann, 2011). Initially, this was motivated by Statistical Machine Translation (SMT) applications (Brown et al., 1993), but wordaligned texts have also been used to transfer linguistic annotation between languages (Yarowsky et al., 2001; T¨ackstr¨om, 2013), for Word Sense Disambiguation (WSD) (Diab and Resnik, 2002) and lexicon extraction (Wu and Xia, 1994). Massively parallel texts, in the sense used by Cysouw and W¨alchli (2007), are essentially the same as bitexts, only with hundreds or thousands of languages rather than just two. Parallel corpora used in SMT, for instance the Europarl Corpus (Koehn, 2005), tend to contain few (up to tens of) languages, but many (up to billions of) words in each language. Massively parallel corpora, on the other hand, contain many (hundreds of) languages, but usually fewer (less than a million) words in each language. Additionally, aligned massively parallel corpora have different applications than traditiona</context>
</contexts>
<marker>Wu, Xia, 1994</marker>
<rawString>Dekai Wu and Xuanyin Xia. 1994. Learning an English-Chinese lexicon from a parallel corpus. In Proceedings of the First Conference of the Association for Machine Translation in the Americas, pages 206–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research, HLT ’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1054" citStr="Yarowsky et al., 2001" startWordPosition="159" endWordPosition="162"> and evaluation between the problems. In this work, I present a nonparametric Bayesian model that can be used for simultaneous word alignment in massively parallel corpora. This method is evaluated on a corpus containing 1144 translations of the New Testament. 1 Introduction Bitext word alignment is the problem of finding links between words given pairs of translated sentences (Tiedemann, 2011). Initially, this was motivated by Statistical Machine Translation (SMT) applications (Brown et al., 1993), but wordaligned texts have also been used to transfer linguistic annotation between languages (Yarowsky et al., 2001; T¨ackstr¨om, 2013), for Word Sense Disambiguation (WSD) (Diab and Resnik, 2002) and lexicon extraction (Wu and Xia, 1994). Massively parallel texts, in the sense used by Cysouw and W¨alchli (2007), are essentially the same as bitexts, only with hundreds or thousands of languages rather than just two. Parallel corpora used in SMT, for instance the Europarl Corpus (Koehn, 2005), tend to contain few (up to tens of) languages, but many (up to billions of) words in each language. Massively parallel corpora, on the other hand, contain many (hundreds of) languages, but usually fewer (less than a mi</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the First International Conference on Human Language Technology Research, HLT ’01, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>