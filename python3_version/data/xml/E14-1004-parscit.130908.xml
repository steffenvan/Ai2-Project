<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000221">
<title confidence="0.969841">
Maximizing Component Quality in Bilingual Word-Aligned Segmentations
</title>
<author confidence="0.964272">
Spyros Martzoukos Christophe Costa Florˆencio Christof Monz
</author>
<affiliation confidence="0.939766">
Intelligent Systems Lab Amsterdam, University of Amsterdam
</affiliation>
<address confidence="0.90556">
Science Park 904, 1098 XH Amsterdam, The Netherlands
</address>
<email confidence="0.996018">
{S.Martzoukos, C.CostaFlorencio, C.Monz}@uva.nl
</email>
<sectionHeader confidence="0.998594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999517454545455">
Given a pair of source and target language
sentences which are translations of each other
with known word alignments between them,
we extract bilingual phrase-level segmenta-
tions of such a pair. This is done by identi-
fying two appropriate measures that assess the
quality of phrase segments, one on the mono-
lingual level for both language sides, and one
on the bilingual level. The monolingual mea-
sure is based on the notion of partition refine-
ments and the bilingual measure is based on
structural properties of the graph that repre-
sents phrase segments and word alignments.
These two measures are incorporated in a ba-
sic adaptation of the Cross-Entropy method
for the purpose of extracting an N-best list
of bilingual phrase-level segmentations. A
straight-forward application of such lists in
Statistical Machine Translation (SMT) yields
a conservative phrase pair extraction method
that reduces phrase-table sizes by 90% with
insignificant loss in translation quality.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949431818182">
Given a pair of source and target language sen-
tences which are translations of each other with
known word alignments between them, the problem
of extracting high quality bilingual phrase segmen-
tations is defined as follows: Maximize the quality
of phrase segments, i.e., groupings of consecutive
words, in both language sides, subject to constraints
imposed by the underlying word alignments. The
purpose of this work is to provide a solution to this
maximization problem and investigate the effect of
the resulting high quality bilingual phrase segments
on SMT. For brevity, ‘phrase-level sentence segmen-
tation’ and ‘phrase segment’ will henceforth be sim-
ply referred to as ‘segmentation’ and ‘segment’ re-
spectively.
The exact definition of segments’ quality depends
on the application. Our notion of a segmentation of
maximum quality is defined as the set of consecutive
words of the sentence that captures maximum col-
locational and/or grammatical characteristics. This
implies that a sequence of tokens is identified as a
segment if its fully compositional expressive power
is higher than the expressive power of any combina-
tion of partial compositions. Since this definition is
fairly general it is thus suitable for most NLP tasks.
In particular, it is tailored to the type of segments
that are suitable for the purposes of SMT and is in
line with previous work (Blackwood et al., 2008;
Paul et al., 2010).
With this definition in mind, we introduce a
monolingual segment quality measure that is based
on assessing the cost of converting one segmentation
into another by means of an elementary operation.
This operation, namely the ‘splitting’ of a segment
into two segments, together with all possible seg-
mentations of a sentence are known to form a par-
tially ordered set (Guo, 1997). Such a construction
is known as partition refinement and gives rise to the
desired monolingual surface quality measure.
The presence of word alignments between the
sentence pair provides additional structure which
should not be ignored. In the language of graph the-
ory, a segment can also be viewed as a chain, i.e., a
graph in which vertices are the segment’s words and
</bodyText>
<page confidence="0.97872">
30
</page>
<note confidence="0.991706">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30–38,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999916964285714">
an edge between two words exists if and only if these
words are consecutive. Then, a bilingual segmenta-
tion is represented by the graph that is formed by all
its source and target language chains together with
edges induced by word alignments. Motivated by
the phrase pair extraction methods of SMT (Och et
al., 1999; Koehn et al., 2003), we focus on the con-
nected components, or simply components of such a
representation. We explain that the extent to which
we can delete word alignments from a component
without violating its component status, gives rise to
a bilingual, purely structural quality measure.
The surface and structural measures are incorpo-
rated in one algorithm that extracts an N-best list
of bilingual word-aligned segmentations. This algo-
rithm, which is an adaptation of the Cross-Entropy
method (Rubinstein, 1997), performs joint maxi-
mization of surface (in both languages) and struc-
tural quality measures. Components of graph repre-
sentations of the resulting N-best lists give rise to
high quality translation units. These units, which
form a small subset of all possible (continuous) con-
sistent phrase pairs, are used to construct SMT mod-
els. Results on Czech–English and German–English
datasets show a 90% reduction in phrase-table sizes
with insignificant loss in translation quality which
are in line with other pruning techniques in SMT
(Johnson et al., 2007; Zens et al., 2012).
</bodyText>
<sectionHeader confidence="0.987519" genericHeader="introduction">
2 Monolingual Surface Quality Measure
</sectionHeader>
<bodyText confidence="0.9956095">
Given a sentence s1s2...sk that consists of words
sz, 1 &lt; i &lt; k, we introduce an empirical count-
based measure that assesses the quality of its seg-
mentations. By fixing a segmentation σ, we are in-
terested in assessing the cost of perturbing σ and
generating another segmentation σ&apos;. A perturbation
of σ is achieved by splitting a segment of σ into
two new segments, while keeping all other segments
fixed. For example, for a sentence with five words, if
σ : (s1s2)(s3s4s5), where brackets are used to dis-
tinguish the segments s1s2 and s3s4s5, then σ can
be perturbed in three different ways:
</bodyText>
<listItem confidence="0.997495833333333">
• σ&apos; : (s1)(s2)(s3s4s5), by splitting the first seg-
ment of σ.
• σ&apos;&apos; : (s1s2)(s3)(s4s5), by splitting at the first
position of the second segment of σ.
• σ&apos;&apos;&apos; : (s1s2)(s3s4)(s5), by splitting at the sec-
ond position of the second segment of σ,
</listItem>
<bodyText confidence="0.999395625">
so that σ&apos;, σ&apos;&apos; and σ&apos;&apos;&apos; are the perturbations of σ.
Such perturbations are known as partition refine-
ments in the literature (Stanley, 1997). The set of all
segmentations of a sentence, equipped with the split-
ting operation forms a partially ordered set (Guo,
1997), and its visual representation is known as the
Hasse diagram. Figure 1 shows such a partially or-
dered set for a sentence with four words.
</bodyText>
<equation confidence="0.996762">
s1 s2 s3 s4
s1s2s3s4 s1s2s3s4 s1s2 s3s4
s1s2s3s4 s1s2s3s4 s1s2s3s4
s1s2s3s4
</equation>
<figureCaption confidence="0.894821">
Figure 1: Hasse diagram of segmentation refine-
ments for a sentence with four words.
</figureCaption>
<bodyText confidence="0.9989138">
The cost of perturbing a segmentation into an-
other, i.e., the weight of a directed edge in the Hasse
diagram, is calculated from n-gram counts that are
extracted from a monolingual training corpus. Let
n(s) be the empirical count of phrase s in the corpus.
Given a segmentation σ of a sentence, let seg(σ) de-
note the set of σ’s segments. In the above example
we have for instance seg(σ&apos;&apos;) = {s1s2, s3, s4s5}.
The probability of s in σ is given by relative fre-
quencies
</bodyText>
<equation confidence="0.998300333333333">
( _ n(s)
pv S) — � (1)
�,Eseg(�) n(s&apos;).
</equation>
<bodyText confidence="0.9997435">
The cost of perturbing σ into σ&apos; by splitting a seg-
ment s¯s of σ into segments s and s¯ is defined by
</bodyText>
<equation confidence="0.9870405">
cost,,,,(s, ¯s) = log p�(s¯s)pQ (s)P&amp;quot; (2)
p
</equation>
<bodyText confidence="0.9975645">
and we say that s and s¯ are co-responsible for the
perturbation σ → σ&apos;. Intuitively, this cost function
yields the amount of energy (log of probability) that
is lost when performing a perturbation. On a more
</bodyText>
<page confidence="0.999784">
31
</page>
<bodyText confidence="0.997177714285714">
technical level, it is closely related to metric spaces
on partially ordered sets (Monjardet, 1981; Orum
and Joslyn, 2009), but we do not go into further de-
tails here.
The cost function admits a measure for the seg-
ments that are co-responsible for perturbing Q into
Q0 and we define the gain of s from the perturbation
</bodyText>
<equation confidence="0.9794795">
Q → Q0 as
gain,→,,(s) = −cost,→,,(s, ¯s). (3)
</equation>
<bodyText confidence="0.998034666666667">
A segment s may be co-responsible for different per-
turbations, and we have to consider all such pertur-
bations. Let
</bodyText>
<equation confidence="0.999802">
R(s) = {Q → Q0 : s E/ seg(Q), s E seg(Q0)} (4)
</equation>
<bodyText confidence="0.999727">
denote the set of perturbations for which s is co-
responsible. Then, the average gain of s in the sen-
tence is given by
</bodyText>
<equation confidence="0.713234">
gain,→,,(s). (5)
</equation>
<bodyText confidence="0.99855975">
Intuitively, gain(s) measures how difficult it is to
break phrase s into sub-phrases. Finally, the surface
quality measure of a segmentation Q of a sentence is
given by
</bodyText>
<equation confidence="0.9969745">
g(Q) = � gain(s). (6)
s∈seg(,)
</equation>
<bodyText confidence="0.99965525">
Note that g is a real number. The relation g(Q) &gt;
g(Q0) implies that Q is a better segmentation than Q0.
We conclude this section with two remarks: (i)
The exact computation of gain(s) for each possi-
ble segment s is computationally expensive since
all perturbations need to be considered. In prac-
tice we can simply generate a random sample of no
more than 1500 segmentations and compute gain(·)
based on that sample only. (ii) Each sentence of
the monolingual training corpus (from which the n-
gram counts are extracted) should have the begin-
ning and end-of-sentence tokens. The count for each
of them is equal to the number of sentences in the
corpus, and they are treated as regular words. With-
out going into further details they provide the pur-
pose of normalization.
</bodyText>
<sectionHeader confidence="0.962434" genericHeader="method">
3 Bilingual Structural Quality Measure
</sectionHeader>
<bodyText confidence="0.999720413043478">
Given a word-aligned sentence pair, we introduce a
purely structural measure that assesses the quality of
its bilingual segmentations. By ‘purely structural’
we mean that the focus is entirely on combinatorial
aspects of the bilingual segmentations and the word
alignments. For that reason we turn to a graph theo-
retic framework.
A segment can also be viewed as a chain, i.e., a
graph in which vertices are the segment’s words and
an edge between two words exists if and only if these
words are consecutive. Then, a source segmentation
Q and a target segmentation T are graphs that con-
sist of source chains and target chains respectively.
The graph formed by Q, T and the translation edges
induced by word alignments is thus a graph repre-
sentation of a bilingual word-aligned segmentation.
We focus on a particular type of subgraphs of this
representation, namely its connected components, or
simply components. A component is a graph such
that (a) there exists a path between any two of its
vertices, and (b) there does not exist a path between
a vertex of the component and a vertex outside the
component. Condition (a) means, both technically
and intuitively, that a component is connected and
Condition (b) requires connectivity to be maximal.
Components play a key role in SMT. The most
widely used strategy for extracting high quality
phrase-level translations without linguistic informa-
tion, namely the consistency method (Och et al.,
1999; Koehn et al., 2003) is entirely based on com-
ponents of word aligned unsegmented sentence pairs
(Martzoukos et al., 2013). In particular, each ex-
tracted translation is either a component or the union
of components. Since an unsegmented sentence
pair is just one possible configuration of all possi-
ble bilingual segmentations, we consequently have
no direct reason to investigate further than compo-
nents.
In order to get an intuition of the measure that will
be introduced in this section, we begin with an ex-
ample. Figure 2, shows two different configurations
of the pair (Q, T) for the same sentence pair with
known and fixed word alignments. Both configu-
rations have the same number of edges that connect
source vertices (3) and the same number of edges
that connect target vertices (2). However, one would
</bodyText>
<equation confidence="0.6956554">
1
gain(s) =
Rs
()
{,→,,}∈R(s)
</equation>
<page confidence="0.938171">
32
</page>
<figureCaption confidence="0.969313">
Figure 2: Graph representations of two bilingual
segmentations with fixed word alignments. Source
and target vertices are shown with circles and
squares respectively.
</figureCaption>
<bodyText confidence="0.99862">
expect the top configuration to represent a better
bilingual segmentation. This is because it has more
components (4 opposed to 2 for the bottom config-
uration) and because it consists of ‘tighter’ clusters,
i.e., ‘tighter’ components.
A general measure that would capture this obser-
vation requires a balance between the number of
edges of source and target chains, the number of
components and the number of translation edges, all
coupled with how these edges and vertices are con-
nected. This might seem as a daunting task that can
be tackled with a combination of heuristics, but there
is actually a graph-theoretic measure that can fully
describe the sought structure. We proceed with in-
troducing this measure.
Let C denote the set of components of the graph
representation of a bilingual word-aligned segmen-
tation. We are interested in measuring the extent to
which we can delete translation edges from c E C,
while retaining its component status. Let ac denote
the subset of translation edges that are restricted to
the component c. We define the positive integer
</bodyText>
<equation confidence="0.900493">
gain(c) = number of ways of
</equation>
<bodyText confidence="0.989589764705882">
deleting translation edges from ac,
while keeping c connected, (7)
where the option of deleting nothing is counted. In-
tuitively, by keeping the edges of the chains fixed
the quantity gain(c) measures how difficult it is to
perturb a component from its connected state to a
disconnected state.
Figure 3 shows two components c and c&apos; that sat-
isfy gain(c) = gain(c&apos;) = 3. Both components
are equally difficult to be perturbed into a discon-
nected state, but only superficially. The actual struc-
tural quality of c is revealed when it is ‘compared’ to
component c˜ that consists of the same source and tar-
get vertices, the same translation edges but its source
vertices form exactly one chain and similarly for its
target vertices; c˜ is essentially the ‘upper bound’ of
c. In general, the maximum value of gain(c), with
</bodyText>
<figure confidence="0.97435">
C
C&apos;
C̃
</figure>
<figureCaption confidence="0.962334">
Figure 3: Superficially similar components c and c&apos;.
Comparing c with c˜ yields c’s true structural quality.
</figureCaption>
<bodyText confidence="0.9562835">
respect to a fixed set of source and target vertices
and translation edges, is attained when it consists
of exactly one source chain and exactly one target
chain. It is not difficult to see that the desired max-
imum value is always 2|ac |− 1. In the example of
Figure 3, the structural quality of c and c&apos; is thus
3/(25 −1) = 9.7% and 3/(22 −1) = 100% respec-
tively. Hence, the measure that evaluates the struc-
tural quality of a bilingual word-aligned segmenta-
tion (σ, τ) is given by
</bodyText>
<equation confidence="0.977856">
f (σ, τ) = (ri
cEC
</equation>
<bodyText confidence="0.998563411764706">
which takes values in (0, 1]. The relation f(σ, τ) &gt;
f(σ&apos;, τ&apos;) implies that (σ, τ) is a better bilingual seg-
mentation than (σ&apos;, τ&apos;).
We conclude this section with two remarks: (i) A
component with no translation edges, i.e., a source
or target segment whose words are all unaligned, has
a contribution of 1/0 in (8). In practice we exclude
such components from C. (ii) In graph theory the
quantity gain(c) is known as the number of con-
nected spanning subgraphs (CSSGs) of graph c and
is the key quantity of network reliability (Valiant,
1979; Coulbourn, 1987). Finding the number of
CSSGs of a general graph is a known #P-hard prob-
lem (Welsh, 1997). In our setting, graphs have spe-
cific formation (source and target chains connected
via translation edges) and we are interested in the
deletion of translation edges only; it is possible to
</bodyText>
<figure confidence="0.986078571428571">
0 1 2 3 4 5 6
0 1 2 3 4 5
0 1 2 3 4 5 6
0 1 2 3 4 5
� 1 , (8)
gain(c) ICI
2|ac |− 1
</figure>
<page confidence="0.990321">
33
</page>
<bodyText confidence="0.962522">
compute gain(·) in polynomial time, but we do not
go into further details here.
</bodyText>
<sectionHeader confidence="0.852426" genericHeader="method">
4 Extracting Bilingual Segmentations with
the Cross-Entropy Method
</sectionHeader>
<bodyText confidence="0.943033216666667">
Equipped with the measures of Sections 2 and 3 we
turn to extracting an N-best list of bilingual segmen-
tations for a given sentence pair. The search space is
exponential in the total number of words of the sen-
tence pair. We propose a new approach for this task,
by noting a direct connection with the combinato-
rial problems that can be solved efficiently and ef-
fectively with the Cross-Entropy (CE) method (Ru-
binstein, 1997).
The CE method is an iterative self-tuning sam-
pling method that has applications in various com-
binatorial and continuous global optimization prob-
lems as well as in rare event detection. A detailed
account on the CE method is beyond the scope of
this work, and we thus simply describe its applica-
tion to our problem.
In particular, we first establish the connection be-
tween the most basic form of the CE method and the
problem of finding the best monolingual segmen-
tation of a sentence, with respect to some scoring
function (not necessarily the one that was introduced
in Section 2). This connection yields a simple, ef-
ficient and effective algorithm for the monolingual
maximization problem. Then, the transition to the
bilingual level is done by incorporating the measure
of Section 3 in the algorithm, thus performing joint
maximization of surface and structural quality. Fi-
nally, the generation of the N-best list will be trivial.
A segmentation of a given sentence has a bit-
string representation in the following way: If two
consecutive words in the sentence belong to the
same segment in the segmentation, then this pair of
words is encoded by ‘1’, otherwise by ‘0’. Such a
representation is bijective and, thus, for the rest of
this section, we do not distinguish between a seg-
mentation and its bit-string representation. In this
setting, the CE method takes its most basic form
(De Boer et al., 2005). In a nutshell, it is a re-
peated application of (a) sampling bit-strings from
a parametrized probability mass function, (b) scor-
ing them and keeping only a small high-performing
subsample, and (c) updating the parameters of the
probability mass function based on that subsample
only.
We assume no prior knowledge on the quality
of bit-strings, so that they are all equally likely. In
other words, each position of a randomly chosen
bit-string can be either a ‘0’ or a ‘1’ with probability
1/2. The aim is to tune these position probabilities
towards the best bit-string, with respect to some
scoring function g. In particular, let the sentence
have n words and let ` = n — 1 be the length of
bit-strings. A bit-string labeled by an integer i is
denoted by bi and its jth bit by bij. The algorithm is
as follows:
0. Initialize the bit-string position probabilities
p0 = (p01, ...,p0`) = (1/2, ...,1/2) and set M = 20`
(sample size), ρ = r1%Ml (keep top 1% of
samples), α = 0.7 (smoothing parameter) and t = 1
(iteration).
</bodyText>
<listItem confidence="0.983461">
1. Generate a sample b1, ..., bM of bit-strings, each
of length `, such that bij —Bernoulli(pt−1
j ), for all
i = 1, ..., M and j = 1, ..., `.
1.1 Compute scores g(b1), ..., g(bM).
1.2 Order them descendingly as g(bπ(1)) &gt; ... &gt;
g(bπ(M)).
2. Focus on the best performing ones: Compute
γt = g(bπ(ρ)); samples performing less than this
threshold will be ignored.
3. Use the best performing sub-sample of b1, ..., bM
to update position probabilities:
</listItem>
<equation confidence="0.897801666666667">
t EMi=1 Ii(γt)bij( )
pj = M , j = 1, ..., E, 9
�i=1 Ii(γt)
</equation>
<bodyText confidence="0.97714">
where the choice function Ii is given by
</bodyText>
<equation confidence="0.9002765">
� 1, if g(bi) &gt; γt
Ii(γt) =
0, otherwise.
4. Smooth the updated position probabilities as
ptj := αptj + (1 — α)pt−1
j , j = 1, ..., `. (10)
</equation>
<bodyText confidence="0.5769035">
E. If for some t &gt; 5 we have γt = γt−1 = ... = γt−5
then stop. Else, t := t + 1 and go to Step 1.
</bodyText>
<page confidence="0.997709">
34
</page>
<bodyText confidence="0.999988">
The values for the parameters M, p and α re-
ported here are in line with the ones suggested in the
literature (Rubinstein and Kroese, 2004) for combi-
natorial problems such as this one. After the execu-
tion of the algorithm, the updated vector of position
probabilities converges to sequence of ‘0’s and ‘1’s,
which corresponds to the best segmentation under g.
The extension to bilingual level is done by incor-
porating the structural quality measure of Section 3.
The setting is similar, i.e., samples are again bit-
strings, but of length E = n + m − 2, where n and
m are the number of words in the source and tar-
get sentence respectively. The first n − 1 bits corre-
spond to the source sentence and the rest to the target
sentence. The surface quality score of such a bit-
string is given by the harmonic mean of its source
and target surface quality scores.1 The bit-string
scoring function throughout Steps 1 – 3 is given by
the harmonic mean of surface and structural quality
scores. Finally, N-best lists are trivially generated,
simply by collecting the top-N performing accumu-
lated samples of a maximization process.
</bodyText>
<sectionHeader confidence="0.999639" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.897409041666667">
Given a sentence pair with known and fixed word
alignments, the result of the method described in
Section 4 is an N-best list of bilingual segmenta-
tions of such a pair. The objective function provides
a balance between compositional expressive power
of segments in both languages and synchronization
via word alignments. Thus, each (continuous) com-
ponent of such a bilingual segmentation leads to the
extraction of a high quality phrase pair.
As was mentioned in Section 3, each extracted
phrase pair of standard phrase-based SMT is con-
structed from a component or from the union of
components of an unsegmented word-aligned sen-
tence pair. For each sentence pair, all possible
(continuous) components and (continuous) unions
of components give rise to the extracted (contin-
uous) phrase pairs. In this section we investigate
the impact to SMT models and translation quality,
when extracting phrase pairs (from the N-best lists)
1As it was mentioned in Section 2 the surface quality score
in (6) is a real number. At each iteration of the algorithm the
surface score of a segmentation can be converted into a number
in [0, 1] via Min-Max normalization. This holds for both source
and target sides of a bit-string (independently).
</bodyText>
<table confidence="0.99916475">
Cz–En De–En
Europarl (v7) 642,505 1,889,791
News Commentary (v8) 139,679 177,079
Total 782,184 2,066,870
</table>
<tableCaption confidence="0.9705705">
Table 1: Number of filtered parallel sentences for
Czech–English and German–English.
</tableCaption>
<bodyText confidence="0.999465692307692">
that correspond to components only. A reduction
in phrase-table size is guaranteed because we are
essentially extracting only a subset of all possible
continuous phrase pairs. The challenge is to verify
whether this subset can provide a sufficient transla-
tion model.
Both the baseline and our system are standard
phrase-based MT systems. Bidirectional word align-
ments are generated with GIZA++ (Och and Ney,
2003) and ‘grow-diag-final-and’. These are used
to construct a phrase-table with bidirectional phrase
probabilities, lexical weights and a reordering model
with monotone, swap and discontinuous orienta-
tions, conditioned on both the previous and the next
phrase. 4-gram interpolated language models with
Kneser-Ney smoothing are built with SRILM (Stol-
cke, 2002). A distortion limit of 6 and a phrase-
penalty are also used. All model parameters are
tuned with MERT (Och, 2003). Decoding during
tuning and testing is done with Moses (Koehn et. al,
2007). Since our system only affects which phrases
are extracted, lexical weights and reordering orien-
tations are the same for both systems.
Datasets are from the WMT’13 translation task
(Bojar et al., 2013): Translation and reordering
models are trained on Czech–English and German–
English corpora (Table 1). Language models and
segment measures gain, as defined in (5), are trained
on 35.3M Czech, 50.0M German and 94.5M En-
glish sentences from the provided monolingual data.
Tuning is done on newstest2010 and performance
is evaluated on newstest2008, newstest2009, new-
stest2011 and newstest2012 with BLEU (Papineni
et al., 2001).
In our experiments the size of an N-best list varies
according to the total number of words in the sen-
tence pair, say w. For the purposes of phrase ex-
traction in SMT we would ideally require all local
maxima to be part of an N-best list. This would
</bodyText>
<page confidence="0.997239">
35
</page>
<table confidence="0.994661421052632">
Method
Baseline
N-best
N-best &amp; unseg.
Czech-*English
’08 ’09 ’11 ’12
19.6 20.6 22.6 20.6
19.7 20.4 22.4 20.3
19.6 20.5 22.6 20.7
English-*Czech
’08 ’09 ’11 ’12
14.8 15.6 16.6 14.9
14.4 15.2 16.3 14.3
14.6 15.4 16.8 14.7
Czech–English
PT size (retain%)
44.6M (100%)
4.4M (9.8%)
4.6M (10.4%)
</table>
<tableCaption confidence="0.9825592">
Table 2: BLEU scores and phrase-table (PT) sizes for Czech–English. Phrase-table of ‘Baseline’ is con-
structed from all consistent phrase pairs. Phrase-table of ‘N-best’ is constructed from consistent phrase
pairs that are components of the top-N bilingual word-aligned segmentations of each sentence pair. Simi-
larly for ‘N-best &amp; unseg.’, but consistent phrase pairs that are components of each (unsegmented) sentence
pair are also included.
</tableCaption>
<table confidence="0.997247684210526">
Method
Baseline
N-best
N-best &amp; unseg.
German-*English
’08 ’09 ’11 ’12
21.4 20.8 21.3 22.1
21.3 20.6 21.3 21.8
21.5 20.8 21.5 22.0
English-*German
’08 ’09 ’11 ’12
15.1 15.1 16.0 16.5
15.0 15.0 15.6 16.0
15.4 15.2 15.7 16.2
German–English
PT size (retain%)
102.3M (100 %)
9.4M (9.2 %)
9.9M (9.7 %)
</table>
<tableCaption confidence="0.999841">
Table 3: Similar to Table 2, but for German–English.
</tableCaption>
<bodyText confidence="0.999938290322581">
guarantee the extraction of all high quality phrase
pairs, with (empirically) desired variations, while
keeping N small. Since the CE method performs
global optimization, the resulting members of an N-
best list are in the vicinity of the global maximum.
Consequently, we cannot guarantee the inclusion of
local maxima. We set N = F30%wl so that at
least some variation from the global maximum is in-
cluded, but is not large enough to contaminate the
lists with noisy bilingual segmentations. The result-
ing lists have 22 bilingual segmentations on aver-
age for both language pairs. Figure 4 shows typical
German–English best performing bilingual segmen-
tations.
BLEU scores are reported in Tables 2 and 3 for
Czech–English and German–English respectively.
Methods ‘Baseline’ and ‘N-best’ are the ones de-
scribed above. Phrase-table sizes are reduced as
expected and performance when translating to En-
glish is comparable. The significant drops in new-
stest2012 when translating from the morphologi-
cally poorer language (English) prompts us to in-
clude more ‘basic’ phrase pairs in the phrase-tables.
This leads to augmenting each N-best list by its un-
segmented sentence pair. Consequently, method ‘N-
best &amp; unseg.’ extracts the same phrase pairs as ‘N-
best’, together with those from components of the
unsegmented sentence pairs. As a result, transla-
tion quality is comparable to ‘Baseline’ across all
language directions and small phrase-table sizes are
retained.
</bodyText>
<sectionHeader confidence="0.996227" genericHeader="discussions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99997315">
This work can also be viewed as an attempt to un-
derstand bilinguality as a generalization of mono-
linguality. There is conceptual common ground on
what gain(x) for phrase x (Section 2) or component
x (Section 3) computes. In both cases it measures
how ‘stable’ a unit is. The stability of a phrase x is
determined by how difficult it is to split x into multi-
ple phrases. The partially ordered set framework of
partition refinements is the natural setting for such
computations. In order to determine the stability
of a component we turn to empirical evidence from
SMT: ‘good’ phrase pairs are extracted from com-
ponents or unions of components of the graph that
represents word-aligned sentence pairs. The stabil-
ity of a component x is therefore determined by how
difficult it is to break x into multiple components. It
is thus interesting to investigate whether there exists
a general approach that unifies partition refinements
and network reliability for the purpose of identifying
highly stable multilingual units.
</bodyText>
<page confidence="0.996887">
36
</page>
<figureCaption confidence="0.9597565">
Figure 4: Typical fragments from best performing
German–English segmentations.
</figureCaption>
<bodyText confidence="0.99998434375">
The focus has been on bilingual segmentations,
but as was mentioned in Section 2, it is possible
to apply the CE method for generating monolingual
segmentations. By using (6) as the objective func-
tion, we observed that the resulting segmentations
yield promising applications in n-gram topic model-
ing, named entity recognition and Chinese segmen-
tation. However, in the spirit of Ries et al. (1996),
attempts to minimize perplexity instead of maximiz-
ing (6), resulted in larger segments and the segment
quality definition of Section 1 was not met.
The sizes of the resulting phrase-tables together
with the type of phrase pairs that are extracted lead
to applications involving discontinuous phrase pairs.
In (Galley and Manning, 2010) there was evidence
that discontinuous phrase pairs that are extracted
from discontinuous components of word-aligned
sentence pairs can improve translation quality.1 As
the number of such components is much bigger than
the continuous ones, (Gimpel and Smith, 2011) pro-
pose a Bayesian nonparametric model for finding the
most probable discontinuous phrase pairs. This can
also be done from the N-best lists that are generated
in Section 4, and it would be interesting to see the
effect of such phrase pairs in our existing models.
In a longer version of this work we intend to
study the effect in translation quality when varying
some of the parameters (size of N-best lists, sample
sizes for training gain in Section 2 and for the CE
method), as well as when extracting source-driven
bilingual segmentations as in (Sanchis-Trilles et al.,
2011).
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999964875">
In this work, we have presented a solution to the
problem of extracting bilingual segmentations in the
presence of word alignments. Two measures that as-
sess the quality of bilingual segmentations based on
the expressive power of segments in both languages
and their synchronization via word alignments have
been introduced. We have established the link be-
tween the CE method and finding the best monolin-
gual and bilingual segmentations. These measures
formed the objective function of the CE method
whose maximization resulted in an N-best list of
bilingual segmentations for a given sentence pair.
By extracting only phrase pairs that correspond to
components from bilingual segmentations of those
lists, we found that phrase table sizes can be reduced
with insignificant loss in translation quality.
</bodyText>
<sectionHeader confidence="0.996492" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9990416">
This research was funded in part by the Euro-
pean Commission through the CoSyne project FP7-
ICT-4-248531 and the Netherlands Organisation
for Scientific Research (NWO) under project nr.
639.022.213.
</bodyText>
<footnote confidence="0.937359333333333">
1By ‘discontinuous component’ we mean a component
whose source or target words (vertices) form a discontinuous
substring in the source or target sentence respectively.
</footnote>
<note confidence="0.514059666666667">
im anschluss an den 11. september gab es
jede menge
good deal of
</note>
<bodyText confidence="0.954976090909091">
in the aftermath of 9 / 11 , there was a
für amerikas militärische reaktion
verständnis
auf der ganzen welt
understanding around the world
for america’s military response
das wort hat herr patten im namen der kommission
mr patten has the floor on behalf of the commission
in verhandlungen zwischen der schweiz und der europäischen union
negotiations between switzerland and the european union resulted in
die entwicklung
</bodyText>
<figure confidence="0.96984246875">
eines umweltgerechten energiesektors ausgegeben
für
on the development
environmentally sustainable energy sector
of
spent
an
der reform wichtiger als
qualität
für
mich ist die
far as
i am
concerned
,
the
quality
important
as
than
of reform is more
können
sie
mir sagen
,
wie sie sicherstellen werden
,
daß
me how you will be able to
ensure that
can
you tell
</figure>
<page confidence="0.995159">
37
</page>
<sectionHeader confidence="0.998075" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999974264367816">
Graeme Blackwood, Adria de Gispert, and William
Byrne. 2008. Phrasal Segmentation Models for Sta-
tistical Machine Translation. In COLING.
Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp Koehn,
Christof Monz, Matt Post, Radu Soricut, and Lucia
Specia. 2013. Findings of the 2013 Workshop on Sta-
tistical Machine Translation. In WMT.
Charlie J. Coulbourn. 1987. The Combinatorics of Net-
work Reliability. Oxford University Press.
Pieter-Tjerk De Boer, Dirk P. Kroese, Shie Mannor, and
Reuven Y. Rubinstein. 2005. A Tutorial on the Cross-
Entropy Method. Annals of Operations Research,
vol. 134, pages 19–67.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate Non-Hierarchical Phrase-Based Translation. In
NAACL.
Kevin Gimpel and Noah A. Smith. 2011. Generative
Models of Monolingual and Bilingual Gappy Patterns.
In WMT.
Jin Guo. 1997. Critical Tokenization and its Properties.
Computational Linguistics, vol. 23(4), pages 569–596.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by discard-
ing most of the phrase-table. In EMNLP-CoNLL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL,
demonstration session.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-NAACL.
Spyros Martzoukos, Christophe Costa Florˆencio, and
Christof Monz. 2013. Investigating Connectivity and
Consistency Criteria for Phrase Pair Extraction in Sta-
tistical Machine Translation. In Meeting on Mathe-
matics of Language.
Bernard Monjardet. 1981. Metrics on partially ordered
sets – a survey. Discrete Mathematics, vol. 35, pages
173–184.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, vol. 29 (1), pages 19–51.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In EMNLP-VLC.
Chris Orum and Cliff A. Joslyn. 2009. Valuations and
Metrics on Partially Ordered Sets. Computing Re-
search Repository - CORR, vol. abs/0903.2.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2010.
Integration of Multiple Bilingually-Learned Segmen-
tation Schemes into Statistical Machine Translation.
In WMT and MetricsMATR.
Klaus Ries, Finn Dag Bu, and Alex Waibel. 1996. Class
phrase models for language modeling. In ICSLP.
Reuven Y. Rubinstein. 1997. Optimization of Computer
Simulation Models with Rare Events. European Jour-
nal of Operations Research, vol. 99, pages 89–112.
Reuven Y. Rubinstein and Dirk P. Kroese. 2004. The
Cross-Entropy Method: A Unified Approach to Com-
binatorial Optimization, Monte-Carlo Simulation and
Machine Learning. Springer-Verlag, New York.
Germ´an Sanchis-Trilles, Daniel Ortiz-Martinez, Jes´us
Gonz´alez-Rubio, Jorge Gonz´alez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in Statistical Machine Translation.
In EAMT.
Richard P. Stanley. 1997. Enumerative Combinatorics,
Volume 1. Cambridge University Press.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In ICSLP.
Leslie G. Valiant. 1979. The complexity of enumeration
and reliability problems. SIAM Journal on Comput-
ing, vol. 8, pages 410–421.
Dominic J. A. Welsh. 1997. Approximate counting.
Surveys in Combinatorics, London Math. Soc. Lecture
Notes Ser., 241, pages 287–324.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
Systematic Comparison of Phrase Table Pruning Tech-
niques. In EMNLP.
</reference>
<page confidence="0.999353">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.539334">
<title confidence="0.999849">Maximizing Component Quality in Bilingual Word-Aligned Segmentations</title>
<author confidence="0.743677">Spyros Martzoukos Christophe Costa Florˆencio Christof</author>
<affiliation confidence="0.808776">Intelligent Systems Lab Amsterdam, University of</affiliation>
<address confidence="0.671122">Science Park 904, 1098 XH Amsterdam, The</address>
<email confidence="0.814524">C.CostaFlorencio,</email>
<abstract confidence="0.999784434782609">Given a pair of source and target language sentences which are translations of each other with known word alignments between them, we extract bilingual phrase-level segmentations of such a pair. This is done by identifying two appropriate measures that assess the quality of phrase segments, one on the monolingual level for both language sides, and one on the bilingual level. The monolingual measure is based on the notion of partition refinements and the bilingual measure is based on structural properties of the graph that represents phrase segments and word alignments. These two measures are incorporated in a basic adaptation of the Cross-Entropy method the purpose of extracting an list of bilingual phrase-level segmentations. A straight-forward application of such lists in Statistical Machine Translation (SMT) yields a conservative phrase pair extraction method that reduces phrase-table sizes by 90% with insignificant loss in translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>Adria de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Phrasal Segmentation Models for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<marker>Blackwood, de Gispert, Byrne, 2008</marker>
<rawString>Graeme Blackwood, Adria de Gispert, and William Byrne. 2008. Phrasal Segmentation Models for Statistical Machine Translation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In WMT.</booktitle>
<contexts>
<context position="22489" citStr="Bojar et al., 2013" startWordPosition="3813" endWordPosition="3816"> and a reordering model with monotone, swap and discontinuous orientations, conditioned on both the previous and the next phrase. 4-gram interpolated language models with Kneser-Ney smoothing are built with SRILM (Stolcke, 2002). A distortion limit of 6 and a phrasepenalty are also used. All model parameters are tuned with MERT (Och, 2003). Decoding during tuning and testing is done with Moses (Koehn et. al, 2007). Since our system only affects which phrases are extracted, lexical weights and reordering orientations are the same for both systems. Datasets are from the WMT’13 translation task (Bojar et al., 2013): Translation and reordering models are trained on Czech–English and German– English corpora (Table 1). Language models and segment measures gain, as defined in (5), are trained on 35.3M Czech, 50.0M German and 94.5M English sentences from the provided monolingual data. Tuning is done on newstest2010 and performance is evaluated on newstest2008, newstest2009, newstest2011 and newstest2012 with BLEU (Papineni et al., 2001). In our experiments the size of an N-best list varies according to the total number of words in the sentence pair, say w. For the purposes of phrase extraction in SMT we woul</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlie J Coulbourn</author>
</authors>
<title>The Combinatorics of Network Reliability.</title>
<date>1987</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="14622" citStr="Coulbourn, 1987" startWordPosition="2442" endWordPosition="2443">igned segmentation (σ, τ) is given by f (σ, τ) = (ri cEC which takes values in (0, 1]. The relation f(σ, τ) &gt; f(σ&apos;, τ&apos;) implies that (σ, τ) is a better bilingual segmentation than (σ&apos;, τ&apos;). We conclude this section with two remarks: (i) A component with no translation edges, i.e., a source or target segment whose words are all unaligned, has a contribution of 1/0 in (8). In practice we exclude such components from C. (ii) In graph theory the quantity gain(c) is known as the number of connected spanning subgraphs (CSSGs) of graph c and is the key quantity of network reliability (Valiant, 1979; Coulbourn, 1987). Finding the number of CSSGs of a general graph is a known #P-hard problem (Welsh, 1997). In our setting, graphs have specific formation (source and target chains connected via translation edges) and we are interested in the deletion of translation edges only; it is possible to 0 1 2 3 4 5 6 0 1 2 3 4 5 0 1 2 3 4 5 6 0 1 2 3 4 5 � 1 , (8) gain(c) ICI 2|ac |− 1 33 compute gain(·) in polynomial time, but we do not go into further details here. 4 Extracting Bilingual Segmentations with the Cross-Entropy Method Equipped with the measures of Sections 2 and 3 we turn to extracting an N-best list of</context>
</contexts>
<marker>Coulbourn, 1987</marker>
<rawString>Charlie J. Coulbourn. 1987. The Combinatorics of Network Reliability. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pieter-Tjerk De Boer</author>
<author>Dirk P Kroese</author>
<author>Shie Mannor</author>
<author>Reuven Y Rubinstein</author>
</authors>
<date>2005</date>
<journal>A Tutorial on the CrossEntropy Method. Annals of Operations Research,</journal>
<volume>134</volume>
<pages>pages</pages>
<marker>De Boer, Kroese, Mannor, Rubinstein, 2005</marker>
<rawString>Pieter-Tjerk De Boer, Dirk P. Kroese, Shie Mannor, and Reuven Y. Rubinstein. 2005. A Tutorial on the CrossEntropy Method. Annals of Operations Research, vol. 134, pages 19–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Non-Hierarchical Phrase-Based Translation.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="27567" citStr="Galley and Manning, 2010" startWordPosition="4629" endWordPosition="4632"> CE method for generating monolingual segmentations. By using (6) as the objective function, we observed that the resulting segmentations yield promising applications in n-gram topic modeling, named entity recognition and Chinese segmentation. However, in the spirit of Ries et al. (1996), attempts to minimize perplexity instead of maximizing (6), resulted in larger segments and the segment quality definition of Section 1 was not met. The sizes of the resulting phrase-tables together with the type of phrase pairs that are extracted lead to applications involving discontinuous phrase pairs. In (Galley and Manning, 2010) there was evidence that discontinuous phrase pairs that are extracted from discontinuous components of word-aligned sentence pairs can improve translation quality.1 As the number of such components is much bigger than the continuous ones, (Gimpel and Smith, 2011) propose a Bayesian nonparametric model for finding the most probable discontinuous phrase pairs. This can also be done from the N-best lists that are generated in Section 4, and it would be interesting to see the effect of such phrase pairs in our existing models. In a longer version of this work we intend to study the effect in tran</context>
</contexts>
<marker>Galley, Manning, 2010</marker>
<rawString>Michel Galley and Christopher D. Manning. 2010. Accurate Non-Hierarchical Phrase-Based Translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Generative Models of Monolingual and Bilingual Gappy Patterns.</title>
<date>2011</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="27831" citStr="Gimpel and Smith, 2011" startWordPosition="4667" endWordPosition="4670"> of Ries et al. (1996), attempts to minimize perplexity instead of maximizing (6), resulted in larger segments and the segment quality definition of Section 1 was not met. The sizes of the resulting phrase-tables together with the type of phrase pairs that are extracted lead to applications involving discontinuous phrase pairs. In (Galley and Manning, 2010) there was evidence that discontinuous phrase pairs that are extracted from discontinuous components of word-aligned sentence pairs can improve translation quality.1 As the number of such components is much bigger than the continuous ones, (Gimpel and Smith, 2011) propose a Bayesian nonparametric model for finding the most probable discontinuous phrase pairs. This can also be done from the N-best lists that are generated in Section 4, and it would be interesting to see the effect of such phrase pairs in our existing models. In a longer version of this work we intend to study the effect in translation quality when varying some of the parameters (size of N-best lists, sample sizes for training gain in Section 2 and for the CE method), as well as when extracting source-driven bilingual segmentations as in (Sanchis-Trilles et al., 2011). 7 Conclusions In t</context>
</contexts>
<marker>Gimpel, Smith, 2011</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2011. Generative Models of Monolingual and Bilingual Gappy Patterns. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Guo</author>
</authors>
<title>Critical Tokenization and its Properties.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<pages>569--596</pages>
<contexts>
<context position="3075" citStr="Guo, 1997" startWordPosition="472" endWordPosition="473">is fairly general it is thus suitable for most NLP tasks. In particular, it is tailored to the type of segments that are suitable for the purposes of SMT and is in line with previous work (Blackwood et al., 2008; Paul et al., 2010). With this definition in mind, we introduce a monolingual segment quality measure that is based on assessing the cost of converting one segmentation into another by means of an elementary operation. This operation, namely the ‘splitting’ of a segment into two segments, together with all possible segmentations of a sentence are known to form a partially ordered set (Guo, 1997). Such a construction is known as partition refinement and gives rise to the desired monolingual surface quality measure. The presence of word alignments between the sentence pair provides additional structure which should not be ignored. In the language of graph theory, a segment can also be viewed as a chain, i.e., a graph in which vertices are the segment’s words and 30 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30–38, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics an edge between</context>
<context position="6203" citStr="Guo, 1997" startWordPosition="987" endWordPosition="988">are used to distinguish the segments s1s2 and s3s4s5, then σ can be perturbed in three different ways: • σ&apos; : (s1)(s2)(s3s4s5), by splitting the first segment of σ. • σ&apos;&apos; : (s1s2)(s3)(s4s5), by splitting at the first position of the second segment of σ. • σ&apos;&apos;&apos; : (s1s2)(s3s4)(s5), by splitting at the second position of the second segment of σ, so that σ&apos;, σ&apos;&apos; and σ&apos;&apos;&apos; are the perturbations of σ. Such perturbations are known as partition refinements in the literature (Stanley, 1997). The set of all segmentations of a sentence, equipped with the splitting operation forms a partially ordered set (Guo, 1997), and its visual representation is known as the Hasse diagram. Figure 1 shows such a partially ordered set for a sentence with four words. s1 s2 s3 s4 s1s2s3s4 s1s2s3s4 s1s2 s3s4 s1s2s3s4 s1s2s3s4 s1s2s3s4 s1s2s3s4 Figure 1: Hasse diagram of segmentation refinements for a sentence with four words. The cost of perturbing a segmentation into another, i.e., the weight of a directed edge in the Hasse diagram, is calculated from n-gram counts that are extracted from a monolingual training corpus. Let n(s) be the empirical count of phrase s in the corpus. Give</context>
</contexts>
<marker>Guo, 1997</marker>
<rawString>Jin Guo. 1997. Critical Tokenization and its Properties. Computational Linguistics, vol. 23(4), pages 569–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrase-table.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="5045" citStr="Johnson et al., 2007" startWordPosition="781" endWordPosition="784">which is an adaptation of the Cross-Entropy method (Rubinstein, 1997), performs joint maximization of surface (in both languages) and structural quality measures. Components of graph representations of the resulting N-best lists give rise to high quality translation units. These units, which form a small subset of all possible (continuous) consistent phrase pairs, are used to construct SMT models. Results on Czech–English and German–English datasets show a 90% reduction in phrase-table sizes with insignificant loss in translation quality which are in line with other pruning techniques in SMT (Johnson et al., 2007; Zens et al., 2012). 2 Monolingual Surface Quality Measure Given a sentence s1s2...sk that consists of words sz, 1 &lt; i &lt; k, we introduce an empirical countbased measure that assesses the quality of its segmentations. By fixing a segmentation σ, we are interested in assessing the cost of perturbing σ and generating another segmentation σ&apos;. A perturbation of σ is achieved by splitting a segment of σ into two new segments, while keeping all other segments fixed. For example, for a sentence with five words, if σ : (s1s2)(s3s4s5), where brackets are used to distinguish the segments s1s2 and s3s4s5</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrase-table. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. In ACL, demonstration session.</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL, demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="3997" citStr="Koehn et al., 2003" startWordPosition="619" endWordPosition="622"> a chain, i.e., a graph in which vertices are the segment’s words and 30 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30–38, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics an edge between two words exists if and only if these words are consecutive. Then, a bilingual segmentation is represented by the graph that is formed by all its source and target language chains together with edges induced by word alignments. Motivated by the phrase pair extraction methods of SMT (Och et al., 1999; Koehn et al., 2003), we focus on the connected components, or simply components of such a representation. We explain that the extent to which we can delete word alignments from a component without violating its component status, gives rise to a bilingual, purely structural quality measure. The surface and structural measures are incorporated in one algorithm that extracts an N-best list of bilingual word-aligned segmentations. This algorithm, which is an adaptation of the Cross-Entropy method (Rubinstein, 1997), performs joint maximization of surface (in both languages) and structural quality measures. Component</context>
<context position="10545" citStr="Koehn et al., 2003" startWordPosition="1741" endWordPosition="1744">ntation, namely its connected components, or simply components. A component is a graph such that (a) there exists a path between any two of its vertices, and (b) there does not exist a path between a vertex of the component and a vertex outside the component. Condition (a) means, both technically and intuitively, that a component is connected and Condition (b) requires connectivity to be maximal. Components play a key role in SMT. The most widely used strategy for extracting high quality phrase-level translations without linguistic information, namely the consistency method (Och et al., 1999; Koehn et al., 2003) is entirely based on components of word aligned unsegmented sentence pairs (Martzoukos et al., 2013). In particular, each extracted translation is either a component or the union of components. Since an unsegmented sentence pair is just one possible configuration of all possible bilingual segmentations, we consequently have no direct reason to investigate further than components. In order to get an intuition of the measure that will be introduced in this section, we begin with an example. Figure 2, shows two different configurations of the pair (Q, T) for the same sentence pair with known and</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Martzoukos</author>
<author>Christophe Costa Florˆencio</author>
<author>Christof Monz</author>
</authors>
<title>Investigating Connectivity and Consistency Criteria for Phrase Pair Extraction in Statistical Machine Translation.</title>
<date>2013</date>
<booktitle>In Meeting on Mathematics of Language.</booktitle>
<marker>Martzoukos, Florˆencio, Monz, 2013</marker>
<rawString>Spyros Martzoukos, Christophe Costa Florˆencio, and Christof Monz. 2013. Investigating Connectivity and Consistency Criteria for Phrase Pair Extraction in Statistical Machine Translation. In Meeting on Mathematics of Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Monjardet</author>
</authors>
<title>Metrics on partially ordered sets – a survey.</title>
<date>1981</date>
<journal>Discrete Mathematics,</journal>
<volume>35</volume>
<pages>173--184</pages>
<contexts>
<context position="7506" citStr="Monjardet, 1981" startWordPosition="1218" endWordPosition="1219">e example we have for instance seg(σ&apos;&apos;) = {s1s2, s3, s4s5}. The probability of s in σ is given by relative frequencies ( _ n(s) pv S) — � (1) �,Eseg(�) n(s&apos;). The cost of perturbing σ into σ&apos; by splitting a segment s¯s of σ into segments s and s¯ is defined by cost,,,,(s, ¯s) = log p�(s¯s)pQ (s)P&amp;quot; (2) p and we say that s and s¯ are co-responsible for the perturbation σ → σ&apos;. Intuitively, this cost function yields the amount of energy (log of probability) that is lost when performing a perturbation. On a more 31 technical level, it is closely related to metric spaces on partially ordered sets (Monjardet, 1981; Orum and Joslyn, 2009), but we do not go into further details here. The cost function admits a measure for the segments that are co-responsible for perturbing Q into Q0 and we define the gain of s from the perturbation Q → Q0 as gain,→,,(s) = −cost,→,,(s, ¯s). (3) A segment s may be co-responsible for different perturbations, and we have to consider all such perturbations. Let R(s) = {Q → Q0 : s E/ seg(Q), s E seg(Q0)} (4) denote the set of perturbations for which s is coresponsible. Then, the average gain of s in the sentence is given by gain,→,,(s). (5) Intuitively, gain(s) measures how di</context>
</contexts>
<marker>Monjardet, 1981</marker>
<rawString>Bernard Monjardet. 1981. Metrics on partially ordered sets – a survey. Discrete Mathematics, vol. 35, pages 173–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="22211" citStr="Och, 2003" startWordPosition="3770" endWordPosition="3771"> the baseline and our system are standard phrase-based MT systems. Bidirectional word alignments are generated with GIZA++ (Och and Ney, 2003) and ‘grow-diag-final-and’. These are used to construct a phrase-table with bidirectional phrase probabilities, lexical weights and a reordering model with monotone, swap and discontinuous orientations, conditioned on both the previous and the next phrase. 4-gram interpolated language models with Kneser-Ney smoothing are built with SRILM (Stolcke, 2002). A distortion limit of 6 and a phrasepenalty are also used. All model parameters are tuned with MERT (Och, 2003). Decoding during tuning and testing is done with Moses (Koehn et. al, 2007). Since our system only affects which phrases are extracted, lexical weights and reordering orientations are the same for both systems. Datasets are from the WMT’13 translation task (Bojar et al., 2013): Translation and reordering models are trained on Czech–English and German– English corpora (Table 1). Language models and segment measures gain, as defined in (5), are trained on 35.3M Czech, 50.0M German and 94.5M English sentences from the provided monolingual data. Tuning is done on newstest2010 and performance is e</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>pages</pages>
<contexts>
<context position="21743" citStr="Och and Ney, 2003" startWordPosition="3698" endWordPosition="3701">ndependently). Cz–En De–En Europarl (v7) 642,505 1,889,791 News Commentary (v8) 139,679 177,079 Total 782,184 2,066,870 Table 1: Number of filtered parallel sentences for Czech–English and German–English. that correspond to components only. A reduction in phrase-table size is guaranteed because we are essentially extracting only a subset of all possible continuous phrase pairs. The challenge is to verify whether this subset can provide a sufficient translation model. Both the baseline and our system are standard phrase-based MT systems. Bidirectional word alignments are generated with GIZA++ (Och and Ney, 2003) and ‘grow-diag-final-and’. These are used to construct a phrase-table with bidirectional phrase probabilities, lexical weights and a reordering model with monotone, swap and discontinuous orientations, conditioned on both the previous and the next phrase. 4-gram interpolated language models with Kneser-Ney smoothing are built with SRILM (Stolcke, 2002). A distortion limit of 6 and a phrasepenalty are also used. All model parameters are tuned with MERT (Och, 2003). Decoding during tuning and testing is done with Moses (Koehn et. al, 2007). Since our system only affects which phrases are extrac</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, vol. 29 (1), pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Alignment Models for Statistical Machine Translation.</title>
<date>1999</date>
<booktitle>In EMNLP-VLC.</booktitle>
<contexts>
<context position="3976" citStr="Och et al., 1999" startWordPosition="615" endWordPosition="618"> also be viewed as a chain, i.e., a graph in which vertices are the segment’s words and 30 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30–38, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics an edge between two words exists if and only if these words are consecutive. Then, a bilingual segmentation is represented by the graph that is formed by all its source and target language chains together with edges induced by word alignments. Motivated by the phrase pair extraction methods of SMT (Och et al., 1999; Koehn et al., 2003), we focus on the connected components, or simply components of such a representation. We explain that the extent to which we can delete word alignments from a component without violating its component status, gives rise to a bilingual, purely structural quality measure. The surface and structural measures are incorporated in one algorithm that extracts an N-best list of bilingual word-aligned segmentations. This algorithm, which is an adaptation of the Cross-Entropy method (Rubinstein, 1997), performs joint maximization of surface (in both languages) and structural qualit</context>
<context position="10524" citStr="Och et al., 1999" startWordPosition="1737" endWordPosition="1740">hs of this representation, namely its connected components, or simply components. A component is a graph such that (a) there exists a path between any two of its vertices, and (b) there does not exist a path between a vertex of the component and a vertex outside the component. Condition (a) means, both technically and intuitively, that a component is connected and Condition (b) requires connectivity to be maximal. Components play a key role in SMT. The most widely used strategy for extracting high quality phrase-level translations without linguistic information, namely the consistency method (Och et al., 1999; Koehn et al., 2003) is entirely based on components of word aligned unsegmented sentence pairs (Martzoukos et al., 2013). In particular, each extracted translation is either a component or the union of components. Since an unsegmented sentence pair is just one possible configuration of all possible bilingual segmentations, we consequently have no direct reason to investigate further than components. In order to get an intuition of the measure that will be introduced in this section, we begin with an example. Figure 2, shows two different configurations of the pair (Q, T) for the same sentenc</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz J. Och, Christoph Tillmann, and Hermann Ney. 1999. Improved Alignment Models for Statistical Machine Translation. In EMNLP-VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Orum</author>
<author>Cliff A Joslyn</author>
</authors>
<title>Valuations and Metrics on Partially Ordered Sets.</title>
<date>2009</date>
<journal>Computing Research Repository - CORR,</journal>
<volume>vol.</volume>
<pages>0903--2</pages>
<contexts>
<context position="7530" citStr="Orum and Joslyn, 2009" startWordPosition="1220" endWordPosition="1223"> for instance seg(σ&apos;&apos;) = {s1s2, s3, s4s5}. The probability of s in σ is given by relative frequencies ( _ n(s) pv S) — � (1) �,Eseg(�) n(s&apos;). The cost of perturbing σ into σ&apos; by splitting a segment s¯s of σ into segments s and s¯ is defined by cost,,,,(s, ¯s) = log p�(s¯s)pQ (s)P&amp;quot; (2) p and we say that s and s¯ are co-responsible for the perturbation σ → σ&apos;. Intuitively, this cost function yields the amount of energy (log of probability) that is lost when performing a perturbation. On a more 31 technical level, it is closely related to metric spaces on partially ordered sets (Monjardet, 1981; Orum and Joslyn, 2009), but we do not go into further details here. The cost function admits a measure for the segments that are co-responsible for perturbing Q into Q0 and we define the gain of s from the perturbation Q → Q0 as gain,→,,(s) = −cost,→,,(s, ¯s). (3) A segment s may be co-responsible for different perturbations, and we have to consider all such perturbations. Let R(s) = {Q → Q0 : s E/ seg(Q), s E seg(Q0)} (4) denote the set of perturbations for which s is coresponsible. Then, the average gain of s in the sentence is given by gain,→,,(s). (5) Intuitively, gain(s) measures how difficult it is to break p</context>
</contexts>
<marker>Orum, Joslyn, 2009</marker>
<rawString>Chris Orum and Cliff A. Joslyn. 2009. Valuations and Metrics on Partially Ordered Sets. Computing Research Repository - CORR, vol. abs/0903.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="22914" citStr="Papineni et al., 2001" startWordPosition="3876" endWordPosition="3879">ce our system only affects which phrases are extracted, lexical weights and reordering orientations are the same for both systems. Datasets are from the WMT’13 translation task (Bojar et al., 2013): Translation and reordering models are trained on Czech–English and German– English corpora (Table 1). Language models and segment measures gain, as defined in (5), are trained on 35.3M Czech, 50.0M German and 94.5M English sentences from the provided monolingual data. Tuning is done on newstest2010 and performance is evaluated on newstest2008, newstest2009, newstest2011 and newstest2012 with BLEU (Papineni et al., 2001). In our experiments the size of an N-best list varies according to the total number of words in the sentence pair, say w. For the purposes of phrase extraction in SMT we would ideally require all local maxima to be part of an N-best list. This would 35 Method Baseline N-best N-best &amp; unseg. Czech-*English ’08 ’09 ’11 ’12 19.6 20.6 22.6 20.6 19.7 20.4 22.4 20.3 19.6 20.5 22.6 20.7 English-*Czech ’08 ’09 ’11 ’12 14.8 15.6 16.6 14.9 14.4 15.2 16.3 14.3 14.6 15.4 16.8 14.7 Czech–English PT size (retain%) 44.6M (100%) 4.4M (9.8%) 4.6M (10.4%) Table 2: BLEU scores and phrase-table (PT) sizes for Cz</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<date>2010</date>
<booktitle>Integration of Multiple Bilingually-Learned Segmentation Schemes into Statistical Machine Translation. In WMT and MetricsMATR.</booktitle>
<contexts>
<context position="2696" citStr="Paul et al., 2010" startWordPosition="408" endWordPosition="411">notion of a segmentation of maximum quality is defined as the set of consecutive words of the sentence that captures maximum collocational and/or grammatical characteristics. This implies that a sequence of tokens is identified as a segment if its fully compositional expressive power is higher than the expressive power of any combination of partial compositions. Since this definition is fairly general it is thus suitable for most NLP tasks. In particular, it is tailored to the type of segments that are suitable for the purposes of SMT and is in line with previous work (Blackwood et al., 2008; Paul et al., 2010). With this definition in mind, we introduce a monolingual segment quality measure that is based on assessing the cost of converting one segmentation into another by means of an elementary operation. This operation, namely the ‘splitting’ of a segment into two segments, together with all possible segmentations of a sentence are known to form a partially ordered set (Guo, 1997). Such a construction is known as partition refinement and gives rise to the desired monolingual surface quality measure. The presence of word alignments between the sentence pair provides additional structure which shoul</context>
</contexts>
<marker>Paul, Finch, Sumita, 2010</marker>
<rawString>Michael Paul, Andrew Finch, and Eiichiro Sumita. 2010. Integration of Multiple Bilingually-Learned Segmentation Schemes into Statistical Machine Translation. In WMT and MetricsMATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Ries</author>
<author>Finn Dag Bu</author>
<author>Alex Waibel</author>
</authors>
<title>Class phrase models for language modeling.</title>
<date>1996</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="27230" citStr="Ries et al. (1996)" startWordPosition="4577" endWordPosition="4580">neral approach that unifies partition refinements and network reliability for the purpose of identifying highly stable multilingual units. 36 Figure 4: Typical fragments from best performing German–English segmentations. The focus has been on bilingual segmentations, but as was mentioned in Section 2, it is possible to apply the CE method for generating monolingual segmentations. By using (6) as the objective function, we observed that the resulting segmentations yield promising applications in n-gram topic modeling, named entity recognition and Chinese segmentation. However, in the spirit of Ries et al. (1996), attempts to minimize perplexity instead of maximizing (6), resulted in larger segments and the segment quality definition of Section 1 was not met. The sizes of the resulting phrase-tables together with the type of phrase pairs that are extracted lead to applications involving discontinuous phrase pairs. In (Galley and Manning, 2010) there was evidence that discontinuous phrase pairs that are extracted from discontinuous components of word-aligned sentence pairs can improve translation quality.1 As the number of such components is much bigger than the continuous ones, (Gimpel and Smith, 2011</context>
</contexts>
<marker>Ries, Bu, Waibel, 1996</marker>
<rawString>Klaus Ries, Finn Dag Bu, and Alex Waibel. 1996. Class phrase models for language modeling. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reuven Y Rubinstein</author>
</authors>
<title>Optimization of Computer Simulation Models with Rare Events.</title>
<date>1997</date>
<journal>European Journal of Operations Research,</journal>
<volume>99</volume>
<pages>89--112</pages>
<contexts>
<context position="4494" citStr="Rubinstein, 1997" startWordPosition="697" endWordPosition="698">nduced by word alignments. Motivated by the phrase pair extraction methods of SMT (Och et al., 1999; Koehn et al., 2003), we focus on the connected components, or simply components of such a representation. We explain that the extent to which we can delete word alignments from a component without violating its component status, gives rise to a bilingual, purely structural quality measure. The surface and structural measures are incorporated in one algorithm that extracts an N-best list of bilingual word-aligned segmentations. This algorithm, which is an adaptation of the Cross-Entropy method (Rubinstein, 1997), performs joint maximization of surface (in both languages) and structural quality measures. Components of graph representations of the resulting N-best lists give rise to high quality translation units. These units, which form a small subset of all possible (continuous) consistent phrase pairs, are used to construct SMT models. Results on Czech–English and German–English datasets show a 90% reduction in phrase-table sizes with insignificant loss in translation quality which are in line with other pruning techniques in SMT (Johnson et al., 2007; Zens et al., 2012). 2 Monolingual Surface Quali</context>
<context position="15560" citStr="Rubinstein, 1997" startWordPosition="2622" endWordPosition="2624"> 1 2 3 4 5 � 1 , (8) gain(c) ICI 2|ac |− 1 33 compute gain(·) in polynomial time, but we do not go into further details here. 4 Extracting Bilingual Segmentations with the Cross-Entropy Method Equipped with the measures of Sections 2 and 3 we turn to extracting an N-best list of bilingual segmentations for a given sentence pair. The search space is exponential in the total number of words of the sentence pair. We propose a new approach for this task, by noting a direct connection with the combinatorial problems that can be solved efficiently and effectively with the Cross-Entropy (CE) method (Rubinstein, 1997). The CE method is an iterative self-tuning sampling method that has applications in various combinatorial and continuous global optimization problems as well as in rare event detection. A detailed account on the CE method is beyond the scope of this work, and we thus simply describe its application to our problem. In particular, we first establish the connection between the most basic form of the CE method and the problem of finding the best monolingual segmentation of a sentence, with respect to some scoring function (not necessarily the one that was introduced in Section 2). This connection</context>
</contexts>
<marker>Rubinstein, 1997</marker>
<rawString>Reuven Y. Rubinstein. 1997. Optimization of Computer Simulation Models with Rare Events. European Journal of Operations Research, vol. 99, pages 89–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reuven Y Rubinstein</author>
<author>Dirk P Kroese</author>
</authors>
<title>The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning.</title>
<date>2004</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="18918" citStr="Rubinstein and Kroese, 2004" startWordPosition="3235" endWordPosition="3238"> samples performing less than this threshold will be ignored. 3. Use the best performing sub-sample of b1, ..., bM to update position probabilities: t EMi=1 Ii(γt)bij( ) pj = M , j = 1, ..., E, 9 �i=1 Ii(γt) where the choice function Ii is given by � 1, if g(bi) &gt; γt Ii(γt) = 0, otherwise. 4. Smooth the updated position probabilities as ptj := αptj + (1 — α)pt−1 j , j = 1, ..., `. (10) E. If for some t &gt; 5 we have γt = γt−1 = ... = γt−5 then stop. Else, t := t + 1 and go to Step 1. 34 The values for the parameters M, p and α reported here are in line with the ones suggested in the literature (Rubinstein and Kroese, 2004) for combinatorial problems such as this one. After the execution of the algorithm, the updated vector of position probabilities converges to sequence of ‘0’s and ‘1’s, which corresponds to the best segmentation under g. The extension to bilingual level is done by incorporating the structural quality measure of Section 3. The setting is similar, i.e., samples are again bitstrings, but of length E = n + m − 2, where n and m are the number of words in the source and target sentence respectively. The first n − 1 bits correspond to the source sentence and the rest to the target sentence. The surfa</context>
</contexts>
<marker>Rubinstein, Kroese, 2004</marker>
<rawString>Reuven Y. Rubinstein and Dirk P. Kroese. 2004. The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Germ´an Sanchis-Trilles</author>
<author>Daniel Ortiz-Martinez</author>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Jorge Gonz´alez</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Bilingual segmentation for phrasetable pruning in Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In EAMT.</booktitle>
<marker>Sanchis-Trilles, Ortiz-Martinez, Gonz´alez-Rubio, Gonz´alez, Casacuberta, 2011</marker>
<rawString>Germ´an Sanchis-Trilles, Daniel Ortiz-Martinez, Jes´us Gonz´alez-Rubio, Jorge Gonz´alez, and Francisco Casacuberta. 2011. Bilingual segmentation for phrasetable pruning in Statistical Machine Translation. In EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard P Stanley</author>
</authors>
<date>1997</date>
<journal>Enumerative Combinatorics,</journal>
<volume>1</volume>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6078" citStr="Stanley, 1997" startWordPosition="966" endWordPosition="967">ents, while keeping all other segments fixed. For example, for a sentence with five words, if σ : (s1s2)(s3s4s5), where brackets are used to distinguish the segments s1s2 and s3s4s5, then σ can be perturbed in three different ways: • σ&apos; : (s1)(s2)(s3s4s5), by splitting the first segment of σ. • σ&apos;&apos; : (s1s2)(s3)(s4s5), by splitting at the first position of the second segment of σ. • σ&apos;&apos;&apos; : (s1s2)(s3s4)(s5), by splitting at the second position of the second segment of σ, so that σ&apos;, σ&apos;&apos; and σ&apos;&apos;&apos; are the perturbations of σ. Such perturbations are known as partition refinements in the literature (Stanley, 1997). The set of all segmentations of a sentence, equipped with the splitting operation forms a partially ordered set (Guo, 1997), and its visual representation is known as the Hasse diagram. Figure 1 shows such a partially ordered set for a sentence with four words. s1 s2 s3 s4 s1s2s3s4 s1s2s3s4 s1s2 s3s4 s1s2s3s4 s1s2s3s4 s1s2s3s4 s1s2s3s4 Figure 1: Hasse diagram of segmentation refinements for a sentence with four words. The cost of perturbing a segmentation into another, i.e., the weight of a directed edge in the Hasse diagram, is calculated from n-gram </context>
</contexts>
<marker>Stanley, 1997</marker>
<rawString>Richard P. Stanley. 1997. Enumerative Combinatorics, Volume 1. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="22098" citStr="Stolcke, 2002" startWordPosition="3748" endWordPosition="3750">tinuous phrase pairs. The challenge is to verify whether this subset can provide a sufficient translation model. Both the baseline and our system are standard phrase-based MT systems. Bidirectional word alignments are generated with GIZA++ (Och and Ney, 2003) and ‘grow-diag-final-and’. These are used to construct a phrase-table with bidirectional phrase probabilities, lexical weights and a reordering model with monotone, swap and discontinuous orientations, conditioned on both the previous and the next phrase. 4-gram interpolated language models with Kneser-Ney smoothing are built with SRILM (Stolcke, 2002). A distortion limit of 6 and a phrasepenalty are also used. All model parameters are tuned with MERT (Och, 2003). Decoding during tuning and testing is done with Moses (Koehn et. al, 2007). Since our system only affects which phrases are extracted, lexical weights and reordering orientations are the same for both systems. Datasets are from the WMT’13 translation task (Bojar et al., 2013): Translation and reordering models are trained on Czech–English and German– English corpora (Table 1). Language models and segment measures gain, as defined in (5), are trained on 35.3M Czech, 50.0M German an</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>The complexity of enumeration and reliability problems.</title>
<date>1979</date>
<journal>SIAM Journal on Computing,</journal>
<volume>8</volume>
<pages>410--421</pages>
<contexts>
<context position="14604" citStr="Valiant, 1979" startWordPosition="2440" endWordPosition="2441">lingual word-aligned segmentation (σ, τ) is given by f (σ, τ) = (ri cEC which takes values in (0, 1]. The relation f(σ, τ) &gt; f(σ&apos;, τ&apos;) implies that (σ, τ) is a better bilingual segmentation than (σ&apos;, τ&apos;). We conclude this section with two remarks: (i) A component with no translation edges, i.e., a source or target segment whose words are all unaligned, has a contribution of 1/0 in (8). In practice we exclude such components from C. (ii) In graph theory the quantity gain(c) is known as the number of connected spanning subgraphs (CSSGs) of graph c and is the key quantity of network reliability (Valiant, 1979; Coulbourn, 1987). Finding the number of CSSGs of a general graph is a known #P-hard problem (Welsh, 1997). In our setting, graphs have specific formation (source and target chains connected via translation edges) and we are interested in the deletion of translation edges only; it is possible to 0 1 2 3 4 5 6 0 1 2 3 4 5 0 1 2 3 4 5 6 0 1 2 3 4 5 � 1 , (8) gain(c) ICI 2|ac |− 1 33 compute gain(·) in polynomial time, but we do not go into further details here. 4 Extracting Bilingual Segmentations with the Cross-Entropy Method Equipped with the measures of Sections 2 and 3 we turn to extracting</context>
</contexts>
<marker>Valiant, 1979</marker>
<rawString>Leslie G. Valiant. 1979. The complexity of enumeration and reliability problems. SIAM Journal on Computing, vol. 8, pages 410–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic J A Welsh</author>
</authors>
<title>Approximate counting.</title>
<date>1997</date>
<journal>Surveys in Combinatorics, London Math. Soc. Lecture Notes Ser.,</journal>
<volume>241</volume>
<pages>287--324</pages>
<contexts>
<context position="14711" citStr="Welsh, 1997" startWordPosition="2459" endWordPosition="2460">lation f(σ, τ) &gt; f(σ&apos;, τ&apos;) implies that (σ, τ) is a better bilingual segmentation than (σ&apos;, τ&apos;). We conclude this section with two remarks: (i) A component with no translation edges, i.e., a source or target segment whose words are all unaligned, has a contribution of 1/0 in (8). In practice we exclude such components from C. (ii) In graph theory the quantity gain(c) is known as the number of connected spanning subgraphs (CSSGs) of graph c and is the key quantity of network reliability (Valiant, 1979; Coulbourn, 1987). Finding the number of CSSGs of a general graph is a known #P-hard problem (Welsh, 1997). In our setting, graphs have specific formation (source and target chains connected via translation edges) and we are interested in the deletion of translation edges only; it is possible to 0 1 2 3 4 5 6 0 1 2 3 4 5 0 1 2 3 4 5 6 0 1 2 3 4 5 � 1 , (8) gain(c) ICI 2|ac |− 1 33 compute gain(·) in polynomial time, but we do not go into further details here. 4 Extracting Bilingual Segmentations with the Cross-Entropy Method Equipped with the measures of Sections 2 and 3 we turn to extracting an N-best list of bilingual segmentations for a given sentence pair. The search space is exponential in th</context>
</contexts>
<marker>Welsh, 1997</marker>
<rawString>Dominic J. A. Welsh. 1997. Approximate counting. Surveys in Combinatorics, London Math. Soc. Lecture Notes Ser., 241, pages 287–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Daisy Stanton</author>
<author>Peng Xu</author>
</authors>
<title>A Systematic Comparison of Phrase Table Pruning Techniques.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5065" citStr="Zens et al., 2012" startWordPosition="785" endWordPosition="788"> of the Cross-Entropy method (Rubinstein, 1997), performs joint maximization of surface (in both languages) and structural quality measures. Components of graph representations of the resulting N-best lists give rise to high quality translation units. These units, which form a small subset of all possible (continuous) consistent phrase pairs, are used to construct SMT models. Results on Czech–English and German–English datasets show a 90% reduction in phrase-table sizes with insignificant loss in translation quality which are in line with other pruning techniques in SMT (Johnson et al., 2007; Zens et al., 2012). 2 Monolingual Surface Quality Measure Given a sentence s1s2...sk that consists of words sz, 1 &lt; i &lt; k, we introduce an empirical countbased measure that assesses the quality of its segmentations. By fixing a segmentation σ, we are interested in assessing the cost of perturbing σ and generating another segmentation σ&apos;. A perturbation of σ is achieved by splitting a segment of σ into two new segments, while keeping all other segments fixed. For example, for a sentence with five words, if σ : (s1s2)(s3s4s5), where brackets are used to distinguish the segments s1s2 and s3s4s5, then σ can be pert</context>
</contexts>
<marker>Zens, Stanton, Xu, 2012</marker>
<rawString>Richard Zens, Daisy Stanton, and Peng Xu. 2012. A Systematic Comparison of Phrase Table Pruning Techniques. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>