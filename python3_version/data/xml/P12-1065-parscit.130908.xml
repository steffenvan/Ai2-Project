<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.5188635">
Bootstrapping via Graph Propagation
Max Whitney and Anoop Sarkar*
Simon Fraser University, School of Computing Science
Burnaby, BC V5A 1S6, Canada
</note>
<email confidence="0.922271">
{mwhitney,anoop}@sfu.ca
</email>
<sectionHeader confidence="0.981403" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995919416666667">
Bootstrapping a classifier from a small set
of seed rules can be viewed as the propaga-
tion of labels between examples via features
shared between them. This paper introduces a
novel variant of the Yarowsky algorithm based
on this view. It is a bootstrapping learning
method which uses a graph propagation algo-
rithm with a well defined objective function.
The experimental results show that our pro-
posed bootstrapping algorithm achieves state
of the art performance or better on several dif-
ferent natural language data sets.
</bodyText>
<sectionHeader confidence="0.992541" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.967134085714286">
In this paper, we are concerned with a case of semi-
supervised learning that is close to unsupervised
learning, in that the labelled and unlabelled data
points are from the same domain and only a small
set of seed rules is used to derive the labelled points.
We refer to this setting as bootstrapping. In contrast,
typical semi-supervised learning deals with a large
number of labelled points, and a domain adaptation
task with unlabelled points from the new domain.
The two dominant discriminative learning meth-
ods for bootstrapping are self-training (Scud-
der, 1965) and co-training (Blum and Mitchell,
1998). In this paper we focus on a self-training
style bootstrapping algorithm, the Yarowsky algo-
rithm (Yarowsky, 1995). Variants of this algorithm
have been formalized as optimizing an objective
function in previous work by Abney (2004) and Haf-
fari and Sarkar (2007), but it is not clear that any
perform as well as the Yarowsky algorithm itself.
We take advantage of this formalization and in-
troduce a novel algorithm called Yarowsky-prop
which builds on the algorithms of Yarowsky (1995)
and Subramanya et al. (2010). It is theoretically
*This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant. We would like to thank Gho-
lamreza Haffari and the anonymous reviewers for their com-
ments. We particularly thank Michael Collins, Jason Eisner, and
Damianos Karakos for the data we used in our experiments.
x denotes an example
f, g denote features
i, k denote labels
X set of training examples
Fx set of features for example x
Y current labelling of X
Yx current label for example x
</bodyText>
<note confidence="0.949754">
L value of Yx for unlabelled examples
L number of labels (not including L)
A set of currently labelled examples
V set of currently unlabelled examples
Xf set of examples with feature f
Af set of currently labelled examples with f
Vf set of currently unlabelled examples with f
Aj set of examples currently labelled with j
Afj set of examples with f currently labelled with j
</note>
<tableCaption confidence="0.996655">
Table 1: Notation of Abney (2004).
</tableCaption>
<bodyText confidence="0.999911">
well-understood as minimizing an objective func-
tion at each iteration, and it obtains state of the art
performance on several different NLP data sets. To
our knowledge, this is the first theoretically mo-
tivated self-training bootstrapping algorithm which
performs as well as the Yarowsky algorithm.
</bodyText>
<sectionHeader confidence="0.964639" genericHeader="introduction">
2 Bootstrapping
</sectionHeader>
<bodyText confidence="0.993729368421053">
Abney (2004) defines useful notation for semi-
supervised learning, shown in table 1. Note that A,
V , etc. are relative to the current labelling Y . We
additionally define F to be the set of all features,
and use U to denote the uniform distribution. In the
bootstrapping setting the learner is given an initial
partial labelling Y (°) where only a few examples are
labelled (i.e. Y (°)
x = L for most x).
Abney (2004) defines three probability distribu-
tions in his analysis of bootstrapping: Bfj is the pa-
rameter for feature f with label j, taken to be nor-
malized so that Bf is a distribution over labels. ox is
the labelling distribution representing the current Y ;
it is a point distribution for labelled examples and
uniform for unlabelled examples. 7rx is the predic-
tion distribution over labels for example x.
The approach of Haghighi and Klein (2006b) and
Haghighi and Klein (2006a) also uses a small set of
</bodyText>
<note confidence="0.8516822">
620
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 620–628,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
Algorithm 1: The basic Yarowsky algorithm.
Require: training data X and a seed DL B(°)
</note>
<listItem confidence="0.8057978">
1: apply B(°) to X produce a labelling Y (°)
2: for iteration t to maximum or convergence do
3: train a new DL B on Y (t)
4: apply B to X, to produce Y (t+1)
5: end for
</listItem>
<bodyText confidence="0.999701272727273">
seed rules but uses them to inject features into a joint
model p(x, j) which they train using expectation-
maximization for Markov random fields. We focus
on discriminative training which does not require
complex partition functions for normalization. Blum
and Chawla (2001) introduce an early use of trans-
ductive learning using graph propagation. X. Zhu
and Z. Ghahramani and J. Lafferty (2003)’s method
of graph propagation is predominantly transductive,
and the non-transductive version is closely related to
Abney (2004) c.f. Haffari and Sarkar (2007).1
</bodyText>
<sectionHeader confidence="0.897283" genericHeader="method">
3 Existing algorithms
</sectionHeader>
<subsectionHeader confidence="0.991969">
3.1 Yarowsky
</subsectionHeader>
<bodyText confidence="0.999822909090909">
A decision list (DL) is a (ordered) list of feature-
label pairs (rules) which is produced by assigning
a score to each rule and sorting on this score. It
chooses a label for an example from the first rule
whose feature is a feature of the example. For a
DL the prediction distribution is defined by 7rx(j) ∝
maxfEFx Bfj. The basic Yarowsky algorithm is
shown in algorithm 1. Note that at any point some
training examples may be left unlabelled by Y M.
We use Collins and Singer (1999) for our exact
specification of Yarowsky.2 It uses DL rule scores
</bodyText>
<equation confidence="0.99895">
|Afj |+ c
Bfj ∝ (1)
|Af |+ Lc
</equation>
<bodyText confidence="0.945214928571429">
where c is a smoothing constant. When constructing
a DL it keeps only the rules with (pre-normalized)
score over a threshold C. In our implementation we
add the seed rules to each subsequent DL.3
1Large-scale information extraction, e.g. (Hearst, 1992),
Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and
Shepherd, 1997), and Junto (Talukdar, 2010) among others, also
have similarities to our approach. We focus on the formal anal-
ysis of the Yarowsky algorithm by Abney (2004).
2It is similar to that of Yarowsky (1995) but is better spec-
ified and omits word sense disambiguation optimizations. The
general algorithm in Yarowsky (1995) is self-training with any
kind of underlying supervised classifier, but we follow the con-
vention of using Yarowsky to refer to the DL algorithm.
</bodyText>
<footnote confidence="0.631594">
3This is not clearly specified in Collins and Singer (1999),
</footnote>
<subsectionHeader confidence="0.999409">
3.2 Yarowsky-cautious
</subsectionHeader>
<bodyText confidence="0.999691333333333">
Collins and Singer (1999) also introduce a variant
algorithm Yarowsky-cautious. Here the DL training
step keeps only the top n rules (f, j) over the thresh-
old for each label j, ordered by |Af|. Additionally
the threshold ( is checked against |Afj|/|Af |instead
of the smoothed score. n begins at no and is incre-
mented by An at each iteration. We add the seed DL
to the new DL after applying the cautious pruning.
Cautiousness limits not only the size of the DL but
also the number of labelled examples, prioritizing
decisions which are believed to be of high accuracy.
At the final iteration Yarowsky-cautious uses the
current labelling to train a DL without a threshold
or cautiousness, and this DL is used for testing. We
call this the retraining step.4
</bodyText>
<subsectionHeader confidence="0.998305">
3.3 DL-CoTrain
</subsectionHeader>
<bodyText confidence="0.999952333333333">
Collins and Singer (1999) also introduce the co-
training algorithm DL-CoTrain. This algorithm al-
ternates between two DLs using disjoint views of
the features in the data. At each step it trains a DL
and then produces a new labelling for the other DL.
Each DL uses thresholding and cautiousness as we
describe for Yarowsky-cautious. At the end the DLs
are combined, the result is used to label the data, and
a retraining step is done from this single labelling.
</bodyText>
<subsectionHeader confidence="0.975278">
3.4 Y-1/DL-1-VS
</subsectionHeader>
<bodyText confidence="0.94700975">
One of the variant algorithms of Abney (2004) is
Y-1/DL-1-VS (referred to by Haffari and Sarkar
(2007) as simply DL-1). Besides various changes
in the specifics of how the labelling is produced,
this algorithm has two differences versus Yarowsky.
Firstly, the smoothing constant c in (1) is replaced
by 1/|Vf|. Secondly, 7r is redefined as 7rx(j) _
B , which we refer to as the sum def-
</bodyText>
<equation confidence="0.99184">
|Fx�fEFf7
I x
</equation>
<bodyText confidence="0.99741725">
inition of 7r. This definition does not match a literal
DL but is easier to analyze.
We are not concerned here with the details of
Y-1/DL-1-VS, but we note that Haffari and Sarkar
</bodyText>
<footnote confidence="0.8368084">
but is used for DL-CoTrain in the same paper.
4The details of Yarowsky-cautious are not clearly specified
in Collins and Singer (1999). Based on similar parts of DL-
CoTrain we assume the that the top n selection is per label
rather in total, that the thresholding value is unsmoothed, and
</footnote>
<equation confidence="0.414630666666667">
that there is a retraining step. We also assume their notation
Count&apos;(x) to be equivalent to |Af |.
621
</equation>
<bodyText confidence="0.999810833333333">
(2007) provide an objective function for this al-
gorithm using a generalized definition of cross-
entropy in terms of Bregman distance, which mo-
tivates our objective in section 4. The Breg-
man distance between two discrete probability dis-
tributions p and q is defined as BV,(p, q) =
Ei [0(pi) − 0(qi) − 0&apos;(qi)(pi − qi)]. As a specific
case we have Bt2(p, q) = Ei(pi − qi)2 = ||p − q||2.
Then Bregman distance-based entropy is Ht2(p) =
− Ei p2i, KL-Divergence is Bt2, and cross-entropy
follows the standard definition in terms of Ht2 and
Bt2. The objective minimized by Y-1/DL-1-VS is:
</bodyText>
<equation confidence="0.9802625">
� �
�
Bt2(�x||�f) − o2�
x
y
(2)
</equation>
<subsectionHeader confidence="0.924082">
3.5 Yarowsky-sum
</subsectionHeader>
<bodyText confidence="0.998850375">
As a baseline for the sum definition of 7r, we intro-
duce the Yarowsky-sum algorithm. It is the same
as Yarowsky except that we use the sum definition
when labelling: for example x we choose the label j
with the highest (sum) 7rx(j), but set Yx = 1 if the
sum is zero. Note that this is a linear model similar
to a conditional random field (CRF) (Lafferty et al.,
2001) for unstructured multiclass problems.
</bodyText>
<subsectionHeader confidence="0.995002">
3.6 Bipartite graph algorithms
</subsectionHeader>
<bodyText confidence="0.989842263157895">
Haffari and Sarkar (2007) suggest a bipartite
graph framework for semi-supervised learning
based on their analysis of Y-1/DL-1-VS and objec-
tive (2). The graph has vertices X U F and edges
{(x, f) : x E X, f E Fx}, as in the graph shown
in figure 1(a). Each vertex represents a distribution
over labels, and in this view Yarowsky can be seen as
alternately updating the example distributions based
on the feature distributions and visa versa.
Based on this they give algorithm 2, which
we call HS-bipartite. It is parametrized by two
functions which are called features-to-example and
examples-to-feature here. Each can be one of
two choices: average(S) is the normalized aver-
age of the distributions of S, while majority(S)
is a uniform distribution if all labels are supported
by equal numbers of distributions of S, and other-
wise a point distribution with mass on the best sup-
ported label. The average-majority form is similar
</bodyText>
<listItem confidence="0.932799083333333">
Algorithm 2: HS-bipartite.
1: apply B(°) to X produce a labelling Y (°)
2: for iteration t to maximum or convergence do
3: for f E F do
4: let p = examples-to-feature({Ox : x E Xf})
5: if p =� U then let Of = p
6: end for
7: for x E X do
8: let p = features-to-example({Bf : f E Fx})
9: if p =� U then let Ox = p
10: end for
11: end for
</listItem>
<bodyText confidence="0.998422166666666">
to Y-1/DL-1-VS, and the majority-majority form
minimizes a different objective similar to (2).
In our implementation we label training data (for
the convergence check) with the 0 distributions from
the graph. We label test data by constructing new
Ox = examples-to-feature(Fx) for the unseen x.
</bodyText>
<subsectionHeader confidence="0.673799">
3.7 Semi-supervised learning algorithm of Sub-
</subsectionHeader>
<bodyText confidence="0.99281924">
ramanya et al. (2010)
Subramanya et al. (2010) give a semi-supervised al-
gorithm for part of speech tagging. Unlike the algo-
rithms described above, it is for domain adaptation
with large amounts of labelled data rather than boot-
strapping with a small number of seeds.
This algorithm is structurally similar to Yarowsky
in that it begins from an initial partial labelling and
repeatedly trains a classifier on the labelling and
then relabels the data. It uses a CRF (Lafferty et al.,
2001) as the underlying supervised learner. It dif-
fers significantly from Yarowsky in two other ways:
First, instead of only training a CRF it also uses a
step of graph propagation between distributions over
the n-grams in the data. Second, it does the propa-
gation on distributions over n-gram types rather than
over n-gram tokens (instances in the data).
They argue that using propagation over types al-
lows the algorithm to enforce constraints and find
similarities that self-training cannot. We are not con-
cerned here with the details of this algorithm, but
it motivates our work firstly in providing the graph
propagation which we will describe in more detail in
section 4, and secondly in providing an algorithmic
structure that we use for our algorithm in section 5.
</bodyText>
<subsectionHeader confidence="0.996321">
3.8 Collins and Singer (1999)’s EM
</subsectionHeader>
<bodyText confidence="0.9973325">
We implemented the EM algorithm of Collins and
Singer (1999) as a baseline for the other algorithms.
</bodyText>
<equation confidence="0.966116444444445">
E Ht2(�x||�f) = �
xEX xEX
fEFx fEFx
622
Method V N (u) qu
0-B X ∪ F N. = F., Nf = Xf q. = 0., qf = Bf
7r-B X ∪ F N. = F., Nf = Xf q. = 7r., qf = Bf
B-only F Nf = U.EXf F. \ f qf = Bf
BT-only F Nf = U.EXf F. \ f qf = BTf
</equation>
<tableCaption confidence="0.919368">
Table 2: Graph structures for propagation.
</tableCaption>
<bodyText confidence="0.9987248">
They do not specify tuning details, but to get com-
parable accuracy we found it was necessary to do
smoothing and to include weights A1 and A2 on the
expected counts of seed-labelled and initially unla-
belled examples respectively (Nigam et al., 2000).
</bodyText>
<sectionHeader confidence="0.947979" genericHeader="method">
4 Graph propagation
</sectionHeader>
<bodyText confidence="0.9999852">
The graph propagation of Subramanya et al. (2010)
is a method for smoothing distributions attached to
vertices of a graph. Here we present it with an alter-
nate notation using Bregman distances as described
in section 3.4.5 The objective is
</bodyText>
<equation confidence="0.996874">
µ E EwuvBt2(qu, qv) + � Bt2(qu, U) (3)
uEV uEV
vEN(i)
</equation>
<bodyText confidence="0.999590214285714">
where V is a set of vertices, N(v) is the neighbour-
hood of vertex v, and qv is an initial distribution for
each vertex v to be smoothed. They give an iterative
update to minimize (3). Note that (3) is independent
of their specific graph structure, distributions, and
semi-supervised learning algorithm.
We propose four methods for using this propaga-
tion with Yarowsky. These methods all use con-
stant edge weights (wuv = 1). The distributions
and graph structures are shown in table 2. Figure 1
shows example graphs for 0-B and B-only. 7r-B and
BT-only are similar, and are described below.
The graph structure of 0-B is the bipartite graph
of Haffari and Sarkar (2007). In fact, 0-B the propa-
gation objective (3) and Haffari and Sarkar (2007)’s
Y-1/DL-1-VS objective (2) are identical up to con-
stant coefficients and an extra constant term.6 0-B
5We omit the option to hold some of the distributions at fixed
values, which would add an extra term to the objective.
6The differences are specifically: First, (3) adds the con-
stant coefficients µ and v. Second, (3) sums over each edge
twice (once in each direction), while (2) sums over each only
once. Since wuv = wvu and Bt2(qu, qv) = Bt2(qv, qu), this
can be folded into the constant µ. Third, after expanding (2)
there is a term |Fx |inside the sum for Ht2(0x) which is not
present in (3). This does not effect the direction of minimiza-
tion. Fourth, Bt2(qu, U) in (3) expands to Ht2(qu) plus a con-
stant, adding an extra constant term to the total.
</bodyText>
<figureCaption confidence="0.997648">
Figure 1: Example graphs for 0-0 and 0-only propagation.
</figureCaption>
<bodyText confidence="0.999821842105263">
therefore gives us a direct way to optimize (2).
The other three methods do not correspond to the
objective of Haffari and Sarkar (2007). The 7r-B
method is like 0-B except for using 7r as the distribu-
tion for example vertices.
The bipartite graph of the first two methods dif-
fers from the structure used by Subramanya et al.
(2010) in that it does propagation between two dif-
ferent kinds of distributions instead of only one kind.
We also adopt a more comparable approach with a
graph over only features. Here we define adjacency
by co-occurrence in the same example. The B-only
method uses this graph and B as the distribution.
Finally, we noted in section 3.7 that the algo-
rithm of Subramanya et al. (2010) does one addi-
tional step in converting from token level distribu-
tions to type level distributions. The BT-only method
therefore uses the feature-only graph but for the dis-
tribution uses a type level version of B defined by
</bodyText>
<equation confidence="0.628356">
BTf�  |Xf  |&amp;xEXf 7rx(7).
</equation>
<sectionHeader confidence="0.920327" genericHeader="method">
5 Novel Yarowsky-prop algorithm
</sectionHeader>
<bodyText confidence="0.999879923076923">
We call our graph propagation based algorithm
Yarowsky-prop. It is shown with BT-only propaga-
tion in algorithm 3. It is based on the Yarowsky al-
gorithm, with the following changes: an added step
to calculate BT (line 4), an added step to calculate BP
(line 5), the use of BP rather than the DL to update
the labelling (line 6), and the use of the sum defini-
tion of 7r. Line 7 does DL training as we describe in
sections 3.1 and 3.2. Propagation is done with the
iterative update of Subramanya et al. (2010).
This algorithm is adapted to the other propagation
methods described in section 4 by changing the type
of propagation on line 5. In B-only, propagation is
</bodyText>
<figure confidence="0.987733904761905">
(a) 0-0 method
(b) 0-only method
Bf3
Bf1 0.1
Bf2
0.2
Bf1
Bf3
0.3
Bf2
Bf4
Bf4
0.4
...
...
...
Bf|f. |
Bf|„ |
0.|�|
623
Algorithm 3: Yarowsky-prop.
</figure>
<listItem confidence="0.9499471">
1: let Bfj be the scores of the seed rules // crf train
2: for iteration t to maximum or convergence do
3: let 7rx (j) =  |Fx  |E f E Fx B fj // post. decode
x�Xf πx�j�
4: let BT fj = // token to type
|Xf|
5: propagate BT to get BP // graph propagate
6: label the data with BP // viterbi decode
7: train a new DL Bfj // crf train
8: end for
</listItem>
<bodyText confidence="0.999358142857143">
done on B, using the graph of figure 1(b). In 0-B and
7r-B propagation is done on the respective bipartite
graph (figure 1(a) or the equivalent with 7r). Line
4 is skipped for these methods, and 0 is as defined
in section 2. For the bipartite graph methods 0-B
and 7r-B only the propagated B values on the feature
nodes are used for BP (the distributions on the exam-
ple nodes are ignored after the propagation itself).
The algorithm uses Bfj values rather than an ex-
plicit DL for labelling. The (pre-normalized) score
for any (f, j) not in the DL is taken to be zero. Be-
sides using the sum definition of 7r when calculating
BT, we also use a sum in labelling. When labelling
an example x (at line 6 and also on testing data) we
</bodyText>
<equation confidence="0.673730333333333">
�
use arg maxj fj, but set Yx = � if
f�Fx: �P f ��U BP
</equation>
<bodyText confidence="0.999152277777778">
the sum is zero. Ignoring uniform BPf values is in-
tended to provide an equivalent to the DL behaviour
of using evidence only from rules that are in the list.
We include the cautiousness of Yarowsky-
cautious (section 3.2) in the DL training on line 7. At
the labelling step on line 6 we label only examples
which the pre-propagated B would also assign a label
(using the same rules described above for BP). This
choice is intended to provide an equivalent to the
Yarowsky-cautious behaviour of limiting the num-
ber of labelled examples; most BPf are non-uniform,
so without it most examples become labelled early.
We observe further similarity between the
Yarowsky algorithm and the general approach of
Subramanya et al. (2010) by comparing algorithm
3 here with their algorithm 1. The comments in al-
gorithm 3 give the corresponding parts of their algo-
rithm. Note that each line has a similar purpose.
</bodyText>
<sectionHeader confidence="0.999026" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999798">
6.1 Tasks and data
</subsectionHeader>
<bodyText confidence="0.9827925">
For evaluation we use the tasks of Collins and Singer
(1999) and Eisner and Karakos (2005), with data
</bodyText>
<table confidence="0.992707142857143">
Rank Score Feature Label
1 0.999900 New-York loc.
2 0.999900 California loc.
3 0.999900 U.S. loc.
4 0.999900 Microsoft org.
5 0.999900 I.B.M. org.
6 0.999900 Incorporated org.
7 0.999900 Mr. per.
8 0.999976 U.S. loc.
9 0.999957 New-York-Stock-Exchange loc.
10 0.999952 California loc.
11 0.999947 New-York loc.
12 0.999946 court-in loc.
13 0.975154 Company-of loc.
</table>
<figure confidence="0.661883">
...
</figure>
<figureCaption confidence="0.8578012">
Figure 2: A DL from iteration 5 of Yarowsky on the named en-
tity task. Scores are pre-normalized values from the expression
on the left side of (1), not Ofj values. Context features are indi-
cated by italics; all others are spelling features. Specific feature
types are omitted. Seed rules are indicated by bold ranks.
</figureCaption>
<bodyText confidence="0.999881645161291">
kindly provided by the respective authors.
The task of Collins and Singer (1999) is named
entity classification on data from New York Times
text.7 The data set was pre-processed by a statisti-
cal parser (Collins, 1997) and all noun phrases that
are potential named entities were extracted from the
parse tree. Each noun phrase is to be labelled as
a person, organization, or location. The parse tree
provides the surrounding context as context features
such as the words in prepositional phrase and rela-
tive clause modifiers, etc., and the actual words in
the noun phrase provide the spelling features. The
test data additionally contains some noise examples
which are not in the three named entity categories.
We use the seed rules the authors provide, which are
the first seven items in figure 2. For DL-CoTrain,
we use their two views: one view is the spelling fea-
tures, and the other is the context features. Figure 2
shows a DL from Yarowsky training on this task.
The tasks of Eisner and Karakos (2005) are word
sense disambiguation on several English words
which have two senses corresponding to two dif-
ferent words in French. Data was extracted from
the Canadian Hansards, using the English side to
produce training and test data and the French side
to produce the gold labelling. Features are the
original and lemmatized words immediately adja-
7We removed weekday and month examples from the test set
as they describe. They note 88962 examples in their training set,
but the file has 89305. We did not find any filtering criteria that
produced the expected size, and therefore used all examples.
</bodyText>
<page confidence="0.878552">
624
</page>
<bodyText confidence="0.9999764">
cent to the word to be disambiguated, and origi-
nal and lemmatized context words in the same sen-
tence. Their seeds are pairs of adjacent word fea-
tures, with one feature for each label (sense). We
use the ‘drug’, ‘land’, and ‘sentence’ tasks, and
the seed rules from their best seed selection: ‘alco-
hol’/‘medical’, ‘acres’/‘court’, and ‘reads’/‘served’
respectively (they do not provide seeds for their
other three tasks). For DL-CoTrain we use adjacent
words for one view and context words for the other.
</bodyText>
<subsectionHeader confidence="0.996764">
6.2 Experimental set up
</subsectionHeader>
<bodyText confidence="0.999557972222222">
Where applicable we use smoothing c = 0.1, a
threshold ( = 0.95, and cautiousness parameters
n0 = An = 5 as in Collins and Singer (1999)
and propagation parameters µ = 0.6, v = 0.01 as
in Subramanya et al. (2010). Initial experiments
with different propagation parameters suggested that
as long as v was set at this value changing µ had
relatively little effect on the accuracy. We did not
find any propagation parameter settings that outper-
formed this choice. For the Yarowsky-prop algo-
rithms we perform a single iteration of the propa-
gation update for each iteration of the algorithm.
For EM we use weights A1 = 0.98, and A2 = 0.02
(see section 3.8), which were found in initial experi-
ments to be the best values, and results are averaged
over 10 random initializations.
The named entity test set contains some examples
that are neither person, organization, nor location.
Collins and Singer (1999) define noise accuracy as
accuracy that includes such instances, and clean ac-
curacy as accuracy calculated across only the exam-
ples which are one of the known labels. We report
only clean accuracy in this paper; noise accuracy
tracks clean accuracy but is a little lower. There is
no difference on the word sense data sets. We also
report (clean) non-seeded accuracy, which we define
to be clean accuracy over only examples which are
not assigned a label by the seed rules. This is in-
tended to evaluate what the algorithm has learned,
rather than what it can achieve by using the input
information directly (Daume, 2011).
We test Yarowsky, Yarowsky-cautious,
Yarowsky-sum, DL-CoTrain, HS-bipartite in
all four forms, and Yarowsky-prop cautious and
non-cautious and in all four forms. For each algo-
rithm except EM we perform a final retraining step
</bodyText>
<table confidence="0.3423905">
Gold Spelling features Context features
loc. Waukegan maker, LEFT
loc. Mexico, president, of president-of, RIGHT
loc. La-Jolla, La Jolla company, LEFT
</table>
<figureCaption confidence="0.994872333333333">
Figure 3: Named entity test set examples where Yarowsky-prop
0-only is correct and no other tested algorithms are correct. The
specific feature types are omitted.
</figureCaption>
<bodyText confidence="0.999784">
as described for Yarowsky-cautious (section 3.2).
Our programs and experiment scripts have been
made available.8
</bodyText>
<subsectionHeader confidence="0.998854">
6.3 Accuracy
</subsectionHeader>
<bodyText confidence="0.999941857142857">
Table 3 shows the final test set accuracies for the
all the algorithms. The seed DL accuracy is also
included for reference.
The best performing form of our novel algo-
rithm is Yarowsky-prop-cautious 0-only. It numer-
ically outperforms DL-CoTrain on the named entity
task, is not (statistically) significantly worse on the
drug and land tasks, and is significantly better on
the sentence task. It also numerically outperforms
Yarowsky-cautious on the named entity task and is
significantly better on the drug task. Is significantly
worse on the land task, where most algorithms con-
verge at labelling all examples with the first sense. It
is significantly worse on the sentence task, although
it is the second best performing algorithm and sev-
eral percent above DL-CoTrain on that task.
Figure 3 shows (all) three examples from the
named entity test set where Yarowsky-prop-cautious
0-only is correct but none of the other Yarowsky
variants are. Note that it succeeds despite mis-
leading features; “maker” and “company” might be
taken to indicate a company and “president-of” an
organization, but all three examples are locations.
Yarowsky-prop-cautious 0-0 and 7r-0 also per-
form respectably, although not as well. Yarowsky-
prop-cautious 0T-only and the non-cautious versions
are significantly worse. Although 0T-only was in-
tended to incorporate Subramanya et al. (2010)’s
idea of type level distributions, it in fact performs
worse than 0-only. We believe that Collins and
Singer (1999)’s definition (1) of 0 incorporates suf-
ficient type level information that the creation of a
separate distribution is unnecessary in this case.
Figure 4 shows the test set non-seeded accuracies
as a function of the iteration for many of the algo-
</bodyText>
<footnote confidence="0.882032">
8The software is included with the paper submission and
will be maintained at https://github.com/sfu-natlang/yarowsky.
</footnote>
<page confidence="0.551405">
625
</page>
<table confidence="0.9999145">
Algorithm named entity drug Task sentence
land
EM 81.05 78.64 55.96 54.85 32.86 31.07 67.88 65.42
±0.31 ±0.34 ±0.41 ±0.43 ±0.00 ±0.00 ±3.35 ±3.57
Seed DL 11.29 0.00 5.18 0.00 2.89 0.00 7.18 0.00
DL-CoTrain (cautious) 91.56 90.49 59.59 58.17 78.36 77.72 68.16 65.69
Yarowsky 81.19 78.79 55.70 54.02 79.03 78.41 62.91 60.04
Yarowsky-cautious 91.11 89.97 54.40 52.63 79.10 78.48 78.64 76.99
Yarowsky-cautious sum 91.56 90.49 54.40 52.63 78.36 77.72 78.64 76.99
HS-bipartite avg-avg 45.84 45.89 52.33 50.42 78.36 77.72 54.56 51.05
HS-bipartite avg-maj 81.98 79.69 52.07 50.14 78.36 77.72 55.15 51.67
HS-bipartite maj-avg 73.55 70.18 52.07 50.14 78.36 77.72 55.15 51.67
HS-bipartite maj-maj 73.66 70.31 52.07 50.14 78.36 77.72 55.15 51.67
Yarowsky-prop 0-0 80.39 77.89 53.63 51.80 78.36 77.72 55.34 51.88
Yarowsky-prop 7r-0 78.34 75.58 54.15 52.35 78.36 77.72 54.56 51.05
Yarowsky-prop 0-only 78.56 75.84 54.66 52.91 78.36 77.72 54.56 51.05
Yarowsky-prop 0T-only 77.88 75.06 52.07 50.14 78.36 77.72 54.56 51.05
Yarowsky-prop-cautious 0-0 90.19 88.95 56.99 55.40 78.36 77.72 74.17 72.18
Yarowsky-prop-cautious 7r-0 89.40 88.05 58.55 57.06 78.36 77.72 70.10 67.78
Yarowsky-prop-cautious 0-only 92.47 91.52 58.55 57.06 78.36 77.72 75.15 73.22
Yarowsky-prop-cautious 0T-only 78.45 75.71 58.29 56.79 78.36 77.72 54.56 51.05
Num. train/test examples 89305 / 962 134 / 386 1604 / 1488 303 / 515
</table>
<tableCaption confidence="0.940818333333333">
Table 3: Test set percent accuracy and non-seeded test set percent accuracy (respectively) for the algorithms on all tasks. Bold
items are a maximum in their column. Italic items have a statistically significant difference versus DL-CoTrain (p &lt; 0.05 with a
McNemar test). For EM, ± indicates one standard deviation but statistical significance was not measured.
</tableCaption>
<bodyText confidence="0.999917363636364">
rithms on the named entity task. The Yarowsky-prop
non-cautious algorithms quickly converge to the fi-
nal accuracy and are not shown. While the other
algorithms (figure 4(a)) make a large accuracy im-
provement in the final retraining step, the Yarowsky-
prop (figure 4(b)) algorithms reach comparable ac-
curacies earlier and gain much less from retraining.
We did not implement Collins and Singer (1999)’s
CoBoost; however, in their results it performs com-
parably to DL-CoTrain and Yarowsky-cautious. As
with DL-CoTrain, CoBoost requires two views.
</bodyText>
<subsectionHeader confidence="0.995482">
6.4 Cautiousness
</subsectionHeader>
<bodyText confidence="0.999803923076923">
Cautiousness appears to be important in the perfor-
mance of the algorithms we tested. In table 3, only
the cautious algorithms are able to reach the 90%
accuracy range.
To evaluate the effects of cautiousness we ex-
amine the Yarowsky-prop O-only algorithm on the
named entity task in more detail. This algorithm has
two classifiers which are trained in conjunction: the
DL and the propagated OP. Figure 5 shows the train-
ing set coverage (of the labelling on line 6 of algo-
rithm 3) and the test set accuracy of both classifiers,
for the cautious and non-cautious versions.
The non-cautious version immediately learns a
DL over all feature-label pairs, and therefore has full
coverage after the first iteration. The DL and OP con-
verge to similar accuracies within a few more itera-
tions, and the retraining step increases accuracy by
less than one percent. On the other hand, the cau-
tious version gradually increases the coverage over
the iterations. The DL accuracy follows the cover-
age closely (similar to the behaviour of Yarowsky-
cautious, not shown here), while the propagated
classifier accuracy jumps quickly to near 90% and
then increases only gradually.
Although the DL prior to retraining achieves a
roughly similar accuracy in both versions, only the
cautious version is able to reach the 90% accuracy
range in the propagated classifier and retraining.
Presumably the non-cautious version makes an early
mistake, reaching a local minimum which it cannot
escape. The cautious version avoids this by making
only safe rule selection and labelling choices.
Figure 5(b) also helps to clarify the difference in
retraining that we noted in section 6.3. Like the
non-propagated DL algorithms, the DL component
of Yarowsky-prop has much lower accuracy than the
propagated classifier prior to the retraining step. But
after retraining, the DL and OP reach very similar ac-
curacies.
</bodyText>
<figure confidence="0.986772714285714">
626
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
1
0.9
0.8
0.7
0.6
0.5
0.4
0
(a) Collins &amp; Singer algorithms (plus sum form)
(b) Yarowsky propagation cautious
</figure>
<figureCaption confidence="0.98358225">
Figure 4: Non-seeded test accuracy versus iteration for various
algorithms on named entity. The results for the Yarowsky-prop
algorithms are for the propagated classifier BP, except for the
final DL retraining iteration.
</figureCaption>
<subsectionHeader confidence="0.937708">
6.5 Objective function
</subsectionHeader>
<bodyText confidence="0.999867444444445">
The propagation method 0-0 was motivated by opti-
mizing the equivalent objectives (2) and (3) at each
iteration. Figure 6 shows the graph propagation ob-
jective (3) along with accuracy for Yarowsky-prop
0-0 without cautiousness. The objective value de-
creases as expected, and converges along with accu-
racy. Conversely, the cautious version (not shown
here) does not clearly minimize the objective, since
cautiousness limits the effect of the propagation.
</bodyText>
<sectionHeader confidence="0.997717" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.970800666666667">
N
Our novel algorithm achieves accuracy compara-
ble to Yarowsky-cautious, but is better theoretically
motivated by combining ideas from Haffari and
Sarkar (2007) and Subramanya et al. (2010). It also
achieves accuracy comparable to DL-CoTrain, but
does not require the features to be split into two in-
dependent views.
As future work, we would like to apply our al-
</bodyText>
<figure confidence="0.9999425">
(a) Non-cautious
(b) Cautious
</figure>
<figureCaption confidence="0.9011555">
Figure 5: Internal train set coverage and non-seeded test accu-
racy (same scale) for Yarowsky-prop 0-only on named entity.
</figureCaption>
<figure confidence="0.990545">
Iteration
main
coverage
</figure>
<figureCaption confidence="0.6735634">
Figure 6: Non-seeded test accuracy (left axis), coverage (left
axis, same scale), and objective value (right axis) for Yarowsky-
prop 0-0. Iterations are shown on a log scale. We omit the first
iteration (where the DL contains only the seed rules) and start
the plot at iteration 2 where there is a complete DL.
</figureCaption>
<bodyText confidence="0.999503666666667">
gorithm to a structured task such as part of speech
tagging. We also believe that our method for adapt-
ing Collins and Singer (1999)’s cautiousness to
Yarowsky-prop can be applied to similar algorithms
with other underlying classifiers, even to structured
output models such as conditional random fields.
</bodyText>
<figure confidence="0.998707333333333">
0 100 200 300 400 500
Iteration
0.95
0.85
0.75
0.65
0.55
0.9
0.8
0.7
0.6
0.5
Yarowsky-prop-cautious phi-theta
Yarowsky-prop-cautious pi-theta
Yarowsky-prop-cautious theta-only
Yarowsky-prop-cautious thetatype-only
0.9
0.8
0.7
0.6
0.5
0.4
1
0 100 200 300 400 500
Iteration
N
main
dl
coverage
10 100 1000
objective
1
0.9
0.8
0.7
0.6
0.5
0.4
85000
80000
75000
70000
65000
60000
55000
</figure>
<sectionHeader confidence="0.88526" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993385106383">
S. Abney. 2004. Understanding the Yarowsky algorithm.
Computational Linguistics, 30(3).
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries, DL ’00.
A. Blum and S. Chawla. 2001. Learning from labeled
and unlabeled data using graph mincuts. In Proc.
19th International Conference on Machine Learning
(ICML-2001).
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of Computational Learning Theory.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In In EMNLP
1999: Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 100–110.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16–23, Madrid, Spain, July.
Association for Computational Linguistics.
Hal Daume. 2011. Seeding, transduction, out-of-
sample error and the Microsoft approach... Blog
post at http://nlpers.blogspot.com/2011/04/seeding-
transduction-out-of-sample.html, April 6.
Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 395–402, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2007. Analysis
of semi-supervised learning with the Yarowsky algo-
rithm. In UAI 2007, Proceedings of the Twenty-Third
Conference on Uncertainty in Artificial Intelligence,
Vancouver, BC, Canada, pages 159–166.
Aria Haghighi and Dan Klein. 2006a. Prototype-driven
grammar induction. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 881–888, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006b. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320–327, New York
City, USA, June. Association for Computational Lin-
guistics.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics - Vol-
ume 2, COLING ’92, pages 539–545, Stroudsburg,
PA, USA. Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ’01, pages
282–289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using EM. Machine Learning, 30(3).
Ellen Riloff and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 117–
124.
H. J. Scudder. 1965. Probability of error of some adap-
tive pattern-recognition machines. IEEE Transactions
on Information Theory, 11:363–371.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 167–176, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Partha Pratim Talukdar. 2010. Graph-based weakly-
supervised methods for information extraction &amp; in-
tegration. Ph.D. thesis, University of Pennsylvania.
Software: https://github.com/parthatalukdar/junto.
X. Zhu and Z. Ghahramani and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceedings ofInternational Con-
ference on Machine Learning.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 189–196,
Cambridge, Massachusetts, USA, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.925829">
628
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.293521">
<title confidence="0.964877">Bootstrapping via Graph Propagation</title>
<author confidence="0.806812">Whitney</author>
<affiliation confidence="0.35443">Simon Fraser University, School of Computing</affiliation>
<address confidence="0.399276">Burnaby, BC V5A 1S6,</address>
<abstract confidence="0.998037076923077">Bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them. This paper introduces a novel variant of the Yarowsky algorithm based on this view. It is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function. The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Understanding the Yarowsky algorithm.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="1559" citStr="Abney (2004)" startWordPosition="242" endWordPosition="243">o derive the labelled points. We refer to this setting as bootstrapping. In contrast, typical semi-supervised learning deals with a large number of labelled points, and a domain adaptation task with unlabelled points from the new domain. The two dominant discriminative learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically *This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. We would like to thank Gholamreza Haffari and the anonymous reviewers for their comments. We particularly thank Michael Collins, Jason Eisner, and Damianos Karakos for the data we used in our experiments. x denotes</context>
<context position="3064" citStr="Abney (2004)" startWordPosition="496" endWordPosition="497">ently unlabelled examples Xf set of examples with feature f Af set of currently labelled examples with f Vf set of currently unlabelled examples with f Aj set of examples currently labelled with j Afj set of examples with f currently labelled with j Table 1: Notation of Abney (2004). well-understood as minimizing an objective function at each iteration, and it obtains state of the art performance on several different NLP data sets. To our knowledge, this is the first theoretically motivated self-training bootstrapping algorithm which performs as well as the Yarowsky algorithm. 2 Bootstrapping Abney (2004) defines useful notation for semisupervised learning, shown in table 1. Note that A, V , etc. are relative to the current labelling Y . We additionally define F to be the set of all features, and use U to denote the uniform distribution. In the bootstrapping setting the learner is given an initial partial labelling Y (°) where only a few examples are labelled (i.e. Y (°) x = L for most x). Abney (2004) defines three probability distributions in his analysis of bootstrapping: Bfj is the parameter for feature f with label j, taken to be normalized so that Bf is a distribution over labels. ox is </context>
<context position="4943" citStr="Abney (2004)" startWordPosition="819" endWordPosition="820">onvergence do 3: train a new DL B on Y (t) 4: apply B to X, to produce Y (t+1) 5: end for seed rules but uses them to inject features into a joint model p(x, j) which they train using expectationmaximization for Markov random fields. We focus on discriminative training which does not require complex partition functions for normalization. Blum and Chawla (2001) introduce an early use of transductive learning using graph propagation. X. Zhu and Z. Ghahramani and J. Lafferty (2003)’s method of graph propagation is predominantly transductive, and the non-transductive version is closely related to Abney (2004) c.f. Haffari and Sarkar (2007).1 3 Existing algorithms 3.1 Yarowsky A decision list (DL) is a (ordered) list of featurelabel pairs (rules) which is produced by assigning a score to each rule and sorting on this score. It chooses a label for an example from the first rule whose feature is a feature of the example. For a DL the prediction distribution is defined by 7rx(j) ∝ maxfEFx Bfj. The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y M. We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It us</context>
<context position="7757" citStr="Abney (2004)" startWordPosition="1299" endWordPosition="1300">usness, and this DL is used for testing. We call this the retraining step.4 3.3 DL-CoTrain Collins and Singer (1999) also introduce the cotraining algorithm DL-CoTrain. This algorithm alternates between two DLs using disjoint views of the features in the data. At each step it trains a DL and then produces a new labelling for the other DL. Each DL uses thresholding and cautiousness as we describe for Yarowsky-cautious. At the end the DLs are combined, the result is used to label the data, and a retraining step is done from this single labelling. 3.4 Y-1/DL-1-VS One of the variant algorithms of Abney (2004) is Y-1/DL-1-VS (referred to by Haffari and Sarkar (2007) as simply DL-1). Besides various changes in the specifics of how the labelling is produced, this algorithm has two differences versus Yarowsky. Firstly, the smoothing constant c in (1) is replaced by 1/|Vf|. Secondly, 7r is redefined as 7rx(j) _ B , which we refer to as the sum def|Fx�fEFf7 I x inition of 7r. This definition does not match a literal DL but is easier to analyze. We are not concerned here with the details of Y-1/DL-1-VS, but we note that Haffari and Sarkar but is used for DL-CoTrain in the same paper. 4The details of Yaro</context>
</contexts>
<marker>Abney, 2004</marker>
<rawString>S. Abney. 2004. Understanding the Yarowsky algorithm. Computational Linguistics, 30(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth ACM International Conference on Digital Libraries, DL ’00.</booktitle>
<contexts>
<context position="5883" citStr="Agichtein and Gravano, 2000" startWordPosition="982" endWordPosition="985">le. For a DL the prediction distribution is defined by 7rx(j) ∝ maxfEFx Bfj. The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y M. We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores |Afj |+ c Bfj ∝ (1) |Af |+ Lc where c is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold C. In our implementation we add the seed rules to each subsequent DL.3 1Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2It is similar to that of Yarowsky (1995) but is better specified and omits word sense disambiguation optimizations. The general algorithm in Yarowsky (1995) is self-training with any kind of underlying supervised classifier, but we follow the convention of using Yarowsky to refer to the DL algorithm. 3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious Collins and Singer</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the Fifth ACM International Conference on Digital Libraries, DL ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>S Chawla</author>
</authors>
<title>Learning from labeled and unlabeled data using graph mincuts.</title>
<date>2001</date>
<booktitle>In Proc. 19th International Conference on Machine Learning (ICML-2001).</booktitle>
<contexts>
<context position="4693" citStr="Blum and Chawla (2001)" startWordPosition="780" endWordPosition="783">28, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Algorithm 1: The basic Yarowsky algorithm. Require: training data X and a seed DL B(°) 1: apply B(°) to X produce a labelling Y (°) 2: for iteration t to maximum or convergence do 3: train a new DL B on Y (t) 4: apply B to X, to produce Y (t+1) 5: end for seed rules but uses them to inject features into a joint model p(x, j) which they train using expectationmaximization for Markov random fields. We focus on discriminative training which does not require complex partition functions for normalization. Blum and Chawla (2001) introduce an early use of transductive learning using graph propagation. X. Zhu and Z. Ghahramani and J. Lafferty (2003)’s method of graph propagation is predominantly transductive, and the non-transductive version is closely related to Abney (2004) c.f. Haffari and Sarkar (2007).1 3 Existing algorithms 3.1 Yarowsky A decision list (DL) is a (ordered) list of featurelabel pairs (rules) which is produced by assigning a score to each rule and sorting on this score. It chooses a label for an example from the first rule whose feature is a feature of the example. For a DL the prediction distributi</context>
</contexts>
<marker>Blum, Chawla, 2001</marker>
<rawString>A. Blum and S. Chawla. 2001. Learning from labeled and unlabeled data using graph mincuts. In Proc. 19th International Conference on Machine Learning (ICML-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of Computational Learning Theory.</booktitle>
<contexts>
<context position="1327" citStr="Blum and Mitchell, 1998" startWordPosition="205" endWordPosition="208"> 1 Introduction In this paper, we are concerned with a case of semisupervised learning that is close to unsupervised learning, in that the labelled and unlabelled data points are from the same domain and only a small set of seed rules is used to derive the labelled points. We refer to this setting as bootstrapping. In contrast, typical semi-supervised learning deals with a large number of labelled points, and a domain adaptation task with unlabelled points from the new domain. The two dominant discriminative learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically *This research was partially supported by an NSERC, Canada (RGPI</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification. In</title>
<date>1999</date>
<booktitle>In EMNLP 1999: Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="5495" citStr="Collins and Singer (1999)" startWordPosition="917" endWordPosition="920">tive, and the non-transductive version is closely related to Abney (2004) c.f. Haffari and Sarkar (2007).1 3 Existing algorithms 3.1 Yarowsky A decision list (DL) is a (ordered) list of featurelabel pairs (rules) which is produced by assigning a score to each rule and sorting on this score. It chooses a label for an example from the first rule whose feature is a feature of the example. For a DL the prediction distribution is defined by 7rx(j) ∝ maxfEFx Bfj. The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y M. We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores |Afj |+ c Bfj ∝ (1) |Af |+ Lc where c is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold C. In our implementation we add the seed rules to each subsequent DL.3 1Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2It is similar t</context>
<context position="7261" citStr="Collins and Singer (1999)" startWordPosition="1211" endWordPosition="1214">el j, ordered by |Af|. Additionally the threshold ( is checked against |Afj|/|Af |instead of the smoothed score. n begins at no and is incremented by An at each iteration. We add the seed DL to the new DL after applying the cautious pruning. Cautiousness limits not only the size of the DL but also the number of labelled examples, prioritizing decisions which are believed to be of high accuracy. At the final iteration Yarowsky-cautious uses the current labelling to train a DL without a threshold or cautiousness, and this DL is used for testing. We call this the retraining step.4 3.3 DL-CoTrain Collins and Singer (1999) also introduce the cotraining algorithm DL-CoTrain. This algorithm alternates between two DLs using disjoint views of the features in the data. At each step it trains a DL and then produces a new labelling for the other DL. Each DL uses thresholding and cautiousness as we describe for Yarowsky-cautious. At the end the DLs are combined, the result is used to label the data, and a retraining step is done from this single labelling. 3.4 Y-1/DL-1-VS One of the variant algorithms of Abney (2004) is Y-1/DL-1-VS (referred to by Haffari and Sarkar (2007) as simply DL-1). Besides various changes in th</context>
<context position="12640" citStr="Collins and Singer (1999)" startWordPosition="2156" endWordPosition="2159">tion between distributions over the n-grams in the data. Second, it does the propagation on distributions over n-gram types rather than over n-gram tokens (instances in the data). They argue that using propagation over types allows the algorithm to enforce constraints and find similarities that self-training cannot. We are not concerned here with the details of this algorithm, but it motivates our work firstly in providing the graph propagation which we will describe in more detail in section 4, and secondly in providing an algorithmic structure that we use for our algorithm in section 5. 3.8 Collins and Singer (1999)’s EM We implemented the EM algorithm of Collins and Singer (1999) as a baseline for the other algorithms. E Ht2(�x||�f) = � xEX xEX fEFx fEFx 622 Method V N (u) qu 0-B X ∪ F N. = F., Nf = Xf q. = 0., qf = Bf 7r-B X ∪ F N. = F., Nf = Xf q. = 7r., qf = Bf B-only F Nf = U.EXf F. \ f qf = Bf BT-only F Nf = U.EXf F. \ f qf = BTf Table 2: Graph structures for propagation. They do not specify tuning details, but to get comparable accuracy we found it was necessary to do smoothing and to include weights A1 and A2 on the expected counts of seed-labelled and initially unlabelled examples respectively (</context>
<context position="19043" citStr="Collins and Singer (1999)" startWordPosition="3347" endWordPosition="3350">he same rules described above for BP). This choice is intended to provide an equivalent to the Yarowsky-cautious behaviour of limiting the number of labelled examples; most BPf are non-uniform, so without it most examples become labelled early. We observe further similarity between the Yarowsky algorithm and the general approach of Subramanya et al. (2010) by comparing algorithm 3 here with their algorithm 1. The comments in algorithm 3 give the corresponding parts of their algorithm. Note that each line has a similar purpose. 6 Evaluation 6.1 Tasks and data For evaluation we use the tasks of Collins and Singer (1999) and Eisner and Karakos (2005), with data Rank Score Feature Label 1 0.999900 New-York loc. 2 0.999900 California loc. 3 0.999900 U.S. loc. 4 0.999900 Microsoft org. 5 0.999900 I.B.M. org. 6 0.999900 Incorporated org. 7 0.999900 Mr. per. 8 0.999976 U.S. loc. 9 0.999957 New-York-Stock-Exchange loc. 10 0.999952 California loc. 11 0.999947 New-York loc. 12 0.999946 court-in loc. 13 0.975154 Company-of loc. ... Figure 2: A DL from iteration 5 of Yarowsky on the named entity task. Scores are pre-normalized values from the expression on the left side of (1), not Ofj values. Context features are indi</context>
<context position="22042" citStr="Collins and Singer (1999)" startWordPosition="3851" endWordPosition="3854">nd original and lemmatized context words in the same sentence. Their seeds are pairs of adjacent word features, with one feature for each label (sense). We use the ‘drug’, ‘land’, and ‘sentence’ tasks, and the seed rules from their best seed selection: ‘alcohol’/‘medical’, ‘acres’/‘court’, and ‘reads’/‘served’ respectively (they do not provide seeds for their other three tasks). For DL-CoTrain we use adjacent words for one view and context words for the other. 6.2 Experimental set up Where applicable we use smoothing c = 0.1, a threshold ( = 0.95, and cautiousness parameters n0 = An = 5 as in Collins and Singer (1999) and propagation parameters µ = 0.6, v = 0.01 as in Subramanya et al. (2010). Initial experiments with different propagation parameters suggested that as long as v was set at this value changing µ had relatively little effect on the accuracy. We did not find any propagation parameter settings that outperformed this choice. For the Yarowsky-prop algorithms we perform a single iteration of the propagation update for each iteration of the algorithm. For EM we use weights A1 = 0.98, and A2 = 0.02 (see section 3.8), which were found in initial experiments to be the best values, and results are aver</context>
<context position="25576" citStr="Collins and Singer (1999)" startWordPosition="4414" endWordPosition="4417">Yarowsky-prop-cautious 0-only is correct but none of the other Yarowsky variants are. Note that it succeeds despite misleading features; “maker” and “company” might be taken to indicate a company and “president-of” an organization, but all three examples are locations. Yarowsky-prop-cautious 0-0 and 7r-0 also perform respectably, although not as well. Yarowskyprop-cautious 0T-only and the non-cautious versions are significantly worse. Although 0T-only was intended to incorporate Subramanya et al. (2010)’s idea of type level distributions, it in fact performs worse than 0-only. We believe that Collins and Singer (1999)’s definition (1) of 0 incorporates sufficient type level information that the creation of a separate distribution is unnecessary in this case. Figure 4 shows the test set non-seeded accuracies as a function of the iteration for many of the algo8The software is included with the paper submission and will be maintained at https://github.com/sfu-natlang/yarowsky. 625 Algorithm named entity drug Task sentence land EM 81.05 78.64 55.96 54.85 32.86 31.07 67.88 65.42 ±0.31 ±0.34 ±0.41 ±0.43 ±0.00 ±0.00 ±3.35 ±3.57 Seed DL 11.29 0.00 5.18 0.00 2.89 0.00 7.18 0.00 DL-CoTrain (cautious) 91.56 90.49 59.</context>
<context position="28088" citStr="Collins and Singer (1999)" startWordPosition="4796" endWordPosition="4799">re a maximum in their column. Italic items have a statistically significant difference versus DL-CoTrain (p &lt; 0.05 with a McNemar test). For EM, ± indicates one standard deviation but statistical significance was not measured. rithms on the named entity task. The Yarowsky-prop non-cautious algorithms quickly converge to the final accuracy and are not shown. While the other algorithms (figure 4(a)) make a large accuracy improvement in the final retraining step, the Yarowskyprop (figure 4(b)) algorithms reach comparable accuracies earlier and gain much less from retraining. We did not implement Collins and Singer (1999)’s CoBoost; however, in their results it performs comparably to DL-CoTrain and Yarowsky-cautious. As with DL-CoTrain, CoBoost requires two views. 6.4 Cautiousness Cautiousness appears to be important in the performance of the algorithms we tested. In table 3, only the cautious algorithms are able to reach the 90% accuracy range. To evaluate the effects of cautiousness we examine the Yarowsky-prop O-only algorithm on the named entity task in more detail. This algorithm has two classifiers which are trained in conjunction: the DL and the propagated OP. Figure 5 shows the training set coverage (o</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In In EMNLP 1999: Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Madrid, Spain,</location>
<contexts>
<context position="19988" citStr="Collins, 1997" startWordPosition="3504" endWordPosition="3505">.999947 New-York loc. 12 0.999946 court-in loc. 13 0.975154 Company-of loc. ... Figure 2: A DL from iteration 5 of Yarowsky on the named entity task. Scores are pre-normalized values from the expression on the left side of (1), not Ofj values. Context features are indicated by italics; all others are spelling features. Specific feature types are omitted. Seed rules are indicated by bold ranks. kindly provided by the respective authors. The task of Collins and Singer (1999) is named entity classification on data from New York Times text.7 The data set was pre-processed by a statistical parser (Collins, 1997) and all noun phrases that are potential named entities were extracted from the parse tree. Each noun phrase is to be labelled as a person, organization, or location. The parse tree provides the surrounding context as context features such as the words in prepositional phrase and relative clause modifiers, etc., and the actual words in the noun phrase provide the spelling features. The test data additionally contains some noise examples which are not in the three named entity categories. We use the seed rules the authors provide, which are the first seven items in figure 2. For DL-CoTrain, we </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Seeding, transduction, out-ofsample error and the Microsoft approach... Blog post at http://nlpers.blogspot.com/2011/04/seedingtransduction-out-of-sample.html,</title>
<date>2011</date>
<contexts>
<context position="23426" citStr="Daume, 2011" startWordPosition="4089" endWordPosition="4090">noise accuracy as accuracy that includes such instances, and clean accuracy as accuracy calculated across only the examples which are one of the known labels. We report only clean accuracy in this paper; noise accuracy tracks clean accuracy but is a little lower. There is no difference on the word sense data sets. We also report (clean) non-seeded accuracy, which we define to be clean accuracy over only examples which are not assigned a label by the seed rules. This is intended to evaluate what the algorithm has learned, rather than what it can achieve by using the input information directly (Daume, 2011). We test Yarowsky, Yarowsky-cautious, Yarowsky-sum, DL-CoTrain, HS-bipartite in all four forms, and Yarowsky-prop cautious and non-cautious and in all four forms. For each algorithm except EM we perform a final retraining step Gold Spelling features Context features loc. Waukegan maker, LEFT loc. Mexico, president, of president-of, RIGHT loc. La-Jolla, La Jolla company, LEFT Figure 3: Named entity test set examples where Yarowsky-prop 0-only is correct and no other tested algorithms are correct. The specific feature types are omitted. as described for Yarowsky-cautious (section 3.2). Our prog</context>
</contexts>
<marker>Daume, 2011</marker>
<rawString>Hal Daume. 2011. Seeding, transduction, out-ofsample error and the Microsoft approach... Blog post at http://nlpers.blogspot.com/2011/04/seedingtransduction-out-of-sample.html, April 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Damianos Karakos</author>
</authors>
<title>Bootstrapping without the boot.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>395--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="19073" citStr="Eisner and Karakos (2005)" startWordPosition="3352" endWordPosition="3355">for BP). This choice is intended to provide an equivalent to the Yarowsky-cautious behaviour of limiting the number of labelled examples; most BPf are non-uniform, so without it most examples become labelled early. We observe further similarity between the Yarowsky algorithm and the general approach of Subramanya et al. (2010) by comparing algorithm 3 here with their algorithm 1. The comments in algorithm 3 give the corresponding parts of their algorithm. Note that each line has a similar purpose. 6 Evaluation 6.1 Tasks and data For evaluation we use the tasks of Collins and Singer (1999) and Eisner and Karakos (2005), with data Rank Score Feature Label 1 0.999900 New-York loc. 2 0.999900 California loc. 3 0.999900 U.S. loc. 4 0.999900 Microsoft org. 5 0.999900 I.B.M. org. 6 0.999900 Incorporated org. 7 0.999900 Mr. per. 8 0.999976 U.S. loc. 9 0.999957 New-York-Stock-Exchange loc. 10 0.999952 California loc. 11 0.999947 New-York loc. 12 0.999946 court-in loc. 13 0.975154 Company-of loc. ... Figure 2: A DL from iteration 5 of Yarowsky on the named entity task. Scores are pre-normalized values from the expression on the left side of (1), not Ofj values. Context features are indicated by italics; all others a</context>
<context position="20778" citStr="Eisner and Karakos (2005)" startWordPosition="3638" endWordPosition="3641">. The parse tree provides the surrounding context as context features such as the words in prepositional phrase and relative clause modifiers, etc., and the actual words in the noun phrase provide the spelling features. The test data additionally contains some noise examples which are not in the three named entity categories. We use the seed rules the authors provide, which are the first seven items in figure 2. For DL-CoTrain, we use their two views: one view is the spelling features, and the other is the context features. Figure 2 shows a DL from Yarowsky training on this task. The tasks of Eisner and Karakos (2005) are word sense disambiguation on several English words which have two senses corresponding to two different words in French. Data was extracted from the Canadian Hansards, using the English side to produce training and test data and the French side to produce the gold labelling. Features are the original and lemmatized words immediately adja7We removed weekday and month examples from the test set as they describe. They note 88962 examples in their training set, but the file has 89305. We did not find any filtering criteria that produced the expected size, and therefore used all examples. 624 </context>
</contexts>
<marker>Eisner, Karakos, 2005</marker>
<rawString>Jason Eisner and Damianos Karakos. 2005. Bootstrapping without the boot. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 395–402, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Analysis of semi-supervised learning with the Yarowsky algorithm.</title>
<date>2007</date>
<booktitle>In UAI 2007, Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>159--166</pages>
<location>Vancouver, BC, Canada,</location>
<contexts>
<context position="1589" citStr="Haffari and Sarkar (2007)" startWordPosition="245" endWordPosition="249">lled points. We refer to this setting as bootstrapping. In contrast, typical semi-supervised learning deals with a large number of labelled points, and a domain adaptation task with unlabelled points from the new domain. The two dominant discriminative learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically *This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. We would like to thank Gholamreza Haffari and the anonymous reviewers for their comments. We particularly thank Michael Collins, Jason Eisner, and Damianos Karakos for the data we used in our experiments. x denotes an example f, g denote featur</context>
<context position="4974" citStr="Haffari and Sarkar (2007)" startWordPosition="822" endWordPosition="825">rain a new DL B on Y (t) 4: apply B to X, to produce Y (t+1) 5: end for seed rules but uses them to inject features into a joint model p(x, j) which they train using expectationmaximization for Markov random fields. We focus on discriminative training which does not require complex partition functions for normalization. Blum and Chawla (2001) introduce an early use of transductive learning using graph propagation. X. Zhu and Z. Ghahramani and J. Lafferty (2003)’s method of graph propagation is predominantly transductive, and the non-transductive version is closely related to Abney (2004) c.f. Haffari and Sarkar (2007).1 3 Existing algorithms 3.1 Yarowsky A decision list (DL) is a (ordered) list of featurelabel pairs (rules) which is produced by assigning a score to each rule and sorting on this score. It chooses a label for an example from the first rule whose feature is a feature of the example. For a DL the prediction distribution is defined by 7rx(j) ∝ maxfEFx Bfj. The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y M. We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores |Afj |+ c Bfj</context>
<context position="7814" citStr="Haffari and Sarkar (2007)" startWordPosition="1306" endWordPosition="1309">call this the retraining step.4 3.3 DL-CoTrain Collins and Singer (1999) also introduce the cotraining algorithm DL-CoTrain. This algorithm alternates between two DLs using disjoint views of the features in the data. At each step it trains a DL and then produces a new labelling for the other DL. Each DL uses thresholding and cautiousness as we describe for Yarowsky-cautious. At the end the DLs are combined, the result is used to label the data, and a retraining step is done from this single labelling. 3.4 Y-1/DL-1-VS One of the variant algorithms of Abney (2004) is Y-1/DL-1-VS (referred to by Haffari and Sarkar (2007) as simply DL-1). Besides various changes in the specifics of how the labelling is produced, this algorithm has two differences versus Yarowsky. Firstly, the smoothing constant c in (1) is replaced by 1/|Vf|. Secondly, 7r is redefined as 7rx(j) _ B , which we refer to as the sum def|Fx�fEFf7 I x inition of 7r. This definition does not match a literal DL but is easier to analyze. We are not concerned here with the details of Y-1/DL-1-VS, but we note that Haffari and Sarkar but is used for DL-CoTrain in the same paper. 4The details of Yarowsky-cautious are not clearly specified in Collins and Si</context>
<context position="9776" citStr="Haffari and Sarkar (2007)" startWordPosition="1658" endWordPosition="1661">ross-entropy follows the standard definition in terms of Ht2 and Bt2. The objective minimized by Y-1/DL-1-VS is: � � � Bt2(�x||�f) − o2� x y (2) 3.5 Yarowsky-sum As a baseline for the sum definition of 7r, we introduce the Yarowsky-sum algorithm. It is the same as Yarowsky except that we use the sum definition when labelling: for example x we choose the label j with the highest (sum) 7rx(j), but set Yx = 1 if the sum is zero. Note that this is a linear model similar to a conditional random field (CRF) (Lafferty et al., 2001) for unstructured multiclass problems. 3.6 Bipartite graph algorithms Haffari and Sarkar (2007) suggest a bipartite graph framework for semi-supervised learning based on their analysis of Y-1/DL-1-VS and objective (2). The graph has vertices X U F and edges {(x, f) : x E X, f E Fx}, as in the graph shown in figure 1(a). Each vertex represents a distribution over labels, and in this view Yarowsky can be seen as alternately updating the example distributions based on the feature distributions and visa versa. Based on this they give algorithm 2, which we call HS-bipartite. It is parametrized by two functions which are called features-to-example and examples-to-feature here. Each can be one</context>
<context position="14243" citStr="Haffari and Sarkar (2007)" startWordPosition="2461" endWordPosition="2464">eighbourhood of vertex v, and qv is an initial distribution for each vertex v to be smoothed. They give an iterative update to minimize (3). Note that (3) is independent of their specific graph structure, distributions, and semi-supervised learning algorithm. We propose four methods for using this propagation with Yarowsky. These methods all use constant edge weights (wuv = 1). The distributions and graph structures are shown in table 2. Figure 1 shows example graphs for 0-B and B-only. 7r-B and BT-only are similar, and are described below. The graph structure of 0-B is the bipartite graph of Haffari and Sarkar (2007). In fact, 0-B the propagation objective (3) and Haffari and Sarkar (2007)’s Y-1/DL-1-VS objective (2) are identical up to constant coefficients and an extra constant term.6 0-B 5We omit the option to hold some of the distributions at fixed values, which would add an extra term to the objective. 6The differences are specifically: First, (3) adds the constant coefficients µ and v. Second, (3) sums over each edge twice (once in each direction), while (2) sums over each only once. Since wuv = wvu and Bt2(qu, qv) = Bt2(qv, qu), this can be folded into the constant µ. Third, after expanding (2) the</context>
<context position="31157" citStr="Haffari and Sarkar (2007)" startWordPosition="5283" endWordPosition="5286">e propagation method 0-0 was motivated by optimizing the equivalent objectives (2) and (3) at each iteration. Figure 6 shows the graph propagation objective (3) along with accuracy for Yarowsky-prop 0-0 without cautiousness. The objective value decreases as expected, and converges along with accuracy. Conversely, the cautious version (not shown here) does not clearly minimize the objective, since cautiousness limits the effect of the propagation. 7 Conclusions N Our novel algorithm achieves accuracy comparable to Yarowsky-cautious, but is better theoretically motivated by combining ideas from Haffari and Sarkar (2007) and Subramanya et al. (2010). It also achieves accuracy comparable to DL-CoTrain, but does not require the features to be split into two independent views. As future work, we would like to apply our al(a) Non-cautious (b) Cautious Figure 5: Internal train set coverage and non-seeded test accuracy (same scale) for Yarowsky-prop 0-only on named entity. Iteration main coverage Figure 6: Non-seeded test accuracy (left axis), coverage (left axis, same scale), and objective value (right axis) for Yarowskyprop 0-0. Iterations are shown on a log scale. We omit the first iteration (where the DL contai</context>
</contexts>
<marker>Haffari, Sarkar, 2007</marker>
<rawString>Gholamreza Haffari and Anoop Sarkar. 2007. Analysis of semi-supervised learning with the Yarowsky algorithm. In UAI 2007, Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence, Vancouver, BC, Canada, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven grammar induction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>881--888</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="3908" citStr="Haghighi and Klein (2006" startWordPosition="645" endWordPosition="648">orm distribution. In the bootstrapping setting the learner is given an initial partial labelling Y (°) where only a few examples are labelled (i.e. Y (°) x = L for most x). Abney (2004) defines three probability distributions in his analysis of bootstrapping: Bfj is the parameter for feature f with label j, taken to be normalized so that Bf is a distribution over labels. ox is the labelling distribution representing the current Y ; it is a point distribution for labelled examples and uniform for unlabelled examples. 7rx is the prediction distribution over labels for example x. The approach of Haghighi and Klein (2006b) and Haghighi and Klein (2006a) also uses a small set of 620 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 620–628, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Algorithm 1: The basic Yarowsky algorithm. Require: training data X and a seed DL B(°) 1: apply B(°) to X produce a labelling Y (°) 2: for iteration t to maximum or convergence do 3: train a new DL B on Y (t) 4: apply B to X, to produce Y (t+1) 5: end for seed rules but uses them to inject features into a joint model p(x, j) which they train</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006a. Prototype-driven grammar induction. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 881–888, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="3908" citStr="Haghighi and Klein (2006" startWordPosition="645" endWordPosition="648">orm distribution. In the bootstrapping setting the learner is given an initial partial labelling Y (°) where only a few examples are labelled (i.e. Y (°) x = L for most x). Abney (2004) defines three probability distributions in his analysis of bootstrapping: Bfj is the parameter for feature f with label j, taken to be normalized so that Bf is a distribution over labels. ox is the labelling distribution representing the current Y ; it is a point distribution for labelled examples and uniform for unlabelled examples. 7rx is the prediction distribution over labels for example x. The approach of Haghighi and Klein (2006b) and Haghighi and Klein (2006a) also uses a small set of 620 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 620–628, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Algorithm 1: The basic Yarowsky algorithm. Require: training data X and a seed DL B(°) 1: apply B(°) to X produce a labelling Y (°) 2: for iteration t to maximum or convergence do 3: train a new DL B on Y (t) 4: apply B to X, to produce Y (t+1) 5: end for seed rules but uses them to inject features into a joint model p(x, j) which they train</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006b. Prototype-driven learning for sequence models. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 320–327, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics - Volume 2, COLING ’92,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5843" citStr="Hearst, 1992" startWordPosition="979" endWordPosition="980">is a feature of the example. For a DL the prediction distribution is defined by 7rx(j) ∝ maxfEFx Bfj. The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y M. We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores |Afj |+ c Bfj ∝ (1) |Af |+ Lc where c is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold C. In our implementation we add the seed rules to each subsequent DL.3 1Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2It is similar to that of Yarowsky (1995) but is better specified and omits word sense disambiguation optimizations. The general algorithm in Yarowsky (1995) is self-training with any kind of underlying supervised classifier, but we follow the convention of using Yarowsky to refer to the DL algorithm. 3This is not clearly specified in Collins and Singer (1999), </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics - Volume 2, COLING ’92, pages 539–545, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="9681" citStr="Lafferty et al., 2001" startWordPosition="1646" endWordPosition="1649"> q||2. Then Bregman distance-based entropy is Ht2(p) = − Ei p2i, KL-Divergence is Bt2, and cross-entropy follows the standard definition in terms of Ht2 and Bt2. The objective minimized by Y-1/DL-1-VS is: � � � Bt2(�x||�f) − o2� x y (2) 3.5 Yarowsky-sum As a baseline for the sum definition of 7r, we introduce the Yarowsky-sum algorithm. It is the same as Yarowsky except that we use the sum definition when labelling: for example x we choose the label j with the highest (sum) 7rx(j), but set Yx = 1 if the sum is zero. Note that this is a linear model similar to a conditional random field (CRF) (Lafferty et al., 2001) for unstructured multiclass problems. 3.6 Bipartite graph algorithms Haffari and Sarkar (2007) suggest a bipartite graph framework for semi-supervised learning based on their analysis of Y-1/DL-1-VS and objective (2). The graph has vertices X U F and edges {(x, f) : x E X, f E Fx}, as in the graph shown in figure 1(a). Each vertex represents a distribution over labels, and in this view Yarowsky can be seen as alternately updating the example distributions based on the feature distributions and visa versa. Based on this they give algorithm 2, which we call HS-bipartite. It is parametrized by t</context>
<context position="11844" citStr="Lafferty et al., 2001" startWordPosition="2024" endWordPosition="2027">ph. We label test data by constructing new Ox = examples-to-feature(Fx) for the unseen x. 3.7 Semi-supervised learning algorithm of Subramanya et al. (2010) Subramanya et al. (2010) give a semi-supervised algorithm for part of speech tagging. Unlike the algorithms described above, it is for domain adaptation with large amounts of labelled data rather than bootstrapping with a small number of seeds. This algorithm is structurally similar to Yarowsky in that it begins from an initial partial labelling and repeatedly trains a classifier on the labelling and then relabels the data. It uses a CRF (Lafferty et al., 2001) as the underlying supervised learner. It differs significantly from Yarowsky in two other ways: First, instead of only training a CRF it also uses a step of graph propagation between distributions over the n-grams in the data. Second, it does the propagation on distributions over n-gram types rather than over n-gram tokens (instances in the data). They argue that using propagation over types allows the algorithm to enforce constraints and find similarities that self-training cannot. We are not concerned here with the details of this algorithm, but it motivates our work firstly in providing th</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="13259" citStr="Nigam et al., 2000" startWordPosition="2291" endWordPosition="2294">’s EM We implemented the EM algorithm of Collins and Singer (1999) as a baseline for the other algorithms. E Ht2(�x||�f) = � xEX xEX fEFx fEFx 622 Method V N (u) qu 0-B X ∪ F N. = F., Nf = Xf q. = 0., qf = Bf 7r-B X ∪ F N. = F., Nf = Xf q. = 7r., qf = Bf B-only F Nf = U.EXf F. \ f qf = Bf BT-only F Nf = U.EXf F. \ f qf = BTf Table 2: Graph structures for propagation. They do not specify tuning details, but to get comparable accuracy we found it was necessary to do smoothing and to include weights A1 and A2 on the expected counts of seed-labelled and initially unlabelled examples respectively (Nigam et al., 2000). 4 Graph propagation The graph propagation of Subramanya et al. (2010) is a method for smoothing distributions attached to vertices of a graph. Here we present it with an alternate notation using Bregman distances as described in section 3.4.5 The objective is µ E EwuvBt2(qu, qv) + � Bt2(qu, U) (3) uEV uEV vEN(i) where V is a set of vertices, N(v) is the neighbourhood of vertex v, and qv is an initial distribution for each vertex v to be smoothed. They give an iterative update to minimize (3). Note that (3) is independent of their specific graph structure, distributions, and semi-supervised l</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 30(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jessica Shepherd</author>
</authors>
<title>A corpusbased approach for building semantic lexicons. In</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="5921" citStr="Riloff and Shepherd, 1997" startWordPosition="987" endWordPosition="990"> is defined by 7rx(j) ∝ maxfEFx Bfj. The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y M. We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores |Afj |+ c Bfj ∝ (1) |Af |+ Lc where c is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold C. In our implementation we add the seed rules to each subsequent DL.3 1Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2It is similar to that of Yarowsky (1995) but is better specified and omits word sense disambiguation optimizations. The general algorithm in Yarowsky (1995) is self-training with any kind of underlying supervised classifier, but we follow the convention of using Yarowsky to refer to the DL algorithm. 3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious Collins and Singer (1999) also introduce a variant algor</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Ellen Riloff and Jessica Shepherd. 1997. A corpusbased approach for building semantic lexicons. In In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 117– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Scudder</author>
</authors>
<title>Probability of error of some adaptive pattern-recognition machines.</title>
<date>1965</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>11--363</pages>
<contexts>
<context position="1285" citStr="Scudder, 1965" startWordPosition="200" endWordPosition="202">rent natural language data sets. 1 Introduction In this paper, we are concerned with a case of semisupervised learning that is close to unsupervised learning, in that the labelled and unlabelled data points are from the same domain and only a small set of seed rules is used to derive the labelled points. We refer to this setting as bootstrapping. In contrast, typical semi-supervised learning deals with a large number of labelled points, and a domain adaptation task with unlabelled points from the new domain. The two dominant discriminative learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically *This research was par</context>
</contexts>
<marker>Scudder, 1965</marker>
<rawString>H. J. Scudder. 1965. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory, 11:363–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Slav Petrov</author>
<author>Fernando Pereira</author>
</authors>
<title>Efficient graph-based semi-supervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>167--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1841" citStr="Subramanya et al. (2010)" startWordPosition="288" endWordPosition="291">learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically *This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. We would like to thank Gholamreza Haffari and the anonymous reviewers for their comments. We particularly thank Michael Collins, Jason Eisner, and Damianos Karakos for the data we used in our experiments. x denotes an example f, g denote features i, k denote labels X set of training examples Fx set of features for example x Y current labelling of X Yx current label for example x L value of Yx for unlabelled examples L number of labels (not including L) A set of currently labelled examples V </context>
<context position="11378" citStr="Subramanya et al. (2010)" startWordPosition="1945" endWordPosition="1949"> for iteration t to maximum or convergence do 3: for f E F do 4: let p = examples-to-feature({Ox : x E Xf}) 5: if p =� U then let Of = p 6: end for 7: for x E X do 8: let p = features-to-example({Bf : f E Fx}) 9: if p =� U then let Ox = p 10: end for 11: end for to Y-1/DL-1-VS, and the majority-majority form minimizes a different objective similar to (2). In our implementation we label training data (for the convergence check) with the 0 distributions from the graph. We label test data by constructing new Ox = examples-to-feature(Fx) for the unseen x. 3.7 Semi-supervised learning algorithm of Subramanya et al. (2010) Subramanya et al. (2010) give a semi-supervised algorithm for part of speech tagging. Unlike the algorithms described above, it is for domain adaptation with large amounts of labelled data rather than bootstrapping with a small number of seeds. This algorithm is structurally similar to Yarowsky in that it begins from an initial partial labelling and repeatedly trains a classifier on the labelling and then relabels the data. It uses a CRF (Lafferty et al., 2001) as the underlying supervised learner. It differs significantly from Yarowsky in two other ways: First, instead of only training a CRF</context>
<context position="13330" citStr="Subramanya et al. (2010)" startWordPosition="2302" endWordPosition="2305"> as a baseline for the other algorithms. E Ht2(�x||�f) = � xEX xEX fEFx fEFx 622 Method V N (u) qu 0-B X ∪ F N. = F., Nf = Xf q. = 0., qf = Bf 7r-B X ∪ F N. = F., Nf = Xf q. = 7r., qf = Bf B-only F Nf = U.EXf F. \ f qf = Bf BT-only F Nf = U.EXf F. \ f qf = BTf Table 2: Graph structures for propagation. They do not specify tuning details, but to get comparable accuracy we found it was necessary to do smoothing and to include weights A1 and A2 on the expected counts of seed-labelled and initially unlabelled examples respectively (Nigam et al., 2000). 4 Graph propagation The graph propagation of Subramanya et al. (2010) is a method for smoothing distributions attached to vertices of a graph. Here we present it with an alternate notation using Bregman distances as described in section 3.4.5 The objective is µ E EwuvBt2(qu, qv) + � Bt2(qu, U) (3) uEV uEV vEN(i) where V is a set of vertices, N(v) is the neighbourhood of vertex v, and qv is an initial distribution for each vertex v to be smoothed. They give an iterative update to minimize (3). Note that (3) is independent of their specific graph structure, distributions, and semi-supervised learning algorithm. We propose four methods for using this propagation w</context>
<context position="15464" citStr="Subramanya et al. (2010)" startWordPosition="2680" endWordPosition="2683">there is a term |Fx |inside the sum for Ht2(0x) which is not present in (3). This does not effect the direction of minimization. Fourth, Bt2(qu, U) in (3) expands to Ht2(qu) plus a constant, adding an extra constant term to the total. Figure 1: Example graphs for 0-0 and 0-only propagation. therefore gives us a direct way to optimize (2). The other three methods do not correspond to the objective of Haffari and Sarkar (2007). The 7r-B method is like 0-B except for using 7r as the distribution for example vertices. The bipartite graph of the first two methods differs from the structure used by Subramanya et al. (2010) in that it does propagation between two different kinds of distributions instead of only one kind. We also adopt a more comparable approach with a graph over only features. Here we define adjacency by co-occurrence in the same example. The B-only method uses this graph and B as the distribution. Finally, we noted in section 3.7 that the algorithm of Subramanya et al. (2010) does one additional step in converting from token level distributions to type level distributions. The BT-only method therefore uses the feature-only graph but for the distribution uses a type level version of B defined by</context>
<context position="18776" citStr="Subramanya et al. (2010)" startWordPosition="3299" endWordPosition="3302">r of using evidence only from rules that are in the list. We include the cautiousness of Yarowskycautious (section 3.2) in the DL training on line 7. At the labelling step on line 6 we label only examples which the pre-propagated B would also assign a label (using the same rules described above for BP). This choice is intended to provide an equivalent to the Yarowsky-cautious behaviour of limiting the number of labelled examples; most BPf are non-uniform, so without it most examples become labelled early. We observe further similarity between the Yarowsky algorithm and the general approach of Subramanya et al. (2010) by comparing algorithm 3 here with their algorithm 1. The comments in algorithm 3 give the corresponding parts of their algorithm. Note that each line has a similar purpose. 6 Evaluation 6.1 Tasks and data For evaluation we use the tasks of Collins and Singer (1999) and Eisner and Karakos (2005), with data Rank Score Feature Label 1 0.999900 New-York loc. 2 0.999900 California loc. 3 0.999900 U.S. loc. 4 0.999900 Microsoft org. 5 0.999900 I.B.M. org. 6 0.999900 Incorporated org. 7 0.999900 Mr. per. 8 0.999976 U.S. loc. 9 0.999957 New-York-Stock-Exchange loc. 10 0.999952 California loc. 11 0.9</context>
<context position="22118" citStr="Subramanya et al. (2010)" startWordPosition="3866" endWordPosition="3869">e pairs of adjacent word features, with one feature for each label (sense). We use the ‘drug’, ‘land’, and ‘sentence’ tasks, and the seed rules from their best seed selection: ‘alcohol’/‘medical’, ‘acres’/‘court’, and ‘reads’/‘served’ respectively (they do not provide seeds for their other three tasks). For DL-CoTrain we use adjacent words for one view and context words for the other. 6.2 Experimental set up Where applicable we use smoothing c = 0.1, a threshold ( = 0.95, and cautiousness parameters n0 = An = 5 as in Collins and Singer (1999) and propagation parameters µ = 0.6, v = 0.01 as in Subramanya et al. (2010). Initial experiments with different propagation parameters suggested that as long as v was set at this value changing µ had relatively little effect on the accuracy. We did not find any propagation parameter settings that outperformed this choice. For the Yarowsky-prop algorithms we perform a single iteration of the propagation update for each iteration of the algorithm. For EM we use weights A1 = 0.98, and A2 = 0.02 (see section 3.8), which were found in initial experiments to be the best values, and results are averaged over 10 random initializations. The named entity test set contains some</context>
<context position="25459" citStr="Subramanya et al. (2010)" startWordPosition="4395" endWordPosition="4398">ral percent above DL-CoTrain on that task. Figure 3 shows (all) three examples from the named entity test set where Yarowsky-prop-cautious 0-only is correct but none of the other Yarowsky variants are. Note that it succeeds despite misleading features; “maker” and “company” might be taken to indicate a company and “president-of” an organization, but all three examples are locations. Yarowsky-prop-cautious 0-0 and 7r-0 also perform respectably, although not as well. Yarowskyprop-cautious 0T-only and the non-cautious versions are significantly worse. Although 0T-only was intended to incorporate Subramanya et al. (2010)’s idea of type level distributions, it in fact performs worse than 0-only. We believe that Collins and Singer (1999)’s definition (1) of 0 incorporates sufficient type level information that the creation of a separate distribution is unnecessary in this case. Figure 4 shows the test set non-seeded accuracies as a function of the iteration for many of the algo8The software is included with the paper submission and will be maintained at https://github.com/sfu-natlang/yarowsky. 625 Algorithm named entity drug Task sentence land EM 81.05 78.64 55.96 54.85 32.86 31.07 67.88 65.42 ±0.31 ±0.34 ±0.41</context>
<context position="31186" citStr="Subramanya et al. (2010)" startWordPosition="5288" endWordPosition="5291">otivated by optimizing the equivalent objectives (2) and (3) at each iteration. Figure 6 shows the graph propagation objective (3) along with accuracy for Yarowsky-prop 0-0 without cautiousness. The objective value decreases as expected, and converges along with accuracy. Conversely, the cautious version (not shown here) does not clearly minimize the objective, since cautiousness limits the effect of the propagation. 7 Conclusions N Our novel algorithm achieves accuracy comparable to Yarowsky-cautious, but is better theoretically motivated by combining ideas from Haffari and Sarkar (2007) and Subramanya et al. (2010). It also achieves accuracy comparable to DL-CoTrain, but does not require the features to be split into two independent views. As future work, we would like to apply our al(a) Non-cautious (b) Cautious Figure 5: Internal train set coverage and non-seeded test accuracy (same scale) for Yarowsky-prop 0-only on named entity. Iteration main coverage Figure 6: Non-seeded test accuracy (left axis), coverage (left axis, same scale), and objective value (right axis) for Yarowskyprop 0-0. Iterations are shown on a log scale. We omit the first iteration (where the DL contains only the seed rules) and s</context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>Amarnag Subramanya, Slav Petrov, and Fernando Pereira. 2010. Efficient graph-based semi-supervised learning of structured tagging models. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 167–176, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
</authors>
<title>Graph-based weaklysupervised methods for information extraction &amp; integration.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<note>Software: https://github.com/parthatalukdar/junto.</note>
<contexts>
<context position="5949" citStr="Talukdar, 2010" startWordPosition="993" endWordPosition="994">e basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y M. We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores |Afj |+ c Bfj ∝ (1) |Af |+ Lc where c is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold C. In our implementation we add the seed rules to each subsequent DL.3 1Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2It is similar to that of Yarowsky (1995) but is better specified and omits word sense disambiguation optimizations. The general algorithm in Yarowsky (1995) is self-training with any kind of underlying supervised classifier, but we follow the convention of using Yarowsky to refer to the DL algorithm. 3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious Collins and Singer (1999) also introduce a variant algorithm Yarowsky-cautious. Here</context>
</contexts>
<marker>Talukdar, 2010</marker>
<rawString>Partha Pratim Talukdar. 2010. Graph-based weaklysupervised methods for information extraction &amp; integration. Ph.D. thesis, University of Pennsylvania. Software: https://github.com/parthatalukdar/junto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings ofInternational Conference on Machine Learning.</booktitle>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu and Z. Ghahramani and J. Lafferty. 2003. Semisupervised learning using Gaussian fields and harmonic functions. In Proceedings ofInternational Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="1441" citStr="Yarowsky, 1995" startWordPosition="224" endWordPosition="225">ng, in that the labelled and unlabelled data points are from the same domain and only a small set of seed rules is used to derive the labelled points. We refer to this setting as bootstrapping. In contrast, typical semi-supervised learning deals with a large number of labelled points, and a domain adaptation task with unlabelled points from the new domain. The two dominant discriminative learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically *This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. We would like to thank Gholamreza Haffari and the anonymous reviewers for their comments. We par</context>
<context position="6120" citStr="Yarowsky (1995)" startWordPosition="1023" endWordPosition="1024">xact specification of Yarowsky.2 It uses DL rule scores |Afj |+ c Bfj ∝ (1) |Af |+ Lc where c is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold C. In our implementation we add the seed rules to each subsequent DL.3 1Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2It is similar to that of Yarowsky (1995) but is better specified and omits word sense disambiguation optimizations. The general algorithm in Yarowsky (1995) is self-training with any kind of underlying supervised classifier, but we follow the convention of using Yarowsky to refer to the DL algorithm. 3This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious Collins and Singer (1999) also introduce a variant algorithm Yarowsky-cautious. Here the DL training step keeps only the top n rules (f, j) over the threshold for each label j, ordered by |Af|. Additionally the threshold ( is checked against |Afj|/|Af |in</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, Massachusetts, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>