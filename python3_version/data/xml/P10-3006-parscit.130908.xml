<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023608">
<title confidence="0.999224">
Unsupervised Search for The Optimal Segmentation for Statistical
Machine Translation
</title>
<author confidence="0.997689">
Cos¸kun Mermer1,3 and Ahmet Afs¸ın Akın2,3
</author>
<affiliation confidence="0.968936">
1Bo˘gazic¸i University, Bebek, Istanbul, Turkey
2Istanbul Technical University, Sarıyer, Istanbul, Turkey
</affiliation>
<address confidence="0.78244">
3T ¨UB˙ITAK-UEKAE, Gebze, Kocaeli, Turkey
</address>
<email confidence="0.998303">
{coskun,ahmetaa}@uekae.tubitak.gov.tr
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990555555556">
We tackle the previously unaddressed
problem of unsupervised determination of
the optimal morphological segmentation
for statistical machine translation (SMT)
and propose a segmentation metric that
takes into account both sides of the SMT
training corpus. We formulate the objec-
tive function as the posterior probability of
the training corpus according to a genera-
tive segmentation-translation model. We
describe how the IBM Model-1 transla-
tion likelihood can be computed incremen-
tally between adjacent segmentation states
for efficient computation. Submerging the
proposed segmentation method in a SMT
task from morphologically-rich Turkish to
English does not exhibit the expected im-
provement in translation BLEU scores and
confirms the robustness of phrase-based
SMT to translation unit combinatorics.
A positive outcome of this work is the
described modification to the sequential
search algorithm of Morfessor (Creutz and
Lagus, 2007) that enables arbitrary-fold
parallelization of the computation, which
unexpectedly improves the translation per-
formance as measured by BLEU.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997425">
In statistical machine translation (SMT), words
are normally considered as the building blocks of
translation models. However, especially for mor-
phologically complex languages such as Finnish,
Turkish, Czech, Arabic etc., it has been shown
that using sub-lexical units obtained after morpho-
logical preprocessing can improve the machine
translation performance over a word-based sys-
tem (Habash and Sadat, 2006; Oflazer and Durgar
El-Kahlout, 2007; Bisazza and Federico, 2009).
However, the effect of segmentation on transla-
tion performance is indirect and difficult to isolate
(Lopez and Resnik, 2006).
The challenge in designing a sub-lexical SMT
system is the decision of what segmentation to use.
Linguistic morphological analysis is intuitive, but
it is language-dependent and could be highly am-
biguous. Furthermore, it is not necessarily opti-
mal in that (i) manually engineered segmentation
schemes can outperform a straightforward linguis-
tic morphological segmentation, e.g., (Habash and
Sadat, 2006), and (ii) it may result in even worse
performance than a word-based system, e.g., (Dur-
gar El-Kahlout and Oflazer, 2006).
A SMT system designer has to decide what
segmentation is optimal for the translation task
at hand. Existing solutions to this problem are
predominantly heuristic, language-dependent, and
as such are not easily portable to other lan-
guages. Another point to consider is that the op-
timal degree of segmentation might decrease as
the amount of training data increases (Lee, 2004;
Habash and Sadat, 2006). This brings into ques-
tion: For the particular language pair and training
corpus at hand, what is the optimal (level of) sub-
word segmentation? Therefore, it is desirable to
learn the optimal segmentation in an unsupervised
manner.
In this work, we extend the method of Creutz
and Lagus (2007) so as to maximize the transla-
tion posterior in unsupervised segmentation. The
learning process is tailored to the particular SMT
task via the same parallel corpus that is used in
training the statistical translation models.
</bodyText>
<sectionHeader confidence="0.999798" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998738571428571">
Most works in SMT-oriented segmentation are su-
pervised in that they consist of manual experimen-
tation to choose the best among a set of segmen-
tation schemes, and are language(pair)-dependent.
For Arabic, Sadat and Habash (2006) present sev-
eral morphological preprocessing schemes that en-
tail varying degrees of decomposition and com-
</bodyText>
<page confidence="0.998829">
31
</page>
<note confidence="0.604209">
Proceedings of the ACL 2010 Student Research Workshop, pages 31–36,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999961348484849">
pare the resulting translation performances in an
Arabic-to-English task. Shen et al. (2007) use a
subset of the morphology and apply only a few
simple rules in segmenting words. Durgar El-
Kahlout and Oflazer (2006) tackle this problem
when translating from English to Turkish, an ag-
glutinative language. They use a morphologi-
cal analyzer and disambiguation to arrive at mor-
phemes as tokens. However, training the trans-
lation models with morphemes actually degrades
the translation performance. They outperform
the word-based baseline only after some selec-
tive morpheme grouping. Bisazza and Federico
(2009) adopt an approach similar to the Arabic
segmentation studies above, this time in a Turkish-
to-English translation setting.
Unsupervised segmentation by itself has gar-
nered considerable attention in the computational
linguistics literature (Poon et al., 2009; Snyder and
Barzilay, 2008; Dasgupta and Ng, 2007; Creutz
and Lagus, 2007; Brent, 1999). However, few
works report their performance in a translation
task. Virpioja et al. (2007) used Morfessor (Creutz
and Lagus, 2007) to segment both sides of the par-
allel training corpora in translation between Dan-
ish, Finnish, and Swedish, but without a consistent
improvement in results.
Morfessor, which gives state of the art results in
many tests (Kurimo et al., 2009), uses only mono-
lingual information in its objective function. It is
conceivable that we can achieve a better segmenta-
tion for translation by considering not one but both
sides of the parallel corpus. A posssible choice is
the post-segmentation alignment accuracy. How-
ever, Elming et al. (2009) show that optimizing
segmentation with respect to alignment error rate
(AER) does not improve and even degrades ma-
chine translation performance. Snyder and Barzi-
lay (2008) use bilingual information but the seg-
mentation is learned independently from transla-
tion modeling.
In Chang et al. (2008), the granularity of the
Chinese word segmentation is optimized by train-
ing SMT systems for several values of a granular-
ity bias parameter and it is found that the value that
maximizes translation performance (as measured
by BLEU) is different than the value that maxi-
mizes segmentation accuracy (as measured by pre-
cision and recall).
One motivation in morphological preprocess-
ing before translation modeling is “morphology
matching” as in Lee (2004) and in the scheme
“EN” of Habash and Sadat (2006). In Lee (2004),
the goal is to match the lexical granularities of the
two languages by starting with a fine-grained seg-
mentation of the Arabic side of the corpus and
then merging or deleting Arabic morphemes us-
ing alignments with a part-of-speech tagged En-
glish corpus. But this method is not completely
unsupervised since it requires external linguistic
resources in initializing the segmentation with the
output of a morphological analyzer and disam-
biguator. Talbot and Osborne (2006) tackle a spe-
cial case of morphology matching by identifying
redundant distinctions in the morphology of one
language compared to another.
</bodyText>
<sectionHeader confidence="0.991755" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999883333333333">
Maximizing translation performance directly
would require SMT training and decoding for
each segmentation hypothesis considered, which
is computationally infeasible. So we make some
conditional independence assumptions using a
generative model and decompose the posterior
probability P(Mf|e, f). In this notation e and f
denote the two sides of a parallel corpus and Mf
denotes the segmentation model hypothesized for
f. Our approach is an extension of Morfessor
(Creutz and Lagus, 2007) so as to include the
translation model probability in its cost calcula-
tion. Specifically, the segmentation model takes
into account the likelihood of both sides of the
parallel corpus while searching for the optimal
segmentation. The joint likelihood is decomposed
into a prior, a monolingual likelihood, and a
translation likelihood, as shown in Eq. 1.
</bodyText>
<equation confidence="0.9966655">
P(e, f, Mf) = P(Mf)P(f|Mf)P(e|f, Mf)
(1)
</equation>
<bodyText confidence="0.867111666666667">
Assuming conditional independence between
e and Mf given f, the maximum a posteriori
(MAP) objective can be written as:
</bodyText>
<equation confidence="0.9228925">
ˆMf = arg max P(Mf)P(f|Mf)P(e|f) (2)
Mf
</equation>
<bodyText confidence="0.999969375">
The role of the bilingual component P(e|f)
in Eq. 2 can be motivated with a simple exam-
ple as follows. Consider an occurrence of two
phrase pairs in a Turkish-English parallel corpus
and the two hypothesized sets of segmentations
for the Turkish phrases as in Table 1. Without ac-
cess to the English side of the corpus, a monolin-
gual segmenter can quite possibly score Seg. #1
</bodyText>
<page confidence="0.995721">
32
</page>
<table confidence="0.556443">
Phrase #1 Phrase #2
Turkish phrase: anahtar anahtarım
English phrase: key my key
Seg. #1: anahtar anahtarı +m
Seg. #2: anahtar anahtar +ım
</table>
<tableCaption confidence="0.998274">
Table 1: Example segmentation hypotheses
</tableCaption>
<bodyText confidence="0.999970074074074">
higher than Seg. #2 (e.g., due to the high fre-
quency of the observed morph “+m”). On the
other hand, a bilingual segmenter is expected to
assign a higher alignment probability P(e|f) to
Seg. #2 than Seg. #1, because of the aligned words
key||anahtar, therefore ranking Seg. #2 higher.
The two monolingual components of Eq. 2 are
computed as in Creutz and Lagus (2007). To sum-
marize briefly, the prior P(Mf) is assumed to only
depend on the frequencies and lengths of the indi-
vidual morphs, which are also assumed to be in-
dependent. The monolingual likelihood P(f|Mf)
is computed as the product of morph probabilities
estimated from their frequencies in the corpus.
To compute the bilingual (translation) likeli-
hood P(e|f), we use IBM Model 1 (Brown et
al., 1993). Let an aligned sentence pair be rep-
resented by (se, sf), which consists of word se-
quences se = e1,..., el and sf = f1, ..., fm. Us-
ing a purely notational switch of the corpus labels
from here on to be consistent with the SMT lit-
erature, where the derivations are in the form of
P(f|e), the desired translation probability is given
by the expression:
The sentence length probability distribution
P(m|e) is assumed to be Poisson with the ex-
pected sentence length equal to m.
</bodyText>
<subsectionHeader confidence="0.8548685">
3.1 Incremental computation of Model-1
likelihood
</subsectionHeader>
<bodyText confidence="0.999825727272727">
During search, the translation likelihood P(e|f)
needs to be calculated according to Eq. 3 for every
hypothesized segmentation.
To compute Eq. 3, we need to have at hand the
individual morph translation probabilities t(fj|ei).
These can be estimated using the EM algorithm
given by (Brown, 1993), which is guaranteed to
converge to a global maximum of the likelihood
for Model 1. However, running the EM algorithm
to optimization for each considered segmentation
model can be computationally expensive, and can
result in overtraining. Therefore, in this work we
used the likelihood computed after the first EM
iteration, which also has the nice property that
P(f|e) can be computed incrementally from one
segmentation hypothesis to the next.
The incremental updates are derived from the
equations for the count collection and probability
estimation steps of the EM algorithm as follows.
In the count collection step, in the first iteration,
we need to compute the fractional counts c(fj|ei)
(Brown et al., 1993):
</bodyText>
<equation confidence="0.997549">
1
c(fj|ei) = l + 1(#fj)(#ei), (4)
</equation>
<bodyText confidence="0.998126454545455">
where (#fj) and (#ei) denote the number of occur-
rences of fj in sf and ei in se, respectively.
Let fk denote the word hypothesized to be seg-
mented. Let the resulting two sub-words be fp and
fq, any of which may or may not previously exist
in the vocabulary. Then, according to Eq. (4), as a
result of the segmentation no update is needed for
c(fj|ei) for j = 1... N, j =6 p, q, i = 1... M
(note that fk no longer exists); and the necessary
updates Δc(fj|ei) for c(fj|ei), where j = p, q;
i = 1... M are given by:
</bodyText>
<equation confidence="0.9981755">
1
Δc(fj|ei) = l + 1(#fk)(#ei). (5)
</equation>
<bodyText confidence="0.999941125">
Note that Eq. (5) is nothing but the previous
count value for the segmented word, c(fk|ei). So,
all needed in the count collection step is to copy
the set of values c(fk|ei) to c(fp|ei) and c(fq|ei),
adding if they already exist.
Then in the probability estimation step, the nor-
malization is performed including the newly added
fractional counts.
</bodyText>
<subsectionHeader confidence="0.999966">
3.2 Parallelization of search
</subsectionHeader>
<bodyText confidence="0.999970615384615">
In an iteration of the algorithm, all words are pro-
cessed in random order, computing for each word
the posterior probability of the generative model
after each possible binary segmentation (splitting)
of the word. If the highest-scoring split increases
the posterior probability compared to not splitting,
that split is accepted (for all occurrences of the
word) and the resulting sub-words are explored re-
cursively for further segmentations. The process is
repeated until an iteration no more results in a sig-
nificant increase in the posterior probability.
The search algorithm of Morfessor is a greedy
algorithm where the costs of the next search points
</bodyText>
<equation confidence="0.990126">
�m l
P(f |e) = (P+m|)) t(fj |ei), (3)
j=1 i=0
</equation>
<page confidence="0.996982">
33
</page>
<figureCaption confidence="0.961656">
Figure 1: BLEU scores obtained with different
segmentation methods. Multiple data points for
a system correspond to different random orders in
processing the data (Creutz and Lagus, 2007).
</figureCaption>
<bodyText confidence="0.999901833333333">
are affected by the decision in the current step.
This leads to a sequential search and does not lend
itself to parallelization.
We propose a slightly modified search proce-
dure, where the segmentation decisions are stored
but not applied until the end of an iteration. In
this way, the cost calculations (which is the most
time-consuming component) can all be performed
independently and in parallel. Since the model is
not updated at every decision, the search path can
differ from that in the sequential greedy search and
hence result in different segmentations.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99935275">
We performed in vivo testing of the segmenta-
tion algorithm on the Turkish side of a Turkish-
to-English task. We compared the segmenta-
tions produced by Morfessor, Morfessor modi-
fied for parallel search (Morfessor-p), and Mor-
fessor with bilingual cost (Morfessor-bi) against
the word-based performance. We used the ATR
Basic Travel Expression Corpus (BTEC) (Kikui
et al., 2006), which contains travel conversa-
tion sentences similar to those in phrase-books
for tourists traveling abroad. The training cor-
pus contained 19,972 sentences with average sen-
tence length 5.6 and 7.7 words for Turkish and
English, respectively. The test corpus consisted
of 1,512 sentences with 16 reference translations.
We used GIZA++ (Och and Ney, 2003) for post-
segmentation token alignments and the Moses
toolkit (Koehn et al., 2007) with default param-
eters for phrase-based translation model genera-
tion and decoding. Target language models were
</bodyText>
<figureCaption confidence="0.993480333333333">
Figure 2: Cost-BLEU plots of Morfessor and
Morfessor-bi. Correlation coefficients are -0.005
and -0.279, respectively.
</figureCaption>
<bodyText confidence="0.867170685714286">
trained on the English side of the training cor-
pus using the SRILM toolkit (Stolcke, 2002). The
BLEU metric (Papineni et al., 2002) was used for
translation evaluation.
Figure 1 compares the translation performance
obtained using the described segmentation meth-
ods. All segmentation methods generally im-
prove the translation performance (Morfessor and
Morfessor-p) compared to the word-based models.
However, Morfessor-bi, which utilizes both sides
of the parallel corpus in segmenting, does not con-
vincingly outperform the monolingual methods.
In order to investigate whether the proposed
bilingual segmentation cost correlates any better
than the monolingual segmentation cost of Mor-
fessor, we show several cost-BLEU pairs obtained
from the final and intermediate segmentations of
Morfessor and Morfessor-bi in Fig. 2. The cor-
relation coefficients show that the proposed bilin-
gual metric is somewhat predictive of the trans-
lation performance as measured by BLEU, while
the monolingual Morfessor cost metric has almost
no correlation. Yet, the strong noise in the BLEU
scores (vertical variation in Fig. 2) diminishes the
effect of this correlation, which explains the incon-
sistency of the results in Fig. 1. Indeed, in our ex-
periments even though the total cost kept decreas-
ing at each iteration of the search algorithm, the
BLEU scores obtained by those intermediate seg-
mentations fluctuated without any consistent im-
provement.
Table 2 displays sample segmentations pro-
duced by both the monolingual and bilingual seg-
mentation algorithms. We can observe that uti-
lizing the English side of the corpus enabled
</bodyText>
<page confidence="0.997178">
34
</page>
<table confidence="0.992046272727273">
Count Morfessor Morfessor-bi English Gloss
7 anahtar anahtar (the) key
anahtar + imi anahtar + imi
anahtarla anahtar + la
anahtari anahtar + i
anahtari + m anahtar + im
anahtari + n anahtar + in
anahtari + niz anahtar + iniz
anahtari + ni anahtar + ini
anahtar + inizi anahtar + inizi
6 my key (ACC.)
5 with (the) key
4 &apos;(the) key (ACC.); 2his/her key
3 my key
3 &apos;your key; 2of (the) key
1 your (pl.) key
1 &apos;your key (ACC.); 2his/her key (ACC.)
1 your (pl.) key (ACC.)
1 oyun + lar oyunlar (the) games
2 oyun + lari oyunlar + i &apos;(the) games (ACC.); 2his/her games; 3their game(s)
1 oyun + larin oyunlar + i + n &apos;of (the) games; 2your games
1 oyun + larinizi oyunlar + i + n + izi your (pl.) games (ACC.)
</table>
<tableCaption confidence="0.994041">
Table 2: Sample segmentations produced by Morfessor and Morfessor-bi
</tableCaption>
<bodyText confidence="0.965851333333333">
Morfessor-bi: (i) to consistently identify the root
word “anahtar” (top portion), and (ii) to match the
English plural word form “games” with the Turk-
ish plural word form “oyunlar” (bottom portion).
Monolingual Morfessor is unaware of the target
segmentation, and hence it is up to the subsequent
translation model training to learn that “oyun” is
sometimes translated as “game” and sometimes as
“games” in the segmented training corpus.
</bodyText>
<sectionHeader confidence="0.993861" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999934743589744">
We have presented a method for determining opti-
mal sub-word translation units automatically from
a parallel corpus. We have also showed a method
of incrementally computing the first iteration pa-
rameters of IBM Model-1 between segmentation
hypotheses. Being language-independent, the pro-
posed algorithm can be added as a one-time pre-
processing step prior to training in a SMT system
without requiring any additional data/linguistic re-
sources. The initial experiments presented here
show that the translation units learned by the
proposed algorithm improves on the word-based
baseline in both translation directions.
One avenue for future work is to relax some of
the several independence assumptions made in the
generative model. For example, independence of
consecutive morphs could be relaxed by an HMM
model for transitions between morphs (Creutz and
Lagus, 2007). Other future work includes optimiz-
ing the segmentation of both sides of the corpus
and experimenting with other language pairs.
It is also possible that the probability distribu-
tions are not discriminative enough to outweigh
the model prior tendencies since the translation
probabilities are estimated only crudely (single it-
eration of Model-1 EM algorithm). A possible
candidate solution would be to weigh the transla-
tion likelihood more in calculating the overall cost.
In fact, this idea could be generalized into a log-
linear modeling (e.g., (Poon et al., 2009)) of the
various components of the joint corpus likelihood
and possibly other features.
Finally, integration of sub-word segmentation
with the phrasal lexicon learning process in SMT
is desireable (e.g., translation-driven segmenta-
tion in Wu (1997)). Hierarchical models (Chiang,
2007) could cover this gap and provide a means to
seamlessly integrate sub-word segmentation with
statistical machine translation.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999684">
The authors would like to thank Murat Sarac¸lar
for valuable discussions and guidance in this work,
and the anonymous reviewers for very useful com-
ments and suggestions. Murat Sarac¸lar is sup-
ported by the T ¨UBA-GEB˙IP award.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99465125">
Arianna Bisazza and Marcello Federico. 2009. Mor-
phological Pre-Processing for Turkish to English
Statistical Machine Translation. In Proc. of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 129–135, Tokyo, Japan.
M.R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1):71–105.
</reference>
<page confidence="0.993093">
35
</page>
<reference confidence="0.999805696428571">
P.F. Brown, V.J. Della Pietra, S.A. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263–311.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224–232, Columbus, Ohio.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learn-
ing. ACM Transactions on Speech and Language
Processing, 4(1):1–34.
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In Proceedings of HLT-NAACL,
pages 155–163, Rochester, New York.
˙Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006.
Initial explorations in English to Turkish statistical
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation, pages 7–
14, New York City, New York, USA.
Jakob Elming, Nizar Habash, and Josep M. Crego.
2009. Combination of statistical word alignments
based on multiple preprocessing schemes. In Cyrill
Goutte, Nicola Cancedda, Marc Dymetman, and
George Foster, editors, Learning Machine Transla-
tion, chapter 5, pages 93–110. MIT Press.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proc. of the HLT-NAACL, Companion Volume:
Short Papers, pages 49–52, New York City, USA.
G. Kikui, S. Yamamoto, T. Takezawa, and E. Sumita.
2006. Comparative study on corpora for speech
translation. IEEE Transactions on Audio, Speech
and Language Processing, 14(5):1674–1682.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Companion
Volume: Proceedings of the Demo and Poster Ses-
sions, pages 177–180, Prague, Czech Republic.
M. Kurimo, S. Virpioja, V.T. Turunen, G.W. Black-
wood, and W. Byrne. 2009. Overview and Results
of Morpho Challenge 2009. In Working notes of the
CLEF workshop.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL, Companion Volume: Short Papers, pages
57–60, Boston, Massachusetts, USA.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What’s the
link? In Proceedings of the 7th Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-06), pages 90–99.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Kemal Oflazer and ˙Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 25–32, Prague,
Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of HLT-
NAACL, pages 209–217, Boulder, Colorado.
Fatiha Sadat and Nizar Habash. 2006. Combination
of Arabic preprocessing schemes for statistical ma-
chine translation. In Proc. of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1–8, Sydney, Australia.
Wade Shen, Brian Delaney, and Tim Anderson. 2007.
The MIT-LL/AFRL IWSLT-2007 MT system. In
Proc. of the International Workshop on Spoken Lan-
guage Translation, Trento, Italy.
Benjamin Snyder and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics: HLT, pages 737–745, Columbus, Ohio.
A. Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing, volume 3.
David Talbot and Miles Osborne. 2006. Modelling
lexical redundancy for machine translation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 969–976, Sydney, Australia.
S. Virpioja, J.J. V¨ayrynen, M. Creutz, and M. Sade-
niemi. 2007. Morphology-aware statistical machine
translation based on morphs induced in an unsuper-
vised manner. In Machine Translation Summit XI,
pages 491–498, Copenhagen, Denmark.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–403.
</reference>
<page confidence="0.998943">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.541996">
<title confidence="0.9643425">Unsupervised Search for The Optimal Segmentation for Statistical Machine Translation</title>
<affiliation confidence="0.861715">University, Bebek, Istanbul, Turkey Technical University, Sarıyer, Istanbul, Turkey</affiliation>
<address confidence="0.998862">Gebze, Kocaeli, Turkey</address>
<abstract confidence="0.996391428571428">We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>Morphological Pre-Processing for Turkish to English Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<pages>129--135</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="1894" citStr="Bisazza and Federico, 2009" startWordPosition="251" endWordPosition="254">t enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU. 1 Introduction In statistical machine translation (SMT), words are normally considered as the building blocks of translation models. However, especially for morphologically complex languages such as Finnish, Turkish, Czech, Arabic etc., it has been shown that using sub-lexical units obtained after morphological preprocessing can improve the machine translation performance over a word-based system (Habash and Sadat, 2006; Oflazer and Durgar El-Kahlout, 2007; Bisazza and Federico, 2009). However, the effect of segmentation on translation performance is indirect and difficult to isolate (Lopez and Resnik, 2006). The challenge in designing a sub-lexical SMT system is the decision of what segmentation to use. Linguistic morphological analysis is intuitive, but it is language-dependent and could be highly ambiguous. Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based </context>
<context position="4568" citStr="Bisazza and Federico (2009)" startWordPosition="663" endWordPosition="666">tational Linguistics pare the resulting translation performances in an Arabic-to-English task. Shen et al. (2007) use a subset of the morphology and apply only a few simple rules in segmenting words. Durgar ElKahlout and Oflazer (2006) tackle this problem when translating from English to Turkish, an agglutinative language. They use a morphological analyzer and disambiguation to arrive at morphemes as tokens. However, training the translation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selective morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkishto-English translation setting. Unsupervised segmentation by itself has garnered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a c</context>
</contexts>
<marker>Bisazza, Federico, 2009</marker>
<rawString>Arianna Bisazza and Marcello Federico. 2009. Morphological Pre-Processing for Turkish to English Statistical Machine Translation. In Proc. of the International Workshop on Spoken Language Translation, pages 129–135, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="4913" citStr="Brent, 1999" startWordPosition="715" endWordPosition="716">nd disambiguation to arrive at morphemes as tokens. However, training the translation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selective morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkishto-English translation setting. Unsupervised segmentation by itself has garnered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a consistent improvement in results. Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only monolingual information in its objective function. It is conceivable that we can achieve a better segmentation for translation by considering not one but both sides of the parallel corpus. A posssible choice is the p</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M.R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34(1):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9372" citStr="Brown et al., 1993" startWordPosition="1431" endWordPosition="1434">n a higher alignment probability P(e|f) to Seg. #2 than Seg. #1, because of the aligned words key||anahtar, therefore ranking Seg. #2 higher. The two monolingual components of Eq. 2 are computed as in Creutz and Lagus (2007). To summarize briefly, the prior P(Mf) is assumed to only depend on the frequencies and lengths of the individual morphs, which are also assumed to be independent. The monolingual likelihood P(f|Mf) is computed as the product of morph probabilities estimated from their frequencies in the corpus. To compute the bilingual (translation) likelihood P(e|f), we use IBM Model 1 (Brown et al., 1993). Let an aligned sentence pair be represented by (se, sf), which consists of word sequences se = e1,..., el and sf = f1, ..., fm. Using a purely notational switch of the corpus labels from here on to be consistent with the SMT literature, where the derivations are in the form of P(f|e), the desired translation probability is given by the expression: The sentence length probability distribution P(m|e) is assumed to be Poisson with the expected sentence length equal to m. 3.1 Incremental computation of Model-1 likelihood During search, the translation likelihood P(e|f) needs to be calculated acc</context>
<context position="10908" citStr="Brown et al., 1993" startWordPosition="1681" endWordPosition="1684">ning the EM algorithm to optimization for each considered segmentation model can be computationally expensive, and can result in overtraining. Therefore, in this work we used the likelihood computed after the first EM iteration, which also has the nice property that P(f|e) can be computed incrementally from one segmentation hypothesis to the next. The incremental updates are derived from the equations for the count collection and probability estimation steps of the EM algorithm as follows. In the count collection step, in the first iteration, we need to compute the fractional counts c(fj|ei) (Brown et al., 1993): 1 c(fj|ei) = l + 1(#fj)(#ei), (4) where (#fj) and (#ei) denote the number of occurrences of fj in sf and ei in se, respectively. Let fk denote the word hypothesized to be segmented. Let the resulting two sub-words be fp and fq, any of which may or may not previously exist in the vocabulary. Then, according to Eq. (4), as a result of the segmentation no update is needed for c(fj|ei) for j = 1... N, j =6 p, q, i = 1... M (note that fk no longer exists); and the necessary updates Δc(fj|ei) for c(fj|ei), where j = p, q; i = 1... M are given by: 1 Δc(fj|ei) = l + 1(#fk)(#ei). (5) Note that Eq. (5</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, V.J. Della Pietra, S.A. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="5873" citStr="Chang et al. (2008)" startWordPosition="865" endWordPosition="868">any tests (Kurimo et al., 2009), uses only monolingual information in its objective function. It is conceivable that we can achieve a better segmentation for translation by considering not one but both sides of the parallel corpus. A posssible choice is the post-segmentation alignment accuracy. However, Elming et al. (2009) show that optimizing segmentation with respect to alignment error rate (AER) does not improve and even degrades machine translation performance. Snyder and Barzilay (2008) use bilingual information but the segmentation is learned independently from translation modeling. In Chang et al. (2008), the granularity of the Chinese word segmentation is optimized by training SMT systems for several values of a granularity bias parameter and it is found that the value that maximizes translation performance (as measured by BLEU) is different than the value that maximizes segmentation accuracy (as measured by precision and recall). One motivation in morphological preprocessing before translation modeling is “morphology matching” as in Lee (2004) and in the scheme “EN” of Habash and Sadat (2006). In Lee (2004), the goal is to match the lexical granularities of the two languages by starting wit</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Creutz</author>
<author>K Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1263" citStr="Creutz and Lagus, 2007" startWordPosition="164" endWordPosition="167">ity of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU. 1 Introduction In statistical machine translation (SMT), words are normally considered as the building blocks of translation models. However, especially for morphologically complex languages such as Finnish, Turkish, Czech, Arabic etc., it has been shown that using sub-lexical units obtained after morphological preprocessing can improve the machine translation performance over a word-based system (Habash and Sadat, 2006; Oflazer and Durgar El-Kahlout, 20</context>
<context position="3239" citStr="Creutz and Lagus (2007)" startWordPosition="461" endWordPosition="464">the translation task at hand. Existing solutions to this problem are predominantly heuristic, language-dependent, and as such are not easily portable to other languages. Another point to consider is that the optimal degree of segmentation might decrease as the amount of training data increases (Lee, 2004; Habash and Sadat, 2006). This brings into question: For the particular language pair and training corpus at hand, what is the optimal (level of) subword segmentation? Therefore, it is desirable to learn the optimal segmentation in an unsupervised manner. In this work, we extend the method of Creutz and Lagus (2007) so as to maximize the translation posterior in unsupervised segmentation. The learning process is tailored to the particular SMT task via the same parallel corpus that is used in training the statistical translation models. 2 Related Work Most works in SMT-oriented segmentation are supervised in that they consist of manual experimentation to choose the best among a set of segmentation schemes, and are language(pair)-dependent. For Arabic, Sadat and Habash (2006) present several morphological preprocessing schemes that entail varying degrees of decomposition and com31 Proceedings of the ACL 20</context>
<context position="4899" citStr="Creutz and Lagus, 2007" startWordPosition="711" endWordPosition="714">morphological analyzer and disambiguation to arrive at morphemes as tokens. However, training the translation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selective morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkishto-English translation setting. Unsupervised segmentation by itself has garnered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a consistent improvement in results. Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only monolingual information in its objective function. It is conceivable that we can achieve a better segmentation for translation by considering not one but both sides of the parallel corpus. A posssible c</context>
<context position="7495" citStr="Creutz and Lagus, 2007" startWordPosition="1117" endWordPosition="1120">ecial case of morphology matching by identifying redundant distinctions in the morphology of one language compared to another. 3 Method Maximizing translation performance directly would require SMT training and decoding for each segmentation hypothesis considered, which is computationally infeasible. So we make some conditional independence assumptions using a generative model and decompose the posterior probability P(Mf|e, f). In this notation e and f denote the two sides of a parallel corpus and Mf denotes the segmentation model hypothesized for f. Our approach is an extension of Morfessor (Creutz and Lagus, 2007) so as to include the translation model probability in its cost calculation. Specifically, the segmentation model takes into account the likelihood of both sides of the parallel corpus while searching for the optimal segmentation. The joint likelihood is decomposed into a prior, a monolingual likelihood, and a translation likelihood, as shown in Eq. 1. P(e, f, Mf) = P(Mf)P(f|Mf)P(e|f, Mf) (1) Assuming conditional independence between e and Mf given f, the maximum a posteriori (MAP) objective can be written as: ˆMf = arg max P(Mf)P(f|Mf)P(e|f) (2) Mf The role of the bilingual component P(e|f) i</context>
<context position="8977" citStr="Creutz and Lagus (2007)" startWordPosition="1365" endWordPosition="1368"> of the corpus, a monolingual segmenter can quite possibly score Seg. #1 32 Phrase #1 Phrase #2 Turkish phrase: anahtar anahtarım English phrase: key my key Seg. #1: anahtar anahtarı +m Seg. #2: anahtar anahtar +ım Table 1: Example segmentation hypotheses higher than Seg. #2 (e.g., due to the high frequency of the observed morph “+m”). On the other hand, a bilingual segmenter is expected to assign a higher alignment probability P(e|f) to Seg. #2 than Seg. #1, because of the aligned words key||anahtar, therefore ranking Seg. #2 higher. The two monolingual components of Eq. 2 are computed as in Creutz and Lagus (2007). To summarize briefly, the prior P(Mf) is assumed to only depend on the frequencies and lengths of the individual morphs, which are also assumed to be independent. The monolingual likelihood P(f|Mf) is computed as the product of morph probabilities estimated from their frequencies in the corpus. To compute the bilingual (translation) likelihood P(e|f), we use IBM Model 1 (Brown et al., 1993). Let an aligned sentence pair be represented by (se, sf), which consists of word sequences se = e1,..., el and sf = f1, ..., fm. Using a purely notational switch of the corpus labels from here on to be co</context>
<context position="12761" citStr="Creutz and Lagus, 2007" startWordPosition="2005" endWordPosition="2008">obability compared to not splitting, that split is accepted (for all occurrences of the word) and the resulting sub-words are explored recursively for further segmentations. The process is repeated until an iteration no more results in a significant increase in the posterior probability. The search algorithm of Morfessor is a greedy algorithm where the costs of the next search points �m l P(f |e) = (P+m|)) t(fj |ei), (3) j=1 i=0 33 Figure 1: BLEU scores obtained with different segmentation methods. Multiple data points for a system correspond to different random orders in processing the data (Creutz and Lagus, 2007). are affected by the decision in the current step. This leads to a sequential search and does not lend itself to parallelization. We propose a slightly modified search procedure, where the segmentation decisions are stored but not applied until the end of an iteration. In this way, the cost calculations (which is the most time-consuming component) can all be performed independently and in parallel. Since the model is not updated at every decision, the search path can differ from that in the sequential greedy search and hence result in different segmentations. 4 Results We performed in vivo te</context>
<context position="18082" citStr="Creutz and Lagus, 2007" startWordPosition="2853" endWordPosition="2856">otheses. Being language-independent, the proposed algorithm can be added as a one-time preprocessing step prior to training in a SMT system without requiring any additional data/linguistic resources. The initial experiments presented here show that the translation units learned by the proposed algorithm improves on the word-based baseline in both translation directions. One avenue for future work is to relax some of the several independence assumptions made in the generative model. For example, independence of consecutive morphs could be relaxed by an HMM model for transitions between morphs (Creutz and Lagus, 2007). Other future work includes optimizing the segmentation of both sides of the corpus and experimenting with other language pairs. It is also possible that the probability distributions are not discriminative enough to outweigh the model prior tendencies since the translation probabilities are estimated only crudely (single iteration of Model-1 EM algorithm). A possible candidate solution would be to weigh the translation likelihood more in calculating the overall cost. In fact, this idea could be generalized into a loglinear modeling (e.g., (Poon et al., 2009)) of the various components of the</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>M. Creutz and K. Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Highperformance, language-independent morphological segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>155--163</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="4875" citStr="Dasgupta and Ng, 2007" startWordPosition="707" endWordPosition="710">e language. They use a morphological analyzer and disambiguation to arrive at morphemes as tokens. However, training the translation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selective morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkishto-English translation setting. Unsupervised segmentation by itself has garnered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a consistent improvement in results. Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only monolingual information in its objective function. It is conceivable that we can achieve a better segmentation for translation by considering not one but both sides of the parall</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng. 2007. Highperformance, language-independent morphological segmentation. In Proceedings of HLT-NAACL, pages 155–163, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>˙Ilknur Durgar El-Kahlout</author>
<author>Kemal Oflazer</author>
</authors>
<title>Initial explorations in English to Turkish statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation, pages 7– 14,</booktitle>
<location>New York City, New York, USA.</location>
<contexts>
<context position="2545" citStr="El-Kahlout and Oflazer, 2006" startWordPosition="348" endWordPosition="351">of segmentation on translation performance is indirect and difficult to isolate (Lopez and Resnik, 2006). The challenge in designing a sub-lexical SMT system is the decision of what segmentation to use. Linguistic morphological analysis is intuitive, but it is language-dependent and could be highly ambiguous. Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). A SMT system designer has to decide what segmentation is optimal for the translation task at hand. Existing solutions to this problem are predominantly heuristic, language-dependent, and as such are not easily portable to other languages. Another point to consider is that the optimal degree of segmentation might decrease as the amount of training data increases (Lee, 2004; Habash and Sadat, 2006). This brings into question: For the particular language pair and training corpus at hand, what is the optimal (level of) subword segmentation? Therefore, it is desirable to learn the optimal segment</context>
</contexts>
<marker>El-Kahlout, Oflazer, 2006</marker>
<rawString>˙Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Initial explorations in English to Turkish statistical machine translation. In Proceedings of the Workshop on Statistical Machine Translation, pages 7– 14, New York City, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Nizar Habash</author>
<author>Josep M Crego</author>
</authors>
<title>Combination of statistical word alignments based on multiple preprocessing schemes.</title>
<date>2009</date>
<booktitle>Learning Machine Translation, chapter 5,</booktitle>
<pages>93--110</pages>
<editor>In Cyrill Goutte, Nicola Cancedda, Marc Dymetman, and George Foster, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5579" citStr="Elming et al. (2009)" startWordPosition="820" endWordPosition="823">in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a consistent improvement in results. Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only monolingual information in its objective function. It is conceivable that we can achieve a better segmentation for translation by considering not one but both sides of the parallel corpus. A posssible choice is the post-segmentation alignment accuracy. However, Elming et al. (2009) show that optimizing segmentation with respect to alignment error rate (AER) does not improve and even degrades machine translation performance. Snyder and Barzilay (2008) use bilingual information but the segmentation is learned independently from translation modeling. In Chang et al. (2008), the granularity of the Chinese word segmentation is optimized by training SMT systems for several values of a granularity bias parameter and it is found that the value that maximizes translation performance (as measured by BLEU) is different than the value that maximizes segmentation accuracy (as measur</context>
</contexts>
<marker>Elming, Habash, Crego, 2009</marker>
<rawString>Jakob Elming, Nizar Habash, and Josep M. Crego. 2009. Combination of statistical word alignments based on multiple preprocessing schemes. In Cyrill Goutte, Nicola Cancedda, Marc Dymetman, and George Foster, editors, Learning Machine Translation, chapter 5, pages 93–110. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the HLT-NAACL, Companion Volume: Short Papers,</booktitle>
<pages>49--52</pages>
<location>New York City, USA.</location>
<contexts>
<context position="1828" citStr="Habash and Sadat, 2006" startWordPosition="242" endWordPosition="245">al search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU. 1 Introduction In statistical machine translation (SMT), words are normally considered as the building blocks of translation models. However, especially for morphologically complex languages such as Finnish, Turkish, Czech, Arabic etc., it has been shown that using sub-lexical units obtained after morphological preprocessing can improve the machine translation performance over a word-based system (Habash and Sadat, 2006; Oflazer and Durgar El-Kahlout, 2007; Bisazza and Federico, 2009). However, the effect of segmentation on translation performance is indirect and difficult to isolate (Lopez and Resnik, 2006). The challenge in designing a sub-lexical SMT system is the decision of what segmentation to use. Linguistic morphological analysis is intuitive, but it is language-dependent and could be highly ambiguous. Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), a</context>
<context position="6373" citStr="Habash and Sadat (2006)" startWordPosition="946" endWordPosition="949">) use bilingual information but the segmentation is learned independently from translation modeling. In Chang et al. (2008), the granularity of the Chinese word segmentation is optimized by training SMT systems for several values of a granularity bias parameter and it is found that the value that maximizes translation performance (as measured by BLEU) is different than the value that maximizes segmentation accuracy (as measured by precision and recall). One motivation in morphological preprocessing before translation modeling is “morphology matching” as in Lee (2004) and in the scheme “EN” of Habash and Sadat (2006). In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained segmentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes using alignments with a part-of-speech tagged English corpus. But this method is not completely unsupervised since it requires external linguistic resources in initializing the segmentation with the output of a morphological analyzer and disambiguator. Talbot and Osborne (2006) tackle a special case of morphology matching by identifying redundant distinctions in the morphology of one lang</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proc. of the HLT-NAACL, Companion Volume: Short Papers, pages 49–52, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kikui</author>
<author>S Yamamoto</author>
<author>T Takezawa</author>
<author>E Sumita</author>
</authors>
<title>Comparative study on corpora for speech translation.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="13712" citStr="Kikui et al., 2006" startWordPosition="2157" endWordPosition="2160">nsuming component) can all be performed independently and in parallel. Since the model is not updated at every decision, the search path can differ from that in the sequential greedy search and hence result in different segmentations. 4 Results We performed in vivo testing of the segmentation algorithm on the Turkish side of a Turkishto-English task. We compared the segmentations produced by Morfessor, Morfessor modified for parallel search (Morfessor-p), and Morfessor with bilingual cost (Morfessor-bi) against the word-based performance. We used the ATR Basic Travel Expression Corpus (BTEC) (Kikui et al., 2006), which contains travel conversation sentences similar to those in phrase-books for tourists traveling abroad. The training corpus contained 19,972 sentences with average sentence length 5.6 and 7.7 words for Turkish and English, respectively. The test corpus consisted of 1,512 sentences with 16 reference translations. We used GIZA++ (Och and Ney, 2003) for postsegmentation token alignments and the Moses toolkit (Koehn et al., 2007) with default parameters for phrase-based translation model generation and decoding. Target language models were Figure 2: Cost-BLEU plots of Morfessor and Morfesso</context>
</contexts>
<marker>Kikui, Yamamoto, Takezawa, Sumita, 2006</marker>
<rawString>G. Kikui, S. Yamamoto, T. Takezawa, and E. Sumita. 2006. Comparative study on corpora for speech translation. IEEE Transactions on Audio, Speech and Language Processing, 14(5):1674–1682.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Companion Volume: Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="14148" citStr="Koehn et al., 2007" startWordPosition="2224" endWordPosition="2227">l search (Morfessor-p), and Morfessor with bilingual cost (Morfessor-bi) against the word-based performance. We used the ATR Basic Travel Expression Corpus (BTEC) (Kikui et al., 2006), which contains travel conversation sentences similar to those in phrase-books for tourists traveling abroad. The training corpus contained 19,972 sentences with average sentence length 5.6 and 7.7 words for Turkish and English, respectively. The test corpus consisted of 1,512 sentences with 16 reference translations. We used GIZA++ (Och and Ney, 2003) for postsegmentation token alignments and the Moses toolkit (Koehn et al., 2007) with default parameters for phrase-based translation model generation and decoding. Target language models were Figure 2: Cost-BLEU plots of Morfessor and Morfessor-bi. Correlation coefficients are -0.005 and -0.279, respectively. trained on the English side of the training corpus using the SRILM toolkit (Stolcke, 2002). The BLEU metric (Papineni et al., 2002) was used for translation evaluation. Figure 1 compares the translation performance obtained using the described segmentation methods. All segmentation methods generally improve the translation performance (Morfessor and Morfessor-p) com</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Companion Volume: Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kurimo</author>
<author>S Virpioja</author>
<author>V T Turunen</author>
<author>G W Blackwood</author>
<author>W Byrne</author>
</authors>
<title>Overview and Results of Morpho Challenge</title>
<date>2009</date>
<booktitle>In Working notes of the CLEF workshop.</booktitle>
<contexts>
<context position="5285" citStr="Kurimo et al., 2009" startWordPosition="773" endWordPosition="776">lish translation setting. Unsupervised segmentation by itself has garnered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a consistent improvement in results. Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only monolingual information in its objective function. It is conceivable that we can achieve a better segmentation for translation by considering not one but both sides of the parallel corpus. A posssible choice is the post-segmentation alignment accuracy. However, Elming et al. (2009) show that optimizing segmentation with respect to alignment error rate (AER) does not improve and even degrades machine translation performance. Snyder and Barzilay (2008) use bilingual information but the segmentation is learned independently from translation modeling. In Chang et al. (2008), the granul</context>
</contexts>
<marker>Kurimo, Virpioja, Turunen, Blackwood, Byrne, 2009</marker>
<rawString>M. Kurimo, S. Virpioja, V.T. Turunen, G.W. Blackwood, and W. Byrne. 2009. Overview and Results of Morpho Challenge 2009. In Working notes of the CLEF workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="2921" citStr="Lee, 2004" startWordPosition="410" endWordPosition="411">on schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). A SMT system designer has to decide what segmentation is optimal for the translation task at hand. Existing solutions to this problem are predominantly heuristic, language-dependent, and as such are not easily portable to other languages. Another point to consider is that the optimal degree of segmentation might decrease as the amount of training data increases (Lee, 2004; Habash and Sadat, 2006). This brings into question: For the particular language pair and training corpus at hand, what is the optimal (level of) subword segmentation? Therefore, it is desirable to learn the optimal segmentation in an unsupervised manner. In this work, we extend the method of Creutz and Lagus (2007) so as to maximize the translation posterior in unsupervised segmentation. The learning process is tailored to the particular SMT task via the same parallel corpus that is used in training the statistical translation models. 2 Related Work Most works in SMT-oriented segmentation ar</context>
<context position="6323" citStr="Lee (2004)" startWordPosition="938" endWordPosition="939">erformance. Snyder and Barzilay (2008) use bilingual information but the segmentation is learned independently from translation modeling. In Chang et al. (2008), the granularity of the Chinese word segmentation is optimized by training SMT systems for several values of a granularity bias parameter and it is found that the value that maximizes translation performance (as measured by BLEU) is different than the value that maximizes segmentation accuracy (as measured by precision and recall). One motivation in morphological preprocessing before translation modeling is “morphology matching” as in Lee (2004) and in the scheme “EN” of Habash and Sadat (2006). In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained segmentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes using alignments with a part-of-speech tagged English corpus. But this method is not completely unsupervised since it requires external linguistic resources in initializing the segmentation with the output of a morphological analyzer and disambiguator. Talbot and Osborne (2006) tackle a special case of morphology matching by identifying re</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-Suk Lee. 2004. Morphological analysis for statistical machine translation. In Proceedings of HLTNAACL, Companion Volume: Short Papers, pages 57–60, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Word-based alignment, phrase-based translation: What’s the link?</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-06),</booktitle>
<pages>90--99</pages>
<contexts>
<context position="2020" citStr="Lopez and Resnik, 2006" startWordPosition="270" endWordPosition="273">y BLEU. 1 Introduction In statistical machine translation (SMT), words are normally considered as the building blocks of translation models. However, especially for morphologically complex languages such as Finnish, Turkish, Czech, Arabic etc., it has been shown that using sub-lexical units obtained after morphological preprocessing can improve the machine translation performance over a word-based system (Habash and Sadat, 2006; Oflazer and Durgar El-Kahlout, 2007; Bisazza and Federico, 2009). However, the effect of segmentation on translation performance is indirect and difficult to isolate (Lopez and Resnik, 2006). The challenge in designing a sub-lexical SMT system is the decision of what segmentation to use. Linguistic morphological analysis is intuitive, but it is language-dependent and could be highly ambiguous. Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). A SMT system designer has to decide what segmentation is optimal for the </context>
</contexts>
<marker>Lopez, Resnik, 2006</marker>
<rawString>Adam Lopez and Philip Resnik. 2006. Word-based alignment, phrase-based translation: What’s the link? In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-06), pages 90–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="14067" citStr="Och and Ney, 2003" startWordPosition="2211" endWordPosition="2214">compared the segmentations produced by Morfessor, Morfessor modified for parallel search (Morfessor-p), and Morfessor with bilingual cost (Morfessor-bi) against the word-based performance. We used the ATR Basic Travel Expression Corpus (BTEC) (Kikui et al., 2006), which contains travel conversation sentences similar to those in phrase-books for tourists traveling abroad. The training corpus contained 19,972 sentences with average sentence length 5.6 and 7.7 words for Turkish and English, respectively. The test corpus consisted of 1,512 sentences with 16 reference translations. We used GIZA++ (Och and Ney, 2003) for postsegmentation token alignments and the Moses toolkit (Koehn et al., 2007) with default parameters for phrase-based translation model generation and decoding. Target language models were Figure 2: Cost-BLEU plots of Morfessor and Morfessor-bi. Correlation coefficients are -0.005 and -0.279, respectively. trained on the English side of the training corpus using the SRILM toolkit (Stolcke, 2002). The BLEU metric (Papineni et al., 2002) was used for translation evaluation. Figure 1 compares the translation performance obtained using the described segmentation methods. All segmentation meth</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>˙Ilknur Durgar El-Kahlout</author>
</authors>
<title>Exploring different representational units in English-to-Turkish statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>25--32</pages>
<location>Prague, Czech Republic.</location>
<marker>Oflazer, El-Kahlout, 2007</marker>
<rawString>Kemal Oflazer and ˙Ilknur Durgar El-Kahlout. 2007. Exploring different representational units in English-to-Turkish statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 25–32, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="14511" citStr="Papineni et al., 2002" startWordPosition="2278" endWordPosition="2281">ce length 5.6 and 7.7 words for Turkish and English, respectively. The test corpus consisted of 1,512 sentences with 16 reference translations. We used GIZA++ (Och and Ney, 2003) for postsegmentation token alignments and the Moses toolkit (Koehn et al., 2007) with default parameters for phrase-based translation model generation and decoding. Target language models were Figure 2: Cost-BLEU plots of Morfessor and Morfessor-bi. Correlation coefficients are -0.005 and -0.279, respectively. trained on the English side of the training corpus using the SRILM toolkit (Stolcke, 2002). The BLEU metric (Papineni et al., 2002) was used for translation evaluation. Figure 1 compares the translation performance obtained using the described segmentation methods. All segmentation methods generally improve the translation performance (Morfessor and Morfessor-p) compared to the word-based models. However, Morfessor-bi, which utilizes both sides of the parallel corpus in segmenting, does not convincingly outperform the monolingual methods. In order to investigate whether the proposed bilingual segmentation cost correlates any better than the monolingual segmentation cost of Morfessor, we show several cost-BLEU pairs obtain</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>209--217</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="4825" citStr="Poon et al., 2009" startWordPosition="699" endWordPosition="702">ating from English to Turkish, an agglutinative language. They use a morphological analyzer and disambiguation to arrive at morphemes as tokens. However, training the translation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selective morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkishto-English translation setting. Unsupervised segmentation by itself has garnered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a consistent improvement in results. Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only monolingual information in its objective function. It is conceivable that we can achieve a better segmentation for translation b</context>
<context position="18648" citStr="Poon et al., 2009" startWordPosition="2942" endWordPosition="2945"> transitions between morphs (Creutz and Lagus, 2007). Other future work includes optimizing the segmentation of both sides of the corpus and experimenting with other language pairs. It is also possible that the probability distributions are not discriminative enough to outweigh the model prior tendencies since the translation probabilities are estimated only crudely (single iteration of Model-1 EM algorithm). A possible candidate solution would be to weigh the translation likelihood more in calculating the overall cost. In fact, this idea could be generalized into a loglinear modeling (e.g., (Poon et al., 2009)) of the various components of the joint corpus likelihood and possibly other features. Finally, integration of sub-word segmentation with the phrasal lexicon learning process in SMT is desireable (e.g., translation-driven segmentation in Wu (1997)). Hierarchical models (Chiang, 2007) could cover this gap and provide a means to seamlessly integrate sub-word segmentation with statistical machine translation. Acknowledgements The authors would like to thank Murat Sarac¸lar for valuable discussions and guidance in this work, and the anonymous reviewers for very useful comments and suggestions. Mu</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proceedings of HLTNAACL, pages 209–217, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fatiha Sadat</author>
<author>Nizar Habash</author>
</authors>
<title>Combination of Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3706" citStr="Sadat and Habash (2006)" startWordPosition="535" endWordPosition="538">ation? Therefore, it is desirable to learn the optimal segmentation in an unsupervised manner. In this work, we extend the method of Creutz and Lagus (2007) so as to maximize the translation posterior in unsupervised segmentation. The learning process is tailored to the particular SMT task via the same parallel corpus that is used in training the statistical translation models. 2 Related Work Most works in SMT-oriented segmentation are supervised in that they consist of manual experimentation to choose the best among a set of segmentation schemes, and are language(pair)-dependent. For Arabic, Sadat and Habash (2006) present several morphological preprocessing schemes that entail varying degrees of decomposition and com31 Proceedings of the ACL 2010 Student Research Workshop, pages 31–36, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics pare the resulting translation performances in an Arabic-to-English task. Shen et al. (2007) use a subset of the morphology and apply only a few simple rules in segmenting words. Durgar ElKahlout and Oflazer (2006) tackle this problem when translating from English to Turkish, an agglutinative language. They use a morphological analyzer and di</context>
</contexts>
<marker>Sadat, Habash, 2006</marker>
<rawString>Fatiha Sadat and Nizar Habash. 2006. Combination of Arabic preprocessing schemes for statistical machine translation. In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1–8, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wade Shen</author>
<author>Brian Delaney</author>
<author>Tim Anderson</author>
</authors>
<date>2007</date>
<booktitle>The MIT-LL/AFRL IWSLT-2007 MT system. In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="4054" citStr="Shen et al. (2007)" startWordPosition="584" endWordPosition="587">istical translation models. 2 Related Work Most works in SMT-oriented segmentation are supervised in that they consist of manual experimentation to choose the best among a set of segmentation schemes, and are language(pair)-dependent. For Arabic, Sadat and Habash (2006) present several morphological preprocessing schemes that entail varying degrees of decomposition and com31 Proceedings of the ACL 2010 Student Research Workshop, pages 31–36, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics pare the resulting translation performances in an Arabic-to-English task. Shen et al. (2007) use a subset of the morphology and apply only a few simple rules in segmenting words. Durgar ElKahlout and Oflazer (2006) tackle this problem when translating from English to Turkish, an agglutinative language. They use a morphological analyzer and disambiguation to arrive at morphemes as tokens. However, training the translation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selective morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Tu</context>
</contexts>
<marker>Shen, Delaney, Anderson, 2007</marker>
<rawString>Wade Shen, Brian Delaney, and Tim Anderson. 2007. The MIT-LL/AFRL IWSLT-2007 MT system. In Proc. of the International Workshop on Spoken Language Translation, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: HLT,</booktitle>
<pages>737--745</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="4852" citStr="Snyder and Barzilay, 2008" startWordPosition="703" endWordPosition="706">to Turkish, an agglutinative language. They use a morphological analyzer and disambiguation to arrive at morphemes as tokens. However, training the translation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selective morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkishto-English translation setting. Unsupervised segmentation by itself has garnered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a consistent improvement in results. Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only monolingual information in its objective function. It is conceivable that we can achieve a better segmentation for translation by considering not one but b</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: HLT, pages 737–745, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Seventh International Conference on Spoken Language Processing,</booktitle>
<volume>3</volume>
<contexts>
<context position="14470" citStr="Stolcke, 2002" startWordPosition="2273" endWordPosition="2274">972 sentences with average sentence length 5.6 and 7.7 words for Turkish and English, respectively. The test corpus consisted of 1,512 sentences with 16 reference translations. We used GIZA++ (Och and Ney, 2003) for postsegmentation token alignments and the Moses toolkit (Koehn et al., 2007) with default parameters for phrase-based translation model generation and decoding. Target language models were Figure 2: Cost-BLEU plots of Morfessor and Morfessor-bi. Correlation coefficients are -0.005 and -0.279, respectively. trained on the English side of the training corpus using the SRILM toolkit (Stolcke, 2002). The BLEU metric (Papineni et al., 2002) was used for translation evaluation. Figure 1 compares the translation performance obtained using the described segmentation methods. All segmentation methods generally improve the translation performance (Morfessor and Morfessor-p) compared to the word-based models. However, Morfessor-bi, which utilizes both sides of the parallel corpus in segmenting, does not convincingly outperform the monolingual methods. In order to investigate whether the proposed bilingual segmentation cost correlates any better than the monolingual segmentation cost of Morfesso</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Seventh International Conference on Spoken Language Processing, volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Modelling lexical redundancy for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>969--976</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="6860" citStr="Talbot and Osborne (2006)" startWordPosition="1023" endWordPosition="1026">ical preprocessing before translation modeling is “morphology matching” as in Lee (2004) and in the scheme “EN” of Habash and Sadat (2006). In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained segmentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes using alignments with a part-of-speech tagged English corpus. But this method is not completely unsupervised since it requires external linguistic resources in initializing the segmentation with the output of a morphological analyzer and disambiguator. Talbot and Osborne (2006) tackle a special case of morphology matching by identifying redundant distinctions in the morphology of one language compared to another. 3 Method Maximizing translation performance directly would require SMT training and decoding for each segmentation hypothesis considered, which is computationally infeasible. So we make some conditional independence assumptions using a generative model and decompose the posterior probability P(Mf|e, f). In this notation e and f denote the two sides of a parallel corpus and Mf denotes the segmentation model hypothesized for f. Our approach is an extension of</context>
</contexts>
<marker>Talbot, Osborne, 2006</marker>
<rawString>David Talbot and Miles Osborne. 2006. Modelling lexical redundancy for machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 969–976, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Virpioja</author>
<author>J J V¨ayrynen</author>
<author>M Creutz</author>
<author>M Sadeniemi</author>
</authors>
<title>Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</title>
<date>2007</date>
<booktitle>In Machine Translation Summit XI,</booktitle>
<pages>491--498</pages>
<location>Copenhagen, Denmark.</location>
<marker>Virpioja, V¨ayrynen, Creutz, Sadeniemi, 2007</marker>
<rawString>S. Virpioja, J.J. V¨ayrynen, M. Creutz, and M. Sadeniemi. 2007. Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner. In Machine Translation Summit XI, pages 491–498, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>