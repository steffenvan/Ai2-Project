<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.987827">
Dynamic Programming Algorithms
for Transition-Based Dependency Parsers
</title>
<author confidence="0.997748">
Marco Kuhlmann Carlos Gómez-Rodríguez Giorgio Satta
</author>
<affiliation confidence="0.9961345">
Dept. of Linguistics and Philology Departamento de Computación Dept. of Information Engineering
Uppsala University, Sweden Universidade da Coruña, Spain University of Padua, Italy
</affiliation>
<email confidence="0.989071">
marco.kuhlmann@lingfil.uu.se cgomezr@udc.es satta@dei.unipd.it
</email>
<sectionHeader confidence="0.994621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922666666667">
We develop a general dynamic programming
technique for the tabulation of transition-based
dependency parsers, and apply it to obtain
novel, polynomial-time algorithms for parsing
with the arc-standard and arc-eager models. We
also show how to reverse our technique to ob-
tain new transition-based dependency parsers
from existing tabular methods. Additionally,
we provide a detailed discussion of the con-
ditions under which the feature models com-
monly used in transition-based parsing can be
integrated into our algorithms.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931894736842">
Dynamic programming algorithms, also known as
tabular or chart-based algorithms, are at the core of
many applications in natural language processing.
When applied to formalisms such as context-free
grammar, they provide polynomial-time parsing al-
gorithms and polynomial-space representations of
the resulting parse forests, even in cases where the
size of the search space is exponential in the length
of the input string. In combination with appropri-
ate semirings, these packed representations can be
exploited to compute many values of interest for ma-
chine learning, such as best parses and feature expec-
tations (Goodman, 1999; Li and Eisner, 2009).
In this paper, we follow the line of investigation
started by Huang and Sagae (2010) and apply dy-
namic programming to (projective) transition-based
dependency parsing (Vivre, 2008). The basic idea,
originally developed in the context of push-down
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989), is that while the number of computa-
tions of a transition-based parser may be exponential
in the length of the input string, several portions of
these computations, when appropriately represented,
can be shared. This can be effectively implemented
through dynamic programming, resulting in a packed
representation of the set of all computations.
The contributions of this paper can be summarized
as follows. We provide (declarative specifications of)
novel, polynomial-time algorithms for two widely-
used transition-based parsing models: arc-standard
(Vivre, 2004; Huang and Sagae, 2010) and arc-eager
(Vivre, 2003; Zhang and Clark, 2008). Our algorithm
for the arc-eager model is the first tabular algorithm
for this model that runs in polynomial time. Both
algorithms are derived using the same general tech-
nique; in fact, we show that this technique is applica-
ble to all transition-parsing models whose transitions
can be classified into “shift” and “reduce” transitions.
We also show how to reverse the tabulation to de-
rive a new transition system from an existing tabular
algorithm for dependency parsing, originally devel-
oped by Gómez-Rodríguez et al. (2008). Finally, we
discuss in detail the role of feature information in
our algorithms, and in particular the conditions under
which the feature models traditionally used in transi-
tion-based dependency parsing can be integrated into
our framework.
While our general approach is the same as the one
of Huang and Sagae (2010), we depart from their
framework by not representing the computations of
a parser as a graph-structured stack in the sense of
Tomita (1986). We instead simulate computations
as in Lang (1974), which results in simpler algo-
rithm specifications, and also reveals deep similari-
ties between transition-based systems for dependency
parsing and existing tabular methods for lexicalized
context-free grammars.
</bodyText>
<page confidence="0.990029">
673
</page>
<note confidence="0.986849">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 673–682,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.673578" genericHeader="introduction">
2 Transition-Based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999859666666667">
We start by briefly introducing the framework of
transition-based dependency parsing; for details, we
refer to Nivre (2008).
</bodyText>
<subsectionHeader confidence="0.983938">
2.1 Dependency Graphs
</subsectionHeader>
<bodyText confidence="0.999989909090909">
Let w D w0 • • • w,t_1 be a string over some fixed
alphabet, where n &gt; 1 and w0 is the special token
ROOT. A dependency graph for w is a directed graph
G D (V„, A), where V„ D f0, ... , n — 1g is the set
of nodes, and A C_ V„ x V„ is the set of arcs. Each
node in V„ encodes the position of a token in w, and
each arc in A encodes a dependency relation between
two tokens. To denote an arc (i, j) 2 A, we write
i ! j; here, the node i is the head, and the node j is
the dependent. A sample dependency graph is given
in the left part of Figure 2.
</bodyText>
<subsectionHeader confidence="0.997427">
2.2 Transition Systems
</subsectionHeader>
<bodyText confidence="0.999982821428571">
A transition system is a structure S D (C, T, I, Cr),
where C is a set of configurations, T is a finite set
of transitions, which are partial functions tW C * C,
I is a total initialization function mapping each input
string to a unique initial configuration, and Cr c C
is a set of terminal configurations.
The transition systems that we investigate in this
paper differ from each other only with respect to
their sets of transitions, and are identical in all other
aspects. In each of them, a configuration is de-
fined relative to a string w as above, and is a triple
c D (u, P, A), where u and P are disjoint lists of
nodes from V„, called stack and buffer, respectively,
and A C_ V„ x V„ is a set of arcs. We denote the
stack, buffer and arc set associated with c by u(c),
P(c), and A(c), respectively. We follow a standard
convention and write the stack with its topmost ele-
ment to the right, and the buffer with its first element
to the left; furthermore, we indicate concatenation
in the stack and in the buffer by a vertical bar. The
initialization function maps each string w to the ini-
tial configuration ([], [0, ... , jwj — 1], ;). The set of
terminal configurations contains all configurations of
the form ([0], [], A), where A is some set of arcs.
Given an input string w, a parser based on S pro-
cesses w from left to right, starting in the initial con-
figuration I(w). At each point, it applies one of
the transitions, until at the end it reaches a terminal
</bodyText>
<equation confidence="0.962071">
(u, ijP, A) ` (uji, P, A) (sh)
(ujijj, P, A) ` (ujj, P, A [ fj ! ig) (la)
(ujijj, P, A) ` (uji, P, A [ fi ! j g) (ra)
</equation>
<figureCaption confidence="0.998105">
Figure 1: Transitions in the arc-standard model.
</figureCaption>
<bodyText confidence="0.999864230769231">
configuration; the dependency graph defined by the
arc set associated with that configuration is then re-
turned as the analysis for w. Formally, a computation
of S on w is a sequence y D c0, ... , cm, m &gt; 0, of
configurations (defined relative to w) in which each
configuration is obtained as the value of the preced-
ing one under some transition. It is called complete
whenever c0 D I(w), and cm 2 Cr. We note that a
computation can be uniquely specified by its initial
configuration c0 and the sequence of its transitions,
understood as a string over T . Complete computa-
tions, where c0 is fixed, can be specified by their
transition sequences alone.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="method">
3 Arc-Standard Model
</sectionHeader>
<bodyText confidence="0.9995635">
To introduce the core concepts of the paper, we first
look at a particularly simple model for transition-
based dependency parsing, known as the arc-stan-
dard model. This model has been used, in slightly
different variants, by a number of parsers (Nivre,
2004; Attardi, 2006; Huang and Sagae, 2010).
</bodyText>
<subsectionHeader confidence="0.999805">
3.1 Transition System
</subsectionHeader>
<bodyText confidence="0.999977235294118">
The arc-standard model uses three types of transi-
tions: SHIFT (sh) removes the first node in the buffer
and pushes it to the stack. LEFT-ARc (la) creates a
new arc with the topmost node on the stack as the
head and the second-topmost node as the dependent,
and removes the second-topmost node from the stack.
RIGHT-ARc (ra) is symmetric to LEFT-ARc in that it
creates an arc with the second-topmost node as the
head and the topmost node as the dependent, and
removes the topmost node.
The three transitions can be formally specified as
in Figure 1. The right half of Figure 2 shows a com-
plete computation of the arc-standard transition sys-
tem, specified by its transition sequence. The picture
also shows the contents of the stack over the course of
the computation; more specifically, column i shows
the stack u(ci) associated with the configuration ci.
</bodyText>
<page confidence="0.996693">
674
</page>
<figure confidence="0.997324660377359">
ROOT This news had little effect on the markets
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
sh sh sh la sh la sh sh la sh sh sh la ra ra ra ra
y1 y2
8
7 7
8
5
6
6 6
6
6
2
3
4 4 5
5
5
5
5
5
5
y0
1
1
2
2
3
3
3
3
3
3
3
3
3
3
3
0
0 0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
</figure>
<figureCaption confidence="0.998689">
Figure 2: A dependency tree (left) and a computation generating this tree in the arc-standard system (right).
</figureCaption>
<subsectionHeader confidence="0.999828">
3.2 Push Computations
</subsectionHeader>
<bodyText confidence="0.999988571428572">
The key to the tabulation of transition-based depen-
dency parsers is to find a way to decompose com-
putations into smaller, shareable parts. For the arc-
standard model, as well as for the other transition
systems that we consider in this paper, we base our
decomposition on the concept of push computations.
By this, we mean computations
</bodyText>
<listItem confidence="0.902368555555556">
y= c0,...,cm , m ? 1,
on some input string w with the following properties:
(P1) The initial stack a(c0) is not modified during
the computation, and is not even exposed after the
first transition: For every 1 &lt; i &lt; m, there exists a
non-empty stack ai such that a(ci) = a(c0)Iai.
(P2) The overall effect of the computation is to
push a single node to the stack: The stack a(cm) can
be written as a(cm) = a(c0)Ih, for some h E Vw.
</listItem>
<bodyText confidence="0.999701733333333">
We can verify that the computation in Figure 2 is
a push computation. We can also see that it contains
shorter computations that are push computations; one
example is the computation y0 = c1, ... , c16, whose
overall effect is to push the node 3. In Figure 2, this
computation is marked by the zig-zag path traced
in bold. The dashed line delineates the stack a(c1),
which is not modified during y0.
Every computation that consists of a single sh tran-
sition is a push computation. Starting from these
atoms, we can build larger push computations by
means of two (partial) binary operations fla and fra,
defined as follows. Let y1 = c10, ... , c1m1 and
y2 = c20, . . . , c2m2 be push computations on the
same input string w such that c1m1 = c20. Then
</bodyText>
<equation confidence="0.687151">
fra(y1, y2) = c10, . . . , c1m1, c21, . . . , c2m2, c ,
</equation>
<bodyText confidence="0.998457142857143">
where c is obtained from c2m2 by applying the ra
transition. (The operation fla is defined analogously.)
We can verify that f,a(y1, y2) is another push com-
putation. For instance, with respect to Figure 2,
fra(y1, y2) = y0. Conversely, we say that the push
computation y0 can be decomposed into the subcom-
putations y1 and y2, and the operation fra.
</bodyText>
<subsectionHeader confidence="0.999669">
3.3 Deduction System
</subsectionHeader>
<bodyText confidence="0.990405555555555">
Building on the compositional structure of push com-
putations, we now construct a deduction system (in
the sense of Shieber et al. (1995)) that tabulates the
computations of the arc-standard model for a given
input string w = w0 • • • wn_1. For 0 &lt; i &lt; n, we
shall write Pi to denote the buffer [i, ... , n-1]. Thus,
P0 denotes the full buffer, associated with the initial
configuration I(w), and Pn denotes the empty buffer,
associated with a terminal configuration c E Ct.
Item form. The items of our deduction system
take the form [i, h, j], where 0 &lt; i &lt; h &lt; j &lt; n.
The intended interpretation of an item [i, h, j ] is:
For every configuration c0 with P(c0) = Pi, there
exists a push computation y = c0, ... , cm such that
P(cm) = Pj, and a(cm) = a(c0)Ih.
Goal. The only goal item is [0, 0, n], asserting
that there exists a complete computation for w.
Axioms. For every stack a, position i &lt; n and
arc set A, by a single sh transition we obtain the
push computation (a, Pi, A), (aIi, Pi+1, A). There-
fore we can take the set of all items of the form
[i, i, i + 1] as the axioms of our system.
Inference rules. The inference rules parallel the
composition operations fla and fra. Suppose that
we have deduced the items [i, h1, k] and [k, h2, j ],
where 0 &lt; i &lt; h1 &lt; k &lt; h2 &lt; j &lt; n. The
item [i, h1, k] asserts that for every configuration c10
</bodyText>
<page confidence="0.969226">
675
</page>
<equation confidence="0.81651575">
Item form: Œi; h; j• , 0 &lt; i &lt; h &lt; j &lt; IwI Goal: Œ0; 0; IwI• Axioms: Œi; i; i + 1•
Œi;h1;k• Œk;h2; j• Œi;h1;k• Œk;h2; j•
Inference rules: .la; h2 —+ h1/ .ra; h1 —+ h2/
Œi; h2; j• Œi; h1; j•
</equation>
<figureCaption confidence="0.8849">
Figure 3: Deduction system for the arc-standard model.
</figureCaption>
<bodyText confidence="0.877045125">
with ˇ.c10/ = ˇi, there exists a push computation
y1 = c10; : : : ; c1m1 such that ˇ.c1m1/ = ˇk, and
u.c1m1/ = u.c10/Ih1. Using the item Œk; h2; j•,
we deduce the existence of a second push compu-
tation y2 = c20; ::: ; c2m2 such that c20 = c1m1,
ˇ.c2m2/ = ˇj, and Q.c2m2/ = u.c10/Ih1Ih2. By
means of fra, we can then compose y1 and y2 into a
new push computation
</bodyText>
<equation confidence="0.934563">
fra.y1;y2/ = c10;:::;c1m1;c21;:::;c2m2;c :
</equation>
<bodyText confidence="0.99354825">
Here, ˇ.c/ = ˇj, and a.c/ = a.c10/Ih1. Therefore,
we may generate the item Œi; h1; j •. The inference
rule for la can be derived analogously.
Figure 3 shows the complete deduction system.
</bodyText>
<subsectionHeader confidence="0.995717">
3.4 Completeness and Non-Ambiguity
</subsectionHeader>
<bodyText confidence="0.987534483870968">
We have informally argued that our deduction sys-
tem is sound. To show completeness, we prove the
following lemma: For all 0 &lt; i &lt; h &lt; j &lt; IwI and
every push computation y = c0; ::: ; cm on w with
ˇ.c0/ = ˇi, ˇ.cm/ = ˇj and a.cm/ = a.c0/Ih, the
item Œi; h; j• is generated. The proof is by induction
on m, and there are two cases:
m = 1. In this case, y consists of a single sh transi-
tion, h = i, j = i + 1, and we need to show that the
item Œi; i; i + 1• is generated. This holds because this
item is an axiom.
m &gt; 2. In this case, y ends with either a la or a ra
transition. Let c be the rightmost configuration in y
that is different from cm and whose stack size is one
larger than the size of a.c0/. The computations
y1 = c0;:::;c and y2 = c;:::;cm-1
are both push computations with strictly fewer tran-
sitions than y. Suppose that the last transition in y
is ra. In this case, ˇ.c/ = ˇk for some i &lt; k &lt; j,
u.c/ = u.c0/Ih with h &lt; k, ˇ.cm-1/ = ˇj, and
a.cm-1/ = u.c0/IhIh0 for some k &lt; h0 &lt; j. By
induction, we may assume that we have generated
items Œi; h; k• and Œk; h0; j •. Applying the inference
rule for ra, we deduce the item Œi; h; j•. An analo-
gous argument can be made for fla.
Apart from being sound and complete, our deduc-
tion system also has the property that it assigns at
most one derivation to a given item. To see this,
note that in the proof of the lemma, the choice of c
is uniquely determined: If we take any other con-
figuration c0 that meets the selection criteria, then
</bodyText>
<equation confidence="0.897638">
2 = c&apos;, ... , cm-1 is not a push
</equation>
<bodyText confidence="0.964273333333333">
the computation y
computation, as it contains c as an intermediate con-
figuration, and thereby violates property P1.
</bodyText>
<sectionHeader confidence="0.592325" genericHeader="method">
3.5 Discussion
</sectionHeader>
<bodyText confidence="0.999984352941176">
Let us briefly take stock of what we have achieved
so far. We have provided a deduction system capable
of tabulating the set of all computations of an arc-
standard parser on a given input string, and proved
the correctness of this system relative to an interpre-
tation based on push computations. Inspecting the
system, we can see that its generic implementation
takes space in O.IwI3/ and time in O.IwI5/.
Our deduction system is essentially the same as the
one for the CKY algorithm for bilexicalized context-
free grammar (Collins, 1996; Gómez-Rodríguez et
al., 2008). This equivalence reveals a deep correspon-
dence between the arc-standard model and bilexical-
ized context-free grammar, and, via results by Eisner
and Satta (1999), to head automata. In particular,
Eisner’s and Satta’s “hook trick” can be applied to
our tabulation to reduce its runtime to O.IwI4/.
</bodyText>
<sectionHeader confidence="0.963565" genericHeader="method">
4 Adding Features
</sectionHeader>
<bodyText confidence="0.999858">
The main goal with the tabulation of transition-based
dependency parsers is to obtain a representation
based on which semiring values such as the high-
est-scoring computation for a given input (and with
it, a dependency tree) can be calculated. Such com-
putations involve the use of feature information. In
this section, we discuss how our tabulation of the arc-
standard system can be extended for this purpose.
</bodyText>
<page confidence="0.922055">
676
</page>
<equation confidence="0.995788166666667">
Œi; h1; kI hx2; x1i; hx1; x3i• W v1 Œk; h2; jI hx1; x3i; hx3; x4i• W v2
.ra/
Œi; h1; jI hx2; x1i; hx1; x3i• W v1 C v2 C hx3; x4i &apos; E˛ra
Œi; h; jI hx2; x1i; hx1; x3i• W v
.sh/
Œj; j; j C 1I hx1; x3i; hx3; wj i• W hx1; x3i - E˛sh
</equation>
<figureCaption confidence="0.9082605">
Figure 4: Extended inference rules under the feature model ˚ D hs1:w; s0:wi. The annotations indicate how to calculate
a candidate for an update of the Viterbi score of the conclusion using the Viterbi scores of the premises.
</figureCaption>
<subsectionHeader confidence="0.997926">
4.1 Scoring Computations
</subsectionHeader>
<bodyText confidence="0.997611571428571">
For the sake of concreteness, suppose that we want
to score computations based on the following model,
taken from Zhang and Clark (2008). The score of a
computation y is broken down into a sum of scores
score.t; ct/ for combinations of a transition t in the
transition sequence associated with y and the config-
uration ct in which t was taken:
</bodyText>
<equation confidence="0.9962065">
Xscore.y/ D score.t; ct/ (1)
t2Y
</equation>
<bodyText confidence="0.990930466666667">
The score score.t; ct/ is defined as the dot product of
the feature representation of ct relative to a feature
model ˚ and a transition-specific weight vector E˛t:
score.t; ct/ D ˚.ct/ • E˛t
The feature model ˚ is a vector h01; ::: ; Oni of
elementary feature functions, and the feature rep-
resentation ˚.c/ of a configuration c is a vector
xE D h01.c/; ::: ; On.c/i of atomic values. Two ex-
amples of feature functions are the word form associ-
ated with the topmost and second-topmost node on
the stack; adopting the notation of Huang and Sagae
(2010), we will write these functions as s0:w and
s1:w, respectively. Feature functions like these have
been used in several parsers (Nivre, 2006; Zhang and
Clark, 2008; Huang et al., 2009).
</bodyText>
<subsectionHeader confidence="0.994165">
4.2 Integration of Feature Models
</subsectionHeader>
<bodyText confidence="0.983012695652174">
To integrate feature models into our tabulation of
the arc-standard system, we can use extended items
of the form Œi; h; jI ExL; ExR• with the same intended
interpretation as the old items Œi; h; j •, except that
the initial configuration of the asserted computations
y D c0; ::: ; cm now is required to have the feature
representation ExL, and the final configuration is re-
quired to have the representation ExR:
˚.c0/ D ExL and ˚.cm/ D ExR
We shall refer to the vectors ExL and ExR as the left-
context vector and the right-context vector of the
computation y, respectively.
We now need to change the deduction rules so that
they become faithful to the extended interpretation.
Intuitively speaking, we must ensure that the feature
values can be computed along the inference rules.
As a concrete example, consider the feature model
˚ D hs1:w; s0:wi. In order to integrate this model
into our tabulation, we change the rule for ra as in
Figure 4, where x1; ::: ; x4 range over possible word
forms. The shared variable occurrences in this rule
capture the constraints that hold between the feature
values of the subcomputations y1 and y2 asserted
by the premises, and the computations fra.y1; y2/
asserted by the conclusion. To illustrate this, suppose
that y1 and y2 are as in Figure 2. Then the three
occurrences of x3 for instance encode that
Œs0:w•.c6/ D Œs1:w•.c15/ D Œs0:w•.c16/ D w3 :
We also need to extend the axioms, which cor-
respond to computations consisting of a single sh
transition. The most conservative way to do this is
to use a generate-and-test technique: Extend the ex-
isting axioms by all valid choices of left-context and
right-context vectors, that is, by all pairs ExL; ExR such
that there exists a configuration c with ˚.c/ D ExL
and ˚.sh.c// D ExR. The task of filtering out use-
less guesses can then be delegated to the deduction
system.
A more efficient way is to only have one axiom, for
the case where c D I.w/, and to add to the deduction
system a new, unary inference rule for sh as in Fig-
ure 4. This rule only creates items whose left-context
vector is the right-context vector of some other item,
which prevents the generation of useless items. In
the following, we take this second approach, which
is also the approach of Huang and Sagae (2010).
</bodyText>
<page confidence="0.953079">
677
</page>
<equation confidence="0.994924">
Œi; h; jI hx2; x1i; hx1; x3i• W .p; v/ .sh/ , where 6 D hx1; x3i E˛sh
Œj; j; j C 1I hx1; x3i; hx3; wj i• W .p C 6; 6/
Œi; h1; kI hx2; x1i; hx1; x3i• W .p1; v1/ Œk; h2; jI hx1; x3i; hx3; x4i• W .p2; v2/ .ra/ , where p D hx3; x4i &apos; E˛ra
</equation>
<figureCaption confidence="0.95014">
Figure 5: Extended inference rules under the feature model ˚ D hs0:w; s1:wi. The annotations indicate how to calculate
a candidate for an update of the prefix score and Viterbi score of the conclusion.
</figureCaption>
<bodyText confidence="0.436242">
Œi; h1; jI hx2; x1i; hx1; x3i• W .p1 C v2 C p; v1 C v2 C P/
</bodyText>
<subsectionHeader confidence="0.999292">
4.3 Computing Viterbi Scores
</subsectionHeader>
<bodyText confidence="0.99997475">
Once we have extended our deduction system with
feature information, many values of interest can be
computed. One simple example is the Viterbi score
for an input w, defined as
</bodyText>
<equation confidence="0.9658595">
arg max score.y/; (2)
y2r .w/
</equation>
<bodyText confidence="0.999907888888889">
where F .w/ denotes the set of all complete compu-
tations for w. The score of a complex computation
ft.y1; y2/ is the sum of the scores of its subcomputa-
tions y1; y2, plus the transition-specific dot product.
Since this dot product only depends on the feature
representation of the final configuration of y2, the
Viterbi score can be computed on top of the infer-
ence rules using standard techniques. The crucial
calculation is indicated in Figure 4.
</bodyText>
<subsectionHeader confidence="0.999009">
4.4 Computing Prefix Scores
</subsectionHeader>
<bodyText confidence="0.997074363636364">
Another interesting value is the prefix score of an
item, which, apart from the Viterbi score, also in-
cludes the cost of the best search path leading to
the item. Huang and Sagae (2010) use this quan-
tity to order the items in a beam search on top of
their dynamic programming method. In our frame-
work, prefix scores can be computed as indicated in
Figure 5. Alternatively, we can also use the more
involved calculation employed by Huang and Sagae
(2010), which allows them to get rid of the left-con-
text vector from their items.1
</bodyText>
<subsectionHeader confidence="0.956833">
4.5 Compatibility
</subsectionHeader>
<bodyText confidence="0.99978425">
So far we have restricted our attention to a concrete
and extremely simplistic feature model. The fea-
ture models that are used in practical systems are
considerably more complex, and not all of them are
</bodyText>
<footnote confidence="0.882799333333333">
1The essential idea in the calculation by Huang and Sagae
(2010) is to delegate (in the computation of the Viterbi score)
the scoring of sh transitions to the inference rules for la/ra.
</footnote>
<bodyText confidence="0.99969465625">
compatible with our framework in the sense that they
can be integrated into our deduction system in the
way described in Section 4.2.
For a simple example of a feature model that is
incompatible with our tabulation, consider the model
˚&apos; D hs0:rc:wi, whose single feature function ex-
tracts the word form of the right child (rc) of the
topmost node on the stack. Even if we know the val-
ues of this feature for two computations y1; y2, we
have no way to compute its value for the composed
computation fra.y1; y2/: This value coincides with
the word form of the topmost node on the stack asso-
ciated with y2, but in order to have access to it in the
context of the ra rule, our feature model would need
to also include the feature function s0:w.
The example just given raises the question whether
there is a general criterion based on which we can de-
cide if a given feature model is compatible with our
tabulation. An attempt to provide such a criterion has
been made by Huang and Sagae (2010), who define
a constraint on feature models called “monotonicity”
and claim that this constraint guarantees that feature
values can be computed using their dynamic program-
ming approach. Unfortunately, this claim is wrong.
In particular, the feature model ˚&apos; given above is
“monotonic”, but cannot be tabulated, neither in our
nor in their framework. In general, it seems clear
that the question of compatibility is a question about
the relation between the tabulation and the feature
model, and not about the feature model alone. To find
practically useful characterizations of compatibility
is an interesting avenue for future research.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="method">
5 Arc-Eager Model
</sectionHeader>
<bodyText confidence="0.9997712">
Up to now, we have only discussed the arc-standard
model. In this section, we show that the framework
of push computations also provides a tabulation of
another widely-used model for dependency parsing,
the arc-eager model (Nivre, 2003).
</bodyText>
<page confidence="0.964733">
678
</page>
<equation confidence="0.759119666666667">
(6, ijP, A) ` (6ji, P, A) (sh)
(6ji, j jP, A) ` (6, j jP, A [ fj ! ig) (lae)
only if i does not have an incoming arc
(6ji, j jP, A) ` (6jijj, P, A [ fi ! j g) (rae)
(6ji, P, A) ` (6, P, A) (re)
only if i has an incoming arc
</equation>
<figureCaption confidence="0.995972">
Figure 6: Transitions in the arc-eager model.
</figureCaption>
<subsectionHeader confidence="0.99786">
5.1 Transition System
</subsectionHeader>
<bodyText confidence="0.999991277777778">
The arc-eager model has three types of transitions,
shown in Figure 6: SHIFT (sh) works just like in arc-
standard, moving the first node in the buffer to the
stack. LEFT-ARC (lae) creates a new arc with the first
node in the buffer as the head and the topmost node
on the stack as the dependent, and pops the stack.
It can only be applied if the topmost node on the
stack has not already been assigned a head, so as to
preserve the single-head constraint. RIGHT-ARC (rae)
creates an arc in the opposite direction as LEFT-ARC,
and moves the first node in the buffer to the stack.
Finally, REDUCE (re) simply pops the stack; it can
only be applied if the topmost node on the stack has
already been assigned a head.
Note that, unlike in the case of arc-standard, the
parsing process in the arc-eager model is not bottom-
up: the right dependents of a node are attached before
they have been assigned their own right dependents.
</bodyText>
<subsectionHeader confidence="0.998891">
5.2 Shift-Reduce Parsing
</subsectionHeader>
<bodyText confidence="0.9999914">
If we look at the specification of the transitions of the
arc-standard and the arc-eager model and restrict our
attention to the effect that they have on the stack and
the buffer, then we can see that all seven transitions
fall into one of three types:
</bodyText>
<equation confidence="0.995255666666667">
(6, ijP) ` (6ji, P) sh, rae (T1)
(6jijj, P) ` (6jj, P) la (T2)
(6ji, P) ` (6, P) ra, lae, re (T3)
</equation>
<bodyText confidence="0.999989">
We refer to transitions of type T1 as shift and to
transitions of type T2 and T3 as reduce transitions.
The crucial observation now is that the concept of
push computations and the approach to their tabula-
tion that we have taken for the arc-standard system
can easily be generalized to other transition systems
whose transitions are of the type shift or reduce. In
particular, the proof of the correctness of our de-
duction system that we gave in Section 3 still goes
through if instead of sh we write “shift” and instead
of la and ra we write “reduce”.
</bodyText>
<subsectionHeader confidence="0.999692">
5.3 Deduction System
</subsectionHeader>
<bodyText confidence="0.999967945945946">
Generalizing our construction for the arc-standard
model along these lines, we obtain a tabulation of
the arc-eager model. Just like in the case of arc-
standard, each single shift transition in that model
(be it sh or rae) constitutes a push computation, while
the reduce transitions induce operations fae and fre.
The only difference is that the preconditions of lae
and re must be met. Therefore, fae(y1, y2) is only
defined if the topmost node on the stack in the final
configuration of y2 has not yet been assigned a head,
and fm(y1, y2) is only defined in the opposite case.
Item form. In our deduction system for the arc-ea-
ger model we use items of the form [i, hb, j], where
0&lt; i&lt; h &lt; j&lt; jwj, and b 2 f0, 1g. An item
[i, hb, j ] has the same meaning as the corresponding
item in our deduction system for arc-standard, but
also keeps record of whether the node h has been
assigned a head (b D 1) or not (b D 0).
Goal. The only goal item is [0, 00, jwj]. (The item
[0, 01,jwj] asserts that the node 0 has a head, which
never happens in a complete computation.)
Axioms. Reasoning as in arc-standard, the axioms
of the deduction system for the arc-eager model are
the items of the form [i, i0, i C 1] and [j, j 1, j C 1],
where j &gt; 0: the former correspond to the push
computations obtained from a single sh, the latter to
those obtained from a single rae, which apart from
shifting a node also assigns it a head.
Inference rules. Also analogously to arc-standard,
if we know that there exists a push computation y1
of the form asserted by the item [i, hb, k], and a push
computation y2 of the form asserted by [k, g0, j ],
where j &lt; jwj, then we can build the push compu-
tation fjae(y1, y2) of the form asserted by the item
[i, hb, j ]. Similarly, if y2 is of the form asserted by
[k, g1, j], then we can build fre(y1, y2), which again
is of the form by asserted [i, hb, j ]. Thus:
</bodyText>
<equation confidence="0.604141142857143">
[i, ib, k] [k,k0,j] (lae), [i, ib, k] [k,k1,j] (re).
[i,ib , j] [i,ib , j]
679
Item form: Œib; j • , 0 &lt; i &lt; j &lt; jwj , b 2 f0;1g Goal: Œ00; jwj• Axioms: Œ00; 1•
Œib; j• Œib; k• Œk0; j• Œib;j• Œib;k• Œk1;j•
Œj0;j C 1• .sh/ .laeI j ! k/, j &lt; jwj Œj 1;j C 1• .raeI i ! j/ .re/
Œib;j• Œib; j •
</equation>
<figureCaption confidence="0.99833">
Figure 7: Deduction system for the arc-eager model.
</figureCaption>
<bodyText confidence="0.999508">
As mentioned above, the correctness and non-am-
biguity of the system can be proved as in Section 3.
Features can be added in the same way as discussed
in Section 4.
</bodyText>
<subsectionHeader confidence="0.985012">
5.4 Computational Complexity
</subsectionHeader>
<bodyText confidence="0.999924684210526">
Looking at the inference rules, it is clear that an im-
plementation of the deduction system for arc-eager
takes space in O.jwj3/ and time in O.jwj5/, just like
in the case of arc-standard. However, a closer inspec-
tion reveals that we can give even tighter bounds.
In all derivable items Œi; hb; j •, it holds that i D h.
This can easily be shown by induction: The property
holds for the axioms, and the first two indexes of a
consequent of a deduction rule coincide with the first
two indexes of the left antecedent. Thus, if we use
the notation Œib; k• as a shorthand for Œi; ib; k•, then
we can rewrite the inference rules for the arc-eager
system as in Figure 7, where, additionally, we have
added unary rules for sh and ra and restricted the
set of axioms along the lines set out in Section 4.2.
With this formulation, it is apparent that the space
complexity of the generic implementation of the de-
duction system is in fact even in O.jwj2/, and its
time complexity is in O.jwj3/.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="method">
6 Hybrid Model
</sectionHeader>
<bodyText confidence="0.999471333333333">
We now reverse the approach that we have taken in
the previous sections: Instead of tabulating a transi-
tion system in order to get a dynamic-programming
parser that simulates its computations, we start with
a tabular parser and derive a transition system from
it. In the new model, dependency trees are built bot-
tom-up as in the arc-standard model, but the set of all
computations in the system can be tabulated in space
O.jwj2/ and time O.jwj3/, as in arc-eager.
</bodyText>
<subsectionHeader confidence="0.996891">
6.1 Deduction System
</subsectionHeader>
<bodyText confidence="0.999808">
Gómez-Rodríguez et al. (2008) present a deductive
version of the dependency parser of Yamada and Mat-
sumoto (2003); their deduction system is given in Fig-
ure 8. The generic implementation of the deduction
system takes space O.jwj2/ and time O.jwj3/.
In the original interpretation of the deduction
system, an item Œi; j• asserts the existence of a
pair of (projective) dependency trees: the first tree
rooted at token wi, having all nodes in the substring
wi • • • wk_1 as descendants, where i &lt; k &lt; j; and
the second tree rooted at token wj, having all nodes
in the substring wk • • • wj as descendants. (Note that
we use fencepost indexes, while Gómez-Rodríguez
et al. (2008) indexes positions.)
</bodyText>
<subsectionHeader confidence="0.998142">
6.2 Transition System
</subsectionHeader>
<bodyText confidence="0.924336523809524">
In the context of our tabulation framework, we adopt
a new interpretation of items: An item Œi; j • has the
same meaning as an item Œi; i; j• in the tabulation
of the arc-standard model; for every configuration c
with ˇ.c/ D ˇi, it asserts the existence of a push
computation that starts with c and ends with a config-
uration c&apos; for which ˇ.c&apos;/ D ˇj and a.c&apos;/ D a.c/ji.
If we interpret the inference rules of the system in
terms of composition operations on push computa-
tions as usual, and also take the intended direction of
the dependency arcs into account, then this induces a
transition system with three transitions:
.Q; ijˇ; A/ ` .aji; ˇ; A/ .sh/
.aji; jjˇ;A/ ` .a;jjˇ;A [ fj ! ig/ .lah/
.ajijj; ˇ; A/ ` .Qji; ˇ; A [ fi ! j g/ .ra/
We call this transition system the hybrid model, as sh
and ra are just like in arc-standard, while lah is like
the LEFT-ARc transition in the arc-eager model (lae),
except that it does not have the precondition. Like
the arc-standard but unlike the arc-eager model, the
hybrid model builds dependencies bottom-up.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99989425">
In this paper, we have provided a general technique
for the tabulation of transition-based dependency
parsers, and applied it to obtain dynamic program-
ming algorithms for two widely-used parsing models,
</bodyText>
<page confidence="0.986648">
680
</page>
<figure confidence="0.491462666666667">
Item form: Œi; j • , 0 &lt; i &lt; j &lt; jwj Goal: Œ0; jwj• Axioms: Œ0; 1•
Œi;j • Œi; k• Œk; j • Œi; k• Œk; j •
Inference rules: Œj; j C 1• .sh/ Œi; j • .lahI j ! k/ , j &lt; jwj Œi; j • .raI i ! k/
</figure>
<figureCaption confidence="0.999775">
Figure 8: Deduction system for the hybrid model.
</figureCaption>
<bodyText confidence="0.999975685714286">
arc-standard and (for the first time) arc-eager. The
basic idea behind our technique is the same as the
one implemented by Huang and Sagae (2010) for
the special case of the arc-standard model, but in-
stead of their graph-structured stack representation
we use a tabulation akin to Lang’s approach to the
simulation of pushdown automata (Lang, 1974). This
considerably simplifies both the presentation and the
implementation of parsing algorithms. It has also
enabled us to give simple proofs of correctness and
establish relations between transition-based parsers
and existing parsers based on dynamic programming.
While this paper has focused on the theoretical
aspects and the analysis of dynamic programming
versions of transition-based parsers, an obvious av-
enue for future work is the evaluation of the empiri-
cal performance and efficiency of these algorithms in
connection with specific feature models. The feature
models used in transition-based dependency parsing
are typically very expressive, and exhaustive search
with them quickly becomes impractical even for our
cubic-time algorithms of the arc-eager and hybrid
model. However, Huang and Sagae (2010) have pro-
vided evidence that the use of dynamic programming
on top of a transition-based dependency parser can
improve accuracy even without exhaustive search.
The tradeoff between expressivity of the feature mod-
els on the one hand and the efficiency of the search
on the other is a topic that we find worth investigat-
ing. Another interesting observation is that dynamic
programming makes it possible to use predictive fea-
tures, which cannot easily be integrated into a non-
tabular transition-based parser. This could lead to the
development of parsing models that cross the border
between transition-based and tabular parsing.
</bodyText>
<sectionHeader confidence="0.999126" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.715654375">
All authors contributed equally to the work presented in
this paper. M. K. wrote most of the manuscript. C. G.-R.
has been partially supported by Ministerio de Educación
y Ciencia and FEDER (HUM2007-66607-C04) and Xun-
ta de Galicia (PGIDIT07SIN005206PR, Rede Galega
de Procesamento da Linguaxe e Recuperación de Infor-
mación, Rede Galega de Lingüística de Corpus, Bolsas
Estadías INCITE/FSE cofinanced).
</bodyText>
<sectionHeader confidence="0.975731" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999807375">
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of the Tenth Conference on Computational Natural
Language Learning (CoNLL), pages 166–170, New
York, USA.
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings of
the 27th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 143–151, Vancouver,
Canada.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of the
34th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 184–191, Santa Cruz,
CA, USA.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and Head Automa-
ton Grammars. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 457–464, College Park, MD, USA.
Carlos Gómez-Rodríguez, John Carroll, and David J. Weir.
2008. A deductive approach to dependency parsing. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL): Human
Language Technologies, pages 968–976, Columbus,
OH, USA.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573–605.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1077–1086,
Uppsala, Sweden.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1222–1231, Singapore.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Jacques Loecx,
</reference>
<page confidence="0.978104">
681
</page>
<reference confidence="0.999621365853659">
editor, Automata, Languages and Programming, 2nd
Colloquium, University of Saarbrücken, July 29–August
2, 1974, number 14 in Lecture Notes in Computer Sci-
ence, pages 255–269. Springer.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 40–51, Singa-
pore.
Joakim Nivre. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of the Eighth In-
ternational Workshop on Parsing Technologies (IWPT),
pages 149–160, Nancy, France.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Workshop on Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 50–57, Barcelona, Spain.
Joakim Nivre. 2006. Inductive Dependency Parsing,
volume 34 of Text, Speech and Language Technology.
Springer.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513–553.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive pars-
ing. Journal of Logic Programming, 24(1–2):3–36.
Masaru Tomita. 1986. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies (IWPT), pages 195–206, Nancy,
France.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 562—571,
Honolulu, HI, USA.
</reference>
<page confidence="0.99797">
682
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.475813">
<title confidence="0.9998435">Dynamic Programming for Transition-Based Dependency Parsers</title>
<author confidence="0.999979">Marco Kuhlmann Carlos Gómez-Rodríguez Giorgio Satta</author>
<affiliation confidence="0.999954">Dept. of Linguistics and Philology Departamento de Computación Dept. of Information Engineering</affiliation>
<address confidence="0.528493">Uppsala University, Sweden Universidade da Coruña, Spain University of Padua, Italy</address>
<email confidence="0.861609">marco.kuhlmann@lingfil.uu.secgomezr@udc.essatta@dei.unipd.it</email>
<abstract confidence="0.999218461538461">We develop a general dynamic programming technique for the tabulation of transition-based dependency parsers, and apply it to obtain novel, polynomial-time algorithms for parsing with the arc-standard and arc-eager models. We also show how to reverse our technique to obtain new transition-based dependency parsers from existing tabular methods. Additionally, we provide a detailed discussion of the conditions under which the feature models commonly used in transition-based parsing can be integrated into our algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a multilanguage non-projective dependency parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>166--170</pages>
<location>New York, USA.</location>
<contexts>
<context position="7282" citStr="Attardi, 2006" startWordPosition="1212" endWordPosition="1213">ome transition. It is called complete whenever c0 D I(w), and cm 2 Cr. We note that a computation can be uniquely specified by its initial configuration c0 and the sequence of its transitions, understood as a string over T . Complete computations, where c0 is fixed, can be specified by their transition sequences alone. 3 Arc-Standard Model To introduce the core concepts of the paper, we first look at a particularly simple model for transitionbased dependency parsing, known as the arc-standard model. This model has been used, in slightly different variants, by a number of parsers (Nivre, 2004; Attardi, 2006; Huang and Sagae, 2010). 3.1 Transition System The arc-standard model uses three types of transitions: SHIFT (sh) removes the first node in the buffer and pushes it to the stack. LEFT-ARc (la) creates a new arc with the topmost node on the stack as the head and the second-topmost node as the dependent, and removes the second-topmost node from the stack. RIGHT-ARc (ra) is symmetric to LEFT-ARc in that it creates an arc with the second-topmost node as the head and the topmost node as the dependent, and removes the topmost node. The three transitions can be formally specified as in Figure 1. The</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL), pages 166–170, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>143--151</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1870" citStr="Billot and Lang, 1989" startWordPosition="258" endWordPosition="261">es where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Vivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Vivre, 2004; Huang and Sagae, 2010) and arc-eage</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL), pages 143–151, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>184--191</pages>
<location>Santa Cruz, CA, USA.</location>
<contexts>
<context position="14934" citStr="Collins, 1996" startWordPosition="2736" endWordPosition="2737"> intermediate configuration, and thereby violates property P1. 3.5 Discussion Let us briefly take stock of what we have achieved so far. We have provided a deduction system capable of tabulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.IwI3/ and time in O.IwI5/. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.IwI4/. 4 Adding Features The main goal with the tabulation of transition-based dependency parsers is to obtain a representation based on which semiring values such as the highest-scoring computation for a given input (and with it, a dependency tree) can be calculated. Such com</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 184–191, Santa Cruz, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and Head Automaton Grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>457--464</pages>
<location>College Park, MD, USA.</location>
<contexts>
<context position="15128" citStr="Eisner and Satta (1999)" startWordPosition="2762" endWordPosition="2765">tabulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.IwI3/ and time in O.IwI5/. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.IwI4/. 4 Adding Features The main goal with the tabulation of transition-based dependency parsers is to obtain a representation based on which semiring values such as the highest-scoring computation for a given input (and with it, a dependency tree) can be calculated. Such computations involve the use of feature information. In this section, we discuss how our tabulation of the arcstandard system can be extended for this purpose. 676 Œi; h1; kI hx2; x1i; hx1; x3i• W </context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and Head Automaton Grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pages 457–464, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gómez-Rodríguez</author>
<author>John Carroll</author>
<author>David J Weir</author>
</authors>
<title>A deductive approach to dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL): Human Language Technologies,</booktitle>
<pages>968--976</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="3034" citStr="Gómez-Rodríguez et al. (2008)" startWordPosition="434" endWordPosition="437">s: arc-standard (Vivre, 2004; Huang and Sagae, 2010) and arc-eager (Vivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our algorithms, and in particular the conditions under which the feature models traditionally used in transition-based dependency parsing can be integrated into our framework. While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). We instead simulate computations as in Lang (1974), which results in simpler algorithm specifications, and also reveals deep similarities bet</context>
<context position="14965" citStr="Gómez-Rodríguez et al., 2008" startWordPosition="2738" endWordPosition="2741">onfiguration, and thereby violates property P1. 3.5 Discussion Let us briefly take stock of what we have achieved so far. We have provided a deduction system capable of tabulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.IwI3/ and time in O.IwI5/. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.IwI4/. 4 Adding Features The main goal with the tabulation of transition-based dependency parsers is to obtain a representation based on which semiring values such as the highest-scoring computation for a given input (and with it, a dependency tree) can be calculated. Such computations involve the use of fe</context>
<context position="29729" citStr="Gómez-Rodríguez et al. (2008)" startWordPosition="5455" endWordPosition="5458">entation of the deduction system is in fact even in O.jwj2/, and its time complexity is in O.jwj3/. 6 Hybrid Model We now reverse the approach that we have taken in the previous sections: Instead of tabulating a transition system in order to get a dynamic-programming parser that simulates its computations, we start with a tabular parser and derive a transition system from it. In the new model, dependency trees are built bottom-up as in the arc-standard model, but the set of all computations in the system can be tabulated in space O.jwj2/ and time O.jwj3/, as in arc-eager. 6.1 Deduction System Gómez-Rodríguez et al. (2008) present a deductive version of the dependency parser of Yamada and Matsumoto (2003); their deduction system is given in Figure 8. The generic implementation of the deduction system takes space O.jwj2/ and time O.jwj3/. In the original interpretation of the deduction system, an item Œi; j• asserts the existence of a pair of (projective) dependency trees: the first tree rooted at token wi, having all nodes in the substring wi • • • wk_1 as descendants, where i &lt; k &lt; j; and the second tree rooted at token wj, having all nodes in the substring wk • • • wj as descendants. (Note that we use fencepo</context>
</contexts>
<marker>Gómez-Rodríguez, Carroll, Weir, 2008</marker>
<rawString>Carlos Gómez-Rodríguez, John Carroll, and David J. Weir. 2008. A deductive approach to dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL): Human Language Technologies, pages 968–976, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1541" citStr="Goodman, 1999" startWordPosition="210" endWordPosition="211">ming algorithms, also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Vivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic progra</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1077--1086</pages>
<location>Uppsala,</location>
<contexts>
<context position="1649" citStr="Huang and Sagae (2010)" startWordPosition="227" endWordPosition="230">ons in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Vivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper </context>
<context position="3352" citStr="Huang and Sagae (2010)" startWordPosition="485" endWordPosition="488">icable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our algorithms, and in particular the conditions under which the feature models traditionally used in transition-based dependency parsing can be integrated into our framework. While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). We instead simulate computations as in Lang (1974), which results in simpler algorithm specifications, and also reveals deep similarities between transition-based systems for dependency parsing and existing tabular methods for lexicalized context-free grammars. 673 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 673–682, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Tr</context>
<context position="7306" citStr="Huang and Sagae, 2010" startWordPosition="1214" endWordPosition="1217"> It is called complete whenever c0 D I(w), and cm 2 Cr. We note that a computation can be uniquely specified by its initial configuration c0 and the sequence of its transitions, understood as a string over T . Complete computations, where c0 is fixed, can be specified by their transition sequences alone. 3 Arc-Standard Model To introduce the core concepts of the paper, we first look at a particularly simple model for transitionbased dependency parsing, known as the arc-standard model. This model has been used, in slightly different variants, by a number of parsers (Nivre, 2004; Attardi, 2006; Huang and Sagae, 2010). 3.1 Transition System The arc-standard model uses three types of transitions: SHIFT (sh) removes the first node in the buffer and pushes it to the stack. LEFT-ARc (la) creates a new arc with the topmost node on the stack as the head and the second-topmost node as the dependent, and removes the second-topmost node from the stack. RIGHT-ARc (ra) is symmetric to LEFT-ARc in that it creates an arc with the second-topmost node as the head and the topmost node as the dependent, and removes the topmost node. The three transitions can be formally specified as in Figure 1. The right half of Figure 2 </context>
<context position="17099" citStr="Huang and Sagae (2010)" startWordPosition="3124" endWordPosition="3127"> configuration ct in which t was taken: Xscore.y/ D score.t; ct/ (1) t2Y The score score.t; ct/ is defined as the dot product of the feature representation of ct relative to a feature model ˚ and a transition-specific weight vector E˛t: score.t; ct/ D ˚.ct/ • E˛t The feature model ˚ is a vector h01; ::: ; Oni of elementary feature functions, and the feature representation ˚.c/ of a configuration c is a vector xE D h01.c/; ::: ; On.c/i of atomic values. Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0:w and s1:w, respectively. Feature functions like these have been used in several parsers (Nivre, 2006; Zhang and Clark, 2008; Huang et al., 2009). 4.2 Integration of Feature Models To integrate feature models into our tabulation of the arc-standard system, we can use extended items of the form Œi; h; jI ExL; ExR• with the same intended interpretation as the old items Œi; h; j •, except that the initial configuration of the asserted computations y D c0; ::: ; cm now is required to have the feature representation ExL, and the final configuration is required </context>
<context position="19593" citStr="Huang and Sagae (2010)" startWordPosition="3562" endWordPosition="3565">t vectors, that is, by all pairs ExL; ExR such that there exists a configuration c with ˚.c/ D ExL and ˚.sh.c// D ExR. The task of filtering out useless guesses can then be delegated to the deduction system. A more efficient way is to only have one axiom, for the case where c D I.w/, and to add to the deduction system a new, unary inference rule for sh as in Figure 4. This rule only creates items whose left-context vector is the right-context vector of some other item, which prevents the generation of useless items. In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). 677 Œi; h; jI hx2; x1i; hx1; x3i• W .p; v/ .sh/ , where 6 D hx1; x3i E˛sh Œj; j; j C 1I hx1; x3i; hx3; wj i• W .p C 6; 6/ Œi; h1; kI hx2; x1i; hx1; x3i• W .p1; v1/ Œk; h2; jI hx1; x3i; hx3; x4i• W .p2; v2/ .ra/ , where p D hx3; x4i &apos; E˛ra Figure 5: Extended inference rules under the feature model ˚ D hs0:w; s1:wi. The annotations indicate how to calculate a candidate for an update of the prefix score and Viterbi score of the conclusion. Œi; h1; jI hx2; x1i; hx1; x3i• W .p1 C v2 C p; v1 C v2 C P/ 4.3 Computing Viterbi Scores Once we have extended our deduction system with feature information,</context>
<context position="20994" citStr="Huang and Sagae (2010)" startWordPosition="3833" endWordPosition="3836">ete computations for w. The score of a complex computation ft.y1; y2/ is the sum of the scores of its subcomputations y1; y2, plus the transition-specific dot product. Since this dot product only depends on the feature representation of the final configuration of y2, the Viterbi score can be computed on top of the inference rules using standard techniques. The crucial calculation is indicated in Figure 4. 4.4 Computing Prefix Scores Another interesting value is the prefix score of an item, which, apart from the Viterbi score, also includes the cost of the best search path leading to the item. Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. In our framework, prefix scores can be computed as indicated in Figure 5. Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left-context vector from their items.1 4.5 Compatibility So far we have restricted our attention to a concrete and extremely simplistic feature model. The feature models that are used in practical systems are considerably more complex, and not all of them are 1The essential idea in the calcula</context>
<context position="22735" citStr="Huang and Sagae (2010)" startWordPosition="4147" endWordPosition="4150">if we know the values of this feature for two computations y1; y2, we have no way to compute its value for the composed computation fra.y1; y2/: This value coincides with the word form of the topmost node on the stack associated with y2, but in order to have access to it in the context of the ra rule, our feature model would need to also include the feature function s0:w. The example just given raises the question whether there is a general criterion based on which we can decide if a given feature model is compatible with our tabulation. An attempt to provide such a criterion has been made by Huang and Sagae (2010), who define a constraint on feature models called “monotonicity” and claim that this constraint guarantees that feature values can be computed using their dynamic programming approach. Unfortunately, this claim is wrong. In particular, the feature model ˚&apos; given above is “monotonic”, but cannot be tabulated, neither in our nor in their framework. In general, it seems clear that the question of compatibility is a question about the relation between the tabulation and the feature model, and not about the feature model alone. To find practically useful characterizations of compatibility is an in</context>
<context position="32072" citStr="Huang and Sagae (2010)" startWordPosition="5892" endWordPosition="5895">ndencies bottom-up. 7 Conclusion In this paper, we have provided a general technique for the tabulation of transition-based dependency parsers, and applied it to obtain dynamic programming algorithms for two widely-used parsing models, 680 Item form: Œi; j • , 0 &lt; i &lt; j &lt; jwj Goal: Œ0; jwj• Axioms: Œ0; 1• Œi;j • Œi; k• Œk; j • Œi; k• Œk; j • Inference rules: Œj; j C 1• .sh/ Œi; j • .lahI j ! k/ , j &lt; jwj Œi; j • .raI i ! k/ Figure 8: Deduction system for the hybrid model. arc-standard and (for the first time) arc-eager. The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang’s approach to the simulation of pushdown automata (Lang, 1974). This considerably simplifies both the presentation and the implementation of parsing algorithms. It has also enabled us to give simple proofs of correctness and establish relations between transition-based parsers and existing parsers based on dynamic programming. While this paper has focused on the theoretical aspects and the analysis of dynamic programming versions of transition-based parse</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1077–1086, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1222--1231</pages>
<contexts>
<context position="17282" citStr="Huang et al., 2009" startWordPosition="3154" endWordPosition="3157">del ˚ and a transition-specific weight vector E˛t: score.t; ct/ D ˚.ct/ • E˛t The feature model ˚ is a vector h01; ::: ; Oni of elementary feature functions, and the feature representation ˚.c/ of a configuration c is a vector xE D h01.c/; ::: ; On.c/i of atomic values. Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0:w and s1:w, respectively. Feature functions like these have been used in several parsers (Nivre, 2006; Zhang and Clark, 2008; Huang et al., 2009). 4.2 Integration of Feature Models To integrate feature models into our tabulation of the arc-standard system, we can use extended items of the form Œi; h; jI ExL; ExR• with the same intended interpretation as the old items Œi; h; j •, except that the initial configuration of the asserted computations y D c0; ::: ; cm now is required to have the feature representation ExL, and the final configuration is required to have the representation ExR: ˚.c0/ D ExL and ˚.cm/ D ExR We shall refer to the vectors ExL and ExR as the leftcontext vector and the right-context vector of the computation y, resp</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1222–1231, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>Automata, Languages and Programming, 2nd Colloquium, University of Saarbrücken, July 29–August 2, 1974, number 14 in Lecture Notes in Computer Science,</booktitle>
<pages>255--269</pages>
<editor>In Jacques Loecx, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1832" citStr="Lang, 1974" startWordPosition="254" endWordPosition="255">parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Vivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Vivre, 200</context>
<context position="3543" citStr="Lang (1974)" startWordPosition="518" endWordPosition="519">isting tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our algorithms, and in particular the conditions under which the feature models traditionally used in transition-based dependency parsing can be integrated into our framework. While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). We instead simulate computations as in Lang (1974), which results in simpler algorithm specifications, and also reveals deep similarities between transition-based systems for dependency parsing and existing tabular methods for lexicalized context-free grammars. 673 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 673–682, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Transition-Based Dependency Parsing We start by briefly introducing the framework of transition-based dependency parsing; for details, we refer to Nivre (2008). 2.1 Dependency Graphs Let w D w0</context>
<context position="32275" citStr="Lang, 1974" startWordPosition="5926" endWordPosition="5927">-used parsing models, 680 Item form: Œi; j • , 0 &lt; i &lt; j &lt; jwj Goal: Œ0; jwj• Axioms: Œ0; 1• Œi;j • Œi; k• Œk; j • Œi; k• Œk; j • Inference rules: Œj; j C 1• .sh/ Œi; j • .lahI j ! k/ , j &lt; jwj Œi; j • .raI i ! k/ Figure 8: Deduction system for the hybrid model. arc-standard and (for the first time) arc-eager. The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang’s approach to the simulation of pushdown automata (Lang, 1974). This considerably simplifies both the presentation and the implementation of parsing algorithms. It has also enabled us to give simple proofs of correctness and establish relations between transition-based parsers and existing parsers based on dynamic programming. While this paper has focused on the theoretical aspects and the analysis of dynamic programming versions of transition-based parsers, an obvious avenue for future work is the evaluation of the empirical performance and efficiency of these algorithms in connection with specific feature models. The feature models used in transition-b</context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>Bernard Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In Jacques Loecx, editor, Automata, Languages and Programming, 2nd Colloquium, University of Saarbrücken, July 29–August 2, 1974, number 14 in Lecture Notes in Computer Science, pages 255–269. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>40--51</pages>
<contexts>
<context position="1563" citStr="Li and Eisner, 2009" startWordPosition="212" endWordPosition="215">, also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Vivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a </context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 40–51, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<location>Nancy, France.</location>
<contexts>
<context position="23627" citStr="Nivre, 2003" startWordPosition="4286" endWordPosition="4287">nic”, but cannot be tabulated, neither in our nor in their framework. In general, it seems clear that the question of compatibility is a question about the relation between the tabulation and the feature model, and not about the feature model alone. To find practically useful characterizations of compatibility is an interesting avenue for future research. 5 Arc-Eager Model Up to now, we have only discussed the arc-standard model. In this section, we show that the framework of push computations also provides a tabulation of another widely-used model for dependency parsing, the arc-eager model (Nivre, 2003). 678 (6, ijP, A) ` (6ji, P, A) (sh) (6ji, j jP, A) ` (6, j jP, A [ fj ! ig) (lae) only if i does not have an incoming arc (6ji, j jP, A) ` (6jijj, P, A [ fi ! j g) (rae) (6ji, P, A) ` (6, P, A) (re) only if i has an incoming arc Figure 6: Transitions in the arc-eager model. 5.1 Transition System The arc-eager model has three types of transitions, shown in Figure 6: SHIFT (sh) works just like in arcstandard, moving the first node in the buffer to the stack. LEFT-ARC (lae) creates a new arc with the first node in the buffer as the head and the topmost node on the stack as the dependent, and pop</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT), pages 149–160, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<location>Barcelona,</location>
<contexts>
<context position="7267" citStr="Nivre, 2004" startWordPosition="1210" endWordPosition="1211">g one under some transition. It is called complete whenever c0 D I(w), and cm 2 Cr. We note that a computation can be uniquely specified by its initial configuration c0 and the sequence of its transitions, understood as a string over T . Complete computations, where c0 is fixed, can be specified by their transition sequences alone. 3 Arc-Standard Model To introduce the core concepts of the paper, we first look at a particularly simple model for transitionbased dependency parsing, known as the arc-standard model. This model has been used, in slightly different variants, by a number of parsers (Nivre, 2004; Attardi, 2006; Huang and Sagae, 2010). 3.1 Transition System The arc-standard model uses three types of transitions: SHIFT (sh) removes the first node in the buffer and pushes it to the stack. LEFT-ARc (la) creates a new arc with the topmost node on the stack as the head and the second-topmost node as the dependent, and removes the second-topmost node from the stack. RIGHT-ARc (ra) is symmetric to LEFT-ARc in that it creates an arc with the second-topmost node as the head and the topmost node as the dependent, and removes the topmost node. The three transitions can be formally specified as i</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing,</title>
<date>2006</date>
<journal>Text, Speech and Language</journal>
<volume>34</volume>
<publisher>Technology. Springer.</publisher>
<contexts>
<context position="17238" citStr="Nivre, 2006" startWordPosition="3148" endWordPosition="3149">ation of ct relative to a feature model ˚ and a transition-specific weight vector E˛t: score.t; ct/ D ˚.ct/ • E˛t The feature model ˚ is a vector h01; ::: ; Oni of elementary feature functions, and the feature representation ˚.c/ of a configuration c is a vector xE D h01.c/; ::: ; On.c/i of atomic values. Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0:w and s1:w, respectively. Feature functions like these have been used in several parsers (Nivre, 2006; Zhang and Clark, 2008; Huang et al., 2009). 4.2 Integration of Feature Models To integrate feature models into our tabulation of the arc-standard system, we can use extended items of the form Œi; h; jI ExL; ExR• with the same intended interpretation as the old items Œi; h; j •, except that the initial configuration of the asserted computations y D c0; ::: ; cm now is required to have the feature representation ExL, and the final configuration is required to have the representation ExR: ˚.c0/ D ExL and ˚.cm/ D ExR We shall refer to the vectors ExL and ExR as the leftcontext vector and the rig</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Inductive Dependency Parsing, volume 34 of Text, Speech and Language Technology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="4109" citStr="Nivre (2008)" startWordPosition="592" endWordPosition="593">nstead simulate computations as in Lang (1974), which results in simpler algorithm specifications, and also reveals deep similarities between transition-based systems for dependency parsing and existing tabular methods for lexicalized context-free grammars. 673 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 673–682, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Transition-Based Dependency Parsing We start by briefly introducing the framework of transition-based dependency parsing; for details, we refer to Nivre (2008). 2.1 Dependency Graphs Let w D w0 • • • w,t_1 be a string over some fixed alphabet, where n &gt; 1 and w0 is the special token ROOT. A dependency graph for w is a directed graph G D (V„, A), where V„ D f0, ... , n — 1g is the set of nodes, and A C_ V„ x V„ is the set of arcs. Each node in V„ encodes the position of a token in w, and each arc in A encodes a dependency relation between two tokens. To denote an arc (i, j) 2 A, we write i ! j; here, the node i is the head, and the node j is the dependent. A sample dependency graph is given in the left part of Figure 2. 2.2 Transition Systems A trans</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<contexts>
<context position="10655" citStr="Shieber et al. (1995)" startWordPosition="1871" endWordPosition="1874">tions on the same input string w such that c1m1 = c20. Then fra(y1, y2) = c10, . . . , c1m1, c21, . . . , c2m2, c , where c is obtained from c2m2 by applying the ra transition. (The operation fla is defined analogously.) We can verify that f,a(y1, y2) is another push computation. For instance, with respect to Figure 2, fra(y1, y2) = y0. Conversely, we say that the push computation y0 can be decomposed into the subcomputations y1 and y2, and the operation fra. 3.3 Deduction System Building on the compositional structure of push computations, we now construct a deduction system (in the sense of Shieber et al. (1995)) that tabulates the computations of the arc-standard model for a given input string w = w0 • • • wn_1. For 0 &lt; i &lt; n, we shall write Pi to denote the buffer [i, ... , n-1]. Thus, P0 denotes the full buffer, associated with the initial configuration I(w), and Pn denotes the empty buffer, associated with a terminal configuration c E Ct. Item form. The items of our deduction system take the form [i, h, j], where 0 &lt; i &lt; h &lt; j &lt; n. The intended interpretation of an item [i, h, j ] is: For every configuration c0 with P(c0) = Pi, there exists a push computation y = c0, ... , cm such that P(cm) = Pj</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems.</title>
<date>1986</date>
<publisher>Springer.</publisher>
<contexts>
<context position="1846" citStr="Tomita, 1986" startWordPosition="256" endWordPosition="257">s, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Vivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Vivre, 2004; Huang and S</context>
<context position="3491" citStr="Tomita (1986)" startWordPosition="510" endWordPosition="511">abulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our algorithms, and in particular the conditions under which the feature models traditionally used in transition-based dependency parsing can be integrated into our framework. While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). We instead simulate computations as in Lang (1974), which results in simpler algorithm specifications, and also reveals deep similarities between transition-based systems for dependency parsing and existing tabular methods for lexicalized context-free grammars. 673 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 673–682, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Transition-Based Dependency Parsing We start by briefly introducing the framework of transition-based dependency parsing; for details, we ref</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Masaru Tomita. 1986. Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>195--206</pages>
<location>Nancy, France.</location>
<contexts>
<context position="29813" citStr="Yamada and Matsumoto (2003)" startWordPosition="5468" endWordPosition="5472">s in O.jwj3/. 6 Hybrid Model We now reverse the approach that we have taken in the previous sections: Instead of tabulating a transition system in order to get a dynamic-programming parser that simulates its computations, we start with a tabular parser and derive a transition system from it. In the new model, dependency trees are built bottom-up as in the arc-standard model, but the set of all computations in the system can be tabulated in space O.jwj2/ and time O.jwj3/, as in arc-eager. 6.1 Deduction System Gómez-Rodríguez et al. (2008) present a deductive version of the dependency parser of Yamada and Matsumoto (2003); their deduction system is given in Figure 8. The generic implementation of the deduction system takes space O.jwj2/ and time O.jwj3/. In the original interpretation of the deduction system, an item Œi; j• asserts the existence of a pair of (projective) dependency trees: the first tree rooted at token wi, having all nodes in the substring wi • • • wk_1 as descendants, where i &lt; k &lt; j; and the second tree rooted at token wj, having all nodes in the substring wk • • • wj as descendants. (Note that we use fencepost indexes, while Gómez-Rodríguez et al. (2008) indexes positions.) 6.2 Transition S</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT), pages 195–206, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>562--571</pages>
<location>Honolulu, HI, USA.</location>
<contexts>
<context position="2508" citStr="Zhang and Clark, 2008" startWordPosition="350" endWordPosition="353">the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Vivre, 2004; Huang and Sagae, 2010) and arc-eager (Vivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our alg</context>
<context position="16310" citStr="Zhang and Clark (2008)" startWordPosition="2979" endWordPosition="2982">e. 676 Œi; h1; kI hx2; x1i; hx1; x3i• W v1 Œk; h2; jI hx1; x3i; hx3; x4i• W v2 .ra/ Œi; h1; jI hx2; x1i; hx1; x3i• W v1 C v2 C hx3; x4i &apos; E˛ra Œi; h; jI hx2; x1i; hx1; x3i• W v .sh/ Œj; j; j C 1I hx1; x3i; hx3; wj i• W hx1; x3i - E˛sh Figure 4: Extended inference rules under the feature model ˚ D hs1:w; s0:wi. The annotations indicate how to calculate a candidate for an update of the Viterbi score of the conclusion using the Viterbi scores of the premises. 4.1 Scoring Computations For the sake of concreteness, suppose that we want to score computations based on the following model, taken from Zhang and Clark (2008). The score of a computation y is broken down into a sum of scores score.t; ct/ for combinations of a transition t in the transition sequence associated with y and the configuration ct in which t was taken: Xscore.y/ D score.t; ct/ (1) t2Y The score score.t; ct/ is defined as the dot product of the feature representation of ct relative to a feature model ˚ and a transition-specific weight vector E˛t: score.t; ct/ D ˚.ct/ • E˛t The feature model ˚ is a vector h01; ::: ; Oni of elementary feature functions, and the feature representation ˚.c/ of a configuration c is a vector xE D h01.c/; ::: ; O</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 562—571, Honolulu, HI, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>