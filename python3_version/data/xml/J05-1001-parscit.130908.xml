<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.8978295">
ACL Lifetime Achievement Award
Some Points in a Time1
</title>
<author confidence="0.994396">
Karen Spa¨rck Jones*
</author>
<affiliation confidence="0.99249">
University of Cambridge
</affiliation>
<construct confidence="0.8701866">
This article offers a personal perspective on the development of language and information
processing over the last half century, focusing on the use of statistical methods. Introduced,
with computers, in the 1950s, these have not always been highly regarded, but were revived in
the 1990s. They have proved effective in more ways than might have been expected, and
encourage new thinking about what language and information processing involve.
</construct>
<bodyText confidence="0.999567571428571">
First, to say how much I appreciate the completely unexpected honour of this award,
and to thank the ACL for it.
I want to look at one line, or thread, in natural language processing (NLP)
research: how it began, what happened to it, what it suggests we need to investigate
now. I shall take some papers of my own as pegs to hang the story on, but without
making any claims for particular merit in these papers.
My first paper, “The analogy between MT and IR,’’ with Margaret Masterman and
Roger Needham, was for a conference in 1958. The analogy it proclaimed was in the
need for a thesaurus, that is, a semantic classification. Machine translation (MT) and
information retrieval (IR) are different tasks in their granularity and in the role of
syntax. But we argued that both need a means of finding common concepts behind
surface words, so we at once identify text content and word senses. We took Roget’s
Thesaurus as such a tool in our MT experiments (with punched cards), where we
exploited the redundancy that text always has to select class labels, that indicate
senses, for words.
The essential idea is illustrated, in extremely simple form (as with all my
examples) in Figure 1. If we have to translate The farmer cultivates the field, where field
has a range of senses including LAND and SUBJECT that may well have different target
language equivalents, the fact that the general concept AGRICULTURE underlies each
of farmer, cultivates, and field selects the sense LAND for field.
But we found in our research that existing thesauruses, like Roget’s, were not
wholly satisfactory, for example through missing senses; and we wanted to build a
better one, ideally automatically. The natural way to do this, the obverse of the way the
thesaurus would be applied once it was constructed, was by using text distribution
data for words and applying statistical classification methods to these data.
There were of course no corpora available then, so in my thesis work I finessed
getting the input data for classification by taking dictionary definitions, which often
consist of close synonym sets, as showing the most primitive and minimal shared-
</bodyText>
<footnote confidence="0.83532675">
Computer Laboratory, William Gates Building, JJ Thomson Avenue, Cambridge CB3 0FD, UK.
E-mail: ksj@cl.cam.ac.uk.
1 This article is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2004. The
figures provided in the article reproduce the slides used for the talk.
</footnote>
<note confidence="0.592558">
0 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 1
</note>
<figureCaption confidence="0.993894">
Figure 1
</figureCaption>
<bodyText confidence="0.999488736842105">
context behavior for the words concerned. The words in such a set could be taken as
mutually substitutible in some context, which could be checked by reference to the text
examples given in the dictionary. This mimicked what could in principle be delivered by
a corpus which showed one word or another in the set occurring in the same text context.
I then showed that one could apply general classification methods, notably Roger
Needham’s theory of clumps, to find larger classes of words with similar behavior.
For example, suppose we have synonym sets, or “rows,’’ as shown in Figure 2, that
is, activity animation movement, activity movement business, briskness business liveliness,
etc., each representing substitutive, i.e., synonymous, uses of the words concerned. We
can then derive classes of words with similar uses by exploiting the recurrence of the
same word symbols across different rows: The senses of a word in different rows are
distinct senses, but they can legitimately be assumed to be semantically related simply
through the fact that they are uses of the same word sign.
In subsequent work, Kenneth Harper and I showed that it was possible to derive
classes from actual word occurrences and cooccurences in text, as illustrated in
Figure 3. Given a (hand-) parsed Russian scientific text corpus, we investigated the
collocation behavior of 40 Russian nouns (here represented by English equivalents) in
governor/dependent relations, and grouped them by their shared collocates. Some-
what suprisingly, for such a small corpus, we were able to extract semantically
</bodyText>
<figureCaption confidence="0.724724">
Figure 2
</figureCaption>
<page confidence="0.995481">
2
</page>
<note confidence="0.867574">
Spa¨rck Jones Some Points in a Time
</note>
<figureCaption confidence="0.953113">
Figure 3
</figureCaption>
<bodyText confidence="0.9932693">
plausible classes, like height depth width or calculation measurement study investigation
etc. as shown in Figure 3.
These tests were very small in absolute terms (though less so relative to quite
recent research than might be expected). But the much more serious problem, for
evaluation, was that there was no MT system you could plug a classification into to test
it. Different general classification methods can give different, but equally plausible,
classifications, so you need an application context to choose among them, as well as to
check that the generic idea of such a sense selection apparatus is sound.
We also needed a theory of discourse structure, above the sentence, to support and
constrain the use of a classification. MT then was sentence-limited, but in general you
cannot rely on individual sentences for all the information you need for sense
resolution. Figure 4 shows a very rudimentary attempt to model discourse structure
using topic/comment relations across sentences, shown using T or C with numbers for
concepts. Topic/content linkage patterns could show where to look for information
from other words to help select the sense of a word in a particular location. For
instance, in selecting a sense for a given topic word, concepts that were shared with
previous topic words, or recent comment words, would be preferred. In the illustration
the same word is repeated, but it is easy to imagine, for example, that ocean could occur
at one point and sea at another, so selection would involve semantic classes.
Figure 4
</bodyText>
<page confidence="0.98245">
3
</page>
<note confidence="0.298807">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.999962105263158">
But work like this was only the sketchiest beginning as far as MT was concerned.
Retrieval was a much better initial test field for semantic classification, because you
can get index keys for documents without interference from the need for sentence
parses. At the same time, you have to consider the document as a whole. Retrieval, for
the growing technical literature, was also a very pressing practical task, so people were
developing evaluation methods for different indexing and searching strategies.
Researchers wanted to show that a statistical classification, based on term distributions
across documents, was effective as a lexical substitution device, helping recall (an idea
first published by Luhn [1957]).
The task function for classification in IR was important because it forced thinking
about the choice of classification model, for example should classes be overlapping or
exclusive? Deriving classes is a major step beyond computing word pair associations.
It needs sound and appropriate, as well as computable, approaches, desiderata that are
not always recognized even now.
We began to test automatic classification for retrieval with a small collection built
by Cyril Cleverdon and also used by Gerard Salton (to whom, as long-standing
colleagues, I owe a great deal).
The work started well, and I continued to work in retrieval because, for everyone
in NLP, the world turned cold in the later 1960s under the combined effects of the
assaults on MT research and the fashion for Chomskyan linguistics. But it turned out
to be much more complicated than expected to get term classes to do better than
simple term matching in retrieving relevant documents. Though I could get benefits
from classification for my first collection, when I tried others I could not get anywhere
much.
Trying to understand why, I realized that term occurrence frequency, in the
document file as a whole, not just cooccurrence frequency, was much more important
than anyone had recognized.
For retrieval, you need to consider the discriminative as well as the descriptive
value of a term, so you do not want to increase the matching power of a common term.
As there are few relevant documents for a request among the many in the file, any term
that occurs in a lot of documents is bound to retrieve more non-relevant than relevant
ones. Allowing common terms to substitute for others compounds the problem.
This observation led to the proposal for term weighting, later called idf weighting,
that turned out to be a big win: Terms should be weighted inversely by the number of
documents in which they occur. The particular function, shown in Figure 5, is
extremely, and pleasingly, simple but has been shown to be generally useful. Salton
saw the value of tf weighting, that is, the value of term frequency within a single
document, where the greater the frequency, the more important the term is for the
</bodyText>
<figureCaption confidence="0.397804">
Figure 5
</figureCaption>
<page confidence="0.987808">
4
</page>
<subsectionHeader confidence="0.560864">
Spa¨rck Jones Some Points in a Time
</subsectionHeader>
<bodyText confidence="0.9998549375">
document. With both ideas about weighting, researchers were beginning to get a better
idea of the significance of a word’s text statistics.
This was only within IR. Computational linguistics (CL) was focused on sentence
grammar, artificial intelligence on world knowledge, and text interpretation from that
point of view.
But in IR, the weighting idea had yet more to it. Idf weighting tries to use general
term frequency data, for a whole collection, to predict specific query relevance. If you
have some past relevance information, you can do better.
As with classification, it is tricky to get the formula right. I went through four
successive versions, in an experiment/theory interaction with Stephen Robertson, at
the beginning of a long and valued collaboration. But the outcome was very satisfying,
not just for the performance results, but because Robertson’s Probabilistic Model for
retrieval relates the task to the text data in a convincing way. Figure 6 shows how
effective this relevance weighting approach is, even for a difficult later data set using
only document titles though rather rich requests. Even with only a few known relevant
items to supply information about query terms in an iterative search, performance is
much better than with idf weights alone, and when there are many known relevant to
exploit, performance is strikingly better. The results illustrate the value of having
training data for what would now be called a machine-learning strategy. Full relevance
information can also be used to search retrospectively, to define a rational upper
bound for performance.
This work in the 1970s looked very good, but there was a practical challenge in
scaling experiments up to a realistic level, because testing needs reference data in the
form of relevant documents for requests, and getting this for large collections is
expensive. Operational services were not interested, partly because they believed in
older conventional methods and partly because they had genuine other concerns like
efficiency. The CL community was addressing the quite different goal of building
natural language front ends to databases, which seemed a much more important
project, addressing a task that needed proper syntactic and semantic interpretation. I
thought it was an interestingly distinct challenge and joined in.
Natural language access to databases, converting a text question into, for example,
SQL, looked like a tractable task at the time. But it turned out much less so than
</bodyText>
<footnote confidence="0.579816">
Figure 6
</footnote>
<page confidence="0.839905">
5
</page>
<note confidence="0.247001">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.993864636363636">
expected because it needs a large, rich domain model to bridge the gap between the
user’s free input and the particular data model, and also to trap the user’s unwitting
excursions outside the database boundary.
The difficulties of doing data access well also emphasized how narrow and limited
database query, as normally implemented, actually is as a form of information access.
It is much more reasonable to envisage a family of access modes to different types of
information giving the user whatever can be found, in one way or another, in text,
data, or even knowledge bases, as these happen to be available. A system would take
the user’s single input and apply different types of interpretation at different levels,
within a single framework. It would treat a query deliberately as something to be
unpacked from different points of view, rather than in a preferred mode, with backup
defaults, as in LUNAR (Woods 1978).
Figure 7 illustrates the idea, very simplistically. Thus for the user’s input, Who
supplies green parts?, suppose we have an initial interpretation, say as a dependency
tree. This can be subjected to further processing as a whole to construct a formal
database query or taken simply as a base from which variant phrasal terms for text
searching can be derived. Alternatively, if we consider the input text itself as its own,
direct, representation, we can use this representation as a source for simple retrieval
terms. Other versions of the input are drawn from different, perhaps successively
deeper, representations as needed for particular tasks. Treating the text itself as a
representation emphasizes the point that document retrieval is one proper task in its
own right, among several, not a poor substitute for “true”question answering.
The idea that one can usefully work on the linguistic surface was reinforced in the
1980s by the experience of trying to build interfaces to more complex (expert) systems,
for instance for advice.
The presumption was that we need to model the user’s beliefs, goals, etc., to
ensure an appropriate system response. It is not enough to take an input’s “obvious”meaning. But in many cases, the system cannot get enough information about the
user’s knowledge, desires, or intentions to make reliable inferences about input
motivations. So it may be more sensible to adopt a conservative system response
strategy.
For example, suppose we have a travel inquiry system, with the user asking about
trains, as in Figure 8. The system, seeking the reason behind the user’s Can I travel to
Istanbul by train?, might hypothesize that the user wants train travel because he or she
</bodyText>
<figureCaption confidence="0.990422">
Figure 7
</figureCaption>
<page confidence="0.705537">
6
</page>
<note confidence="0.344834">
Spa¨rck Jones Some Points in a Time
</note>
<figureCaption confidence="0.991124">
Figure 8
</figureCaption>
<bodyText confidence="0.999872222222222">
believes that trains are fast and cheap and would therefore, using its own information
about what train travel is actually like, reply that it is slow and expensive. But the user
may in fact have quite different (of many possible) reasons for wanting to travel by
train, like wanting to see the scenery, and find the system’s response distinctly
inappropriate. The system would do much better simply by giving a straightforward
response, perhaps adding some immediately pertinent amplifying detail.
The basic NLP tools for tasks like this, at the sentence and local discourse level,
were improving all the time. But the suggestion that you don’t need to dig very deep in
language interpretation for useful task systems was indicative of a coming change.
In fact, there was an upheaval in the early 1990s like that of the early 1960s.
Language and information processing (LIP) in the 1960s had had incipient machine
text, and computers came on stream. In the 1990s we had enormously more machine
power, and bulk text came on stream. We now have the whole Web as a language data
source and arena for applications. But already in the early 1990s, more text and
processor power stimulated the community to look again, as not since the 1960s for
both practical and intellectual reasons, at statistical approaches to LIP. This was not
just for resources, e.g., finding lexical associations or building probabilistic grammars,
but for task systems, especially task systems responding to bulk text, like information
extraction and summarizing.
Both of these are challenges to language processing, but especially summarizing,
as for summarizing we need to understand large-scale discourse structure to do it, and
there are different forms or aspects of such structure to investigate. For example, we
can distinguish structure of a primarily linguistic type from communicative structure
and from structure referring to the world. The same text can be characterized in
different ways using these kinds of structure, as illustrated in Figure 9; and we can
exploit each of these structure types for summarizing, since the way they are
instantiated in the text can indicate more or less important content.
Thus considering the source text (even as abbreviated) shown in Figure 9, we can
identify a linguistic structure of parallel description, saying something, in similar style,
about biographies, about histories, and about novels. We can also see a communicative
structure of the form: Say X to motivate act Y. And there is a further structure to the
world being talked about, which characterizes books through their properties and
uses. Each of these structures conveys information, through its form and associated
content detail, about what is more or less important in the source text from the relevant
point of view, and can therefore be used for summarizing. Thus the linguistic structure
leads to a contrastive summary emphasizing key book-type features (LS), the
</bodyText>
<page confidence="0.991146">
7
</page>
<figure confidence="0.810312">
Computational Linguistics Volume 31, Number 1
</figure>
<figureCaption confidence="0.996715">
Figure 9
</figureCaption>
<bodyText confidence="0.99998372">
communicative structure to a justification for action summary (CS), and the world
structure to a simple descriptive summary (WS), without the presentational or func-
tional character of the other two. The three summaries have something in common,
because they all deal with what the text is about. They are also all different, but equally
convincing as summaries, albeit from their different points of view. (Of course this
illustration is a mere indicative sketch, where the reality would be far more complex.)
But the most obvious task to work on at the beginning of the 1990s was retrieval.
Could the established research methods scale up from thousands of items to select
relevant documents from millions? The TREC (Text REtrieval Conferences) evaluation
program (Voorhees and Harman, in press) was designed to answer this question, but
its scale and development, in a major, long-term activity under Donna Harman and
Ellen Voorhees, have had a far wider impact on the whole of LIP.
I was delighted to see that the Probabilistic Model, in work led by Stephen
Robertson, did just fine in TREC, combining tf with idf, as Salton had advocated, in a
robust way and incorporating query expansion in feedback, as originally suggested by
Rocchio in the 1960s (Rocchio 1966). Figure 10 shows how effective the statistical
approach to IR can be (using results taken from a later paper). For two versions of the
users’ information needs, minimal and amplified, the performance gains are similar,
though with richer starting requests they are, not suprisingly, larger. Performance for
the rock bottom baseline, unweighted terms with a best-match strategy, gives only 1
relevant item in the top 10 retrieved. Using tf and idf, with a suitable elaboration to
factor document length (dl) into the formula suggested by the theory, immediately
gives a very substantial improvement, so half the top 10 retrieved are relevant.
Feedback using only a small sample of known relevant documents gives a further gain
when the queries are expanded with terms from these documents. Finally, it may be
</bodyText>
<page confidence="0.994597">
8
</page>
<note confidence="0.687069">
Spa¨rck Jones Some Points in a Time
</note>
<figureCaption confidence="0.962846">
Figure 10
</figureCaption>
<bodyText confidence="0.998960580645161">
sufficient, with good starting requests, simply to assume that the best-matching docu-
ments in an initial pass are relevant, without invoking the user, to get a feedback gain.
These statistical techniques are very easy to apply, and the basic tf * idf with dl
scheme was taken from our research for the first serious Web search engine, AltaVista,
though 25 years after my idf paper, showing how long research can take for timely
exploitation even in the rapidly moving IT world.
But the TREC program became more than just a regular document retrieval
project. It has expanded over data types (e.g., to spoken documents) and into related
tasks like question answering. This is important, because it has brought retrieval in
from the cold to the NLP community, and encouraged a more generic view of it.
It is also, and more, important because it has shown how powerful the statistical
approach to LIP is, and how widely it can be applied. Progress with statistical speech
recognition and machine learning have helped in this, but retrieval has been vital
because it engages with text content and has exported ideas about ways of handling
this elsewhere.
Over the 1990s there has been a real growth in the use of statistics for LIP tasks.
This has included some research on automatic lexical classification, i.e., on resource
building, though ironically the main, widely used generic classification, WordNet, is
manually built. But there has been far more emphasis on statistically determined word
importance in text and also on statistically justified word sequences, leaving class
relations implicit. Word importance, and the associated extractive view of text
meaning, has turned out to be very valuable.
With rough tasks, like retrieval, you can do the whole job statistically, and well.
With other tasks you can do a crude but useful job, wholly or largely statistically, for
example by combining statistics with lightweight parsing in extractive summarizing
or question answering. Using statistics for summarizing goes back to Luhn (1958).
Figure 11 shows Luhn’s “auto-abstract”for the 1958 conference paper with which I
began. Concatenating source sentences that have been selected because they contain
statistically important words does not make for readable abstracts. But the important
fact about the example is that the mechanism for selecting sentences has chosen ones
containing thesaurus, which does not occur in the paper title but is the focus of the text
</bodyText>
<page confidence="0.96705">
9
</page>
<figure confidence="0.82998">
Computational Linguistics Volume 31, Number 1
</figure>
<figureCaption confidence="0.994316">
Figure 11
</figureCaption>
<bodyText confidence="0.999388307692307">
argument. Modern techniques can combine statistical methods for identifying key
content words with light sentence processing, e.g., to prune peripheral material, for
more compact and coherent summaries.
Further, even with more sophisticated uses of NLP, for instance for question
answering, searching bulk data means you need preliminary retrieval, which can be
statistical, to find text passages likely to contain the answer, and may benefit from
exploiting other statistical data, e.g., to identify related words or check proposed word
relationships. In all of this, retrieval has supplied not only a general view of text, but
specific techniques, notably tf * idf–style weighting.
Retrieval has also played a major role in research in the 1990s through its
experience of system evaluation. This has not been just as an exporter of the often-
misused notions of recall and precision, but through its emphasis on careful
methodology and on evaluation geared to the task’s function in its context. Thus
retrieval is not about indexing per se, but about delivering relevant documents to
users. Figure 12 sketches, in a very abbreviated form drawn from a fuller illustration,
the kind of decompositional analysis required to design and conduct an evaluation of a
task system operating in some context, from a particular point of view.
Thus if we imagine having a natural language interface to a house design system,
we suppose that we want to discover whether this is effective as a training device for
architecture students, and choose comparison with a menu system as the way of doing
this. These decisions, and others, form part of the remit for the evaluation. They have
to be fleshed out in the detailed design, which has to take account of environment
factors, that is, of the values for environmental variables, like the difficulty of the
planning problems set for students to tackle with the system, and of the settings for
the system parameters, notably in this case the alternative interfaces being studied. The
design also covers the choice of generic performance measures, e.g., output plan
</bodyText>
<page confidence="0.991335">
10
</page>
<note confidence="0.795499">
Spa¨rck Jones Some Points in a Time
</note>
<figureCaption confidence="0.978116">
Figure 12
</figureCaption>
<bodyText confidence="0.9990354">
quality, and the particular ways of measuring this, along with the specification of the
test data and of the procedure for carrying out the evaluation, e.g., choosing the
planning problem sample, the students, etc.
As even this brief outline suggests, proper evaluation is a complex and challenging
business. It implies, in particular, that we need to make a very rigorous “deconstructive”analysis of all the factors that affect the system being tested, as, for example, in the
summarizing case sketched in Figure 13.
Here we have a particular purpose in summarizing, implying a specific function,
namely, to alert, and a specific audience (i.e., readership), namely, the police; and in
practice there are other purpose factors as well. Summarizing to serve this purpose has
to take a whole range of input source document factors into account, like their subject
domain and text form, in this case, weather reports. The purpose imposes constraints
on output summaries, but there are still specific choices to be made about output
factors like the language style and summary length, etc., which we suppose here leads
to the production of very brief items in telegraphese. A breakdown of all the factors
affecting a system is essential to guide evaluation.
</bodyText>
<figureCaption confidence="0.639328">
Figure 13
</figureCaption>
<page confidence="0.99055">
11
</page>
<note confidence="0.594533">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.999307117647059">
External developments, in particular the huge growth of miscellaneous stuff on
the Web, and the arrival of end users bypassing professional intermediaries, have
encouraged the simple surface type of approach to LIP, as a general trend. But retrieval
has been significant because it emphasizes the real status of text: Language meaning is
behind text because it is also on the text surface. IR has also emphasized the fact that at
some point with LIP tasks, maybe not locally but somewhere along the line, the human
user has to interpret and assess text content. This is not because systems are deficient,
but because natural language systems are for humans. For example, humans have to
interpret and assess the answers to questions, even when these are “correct”in some
obvious sense, in just the same way that they have to interpret and assess, albeit more
elaborately and with more inference, a response text where the answer may be only
approximate, qualified, or too deeply embedded for system unpacking.
There are many intellectual challenges in understanding what one is doing with
language and language use under the statistical approach, compared with the natural
and easy rationales one can give for grammars and parsing. For example, how does
surface word behavior relate to forms of discourse structure? We also need to explain,
in a principled way, what happens when we combine statistical and syntactic (or
semantic or pragmatic) description and processing. Figure 14 shows just some of the
questions to answer, for example, “How do data patterns relate to linguistic actions?”or “How do data units match linguistic units?”, drawn from a much longer list.
We need to explain what is happening with statistics in generic language
description and processing, for resources and operations that may apply across tasks.
But we equally have to do it for task systems. The spread of statistics is currently
making this very interesting for purely statistical systems (though we must also do
it for hybrid strategies). One can apply Bayes’ theorem, as the classical statistical
tool, to anything. But even if this works very well in practice, you need to say what the
grounding model for the task is. With a properly grounded formal model for the
task that explains why the statistics work, you can hope to push further than with
the super-abstract account, or the purely ad hoc apparatus, that statistics can easily
supply.
This is very much an issue for the presently fashionable use of so-called Language
Modeling. The Probabilistic Model for retrieval is grounded in the task, that is, it
relates the term distribution data to the probability of document relevance. Language
Modeling for speech transcription, which is where the approach came from, has a
convincing grounding model in the idea of recovering a corrupted signal. But the
</bodyText>
<figureCaption confidence="0.674804">
Figure 14
</figureCaption>
<page confidence="0.908714">
12
</page>
<note confidence="0.562072">
Spa¨rck Jones Some Points in a Time
</note>
<figureCaption confidence="0.9803">
Figure 15
</figureCaption>
<bodyText confidence="0.999309888888889">
recovery idea is much less plausible when taken as a justification for Language
Modeling for summarizing, translation, or other tasks, under the interpretations
shown in Figure 15. Is summarizing no more than recovering the original crisp few-
liner from a mass of verbiage? More strikingly, is translating Shakespeare into
Spanish just recovering, from the defective English, the Spanish in which he originally
wrote?
As this current, active research implies, there is a lot of challenging work to do. I
am very happy to have been in at the beginning of automated LIP, I am happy to be
still in it now, and I am happy to have plenty more to look forward to.
In conclusion, I would first like to thank all my research students and assistants,
from whom I have learned so much.
Then, referring again to the author names on my first paper, I want especially to
thank Margaret Masterman who employed me at the start when the only qualification
for research in LIP I had was a year reading philosophy (though this was a good
qualification in fact). But I want most of all to thank my late husband Roger Needham,
not only because we worked and published together at particular times, but because I
could always talk to him about my research, and he always encouraged me.
Thank you again.
</bodyText>
<sectionHeader confidence="0.989028" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.969677774193549">
Items cited in the figures (in citation
order)
Masterman, Margaret, Roger M. Needham,
and Karen Spa¨rck Jones. 1959. The analogy
between mechanical translation and library
retrieval. In Proceedings of the International
Conference on Scientific Information (1958),
National Academy of Sciences–National
Research Council, Washington, DC, Vol. 2,
pages 917–935.
Spa¨rck Jones, Karen. 1964. Synonymy and
Semantic Classification. Ph.D. thesis,
University of Cambridge: Report ML
170, Cambridge Language Research
Unit. (Edinburgh University Press,
Edinburgh, 1986.)
Spa¨rck Jones, Karen. 1967. A small
semantic classification experiment
using cooccurrence data. Report ML
196, Cambridge Language Research
Unit, Cambridge.
Spa¨rck Jones, Karen. 1967. Notes on
semantic discourse structure. Report
SP-2714, System Development Corporation,
Santa Monica, CA.
Spa¨rck Jones, Karen. 1972. A statistical
interpretation of term specificity and
its application in retrieval. Journal of
Documentation, 28:11–21.
Robertson, Stephen E. and Karen Spa¨rck
Jones. 1976. Relevance weighting of search
</reference>
<page confidence="0.995306">
13
</page>
<note confidence="0.557281">
Computational Linguistics Volume 31, Number 1
</note>
<reference confidence="0.999179329268293">
terms. Journal of the American Society for
Information Science, 27:129–146.
Spa¨rck Jones, Karen. 1979. Search term
relevance weighting given little relevance
information. Journal of Documentation,
35:30–48.
Spa¨rck Jones, Karen. 1983. Shifting meaning
representations. In IJCAI-83, Proceedings of
the Eighth International Joint Conference on
Artificial Intelligence, Karlsruhe, Germany,
pages 621–623.
Spa¨rck Jones, Karen. 1991. Tailoring
output to the user: What does user
modelling in generation mean? In C. L.
Paris, W. R. Swartout, and W. C. Mann,
editors, Natural Language Generation in
Artificial Intelligence and Computational
Linguistics. Kluwer, Dordrecht,
pages 201–225.
Spa¨rck Jones, Karen. 1995. Discourse
modelling for automatic summarising.
In Eva Hajicova, Miroslav Cervenka,
Oldrich Leska, and Petr Sgall, editors,
Travaux du Cercle Linguistique de Prague
(Prague Linguistic Circle Papers), New
Series, Volume 1. John Benjamins,
Amsterdam, pages 201–227.
Spa¨rck Jones, Karen and Julia R. Galliers. 1996.
Evaluating Natural Language Processing
Systems. Lecture Notes in Artificial
Intelligence 1083. Springer-Verlag, Berlin.
Spa¨rck Jones, Karen. 1999. Automatic
summarising: Factors and directions.
In I. Mani and M. T. Maybury,
editors, Advances in Automatic Text
Summarisation, MIT Press, Cambridge, MA,
pages 112.
Spa¨rck Jones, Karen, Gerald Gazdar, and
Roger M. Needham, editors. 2000.
Computers, language and speech:
Formal theories and statistical data.
Philosophical Transactions of the Royal
Society of London, Series A, 358(1769):
1225–1431.
Spa¨rck Jones, Karen, Stephen Walker,
and Stephen E. Robertson. 2000. A
probabilistic model of information
retrieval: Development and comparative
experiments (Parts 1 and 2). Information
Processing and Management, 36(6):779–840.
Spa¨rck Jones, Karen. 2004. Language
modelling’s generative model: Is it
rational? Working paper, Computer
Laboratory, University of Cambridge.
Other items cited in the paper
Luhn, Hans Peter. 1957. A statistical
approach to mechanised literature
searching. IBM Journal of Research and
Development, 1(4):309–317.
Luhn, Hans Peter. 1958. An experiment
in auto-abstracting. Auto-abstracts of
Area 5 conference papers. International
Conference on Scientific Information,
Washington, DC, November 16–21,
1958. IBM Research Center, Yorktown
Heights, NY. (Reprinted in C. K. Schultz,
editor, H. P. Luhn: Pioneer of Information
Science, Spartan Books, New York, 1968,
pages 145–163.)
Rocchio, Joseph J. 1966. Document Retrieval
Systems—Optimization and Evaluation.
Report ISR-10, Computation Laboratory,
Harvard University.
Voorhees, Ellen M. and Donna K. Harman,
editors. In press. TREC: An Experiment
and Evaluation in Information Retrieval.
MIT Press, Cambridge, MA.
Woods, William A. 1978. Semantics and
quantification in natural language
question answering. In M. Yovits, editor,
Advances in Computers, Vol. 17. Academic
Press, New York, pages 2–64.
</reference>
<page confidence="0.999269">
14
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.7891065">ACL Lifetime Achievement Award Points in a</title>
<author confidence="0.797265">Spa¨rck</author>
<affiliation confidence="0.950782">University of Cambridge</affiliation>
<abstract confidence="0.97994303030303">This article offers a personal perspective on the development of language and information processing over the last half century, focusing on the use of statistical methods. Introduced, with computers, in the 1950s, these have not always been highly regarded, but were revived in the 1990s. They have proved effective in more ways than might have been expected, and encourage new thinking about what language and information processing involve. First, to say how much I appreciate the completely unexpected honour of this award, and to thank the ACL for it. I want to look at one line, or thread, in natural language processing (NLP) research: how it began, what happened to it, what it suggests we need to investigate now. I shall take some papers of my own as pegs to hang the story on, but without making any claims for particular merit in these papers. My first paper, “The analogy between MT and IR,’’ with Margaret Masterman and Roger Needham, was for a conference in 1958. The analogy it proclaimed was in the need for a thesaurus, that is, a semantic classification. Machine translation (MT) and information retrieval (IR) are different tasks in their granularity and in the role of syntax. But we argued that both need a means of finding common concepts behind surface words, so we at once identify text content and word senses. We took Roget’s such a tool in our MT experiments (with punched cards), where we exploited the redundancy that text always has to select class labels, that indicate senses, for words. The essential idea is illustrated, in extremely simple form (as with all my in Figure 1. If we have to translate farmer cultivates the field, a range of senses including may well have different target language equivalents, the fact that the general concept AGRICULTURE underlies each the sense But we found in our research that existing thesauruses, like Roget’s, were not wholly satisfactory, for example through missing senses; and we wanted to build a better one, ideally automatically. The natural way to do this, the obverse of the way the thesaurus would be applied once it was constructed, was by using text distribution data for words and applying statistical classification methods to these data. There were of course no corpora available then, so in my thesis work I finessed getting the input data for classification by taking dictionary definitions, which often of close synonym sets, as showing the most primitive and minimal shared-</abstract>
<address confidence="0.558161">Computer Laboratory, William Gates Building, JJ Thomson Avenue, Cambridge CB3 0FD, UK.</address>
<email confidence="0.715442">E-mail:ksj@cl.cam.ac.uk.</email>
<note confidence="0.8818146">1 This article is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2004. The figures provided in the article reproduce the slides used for the talk. Association for Computational Linguistics Computational Linguistics Volume 31, Number 1 Figure 1</note>
<abstract confidence="0.993419991780822">context behavior for the words concerned. The words in such a set could be taken as mutually substitutible in some context, which could be checked by reference to the text examples given in the dictionary. This mimicked what could in principle be delivered by a corpus which showed one word or another in the set occurring in the same text context. I then showed that one could apply general classification methods, notably Roger Needham’s theory of clumps, to find larger classes of words with similar behavior. For example, suppose we have synonym sets, or “rows,’’ as shown in Figure 2, that animation movement, activity movement business, briskness business each representing substitutive, i.e., synonymous, the words concerned. We can then derive classes of words with similar uses by exploiting the recurrence of the same word symbols across different rows: The senses of a word in different rows are distinct senses, but they can legitimately be assumed to be semantically related simply through the fact that they are uses of the same word sign. In subsequent work, Kenneth Harper and I showed that it was possible to derive classes from actual word occurrences and cooccurences in text, as illustrated in Figure 3. Given a (hand-) parsed Russian scientific text corpus, we investigated the collocation behavior of 40 Russian nouns (here represented by English equivalents) in governor/dependent relations, and grouped them by their shared collocates. Somewhat suprisingly, for such a small corpus, we were able to extract semantically Figure 2 2 Spa¨rck Jones Some Points in a Time Figure 3 classes, like depth width measurement study investigation etc. as shown in Figure 3. These tests were very small in absolute terms (though less so relative to quite recent research than might be expected). But the much more serious problem, for evaluation, was that there was no MT system you could plug a classification into to test it. Different general classification methods can give different, but equally plausible, classifications, so you need an application context to choose among them, as well as to check that the generic idea of such a sense selection apparatus is sound. We also needed a theory of discourse structure, above the sentence, to support and constrain the use of a classification. MT then was sentence-limited, but in general you cannot rely on individual sentences for all the information you need for sense resolution. Figure 4 shows a very rudimentary attempt to model discourse structure using topic/comment relations across sentences, shown using T or C with numbers for concepts. Topic/content linkage patterns could show where to look for information from other words to help select the sense of a word in a particular location. For instance, in selecting a sense for a given topic word, concepts that were shared with previous topic words, or recent comment words, would be preferred. In the illustration same word is repeated, but it is easy to imagine, for example, that occur one point and another, so selection would involve semantic classes. Figure 4 3 Computational Linguistics Volume 31, Number 1 But work like this was only the sketchiest beginning as far as MT was concerned. Retrieval was a much better initial test field for semantic classification, because you can get index keys for documents without interference from the need for sentence parses. At the same time, you have to consider the document as a whole. Retrieval, for the growing technical literature, was also a very pressing practical task, so people were developing evaluation methods for different indexing and searching strategies. Researchers wanted to show that a statistical classification, based on term distributions across documents, was effective as a lexical substitution device, helping recall (an idea first published by Luhn [1957]). The task function for classification in IR was important because it forced thinking about the choice of classification model, for example should classes be overlapping or exclusive? Deriving classes is a major step beyond computing word pair associations. It needs sound and appropriate, as well as computable, approaches, desiderata that are not always recognized even now. We began to test automatic classification for retrieval with a small collection built by Cyril Cleverdon and also used by Gerard Salton (to whom, as long-standing colleagues, I owe a great deal). The work started well, and I continued to work in retrieval because, for everyone in NLP, the world turned cold in the later 1960s under the combined effects of the assaults on MT research and the fashion for Chomskyan linguistics. But it turned out to be much more complicated than expected to get term classes to do better than simple term matching in retrieving relevant documents. Though I could get benefits from classification for my first collection, when I tried others I could not get anywhere much. Trying to understand why, I realized that term occurrence frequency, in the document file as a whole, not just cooccurrence frequency, was much more important than anyone had recognized. For retrieval, you need to consider the discriminative as well as the descriptive value of a term, so you do not want to increase the matching power of a common term. As there are few relevant documents for a request among the many in the file, any term that occurs in a lot of documents is bound to retrieve more non-relevant than relevant ones. Allowing common terms to substitute for others compounds the problem. observation led to the proposal for term weighting, later called that turned out to be a big win: Terms should be weighted inversely by the number of documents in which they occur. The particular function, shown in Figure 5, is extremely, and pleasingly, simple but has been shown to be generally useful. Salton the value of that is, the value of term frequency within a single document, where the greater the frequency, the more important the term is for the Figure 5 4 Spa¨rck Jones Some Points in a Time document. With both ideas about weighting, researchers were beginning to get a better idea of the significance of a word’s text statistics. This was only within IR. Computational linguistics (CL) was focused on sentence grammar, artificial intelligence on world knowledge, and text interpretation from that point of view. in IR, the weighting idea had yet more to it. tries to use general term frequency data, for a whole collection, to predict specific query relevance. If you have some past relevance information, you can do better. As with classification, it is tricky to get the formula right. I went through four successive versions, in an experiment/theory interaction with Stephen Robertson, at the beginning of a long and valued collaboration. But the outcome was very satisfying, not just for the performance results, but because Robertson’s Probabilistic Model for retrieval relates the task to the text data in a convincing way. Figure 6 shows how effective this relevance weighting approach is, even for a difficult later data set using only document titles though rather rich requests. Even with only a few known relevant items to supply information about query terms in an iterative search, performance is better than with alone, and when there are many known relevant to exploit, performance is strikingly better. The results illustrate the value of having training data for what would now be called a machine-learning strategy. Full relevance information can also be used to search retrospectively, to define a rational upper bound for performance. This work in the 1970s looked very good, but there was a practical challenge in scaling experiments up to a realistic level, because testing needs reference data in the form of relevant documents for requests, and getting this for large collections is expensive. Operational services were not interested, partly because they believed in older conventional methods and partly because they had genuine other concerns like efficiency. The CL community was addressing the quite different goal of building natural language front ends to databases, which seemed a much more important project, addressing a task that needed proper syntactic and semantic interpretation. I thought it was an interestingly distinct challenge and joined in. Natural language access to databases, converting a text question into, for example, SQL, looked like a tractable task at the time. But it turned out much less so than Figure 6 5 Computational Linguistics Volume 31, Number 1 expected because it needs a large, rich domain model to bridge the gap between the user’s free input and the particular data model, and also to trap the user’s unwitting excursions outside the database boundary. The difficulties of doing data access well also emphasized how narrow and limited database query, as normally implemented, actually is as a form of information access. It is much more reasonable to envisage a family of access modes to different types of information giving the user whatever can be found, in one way or another, in text, data, or even knowledge bases, as these happen to be available. A system would take the user’s single input and apply different types of interpretation at different levels, within a single framework. It would treat a query deliberately as something to be unpacked from different points of view, rather than in a preferred mode, with backup defaults, as in LUNAR (Woods 1978). 7 illustrates the idea, very simplistically. Thus for the user’s input, green parts?, we have an initial interpretation, say as a dependency tree. This can be subjected to further processing as a whole to construct a formal database query or taken simply as a base from which variant phrasal terms for text searching can be derived. Alternatively, if we consider the input text itself as its own, direct, representation, we can use this representation as a source for simple retrieval terms. Other versions of the input are drawn from different, perhaps successively deeper, representations as needed for particular tasks. Treating the text itself as a representation emphasizes the point that document retrieval is one proper task in its own right, among several, not a poor substitute for “true”question answering. The idea that one can usefully work on the linguistic surface was reinforced in the 1980s by the experience of trying to build interfaces to more complex (expert) systems, for instance for advice. The presumption was that we need to model the user’s beliefs, goals, etc., to an appropriate system response. It is not enough to take an input’s But in many cases, the system cannot get enough information about the user’s knowledge, desires, or intentions to make reliable inferences about input motivations. So it may be more sensible to adopt a conservative system response strategy. For example, suppose we have a travel inquiry system, with the user asking about as in Figure 8. The system, seeking the reason behind the user’s I travel to by train?, hypothesize that the user wants train travel because he or she Figure 7 6 Spa¨rck Jones Some Points in a Time Figure 8 believes that trains are fast and cheap and would therefore, using its own information about what train travel is actually like, reply that it is slow and expensive. But the user may in fact have quite different (of many possible) reasons for wanting to travel by train, like wanting to see the scenery, and find the system’s response distinctly inappropriate. The system would do much better simply by giving a straightforward response, perhaps adding some immediately pertinent amplifying detail. The basic NLP tools for tasks like this, at the sentence and local discourse level, were improving all the time. But the suggestion that you don’t need to dig very deep in language interpretation for useful task systems was indicative of a coming change. In fact, there was an upheaval in the early 1990s like that of the early 1960s. Language and information processing (LIP) in the 1960s had had incipient machine text, and computers came on stream. In the 1990s we had enormously more machine power, and bulk text came on stream. We now have the whole Web as a language data source and arena for applications. But already in the early 1990s, more text and processor power stimulated the community to look again, as not since the 1960s for both practical and intellectual reasons, at statistical approaches to LIP. This was not just for resources, e.g., finding lexical associations or building probabilistic grammars, but for task systems, especially task systems responding to bulk text, like information extraction and summarizing. Both of these are challenges to language processing, but especially summarizing, as for summarizing we need to understand large-scale discourse structure to do it, and there are different forms or aspects of such structure to investigate. For example, we can distinguish structure of a primarily linguistic type from communicative structure and from structure referring to the world. The same text can be characterized in different ways using these kinds of structure, as illustrated in Figure 9; and we can exploit each of these structure types for summarizing, since the way they are instantiated in the text can indicate more or less important content. Thus considering the source text (even as abbreviated) shown in Figure 9, we can identify a linguistic structure of parallel description, saying something, in similar style, about biographies, about histories, and about novels. We can also see a communicative of the form: Say motivate act And there is a further structure to the world being talked about, which characterizes books through their properties and uses. Each of these structures conveys information, through its form and associated content detail, about what is more or less important in the source text from the relevant point of view, and can therefore be used for summarizing. Thus the linguistic structure leads to a contrastive summary emphasizing key book-type features (LS), the 7 Computational Linguistics Volume 31, Number 1 Figure 9 communicative structure to a justification for action summary (CS), and the world structure to a simple descriptive summary (WS), without the presentational or functional character of the other two. The three summaries have something in common, because they all deal with what the text is about. They are also all different, but equally convincing as summaries, albeit from their different points of view. (Of course this illustration is a mere indicative sketch, where the reality would be far more complex.) But the most obvious task to work on at the beginning of the 1990s was retrieval. Could the established research methods scale up from thousands of items to select relevant documents from millions? The TREC (Text REtrieval Conferences) evaluation program (Voorhees and Harman, in press) was designed to answer this question, but its scale and development, in a major, long-term activity under Donna Harman and Ellen Voorhees, have had a far wider impact on the whole of LIP. I was delighted to see that the Probabilistic Model, in work led by Stephen did just fine in TREC, combining as Salton had advocated, in a robust way and incorporating query expansion in feedback, as originally suggested by Rocchio in the 1960s (Rocchio 1966). Figure 10 shows how effective the statistical approach to IR can be (using results taken from a later paper). For two versions of the users’ information needs, minimal and amplified, the performance gains are similar, though with richer starting requests they are, not suprisingly, larger. Performance for the rock bottom baseline, unweighted terms with a best-match strategy, gives only 1 item in the top 10 retrieved. Using with a suitable elaboration to document length into the formula suggested by the theory, immediately gives a very substantial improvement, so half the top 10 retrieved are relevant. Feedback using only a small sample of known relevant documents gives a further gain when the queries are expanded with terms from these documents. Finally, it may be 8 Spa¨rck Jones Some Points in a Time Figure 10 with good starting requests, simply to the best-matching documents in an initial pass are relevant, without invoking the user, to get a feedback gain. statistical techniques are very easy to apply, and the basic with dl scheme was taken from our research for the first serious Web search engine, AltaVista, 25 years after my showing how long research can take for timely exploitation even in the rapidly moving IT world. But the TREC program became more than just a regular document retrieval project. It has expanded over data types (e.g., to spoken documents) and into related tasks like question answering. This is important, because it has brought retrieval in from the cold to the NLP community, and encouraged a more generic view of it. It is also, and more, important because it has shown how powerful the statistical approach to LIP is, and how widely it can be applied. Progress with statistical speech recognition and machine learning have helped in this, but retrieval has been vital because it engages with text content and has exported ideas about ways of handling this elsewhere. Over the 1990s there has been a real growth in the use of statistics for LIP tasks. This has included some research on automatic lexical classification, i.e., on resource building, though ironically the main, widely used generic classification, WordNet, is manually built. But there has been far more emphasis on statistically determined word importance in text and also on statistically justified word sequences, leaving class relations implicit. Word importance, and the associated extractive view of text meaning, has turned out to be very valuable. With rough tasks, like retrieval, you can do the whole job statistically, and well. With other tasks you can do a crude but useful job, wholly or largely statistically, for example by combining statistics with lightweight parsing in extractive summarizing or question answering. Using statistics for summarizing goes back to Luhn (1958). Figure 11 shows Luhn’s “auto-abstract”for the 1958 conference paper with which I began. Concatenating source sentences that have been selected because they contain statistically important words does not make for readable abstracts. But the important fact about the example is that the mechanism for selecting sentences has chosen ones which does not occur in the paper title but is the focus of the text 9 Computational Linguistics Volume 31, Number 1 Figure 11 argument. Modern techniques can combine statistical methods for identifying key content words with light sentence processing, e.g., to prune peripheral material, for more compact and coherent summaries. Further, even with more sophisticated uses of NLP, for instance for question answering, searching bulk data means you need preliminary retrieval, which can be statistical, to find text passages likely to contain the answer, and may benefit from exploiting other statistical data, e.g., to identify related words or check proposed word relationships. In all of this, retrieval has supplied not only a general view of text, but techniques, notably weighting. Retrieval has also played a major role in research in the 1990s through its experience of system evaluation. This has not been just as an exporter of the oftenmisused notions of recall and precision, but through its emphasis on careful methodology and on evaluation geared to the task’s function in its context. Thus retrieval is not about indexing per se, but about delivering relevant documents to users. Figure 12 sketches, in a very abbreviated form drawn from a fuller illustration, the kind of decompositional analysis required to design and conduct an evaluation of a task system operating in some context, from a particular point of view. Thus if we imagine having a natural language interface to a house design system, we suppose that we want to discover whether this is effective as a training device for architecture students, and choose comparison with a menu system as the way of doing this. These decisions, and others, form part of the remit for the evaluation. They have to be fleshed out in the detailed design, which has to take account of environment factors, that is, of the values for environmental variables, like the difficulty of the planning problems set for students to tackle with the system, and of the settings for the system parameters, notably in this case the alternative interfaces being studied. The design also covers the choice of generic performance measures, e.g., output plan 10 Spa¨rck Jones Some Points in a Time Figure 12 quality, and the particular ways of measuring this, along with the specification of the test data and of the procedure for carrying out the evaluation, e.g., choosing the planning problem sample, the students, etc. As even this brief outline suggests, proper evaluation is a complex and challenging business. It implies, in particular, that we need to make a very rigorous “deconstructive”analysis of all the factors that affect the system being tested, as, for example, in the summarizing case sketched in Figure 13. Here we have a particular purpose in summarizing, implying a specific function, namely, to alert, and a specific audience (i.e., readership), namely, the police; and in practice there are other purpose factors as well. Summarizing to serve this purpose has to take a whole range of input source document factors into account, like their subject domain and text form, in this case, weather reports. The purpose imposes constraints on output summaries, but there are still specific choices to be made about output factors like the language style and summary length, etc., which we suppose here leads to the production of very brief items in telegraphese. A breakdown of all the factors affecting a system is essential to guide evaluation. Figure 13 11 Computational Linguistics Volume 31, Number 1 External developments, in particular the huge growth of miscellaneous stuff on the Web, and the arrival of end users bypassing professional intermediaries, have encouraged the simple surface type of approach to LIP, as a general trend. But retrieval has been significant because it emphasizes the real status of text: Language meaning is behind text because it is also on the text surface. IR has also emphasized the fact that at some point with LIP tasks, maybe not locally but somewhere along the line, the human user has to interpret and assess text content. This is not because systems are deficient, but because natural language systems are for humans. For example, humans have to interpret and assess the answers to questions, even when these are “correct”in some obvious sense, in just the same way that they have to interpret and assess, albeit more elaborately and with more inference, a response text where the answer may be only approximate, qualified, or too deeply embedded for system unpacking. There are many intellectual challenges in understanding what one is doing with language and language use under the statistical approach, compared with the natural and easy rationales one can give for grammars and parsing. For example, how does surface word behavior relate to forms of discourse structure? We also need to explain, in a principled way, what happens when we combine statistical and syntactic (or semantic or pragmatic) description and processing. Figure 14 shows just some of the questions to answer, for example, “How do data patterns relate to linguistic actions?”or “How do data units match linguistic units?”, drawn from a much longer list. We need to explain what is happening with statistics in generic language description and processing, for resources and operations that may apply across tasks. But we equally have to do it for task systems. The spread of statistics is currently making this very interesting for purely statistical systems (though we must also do it for hybrid strategies). One can apply Bayes’ theorem, as the classical statistical tool, to anything. But even if this works very well in practice, you need to say what the grounding model for the task is. With a properly grounded formal model for the task that explains why the statistics work, you can hope to push further than with the super-abstract account, or the purely ad hoc apparatus, that statistics can easily supply. This is very much an issue for the presently fashionable use of so-called Language Modeling. The Probabilistic Model for retrieval is grounded in the task, that is, it relates the term distribution data to the probability of document relevance. Language Modeling for speech transcription, which is where the approach came from, has a convincing grounding model in the idea of recovering a corrupted signal. But the Figure 14 12 Spa¨rck Jones Some Points in a Time Figure 15 recovery idea is much less plausible when taken as a justification for Language Modeling for summarizing, translation, or other tasks, under the interpretations shown in Figure 15. Is summarizing no more than recovering the original crisp fewliner from a mass of verbiage? More strikingly, is translating Shakespeare into Spanish just recovering, from the defective English, the Spanish in which he originally wrote? As this current, active research implies, there is a lot of challenging work to do. I am very happy to have been in at the beginning of automated LIP, I am happy to be still in it now, and I am happy to have plenty more to look forward to. In conclusion, I would first like to thank all my research students and assistants, from whom I have learned so much. Then, referring again to the author names on my first paper, I want especially to thank Margaret Masterman who employed me at the start when the only qualification for research in LIP I had was a year reading philosophy (though this was a good qualification in fact). But I want most of all to thank my late husband Roger Needham, not only because we worked and published together at particular times, but because I could always talk to him about my research, and he always encouraged me. Thank you again. References Items cited in the figures (in citation order)</abstract>
<author confidence="0.7843315">The analogy</author>
<affiliation confidence="0.76136225">between mechanical translation and library In of the International on Scientific Information National Academy of Sciences–National</affiliation>
<address confidence="0.498348">Research Council, Washington, DC, Vol. 2, pages 917–935. Jones, Karen. 1964. and</address>
<author confidence="0.547767">Ph D thesis</author>
<affiliation confidence="0.995152">University of Cambridge: Report ML</affiliation>
<address confidence="0.917842">170, Cambridge Language Research</address>
<affiliation confidence="0.989843">Unit. (Edinburgh University Press,</affiliation>
<address confidence="0.96919">Edinburgh, 1986.</address>
<abstract confidence="0.836156333333333">Spa¨rck Jones, Karen. 1967. A small semantic classification experiment using cooccurrence data. Report ML</abstract>
<address confidence="0.701961333333333">196, Cambridge Language Research Unit, Cambridge. Spa¨rck Jones, Karen. 1967. Notes on</address>
<affiliation confidence="0.6328935">semantic discourse structure. Report SP-2714, System Development Corporation,</affiliation>
<address confidence="0.671204">Santa Monica, CA. Spa¨rck Jones, Karen. 1972. A statistical</address>
<abstract confidence="0.921583904761905">interpretation of term specificity and application in retrieval. of 28:11–21. Robertson, Stephen E. and Karen Spa¨rck Jones. 1976. Relevance weighting of search 13 Computational Linguistics Volume 31, Number 1 of the American Society for 27:129–146. Spa¨rck Jones, Karen. 1979. Search term relevance weighting given little relevance of 35:30–48. Spa¨rck Jones, Karen. 1983. Shifting meaning In Proceedings of the Eighth International Joint Conference on Intelligence, Germany, pages 621–623. Spa¨rck Jones, Karen. 1991. Tailoring output to the user: What does user modelling in generation mean? In C. L.</abstract>
<author confidence="0.561148">W R Swartout Paris</author>
<author confidence="0.561148">W C Mann</author>
<affiliation confidence="0.6874955">Language Generation in Artificial Intelligence and Computational</affiliation>
<note confidence="0.727351454545455">Kluwer, Dordrecht, pages 201–225. Spa¨rck Jones, Karen. 1995. Discourse modelling for automatic summarising. In Eva Hajicova, Miroslav Cervenka, Oldrich Leska, and Petr Sgall, editors, Travaux du Cercle Linguistique de Prague (Prague Linguistic Circle Papers), New Series, Volume 1. John Benjamins, Amsterdam, pages 201–227. Spa¨rck Jones, Karen and Julia R. Galliers. 1996.</note>
<title confidence="0.9299185">Evaluating Natural Language Processing Lecture Notes in Artificial</title>
<note confidence="0.808521933333333">Intelligence 1083. Springer-Verlag, Berlin. Spa¨rck Jones, Karen. 1999. Automatic summarising: Factors and directions. In I. Mani and M. T. Maybury, in Automatic Text Press, Cambridge, MA, pages 112. Spa¨rck Jones, Karen, Gerald Gazdar, and Roger M. Needham, editors. 2000. Computers, language and speech: Formal theories and statistical data. Philosophical Transactions of the Royal of London, A, 358(1769): 1225–1431. Spa¨rck Jones, Karen, Stephen Walker,</note>
<abstract confidence="0.87263375">and Stephen E. Robertson. 2000. A probabilistic model of information retrieval: Development and comparative (Parts 1 and 2). and Management, Spa¨rck Jones, Karen. 2004. Language modelling’s generative model: Is it rational? Working paper, Computer</abstract>
<affiliation confidence="0.367079">Laboratory, University of Cambridge.</affiliation>
<note confidence="0.668915647058823">Other items cited in the paper Luhn, Hans Peter. 1957. A statistical approach to mechanised literature Journal of Research and Luhn, Hans Peter. 1958. An experiment in auto-abstracting. Auto-abstracts of Area 5 conference papers. International Conference on Scientific Information, Washington, DC, November 16–21, 1958. IBM Research Center, Yorktown Heights, NY. (Reprinted in C. K. Schultz, P. Luhn: Pioneer of Information Books, New York, 1968, pages 145–163.) Joseph J. 1966. Retrieval Systems—Optimization and Evaluation. Report ISR-10, Computation Laboratory,</note>
<affiliation confidence="0.945541">Harvard University.</affiliation>
<author confidence="0.423614">Ellen M Voorhees</author>
<author confidence="0.423614">Donna K Harman</author>
<note confidence="0.881572">In press. An Experiment Evaluation in Information MIT Press, Cambridge, MA. Woods, William A. 1978. Semantics and quantification in natural language question answering. In M. Yovits, editor, in Computers, 17. Academic Press, New York, pages 2–64.</note>
<intro confidence="0.539795">14</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Items cited in the figures (in citation order)</title>
<marker></marker>
<rawString>Items cited in the figures (in citation order)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Masterman</author>
<author>Roger M Needham</author>
<author>Karen Spa¨rck Jones</author>
</authors>
<title>The analogy between mechanical translation and library retrieval.</title>
<date>1959</date>
<booktitle>In Proceedings of the International Conference on Scientific Information</booktitle>
<volume>2</volume>
<pages>917--935</pages>
<location>Washington, DC,</location>
<marker>Masterman, Needham, Jones, 1959</marker>
<rawString>Masterman, Margaret, Roger M. Needham, and Karen Spa¨rck Jones. 1959. The analogy between mechanical translation and library retrieval. In Proceedings of the International Conference on Scientific Information (1958), National Academy of Sciences–National Research Council, Washington, DC, Vol. 2, pages 917–935.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>Synonymy and Semantic Classification.</title>
<date>1964</date>
<tech>Ph.D. thesis,</tech>
<publisher>Press,</publisher>
<institution>University of Cambridge:</institution>
<location>Edinburgh,</location>
<marker>Jones, Karen, 1964</marker>
<rawString>Spa¨rck Jones, Karen. 1964. Synonymy and Semantic Classification. Ph.D. thesis, University of Cambridge: Report ML 170, Cambridge Language Research Unit. (Edinburgh University Press, Edinburgh, 1986.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>A small semantic classification experiment using cooccurrence data. Report ML 196, Cambridge Language Research Unit,</title>
<date>1967</date>
<location>Cambridge.</location>
<marker>Jones, Karen, 1967</marker>
<rawString>Spa¨rck Jones, Karen. 1967. A small semantic classification experiment using cooccurrence data. Report ML 196, Cambridge Language Research Unit, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<date>1967</date>
<booktitle>Notes on semantic discourse structure. Report SP-2714, System Development Corporation,</booktitle>
<location>Santa Monica, CA.</location>
<marker>Jones, Karen, 1967</marker>
<rawString>Spa¨rck Jones, Karen. 1967. Notes on semantic discourse structure. Report SP-2714, System Development Corporation, Santa Monica, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<pages>28--11</pages>
<marker>Jones, Karen, 1972</marker>
<rawString>Spa¨rck Jones, Karen. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28:11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Karen Spa¨rck Jones</author>
</authors>
<title>Relevance weighting of search terms.</title>
<date>1976</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>27--129</pages>
<marker>Robertson, Jones, 1976</marker>
<rawString>Robertson, Stephen E. and Karen Spa¨rck Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information Science, 27:129–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>Search term relevance weighting given little relevance information.</title>
<date>1979</date>
<journal>Journal of Documentation,</journal>
<pages>35--30</pages>
<marker>Jones, Karen, 1979</marker>
<rawString>Spa¨rck Jones, Karen. 1979. Search term relevance weighting given little relevance information. Journal of Documentation, 35:30–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>Shifting meaning representations.</title>
<date>1983</date>
<booktitle>In IJCAI-83, Proceedings of the Eighth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>621--623</pages>
<location>Karlsruhe, Germany,</location>
<marker>Jones, Karen, 1983</marker>
<rawString>Spa¨rck Jones, Karen. 1983. Shifting meaning representations. In IJCAI-83, Proceedings of the Eighth International Joint Conference on Artificial Intelligence, Karlsruhe, Germany, pages 621–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>Tailoring output to the user: What does user modelling in generation mean? In</title>
<date>1991</date>
<booktitle>Natural Language Generation in Artificial Intelligence and Computational Linguistics. Kluwer,</booktitle>
<pages>201--225</pages>
<editor>C. L. Paris, W. R. Swartout, and W. C. Mann, editors,</editor>
<location>Dordrecht,</location>
<marker>Jones, Karen, 1991</marker>
<rawString>Spa¨rck Jones, Karen. 1991. Tailoring output to the user: What does user modelling in generation mean? In C. L. Paris, W. R. Swartout, and W. C. Mann, editors, Natural Language Generation in Artificial Intelligence and Computational Linguistics. Kluwer, Dordrecht, pages 201–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>Discourse modelling for automatic summarising.</title>
<date>1995</date>
<booktitle>Travaux du Cercle Linguistique de Prague (Prague Linguistic Circle Papers), New Series, Volume 1. John Benjamins,</booktitle>
<pages>201--227</pages>
<editor>In Eva Hajicova, Miroslav Cervenka, Oldrich Leska, and Petr Sgall, editors,</editor>
<location>Amsterdam,</location>
<marker>Jones, Karen, 1995</marker>
<rawString>Spa¨rck Jones, Karen. 1995. Discourse modelling for automatic summarising. In Eva Hajicova, Miroslav Cervenka, Oldrich Leska, and Petr Sgall, editors, Travaux du Cercle Linguistique de Prague (Prague Linguistic Circle Papers), New Series, Volume 1. John Benjamins, Amsterdam, pages 201–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
<author>Julia R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems.</title>
<date>1996</date>
<booktitle>Lecture Notes in Artificial Intelligence 1083.</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Jones, Karen, Galliers, 1996</marker>
<rawString>Spa¨rck Jones, Karen and Julia R. Galliers. 1996. Evaluating Natural Language Processing Systems. Lecture Notes in Artificial Intelligence 1083. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>Automatic summarising: Factors and directions.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarisation,</booktitle>
<pages>112</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>Jones, Karen, 1999</marker>
<rawString>Spa¨rck Jones, Karen. 1999. Automatic summarising: Factors and directions. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarisation, MIT Press, Cambridge, MA, pages 112.</rawString>
</citation>
<citation valid="true">
<title>Computers, language and speech: Formal theories and statistical data.</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society of London, Series A,</journal>
<volume>358</volume>
<issue>1769</issue>
<pages>1225--1431</pages>
<editor>Spa¨rck Jones, Karen, Gerald Gazdar, and Roger M. Needham, editors.</editor>
<marker>2000</marker>
<rawString>Spa¨rck Jones, Karen, Gerald Gazdar, and Roger M. Needham, editors. 2000. Computers, language and speech: Formal theories and statistical data. Philosophical Transactions of the Royal Society of London, Series A, 358(1769): 1225–1431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Stephen Walker Karen</author>
<author>Stephen E Robertson</author>
</authors>
<title>A probabilistic model of information retrieval:</title>
<date>2000</date>
<booktitle>Development and comparative experiments (Parts 1 and 2). Information Processing and Management,</booktitle>
<pages>36--6</pages>
<marker>Jones, Karen, Robertson, 2000</marker>
<rawString>Spa¨rck Jones, Karen, Stephen Walker, and Stephen E. Robertson. 2000. A probabilistic model of information retrieval: Development and comparative experiments (Parts 1 and 2). Information Processing and Management, 36(6):779–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spa¨rck Jones</author>
<author>Karen</author>
</authors>
<title>Language modelling’s generative model: Is it rational? Working paper,</title>
<date>2004</date>
<institution>Computer Laboratory, University of Cambridge.</institution>
<marker>Jones, Karen, 2004</marker>
<rawString>Spa¨rck Jones, Karen. 2004. Language modelling’s generative model: Is it rational? Working paper, Computer Laboratory, University of Cambridge.</rawString>
</citation>
<citation valid="false">
<note>Other items cited in the paper</note>
<marker></marker>
<rawString>Other items cited in the paper</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Peter Luhn</author>
</authors>
<title>A statistical approach to mechanised literature searching.</title>
<date>1957</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>1</volume>
<issue>4</issue>
<marker>Luhn, 1957</marker>
<rawString>Luhn, Hans Peter. 1957. A statistical approach to mechanised literature searching. IBM Journal of Research and Development, 1(4):309–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Peter Luhn</author>
</authors>
<title>An experiment in auto-abstracting.</title>
<date>1958</date>
<booktitle>Auto-abstracts of Area 5 conference papers. International Conference on Scientific Information,</booktitle>
<pages>145--163</pages>
<editor>IBM Research Center, Yorktown Heights, NY. (Reprinted in C. K. Schultz, editor, H. P. Luhn:</editor>
<publisher>Spartan Books,</publisher>
<location>Washington, DC,</location>
<contexts>
<context position="21705" citStr="Luhn (1958)" startWordPosition="3514" endWordPosition="3515">here has been far more emphasis on statistically determined word importance in text and also on statistically justified word sequences, leaving class relations implicit. Word importance, and the associated extractive view of text meaning, has turned out to be very valuable. With rough tasks, like retrieval, you can do the whole job statistically, and well. With other tasks you can do a crude but useful job, wholly or largely statistically, for example by combining statistics with lightweight parsing in extractive summarizing or question answering. Using statistics for summarizing goes back to Luhn (1958). Figure 11 shows Luhn’s “auto-abstract”for the 1958 conference paper with which I began. Concatenating source sentences that have been selected because they contain statistically important words does not make for readable abstracts. But the important fact about the example is that the mechanism for selecting sentences has chosen ones containing thesaurus, which does not occur in the paper title but is the focus of the text 9 Computational Linguistics Volume 31, Number 1 Figure 11 argument. Modern techniques can combine statistical methods for identifying key content words with light sentence </context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Luhn, Hans Peter. 1958. An experiment in auto-abstracting. Auto-abstracts of Area 5 conference papers. International Conference on Scientific Information, Washington, DC, November 16–21, 1958. IBM Research Center, Yorktown Heights, NY. (Reprinted in C. K. Schultz, editor, H. P. Luhn: Pioneer of Information Science, Spartan Books, New York, 1968, pages 145–163.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph J Rocchio</author>
</authors>
<title>Document Retrieval Systems—Optimization and Evaluation.</title>
<date>1966</date>
<tech>Report ISR-10,</tech>
<institution>Computation Laboratory, Harvard University.</institution>
<contexts>
<context position="18808" citStr="Rocchio 1966" startWordPosition="3046" endWordPosition="3047">of items to select relevant documents from millions? The TREC (Text REtrieval Conferences) evaluation program (Voorhees and Harman, in press) was designed to answer this question, but its scale and development, in a major, long-term activity under Donna Harman and Ellen Voorhees, have had a far wider impact on the whole of LIP. I was delighted to see that the Probabilistic Model, in work led by Stephen Robertson, did just fine in TREC, combining tf with idf, as Salton had advocated, in a robust way and incorporating query expansion in feedback, as originally suggested by Rocchio in the 1960s (Rocchio 1966). Figure 10 shows how effective the statistical approach to IR can be (using results taken from a later paper). For two versions of the users’ information needs, minimal and amplified, the performance gains are similar, though with richer starting requests they are, not suprisingly, larger. Performance for the rock bottom baseline, unweighted terms with a best-match strategy, gives only 1 relevant item in the top 10 retrieved. Using tf and idf, with a suitable elaboration to factor document length (dl) into the formula suggested by the theory, immediately gives a very substantial improvement, </context>
</contexts>
<marker>Rocchio, 1966</marker>
<rawString>Rocchio, Joseph J. 1966. Document Retrieval Systems—Optimization and Evaluation. Report ISR-10, Computation Laboratory, Harvard University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ellen M Voorhees</author>
<author>K Donna</author>
</authors>
<booktitle>An Experiment and Evaluation in Information Retrieval.</booktitle>
<editor>Harman, editors. In press. TREC:</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Voorhees, Donna, </marker>
<rawString>Voorhees, Ellen M. and Donna K. Harman, editors. In press. TREC: An Experiment and Evaluation in Information Retrieval. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Semantics and quantification in natural language question answering.</title>
<date>1978</date>
<booktitle>Advances in Computers, Vol. 17.</booktitle>
<pages>2--64</pages>
<editor>In M. Yovits, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="12753" citStr="Woods 1978" startWordPosition="2067" endWordPosition="2068"> normally implemented, actually is as a form of information access. It is much more reasonable to envisage a family of access modes to different types of information giving the user whatever can be found, in one way or another, in text, data, or even knowledge bases, as these happen to be available. A system would take the user’s single input and apply different types of interpretation at different levels, within a single framework. It would treat a query deliberately as something to be unpacked from different points of view, rather than in a preferred mode, with backup defaults, as in LUNAR (Woods 1978). Figure 7 illustrates the idea, very simplistically. Thus for the user’s input, Who supplies green parts?, suppose we have an initial interpretation, say as a dependency tree. This can be subjected to further processing as a whole to construct a formal database query or taken simply as a base from which variant phrasal terms for text searching can be derived. Alternatively, if we consider the input text itself as its own, direct, representation, we can use this representation as a source for simple retrieval terms. Other versions of the input are drawn from different, perhaps successively dee</context>
</contexts>
<marker>Woods, 1978</marker>
<rawString>Woods, William A. 1978. Semantics and quantification in natural language question answering. In M. Yovits, editor, Advances in Computers, Vol. 17. Academic Press, New York, pages 2–64.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>