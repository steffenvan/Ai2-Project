<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008811">
<title confidence="0.997421">
Using Generation for Grammar Analysis and Error Detection
</title>
<author confidence="0.994738">
Michael Wayne Goodman∗
</author>
<affiliation confidence="0.986837">
University of Washington
Dept. of Linguistics
</affiliation>
<address confidence="0.811886">
Box 354340 Seattle, WA 98195, USA
</address>
<email confidence="0.998665">
goodmami@u.washington.edu
</email>
<author confidence="0.745481">
Francis Bond
</author>
<affiliation confidence="0.688897">
NICT Language Infrastructure Group
</affiliation>
<address confidence="0.6078525">
3-5 Hikaridai, Seika-cho, S¯oraku-gun,
Kyoto, 619-0289 Japan
</address>
<email confidence="0.996873">
bond@ieee.org
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987363636364">
We demonstrate that the bidirectionality
of deep grammars, allowing them to gen-
erate as well as parse sentences, can be
used to automatically and effectively iden-
tify errors in the grammars. The system is
tested on two implemented HPSG gram-
mars: Jacy for Japanese, and the ERG for
English. Using this system, we were able
to increase generation coverage in Jacy by
18% (45% to 63%) with only four weeks
of grammar development.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.980243230769231">
Linguistically motivated analysis of text provides
much useful information for subsequent process-
ing. However, this is generally at the cost of re-
duced coverage, due both to the difficulty of pro-
viding analyses for all phenomena, and the com-
plexity of implementing these analyses. In this
paper we present a method of identifying prob-
lems in a deep grammar by exploiting the fact that
it can be used for both parsing (interpreting text
into semantics) and generation (realizing seman-
tics as text). Since both parsing and generation use
the same grammar, their performance is closely
related: in general improving the performance or
cover of one direction will also improve the other.
(Flickinger, 2008)
The central idea is that we test the grammar on
a full round trip: parsing text to its semantic repre-
sentation and then generating from it. In general,
any sentence where we cannot reproduce the orig-
inal, or where the generated sentence significantly
differs from the original, identifies a flaw in the
grammar, and with enough examples we can pin-
point the grammar rules causing these problems.
We call our system Egad, which stands for Erro-
neous Generation Analysis and Detection.
∗This research was carried out while visiting NICT.
</bodyText>
<sectionHeader confidence="0.958101" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999940625">
This work was inspired by the error mining ap-
proach of van Noord (2004), who identified prob-
lematic input for a grammar by comparing sen-
tences that parsed and those that didn’t from a
large corpus. Our approach takes this idea and fur-
ther applies it to generation. We were also inspired
by the work of Dickinson and Lee (2008), whose
“variation n-gram method” models the likelihood
a particular argument structure (semantic annota-
tion) is accurate given the verb and some context.
We tested Egad on two grammars: Jacy (Siegel,
2000), a Japanese grammar and the English Re-
source Grammar (ERG) (Flickinger, 2000, 2008)
from the DELPH-IN1 group. Both grammars are
written in the Head-driven Phrase Structure Gram-
mar (HPSG) (Pollard and Sag, 1994) framework,
and use Minimal Recursion Semantics (MRS)
(Copestake et al., 2005) for their semantic rep-
resentations. The Tanaka Corpus (Tanaka, 2001)
provides us with English and Japanese sentences.
The specific motivation for this work was to in-
crease the quality and coverage of generated para-
phrases using Jacy and the ERG. Bond et al.
(2008) showed they could improve the perfor-
mance of a statistical machine translation system
by training on a corpus that included paraphrased
variations of the English text. We want to do the
same with Japanese text, but Jacy was not able to
produce paraphrases as well (the ERG had 83%
generation coverage, while Jacy had 45%) Im-
proving generation would also greatly benefit X-
to-Japanese machine translation tasks using Jacy.
</bodyText>
<subsectionHeader confidence="0.999222">
2.1 Concerning Grammar Performance
</subsectionHeader>
<bodyText confidence="0.9654215">
There is a difference between the theoretical and
practical power of the grammars. Sometimes the
</bodyText>
<footnote confidence="0.962371">
1Deep Linguistic Processing with HPSG Initiative – see
http://www.delph-in.net for background informa-
tion, including the list of current participants and pointers to
available resources and documentation
</footnote>
<page confidence="0.946124">
109
</page>
<note confidence="0.9277195">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 109–112,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999960230769231">
parser or generator can reach the memory (i.e.
edge) limit, resulting in a valid result not being
returned. Also, we only look at the top-ranked2
parse and the first five generations for each item.
This is usually not a problem, but it could cause
Egad to report false positives.
HPSG grammars are theoretically symmetric
between parsing and generation, but in practice
this is not always true. For example, to improve
performance, semantically empty lexemes are not
inserted into a generation unless a “trigger-rule”
defines a context for them. These trigger-rules
may not cover all cases.
</bodyText>
<sectionHeader confidence="0.993634" genericHeader="method">
3 Grammar Analysis
</sectionHeader>
<bodyText confidence="0.999948375">
When analyzing a grammar, Egad looks at all in-
put sentences, parses, and generations processed
by the grammar and uses the information therein
to determine characteristics of these items. These
characteristics are encoded in a vector that can be
used for labeling and searching items. Some char-
acteristics are useful for error mining, while others
are used for grammar analysis.
</bodyText>
<subsectionHeader confidence="0.99911">
3.1 Characteristic Types
</subsectionHeader>
<bodyText confidence="0.999990642857143">
Egad determines both general characteristics of an
item (parsability and generability), and character-
istics comparing parses with generations.
General characteristics show whether each item
could: be parsed (“parsable”), generate from
parsed semantics (“generable”), generate the orig-
inal parsed sentence (“reproducible”), and gener-
ate other sentences (“paraphrasable”).
For comparative characteristics, Egad com-
pares every generated sentence to the parsed sen-
tence whence its semantics originated, and deter-
mines if the generated sentence uses the same set
of lexemes, derivation tree,3 set of rules, surface
form, and MRS as the original.
</bodyText>
<subsectionHeader confidence="0.999375">
3.2 Characteristic Patterns
</subsectionHeader>
<bodyText confidence="0.964599333333333">
Having determined all applicable characteristics
for an item or a generated sentence, we encode the
values of those characteristics into a vector. We
call this vector a characteristic pattern, or CP.
An example CP showing general characteristics is:
0010
</bodyText>
<footnote confidence="0.938554">
2Jacy and the ERG both have parse-ranking models.
3In comparing the derivation trees, we only look at phrasal
nodes. Lexemes and surface forms are not compared.
</footnote>
<bodyText confidence="0.9998866">
The first four digits are read as: the item is
parsable, generable, not reproducible, and is para-
phrasable. The five following dashes are for com-
parative characteristics and are inapplicable except
for generations.
</bodyText>
<subsectionHeader confidence="0.99982">
3.3 Utility of Characteristics
</subsectionHeader>
<bodyText confidence="0.999862222222222">
Not all characteristics are useful for all tasks. We
were interested in improving Jacy’s ability to gen-
erate sentences, so we primarily looked at items
that were parsable but ungenerable. In comparing
generated sentences with the original parsed sen-
tence, those with differing semantics often point to
errors, as do those with a different surface form but
the same derivation tree and lexemes (which usu-
ally means an inflectional rule was misapplied).
</bodyText>
<sectionHeader confidence="0.996912" genericHeader="method">
4 Problematic Rule Detection
</sectionHeader>
<bodyText confidence="0.999933">
Our method for detecting problematic rules is to
train a maximum entropy-based classifier4 with n-
gram paths of rules from a derivation tree as fea-
tures and characteristic patterns as labels. Once
trained, we do feature-selection to look at what
paths of rules are most predictive of certain labels.
</bodyText>
<subsectionHeader confidence="0.990329">
4.1 Rule Paths
</subsectionHeader>
<bodyText confidence="0.999924111111111">
We extract n-grams over rule paths, or RPs,
which are downward paths along the derivation
tree. (Toutanova et al., 2005) By creating sepa-
rate RPs for each branch in the derivation tree, we
retain some information about the order of rule ap-
plication without overfitting to specific tree struc-
tures. For example, Figure 1 is the derivation tree
for (1). A couple of RPs extracted from the deriva-
tion tree are shown in Figure 2.
</bodyText>
<equation confidence="0.817558">
(1) 写真 写り が いい
shashin-utsuri-ga ii
picture-taking-NOM good
(X is) good at taking pictures.
</equation>
<subsectionHeader confidence="0.991573">
4.2 Building a Model
</subsectionHeader>
<bodyText confidence="0.9998918">
We build a classification model by using a parsed
or generated sentence’s RPs as features and that
sentence’s CP as a label. The set of RPs includes
n-grams over all specified values of N. The labels
are, to be more accurate, regular expressions of
</bodyText>
<footnote confidence="0.964945">
4We would like to look at using different classifiers here,
such as Decision Trees. We initially chose MaxEnt because
it was easy to implement, and have since had little motivation
to change it because it produced useful results.
</footnote>
<page confidence="0.988126">
110
</page>
<figure confidence="0.951862">
utterance rule-decl-finite
head subj rule
unary-vstem-vend-rule
adj-i-lexeme-infl-rule
ii-adj
いい
shashin utsuri 1
写真 写り
</figure>
<figureCaption confidence="0.998547">
Figure 1: Derivation tree for (1)
</figureCaption>
<figure confidence="0.93459">
quantify-n-lrule → compounds-rule → shashin
quantify-n-lrule → compounds-rule → utsuri 1
</figure>
<figureCaption confidence="0.99987">
Figure 2: Example RPs extracted from Figure 1
</figureCaption>
<bodyText confidence="0.993323666666667">
CPs and may be fully specified to a unique CP or
generalize over several.5 The user can weight the
RPs by their N value (e.g. to target unigrams).
</bodyText>
<subsectionHeader confidence="0.999425">
4.3 Finding Problematic Rules
</subsectionHeader>
<bodyText confidence="0.999992684210526">
After training the model, we have a classifier that
predicts CPs given a set of RPs. What we want,
however, is the RP most strongly associated with
a given CP. The classifier we use provides an easy
method to get the score a given feature has for
some label. We iterate over all RPs, get their score,
then sort them based on the score. To help elim-
inate redundant results, we exclude any RP that
either subsumes or is subsumed by a previous (i.e.
higher ranked) RP.
Given a CP, the RP with the highest score
should indeed be the one most closely associated
to that CP, but it might not lead to the greatest
number of items affected. Fixing the second high-
est ranked RP, for example, may improve more
items than fixing the top ranked one. To help the
grammar developer decide the priority of prob-
lems to fix, we also output the count of items ob-
served with the given CP and RP.
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="method">
5 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999864">
We can look at two sets of results: how well
Egad was able to analyze a grammar and detect
errors, and how well a grammar developer could
use Egad to fix a problematic grammar. While the
latter is also influenced by the skill of the gram-
mar developer, we are interested in how well Egad
</bodyText>
<footnote confidence="0.769161">
5For example, /0010 / is fully specified.
/00.. / marginalizes two general characteristics
</footnote>
<bodyText confidence="0.999659">
points to the most significant errors, and how it can
help reduce development time.
</bodyText>
<subsectionHeader confidence="0.834504">
5.1 Error Mining
</subsectionHeader>
<bodyText confidence="0.999944086956522">
Table 1 lists the ten highest ranked RPs associated
with items that could parse but could not generate
in Jacy. Some RPs appear several times in differ-
ent contexts. We made an effort to decrease the
redundancy, but clearly this could be improved.
From this list of ten problematic RPs, there
are four unique problems: quantify-n-lrule (noun
quantification), no-nspec (noun specification), to-
comp-quotarg (L to quotative particle), and te-
adjunct (verb conjugation). The extra rules listed
in each RP show the context in which each
problem occurs, and this can be informative as
well. For instance, quantify-n-lrule occurs in
two primary contexts (above compounds-rule and
nominal-numcl-rule). The symptoms of the prob-
lem occur in the interation of rules in each context,
but the source of the problem is quantify-n-lrule.
Further, the problems identified are not always
lexically marked. quantify-n-lrule occurs for all
bare noun phrases (ie. without determiners). This
kind of error cannot be accurately identified by us-
ing just word or POS n-grams, we need to use the
actual parse tree.
</bodyText>
<subsectionHeader confidence="0.967844">
5.2 Error Correction
</subsectionHeader>
<bodyText confidence="0.999224428571428">
Egad greatly facilitated our efforts to find and fix
a wide variety of errors in Jacy. For example, we
restructured semantic predicate hierarchies, fixed
noun quantification, allowed some semantically
empty lexemes to generate in certain contexts,
added pragmatic information to distinguish be-
tween politeness levels in pronouns, allowed im-
peratives to generate, allowed more constructions
for numeral classifiers, and more.
Egad also identified some issues with the ERG:
both over-generation (an under-constrained inflec-
tional rule) and under-generation (sentences with
the construction take {care|charge|... } of were
not generating).
</bodyText>
<subsectionHeader confidence="0.997471">
5.3 Updated Grammar Statistics
</subsectionHeader>
<bodyText confidence="0.999892142857143">
After fixing the most significant problems in Jacy
(outlined in Section 5.2) as reported by Egad,
we obtained new statistics about the grammar’s
coverage and characteristics. Table 2 shows the
original and updated general statistics for Jacy.
We increased generability by 18%, doubled repro-
ducibility, and increased paraphrasability by 17%.
</bodyText>
<figure confidence="0.97155">
hf-complement-rule
quantify-n-lrule ga
が
compounds-rule
</figure>
<page confidence="0.987349">
111
</page>
<table confidence="0.987914818181818">
Score Count
1.42340952569648 109
0.960090299833317 54
0.756227560530811 63
0.739668926140179 62
0.739090261637851 22
0.694215264789286 36
0.676244980660372 82
0.617621482523537 26
0.592260546433334 36
0.564790702894285 62
</table>
<figure confidence="0.535402636363636">
Rule Path N-grams
hf-complement-rule → quantify-n-lrule → compounds-rule
hf-complement-rule → quantify-n-lrule → nominal-numcl-rule → head-specifier-rule
head-specifier-rule → hf-complement-rule → no-nspec → ”の”
hf-complement-rule → head-specifier-rule → hf-complement-rule → no-nspec
hf-complement-rule → hf-adj-i-rule → quantify-n-lrule → compounds-rule
hf-complement-rule → hf-complement-rule → to-comp-quotarg → ”と”
vstem-vend-rule → te-adjunct → ”Z”
hf-complement-rule → hf-complement-rule → to-comp-varg → ”と”
hf-adj-i-rule → hf-complement-rule → quantify-n-lrule → nominal-numcl-rule
quantify-n-lrule → compounds-rule → vn2n-det-lrule
</figure>
<tableCaption confidence="0.99942">
Table 1: Top 10 RPs for ungenerable items
</tableCaption>
<table confidence="0.9953928">
Original Modified
Parsable 82% 83%
Generable 45% 63%
Reproducible 11% 22%
Paraphrasable 44% 61%
</table>
<tableCaption confidence="0.999385">
Table 2: Jacy’s improved general statistics
</tableCaption>
<bodyText confidence="0.9998695">
As an added bonus, our work focused on improv-
ing generation also improved parsability by 1%.
Work is now continuing on fixing the remainder
of the identified errors.
</bodyText>
<sectionHeader confidence="0.999753" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999992666666667">
In future iterations of Egad, we would like to ex-
pand our feature set (e.g. information from failed
parses), and make the system more robust, such
as replacing lexical-ids (specific to a lexeme) with
lexical-types, since all lexemes of the same type
should behave identically. A more long-term goal
would allow Egad to analyze the internals of the
grammar and point out specific features within the
grammar rules that are causing problems. Some
of the errors detected by Egad have simple fixes,
and we believe there is room to explore methods
of automatic error correction.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999738083333333">
We have introduced a system that identifies er-
rors in implemented HPSG grammars, and further
finds and ranks the possible sources of those prob-
lems. This tool can greatly reduce the amount
of time a grammar developer would spend find-
ing bugs, and helps them make informed decisions
about which bugs are best to fix. In effect, we are
substituting cheap CPU time for expensive gram-
mar developer time. Using our system, we were
able to improve Jacy’s absolute generation cover-
age by 18% (45% to 63%) with only four weeks
of grammar development.
</bodyText>
<sectionHeader confidence="0.998776" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.998557333333333">
Thanks to NICT for their support, Takayuki Kurib-
ayashi for providing native judgments, and Mar-
cus Dickinson for comments on an early draft.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808641025641">
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving statistical machine trans-
lation by paraphrasing the training data. In International
Workshop on Spoken Language Translation, pages 150–
157. Honolulu.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.
Sag. 2005. Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4):281–
332.
Markus Dickinson and Chong Min Lee. 2008. Detecting
errors in semantic annotation. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC’08). Marrakech, Morocco.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15–28. (Special Issue on Efficient Processing with
HPSG).
Dan Flickinger. 2008. The English resource grammar. Tech-
nical Report 2007-7, LOGON, http://www.emmtee.
net/reports/7.pdf. (Draft of 2008-11-30).
Carl Pollard and Ivan A. Sag. 1994. Head Driven
Phrase Structure Grammar. University of Chicago Press,
Chicago.
Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil: Foundations of Speech-
to-Speech Translation, pages 265 – 280. Springer, Berlin,
Germany.
Yasuhito Tanaka. 2001. Compilation of a multilingual paral-
lel corpus. In Proceedings of PACLING 2001, pages 265–
268. Kyushu. (http://www.colips.org/afnlp/
archives/pacling2001/pdf/tanaka.pdf).
Kristina Toutanova, Christopher D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse disam-
biguation using the redwoods corpus. Research on Lan-
guage and Computation, 3(1):83–105.
Gertjan van Noord. 2004. Error mining for wide-coverage
grammar engineering. In 42nd Annual Meeting of the
Association for Computational Linguistics: ACL-2004.
Barcelona.
</reference>
<page confidence="0.998284">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857679">
<title confidence="0.99971">Using Generation for Grammar Analysis and Error Detection</title>
<author confidence="0.962082">Wayne</author>
<affiliation confidence="0.99996">University of Washington Dept. of Linguistics</affiliation>
<address confidence="0.999718">Box 354340 Seattle, WA 98195, USA</address>
<email confidence="0.999747">goodmami@u.washington.edu</email>
<author confidence="0.999048">Francis Bond</author>
<affiliation confidence="0.999877">NICT Language Infrastructure Group</affiliation>
<address confidence="0.9727585">3-5 Hikaridai, Seika-cho, S¯oraku-gun, Kyoto, 619-0289 Japan</address>
<email confidence="0.996164">bond@ieee.org</email>
<abstract confidence="0.9908515">We demonstrate that the bidirectionality of deep grammars, allowing them to generate as well as parse sentences, can be used to automatically and effectively identify errors in the grammars. The system is tested on two implemented HPSG grammars: Jacy for Japanese, and the ERG for English. Using this system, we were able to increase generation coverage in Jacy by 18% (45% to 63%) with only four weeks of grammar development.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Eric Nichols</author>
<author>Darren Scott Appling</author>
<author>Michael Paul</author>
</authors>
<title>Improving statistical machine translation by paraphrasing the training data.</title>
<date>2008</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<pages>150--157</pages>
<location>Honolulu.</location>
<contexts>
<context position="3097" citStr="Bond et al. (2008)" startWordPosition="493" endWordPosition="496">ome context. We tested Egad on two grammars: Jacy (Siegel, 2000), a Japanese grammar and the English Resource Grammar (ERG) (Flickinger, 2000, 2008) from the DELPH-IN1 group. Both grammars are written in the Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) framework, and use Minimal Recursion Semantics (MRS) (Copestake et al., 2005) for their semantic representations. The Tanaka Corpus (Tanaka, 2001) provides us with English and Japanese sentences. The specific motivation for this work was to increase the quality and coverage of generated paraphrases using Jacy and the ERG. Bond et al. (2008) showed they could improve the performance of a statistical machine translation system by training on a corpus that included paraphrased variations of the English text. We want to do the same with Japanese text, but Jacy was not able to produce paraphrases as well (the ERG had 83% generation coverage, while Jacy had 45%) Improving generation would also greatly benefit Xto-Japanese machine translation tasks using Jacy. 2.1 Concerning Grammar Performance There is a difference between the theoretical and practical power of the grammars. Sometimes the 1Deep Linguistic Processing with HPSG Initiati</context>
</contexts>
<marker>Bond, Nichols, Appling, Paul, 2008</marker>
<rawString>Francis Bond, Eric Nichols, Darren Scott Appling, and Michael Paul. 2008. Improving statistical machine translation by paraphrasing the training data. In International Workshop on Spoken Language Translation, pages 150– 157. Honolulu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal Recursion Semantics. An introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>332</pages>
<contexts>
<context position="2832" citStr="Copestake et al., 2005" startWordPosition="449" endWordPosition="452">Our approach takes this idea and further applies it to generation. We were also inspired by the work of Dickinson and Lee (2008), whose “variation n-gram method” models the likelihood a particular argument structure (semantic annotation) is accurate given the verb and some context. We tested Egad on two grammars: Jacy (Siegel, 2000), a Japanese grammar and the English Resource Grammar (ERG) (Flickinger, 2000, 2008) from the DELPH-IN1 group. Both grammars are written in the Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) framework, and use Minimal Recursion Semantics (MRS) (Copestake et al., 2005) for their semantic representations. The Tanaka Corpus (Tanaka, 2001) provides us with English and Japanese sentences. The specific motivation for this work was to increase the quality and coverage of generated paraphrases using Jacy and the ERG. Bond et al. (2008) showed they could improve the performance of a statistical machine translation system by training on a corpus that included paraphrased variations of the English text. We want to do the same with Japanese text, but Jacy was not able to produce paraphrases as well (the ERG had 83% generation coverage, while Jacy had 45%) Improving ge</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag. 2005. Minimal Recursion Semantics. An introduction. Research on Language and Computation, 3(4):281– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>Chong Min Lee</author>
</authors>
<title>Detecting errors in semantic annotation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08).</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="2337" citStr="Dickinson and Lee (2008)" startWordPosition="374" endWordPosition="377">y differs from the original, identifies a flaw in the grammar, and with enough examples we can pinpoint the grammar rules causing these problems. We call our system Egad, which stands for Erroneous Generation Analysis and Detection. ∗This research was carried out while visiting NICT. 2 Background This work was inspired by the error mining approach of van Noord (2004), who identified problematic input for a grammar by comparing sentences that parsed and those that didn’t from a large corpus. Our approach takes this idea and further applies it to generation. We were also inspired by the work of Dickinson and Lee (2008), whose “variation n-gram method” models the likelihood a particular argument structure (semantic annotation) is accurate given the verb and some context. We tested Egad on two grammars: Jacy (Siegel, 2000), a Japanese grammar and the English Resource Grammar (ERG) (Flickinger, 2000, 2008) from the DELPH-IN1 group. Both grammars are written in the Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) framework, and use Minimal Recursion Semantics (MRS) (Copestake et al., 2005) for their semantic representations. The Tanaka Corpus (Tanaka, 2001) provides us with English and Japane</context>
</contexts>
<marker>Dickinson, Lee, 2008</marker>
<rawString>Markus Dickinson and Chong Min Lee. 2008. Detecting errors in semantic annotation. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08). Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<booktitle>(Special Issue on Efficient Processing with HPSG).</booktitle>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="2620" citStr="Flickinger, 2000" startWordPosition="419" endWordPosition="420">ground This work was inspired by the error mining approach of van Noord (2004), who identified problematic input for a grammar by comparing sentences that parsed and those that didn’t from a large corpus. Our approach takes this idea and further applies it to generation. We were also inspired by the work of Dickinson and Lee (2008), whose “variation n-gram method” models the likelihood a particular argument structure (semantic annotation) is accurate given the verb and some context. We tested Egad on two grammars: Jacy (Siegel, 2000), a Japanese grammar and the English Resource Grammar (ERG) (Flickinger, 2000, 2008) from the DELPH-IN1 group. Both grammars are written in the Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) framework, and use Minimal Recursion Semantics (MRS) (Copestake et al., 2005) for their semantic representations. The Tanaka Corpus (Tanaka, 2001) provides us with English and Japanese sentences. The specific motivation for this work was to increase the quality and coverage of generated paraphrases using Jacy and the ERG. Bond et al. (2008) showed they could improve the performance of a statistical machine translation system by training on a corpus that include</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering, 6(1):15–28. (Special Issue on Efficient Processing with HPSG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>The English resource grammar.</title>
<date>2008</date>
<tech>Technical Report 2007-7,</tech>
<pages>2008--11</pages>
<note>LOGON, http://www.emmtee. net/reports/7.pdf. (Draft of</note>
<contexts>
<context position="1463" citStr="Flickinger, 2008" startWordPosition="225" endWordPosition="226">uent processing. However, this is generally at the cost of reduced coverage, due both to the difficulty of providing analyses for all phenomena, and the complexity of implementing these analyses. In this paper we present a method of identifying problems in a deep grammar by exploiting the fact that it can be used for both parsing (interpreting text into semantics) and generation (realizing semantics as text). Since both parsing and generation use the same grammar, their performance is closely related: in general improving the performance or cover of one direction will also improve the other. (Flickinger, 2008) The central idea is that we test the grammar on a full round trip: parsing text to its semantic representation and then generating from it. In general, any sentence where we cannot reproduce the original, or where the generated sentence significantly differs from the original, identifies a flaw in the grammar, and with enough examples we can pinpoint the grammar rules causing these problems. We call our system Egad, which stands for Erroneous Generation Analysis and Detection. ∗This research was carried out while visiting NICT. 2 Background This work was inspired by the error mining approach </context>
</contexts>
<marker>Flickinger, 2008</marker>
<rawString>Dan Flickinger. 2008. The English resource grammar. Technical Report 2007-7, LOGON, http://www.emmtee. net/reports/7.pdf. (Draft of 2008-11-30).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="2754" citStr="Pollard and Sag, 1994" startWordPosition="438" endWordPosition="441">y comparing sentences that parsed and those that didn’t from a large corpus. Our approach takes this idea and further applies it to generation. We were also inspired by the work of Dickinson and Lee (2008), whose “variation n-gram method” models the likelihood a particular argument structure (semantic annotation) is accurate given the verb and some context. We tested Egad on two grammars: Jacy (Siegel, 2000), a Japanese grammar and the English Resource Grammar (ERG) (Flickinger, 2000, 2008) from the DELPH-IN1 group. Both grammars are written in the Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) framework, and use Minimal Recursion Semantics (MRS) (Copestake et al., 2005) for their semantic representations. The Tanaka Corpus (Tanaka, 2001) provides us with English and Japanese sentences. The specific motivation for this work was to increase the quality and coverage of generated paraphrases using Jacy and the ERG. Bond et al. (2008) showed they could improve the performance of a statistical machine translation system by training on a corpus that included paraphrased variations of the English text. We want to do the same with Japanese text, but Jacy was not able to produce paraphrases </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
</authors>
<title>HPSG analysis of Japanese.</title>
<date>2000</date>
<booktitle>Verbmobil: Foundations of Speechto-Speech Translation,</booktitle>
<pages>265--280</pages>
<editor>In Wolfgang Wahlster, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="2543" citStr="Siegel, 2000" startWordPosition="407" endWordPosition="408"> and Detection. ∗This research was carried out while visiting NICT. 2 Background This work was inspired by the error mining approach of van Noord (2004), who identified problematic input for a grammar by comparing sentences that parsed and those that didn’t from a large corpus. Our approach takes this idea and further applies it to generation. We were also inspired by the work of Dickinson and Lee (2008), whose “variation n-gram method” models the likelihood a particular argument structure (semantic annotation) is accurate given the verb and some context. We tested Egad on two grammars: Jacy (Siegel, 2000), a Japanese grammar and the English Resource Grammar (ERG) (Flickinger, 2000, 2008) from the DELPH-IN1 group. Both grammars are written in the Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) framework, and use Minimal Recursion Semantics (MRS) (Copestake et al., 2005) for their semantic representations. The Tanaka Corpus (Tanaka, 2001) provides us with English and Japanese sentences. The specific motivation for this work was to increase the quality and coverage of generated paraphrases using Jacy and the ERG. Bond et al. (2008) showed they could improve the performance of </context>
</contexts>
<marker>Siegel, 2000</marker>
<rawString>Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolfgang Wahlster, editor, Verbmobil: Foundations of Speechto-Speech Translation, pages 265 – 280. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhito Tanaka</author>
</authors>
<title>Compilation of a multilingual parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of PACLING</booktitle>
<pages>265--268</pages>
<note>Kyushu. (http://www.colips.org/afnlp/ archives/pacling2001/pdf/tanaka.pdf).</note>
<contexts>
<context position="2901" citStr="Tanaka, 2001" startWordPosition="461" endWordPosition="462"> inspired by the work of Dickinson and Lee (2008), whose “variation n-gram method” models the likelihood a particular argument structure (semantic annotation) is accurate given the verb and some context. We tested Egad on two grammars: Jacy (Siegel, 2000), a Japanese grammar and the English Resource Grammar (ERG) (Flickinger, 2000, 2008) from the DELPH-IN1 group. Both grammars are written in the Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) framework, and use Minimal Recursion Semantics (MRS) (Copestake et al., 2005) for their semantic representations. The Tanaka Corpus (Tanaka, 2001) provides us with English and Japanese sentences. The specific motivation for this work was to increase the quality and coverage of generated paraphrases using Jacy and the ERG. Bond et al. (2008) showed they could improve the performance of a statistical machine translation system by training on a corpus that included paraphrased variations of the English text. We want to do the same with Japanese text, but Jacy was not able to produce paraphrases as well (the ERG had 83% generation coverage, while Jacy had 45%) Improving generation would also greatly benefit Xto-Japanese machine translation </context>
</contexts>
<marker>Tanaka, 2001</marker>
<rawString>Yasuhito Tanaka. 2001. Compilation of a multilingual parallel corpus. In Proceedings of PACLING 2001, pages 265– 268. Kyushu. (http://www.colips.org/afnlp/ archives/pacling2001/pdf/tanaka.pdf).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Stochastic HPSG parse disambiguation using the redwoods corpus.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="7249" citStr="Toutanova et al., 2005" startWordPosition="1130" endWordPosition="1133"> point to errors, as do those with a different surface form but the same derivation tree and lexemes (which usually means an inflectional rule was misapplied). 4 Problematic Rule Detection Our method for detecting problematic rules is to train a maximum entropy-based classifier4 with ngram paths of rules from a derivation tree as features and characteristic patterns as labels. Once trained, we do feature-selection to look at what paths of rules are most predictive of certain labels. 4.1 Rule Paths We extract n-grams over rule paths, or RPs, which are downward paths along the derivation tree. (Toutanova et al., 2005) By creating separate RPs for each branch in the derivation tree, we retain some information about the order of rule application without overfitting to specific tree structures. For example, Figure 1 is the derivation tree for (1). A couple of RPs extracted from the derivation tree are shown in Figure 2. (1) 写真 写り が いい shashin-utsuri-ga ii picture-taking-NOM good (X is) good at taking pictures. 4.2 Building a Model We build a classification model by using a parsed or generated sentence’s RPs as features and that sentence’s CP as a label. The set of RPs includes n-grams over all specified value</context>
</contexts>
<marker>Toutanova, Manning, Flickinger, Oepen, 2005</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, Dan Flickinger, and Stephan Oepen. 2005. Stochastic HPSG parse disambiguation using the redwoods corpus. Research on Language and Computation, 3(1):83–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Error mining for wide-coverage grammar engineering.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics: ACL-2004.</booktitle>
<location>Barcelona.</location>
<marker>van Noord, 2004</marker>
<rawString>Gertjan van Noord. 2004. Error mining for wide-coverage grammar engineering. In 42nd Annual Meeting of the Association for Computational Linguistics: ACL-2004. Barcelona.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>