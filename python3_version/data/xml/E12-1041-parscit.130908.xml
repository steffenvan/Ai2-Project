<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006559">
<title confidence="0.986648">
Lexical surprisal as a general predictor of reading time
</title>
<author confidence="0.996059">
Irene Fernandez Monsalve, Stefan L. Frank and Gabriella Vigliocco
</author>
<affiliation confidence="0.995166">
Division of Psychology and Language Sciences
University College London
</affiliation>
<email confidence="0.994678">
{ucjtife, s.frank, g.vigliocco}@ucl.ac.uk
</email>
<sectionHeader confidence="0.994747" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994519">
Probabilistic accounts of language process-
ing can be psychologically tested by com-
paring word-reading times (RT) to the con-
ditional word probabilities estimated by
language models. Using surprisal as a link-
ing function, a significant correlation be-
tween unlexicalized surprisal and RT has
been reported (e.g., Demberg and Keller,
2008), but success using lexicalized models
has been limited. In this study, phrase struc-
ture grammars and recurrent neural net-
works estimated both lexicalized and unlex-
icalized surprisal for words of independent
sentences from narrative sources. These
same sentences were used as stimuli in
a self-paced reading experiment to obtain
RTs. The results show that lexicalized sur-
prisal according to both models is a signif-
icant predictor of RT, outperforming its un-
lexicalized counterparts.
</bodyText>
<sectionHeader confidence="0.998887" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999830039215687">
Context-sensitive, prediction-based processing
has been proposed as a fundamental mechanism
of cognition (Bar, 2007): Faced with the prob-
lem of responding in real-time to complex stim-
uli, the human brain would use basic information
from the environment, in conjunction with previ-
ous experience, in order to extract meaning and
anticipate the immediate future. Such a cognitive
style is a well-established finding in low level sen-
sory processing (e.g., Kveraga et al., 2007), but
has also been proposed as a relevant mechanism
in higher order processes, such as language. In-
deed, there is ample evidence to show that human
language comprehension is both incremental and
predictive. For example, on-line detection of se-
mantic or syntactic anomalies can be observed in
the brain’s EEG signal (Hagoort et al., 2004) and
eye gaze is directed in anticipation at depictions
of plausible sentence completions (Kamide et al.,
2003). Moreover, probabilistic accounts of lan-
guage processing have identified unpredictability
as a major cause of processing difficulty in lan-
guage comprehension. In such incremental pro-
cessing, parsing would entail a pre-allocation of
resources to expected interpretations, so that ef-
fort would be related to the suitability of such
an allocation to the actually encountered stimulus
(Levy, 2008).
Possible sentence interpretations can be con-
strained by both linguistic and extra-linguistic
context, but while the latter is difficult to evalu-
ate, the former can be easily modeled: The pre-
dictability of a word for the human parser can be
expressed as the conditional probability of a word
given the sentence so far, which can in turn be es-
timated by language models trained on text cor-
pora. These probabilistic accounts of language
processing difficulty can then be validated against
empirical data, by taking reading time (RT) on a
word as a measure of the effort involved in its pro-
cessing.
Recently, several studies have followed this ap-
proach, using “surprisal” (see Section 1.1) as the
linking function between effort and predictabil-
ity. These can be computed for each word in a
text, or alternatively for the words’ parts of speech
(POS). In the latter case, the obtained estimates
can give an indication of the importance of syn-
tactic structure in developing upcoming-word ex-
pectations, but ignore the rich lexical information
that is doubtlessly employed by the human parser
</bodyText>
<page confidence="0.97642">
398
</page>
<note confidence="0.9766585">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 398–408,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999828916666667">
to constrain predictions. However, whereas such
an unlexicalized (i.e., POS-based) surprisal has
been shown to significantly predict RTs, success
with lexical (i.e., word-based) surprisal has been
limited. This can be attributed to data sparsity
(larger training corpora might be needed to pro-
vide accurate lexical surprisal than for the unlex-
icalized counterpart), or to the noise introduced
by participant’s world knowledge, inaccessible to
the models. The present study thus sets out to find
such a lexical surprisal effect, trying to overcome
possible limitations of previous research.
</bodyText>
<subsectionHeader confidence="0.998697">
1.1 Surprisal theory
</subsectionHeader>
<bodyText confidence="0.999078">
The concept of surprisal originated in the field of
information theory, as a measure of the amount of
information conveyed by a particular event. Im-
probable (‘surprising’) events carry more infor-
mation than expected ones, so that surprisal is in-
versely related to probability, through a logarith-
mic function. In the context of sentence process-
ing, if w1, ..., wt−1 denotes the sentence so far,
then the cognitive effort required for processing
the next word, wt, is assumed to be proportional
to its surprisal:
</bodyText>
<equation confidence="0.992952">
effort(t) a surprisal(wt)
_ − log(P(wt|w1, ..., wt−1)) (1)
</equation>
<bodyText confidence="0.999934272727273">
Different theoretical groundings for this rela-
tionship have been proposed (Hale, 2001; Levy
2008; Smith and Levy, 2008). Smith and Levy
derive it by taking a scale free assumption: Any
linguistic unit can be subdivided into smaller en-
tities (e.g., a sentence is comprised of words, a
word of phonemes), so that time to process the
whole will equal the sum of processing times for
each part. Since the probability of the whole can
be expressed as the product of the probabilities of
the subunits, the function relating probability and
effort must be logarithmic. Levy (2008), on the
other hand, grounds surprisal in its information-
theoretical context, describing difficulty encoun-
tered in on-line sentence processing as a result of
the need to update a probability distribution over
possible parses, being directly proportional to the
difference between the previous and updated dis-
tributions. By expressing the difference between
these in terms of relative entropy, Levy shows that
difficulty at each newly encountered word should
be equal to its surprisal.
</bodyText>
<subsectionHeader confidence="0.993872">
1.2 Empirical evidence for surprisal
</subsectionHeader>
<bodyText confidence="0.999797183673469">
The simplest statistical language models that can
be used to estimate surprisal values are n-gram
models or Markov chains, which condition the
probability of a given word only on its n − 1 pre-
ceding ones. Although Markov models theoret-
ically limit the amount of prior information that
is relevant for prediction of the next step, they
are often used in linguistic context as an approx-
imation to the full conditional probability. The
effect of bigram probability (or forward transi-
tional probability) has been repeatedly observed
(e.g. McDonald and Shillcock, 2003), and Smith
and Levy (2008) report an effect of lexical sur-
prisal as estimated by a trigram model on RTs
for the Dundee corpus (a collection of newspaper
texts with eye-tracking data from ten participants;
Kennedy and Pynte, 2005).
Phrase structure grammars (PSGs) have also
been amply used as language models (Boston et
al., 2008; Brouwer et al., 2010; Demberg and
Keller, 2008; Hale, 2001; Levy, 2008). PSGs
can combine statistical exposure effects with ex-
plicit syntactic rules, by annotating norms with
their respective probabilities, which can be es-
timated from occurrence counts in text corpora.
Information about hierarchical sentence structure
can thus be included in the models. In this way,
Brouwer et al. trained a probabilistic context-
free grammar (PCFG) on 204,000 sentences ex-
tracted from Dutch newspapers to estimate lexi-
cal surprisal (using an Earley-Stolcke parser; Stol-
cke, 1995), showing that it could account for
the noun phrase coordination bias previously de-
scribed and explained by Frazier (1987) in terms
of a minimal-attachment preference of the human
parser. In contrast, Demberg and Keller used texts
from a naturalistic source (the Dundee corpus) as
the experimental stimuli, thus evaluating surprisal
as a wide-coverage account of processing diffi-
culty. They also employed a PSG, trained on a
one-million-word language sample from the Wall
Street Journal (part of the Penn Treebank II, Mar-
cus et al., 1993). Using Roark’s (2001) incremen-
tal parser, they found significant effects of unlexi-
calized surprisal on RTs (see also Boston et al. for
a similar approach and results for German texts).
However, they failed to find an effect for lexical-
ized surprisal, over and above forward transitional
probability. Roark et al. (2009) also looked at the
</bodyText>
<page confidence="0.998516">
399
</page>
<bodyText confidence="0.999920145833333">
effects of syntactic and lexical surprisal, using RT
data for short narrative texts. However, their es-
timates of these two surprisal values differ from
those described above: In order to tease apart se-
mantic and syntactic effects, they used Demberg
and Keller’s lexicalized surprisal as a total sur-
prisal measure, which they decompose into syn-
tactic and lexical components. Their results show
significant effects of both syntactic and lexical
surprisal, although the latter was found to hold
only for closed class words. Lack of a wider effect
was attributed to data sparsity: The models were
trained on the relatively small Brown corpus (over
one million words from 500 samples of American
English text), so that surprisal estimates for the
less frequent content words would not have been
accurate enough.
Using the same training and experimental lan-
guage samples as Demberg and Keller (2008),
and only unlexicalized surprisal estimates, Frank
(2009) and Frank and Bod (2011) focused on
comparing different language models, including
various n-gram models, PSGs and recurrent net-
works (RNN). The latter were found to be the bet-
ter predictors of RTs, and PSGs could not explain
any variance in RT over and above the RNNs,
suggesting that human processing relies on linear
rather than hierarchical representations.
Summing up, the only models taking into ac-
count actual words that have been consistently
shown to simulate human behaviour with natural-
istic text samples are bigram models.1 A possi-
ble limitation in previous studies can be found in
the stimuli employed. In reading real newspaper
texts, prior knowledge of current affairs is likely
to highly influence RTs, however, this source of
variability cannot be accounted for by the mod-
els. In addition, whereas the models treat each
sentence as an independent unit, in the text cor-
pora employed they make up coherent texts, and
are therefore clearly dependent. Thirdly, the stim-
uli used by Demberg and Keller (2008) comprise
a very particular linguistic style: journalistic edi-
torials, reducing the ability to generalize conclu-
sions to language in general. Finally, failure to
find lexical surprisal effects can also be attributed
to the training texts. Larger corpora are likely to
be needed for training language models on actual
</bodyText>
<footnote confidence="0.912960666666667">
1Although Smith and Levy (2008) report an effect of tri-
grams, they did not check if it exceeded that of simpler bi-
grams.
</footnote>
<bodyText confidence="0.997998454545455">
words than on POS (both the Brown corpus and
the WSJ are relatively small), and in addition, the
particular journalistic style of the WSJ might not
be the best alternative for modeling human be-
haviour. Although similarity between the train-
ing and experimental data sets (both from news-
paper sources) can improve the linguistic perfor-
mance of the models, their ability to simulate hu-
man behaviour might be limited: Newspaper texts
probably form just a small fraction of a person’s
linguistic experience. This study thus aims to
tackle some of the identified limitations: Rather
than cohesive texts, independent sentences, from
a narrative style are used as experimental stim-
uli for which word-reading times are collected
(as explained in Section 3). In addition, as dis-
cussed in the following section, language mod-
els are trained on a larger corpus, from a more
representative language sample. Following Frank
(2009) and Frank and Bod (2011), two contrasting
types of models are employed: hierarchical PSGs
and linear RNNs.
</bodyText>
<sectionHeader confidence="0.993833" genericHeader="introduction">
2 Models
</sectionHeader>
<subsectionHeader confidence="0.999548">
2.1 Training data
</subsectionHeader>
<bodyText confidence="0.999995615384615">
The training texts were extracted from the writ-
ten section of the British National Corpus (BNC),
a collection of language samples from a variety
of sources, designed to provide a comprehensive
representation of current British English. A total
of 702,412 sentences, containing only the 7,754
most frequent words (the open-class words used
by Andrews et al., 2009, plus the 200 most fre-
quent words in English) were selected, making up
a 7.6-million-word training corpus. In addition to
providing a larger amount of data than the WSJ,
this training set thus provides a more representa-
tive language sample.
</bodyText>
<subsectionHeader confidence="0.997706">
2.2 Experimental sentences
</subsectionHeader>
<bodyText confidence="0.999994">
Three hundred and sixty-one sentences, all com-
prehensible out of context and containing only
words included in the subset of the BNC used
to train the models, were randomly selected from
three freely accessible on-line novels2 (for addi-
tional details, see Frank, 2012). The fictional
narrative provides a good contrast to the pre-
</bodyText>
<footnote confidence="0.55982">
2Obtained from www.free-online-novels.com.
Having not been published elsewhere, it is unlikely partici-
pants had read the novels previously.
</footnote>
<page confidence="0.988179">
400
</page>
<bodyText confidence="0.999923666666667">
viously examined newspaper editorials from the
Dundee corpus, since participants did not need
prior knowledge regarding the details of the sto-
ries, and a less specialised language and style
were employed. In addition, the randomly se-
lected sentences did not make up coherent texts
(in contrast, Roark et al., 2009, employed short
stories), so that they were independent from each
other, both for the models and the readers.
</bodyText>
<subsectionHeader confidence="0.999302">
2.3 Part-of-speech tagging
</subsectionHeader>
<bodyText confidence="0.999985230769231">
In order to produce POS-based surprisal esti-
mates, versions of both the training and exper-
imental texts with their words replaced by POS
were developed: The BNC sentences were parsed
by the Stanford Parser, version 1.6.7 (Klein and
Manning, 2003), whilst the experimental texts
were tagged by an automatic tagger (Tsuruoka
and Tsujii, 2005), with posterior review and cor-
rection by hand following the Penn Treebank
Project Guidelines (Santorini, 1991). By training
language models and subsequently running them
on the POS versions of the texts, unlexicalized
surprisal values were estimated.
</bodyText>
<subsectionHeader confidence="0.996906">
2.4 Phrase-structure grammars
</subsectionHeader>
<bodyText confidence="0.99999885">
The Treebank formed by the parsed BNC sen-
tences served as training data for Roark’s (2001)
incremental parser. Following Frank and Bod
(2011), a range of grammars was induced, dif-
fering in the features of the tree structure upon
which rule probabilities were conditioned. In
four grammars, probabilities depended on the left-
hand side’s ancestors, from one up to four levels
up in the parse tree (these grammars will be de-
noted a1 to a4). In four other grammars (s1 to
s4), the ancestors’ left siblings were also taken
into account. In addition, probabilities were con-
ditioned on the current head node in all grammars.
Subsequently, Roark’s (2001) incremental parser
parsed the experimental sentences under each of
the eight grammars, obtaining eight surprisal val-
ues for each word. Since earlier research (Frank,
2009) showed that decreasing the parser’s base
beam width parameter improves performance, it
was set to 10−18 (the default being 10−12).
</bodyText>
<subsectionHeader confidence="0.998878">
2.5 Recurrent neural network
</subsectionHeader>
<bodyText confidence="0.998532">
The RNN (see Figure 1) was trained in three
stages, each taking the selected (unparsed) BNC
sentences as training data.
</bodyText>
<figureCaption confidence="0.994146">
Figure 1: Architecture of neural network language
model, and its three learning stages. Numbers indicate
the number of units in each network layer.
</figureCaption>
<subsubsectionHeader confidence="0.555009">
Stage 1: Developing word representations
</subsubsectionHeader>
<bodyText confidence="0.970072148148148">
Neural network language models can bene-
fit from using distributed word representations:
Each word is assigned a vector in a continu-
ous, high-dimensional space, such that words that
are paradigmatically more similar are closer to-
gether (e.g., Bengio et al., 2003; Mnih and Hin-
ton, 2007). Usually, these representations are
learned together with the rest of the model, but
here we used a more efficient approach in which
word representations are learned in an unsuper-
vised manner from simple co-occurrences in the
training data. First, vectors of word co-occurrence
frequencies were developed using Good-Turing
(Gale and Sampson, 1995) smoothed frequency
counts from the training corpus. Values in the
vector corresponded to the smoothed frequencies
with which each word directly preceded or fol-
lowed the represented word. Thus, each word
w was assigned a vector (f.,1, ..., f.,15508), such
that f,,,,,, is the number of times word v directly
precedes (for v &lt; 7754) or follows (for v &gt;
7754) word w. Next, the frequency counts were
transformed into Pointwise Mutual Information
(PMI) values (see Equation 2), following Bulli-
naria and Levy’s (2007) findings that PMI pro-
duced more psychologically accurate predictions
than other measures:
</bodyText>
<figure confidence="0.982348857142857">
500
probability distribution
over 7,754 word types
400
400
7,754 word types
200
</figure>
<page confidence="0.943689">
401
</page>
<equation confidence="0.87968">
PMI(w, v) = log fw,v Ei,j fi,j (2)
Ei fi,v Ej fw,j
</equation>
<bodyText confidence="0.9995038">
Finally, the 400 columns with the highest vari-
ance were selected from the 7754×15508-matrix
of row vectors, making them more computation-
ally manageable, but not significantly less infor-
mative.
</bodyText>
<subsubsectionHeader confidence="0.984424">
Stage 2: Learning temporal structure
</subsubsectionHeader>
<bodyText confidence="0.999952571428571">
Using the standard backpropagation algorithm,
a simple recurrent network (SRN) learned to pre-
dict, at each point in the training corpus, the next
word’s vector given the sequence of word vectors
corresponding to the sentence so far. The total
corpus was presented five times, each time with
the sentences in a different random order.
</bodyText>
<subsubsectionHeader confidence="0.985846">
Stage 3: Decoding predicted word
representations
</subsubsectionHeader>
<bodyText confidence="0.9999931">
The distributed output of the trained SRN
served as training input to the feedforward “de-
coder” network, that learned to map the dis-
tributed representations back to localist ones.
This network, too, used standard backpropaga-
tion. Its output units had softmax activation func-
tions, so that the output vector constitutes a prob-
ability distribution over word types. These trans-
late directly into surprisal values, which were col-
lected over the experimental sentences at ten in-
tervals over the course of Stage 3 training (after
presenting 2K, 5K, 10K, 20K, 50K, 100K, 200K,
and 350K sentences, and after presenting the full
training corpus once and twice). These will be
denoted by RNN-1 to RNN-10.
A much simpler RNN model suffices for ob-
taining unlexicalized surprisal. Here, we used
the same models as described by Frank and Bod
(2011), albeit trained on the POS tags of our
BNC training corpus. These models employed
so-called Echo State Networks (ESN; Jaeger and
Haas, 2004), which are RNNs that do not develop
internal representations because weights of input
and recurrent connections remain fixed at ran-
dom values (only the output connection weights
are trained). Networks of six different sizes were
used. Of each size, three networks were trained,
using different random weights. The best and
worst model of each size were discarded to reduce
the effect of the random weights.
</bodyText>
<sectionHeader confidence="0.997564" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.993939">
3.1 Procedure
</subsectionHeader>
<bodyText confidence="0.999975055555556">
Text display followed a self-paced reading
paradigm: Sentences were presented on a com-
puter screen one word at a time, with onset of
the next word being controlled by the subject
through a key press. The time between word
onset and subsequent key press was recorded as
the RT (measured in milliseconds) on that word
by that subject.3 Words were presented centrally
aligned in the screen, and punctuation marks ap-
peared with the word that preceded them. A fixed-
width font type (Courier New) was used, so that
physical size of a word equalled number of char-
acters. Order of presentation was randomized for
each subject. The experiment was time-bounded
to 40 minutes, and the number of sentences read
by each participant varied between 120 and 349,
with an average of 224. Yes-no comprehension
questions followed 46% of the sentences.
</bodyText>
<subsectionHeader confidence="0.99515">
3.2 Participants
</subsectionHeader>
<bodyText confidence="0.999973">
A total of 117 first year psychology students took
part in the experiment. Subjects unable to an-
swer correctly to more than 20% of the questions
and 47 participants who were non-native English
speakers were excluded from the analysis, leaving
a total of 54 subjects.
</bodyText>
<subsectionHeader confidence="0.97237">
3.3 Design
</subsectionHeader>
<bodyText confidence="0.999991705882353">
The obtained RTs served as the dependent vari-
able against which a mixed-effects multiple re-
gression analysis with crossed random effects for
subjects and items (Baayen et al., 2008) was per-
formed. In order to control for low-level lexical
factors that are known to influence RTs, such as
word length or frequency, a baseline regression
model taking them into account was built. Subse-
quently, the decrease in the model’s deviance, af-
ter the inclusion of surprisal as a fixed factor to the
baseline, was assessed using likelihood tests. The
resulting x2 statistic indicates the extent to which
each surprisal estimate accounts for RT, and can
thus serve as a measure of the psychological ac-
curacy of each model.
However, this kind of analysis assumes that RT
for a word reflects processing of only that word,
</bodyText>
<footnote confidence="0.999683">
3The collected RT data are available for download at
www.stefanfrank.info/EACL2012.
</footnote>
<page confidence="0.995856">
402
</page>
<bodyText confidence="0.9413587">
but spill-over effects (in which processing diffi-
culty at word wt shows up in the RT on wt+1)
have been found in self-paced and natural read-
ing (Just et al., 1982; Rayner, 1998; Rayner and
Pollatsek, 1987). To evaluate these effects, the
decrease in deviance after adding surprisal of the
previous item to the baseline was also assessed.
The following control predictors were included
in the baseline regression model:
Lexical factors:
</bodyText>
<listItem confidence="0.98809775">
• Number of characters: Both physical size
and number of characters have been found
to affect RTs for a word (Rayner and Pollat-
sek, 1987), but the fixed-width font used in
the experiment assured number of characters
also encoded physical word length.
• Frequency and forward transitional proba-
bility: The effects of these two factors have
been repeatedly reported (e.g. Juhasz and
Rayner, 2003; Rayner, 1998). Given the high
correlations between surprisal and these two
measures, their inclusion in the baseline as-
sures that the results can be attributed to pre-
dictability in context, over and above fre-
quency and bigram probability. Frequency
was estimated from occurrence counts of
each word in the full BNC corpus (written
section). The same transformation (nega-
tive logarithm) was applied as for computing
surprisal, thus obtaining “unconditional” and
bigram surprisal values.
• Previous word lexical factors: Lexical fac-
tors for the previous word were included in
the analysis to control for spill-over effects.
</listItem>
<bodyText confidence="0.951147375">
Temporal factors and autocorrelation:
RT data over naturalistic texts violate the re-
gression assumption of independence of obser-
vations in several ways, and important word-by-
word sequential correlations exist. In order to en-
sure validity of the statistical analysis, as well as
providing a better model fit, the following factors
were also included:
</bodyText>
<listItem confidence="0.998249176470588">
• Sentence position: Fatigue and practice ef-
fects can influence RTs. Sentence position
in the experiment was included both as linear
and quadratic factor, allowing for the model-
ing of initial speed-up due to practice, fol-
lowed by a slowing down due to fatigue.
• Word position: Low-level effects of word or-
der, not related to predictability itself, were
modeled by including word position in the
sentence, both as a linear and quadratic fac-
tor (some of the sentences were quite long,
so that the effect of word position is unlikely
to be linear).
• Reading time for previous word: As sug-
gested by Baayen and Milin (2010), includ-
ing RT on the previous word can control for
several autocorrelation effects.
</listItem>
<sectionHeader confidence="0.999602" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999689538461539">
Data were analysed using the free statistical soft-
ware package R (R Development Core Team,
2009) and the lme4 library (Bates et al., 2011).
Two analyses were performed for each language
model, using surprisal for either current or pre-
vious word as the dependent variable. Unlikely
reading times (lower than 50ms or over 3000ms)
were removed from the analysis, as were clitics,
words followed by punctuation, words follow-
ing punctuation or clitics (since factors for pre-
vious word were included in the analysis), and
sentence-initial words, leaving a total of 132,298
data points (between 1,335 and 3,829 per subject).
</bodyText>
<subsectionHeader confidence="0.935343">
4.1 Baseline model
</subsectionHeader>
<bodyText confidence="0.999986954545454">
Theoretical considerations guided the selection
of the initial predictors presented above, but an
empirical approach led actual regression model
building. Initial models with the original set of
fixed effects, all two-way interactions, plus ran-
dom intercepts for subjects and items were evalu-
ated, and least significant factors were removed
one at a time, until only significant predictors
were left (|t |&gt; 2). A different strategy was
used to assess which by-subject and by item ran-
dom slopes to include in the model. Given the
large number of predictors, starting from the sat-
urated model with all random slopes generated
non-convergence problems and excessively long
running times. By-subject and by-item random
slopes for each fixed effect were therefore as-
sessed individually, using likelihood tests. The
final baseline model included by-subject random
intercepts, by-subject random slopes for sentence
position and word position, and by-item slopes for
previous RT. All factors (random slopes and fixed
effects) were centred and standardized to avoid
</bodyText>
<page confidence="0.999438">
403
</page>
<figureCaption confidence="0.99962775">
Figure 2: Psychological accuracy (combined effect of current and previous surprisal) against linguistic accuracy
of the different models. Numbered labels denote the maximum number of levels up in the tree from which
conditional information is used (PSG); point in training when estimates were collected (word-based RNN); or
network size (POS-based RNN).
</figureCaption>
<figure confidence="0.992156744680851">
Lexicalized models
-6.6 -6.4 -6.2 -6 -5.8 -5.6 -5.4 -5.2 -5
Unlexicalized models
70
30
43
60
25
Psychological accuracy (χ2)
2
50
20
56
1
40
3 4
15
30
2
10
20
5
10
1
0
0
4
1 23
-2.55 -2.5 -2.45 -2.4 -2.35 -2.3 -2.25 -2.2 -2.15 -2.1
Linguistic accuracy
(-average surprisal)
PSG-a PSG-s RNN
4
3
2
2
1
1
7
6
3 2
4
1
5
34
8 910
multicollinearity-related problems.
</figure>
<subsectionHeader confidence="0.977928">
4.2 Surprisal effects
</subsectionHeader>
<bodyText confidence="0.98830015">
All model categories (PSGs and RNNs) produced
lexicalized surprisal estimates that led to a signif-
icant (p &lt; 0.05) decrease in deviance when in-
cluded as a fixed factor in the baseline, with pos-
itive coefficients: Higher surprisal led to longer
RTs. Significant effects were also found for their
unlexicalized counterparts, albeit with consider-
ably smaller χ2-values.
Both for the lexicalized and unlexicalized ver-
sions, these effects persisted whether surprisal for
the previous or current word was taken as the in-
dependent variable. However, the effect size was
much larger for previous surprisal, indicating the
presence of strong spill-over effects (e.g. lexical-
ized PSG-s3: current surprisal: χ2(1) = 7.29,
p = 0.007; previous surprisal: χ2(1) = 36.73,
p « 0.001).
From hereon, only results for the combined ef-
fect of both (inclusion of previous and current
surprisal as fixed factors in the baseline) are re-
ported. Figure 2 shows the psychological accu-
racy of each model (χ2(2) values) plotted against
its linguistic accuracy (i.e., its quality as a lan-
guage model, measured by the negative aver-
age surprisal on the experimental sentences: the
higher this value, the “less surprised” the model
is by the test corpus). For the lexicalized models,
RNNs clearly outperform PSGs. Moreover, the
RNN’s accuracy increases as training progresses
(the highest psychological accuracy is achieved
at point 8, when 350K training sentences were
presented). The PSGs taking into account sib-
ling nodes are slightly better than their ancestor-
only counterparts (the best psychological model
is PSG-s3). Contrary to the trend reported by
Frank and Bod (2011), the unlexicalized PSGs
and RNNs reach similar levels of psychological
accuracy, with the PSG-s4 achieving the highest
χ2-value.
Model comparison x2(2) p-value
</bodyText>
<table confidence="0.917989">
PSG over RNN 12.45 0.002
RNN over PSG 30.46 «0.001
</table>
<tableCaption confidence="0.9870865">
Table 1: Model comparison between best performing
word-based PSG and RNN.
</tableCaption>
<bodyText confidence="0.99975475">
Although RNNs outperform PSGs in the lexi-
calized estimates, comparisons between the best
performing model (i.e. highest χ2) in each cate-
gory showed both were able to explain variance
over and above each other (see Table 1). It is
worth noting, however, that if comparisons are
made amongst models including surprisal for cur-
rent, but not previous word, the PSG is unable
</bodyText>
<page confidence="0.9979">
404
</page>
<bodyText confidence="0.999259166666667">
to explain a significant amount of variance over
and above the RNN (x2(1) = 2.28; p = 0.13).4
Lexicalized models achieved greater psychologi-
cal accuracy than their unlexicalized counterparts,
but the latter could still explain a small amount of
variance over and above the former (see Table 2).5
</bodyText>
<table confidence="0.9993661">
Model comparison x2(2) p-value
Best models overall:
POS- over word-based 10.40 0.006
word- over POS-based 47.02 «0.001
PSGs:
POS- over word-based 6.89 0.032
word- over POS-based 25.50 «0.001
RNNs:
POS- over word-based 5.80 0.055
word- over POS-based 49.74 «0.001
</table>
<tableCaption confidence="0.996018666666667">
Table 2: Word-vs. POS-based models: comparisons
between best models overall, and best models within
each category.
</tableCaption>
<subsectionHeader confidence="0.996151">
4.3 Differences across word classes
</subsectionHeader>
<bodyText confidence="0.999065357142857">
In order to make sure that the lexicalized sur-
prisal effects found were not limited to closed-
class words (as Roark et al., 2009, report), a fur-
ther model comparison was performed by adding
by-POS random slopes of surprisal to the models
containing the baseline plus surprisal. If particu-
lar syntactic categories were contributing to the
overall effect of surprisal more than others, in-
cluding such random slopes would lead to addi-
tional variance being explained. However, this
was not the case: inclusion of by-POS random
slopes of surprisal did not lead to a significant im-
provement in model fit (PSG: x2(1) = 0.86,p =
0.35; RNN: x2(1) = 3.20,p = 0.07).6
</bodyText>
<sectionHeader confidence="0.997619" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999951">
The present study aimed to find further evidence
for surprisal as a wide-coverage account of lan-
guage processing difficulty, and indeed, the re-
</bodyText>
<footnote confidence="0.990778285714286">
4Best models in this case were PSG-a3 and RNN-7.
5Since best performing lexicalized and unlexicalized
models belonged to different groups: RNN and PSG, respec-
tively, Table 2 also shows comparisons within model type.
6Comparison was made on the basis of previous word
surprisal (best models in this case were PSG-s3 and RNN-
9).
</footnote>
<bodyText confidence="0.999942490196079">
sults show the ability of lexicalized surprisal to
explain a significant amount of variance in RT
data for naturalistic texts, over and above that
accounted for by other low-level lexical factors,
such as frequency, length, and forward transi-
tional probability. Although previous studies had
presented results supporting such a probabilistic
language processing account, evidence for word-
based surprisal was limited: Brouwer et al. (2010)
only examined a specific psycholinguistic phe-
nomenon, rather than a random language sample;
Demberg and Keller (2008) reported effects that
were only significant for POS but not word-based
surprisal; and Smith and Levy (2008) found an
effect of lexicalized surprisal (according to a tri-
gram model), but did not assess whether simpler
predictability estimates (i.e., by a bigram model)
could have accounted for those effects.
Demberg and Keller’s (2008) failure to find lex-
icalized surprisal effects can be attributed both to
the language corpus used to train the language
models, as well as to the experimental texts used.
Both were sourced from newspaper texts: As
training corpora these are unrepresentative of a
person’s linguistic experience, and as experimen-
tal texts they are heavily dependent on partici-
pant’s world knowledge. Roark et al. (2009), in
contrast, used a more representative, albeit rela-
tively small, training corpus, as well as narrative-
style stimuli, thus obtaining RTs less dependent
on participant’s prior knowledge. With such an
experimental set-up, they were able to demon-
strate the effects of lexical surprisal for RT of
closed-class, but not open-class, words, which
they attributed to their differential frequency and
to training-data sparsity: The limited Brown cor-
pus would have been enough to produce accurate
estimates of surprisal for function words, but not
for the less frequent content words. A larger train-
ing corpus, constituting a broad language sample,
was used in our study, and the detected surprisal
effects were shown to hold across syntactic cate-
gory (modeling slopes for POS separately did not
improve model fit). However, direct comparison
with Roark et al.’s results is not possible: They
employed alternative definitions of structural and
lexical surprisal, which they derived by decom-
posing the total surprisal as obtained with a fully
lexicalized PSG model.
In the current study, a similar approach to that
taken by Demberg and Keller (2008) was used to
</bodyText>
<page confidence="0.997239">
405
</page>
<bodyText confidence="0.999996014285715">
define structural (or unlexicalized), and lexical-
ized surprisal, but the results are strikingly differ-
ent: Whereas Demberg and Keller report a signif-
icant effect for POS-based estimates, but not for
word-based surprisal, our results show that lexi-
calized surprisal is a far better predictor of RTs
than its unlexicalized counterpart. This is not sur-
prising, given that while the unlexicalized mod-
els only have access to syntactic sources of in-
formation, the lexicalized models, like the hu-
man parser, can also take into account lexical co-
occurrence trends. However, when a training cor-
pus is not large enough to accurately capture the
latter, it might still be able to model the former,
given the higher frequency of occurrence of each
possible item (POS vs. word) in the training data.
Roark et al. (2009) also included in their analysis
a POS-based surprisal estimate, which lost signif-
icance when the two components of the lexical-
ized surprisal were present, suggesting that such
unlexicalized estimates can be interpreted only as
a coarse version of the fully lexicalized surprisal,
incorporating both syntactic and lexical sources
of information at the same time. The results pre-
sented here do not replicate this finding: The best
unlexicalized estimates were able to explain ad-
ditional variance over and above the best word-
based estimates. However, this comparison con-
trasted two different model types: a word-based
RNN and a POS-based PSG, so that the observed
effects could be attributed to the model represen-
tations (hierarchical vs. linear) rather than to the
item of analysis (POS vs. words). Within-model
comparisons showed that unlexicalized estimates
were still able to account for additional variance,
although only reaching significance at the 0.05
level for the PSGs.
Previous results reported by Frank (2009) and
Frank and Bod (2011) regarding the higher psy-
chological accuracy of RNNs and the inability of
the PSGs to explain any additional variance in
RT, were not replicated. Although for the word-
based estimates RNNs outperform the PSGs, we
found both to have independent effects. Further-
more, in the POS-based analysis, performance of
PSGs and RNNs reaches similarly high levels of
psychological accuracy, with the best-performing
PSG producing slightly better results than the
best-performing RNN. This discrepancy in the re-
sults could reflect contrasting reading styles in
the two studies: natural reading of newspaper
texts, or self-paced reading of independent, nar-
rative sentences. The absence of global context,
or the unnatural reading methodology employed
in the current experiment, could have led to an
increased reliance on hierarchical structure for
sentence comprehension. The sources and struc-
tures relied upon by the human parser to elabo-
rate upcoming-word expectations could therefore
be task-dependent. On the other hand, our re-
sults show that the independent effects of word-
based PSG estimates only become apparent when
investigating the effect of surprisal of the previous
word. That is, considering only the current word’s
surprisal, as in Frank and Bod’s analysis, did not
reveal a significant contribution of PSGs over and
above RNNs. Thus, additional effects of PSG sur-
prisal might only be apparent when spill-over ef-
fects are investigated by taking previous word sur-
prisal as a predictor of RT.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999857">
The results here presented show that lexicalized
surprisal can indeed model RT over naturalistic
texts, thus providing a wide-coverage account of
language processing difficulty. Failure of previ-
ous studies to find such an effect could be at-
tributed to the size or nature of the training cor-
pus, suggesting that larger and more general cor-
pora are needed to model successfully both the
structural and lexical regularities used by the hu-
man parser to generate predictions. Another cru-
cial finding presented here is the importance of
spill-over effects: Surprisal of a word had a much
larger influence on RT of the following item than
of the word itself. Previous studies where lexi-
calized surprisal was only analysed in relation to
current RT could have missed a significant effect
only manifested on the following item. Whether
spill-over effects are as important for different RT
collection paradigms (e.g., eye-tracking) remains
to be tested.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999414571428572">
The research presented here was funded by the
European Union Seventh Framework Programme
(FP7/2007-2013) under grant number 253803.
The authors acknowledge the use of the UCL Le-
gion High Performance Computing Facility, and
associated support services, in the completion of
this work.
</bodyText>
<page confidence="0.99863">
406
</page>
<sectionHeader confidence="0.978946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997374108108108">
Gerry T.M. Altmann and Yuki Kamide. 1999. Incre-
mental interpretation at verbs: Restricting the do-
main of subsequent reference. Cognition, 73:247–
264.
Mark Andrews, Gabriella Vigliocco, and David P. Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116:463–498.
R. Harald Baayen and Petar Milin. 2010. Analyzing
reaction times. International Journal of Psycholog-
ical Research, 3:12–28.
R. Harald Baayen, Doug J. Davidson, and Douglas M.
Bates. 2008. Mixed-effects modeling with crossed
random effects for subjects and items. Journal of
Memory and Language, 59:390–412.
Moshe Bar. 2007. The proactive brain: using
analogies and associations to generate predictions.
Trends in Cognitive Sciences, 11:280–289.
Douglas Bates, Martin Maechler, and Ben Bolker,
2011. lme4: Linear mixed-effects models using
S4 classes. Available from: http://CRAN.R-
project.org/package=lme4 (R package version
0.999375-39).
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent,
and Christian Jauvin. 2003. A neural probabilis-
tic language model. Journal of Machine Learning
Research, 3:1137–1155.
Marisa Ferrara Boston, John Hale, Reinhold Kliegl,
Umesh Patil, and Shravan Vasishth. 2008. Parsing
costs as predictors of reading difficulty: An evalua-
tion using the potsdam sentence corpus. Journal of
Eye Movement Research,, 2:1–12.
Harm Brouwer, Hartmut Fitz, and John C. J. Hoeks.
2010. Modeling the noun phrase versus sentence
coordination ambiguity in Dutch: evidence from
surprisal theory. In Proceedings of the 2010 Work-
shop on Cognitive Modeling and Computational
Linguistics, pages 72–80, Stroudsburg, PA, USA.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39:510–526.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syn-
tactic processing complexity. Cognition, 109:193–
210.
Stefan L. Frank and Rens Bod. 2011. Insensitivity of
the human sentence-processing system to hierarchi-
cal structure. Psychological Science, 22:829–834.
Stefan L. Frank. 2009. Surprisal-based comparison
between a symbolic and a connectionist model of
sentence processing. In Proceedings of the 31st An-
nual Conference of the Cognitive Science Society,
pages 1139–1144, Austin, TX.
Stefan L. Frank. 2012. Uncertainty reduction as a
measure of cognitive processing load in sentence
comprehension. Manuscript submitted for publica-
tion.
Peter Hagoort, Lea Hald, Marcel Bastiaansen, and
Karl Magnus Petersson. 2004. Integration of word
meaning and world knowledge in language compre-
hension. Science, 304:438–441.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage technologies, pages 1–8, Stroudsburg, PA.
Herbert Jaeger and Harald Haas. 2004. Harnessing
nonlinearity: predicting chaotic systems and saving
energy in wireless communication. Science, pages
78–80.
Barbara J. Juhasz and Keith Rayner. 2003. Investigat-
ing the effects of a set of intercorrelated variables on
eye fixation durations in reading. Journal of Exper-
imental Psychology: Learning, Memory and Cogni-
tion, 29:1312–1318.
Marcel A. Just, Patricia A. Carpenter, and Jacque-
line D. Woolley. 1982. Paradigms and processes
in reading comprehension. Journal of Experimen-
tal Psychology: General, 111:228–238.
Yuki Kamide, Christoph Scheepers, and Gerry T. M.
Altmann. 2003. Integration of syntactic and se-
mantic information in predictive processing: cross-
linguistic evidence from German and English.
Journal of Psycholinguistic Research, 32:37–55.
Alan Kennedy and Jo¨el Pynte. 2005. Parafoveal-on
foveal effects in normal reading. Vision Research,
45:153–168.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics,, pages 423–430.
Kestutis Kveraga, Avniel S. Ghuman, and Moshe Bar.
2007. Top-down predictions in the cognitive brain.
Brain and Cognition, 65:145–168.
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106:1126–1177.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19:313–330.
Scott A. McDonald and Richard C. Shillcock. 2003.
Low-level predictive inference in reading: the influ-
ence of transitional probabilities on eye movements.
Vision Research, 43:1735–1751.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
Proceedings of the 25th International Conference of
Machine Learning, pages 641–648.
Keith Rayner and Alexander Pollatsek. 1987. Eye
movements in reading: A tutorial review. In
</reference>
<page confidence="0.979572">
407
</page>
<reference confidence="0.999896676470588">
M. Coltheart, editor, Attention and performance
XII: the psychology of reading., pages 327–362.
Lawrence Erlbaum Associates, London, UK.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124:372–422.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 1 - Volume 1, pages 324–333, Stroudsburg, PA.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27:249–276.
Beatrice Santorini. 1991. Part-of-speech tagging
guidelines for the Penn Treebank Project. Technical
report, Philadelphia, PA.
Nathaniel J. Smith and Roger Levy. 2008. Optimal
processing times in reading: a formal model and
empirical investigation. In Proceedings of the 30th
Annual Conference of the Cognitive Science Soci-
ety, pages 595–600, Austin,TX.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational linguistics, 21:165–
201.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 467–474, Stroudsburg, PA.
</reference>
<page confidence="0.997888">
408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862168">
<title confidence="0.990117">Lexical surprisal as a general predictor of reading time</title>
<author confidence="0.999236">Fernandez Monsalve</author>
<author confidence="0.999236">Stefan L Frank</author>
<affiliation confidence="0.999495">Division of Psychology and Language University College</affiliation>
<email confidence="0.976479">s.frank,</email>
<abstract confidence="0.994789809523809">Probabilistic accounts of language processing can be psychologically tested by comparing word-reading times (RT) to the conditional word probabilities estimated by language models. Using surprisal as a linking function, a significant correlation between unlexicalized surprisal and RT has been reported (e.g., Demberg and Keller, 2008), but success using lexicalized models has been limited. In this study, phrase structure grammars and recurrent neural networks estimated both lexicalized and unlexicalized surprisal for words of independent sentences from narrative sources. These same sentences were used as stimuli in a self-paced reading experiment to obtain RTs. The results show that lexicalized surprisal according to both models is a significant predictor of RT, outperforming its unlexicalized counterparts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gerry T M Altmann</author>
<author>Yuki Kamide</author>
</authors>
<title>Incremental interpretation at verbs: Restricting the domain of subsequent reference.</title>
<date>1999</date>
<journal>Cognition,</journal>
<volume>73</volume>
<pages>264</pages>
<marker>Altmann, Kamide, 1999</marker>
<rawString>Gerry T.M. Altmann and Yuki Kamide. 1999. Incremental interpretation at verbs: Restricting the domain of subsequent reference. Cognition, 73:247– 264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David P Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations. Psychological Review,</title>
<date>2009</date>
<pages>116--463</pages>
<contexts>
<context position="12135" citStr="Andrews et al., 2009" startWordPosition="1904" endWordPosition="1907">section, language models are trained on a larger corpus, from a more representative language sample. Following Frank (2009) and Frank and Bod (2011), two contrasting types of models are employed: hierarchical PSGs and linear RNNs. 2 Models 2.1 Training data The training texts were extracted from the written section of the British National Corpus (BNC), a collection of language samples from a variety of sources, designed to provide a comprehensive representation of current British English. A total of 702,412 sentences, containing only the 7,754 most frequent words (the open-class words used by Andrews et al., 2009, plus the 200 most frequent words in English) were selected, making up a 7.6-million-word training corpus. In addition to providing a larger amount of data than the WSJ, this training set thus provides a more representative language sample. 2.2 Experimental sentences Three hundred and sixty-one sentences, all comprehensible out of context and containing only words included in the subset of the BNC used to train the models, were randomly selected from three freely accessible on-line novels2 (for additional details, see Frank, 2012). The fictional narrative provides a good contrast to the pre2O</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David P. Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116:463–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
<author>Petar Milin</author>
</authors>
<title>Analyzing reaction times.</title>
<date>2010</date>
<journal>International Journal of Psychological Research,</journal>
<pages>3--12</pages>
<contexts>
<context position="23078" citStr="Baayen and Milin (2010)" startWordPosition="3657" endWordPosition="3660">d: • Sentence position: Fatigue and practice effects can influence RTs. Sentence position in the experiment was included both as linear and quadratic factor, allowing for the modeling of initial speed-up due to practice, followed by a slowing down due to fatigue. • Word position: Low-level effects of word order, not related to predictability itself, were modeled by including word position in the sentence, both as a linear and quadratic factor (some of the sentences were quite long, so that the effect of word position is unlikely to be linear). • Reading time for previous word: As suggested by Baayen and Milin (2010), including RT on the previous word can control for several autocorrelation effects. 4 Results Data were analysed using the free statistical software package R (R Development Core Team, 2009) and the lme4 library (Bates et al., 2011). Two analyses were performed for each language model, using surprisal for either current or previous word as the dependent variable. Unlikely reading times (lower than 50ms or over 3000ms) were removed from the analysis, as were clitics, words followed by punctuation, words following punctuation or clitics (since factors for previous word were included in the anal</context>
</contexts>
<marker>Baayen, Milin, 2010</marker>
<rawString>R. Harald Baayen and Petar Milin. 2010. Analyzing reaction times. International Journal of Psychological Research, 3:12–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
<author>Doug J Davidson</author>
<author>Douglas M Bates</author>
</authors>
<title>Mixed-effects modeling with crossed random effects for subjects and items.</title>
<date>2008</date>
<journal>Journal of Memory and Language,</journal>
<pages>59--390</pages>
<contexts>
<context position="19941" citStr="Baayen et al., 2008" startWordPosition="3149" endWordPosition="3152">nces read by each participant varied between 120 and 349, with an average of 224. Yes-no comprehension questions followed 46% of the sentences. 3.2 Participants A total of 117 first year psychology students took part in the experiment. Subjects unable to answer correctly to more than 20% of the questions and 47 participants who were non-native English speakers were excluded from the analysis, leaving a total of 54 subjects. 3.3 Design The obtained RTs served as the dependent variable against which a mixed-effects multiple regression analysis with crossed random effects for subjects and items (Baayen et al., 2008) was performed. In order to control for low-level lexical factors that are known to influence RTs, such as word length or frequency, a baseline regression model taking them into account was built. Subsequently, the decrease in the model’s deviance, after the inclusion of surprisal as a fixed factor to the baseline, was assessed using likelihood tests. The resulting x2 statistic indicates the extent to which each surprisal estimate accounts for RT, and can thus serve as a measure of the psychological accuracy of each model. However, this kind of analysis assumes that RT for a word reflects proc</context>
</contexts>
<marker>Baayen, Davidson, Bates, 2008</marker>
<rawString>R. Harald Baayen, Doug J. Davidson, and Douglas M. Bates. 2008. Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59:390–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Bar</author>
</authors>
<title>The proactive brain: using analogies and associations to generate predictions.</title>
<date>2007</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<pages>11--280</pages>
<contexts>
<context position="1194" citStr="Bar, 2007" startWordPosition="168" endWordPosition="169">t success using lexicalized models has been limited. In this study, phrase structure grammars and recurrent neural networks estimated both lexicalized and unlexicalized surprisal for words of independent sentences from narrative sources. These same sentences were used as stimuli in a self-paced reading experiment to obtain RTs. The results show that lexicalized surprisal according to both models is a significant predictor of RT, outperforming its unlexicalized counterparts. 1 Introduction Context-sensitive, prediction-based processing has been proposed as a fundamental mechanism of cognition (Bar, 2007): Faced with the problem of responding in real-time to complex stimuli, the human brain would use basic information from the environment, in conjunction with previous experience, in order to extract meaning and anticipate the immediate future. Such a cognitive style is a well-established finding in low level sensory processing (e.g., Kveraga et al., 2007), but has also been proposed as a relevant mechanism in higher order processes, such as language. Indeed, there is ample evidence to show that human language comprehension is both incremental and predictive. For example, on-line detection of s</context>
</contexts>
<marker>Bar, 2007</marker>
<rawString>Moshe Bar. 2007. The proactive brain: using analogies and associations to generate predictions. Trends in Cognitive Sciences, 11:280–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Bates</author>
<author>Martin Maechler</author>
<author>Ben Bolker</author>
</authors>
<title>lme4: Linear mixed-effects models using S4 classes. Available from: http://CRAN.Rproject.org/package=lme4 (R package version 0.999375-39).</title>
<date>2011</date>
<contexts>
<context position="23311" citStr="Bates et al., 2011" startWordPosition="3696" endWordPosition="3699">owing down due to fatigue. • Word position: Low-level effects of word order, not related to predictability itself, were modeled by including word position in the sentence, both as a linear and quadratic factor (some of the sentences were quite long, so that the effect of word position is unlikely to be linear). • Reading time for previous word: As suggested by Baayen and Milin (2010), including RT on the previous word can control for several autocorrelation effects. 4 Results Data were analysed using the free statistical software package R (R Development Core Team, 2009) and the lme4 library (Bates et al., 2011). Two analyses were performed for each language model, using surprisal for either current or previous word as the dependent variable. Unlikely reading times (lower than 50ms or over 3000ms) were removed from the analysis, as were clitics, words followed by punctuation, words following punctuation or clitics (since factors for previous word were included in the analysis), and sentence-initial words, leaving a total of 132,298 data points (between 1,335 and 3,829 per subject). 4.1 Baseline model Theoretical considerations guided the selection of the initial predictors presented above, but an emp</context>
</contexts>
<marker>Bates, Maechler, Bolker, 2011</marker>
<rawString>Douglas Bates, Martin Maechler, and Ben Bolker, 2011. lme4: Linear mixed-effects models using S4 classes. Available from: http://CRAN.Rproject.org/package=lme4 (R package version 0.999375-39).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="15499" citStr="Bengio et al., 2003" startWordPosition="2429" endWordPosition="2432">fault being 10−12). 2.5 Recurrent neural network The RNN (see Figure 1) was trained in three stages, each taking the selected (unparsed) BNC sentences as training data. Figure 1: Architecture of neural network language model, and its three learning stages. Numbers indicate the number of units in each network layer. Stage 1: Developing word representations Neural network language models can benefit from using distributed word representations: Each word is assigned a vector in a continuous, high-dimensional space, such that words that are paradigmatically more similar are closer together (e.g., Bengio et al., 2003; Mnih and Hinton, 2007). Usually, these representations are learned together with the rest of the model, but here we used a more efficient approach in which word representations are learned in an unsupervised manner from simple co-occurrences in the training data. First, vectors of word co-occurrence frequencies were developed using Good-Turing (Gale and Sampson, 1995) smoothed frequency counts from the training corpus. Values in the vector corresponded to the smoothed frequencies with which each word directly preceded or followed the represented word. Thus, each word w was assigned a vector </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marisa Ferrara Boston</author>
<author>John Hale</author>
<author>Reinhold Kliegl</author>
<author>Umesh Patil</author>
<author>Shravan Vasishth</author>
</authors>
<title>Parsing costs as predictors of reading difficulty: An evaluation using the potsdam sentence corpus.</title>
<date>2008</date>
<journal>Journal of Eye Movement Research,,</journal>
<pages>2--1</pages>
<contexts>
<context position="6865" citStr="Boston et al., 2008" startWordPosition="1063" endWordPosition="1066">tion that is relevant for prediction of the next step, they are often used in linguistic context as an approximation to the full conditional probability. The effect of bigram probability (or forward transitional probability) has been repeatedly observed (e.g. McDonald and Shillcock, 2003), and Smith and Levy (2008) report an effect of lexical surprisal as estimated by a trigram model on RTs for the Dundee corpus (a collection of newspaper texts with eye-tracking data from ten participants; Kennedy and Pynte, 2005). Phrase structure grammars (PSGs) have also been amply used as language models (Boston et al., 2008; Brouwer et al., 2010; Demberg and Keller, 2008; Hale, 2001; Levy, 2008). PSGs can combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora. Information about hierarchical sentence structure can thus be included in the models. In this way, Brouwer et al. trained a probabilistic contextfree grammar (PCFG) on 204,000 sentences extracted from Dutch newspapers to estimate lexical surprisal (using an Earley-Stolcke parser; Stolcke, 1995), showing that it could account f</context>
</contexts>
<marker>Boston, Hale, Kliegl, Patil, Vasishth, 2008</marker>
<rawString>Marisa Ferrara Boston, John Hale, Reinhold Kliegl, Umesh Patil, and Shravan Vasishth. 2008. Parsing costs as predictors of reading difficulty: An evaluation using the potsdam sentence corpus. Journal of Eye Movement Research,, 2:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harm Brouwer</author>
<author>Hartmut Fitz</author>
<author>John C J Hoeks</author>
</authors>
<title>Modeling the noun phrase versus sentence coordination ambiguity in Dutch: evidence from surprisal theory.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<pages>72--80</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6887" citStr="Brouwer et al., 2010" startWordPosition="1067" endWordPosition="1070"> for prediction of the next step, they are often used in linguistic context as an approximation to the full conditional probability. The effect of bigram probability (or forward transitional probability) has been repeatedly observed (e.g. McDonald and Shillcock, 2003), and Smith and Levy (2008) report an effect of lexical surprisal as estimated by a trigram model on RTs for the Dundee corpus (a collection of newspaper texts with eye-tracking data from ten participants; Kennedy and Pynte, 2005). Phrase structure grammars (PSGs) have also been amply used as language models (Boston et al., 2008; Brouwer et al., 2010; Demberg and Keller, 2008; Hale, 2001; Levy, 2008). PSGs can combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora. Information about hierarchical sentence structure can thus be included in the models. In this way, Brouwer et al. trained a probabilistic contextfree grammar (PCFG) on 204,000 sentences extracted from Dutch newspapers to estimate lexical surprisal (using an Earley-Stolcke parser; Stolcke, 1995), showing that it could account for the noun phrase coo</context>
<context position="30198" citStr="Brouwer et al. (2010)" startWordPosition="4792" endWordPosition="4795">and PSG, respectively, Table 2 also shows comparisons within model type. 6Comparison was made on the basis of previous word surprisal (best models in this case were PSG-s3 and RNN9). sults show the ability of lexicalized surprisal to explain a significant amount of variance in RT data for naturalistic texts, over and above that accounted for by other low-level lexical factors, such as frequency, length, and forward transitional probability. Although previous studies had presented results supporting such a probabilistic language processing account, evidence for wordbased surprisal was limited: Brouwer et al. (2010) only examined a specific psycholinguistic phenomenon, rather than a random language sample; Demberg and Keller (2008) reported effects that were only significant for POS but not word-based surprisal; and Smith and Levy (2008) found an effect of lexicalized surprisal (according to a trigram model), but did not assess whether simpler predictability estimates (i.e., by a bigram model) could have accounted for those effects. Demberg and Keller’s (2008) failure to find lexicalized surprisal effects can be attributed both to the language corpus used to train the language models, as well as to the e</context>
</contexts>
<marker>Brouwer, Fitz, Hoeks, 2010</marker>
<rawString>Harm Brouwer, Hartmut Fitz, and John C. J. Hoeks. 2010. Modeling the noun phrase versus sentence coordination ambiguity in Dutch: evidence from surprisal theory. In Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, pages 72–80, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>109</volume>
<pages>210</pages>
<contexts>
<context position="6913" citStr="Demberg and Keller, 2008" startWordPosition="1071" endWordPosition="1074"> next step, they are often used in linguistic context as an approximation to the full conditional probability. The effect of bigram probability (or forward transitional probability) has been repeatedly observed (e.g. McDonald and Shillcock, 2003), and Smith and Levy (2008) report an effect of lexical surprisal as estimated by a trigram model on RTs for the Dundee corpus (a collection of newspaper texts with eye-tracking data from ten participants; Kennedy and Pynte, 2005). Phrase structure grammars (PSGs) have also been amply used as language models (Boston et al., 2008; Brouwer et al., 2010; Demberg and Keller, 2008; Hale, 2001; Levy, 2008). PSGs can combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora. Information about hierarchical sentence structure can thus be included in the models. In this way, Brouwer et al. trained a probabilistic contextfree grammar (PCFG) on 204,000 sentences extracted from Dutch newspapers to estimate lexical surprisal (using an Earley-Stolcke parser; Stolcke, 1995), showing that it could account for the noun phrase coordination bias previously </context>
<context position="9207" citStr="Demberg and Keller (2008)" startWordPosition="1433" endWordPosition="1436">lized surprisal as a total surprisal measure, which they decompose into syntactic and lexical components. Their results show significant effects of both syntactic and lexical surprisal, although the latter was found to hold only for closed class words. Lack of a wider effect was attributed to data sparsity: The models were trained on the relatively small Brown corpus (over one million words from 500 samples of American English text), so that surprisal estimates for the less frequent content words would not have been accurate enough. Using the same training and experimental language samples as Demberg and Keller (2008), and only unlexicalized surprisal estimates, Frank (2009) and Frank and Bod (2011) focused on comparing different language models, including various n-gram models, PSGs and recurrent networks (RNN). The latter were found to be the better predictors of RTs, and PSGs could not explain any variance in RT over and above the RNNs, suggesting that human processing relies on linear rather than hierarchical representations. Summing up, the only models taking into account actual words that have been consistently shown to simulate human behaviour with naturalistic text samples are bigram models.1 A pos</context>
<context position="30316" citStr="Demberg and Keller (2008)" startWordPosition="4809" endWordPosition="4812">ious word surprisal (best models in this case were PSG-s3 and RNN9). sults show the ability of lexicalized surprisal to explain a significant amount of variance in RT data for naturalistic texts, over and above that accounted for by other low-level lexical factors, such as frequency, length, and forward transitional probability. Although previous studies had presented results supporting such a probabilistic language processing account, evidence for wordbased surprisal was limited: Brouwer et al. (2010) only examined a specific psycholinguistic phenomenon, rather than a random language sample; Demberg and Keller (2008) reported effects that were only significant for POS but not word-based surprisal; and Smith and Levy (2008) found an effect of lexicalized surprisal (according to a trigram model), but did not assess whether simpler predictability estimates (i.e., by a bigram model) could have accounted for those effects. Demberg and Keller’s (2008) failure to find lexicalized surprisal effects can be attributed both to the language corpus used to train the language models, as well as to the experimental texts used. Both were sourced from newspaper texts: As training corpora these are unrepresentative of a pe</context>
<context position="32192" citStr="Demberg and Keller (2008)" startWordPosition="5103" endWordPosition="5106"> function words, but not for the less frequent content words. A larger training corpus, constituting a broad language sample, was used in our study, and the detected surprisal effects were shown to hold across syntactic category (modeling slopes for POS separately did not improve model fit). However, direct comparison with Roark et al.’s results is not possible: They employed alternative definitions of structural and lexical surprisal, which they derived by decomposing the total surprisal as obtained with a fully lexicalized PSG model. In the current study, a similar approach to that taken by Demberg and Keller (2008) was used to 405 define structural (or unlexicalized), and lexicalized surprisal, but the results are strikingly different: Whereas Demberg and Keller report a significant effect for POS-based estimates, but not for word-based surprisal, our results show that lexicalized surprisal is a far better predictor of RTs than its unlexicalized counterpart. This is not surprising, given that while the unlexicalized models only have access to syntactic sources of information, the lexicalized models, like the human parser, can also take into account lexical cooccurrence trends. However, when a training c</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Vera Demberg and Frank Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109:193– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
<author>Rens Bod</author>
</authors>
<title>Insensitivity of the human sentence-processing system to hierarchical structure.</title>
<date>2011</date>
<pages>22--829</pages>
<publisher>Psychological Science,</publisher>
<contexts>
<context position="9290" citStr="Frank and Bod (2011)" startWordPosition="1445" endWordPosition="1448">xical components. Their results show significant effects of both syntactic and lexical surprisal, although the latter was found to hold only for closed class words. Lack of a wider effect was attributed to data sparsity: The models were trained on the relatively small Brown corpus (over one million words from 500 samples of American English text), so that surprisal estimates for the less frequent content words would not have been accurate enough. Using the same training and experimental language samples as Demberg and Keller (2008), and only unlexicalized surprisal estimates, Frank (2009) and Frank and Bod (2011) focused on comparing different language models, including various n-gram models, PSGs and recurrent networks (RNN). The latter were found to be the better predictors of RTs, and PSGs could not explain any variance in RT over and above the RNNs, suggesting that human processing relies on linear rather than hierarchical representations. Summing up, the only models taking into account actual words that have been consistently shown to simulate human behaviour with naturalistic text samples are bigram models.1 A possible limitation in previous studies can be found in the stimuli employed. In readi</context>
<context position="11663" citStr="Frank and Bod (2011)" startWordPosition="1830" endWordPosition="1833">uistic performance of the models, their ability to simulate human behaviour might be limited: Newspaper texts probably form just a small fraction of a person’s linguistic experience. This study thus aims to tackle some of the identified limitations: Rather than cohesive texts, independent sentences, from a narrative style are used as experimental stimuli for which word-reading times are collected (as explained in Section 3). In addition, as discussed in the following section, language models are trained on a larger corpus, from a more representative language sample. Following Frank (2009) and Frank and Bod (2011), two contrasting types of models are employed: hierarchical PSGs and linear RNNs. 2 Models 2.1 Training data The training texts were extracted from the written section of the British National Corpus (BNC), a collection of language samples from a variety of sources, designed to provide a comprehensive representation of current British English. A total of 702,412 sentences, containing only the 7,754 most frequent words (the open-class words used by Andrews et al., 2009, plus the 200 most frequent words in English) were selected, making up a 7.6-million-word training corpus. In addition to provi</context>
<context position="14091" citStr="Frank and Bod (2011)" startWordPosition="2206" endWordPosition="2209">eveloped: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). By training language models and subsequently running them on the POS versions of the texts, unlexicalized surprisal values were estimated. 2.4 Phrase-structure grammars The Treebank formed by the parsed BNC sentences served as training data for Roark’s (2001) incremental parser. Following Frank and Bod (2011), a range of grammars was induced, differing in the features of the tree structure upon which rule probabilities were conditioned. In four grammars, probabilities depended on the lefthand side’s ancestors, from one up to four levels up in the parse tree (these grammars will be denoted a1 to a4). In four other grammars (s1 to s4), the ancestors’ left siblings were also taken into account. In addition, probabilities were conditioned on the current head node in all grammars. Subsequently, Roark’s (2001) incremental parser parsed the experimental sentences under each of the eight grammars, obtaini</context>
<context position="18057" citStr="Frank and Bod (2011)" startWordPosition="2839" endWordPosition="2842">ackpropagation. Its output units had softmax activation functions, so that the output vector constitutes a probability distribution over word types. These translate directly into surprisal values, which were collected over the experimental sentences at ten intervals over the course of Stage 3 training (after presenting 2K, 5K, 10K, 20K, 50K, 100K, 200K, and 350K sentences, and after presenting the full training corpus once and twice). These will be denoted by RNN-1 to RNN-10. A much simpler RNN model suffices for obtaining unlexicalized surprisal. Here, we used the same models as described by Frank and Bod (2011), albeit trained on the POS tags of our BNC training corpus. These models employed so-called Echo State Networks (ESN; Jaeger and Haas, 2004), which are RNNs that do not develop internal representations because weights of input and recurrent connections remain fixed at random values (only the output connection weights are trained). Networks of six different sizes were used. Of each size, three networks were trained, using different random weights. The best and worst model of each size were discarded to reduce the effect of the random weights. 3 Experiment 3.1 Procedure Text display followed a </context>
<context position="27251" citStr="Frank and Bod (2011)" startWordPosition="4324" endWordPosition="4327">uracy (i.e., its quality as a language model, measured by the negative average surprisal on the experimental sentences: the higher this value, the “less surprised” the model is by the test corpus). For the lexicalized models, RNNs clearly outperform PSGs. Moreover, the RNN’s accuracy increases as training progresses (the highest psychological accuracy is achieved at point 8, when 350K training sentences were presented). The PSGs taking into account sibling nodes are slightly better than their ancestoronly counterparts (the best psychological model is PSG-s3). Contrary to the trend reported by Frank and Bod (2011), the unlexicalized PSGs and RNNs reach similar levels of psychological accuracy, with the PSG-s4 achieving the highest χ2-value. Model comparison x2(2) p-value PSG over RNN 12.45 0.002 RNN over PSG 30.46 «0.001 Table 1: Model comparison between best performing word-based PSG and RNN. Although RNNs outperform PSGs in the lexicalized estimates, comparisons between the best performing model (i.e. highest χ2) in each category showed both were able to explain variance over and above each other (see Table 1). It is worth noting, however, that if comparisons are made amongst models including surpris</context>
<context position="34060" citStr="Frank and Bod (2011)" startWordPosition="5401" endWordPosition="5404">best unlexicalized estimates were able to explain additional variance over and above the best wordbased estimates. However, this comparison contrasted two different model types: a word-based RNN and a POS-based PSG, so that the observed effects could be attributed to the model representations (hierarchical vs. linear) rather than to the item of analysis (POS vs. words). Within-model comparisons showed that unlexicalized estimates were still able to account for additional variance, although only reaching significance at the 0.05 level for the PSGs. Previous results reported by Frank (2009) and Frank and Bod (2011) regarding the higher psychological accuracy of RNNs and the inability of the PSGs to explain any additional variance in RT, were not replicated. Although for the wordbased estimates RNNs outperform the PSGs, we found both to have independent effects. Furthermore, in the POS-based analysis, performance of PSGs and RNNs reaches similarly high levels of psychological accuracy, with the best-performing PSG producing slightly better results than the best-performing RNN. This discrepancy in the results could reflect contrasting reading styles in the two studies: natural reading of newspaper texts, </context>
</contexts>
<marker>Frank, Bod, 2011</marker>
<rawString>Stefan L. Frank and Rens Bod. 2011. Insensitivity of the human sentence-processing system to hierarchical structure. Psychological Science, 22:829–834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1139--1144</pages>
<location>Austin, TX.</location>
<contexts>
<context position="9265" citStr="Frank (2009)" startWordPosition="1442" endWordPosition="1443"> syntactic and lexical components. Their results show significant effects of both syntactic and lexical surprisal, although the latter was found to hold only for closed class words. Lack of a wider effect was attributed to data sparsity: The models were trained on the relatively small Brown corpus (over one million words from 500 samples of American English text), so that surprisal estimates for the less frequent content words would not have been accurate enough. Using the same training and experimental language samples as Demberg and Keller (2008), and only unlexicalized surprisal estimates, Frank (2009) and Frank and Bod (2011) focused on comparing different language models, including various n-gram models, PSGs and recurrent networks (RNN). The latter were found to be the better predictors of RTs, and PSGs could not explain any variance in RT over and above the RNNs, suggesting that human processing relies on linear rather than hierarchical representations. Summing up, the only models taking into account actual words that have been consistently shown to simulate human behaviour with naturalistic text samples are bigram models.1 A possible limitation in previous studies can be found in the s</context>
<context position="11638" citStr="Frank (2009)" startWordPosition="1827" endWordPosition="1828"> improve the linguistic performance of the models, their ability to simulate human behaviour might be limited: Newspaper texts probably form just a small fraction of a person’s linguistic experience. This study thus aims to tackle some of the identified limitations: Rather than cohesive texts, independent sentences, from a narrative style are used as experimental stimuli for which word-reading times are collected (as explained in Section 3). In addition, as discussed in the following section, language models are trained on a larger corpus, from a more representative language sample. Following Frank (2009) and Frank and Bod (2011), two contrasting types of models are employed: hierarchical PSGs and linear RNNs. 2 Models 2.1 Training data The training texts were extracted from the written section of the British National Corpus (BNC), a collection of language samples from a variety of sources, designed to provide a comprehensive representation of current British English. A total of 702,412 sentences, containing only the 7,754 most frequent words (the open-class words used by Andrews et al., 2009, plus the 200 most frequent words in English) were selected, making up a 7.6-million-word training cor</context>
<context position="14768" citStr="Frank, 2009" startWordPosition="2318" endWordPosition="2319">he tree structure upon which rule probabilities were conditioned. In four grammars, probabilities depended on the lefthand side’s ancestors, from one up to four levels up in the parse tree (these grammars will be denoted a1 to a4). In four other grammars (s1 to s4), the ancestors’ left siblings were also taken into account. In addition, probabilities were conditioned on the current head node in all grammars. Subsequently, Roark’s (2001) incremental parser parsed the experimental sentences under each of the eight grammars, obtaining eight surprisal values for each word. Since earlier research (Frank, 2009) showed that decreasing the parser’s base beam width parameter improves performance, it was set to 10−18 (the default being 10−12). 2.5 Recurrent neural network The RNN (see Figure 1) was trained in three stages, each taking the selected (unparsed) BNC sentences as training data. Figure 1: Architecture of neural network language model, and its three learning stages. Numbers indicate the number of units in each network layer. Stage 1: Developing word representations Neural network language models can benefit from using distributed word representations: Each word is assigned a vector in a contin</context>
<context position="34035" citStr="Frank (2009)" startWordPosition="5398" endWordPosition="5399">his finding: The best unlexicalized estimates were able to explain additional variance over and above the best wordbased estimates. However, this comparison contrasted two different model types: a word-based RNN and a POS-based PSG, so that the observed effects could be attributed to the model representations (hierarchical vs. linear) rather than to the item of analysis (POS vs. words). Within-model comparisons showed that unlexicalized estimates were still able to account for additional variance, although only reaching significance at the 0.05 level for the PSGs. Previous results reported by Frank (2009) and Frank and Bod (2011) regarding the higher psychological accuracy of RNNs and the inability of the PSGs to explain any additional variance in RT, were not replicated. Although for the wordbased estimates RNNs outperform the PSGs, we found both to have independent effects. Furthermore, in the POS-based analysis, performance of PSGs and RNNs reaches similarly high levels of psychological accuracy, with the best-performing PSG producing slightly better results than the best-performing RNN. This discrepancy in the results could reflect contrasting reading styles in the two studies: natural rea</context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>Stefan L. Frank. 2009. Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In Proceedings of the 31st Annual Conference of the Cognitive Science Society, pages 1139–1144, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
</authors>
<title>Uncertainty reduction as a measure of cognitive processing load in sentence comprehension. Manuscript submitted for publication.</title>
<date>2012</date>
<contexts>
<context position="12672" citStr="Frank, 2012" startWordPosition="1992" endWordPosition="1993">,754 most frequent words (the open-class words used by Andrews et al., 2009, plus the 200 most frequent words in English) were selected, making up a 7.6-million-word training corpus. In addition to providing a larger amount of data than the WSJ, this training set thus provides a more representative language sample. 2.2 Experimental sentences Three hundred and sixty-one sentences, all comprehensible out of context and containing only words included in the subset of the BNC used to train the models, were randomly selected from three freely accessible on-line novels2 (for additional details, see Frank, 2012). The fictional narrative provides a good contrast to the pre2Obtained from www.free-online-novels.com. Having not been published elsewhere, it is unlikely participants had read the novels previously. 400 viously examined newspaper editorials from the Dundee corpus, since participants did not need prior knowledge regarding the details of the stories, and a less specialised language and style were employed. In addition, the randomly selected sentences did not make up coherent texts (in contrast, Roark et al., 2009, employed short stories), so that they were independent from each other, both for</context>
</contexts>
<marker>Frank, 2012</marker>
<rawString>Stefan L. Frank. 2012. Uncertainty reduction as a measure of cognitive processing load in sentence comprehension. Manuscript submitted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hagoort</author>
<author>Lea Hald</author>
<author>Marcel Bastiaansen</author>
<author>Karl Magnus Petersson</author>
</authors>
<title>Integration of word meaning and world knowledge in language comprehension.</title>
<date>2004</date>
<journal>Science,</journal>
<pages>304--438</pages>
<contexts>
<context position="1889" citStr="Hagoort et al., 2004" startWordPosition="279" endWordPosition="282">e human brain would use basic information from the environment, in conjunction with previous experience, in order to extract meaning and anticipate the immediate future. Such a cognitive style is a well-established finding in low level sensory processing (e.g., Kveraga et al., 2007), but has also been proposed as a relevant mechanism in higher order processes, such as language. Indeed, there is ample evidence to show that human language comprehension is both incremental and predictive. For example, on-line detection of semantic or syntactic anomalies can be observed in the brain’s EEG signal (Hagoort et al., 2004) and eye gaze is directed in anticipation at depictions of plausible sentence completions (Kamide et al., 2003). Moreover, probabilistic accounts of language processing have identified unpredictability as a major cause of processing difficulty in language comprehension. In such incremental processing, parsing would entail a pre-allocation of resources to expected interpretations, so that effort would be related to the suitability of such an allocation to the actually encountered stimulus (Levy, 2008). Possible sentence interpretations can be constrained by both linguistic and extra-linguistic </context>
</contexts>
<marker>Hagoort, Hald, Bastiaansen, Petersson, 2004</marker>
<rawString>Peter Hagoort, Lea Hald, Marcel Bastiaansen, and Karl Magnus Petersson. 2004. Integration of word meaning and world knowledge in language comprehension. Science, 304:438–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,</booktitle>
<pages>1--8</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="4961" citStr="Hale, 2001" startWordPosition="758" endWordPosition="759">eld of information theory, as a measure of the amount of information conveyed by a particular event. Improbable (‘surprising’) events carry more information than expected ones, so that surprisal is inversely related to probability, through a logarithmic function. In the context of sentence processing, if w1, ..., wt−1 denotes the sentence so far, then the cognitive effort required for processing the next word, wt, is assumed to be proportional to its surprisal: effort(t) a surprisal(wt) _ − log(P(wt|w1, ..., wt−1)) (1) Different theoretical groundings for this relationship have been proposed (Hale, 2001; Levy 2008; Smith and Levy, 2008). Smith and Levy derive it by taking a scale free assumption: Any linguistic unit can be subdivided into smaller entities (e.g., a sentence is comprised of words, a word of phonemes), so that time to process the whole will equal the sum of processing times for each part. Since the probability of the whole can be expressed as the product of the probabilities of the subunits, the function relating probability and effort must be logarithmic. Levy (2008), on the other hand, grounds surprisal in its informationtheoretical context, describing difficulty encountered </context>
<context position="6925" citStr="Hale, 2001" startWordPosition="1075" endWordPosition="1076"> used in linguistic context as an approximation to the full conditional probability. The effect of bigram probability (or forward transitional probability) has been repeatedly observed (e.g. McDonald and Shillcock, 2003), and Smith and Levy (2008) report an effect of lexical surprisal as estimated by a trigram model on RTs for the Dundee corpus (a collection of newspaper texts with eye-tracking data from ten participants; Kennedy and Pynte, 2005). Phrase structure grammars (PSGs) have also been amply used as language models (Boston et al., 2008; Brouwer et al., 2010; Demberg and Keller, 2008; Hale, 2001; Levy, 2008). PSGs can combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora. Information about hierarchical sentence structure can thus be included in the models. In this way, Brouwer et al. trained a probabilistic contextfree grammar (PCFG) on 204,000 sentences extracted from Dutch newspapers to estimate lexical surprisal (using an Earley-Stolcke parser; Stolcke, 1995), showing that it could account for the noun phrase coordination bias previously described an</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 1–8, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Jaeger</author>
<author>Harald Haas</author>
</authors>
<title>Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication.</title>
<date>2004</date>
<journal>Science,</journal>
<pages>78--80</pages>
<contexts>
<context position="18198" citStr="Jaeger and Haas, 2004" startWordPosition="2862" endWordPosition="2865">rd types. These translate directly into surprisal values, which were collected over the experimental sentences at ten intervals over the course of Stage 3 training (after presenting 2K, 5K, 10K, 20K, 50K, 100K, 200K, and 350K sentences, and after presenting the full training corpus once and twice). These will be denoted by RNN-1 to RNN-10. A much simpler RNN model suffices for obtaining unlexicalized surprisal. Here, we used the same models as described by Frank and Bod (2011), albeit trained on the POS tags of our BNC training corpus. These models employed so-called Echo State Networks (ESN; Jaeger and Haas, 2004), which are RNNs that do not develop internal representations because weights of input and recurrent connections remain fixed at random values (only the output connection weights are trained). Networks of six different sizes were used. Of each size, three networks were trained, using different random weights. The best and worst model of each size were discarded to reduce the effect of the random weights. 3 Experiment 3.1 Procedure Text display followed a self-paced reading paradigm: Sentences were presented on a computer screen one word at a time, with onset of the next word being controlled b</context>
</contexts>
<marker>Jaeger, Haas, 2004</marker>
<rawString>Herbert Jaeger and Harald Haas. 2004. Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication. Science, pages 78–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Juhasz</author>
<author>Keith Rayner</author>
</authors>
<title>Investigating the effects of a set of intercorrelated variables on eye fixation durations in reading.</title>
<date>2003</date>
<booktitle>Journal of Experimental Psychology: Learning, Memory and Cognition,</booktitle>
<pages>29--1312</pages>
<contexts>
<context position="21484" citStr="Juhasz and Rayner, 2003" startWordPosition="3401" endWordPosition="3404"> To evaluate these effects, the decrease in deviance after adding surprisal of the previous item to the baseline was also assessed. The following control predictors were included in the baseline regression model: Lexical factors: • Number of characters: Both physical size and number of characters have been found to affect RTs for a word (Rayner and Pollatsek, 1987), but the fixed-width font used in the experiment assured number of characters also encoded physical word length. • Frequency and forward transitional probability: The effects of these two factors have been repeatedly reported (e.g. Juhasz and Rayner, 2003; Rayner, 1998). Given the high correlations between surprisal and these two measures, their inclusion in the baseline assures that the results can be attributed to predictability in context, over and above frequency and bigram probability. Frequency was estimated from occurrence counts of each word in the full BNC corpus (written section). The same transformation (negative logarithm) was applied as for computing surprisal, thus obtaining “unconditional” and bigram surprisal values. • Previous word lexical factors: Lexical factors for the previous word were included in the analysis to control </context>
</contexts>
<marker>Juhasz, Rayner, 2003</marker>
<rawString>Barbara J. Juhasz and Keith Rayner. 2003. Investigating the effects of a set of intercorrelated variables on eye fixation durations in reading. Journal of Experimental Psychology: Learning, Memory and Cognition, 29:1312–1318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel A Just</author>
<author>Patricia A Carpenter</author>
<author>Jacqueline D Woolley</author>
</authors>
<title>Paradigms and processes in reading comprehension.</title>
<date>1982</date>
<journal>Journal of Experimental Psychology: General,</journal>
<pages>111--228</pages>
<contexts>
<context position="20817" citStr="Just et al., 1982" startWordPosition="3297" endWordPosition="3300">inclusion of surprisal as a fixed factor to the baseline, was assessed using likelihood tests. The resulting x2 statistic indicates the extent to which each surprisal estimate accounts for RT, and can thus serve as a measure of the psychological accuracy of each model. However, this kind of analysis assumes that RT for a word reflects processing of only that word, 3The collected RT data are available for download at www.stefanfrank.info/EACL2012. 402 but spill-over effects (in which processing difficulty at word wt shows up in the RT on wt+1) have been found in self-paced and natural reading (Just et al., 1982; Rayner, 1998; Rayner and Pollatsek, 1987). To evaluate these effects, the decrease in deviance after adding surprisal of the previous item to the baseline was also assessed. The following control predictors were included in the baseline regression model: Lexical factors: • Number of characters: Both physical size and number of characters have been found to affect RTs for a word (Rayner and Pollatsek, 1987), but the fixed-width font used in the experiment assured number of characters also encoded physical word length. • Frequency and forward transitional probability: The effects of these two </context>
</contexts>
<marker>Just, Carpenter, Woolley, 1982</marker>
<rawString>Marcel A. Just, Patricia A. Carpenter, and Jacqueline D. Woolley. 1982. Paradigms and processes in reading comprehension. Journal of Experimental Psychology: General, 111:228–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuki Kamide</author>
<author>Christoph Scheepers</author>
<author>Gerry T M Altmann</author>
</authors>
<title>Integration of syntactic and semantic information in predictive processing: crosslinguistic evidence from German and English.</title>
<date>2003</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>32--37</pages>
<contexts>
<context position="2000" citStr="Kamide et al., 2003" startWordPosition="296" endWordPosition="299">r to extract meaning and anticipate the immediate future. Such a cognitive style is a well-established finding in low level sensory processing (e.g., Kveraga et al., 2007), but has also been proposed as a relevant mechanism in higher order processes, such as language. Indeed, there is ample evidence to show that human language comprehension is both incremental and predictive. For example, on-line detection of semantic or syntactic anomalies can be observed in the brain’s EEG signal (Hagoort et al., 2004) and eye gaze is directed in anticipation at depictions of plausible sentence completions (Kamide et al., 2003). Moreover, probabilistic accounts of language processing have identified unpredictability as a major cause of processing difficulty in language comprehension. In such incremental processing, parsing would entail a pre-allocation of resources to expected interpretations, so that effort would be related to the suitability of such an allocation to the actually encountered stimulus (Levy, 2008). Possible sentence interpretations can be constrained by both linguistic and extra-linguistic context, but while the latter is difficult to evaluate, the former can be easily modeled: The predictability of</context>
</contexts>
<marker>Kamide, Scheepers, Altmann, 2003</marker>
<rawString>Yuki Kamide, Christoph Scheepers, and Gerry T. M. Altmann. 2003. Integration of syntactic and semantic information in predictive processing: crosslinguistic evidence from German and English. Journal of Psycholinguistic Research, 32:37–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Kennedy</author>
<author>Jo¨el Pynte</author>
</authors>
<title>Parafoveal-on foveal effects in normal reading.</title>
<date>2005</date>
<journal>Vision Research,</journal>
<pages>45--153</pages>
<marker>Kennedy, Jo¨el Pynte, 2005</marker>
<rawString>Alan Kennedy and Jo¨el Pynte. 2005. Parafoveal-on foveal effects in normal reading. Vision Research, 45:153–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="13574" citStr="Klein and Manning, 2003" startWordPosition="2130" endWordPosition="2133">cipants did not need prior knowledge regarding the details of the stories, and a less specialised language and style were employed. In addition, the randomly selected sentences did not make up coherent texts (in contrast, Roark et al., 2009, employed short stories), so that they were independent from each other, both for the models and the readers. 2.3 Part-of-speech tagging In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). By training language models and subsequently running them on the POS versions of the texts, unlexicalized surprisal values were estimated. 2.4 Phrase-structure grammars The Treebank formed by the parsed BNC sentences served as training data for Roark’s (2001) incremental parser. Following Frank and Bod (2011), a range of grammars was induced, differing in the features of the tree structure </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics,, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kestutis Kveraga</author>
<author>Avniel S Ghuman</author>
<author>Moshe Bar</author>
</authors>
<date>2007</date>
<booktitle>Top-down predictions in the cognitive brain. Brain and Cognition,</booktitle>
<pages>65--145</pages>
<contexts>
<context position="1551" citStr="Kveraga et al., 2007" startWordPosition="224" endWordPosition="227"> that lexicalized surprisal according to both models is a significant predictor of RT, outperforming its unlexicalized counterparts. 1 Introduction Context-sensitive, prediction-based processing has been proposed as a fundamental mechanism of cognition (Bar, 2007): Faced with the problem of responding in real-time to complex stimuli, the human brain would use basic information from the environment, in conjunction with previous experience, in order to extract meaning and anticipate the immediate future. Such a cognitive style is a well-established finding in low level sensory processing (e.g., Kveraga et al., 2007), but has also been proposed as a relevant mechanism in higher order processes, such as language. Indeed, there is ample evidence to show that human language comprehension is both incremental and predictive. For example, on-line detection of semantic or syntactic anomalies can be observed in the brain’s EEG signal (Hagoort et al., 2004) and eye gaze is directed in anticipation at depictions of plausible sentence completions (Kamide et al., 2003). Moreover, probabilistic accounts of language processing have identified unpredictability as a major cause of processing difficulty in language compre</context>
</contexts>
<marker>Kveraga, Ghuman, Bar, 2007</marker>
<rawString>Kestutis Kveraga, Avniel S. Ghuman, and Moshe Bar. 2007. Top-down predictions in the cognitive brain. Brain and Cognition, 65:145–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>106--1126</pages>
<contexts>
<context position="2394" citStr="Levy, 2008" startWordPosition="355" endWordPosition="356">ection of semantic or syntactic anomalies can be observed in the brain’s EEG signal (Hagoort et al., 2004) and eye gaze is directed in anticipation at depictions of plausible sentence completions (Kamide et al., 2003). Moreover, probabilistic accounts of language processing have identified unpredictability as a major cause of processing difficulty in language comprehension. In such incremental processing, parsing would entail a pre-allocation of resources to expected interpretations, so that effort would be related to the suitability of such an allocation to the actually encountered stimulus (Levy, 2008). Possible sentence interpretations can be constrained by both linguistic and extra-linguistic context, but while the latter is difficult to evaluate, the former can be easily modeled: The predictability of a word for the human parser can be expressed as the conditional probability of a word given the sentence so far, which can in turn be estimated by language models trained on text corpora. These probabilistic accounts of language processing difficulty can then be validated against empirical data, by taking reading time (RT) on a word as a measure of the effort involved in its processing. Rec</context>
<context position="4972" citStr="Levy 2008" startWordPosition="760" endWordPosition="761">mation theory, as a measure of the amount of information conveyed by a particular event. Improbable (‘surprising’) events carry more information than expected ones, so that surprisal is inversely related to probability, through a logarithmic function. In the context of sentence processing, if w1, ..., wt−1 denotes the sentence so far, then the cognitive effort required for processing the next word, wt, is assumed to be proportional to its surprisal: effort(t) a surprisal(wt) _ − log(P(wt|w1, ..., wt−1)) (1) Different theoretical groundings for this relationship have been proposed (Hale, 2001; Levy 2008; Smith and Levy, 2008). Smith and Levy derive it by taking a scale free assumption: Any linguistic unit can be subdivided into smaller entities (e.g., a sentence is comprised of words, a word of phonemes), so that time to process the whole will equal the sum of processing times for each part. Since the probability of the whole can be expressed as the product of the probabilities of the subunits, the function relating probability and effort must be logarithmic. Levy (2008), on the other hand, grounds surprisal in its informationtheoretical context, describing difficulty encountered in on-line </context>
<context position="6562" citStr="Levy (2008)" startWordPosition="1015" endWordPosition="1016">dence for surprisal The simplest statistical language models that can be used to estimate surprisal values are n-gram models or Markov chains, which condition the probability of a given word only on its n − 1 preceding ones. Although Markov models theoretically limit the amount of prior information that is relevant for prediction of the next step, they are often used in linguistic context as an approximation to the full conditional probability. The effect of bigram probability (or forward transitional probability) has been repeatedly observed (e.g. McDonald and Shillcock, 2003), and Smith and Levy (2008) report an effect of lexical surprisal as estimated by a trigram model on RTs for the Dundee corpus (a collection of newspaper texts with eye-tracking data from ten participants; Kennedy and Pynte, 2005). Phrase structure grammars (PSGs) have also been amply used as language models (Boston et al., 2008; Brouwer et al., 2010; Demberg and Keller, 2008; Hale, 2001; Levy, 2008). PSGs can combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora. Information about hierarc</context>
<context position="10633" citStr="Levy (2008)" startWordPosition="1662" endWordPosition="1663">annot be accounted for by the models. In addition, whereas the models treat each sentence as an independent unit, in the text corpora employed they make up coherent texts, and are therefore clearly dependent. Thirdly, the stimuli used by Demberg and Keller (2008) comprise a very particular linguistic style: journalistic editorials, reducing the ability to generalize conclusions to language in general. Finally, failure to find lexical surprisal effects can also be attributed to the training texts. Larger corpora are likely to be needed for training language models on actual 1Although Smith and Levy (2008) report an effect of trigrams, they did not check if it exceeded that of simpler bigrams. words than on POS (both the Brown corpus and the WSJ are relatively small), and in addition, the particular journalistic style of the WSJ might not be the best alternative for modeling human behaviour. Although similarity between the training and experimental data sets (both from newspaper sources) can improve the linguistic performance of the models, their ability to simulate human behaviour might be limited: Newspaper texts probably form just a small fraction of a person’s linguistic experience. This st</context>
<context position="30424" citStr="Levy (2008)" startWordPosition="4828" endWordPosition="4829">plain a significant amount of variance in RT data for naturalistic texts, over and above that accounted for by other low-level lexical factors, such as frequency, length, and forward transitional probability. Although previous studies had presented results supporting such a probabilistic language processing account, evidence for wordbased surprisal was limited: Brouwer et al. (2010) only examined a specific psycholinguistic phenomenon, rather than a random language sample; Demberg and Keller (2008) reported effects that were only significant for POS but not word-based surprisal; and Smith and Levy (2008) found an effect of lexicalized surprisal (according to a trigram model), but did not assess whether simpler predictability estimates (i.e., by a bigram model) could have accounted for those effects. Demberg and Keller’s (2008) failure to find lexicalized surprisal effects can be attributed both to the language corpus used to train the language models, as well as to the experimental texts used. Both were sourced from newspaper texts: As training corpora these are unrepresentative of a person’s linguistic experience, and as experimental texts they are heavily dependent on participant’s world kn</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Roger Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106:1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7970" citStr="Marcus et al., 1993" startWordPosition="1233" endWordPosition="1237">pers to estimate lexical surprisal (using an Earley-Stolcke parser; Stolcke, 1995), showing that it could account for the noun phrase coordination bias previously described and explained by Frazier (1987) in terms of a minimal-attachment preference of the human parser. In contrast, Demberg and Keller used texts from a naturalistic source (the Dundee corpus) as the experimental stimuli, thus evaluating surprisal as a wide-coverage account of processing difficulty. They also employed a PSG, trained on a one-million-word language sample from the Wall Street Journal (part of the Penn Treebank II, Marcus et al., 1993). Using Roark’s (2001) incremental parser, they found significant effects of unlexicalized surprisal on RTs (see also Boston et al. for a similar approach and results for German texts). However, they failed to find an effect for lexicalized surprisal, over and above forward transitional probability. Roark et al. (2009) also looked at the 399 effects of syntactic and lexical surprisal, using RT data for short narrative texts. However, their estimates of these two surprisal values differ from those described above: In order to tease apart semantic and syntactic effects, they used Demberg and Kel</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A McDonald</author>
<author>Richard C Shillcock</author>
</authors>
<title>Low-level predictive inference in reading: the influence of transitional probabilities on eye movements. Vision Research,</title>
<date>2003</date>
<pages>43--1735</pages>
<contexts>
<context position="6535" citStr="McDonald and Shillcock, 2003" startWordPosition="1008" endWordPosition="1011"> be equal to its surprisal. 1.2 Empirical evidence for surprisal The simplest statistical language models that can be used to estimate surprisal values are n-gram models or Markov chains, which condition the probability of a given word only on its n − 1 preceding ones. Although Markov models theoretically limit the amount of prior information that is relevant for prediction of the next step, they are often used in linguistic context as an approximation to the full conditional probability. The effect of bigram probability (or forward transitional probability) has been repeatedly observed (e.g. McDonald and Shillcock, 2003), and Smith and Levy (2008) report an effect of lexical surprisal as estimated by a trigram model on RTs for the Dundee corpus (a collection of newspaper texts with eye-tracking data from ten participants; Kennedy and Pynte, 2005). Phrase structure grammars (PSGs) have also been amply used as language models (Boston et al., 2008; Brouwer et al., 2010; Demberg and Keller, 2008; Hale, 2001; Levy, 2008). PSGs can combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora</context>
</contexts>
<marker>McDonald, Shillcock, 2003</marker>
<rawString>Scott A. McDonald and Richard C. Shillcock. 2003. Low-level predictive inference in reading: the influence of transitional probabilities on eye movements. Vision Research, 43:1735–1751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>Proceedings of the 25th International Conference of Machine Learning,</booktitle>
<pages>641--648</pages>
<contexts>
<context position="15523" citStr="Mnih and Hinton, 2007" startWordPosition="2433" endWordPosition="2437">.5 Recurrent neural network The RNN (see Figure 1) was trained in three stages, each taking the selected (unparsed) BNC sentences as training data. Figure 1: Architecture of neural network language model, and its three learning stages. Numbers indicate the number of units in each network layer. Stage 1: Developing word representations Neural network language models can benefit from using distributed word representations: Each word is assigned a vector in a continuous, high-dimensional space, such that words that are paradigmatically more similar are closer together (e.g., Bengio et al., 2003; Mnih and Hinton, 2007). Usually, these representations are learned together with the rest of the model, but here we used a more efficient approach in which word representations are learned in an unsupervised manner from simple co-occurrences in the training data. First, vectors of word co-occurrence frequencies were developed using Good-Turing (Gale and Sampson, 1995) smoothed frequency counts from the training corpus. Values in the vector corresponded to the smoothed frequencies with which each word directly preceded or followed the represented word. Thus, each word w was assigned a vector (f.,1, ..., f.,15508), s</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. Proceedings of the 25th International Conference of Machine Learning, pages 641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Rayner</author>
<author>Alexander Pollatsek</author>
</authors>
<title>Eye movements in reading: A tutorial review.</title>
<date>1987</date>
<booktitle>Attention and performance XII: the psychology of reading.,</booktitle>
<pages>327--362</pages>
<editor>In M. Coltheart, editor,</editor>
<location>London, UK.</location>
<contexts>
<context position="20860" citStr="Rayner and Pollatsek, 1987" startWordPosition="3303" endWordPosition="3306"> factor to the baseline, was assessed using likelihood tests. The resulting x2 statistic indicates the extent to which each surprisal estimate accounts for RT, and can thus serve as a measure of the psychological accuracy of each model. However, this kind of analysis assumes that RT for a word reflects processing of only that word, 3The collected RT data are available for download at www.stefanfrank.info/EACL2012. 402 but spill-over effects (in which processing difficulty at word wt shows up in the RT on wt+1) have been found in self-paced and natural reading (Just et al., 1982; Rayner, 1998; Rayner and Pollatsek, 1987). To evaluate these effects, the decrease in deviance after adding surprisal of the previous item to the baseline was also assessed. The following control predictors were included in the baseline regression model: Lexical factors: • Number of characters: Both physical size and number of characters have been found to affect RTs for a word (Rayner and Pollatsek, 1987), but the fixed-width font used in the experiment assured number of characters also encoded physical word length. • Frequency and forward transitional probability: The effects of these two factors have been repeatedly reported (e.g.</context>
</contexts>
<marker>Rayner, Pollatsek, 1987</marker>
<rawString>Keith Rayner and Alexander Pollatsek. 1987. Eye movements in reading: A tutorial review. In M. Coltheart, editor, Attention and performance XII: the psychology of reading., pages 327–362. Lawrence Erlbaum Associates, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Rayner</author>
</authors>
<title>Eye movements in reading and information processing: 20 years of research.</title>
<date>1998</date>
<journal>Psychological Bulletin,</journal>
<pages>124--372</pages>
<contexts>
<context position="20831" citStr="Rayner, 1998" startWordPosition="3301" endWordPosition="3302">sal as a fixed factor to the baseline, was assessed using likelihood tests. The resulting x2 statistic indicates the extent to which each surprisal estimate accounts for RT, and can thus serve as a measure of the psychological accuracy of each model. However, this kind of analysis assumes that RT for a word reflects processing of only that word, 3The collected RT data are available for download at www.stefanfrank.info/EACL2012. 402 but spill-over effects (in which processing difficulty at word wt shows up in the RT on wt+1) have been found in self-paced and natural reading (Just et al., 1982; Rayner, 1998; Rayner and Pollatsek, 1987). To evaluate these effects, the decrease in deviance after adding surprisal of the previous item to the baseline was also assessed. The following control predictors were included in the baseline regression model: Lexical factors: • Number of characters: Both physical size and number of characters have been found to affect RTs for a word (Rayner and Pollatsek, 1987), but the fixed-width font used in the experiment assured number of characters also encoded physical word length. • Frequency and forward transitional probability: The effects of these two factors have b</context>
</contexts>
<marker>Rayner, 1998</marker>
<rawString>Keith Rayner. 1998. Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124:372–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>Christophe Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>324--333</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="8290" citStr="Roark et al. (2009)" startWordPosition="1285" endWordPosition="1288">aturalistic source (the Dundee corpus) as the experimental stimuli, thus evaluating surprisal as a wide-coverage account of processing difficulty. They also employed a PSG, trained on a one-million-word language sample from the Wall Street Journal (part of the Penn Treebank II, Marcus et al., 1993). Using Roark’s (2001) incremental parser, they found significant effects of unlexicalized surprisal on RTs (see also Boston et al. for a similar approach and results for German texts). However, they failed to find an effect for lexicalized surprisal, over and above forward transitional probability. Roark et al. (2009) also looked at the 399 effects of syntactic and lexical surprisal, using RT data for short narrative texts. However, their estimates of these two surprisal values differ from those described above: In order to tease apart semantic and syntactic effects, they used Demberg and Keller’s lexicalized surprisal as a total surprisal measure, which they decompose into syntactic and lexical components. Their results show significant effects of both syntactic and lexical surprisal, although the latter was found to hold only for closed class words. Lack of a wider effect was attributed to data sparsity:</context>
<context position="13190" citStr="Roark et al., 2009" startWordPosition="2069" endWordPosition="2072">omly selected from three freely accessible on-line novels2 (for additional details, see Frank, 2012). The fictional narrative provides a good contrast to the pre2Obtained from www.free-online-novels.com. Having not been published elsewhere, it is unlikely participants had read the novels previously. 400 viously examined newspaper editorials from the Dundee corpus, since participants did not need prior knowledge regarding the details of the stories, and a less specialised language and style were employed. In addition, the randomly selected sentences did not make up coherent texts (in contrast, Roark et al., 2009, employed short stories), so that they were independent from each other, both for the models and the readers. 2.3 Part-of-speech tagging In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). By traini</context>
<context position="28749" citStr="Roark et al., 2009" startWordPosition="4562" endWordPosition="4565">explain a small amount of variance over and above the former (see Table 2).5 Model comparison x2(2) p-value Best models overall: POS- over word-based 10.40 0.006 word- over POS-based 47.02 «0.001 PSGs: POS- over word-based 6.89 0.032 word- over POS-based 25.50 «0.001 RNNs: POS- over word-based 5.80 0.055 word- over POS-based 49.74 «0.001 Table 2: Word-vs. POS-based models: comparisons between best models overall, and best models within each category. 4.3 Differences across word classes In order to make sure that the lexicalized surprisal effects found were not limited to closedclass words (as Roark et al., 2009, report), a further model comparison was performed by adding by-POS random slopes of surprisal to the models containing the baseline plus surprisal. If particular syntactic categories were contributing to the overall effect of surprisal more than others, including such random slopes would lead to additional variance being explained. However, this was not the case: inclusion of by-POS random slopes of surprisal did not lead to a significant improvement in model fit (PSG: x2(1) = 0.86,p = 0.35; RNN: x2(1) = 3.20,p = 0.07).6 5 Discussion The present study aimed to find further evidence for surpr</context>
<context position="31052" citStr="Roark et al. (2009)" startWordPosition="4925" endWordPosition="4928"> effect of lexicalized surprisal (according to a trigram model), but did not assess whether simpler predictability estimates (i.e., by a bigram model) could have accounted for those effects. Demberg and Keller’s (2008) failure to find lexicalized surprisal effects can be attributed both to the language corpus used to train the language models, as well as to the experimental texts used. Both were sourced from newspaper texts: As training corpora these are unrepresentative of a person’s linguistic experience, and as experimental texts they are heavily dependent on participant’s world knowledge. Roark et al. (2009), in contrast, used a more representative, albeit relatively small, training corpus, as well as narrativestyle stimuli, thus obtaining RTs less dependent on participant’s prior knowledge. With such an experimental set-up, they were able to demonstrate the effects of lexical surprisal for RT of closed-class, but not open-class, words, which they attributed to their differential frequency and to training-data sparsity: The limited Brown corpus would have been enough to produce accurate estimates of surprisal for function words, but not for the less frequent content words. A larger training corpu</context>
<context position="33015" citStr="Roark et al. (2009)" startWordPosition="5240" endWordPosition="5243">but not for word-based surprisal, our results show that lexicalized surprisal is a far better predictor of RTs than its unlexicalized counterpart. This is not surprising, given that while the unlexicalized models only have access to syntactic sources of information, the lexicalized models, like the human parser, can also take into account lexical cooccurrence trends. However, when a training corpus is not large enough to accurately capture the latter, it might still be able to model the former, given the higher frequency of occurrence of each possible item (POS vs. word) in the training data. Roark et al. (2009) also included in their analysis a POS-based surprisal estimate, which lost significance when the two components of the lexicalized surprisal were present, suggesting that such unlexicalized estimates can be interpreted only as a coarse version of the fully lexicalized surprisal, incorporating both syntactic and lexical sources of information at the same time. The results presented here do not replicate this finding: The best unlexicalized estimates were able to explain additional variance over and above the best wordbased estimates. However, this comparison contrasted two different model type</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, pages 324–333, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--249</pages>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27:249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank Project.</title>
<date>1991</date>
<tech>Technical report,</tech>
<location>Philadelphia, PA.</location>
<contexts>
<context position="13779" citStr="Santorini, 1991" startWordPosition="2162" endWordPosition="2163">ontrast, Roark et al., 2009, employed short stories), so that they were independent from each other, both for the models and the readers. 2.3 Part-of-speech tagging In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). By training language models and subsequently running them on the POS versions of the texts, unlexicalized surprisal values were estimated. 2.4 Phrase-structure grammars The Treebank formed by the parsed BNC sentences served as training data for Roark’s (2001) incremental parser. Following Frank and Bod (2011), a range of grammars was induced, differing in the features of the tree structure upon which rule probabilities were conditioned. In four grammars, probabilities depended on the lefthand side’s ancestors, from one up to four levels up in the parse tree (these grammars will be denoted a1</context>
</contexts>
<marker>Santorini, 1991</marker>
<rawString>Beatrice Santorini. 1991. Part-of-speech tagging guidelines for the Penn Treebank Project. Technical report, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathaniel J Smith</author>
<author>Roger Levy</author>
</authors>
<title>Optimal processing times in reading: a formal model and empirical investigation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>595--600</pages>
<location>Austin,TX.</location>
<contexts>
<context position="4995" citStr="Smith and Levy, 2008" startWordPosition="762" endWordPosition="765">ry, as a measure of the amount of information conveyed by a particular event. Improbable (‘surprising’) events carry more information than expected ones, so that surprisal is inversely related to probability, through a logarithmic function. In the context of sentence processing, if w1, ..., wt−1 denotes the sentence so far, then the cognitive effort required for processing the next word, wt, is assumed to be proportional to its surprisal: effort(t) a surprisal(wt) _ − log(P(wt|w1, ..., wt−1)) (1) Different theoretical groundings for this relationship have been proposed (Hale, 2001; Levy 2008; Smith and Levy, 2008). Smith and Levy derive it by taking a scale free assumption: Any linguistic unit can be subdivided into smaller entities (e.g., a sentence is comprised of words, a word of phonemes), so that time to process the whole will equal the sum of processing times for each part. Since the probability of the whole can be expressed as the product of the probabilities of the subunits, the function relating probability and effort must be logarithmic. Levy (2008), on the other hand, grounds surprisal in its informationtheoretical context, describing difficulty encountered in on-line sentence processing as </context>
<context position="6562" citStr="Smith and Levy (2008)" startWordPosition="1013" endWordPosition="1016">irical evidence for surprisal The simplest statistical language models that can be used to estimate surprisal values are n-gram models or Markov chains, which condition the probability of a given word only on its n − 1 preceding ones. Although Markov models theoretically limit the amount of prior information that is relevant for prediction of the next step, they are often used in linguistic context as an approximation to the full conditional probability. The effect of bigram probability (or forward transitional probability) has been repeatedly observed (e.g. McDonald and Shillcock, 2003), and Smith and Levy (2008) report an effect of lexical surprisal as estimated by a trigram model on RTs for the Dundee corpus (a collection of newspaper texts with eye-tracking data from ten participants; Kennedy and Pynte, 2005). Phrase structure grammars (PSGs) have also been amply used as language models (Boston et al., 2008; Brouwer et al., 2010; Demberg and Keller, 2008; Hale, 2001; Levy, 2008). PSGs can combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora. Information about hierarc</context>
<context position="10633" citStr="Smith and Levy (2008)" startWordPosition="1660" endWordPosition="1663">iability cannot be accounted for by the models. In addition, whereas the models treat each sentence as an independent unit, in the text corpora employed they make up coherent texts, and are therefore clearly dependent. Thirdly, the stimuli used by Demberg and Keller (2008) comprise a very particular linguistic style: journalistic editorials, reducing the ability to generalize conclusions to language in general. Finally, failure to find lexical surprisal effects can also be attributed to the training texts. Larger corpora are likely to be needed for training language models on actual 1Although Smith and Levy (2008) report an effect of trigrams, they did not check if it exceeded that of simpler bigrams. words than on POS (both the Brown corpus and the WSJ are relatively small), and in addition, the particular journalistic style of the WSJ might not be the best alternative for modeling human behaviour. Although similarity between the training and experimental data sets (both from newspaper sources) can improve the linguistic performance of the models, their ability to simulate human behaviour might be limited: Newspaper texts probably form just a small fraction of a person’s linguistic experience. This st</context>
<context position="30424" citStr="Smith and Levy (2008)" startWordPosition="4826" endWordPosition="4829">isal to explain a significant amount of variance in RT data for naturalistic texts, over and above that accounted for by other low-level lexical factors, such as frequency, length, and forward transitional probability. Although previous studies had presented results supporting such a probabilistic language processing account, evidence for wordbased surprisal was limited: Brouwer et al. (2010) only examined a specific psycholinguistic phenomenon, rather than a random language sample; Demberg and Keller (2008) reported effects that were only significant for POS but not word-based surprisal; and Smith and Levy (2008) found an effect of lexicalized surprisal (according to a trigram model), but did not assess whether simpler predictability estimates (i.e., by a bigram model) could have accounted for those effects. Demberg and Keller’s (2008) failure to find lexicalized surprisal effects can be attributed both to the language corpus used to train the language models, as well as to the experimental texts used. Both were sourced from newspaper texts: As training corpora these are unrepresentative of a person’s linguistic experience, and as experimental texts they are heavily dependent on participant’s world kn</context>
</contexts>
<marker>Smith, Levy, 2008</marker>
<rawString>Nathaniel J. Smith and Roger Levy. 2008. Optimal processing times in reading: a formal model and empirical investigation. In Proceedings of the 30th Annual Conference of the Cognitive Science Society, pages 595–600, Austin,TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational linguistics,</title>
<date>1995</date>
<pages>21--165</pages>
<contexts>
<context position="7432" citStr="Stolcke, 1995" startWordPosition="1150" endWordPosition="1152">ly used as language models (Boston et al., 2008; Brouwer et al., 2010; Demberg and Keller, 2008; Hale, 2001; Levy, 2008). PSGs can combine statistical exposure effects with explicit syntactic rules, by annotating norms with their respective probabilities, which can be estimated from occurrence counts in text corpora. Information about hierarchical sentence structure can thus be included in the models. In this way, Brouwer et al. trained a probabilistic contextfree grammar (PCFG) on 204,000 sentences extracted from Dutch newspapers to estimate lexical surprisal (using an Earley-Stolcke parser; Stolcke, 1995), showing that it could account for the noun phrase coordination bias previously described and explained by Frazier (1987) in terms of a minimal-attachment preference of the human parser. In contrast, Demberg and Keller used texts from a naturalistic source (the Dundee corpus) as the experimental stimuli, thus evaluating surprisal as a wide-coverage account of processing difficulty. They also employed a PSG, trained on a one-million-word language sample from the Wall Street Journal (part of the Penn Treebank II, Marcus et al., 1993). Using Roark’s (2001) incremental parser, they found signific</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational linguistics, 21:165– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>467--474</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="13668" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="2144" endWordPosition="2147">lised language and style were employed. In addition, the randomly selected sentences did not make up coherent texts (in contrast, Roark et al., 2009, employed short stories), so that they were independent from each other, both for the models and the readers. 2.3 Part-of-speech tagging In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). By training language models and subsequently running them on the POS versions of the texts, unlexicalized surprisal values were estimated. 2.4 Phrase-structure grammars The Treebank formed by the parsed BNC sentences served as training data for Roark’s (2001) incremental parser. Following Frank and Bod (2011), a range of grammars was induced, differing in the features of the tree structure upon which rule probabilities were conditioned. In four grammars, probabilities depended on th</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 467–474, Stroudsburg, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>