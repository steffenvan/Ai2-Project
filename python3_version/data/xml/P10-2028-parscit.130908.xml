<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001407">
<title confidence="0.999308">
Unsupervised Discourse Segmentation
of Documents with Inherently Parallel Structure
</title>
<author confidence="0.991371">
Minwoo Jeong and Ivan Titov
</author>
<affiliation confidence="0.99551">
Saarland University
</affiliation>
<address confidence="0.628716">
Saarbr¨ucken, Germany
</address>
<email confidence="0.998562">
{m.jeong|titov}@mmci.uni-saarland.de
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936190476191">
Documents often have inherently parallel
structure: they may consist of a text and
commentaries, or an abstract and a body,
or parts presenting alternative views on
the same problem. Revealing relations be-
tween the parts by jointly segmenting and
predicting links between the segments,
would help to visualize such documents
and construct friendlier user interfaces. To
address this problem, we propose an un-
supervised Bayesian model for joint dis-
course segmentation and alignment. We
apply our method to the “English as a sec-
ond language” podcast dataset where each
episode is composed of two parallel parts:
a story and an explanatory lecture. The
predicted topical links uncover hidden re-
lations between the stories and the lec-
tures. In this domain, our method achieves
competitive results, rivaling those of a pre-
viously proposed supervised technique.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875490909091">
Many documents consist of parts exhibiting a high
degree of parallelism: e.g., abstract and body of
academic publications, summaries and detailed
news stories, etc. This is especially common with
the emergence of the Web 2.0 technologies: many
texts on the web are now accompanied with com-
ments and discussions. Segmentation of these par-
allel parts into coherent fragments and discovery
of hidden relations between them would facilitate
the development of better user interfaces and im-
prove the performance of summarization and in-
formation retrieval systems.
Discourse segmentation of the documents com-
posed of parallel parts is a novel and challeng-
ing problem, as previous research has mostly fo-
cused on the linear segmentation of isolated texts
(e.g., (Hearst, 1994)). The most straightforward
approach would be to use a pipeline strategy,
where an existing segmentation algorithm finds
discourse boundaries of each part independently,
and then the segments are aligned. Or, conversely,
a sentence-alignment stage can be followed by a
segmentation stage. However, as we will see in our
experiments, these strategies may result in poor
segmentation and alignment quality.
To address this problem, we construct a non-
parametric Bayesian model for joint segmenta-
tion and alignment of parallel parts. In com-
parison with the discussed pipeline approaches,
our method has two important advantages: (1) it
leverages the lexical cohesion phenomenon (Hal-
liday and Hasan, 1976) in modeling the paral-
lel parts of documents, and (2) ensures that the
effective number of segments can grow adap-
tively. Lexical cohesion is an idea that topically-
coherent segments display compact lexical distri-
butions (Hearst, 1994; Utiyama and Isahara, 2001;
Eisenstein and Barzilay, 2008). We hypothesize
that not only isolated fragments but also each
group of linked fragments displays a compact and
consistent lexical distribution, and our generative
model leverages this inter-part cohesion assump-
tion.
In this paper, we consider the dataset of “En-
glish as a second language” (ESL) podcast1, where
each episode consists of two parallel parts: a story
(an example monologue or dialogue) and an ex-
planatory lecture discussing the meaning and us-
age of English expressions appearing in the story.
Fig. 1 presents an example episode, consisting of
two parallel parts, and their hidden topical rela-
tions.2 From the figure we may conclude that there
is a tendency of word repetition between each pair
of aligned segments, illustrating our hypothesis of
compactness of their joint distribution. Our goal is
</bodyText>
<footnote confidence="0.999134">
1http://www.eslpod.com/
2Episode no. 232 post on Jan. 08, 2007.
</footnote>
<page confidence="0.921697">
151
</page>
<note confidence="0.661465">
Proceedings of the ACL 2010 Conference Short Papers, pages 151–155,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.907943">
Story Lecture transcript
</subsectionHeader>
<bodyText confidence="0.980371833333333">
I have a day job, but I recently started a
small business on the side.
I didn&apos;t know anything about accounting
and my friend, Roland, said that he would
give me some advice.
Roland: So, the reason that you need to
do your bookkeeping is so you can
manage your cash flow.
...
This podcast is all about business vocabulary related to accounting.
The title of the podcast is Business Bookkeeping. ...
The story begins by Magdalena saying that she has a day job.
</bodyText>
<figure confidence="0.815997818181818">
A day job is your regular job that you work at from nine in the morning &apos;til five in the afternoon, for
example.
She also has a small business on the side. ...
Magdalena continues by saying that she didn&apos;t know anything about accounting and her friend,
Roland, said he would give her some advice.
Accounting is the job of keeping correct records of the money you spend; it&apos;s very similar to
bookkeeping. ...
Roland begins by saying that the reason that you need to do your bookkeeping is so you can
manage your cash flow.
Cash flow, flow, means having enough money to run your business - to pay your bills. ...
...
</figure>
<figureCaption confidence="0.99997">
Figure 1: An example episode of ESL podcast. Co-occurred words are represented in italic and underline.
</figureCaption>
<bodyText confidence="0.999732333333333">
to divide the lecture transcript into discourse units
and to align each unit to the related segment of the
story. Predicting these structures for the ESL pod-
cast could be the first step in development of an
e-learning system and a podcast search engine for
ESL learners.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999984793103448">
Discourse segmentation has been an active area
of research (Hearst, 1994; Utiyama and Isahara,
2001; Galley et al., 2003; Malioutov and Barzilay,
2006). Our work extends the Bayesian segmenta-
tion model (Eisenstein and Barzilay, 2008) for iso-
lated texts, to the problem of segmenting parallel
parts of documents.
The task of aligning each sentence of an abstract
to one or more sentences of the body has been
studied in the context of summarization (Marcu,
1999; Jing, 2002; Daum´e and Marcu, 2004). Our
work is different in that we do not try to extract
the most relevant sentence but rather aim to find
coherent fragments with maximally overlapping
lexical distributions. Similarly, the query-focused
summarization (e.g., (Daum´e and Marcu, 2006))
is also related but it focuses on sentence extraction
rather than on joint segmentation.
We are aware of only one previous work on joint
segmentation and alignment of multiple texts (Sun
et al., 2007) but their approach is based on similar-
ity functions rather than on modeling lexical cohe-
sion in the generative framework. Our application,
the analysis of the ESL podcast, was previously
studied in (Noh et al., 2010). They proposed a su-
pervised method which is driven by pairwise clas-
sification decisions. The main drawback of their
approach is that it neglects the discourse structure
and the lexical cohesion phenomenon.
</bodyText>
<sectionHeader confidence="0.992436" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9873662">
In this section we describe our model for discourse
segmentation of documents with inherently paral-
lel structure. We start by clarifying our assump-
tions about their structure.
We assume that a document x consists of K
parallel parts, that is, x = {x(k)}k=1:K, and
each part of the document consists of segments,
x(k) = {s(k)
i }i=1:�. Note that the effective num-
ber of fragments I is unknown. Each segment can
either be specific to this part (drawn from a part-
specific language model 0(k)
i ) or correspond to
the entire document (drawn from a document-level
language model 0(doc)
i ). For example, the first
and the second sentences of the lecture transcript
in Fig. 1 are part-specific, whereas other linked
sentences belong to the document-level segments.
The document-level language models define top-
ical links between segments in different parts of
the document, whereas the part-specific language
models define the linear segmentation of the re-
maining unaligned text.
Each document-level language model corre-
sponds to the set of aligned segments, at most one
segment per part. Similarly, each part-specific lan-
guage model corresponds to a single segment of
the single corresponding part. Note that all the
documents are modeled independently, as we aim
not to discover collection-level topics (as e.g. in
(Blei et al., 2003)), but to perform joint discourse
segmentation and alignment.
Unlike (Eisenstein and Barzilay, 2008), we can-
not make an assumption that the number of seg-
ments is known a-priori, as the effective number of
part-specific segments can vary significantly from
document to document, depending on their size
and structure. To tackle this problem, we use
Dirichlet processes (DP) (Ferguson, 1973) to de-
</bodyText>
<page confidence="0.992928">
152
</page>
<bodyText confidence="0.999192166666667">
fine priors on the number of segments. We incor-
porate them in our model in a similar way as it
is done for the Latent Dirichlet Allocation (LDA)
by Yu et al. (2005). Unlike the standard LDA, the
topic proportions are chosen not from a Dirichlet
prior but from the marginal distribution GEM(α)
defined by the stick breaking construction (Sethu-
raman, 1994), where α is the concentration param-
eter of the underlying DP distribution. GEM(α)
defines a distribution of partitions of the unit inter-
val into a countable number of parts.
The formal definition of our model is as follows:
</bodyText>
<listItem confidence="0.999142666666667">
• Draw the document-level topic proportions β(doc) —
GEM(a(doc)).
• Choose the document-level language model �(doc)
</listItem>
<equation confidence="0.8784095">
i —
Dir(ry(doc)) for i E {1, 2, ...}.
</equation>
<listItem confidence="0.814282142857143">
• Draw the part-specific topic proportions β(k) —
GEM(a(k)) fork E {1, ... ,K}.
• Choose the part-specific language models �(k)
i —
Dir(ry(k)) for k E {1, ... ,K} and i E {1, 2, ...}.
• For each part k and each sentence n:
– Draw type t(k)
</listItem>
<equation confidence="0.997314916666667">
n — Unif(Doc, Part).
– If (t(k)
n = Doc); draw topic z(k)
n — β(doc); gen-
erate words xn Mult(�(dkj)zW)
n
– Otherwise; draw topic z(k)
n — β(k); generate
words x(k)
n — Mult(�(k) ).
z���
n
</equation>
<bodyText confidence="0.9727919375">
The priors ,y(doc), ,y(k), α(doc) and α(k) can be
estimated at learning time using non-informative
hyperpriors (as we do in our experiments), or set
manually to indicate preferences of segmentation
granularity.
At inference time, we enforce each latent topic
zn to be assigned to a contiguous span of text,
(k)
assuming that coherent topics are not recurring
across the document (Halliday and Hasan, 1976).
It also reduces the search space and, consequently,
speeds up our sampling-based inference by reduc-
ing the time needed for Monte Carlo chains to
mix. In fact, this constraint can be integrated in the
model definition but it would significantly compli-
cate the model description.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.934987333333333">
As exact inference is intractable, we follow Eisen-
stein and Barzilay (2008) and instead use a
Metropolis-Hastings (MH) algorithm. At each
iteration of the MH algorithm, a new potential
alignment-segmentation pair (z0, t0) is drawn from
a proposal distribution Q(z0, t0|z, t), where (z, t)
</bodyText>
<figure confidence="0.991304">
(a) (b) (c)
</figure>
<figureCaption confidence="0.9987885">
Figure 2: Three types of moves: (a) shift, (b) split
and (c) merge.
</figureCaption>
<equation confidence="0.900673">
is the current segmentation and its type. The new
pair (z0, t0) is accepted with the probability
min � 1
P(z0, t0, x)Q(z0, t0 |z, t)
, P(z, t, x)Q(z, t|z0, t0 ) -
</equation>
<bodyText confidence="0.998236666666667">
In order to implement the MH algorithm for our
model, we need to define the set of potential moves
(i.e. admissible changes from (z, t) to (z&apos;, t&apos;)),
and the proposal distribution Q over these moves.
If the actual number of segments is known and
only a linear discourse structure is acceptable, then
a single move, shift of the segment border (Fig.
2(a)), is sufficient (Eisenstein and Barzilay, 2008).
In our case, however, a more complex set of moves
is required.
We make two assumptions which are moti-
vated by the problem considered in Section 5:
we assume that (1) we are given the number of
document-level segments and also that (2) the
aligned segments appear in the same order in each
part of the document. With these assumptions in
mind, we introduce two additional moves (Fig.
2(b) and (c)):
</bodyText>
<listItem confidence="0.9400796">
• Split move: select a segment, and split it at
one of the spanned sentences; if the segment
was a document-level segment then one of
the fragments becomes the same document-
level segment.
• Merge move: select a pair of adjacent seg-
ments where at least one of the segments is
part-specific, and merge them; if one of them
was a document-level segment then the new
segment has the same document-level topic.
</listItem>
<bodyText confidence="0.969155363636364">
All the moves are selected with the uniform prob-
ability, and the distance c for the shift move is
drawn from the proposal distribution proportional
to c−1/cmax. The moves are selected indepen-
dently for each part.
Although the above two assumptions are not
crucial as a simple modification to the set of moves
would support both introduction and deletion of
document-level fragments, this modification was
not necessary for our experiments.
)
</bodyText>
<page confidence="0.998921">
153
</page>
<sectionHeader confidence="0.999223" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999552">
5.1 Dataset and setup
</subsectionHeader>
<bodyText confidence="0.999951510204082">
Dataset We apply our model to the ESL podcast
dataset (Noh et al., 2010) of 200 episodes, with
an average of 17 sentences per story and 80 sen-
tences per lecture transcript. The gold standard
alignments assign each fragment of the story to a
segment of the lecture transcript. We can induce
segmentations at different levels of granularity on
both the story and the lecture side. However, given
that the segmentation of the story was obtained by
an automatic sentence splitter, there is no reason
to attempt to reproduce this segmentation. There-
fore, for quantitative evaluation purposes we fol-
low Noh et al. (2010) and restrict our model to
alignment structures which agree with the given
segmentation of the story. For all evaluations, we
apply standard stemming algorithm and remove
common stop words.
Evaluation metrics To measure the quality of seg-
mentation of the lecture transcript, we use two
standard metrics, Pk (Beeferman et al., 1999) and
WindowDiff (WD) (Pevzner and Hearst, 2002),
but both metrics disregard the alignment links (i.e.
the topic labels). Consequently, we also use the
macro-averaged F1 score on pairs of aligned span,
which measures both the segmentation and align-
ment quality.
Baseline Since there has been little previous re-
search on this problem, we compare our results
against two straightforward unsupervised base-
lines. For the first baseline, we consider the
pairwise sentence alignment (SentAlign) based
on the unigram and bigram overlap. The sec-
ond baseline is a pipeline approach (Pipeline),
where we first segment the lecture transcript with
BayesSeg (Eisenstein and Barzilay, 2008) and
then use the pairwise alignment to find their best
alignment to the segments of the story.
Our model We evaluate our joint model of seg-
mentation and alignment both with and without
the split/merge moves. For the model without
these moves, we set the desired number of seg-
ments in the lecture to be equal to the actual num-
ber of segments in the story I. In this setting,
the moves can only adjust positions of the seg-
ment borders. For the model with the split/merge
moves, we start with the same number of segments
I but it can be increased or decreased during in-
ference. For evaluation of our model, we run our
inference algorithm from five random states, and
</bodyText>
<table confidence="0.999870285714286">
Method Pk WD 1 − F1
Uniform 0.453 0.458 0.682
SentAlign 0.446 0.547 0.313
Pipeline (I) 0.250 0.249 0.443
Pipeline (2I+1) 0.268 0.289 0.318
Our model (I) 0.193 0.204 0.254
+split/merge 0.181 0.193 0.239
</table>
<tableCaption confidence="0.9706695">
Table 1: Results on the ESL podcast dataset. For
all metrics, lower values are better.
</tableCaption>
<bodyText confidence="0.9994922">
take the 100,000th iteration of each chain as a sam-
ple. Results are the average over these five runs.
Also we perform L-BFGS optimization to auto-
matically adjust the non-informative hyperpriors
after each 1,000 iterations of sampling.
</bodyText>
<subsectionHeader confidence="0.942763">
5.2 Result
</subsectionHeader>
<bodyText confidence="0.999849045454546">
Table 1 summarizes the obtained results. ‘Uni-
form’ denotes the minimal baseline which uni-
formly draws a random set of I spans for each lec-
ture, and then aligns them to the segments of the
story preserving the linear order. Also, we con-
sider two variants of the pipeline approach: seg-
menting the lecture on I and 2I + 1 segments, re-
spectively.3 Our joint model substantially outper-
forms the baselines. The difference is statistically
significant with the level p &lt; .01 measured with
the paired t-test. The significant improvement over
the pipeline results demonstrates benefits of joint
modeling for the considered problem. Moreover,
additional benefits are obtained by using the DP
priors and the split/merge moves (the last line in
Table 1). Finally, our model significantly outper-
forms the previously proposed supervised model
(Noh et al., 2010): they report micro-averaged F1
score 0.698 while our best model achieves 0.778
with the same metric. This observation confirms
that lexical cohesion modeling is crucial for suc-
cessful discourse analysis.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999514">
We studied the problem of joint discourse segmen-
tation and alignment of documents with inherently
parallel structure and achieved favorable results on
the ESL podcast dataset outperforming the cas-
caded baselines. Accurate prediction of these hid-
den relations would open interesting possibilities
</bodyText>
<footnote confidence="0.983732666666667">
3The use of the DP priors and the split/merge moves on
the first stage of the pipeline did not result in any improve-
ment in accuracy.
</footnote>
<page confidence="0.999508">
154
</page>
<bodyText confidence="0.9999142">
for construction of friendlier user interfaces. One
example being an application which, given a user-
selected fragment of the abstract, produces a sum-
mary from the aligned segment of the document
body.
</bodyText>
<sectionHeader confidence="0.939544" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9999065">
The authors acknowledge the support of the
Excellence Cluster on Multimodal Computing
and Interaction (MMCI), and also thank Mikhail
Kozhevnikov and the anonymous reviewers for
their valuable comments, and Hyungjong Noh for
providing their data.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987075862069">
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation.
Computational Linguistics, 34(1–3):177–210.
David M. Blei, Andrew Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. JMLR, 3:993–
1022.
Hal Daum´e and Daniel Marcu. 2004. A phrase-based
hmm approach to document/abstract alignment. In
Proceedings of EMNLP, pages 137–144.
Hal Daum´e and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL,
pages 305–312.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334–343.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some non-parametric problems. Annals of Statistics,
1:209–230.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of ACL, pages 562–569.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman.
Marti Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings ofACL, pages 9–16.
Hongyan Jing. 2002. Using hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527–543.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25–32.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
Proceedings of ACM SIGIR, pages 137–144.
Hyungjong Noh, Minwoo Jeong, Sungjin Lee,
Jonghoon Lee, and Gary Geunbae Lee. 2010.
Script-description pair extraction from text docu-
ments of English as second language podcast. In
Proceedings of the 2nd International Conference on
Computer Supported Education.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):19–
36.
Jayaram Sethuraman. 1994. A constructive definition
of Dirichlet priors. Statistica Sinica, 4:639–650.
Bingjun Sun, Prasenjit Mitra, C. Lee Giles, John Yen,
and Hongyuan Zha. 2007. Topic segmentation
with shared topic detection and alignment of mul-
tiple documents. In Proceedings of ACM SIGIR,
pages 199–206.
Masao Utiyama and Hitoshi Isahara. 2001. A statis-
tical model for domain-independent text segmenta-
tion. In Proceedings of ACL, pages 491–498.
Kai Yu, Shipeng Yu, and Vokler Tresp. 2005. Dirichlet
enhanced latent semantic analysis. In Proceedings
ofAISTATS.
</reference>
<page confidence="0.999009">
155
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904181">
<title confidence="0.975347">Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</title>
<author confidence="0.992726">Jeong Titov</author>
<affiliation confidence="0.999996">Saarland University</affiliation>
<address confidence="0.979572">Saarbr¨ucken, Germany</address>
<abstract confidence="0.998934363636364">Documents often have inherently parallel structure: they may consist of a text and commentaries, or an abstract and a body, or parts presenting alternative views on the same problem. Revealing relations between the parts by jointly segmenting and predicting links between the segments, would help to visualize such documents and construct friendlier user interfaces. To address this problem, we propose an unsupervised Bayesian model for joint discourse segmentation and alignment. We apply our method to the “English as a second language” podcast dataset where each episode is composed of two parallel parts: a story and an explanatory lecture. The predicted topical links uncover hidden relations between the stories and the lectures. In this domain, our method achieves competitive results, rivaling those of a previously proposed supervised technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>34--1</pages>
<contexts>
<context position="13485" citStr="Beeferman et al., 1999" startWordPosition="2212" endWordPosition="2215">s of granularity on both the story and the lecture side. However, given that the segmentation of the story was obtained by an automatic sentence splitter, there is no reason to attempt to reproduce this segmentation. Therefore, for quantitative evaluation purposes we follow Noh et al. (2010) and restrict our model to alignment structures which agree with the given segmentation of the story. For all evaluations, we apply standard stemming algorithm and remove common stop words. Evaluation metrics To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). Consequently, we also use the macro-averaged F1 score on pairs of aligned span, which measures both the segmentation and alignment quality. Baseline Since there has been little previous research on this problem, we compare our results against two straightforward unsupervised baselines. For the first baseline, we consider the pairwise sentence alignment (SentAlign) based on the unigram and bigram overlap. The second baseline is a pipeline approach (Pipeline), where we first s</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Computational Linguistics, 34(1–3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>JMLR,</journal>
<volume>3</volume>
<pages>1022</pages>
<contexts>
<context position="8063" citStr="Blei et al., 2003" startWordPosition="1285" endWordPosition="1288">ntences belong to the document-level segments. The document-level language models define topical links between segments in different parts of the document, whereas the part-specific language models define the linear segmentation of the remaining unaligned text. Each document-level language model corresponds to the set of aligned segments, at most one segment per part. Similarly, each part-specific language model corresponds to a single segment of the single corresponding part. Note that all the documents are modeled independently, as we aim not to discover collection-level topics (as e.g. in (Blei et al., 2003)), but to perform joint discourse segmentation and alignment. Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. To tackle this problem, we use Dirichlet processes (DP) (Ferguson, 1973) to de152 fine priors on the number of segments. We incorporate them in our model in a similar way as it is done for the Latent Dirichlet Allocation (LDA) by Yu et al. (2005). Unlike the standard LDA, the topic </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. JMLR, 3:993– 1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A phrase-based hmm approach to document/abstract alignment.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>137--144</pages>
<marker>Daum´e, Marcu, 2004</marker>
<rawString>Hal Daum´e and Daniel Marcu. 2004. A phrase-based hmm approach to document/abstract alignment. In Proceedings of EMNLP, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian queryfocused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>305--312</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e and Daniel Marcu. 2006. Bayesian queryfocused summarization. In Proceedings of ACL, pages 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>334--343</pages>
<contexts>
<context position="2830" citStr="Eisenstein and Barzilay, 2008" startWordPosition="421" endWordPosition="424">in poor segmentation and alignment quality. To address this problem, we construct a nonparametric Bayesian model for joint segmentation and alignment of parallel parts. In comparison with the discussed pipeline approaches, our method has two important advantages: (1) it leverages the lexical cohesion phenomenon (Halliday and Hasan, 1976) in modeling the parallel parts of documents, and (2) ensures that the effective number of segments can grow adaptively. Lexical cohesion is an idea that topicallycoherent segments display compact lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). We hypothesize that not only isolated fragments but also each group of linked fragments displays a compact and consistent lexical distribution, and our generative model leverages this inter-part cohesion assumption. In this paper, we consider the dataset of “English as a second language” (ESL) podcast1, where each episode consists of two parallel parts: a story (an example monologue or dialogue) and an explanatory lecture discussing the meaning and usage of English expressions appearing in the story. Fig. 1 presents an example episode, consisting of two parallel parts, and their hidden topic</context>
<context position="5585" citStr="Eisenstein and Barzilay, 2008" startWordPosition="882" endWordPosition="885">.. ... Figure 1: An example episode of ESL podcast. Co-occurred words are represented in italic and underline. to divide the lecture transcript into discourse units and to align each unit to the related segment of the story. Predicting these structures for the ESL podcast could be the first step in development of an e-learning system and a podcast search engine for ESL learners. 2 Related Work Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum´e and Marcu, 2006)) is also related but it focuses on sentence extraction rather than on joint segmentat</context>
<context position="8163" citStr="Eisenstein and Barzilay, 2008" startWordPosition="1298" endWordPosition="1301"> topical links between segments in different parts of the document, whereas the part-specific language models define the linear segmentation of the remaining unaligned text. Each document-level language model corresponds to the set of aligned segments, at most one segment per part. Similarly, each part-specific language model corresponds to a single segment of the single corresponding part. Note that all the documents are modeled independently, as we aim not to discover collection-level topics (as e.g. in (Blei et al., 2003)), but to perform joint discourse segmentation and alignment. Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. To tackle this problem, we use Dirichlet processes (DP) (Ferguson, 1973) to de152 fine priors on the number of segments. We incorporate them in our model in a similar way as it is done for the Latent Dirichlet Allocation (LDA) by Yu et al. (2005). Unlike the standard LDA, the topic proportions are chosen not from a Dirichlet prior but from the marginal distribution GEM(α) defined </context>
<context position="10395" citStr="Eisenstein and Barzilay (2008)" startWordPosition="1682" endWordPosition="1686">or set manually to indicate preferences of segmentation granularity. At inference time, we enforce each latent topic zn to be assigned to a contiguous span of text, (k) assuming that coherent topics are not recurring across the document (Halliday and Hasan, 1976). It also reduces the search space and, consequently, speeds up our sampling-based inference by reducing the time needed for Monte Carlo chains to mix. In fact, this constraint can be integrated in the model definition but it would significantly complicate the model description. 4 Inference As exact inference is intractable, we follow Eisenstein and Barzilay (2008) and instead use a Metropolis-Hastings (MH) algorithm. At each iteration of the MH algorithm, a new potential alignment-segmentation pair (z0, t0) is drawn from a proposal distribution Q(z0, t0|z, t), where (z, t) (a) (b) (c) Figure 2: Three types of moves: (a) shift, (b) split and (c) merge. is the current segmentation and its type. The new pair (z0, t0) is accepted with the probability min � 1 P(z0, t0, x)Q(z0, t0 |z, t) , P(z, t, x)Q(z, t|z0, t0 ) - In order to implement the MH algorithm for our model, we need to define the set of potential moves (i.e. admissible changes from (z, t) to (z&apos;,</context>
<context position="14160" citStr="Eisenstein and Barzilay, 2008" startWordPosition="2314" endWordPosition="2317">2), but both metrics disregard the alignment links (i.e. the topic labels). Consequently, we also use the macro-averaged F1 score on pairs of aligned span, which measures both the segmentation and alignment quality. Baseline Since there has been little previous research on this problem, we compare our results against two straightforward unsupervised baselines. For the first baseline, we consider the pairwise sentence alignment (SentAlign) based on the unigram and bigram overlap. The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. Our model We evaluate our joint model of segmentation and alignment both with and without the split/merge moves. For the model without these moves, we set the desired number of segments in the lecture to be equal to the actual number of segments in the story I. In this setting, the moves can only adjust positions of the segment borders. For the model with the split/merge moves, we start with the same number of segments I but it can be increased or decreased during inference. For evaluation of our mo</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of EMNLP, pages 334–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A Bayesian analysis of some non-parametric problems.</title>
<date>1973</date>
<journal>Annals of Statistics,</journal>
<pages>1--209</pages>
<contexts>
<context position="8452" citStr="Ferguson, 1973" startWordPosition="1346" endWordPosition="1347">specific language model corresponds to a single segment of the single corresponding part. Note that all the documents are modeled independently, as we aim not to discover collection-level topics (as e.g. in (Blei et al., 2003)), but to perform joint discourse segmentation and alignment. Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. To tackle this problem, we use Dirichlet processes (DP) (Ferguson, 1973) to de152 fine priors on the number of segments. We incorporate them in our model in a similar way as it is done for the Latent Dirichlet Allocation (LDA) by Yu et al. (2005). Unlike the standard LDA, the topic proportions are chosen not from a Dirichlet prior but from the marginal distribution GEM(α) defined by the stick breaking construction (Sethuraman, 1994), where α is the concentration parameter of the underlying DP distribution. GEM(α) defines a distribution of partitions of the unit interval into a countable number of parts. The formal definition of our model is as follows: • Draw the </context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S. Ferguson. 1973. A Bayesian analysis of some non-parametric problems. Annals of Statistics, 1:209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Eric FoslerLussier, and Hongyan Jing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>562--569</pages>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen R. McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of ACL, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in English. Longman.</note>
<contexts>
<context position="2539" citStr="Halliday and Hasan, 1976" startWordPosition="375" endWordPosition="379">here an existing segmentation algorithm finds discourse boundaries of each part independently, and then the segments are aligned. Or, conversely, a sentence-alignment stage can be followed by a segmentation stage. However, as we will see in our experiments, these strategies may result in poor segmentation and alignment quality. To address this problem, we construct a nonparametric Bayesian model for joint segmentation and alignment of parallel parts. In comparison with the discussed pipeline approaches, our method has two important advantages: (1) it leverages the lexical cohesion phenomenon (Halliday and Hasan, 1976) in modeling the parallel parts of documents, and (2) ensures that the effective number of segments can grow adaptively. Lexical cohesion is an idea that topicallycoherent segments display compact lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). We hypothesize that not only isolated fragments but also each group of linked fragments displays a compact and consistent lexical distribution, and our generative model leverages this inter-part cohesion assumption. In this paper, we consider the dataset of “English as a second language” (ESL) podcast1, wh</context>
<context position="10028" citStr="Halliday and Hasan, 1976" startWordPosition="1624" endWordPosition="1627">ch sentence n: – Draw type t(k) n — Unif(Doc, Part). – If (t(k) n = Doc); draw topic z(k) n — β(doc); generate words xn Mult(�(dkj)zW) n – Otherwise; draw topic z(k) n — β(k); generate words x(k) n — Mult(�(k) ). z��� n The priors ,y(doc), ,y(k), α(doc) and α(k) can be estimated at learning time using non-informative hyperpriors (as we do in our experiments), or set manually to indicate preferences of segmentation granularity. At inference time, we enforce each latent topic zn to be assigned to a contiguous span of text, (k) assuming that coherent topics are not recurring across the document (Halliday and Hasan, 1976). It also reduces the search space and, consequently, speeds up our sampling-based inference by reducing the time needed for Monte Carlo chains to mix. In fact, this constraint can be integrated in the model definition but it would significantly complicate the model description. 4 Inference As exact inference is intractable, we follow Eisenstein and Barzilay (2008) and instead use a Metropolis-Hastings (MH) algorithm. At each iteration of the MH algorithm, a new potential alignment-segmentation pair (z0, t0) is drawn from a proposal distribution Q(z0, t0|z, t), where (z, t) (a) (b) (c) Figure </context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1839" citStr="Hearst, 1994" startWordPosition="273" endWordPosition="274">ies, etc. This is especially common with the emergence of the Web 2.0 technologies: many texts on the web are now accompanied with comments and discussions. Segmentation of these parallel parts into coherent fragments and discovery of hidden relations between them would facilitate the development of better user interfaces and improve the performance of summarization and information retrieval systems. Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). The most straightforward approach would be to use a pipeline strategy, where an existing segmentation algorithm finds discourse boundaries of each part independently, and then the segments are aligned. Or, conversely, a sentence-alignment stage can be followed by a segmentation stage. However, as we will see in our experiments, these strategies may result in poor segmentation and alignment quality. To address this problem, we construct a nonparametric Bayesian model for joint segmentation and alignment of parallel parts. In comparison with the discussed pipeline approaches, our method has t</context>
<context position="5424" citStr="Hearst, 1994" startWordPosition="860" endWordPosition="861">do your bookkeeping is so you can manage your cash flow. Cash flow, flow, means having enough money to run your business - to pay your bills. ... ... Figure 1: An example episode of ESL podcast. Co-occurred words are represented in italic and underline. to divide the lecture transcript into discourse units and to align each unit to the related segment of the story. Predicting these structures for the ESL podcast could be the first step in development of an e-learning system and a podcast search engine for ESL learners. 2 Related Work Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions.</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings ofACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden Markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="5824" citStr="Jing, 2002" startWordPosition="926" endWordPosition="927">e ESL podcast could be the first step in development of an e-learning system and a podcast search engine for ESL learners. 2 Related Work Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum´e and Marcu, 2006)) is also related but it focuses on sentence extraction rather than on joint segmentation. We are aware of only one previous work on joint segmentation and alignment of multiple texts (Sun et al., 2007) but their approach is based on similarity functions rather than on modeling lexical cohesion in the generative framework. </context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Hongyan Jing. 2002. Using hidden Markov modeling to decompose human-written summaries. Computational Linguistics, 28(4):527–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="5503" citStr="Malioutov and Barzilay, 2006" startWordPosition="870" endWordPosition="873">flow, flow, means having enough money to run your business - to pay your bills. ... ... Figure 1: An example episode of ESL podcast. Co-occurred words are represented in italic and underline. to divide the lecture transcript into discourse units and to align each unit to the related segment of the story. Predicting these structures for the ESL podcast could be the first step in development of an e-learning system and a podcast search engine for ESL learners. 2 Related Work Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum´e and Marcu, 2006)) is</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="5812" citStr="Marcu, 1999" startWordPosition="924" endWordPosition="925">ctures for the ESL podcast could be the first step in development of an e-learning system and a podcast search engine for ESL learners. 2 Related Work Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum´e and Marcu, 2006)) is also related but it focuses on sentence extraction rather than on joint segmentation. We are aware of only one previous work on joint segmentation and alignment of multiple texts (Sun et al., 2007) but their approach is based on similarity functions rather than on modeling lexical cohesion in the generative</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of large-scale corpora for summarization research. In Proceedings of ACM SIGIR, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyungjong Noh</author>
<author>Minwoo Jeong</author>
<author>Sungjin Lee</author>
<author>Jonghoon Lee</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Script-description pair extraction from text documents of English as second language podcast.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2nd International Conference on Computer Supported Education.</booktitle>
<contexts>
<context position="6518" citStr="Noh et al., 2010" startWordPosition="1035" endWordPosition="1038">ract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum´e and Marcu, 2006)) is also related but it focuses on sentence extraction rather than on joint segmentation. We are aware of only one previous work on joint segmentation and alignment of multiple texts (Sun et al., 2007) but their approach is based on similarity functions rather than on modeling lexical cohesion in the generative framework. Our application, the analysis of the ESL podcast, was previously studied in (Noh et al., 2010). They proposed a supervised method which is driven by pairwise classification decisions. The main drawback of their approach is that it neglects the discourse structure and the lexical cohesion phenomenon. 3 Model In this section we describe our model for discourse segmentation of documents with inherently parallel structure. We start by clarifying our assumptions about their structure. We assume that a document x consists of K parallel parts, that is, x = {x(k)}k=1:K, and each part of the document consists of segments, x(k) = {s(k) i }i=1:�. Note that the effective number of fragments I is u</context>
<context position="12612" citStr="Noh et al., 2010" startWordPosition="2070" endWordPosition="2073">segment then the new segment has the same document-level topic. All the moves are selected with the uniform probability, and the distance c for the shift move is drawn from the proposal distribution proportional to c−1/cmax. The moves are selected independently for each part. Although the above two assumptions are not crucial as a simple modification to the set of moves would support both introduction and deletion of document-level fragments, this modification was not necessary for our experiments. ) 153 5 Experiment 5.1 Dataset and setup Dataset We apply our model to the ESL podcast dataset (Noh et al., 2010) of 200 episodes, with an average of 17 sentences per story and 80 sentences per lecture transcript. The gold standard alignments assign each fragment of the story to a segment of the lecture transcript. We can induce segmentations at different levels of granularity on both the story and the lecture side. However, given that the segmentation of the story was obtained by an automatic sentence splitter, there is no reason to attempt to reproduce this segmentation. Therefore, for quantitative evaluation purposes we follow Noh et al. (2010) and restrict our model to alignment structures which agre</context>
<context position="16207" citStr="Noh et al., 2010" startWordPosition="2662" endWordPosition="2665">e consider two variants of the pipeline approach: segmenting the lecture on I and 2I + 1 segments, respectively.3 Our joint model substantially outperforms the baselines. The difference is statistically significant with the level p &lt; .01 measured with the paired t-test. The significant improvement over the pipeline results demonstrates benefits of joint modeling for the considered problem. Moreover, additional benefits are obtained by using the DP priors and the split/merge moves (the last line in Table 1). Finally, our model significantly outperforms the previously proposed supervised model (Noh et al., 2010): they report micro-averaged F1 score 0.698 while our best model achieves 0.778 with the same metric. This observation confirms that lexical cohesion modeling is crucial for successful discourse analysis. 6 Conclusions We studied the problem of joint discourse segmentation and alignment of documents with inherently parallel structure and achieved favorable results on the ESL podcast dataset outperforming the cascaded baselines. Accurate prediction of these hidden relations would open interesting possibilities 3The use of the DP priors and the split/merge moves on the first stage of the pipelin</context>
</contexts>
<marker>Noh, Jeong, Lee, Lee, Lee, 2010</marker>
<rawString>Hyungjong Noh, Minwoo Jeong, Sungjin Lee, Jonghoon Lee, and Gary Geunbae Lee. 2010. Script-description pair extraction from text documents of English as second language podcast. In Proceedings of the 2nd International Conference on Computer Supported Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>36</pages>
<contexts>
<context position="13532" citStr="Pevzner and Hearst, 2002" startWordPosition="2219" endWordPosition="2222">cture side. However, given that the segmentation of the story was obtained by an automatic sentence splitter, there is no reason to attempt to reproduce this segmentation. Therefore, for quantitative evaluation purposes we follow Noh et al. (2010) and restrict our model to alignment structures which agree with the given segmentation of the story. For all evaluations, we apply standard stemming algorithm and remove common stop words. Evaluation metrics To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). Consequently, we also use the macro-averaged F1 score on pairs of aligned span, which measures both the segmentation and alignment quality. Baseline Since there has been little previous research on this problem, we compare our results against two straightforward unsupervised baselines. For the first baseline, we consider the pairwise sentence alignment (SentAlign) based on the unigram and bigram overlap. The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Ei</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayaram Sethuraman</author>
</authors>
<title>A constructive definition of Dirichlet priors.</title>
<date>1994</date>
<journal>Statistica Sinica,</journal>
<pages>4--639</pages>
<contexts>
<context position="8816" citStr="Sethuraman, 1994" startWordPosition="1410" endWordPosition="1412">the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. To tackle this problem, we use Dirichlet processes (DP) (Ferguson, 1973) to de152 fine priors on the number of segments. We incorporate them in our model in a similar way as it is done for the Latent Dirichlet Allocation (LDA) by Yu et al. (2005). Unlike the standard LDA, the topic proportions are chosen not from a Dirichlet prior but from the marginal distribution GEM(α) defined by the stick breaking construction (Sethuraman, 1994), where α is the concentration parameter of the underlying DP distribution. GEM(α) defines a distribution of partitions of the unit interval into a countable number of parts. The formal definition of our model is as follows: • Draw the document-level topic proportions β(doc) — GEM(a(doc)). • Choose the document-level language model �(doc) i — Dir(ry(doc)) for i E {1, 2, ...}. • Draw the part-specific topic proportions β(k) — GEM(a(k)) fork E {1, ... ,K}. • Choose the part-specific language models �(k) i — Dir(ry(k)) for k E {1, ... ,K} and i E {1, 2, ...}. • For each part k and each sentence n</context>
</contexts>
<marker>Sethuraman, 1994</marker>
<rawString>Jayaram Sethuraman. 1994. A constructive definition of Dirichlet priors. Statistica Sinica, 4:639–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bingjun Sun</author>
<author>Prasenjit Mitra</author>
<author>C Lee Giles</author>
<author>John Yen</author>
<author>Hongyuan Zha</author>
</authors>
<title>Topic segmentation with shared topic detection and alignment of multiple documents.</title>
<date>2007</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>199--206</pages>
<contexts>
<context position="6301" citStr="Sun et al., 2007" startWordPosition="999" endWordPosition="1002"> each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focused summarization (e.g., (Daum´e and Marcu, 2006)) is also related but it focuses on sentence extraction rather than on joint segmentation. We are aware of only one previous work on joint segmentation and alignment of multiple texts (Sun et al., 2007) but their approach is based on similarity functions rather than on modeling lexical cohesion in the generative framework. Our application, the analysis of the ESL podcast, was previously studied in (Noh et al., 2010). They proposed a supervised method which is driven by pairwise classification decisions. The main drawback of their approach is that it neglects the discourse structure and the lexical cohesion phenomenon. 3 Model In this section we describe our model for discourse segmentation of documents with inherently parallel structure. We start by clarifying our assumptions about their str</context>
</contexts>
<marker>Sun, Mitra, Giles, Yen, Zha, 2007</marker>
<rawString>Bingjun Sun, Prasenjit Mitra, C. Lee Giles, John Yen, and Hongyuan Zha. 2007. Topic segmentation with shared topic detection and alignment of multiple documents. In Proceedings of ACM SIGIR, pages 199–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="2798" citStr="Utiyama and Isahara, 2001" startWordPosition="417" endWordPosition="420">hese strategies may result in poor segmentation and alignment quality. To address this problem, we construct a nonparametric Bayesian model for joint segmentation and alignment of parallel parts. In comparison with the discussed pipeline approaches, our method has two important advantages: (1) it leverages the lexical cohesion phenomenon (Halliday and Hasan, 1976) in modeling the parallel parts of documents, and (2) ensures that the effective number of segments can grow adaptively. Lexical cohesion is an idea that topicallycoherent segments display compact lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). We hypothesize that not only isolated fragments but also each group of linked fragments displays a compact and consistent lexical distribution, and our generative model leverages this inter-part cohesion assumption. In this paper, we consider the dataset of “English as a second language” (ESL) podcast1, where each episode consists of two parallel parts: a story (an example monologue or dialogue) and an explanatory lecture discussing the meaning and usage of English expressions appearing in the story. Fig. 1 presents an example episode, consisting of two parall</context>
<context position="5451" citStr="Utiyama and Isahara, 2001" startWordPosition="862" endWordPosition="865">eping is so you can manage your cash flow. Cash flow, flow, means having enough money to run your business - to pay your bills. ... ... Figure 1: An example episode of ESL podcast. Co-occurred words are represented in italic and underline. to divide the lecture transcript into discourse units and to align each unit to the related segment of the story. Predicting these structures for the ESL podcast could be the first step in development of an e-learning system and a podcast search engine for ESL learners. 2 Related Work Discourse segmentation has been an active area of research (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. The task of aligning each sentence of an abstract to one or more sentences of the body has been studied in the context of summarization (Marcu, 1999; Jing, 2002; Daum´e and Marcu, 2004). Our work is different in that we do not try to extract the most relevant sentence but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, the query-focus</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In Proceedings of ACL, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Yu</author>
<author>Shipeng Yu</author>
<author>Vokler Tresp</author>
</authors>
<title>Dirichlet enhanced latent semantic analysis.</title>
<date>2005</date>
<booktitle>In Proceedings ofAISTATS.</booktitle>
<contexts>
<context position="8626" citStr="Yu et al. (2005)" startWordPosition="1380" endWordPosition="1383">llection-level topics (as e.g. in (Blei et al., 2003)), but to perform joint discourse segmentation and alignment. Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. To tackle this problem, we use Dirichlet processes (DP) (Ferguson, 1973) to de152 fine priors on the number of segments. We incorporate them in our model in a similar way as it is done for the Latent Dirichlet Allocation (LDA) by Yu et al. (2005). Unlike the standard LDA, the topic proportions are chosen not from a Dirichlet prior but from the marginal distribution GEM(α) defined by the stick breaking construction (Sethuraman, 1994), where α is the concentration parameter of the underlying DP distribution. GEM(α) defines a distribution of partitions of the unit interval into a countable number of parts. The formal definition of our model is as follows: • Draw the document-level topic proportions β(doc) — GEM(a(doc)). • Choose the document-level language model �(doc) i — Dir(ry(doc)) for i E {1, 2, ...}. • Draw the part-specific topic </context>
</contexts>
<marker>Yu, Yu, Tresp, 2005</marker>
<rawString>Kai Yu, Shipeng Yu, and Vokler Tresp. 2005. Dirichlet enhanced latent semantic analysis. In Proceedings ofAISTATS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>