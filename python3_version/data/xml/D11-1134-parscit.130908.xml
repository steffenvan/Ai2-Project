<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.982152">
Discovering Relations between Noun Categories
</title>
<author confidence="0.982363">
Thahir P Mohamed * Estevam R Hruschka Jr. Tom M Mitchell
</author>
<affiliation confidence="0.991044">
University Of Pittsburgh Federal University of Sao Carlos Carnegie Mellon University
</affiliation>
<email confidence="0.991753">
pmthahir@gmail.com estevam@cs.cmu.edu tom.mitchell@cs.cmu.edu
</email>
<sectionHeader confidence="0.997313" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976384615385">
Traditional approaches to Relation Extraction
from text require manually defining the rela-
tions to be extracted. We propose here an ap-
proach to automatically discovering relevant
relations, given a large text corpus plus an ini-
tial ontology defining hundreds of noun cate-
gories (e.g., Athlete, Musician, Instrument).
Our approach discovers frequently stated rela-
tions between pairs of these categories, using a
two step process. For each pair of categories
(e.g., Musician and Instrument) it first co-
clusters the text contexts that connect known
instances of the two categories, generating a
candidate relation for each resulting cluster. It
then applies a trained classifier to determine
which of these candidate relations is semanti-
cally valid. Our experiments apply this to a text
corpus containing approximately 200 million
web pages and an ontology containing 122 cat-
egories from the NELL system [Carlson et al.,
2010b], producing a set of 781 proposed can-
didate relations, approximately half of which
are semantically valid. We conclude this is a
useful approach to semi-automatic extension of
the ontology for large-scale information extrac-
tion systems such as NELL.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989181466666667">
The Never-Ending Language Learner (NELL)
(Carlson et al., 2010b)) is a computer system that
learns continuously to extract facts from the web.
NELL is given as input an initial ontology that
specifies the semantic categories (e.g. city, compa-
ny, sportsTeam) and semantic relations (e.g. hasOf-
ficesIn(company,city), teamPlay-
sInCity(sportsTeam,city)) it must extract from the
web. In addition, it is provided 10-20 seed positive
training examples for each of these categories and
relations, along with hundreds of millions of unla-
beled web page. Given this input, NELL applies a
large-scale multitask, semisupervised learning
method to learn to extract new instances of these
categories (e.g., city(“London”)) and relations
(e.g., teamPlaysInCity(“Steelers”,”Pittsburgh”))
from the web. During the past 17 months NELL
has been running nearly continuously, learning to
extract over 600 categories and relations, and pop-
ulating a knowledge base containing over 700,000
instances of these categories and relations with a
precision of approximately 0.851
.
This paper considers the problem of automati-
cally discovering new relations to extend the on-
tology of systems such as NELL, enabling them to
increase over time their learning and extraction
capabilities. More precisely, we consider the fol-
lowing problem:
Input:
</bodyText>
<listItem confidence="0.97414325">
· An ontology specifying a set of categories
· A knowledge base containing instances of these
categories (perhaps including errors)
· A large text corpus
</listItem>
<sectionHeader confidence="0.507289" genericHeader="introduction">
Output:
</sectionHeader>
<bodyText confidence="0.9925846875">
· A set of two-argument relations that are fre-
quently mentioned in the text corpus, and
whose argument types correspond to categories
in the input ontology (e.g., RiverFlows-
ThroughCity(&lt;River&gt;,&lt;City&gt;).
· For each proposed relation, a set of instances
(i.e. RiverFlowsThroughCity(“Nile”,”Cairo”)).
· For each proposed relation, a set of text extrac-
tion patterns that can be used to extract addi-
tional instances of the relation (e.g., the text “X
in the heart of Y”, where X is a known river,
and Y a known City, suggests extracting
RiverFlowsThroughCity(X,Y)).
Note the above inputs are easily available from
NELL in the form of its existing ontology and ex-
tracted knowledge base. Note also that the outputs
</bodyText>
<footnote confidence="0.939958666666667">
* Thahir P. Mohamed is currently at Amazon Inc.
1 NELL‟s extracted knowledge can be viewed and
downloaded at http://rtw.ml.cmu.edu.
</footnote>
<page confidence="0.855998">
1447
</page>
<note confidence="0.9582745">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1447–1455,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9894023">
of our system are sufficient to initiate NELL‟s
learning of additional extraction methods to further
populate each proposed relation. One goal of this
research is to create a system that can provide
NELL with an ongoing set of new learning and
extraction tasks. The system is called OntExt (On-
tology Extension System)
Table 1 shows a sample of successful relations
and corresponding relation contexts and sample
seed instances generated by OntExt.
</bodyText>
<tableCaption confidence="0.989660333333333">
Table 1. Examples of valid relations (generated
by OntExt), their text extraction patterns and
extracted instances.
</tableCaption>
<bodyText confidence="0.988524178571428">
name(category1- Extraction pat- Seed
main context- terns Instances
category2)
River „in heart of‟ “Seine, Paris”
-in heart of- „in the center “Nile, Cairo”
City of‟ “Tiber river, Rome”“River arno, Florence”
„which flows
through‟
Food „to produce‟ “Salt, Chlorine”
-to produce- „to make‟ Sugar, Carbon diox-
Chemical „to form‟ ide”“Protein, Serotonin”
StadiumOrVenue„in downtown‟ “Ford field, Detroit”
-in downtown- “Superdome, New Or-
City leans”
“Turner field, Atlanta”
Disease „caused by‟ “pneumonia, legionel-
-caused by- „is the causa- la”
Bacteria tive agent of‟ “mastitis, staphylococ-
„is the cause cus aureus”
of‟ “gonorrhea, neisseriagonorrhoeae”
Disease „destroys‟ &amp;quot;alzheimer, brain cells&amp;quot;
-destroys- „attacks‟ “vitiligo&amp;quot;, melano-
CellType cytes&amp;quot;
&amp;quot;aids, lymphocytes&amp;quot;
County „county‟ &amp;quot;sufolk, massachusetts&amp;quot;
-county- „county of‟ &amp;quot;marin, california&amp;quot;
StateOrProvince „county in‟ &amp;quot;sussex, delaware&amp;quot;
&amp;quot;osceola, michigan&amp;quot;
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="method">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.982453">
Traditional Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999962615384616">
We define Traditional RE systems as those that
require the user to specify information about the
relations to be learned. For instance, SnowBall
(Agichtein and Gravano 2000) &amp; CPL (Carlson et
al. 2009) are bootstrapped learning systems that
require manual input of relation predicates. In the-
se systems, for each relation predicate, the relation
name (e.g. City „Capital of‟ Country), the seed in-
stances and the category type (e.g. City, Country,
Celebrity etc) are provided (for domain and range).
In CPL (Carlson et al. 2009), learning of rela-
tion/category instances is coupled by using con-
straints such as mutual exclusion relationships
among the predicates. The authors show that this
coupling reduces semantic drift, which commonly
occurs with bootstrapping systems, thus leading to
improved precision. CPL achieved 89% precision
for the relation instances extracted (Carlson, Bet-
teridge et al. 2009). KNOWITALL (Etzioni, Ca-
farella et al. 2005) is a web-scale relation extrac-
tion system, which requires as input the relation
names. Hence, in these “traditional relation extrac-
tion” methods, the need to manually define the re-
lations to be extracted makes it difficult to work in
applications having thousands of possible relation
predicates.
</bodyText>
<subsectionHeader confidence="0.984097">
2.1 Open Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999799259259259">
Open RE methods do not require a user to manual-
ly specify the information about the relations to be
learned, such as their names, seed examples, etc.
TextRunner (Banko, Cararella et al. 2007) is such
an Open Information Extraction system that re-
trieves from the web millions of relational tuples
between noun phrase entities. TextRunner uses a
deep linguistic parser to perform self-supervised
learning and extracts a positive set (i.e. valid rela-
tion between entities) and a negative set (i.e. inva-
lid relationships) of relational tuples based on cer-
tain heuristics. Then, a Naive Bayes classifier is
built having features such as part-of-speech tags of
the words in the relation tuples, number of tokens,
stopwords etc., and uses the labeled instances as
the training set. This classifier runs on sentences
from a web corpus to extract millions of relational
tuples. However, of the 11 million high confident
relational tuples extracted by this system only 1
million were concrete facts (Banko, Cararella et al.
2007). Of these concrete facts 88% were estimated
to be correct. For instance, (Mountain View, head-
quarters of, Google) is a tuple representing a valid
concrete fact. The remaining 90% of the tuples are
abstract or do not have well-formed arguments or
well-formed relations. For instance, (Einstein, de-
rived, theory) is an abstract tuple as it does not
</bodyText>
<page confidence="0.987141">
1448
</page>
<bodyText confidence="0.999942583333333">
have enough information to indicate a concrete fact
(Banko, Cararella et al. 2007) because the specific
theory which Einstein derived is missing in that
tuple. In the tuple (45, `went to&apos;, `Boston&apos;), one of
the arguments (i.e. 45) is not well formed.
In (Banko and Etzioni, 2008) a Conditional
Random Field (CRF) classifier is used to perform
Open Relation Extraction which improves by more
than 60% the F-score achieved by the Naive Bayes
model in the TextRunner system. However the
CRF approach does not solve the problem associ-
ated with extraction of abstract/non-well formed
tuples. Further, in the same work, it is shown that
Open RE has a much lower recall in comparison to
Traditional RE systems. On four common relations
(Acquisition, Birthplace, InvetorOf, WonAward),
Open RE attained a recall of 18.4% in comparison
to 58.4% achieved by Traditional RE (Banko and
Etzioni 2008). Both Open RE systems discussed
(Banko, Cararella et al. 2007; Banko and Etzioni
2008) do not perform learning of the category type
of the entities involved in the relations. They are
single-pass and do not perform continuous learning
to improve/extend on what has been learnt.
</bodyText>
<subsectionHeader confidence="0.766592">
2.2 Unsupervised Methods to Extract Rela-
tions between Named Entities
</subsectionHeader>
<bodyText confidence="0.999985462962963">
In general, traditional RE methods extract concrete
facts and have much higher recall for a given rela-
tion, than Open RE methods. This is due to the
knowledge fed into Traditional RE methods such
as the category type of the entities in the relation
and seed instances for the relation. Traditional RE
methods require the relations to be manually de-
fined and extract instances only for them. Open RE
methods, on the other hand, do not require any
such domain specific knowledge to be manually
input. They extract instances for a wide spectrum
of relations that are not manually pre-defined.
To overcome the drawbacks of using Traditional
and Open RE methods, some researchers have used
unsupervised learning methods to automatically
generate new relations (with seeds and contexts)
between specific categories. These automatically
generated relations can then be used as input to
Traditional RE systems.
Hasegawa et.al (Hasegawa, Sekine et al. 2004),
propose an unsupervised clustering based ap-
proach. One feature vector for each co-occurring
NE pair is formed based on the context words in
which the NE pair co-occurs. Then, a cosine-
similarity metric is applied to each pair of feature
vectors to generate a “NE-pair x NE-pair” matrix.
Clustering is done on this matrix and each cluster
of NE-pairs corresponds to a relation predicate.
The work by Zhang et.al (Zhang, Su et al. 2005)
generates a shallow parse tree for each sentence
containing a NE pair to generate relation instances.
A tree similarity metric is used to cluster the rela-
tion instances. This method gives improved F-
score over Hasegawa et.al (Hasegawa, Sekine et al.
2004). Further they use a specialized NE tagger
built to recognize entities that belong to specific
predefined categories. The aforementioned meth-
ods (Hasegawa, Sekine et al. 2004) (Zhang, Su et
al. 2005) were tested on a news corpus to identify
relations between only a couple of pairs of entity
types (Person-GeoPoliticalEntity and Company-
Company).
Both of these methods cluster NE-pairs primari-
ly based on lexical similarity of the context words
connecting the entities. Hence NE-pairs connected
by lexically different but semantically similar con-
text patterns (e.g. river `in heart of&apos; city and river
`flows through&apos; city) would probably not get clus-
tered together. The web data is, however, much
noisier and has a larger number of entity types (i.e.
category predicates), thus, another issue is that for
web scale data NE pairs X NE pairs similarity ma-
trix would not be scalable for many thousands of
NE-pairs.
</bodyText>
<sectionHeader confidence="0.978834" genericHeader="method">
3 Ontology Extension System - OntExt
</sectionHeader>
<bodyText confidence="0.999639666666667">
The OntExt system for ontology extension, pro-
posed in this paper, combines characteristics from
both “Traditional RE” and “Open RE,” to discover
new relations among categories that are already
present in the ontology, and for which many in-
stances have already been extracted.
Our proposed method for automatic relation ex-
traction offers the following advantages over the
methods discussed above.
</bodyText>
<listItem confidence="0.9517435">
• The key idea in our approach is to make use
of redundancy of information in web data - the
same relational fact is often stated multiple
times in large text corpora, using different con-
text patterns. We use this redundancy to clus-
ter together context patterns which are seman-
tically similar although they may be lexically
dissimilar.
</listItem>
<page confidence="0.80189">
1449
</page>
<listItem confidence="0.996024727272727">
• Instead of clustering on the ‘NE-pairs X
NE-pairs’ matrix, clustering is done on a ‘Con-
text-pattern X Context-pattern’ matrix. This is
much more scalable as the context patterns are
fewer in number and since our method applies
several criteria to prune out irrelevant patterns.
• To accommodate errors in the input catego-
ry instances and ambiguity in web data, we
build a classifier which learns to distinguish
valid relations from semantically invalid rela-
tions.
</listItem>
<bodyText confidence="0.9992834">
OntExt has 3 components. 1) It starts exploring
a large web corpus and 2) category instances ex-
tracted by CPL to generate new relations. After the
relations are generated, 3) a classifier is developed
to classify semantically valid relations.
</bodyText>
<subsectionHeader confidence="0.999746">
3.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.989396">
Following along the same strategy used in [Carlson
et al., 2010], OntExt uses as input a corpus of 2
billion sentences, which was generated by using
the OpenNLP2 package to extract, tokenize, and
POS-tag sentences from the 500 million web page
English portion of the ClueWeb09 data [Callan and
Hoy, 2009]. Before performing relation extraction,
this corpus is preprocessed. First, sentences which
contain a pair of known category instances are re-
trieved (e.g. the sentence “Ottawa is the capital of
Canada.”, where `Ottawa&apos; is a known instance of
the `City&apos; category and `Canada&apos; is a known in-
stance of `Country&apos;). For every category pair (e.g.
&lt;City, Country&gt;) the sentences containing known
instances of both categories are grouped into a set
S. The text between the two instances is called the
`context pattern&apos; (e.g. `is the capital of&apos; is a context
pattern). Three types of pruning are done on this
set S.
</bodyText>
<listItem confidence="0.998833818181818">
1. If the context pattern is a rare one (i.e. if the
context pattern occurs in less than a threshold
number of sentences), all sentences with that
context pattern are removed. Thus we retain
only frequently occurring contexts. We use a
threshold requiring at least 5 sentences in the
experiments presented in Section 4.
2. Context patterns which co-occur with very
few instances of either category type are re-
moved. For example, the category pair &lt;Vehi-
cle,SportsTeam&gt; has several sentences such as
</listItem>
<footnote confidence="0.506306">
2 http://opennlp.sourceforge.net.
</footnote>
<bodyText confidence="0.996884642857143">
`Car was engulfed in flames&apos;, `Truck was en-
gulfed in flames&apos; etc. Note that Flames (Calga-
ry Flames) is a SportsTeam. But here flames
clearly does not refer to a Sportsteam. This
context `was engulfed in&apos; connects several in-
stance of a `Vehicle&apos; category to a single in-
stance of SportsTeam instance. Hence all sen-
tences with this context are removed. Note this
context would not have been removed in step 1
as that is just a threshold on the number of sen-
tences in which any pair occurs. We use a
threshold requiring at least 3 distinct instances
of both the domain and the range, for each
context.
</bodyText>
<listItem confidence="0.827204444444444">
3. Banko et.al, 2008 show that most binary re-
lational contexts fall under certain types of lex-
ico-synctatic patterns. They include context
patterns like `C1 Verb C2&apos;, `C1 NP Prep C2&apos;,
`C1 Verb Prep C2&apos; and `C1 to Verb C2&apos; (C1
and C2 are category instances). Hence context
patterns which do not fall under the above
types are removed from the set S as they are
not likely to produce relation instances.
</listItem>
<subsectionHeader confidence="0.998137">
3.2 Relation Generation
</subsectionHeader>
<bodyText confidence="0.9998565">
From the previous pre-processing step OntExt re-
trieves for each category pair a pruned set S&apos; of
sentences. Each sentence has a pair of category
instances and the context connecting them.
</bodyText>
<sectionHeader confidence="0.550011" genericHeader="method">
Algorithm 1: Relation Generator
</sectionHeader>
<bodyText confidence="0.931398666666667">
Input: One pair of Categories (C1, C2) and set of
sentences, each containing a pair of instances
known to belong to C1 and C2. The phrase con-
necting the instances in the sentence is the context.
Output: Relations and their seed instances
Steps:
</bodyText>
<listItem confidence="0.993662454545455">
1. From the input sentences, build a Context by
Context co-occurrence matrix (Shown in figure
1). The matrix is then normalized.
2. Apply K-means clustering on the matrix to
cluster the related contexts together. Each clus-
ter corresponds to a possible new relation be-
tween the two input categories. (Weka Ma-
chine Learning package [Hall et al., 2009] was
used to perform K-means clustering. The value
of K was set to 5 based on trial and error ex-
periments.)
</listItem>
<page confidence="0.727176">
1450
</page>
<listItem confidence="0.710093666666667">
3. Rank the known instance pairs (belonging to
C1,C2) for each cluster and take the top 50 as
seed instances for the relation
</listItem>
<bodyText confidence="0.999729692307692">
The key data structure used by OntExt is a co-
occurrence matrix of the contexts for each category
pair, as shown in Figure 1. In this matrix, each cell
corresponds to the number of pairs of category in-
stances that both contexts co-occur with (e.g. the
sentences “Vioxx can cure Arthritis” and “Vioxx is
a treatment for Arthritis” provide a case where the
2 contexts `can cure&apos; and `is a treatment for&apos; co-
occur with an instance pair [Vioxx, Arthritis]). Ini-
tially, the value of Matrix(I,j) is the number of cat-
egory instance pairs that occur with both context i
and context j. We then normalize each cell in the
matrix, dividing it by by the total count for its row.
</bodyText>
<equation confidence="0.949133625">
Matrix i j
( , )
Matrix i j
( , )
 Matrix(i, j)
N

j0
</equation>
<bodyText confidence="0.995359588235294">
We also give higher weight to contexts which co-
occur with only a few contexts over ones which are
generic and co-occur with most contexts.
Where N is the total number of contexts, and
|{Context(j) : Matrix(i,j) &gt; 0} |refers to the number
of cells in the row Matrix(i) which are greater than
zero.
For example, for the &lt;drug, disease&gt; category
pair after 122 contexts were obtained after prepro-
cessing. Contexts such as `to treat&apos;, `for treatment
of&apos;, `medication&apos; which all indicate the same rela-
tion (drug-to treat-disease) have high co-
occurrence values (see Figure 1). Similarly con-
texts such as `can cause&apos;, `may cause&apos;, `can lead to&apos;
(indicating the relation drug-can cause-disease)
have high co-occurrence values (see Figure 1).
When OntExt performs clustering on this co-
occurrence matrix the contexts with large co-
occurrences get clustered together. Each cluster is
then used to propose a possible new relation. The
centroid of each cluster is used to build the relation
name. If the centroid of a cluster is the context `for
treatment of&apos;, then the relation name is `drug-for-
treatment-of-disease&apos;.
OntExt next generates seed instances for the
proposed relation. The seed instances which co-
occur with contexts corresponding to the cluster
centroid or close to centroid will be best repre-
sentative of the relation. So the strength of the seed
instance is inversely proportional to the standard
deviation of the context from the centroid of the
relation contexts cluster. Also the strength of the
seed instance is directly proportional to the number
of times it co-occurs with the context.
</bodyText>
<figureCaption confidence="0.723866428571428">
Figure 1: This figure shows the Context by Context
sub-matrix (with 6 contexts) for the category pair
(Drug, Disease) and the seed instances for each
relation. As described in the text, each entry gives
the normalized count of the number of known
&lt;drug, disease&gt; pairs that occur with both the row
context and the column context.
</figureCaption>
<bodyText confidence="0.987551333333333">
To summarize, each seed instance s (pair of cat-
egory instances) is weighted as follows
Where,
Pattern_cluster is the cluster of pattern contexts for
this given relation
Occ(c,s) is the number of times instance `s&apos; co-
occurs with the pattern context `c&apos;
sd(c) is the standard deviation of the context from
the centroid of the pattern cluster.
Using this metric the instances are ranked and the
top 50 are output as initial seed instances for the
proposed relation.
</bodyText>
<page confidence="0.980647">
1451
</page>
<subsectionHeader confidence="0.998127">
3.3 Classifying semantically valid relations
</subsectionHeader>
<bodyText confidence="0.999777333333333">
More than half of the relations generated in the
previous step are invalid due to the following rea-
sons
</bodyText>
<listItem confidence="0.934005296296296">
1. Error in category instances: The category
instances input to OntExt come from NELL. In
the version of the knowledge base used in the-
se experiments, the accuracy of these instances
was 78%. Due to the erroneous category in-
stances some invalid relations are generated by
OntExt. For instance the generated relation,
`condiment-wearing-clothing&apos; with seeds
(pig,dress), (rabbit,pants) etc. Here `pig&apos; and
`rabbit&apos; were incorrectly identified by NELL as
instances of `condiment&apos;.
2. Semantic Ambiguity: Consider the generated
relation `bakedgood-baking-magazine&apos; with in-
stances (cookies,time), (cupcakes, people), etc.
Here the instances `time&apos; and `people&apos; do not
refer to magazines, although they can in gen-
eral. Due to the semantic ambiguity of these
instances this invalid relation got generated
3. Semantically Incomplete relations: Some of
the generated relations require a third entity or
some more contextual information, in order to
be considered semantically valid. For instance,
`personUs-said-company&apos; or `newspaper-is-
reporting-that-company&apos;. These don&apos;t stand by
themselves as two-argument relational facts
and need more information to be complete
4. Illogical relations: Some generated relations
</listItem>
<bodyText confidence="0.881368962962963">
simply have no real semantic meaning. These
relations are generated due to the category in-
stances appearing together in some unrelated
contexts. E.g. the generated relation `date-
starting-date&apos; with seeds such as (Wednesday,
June), (friday, July) and the relation `country-
minister-of- economicsector&apos; with seeds (ja-
pan,agriculture), (india, industry).
The introduction of these invalid relations can
adversely affect the performance of NELL. How-
ever, it is a challenging problem to develop auto-
mated ways to distinguish between valid and inva-
lid relations without any domain specific
knowledge. To approach this problem, we identi-
fied a set of features which can help characterizing
valid and invalid relations, and which can be gen-
erated automatically. Below is a description of the
features and the intuition behind their use for this
classification task.
Each generated relation has a pair of category
types (C1, C2), a corresponding set of seed in-
stances (which are pairs of instances belonging to
C1 and C2) and pattern contexts connecting C1
and C2. Let N be the number of seed instance
pairs and N1 and N2 be number of unique instanc-
es (out of these N instance pairs) belonging to cat-
egories C1 and C2 respectively.
</bodyText>
<listItem confidence="0.911229">
1. Normalized frequency count: The frequency
</listItem>
<bodyText confidence="0.999237">
count of each category instance is obtained
from the corpus and normalized by the catego-
ry instance with maximum count. For a given
relation, a feature is generated by averaging
the normalized frequency counts of the in-
stances belonging to C1. Another similar fea-
ture is generated for C2 following the same
strategy. For example the relation &lt;Profession
`believe that&apos; Movie&gt; was generated due to
common words like `predator&apos;, &apos;earthquake&apos;
being identified as movie names out of con-
text. These features can help identify such in-
valid relations.
</bodyText>
<listItem confidence="0.800615666666667">
2. Distribution of extraction patterns: NELL
learns instances as well as extraction patterns
for each category (e.g. the category Actor has
extraction patterns such as `_ got an Oscar
award&apos;, `_ is the movie&apos;s lead actor&apos;). If a cat-
egory instance co-occurs in the web corpus
with several extraction patterns belonging to
other categories, then that instance has large
ambiguity. We measure ambiguity of an in-
stance (i) belonging to category `C&apos; with re-
spect to another category `M&apos; (where M is not
a sub type or super type of `C&apos;) as
</listItem>
<equation confidence="0.759356166666667">
Ambiguity(i,M) =
patterns in
- occurs
patterns in
- occurs
i C
</equation>
<bodyText confidence="0.999863">
We measure the average ambiguity for the set
of instances (of size N) belonging to category
C in the generated seeds as follows,
</bodyText>
<equation confidence="0.335292">
MaxM ( Ambiguity (i, M) /N)
</equation>
<bodyText confidence="0.9225675">
Two features are generated for categories C1
and C2 in the relation.
</bodyText>
<listItem confidence="0.6151235">
3. Relationship characteristics: We identified a
few characteristics of the relation which help
in identifying valid relations. If in the generat-
ed relation, most instances of C1 co-occur only
</listItem>
<figure confidence="0.928489375">
&apos;i&apos; co
# of extraction
&apos; M &apos;that
with
&apos;i&apos; co
# of extraction
&apos; C&apos; that
with
</figure>
<page confidence="0.994069">
1452
</page>
<bodyText confidence="0.997708790697674">
with very few instances of C2 (or vice versa)
then the relation could be weak. For example,
&lt;Organization `Provides&apos; EconomicSector&gt; -
the instance `Information&apos; (of category Eco-
nomicSector) connects to a large percentage of
items in the category `Organization&apos; but does
not express a meaningful relation. So we con-
sider the instance (in this example `Infor-
mation&apos;, let us call it `maxconnect_instance&apos;)
co-occurring with maximum number of in-
stances of the other category. The percentage
of instances it co-occurs with from among the
total number of instances of the other category
which are part of the seed instances is taken as
a feature. Also if that instance is a very com-
mon word (like `information&apos; which in several
contexts does not refer to `EconomicSector&apos;)
then this could indicate the presence of an in-
valid relation. So the normalized frequency
count of this instance (maxconnect_instance) is
taken as another feature.
4. Pattern Contexts: The number of pattern con-
texts attained through pattern clustering for the
relation is taken as another feature. The pres-
ence of several pattern contexts connecting the
instances between the two categories could in-
dicate that the relation is a valid one. The
presence of Hearst patterns (Hearst M, 1992)
referring to a hyponym (“is-a”) relation in pat-
tern contexts indicates the possibility of a valid
relation, and is taken as another feature
Another feature is regarding how specific is
the context pattern to this relation. If the same
context connects say C1 instances to instances
of several other categories apart from C2, then
this context is not unique to this relation and
might not indicate a meaningful valid relation-
ship. So the ratio of the number of instances in
C2 connected to C1 versus the number of in-
stances from all categories connected to C1 by
the most significant pattern context (i.e. cen-
troid in pattern cluster) is taken as a feature. A
similar feature is generated for C2 as well.
</bodyText>
<sectionHeader confidence="0.996393" genericHeader="method">
4 Experimental Setup and Results
</sectionHeader>
<subsectionHeader confidence="0.997821">
4.1 CPL System
</subsectionHeader>
<bodyText confidence="0.999962818181818">
CPL (Carlson et al., 2010) is a semi-supervised
learning system which takes in an input ontology
(containing category and relation predicates and
corresponding seed instances) and constraints
(such as Mutual exclusion rules between predi-
cates). The system iteratively extracts patterns and
instances for the category/relation predicates from
a web corpus of around 500 million web pages.
CPL is one learning component in NELL (the Nev-
er Ending Language Learner) (Carlson et al.,
2010b).
</bodyText>
<subsectionHeader confidence="0.9829">
4.2 Relation Generation:
</subsectionHeader>
<bodyText confidence="0.999596421052632">
We use approximately 22,000 category instances
belonging to 122 categories extracted by CPL at
the end of its 20th iteration and the web corpus as
input to perform the co-clustering described in
Section 3.2 and generate the new relations. The
process generated 781 relations. For each relation,
the relation name, types of the categories involved
in the relation and the seed instances and patterns
for each relation were generated. Table 1 in section
1 shows a sample of valid relations generated by
this method.
Tables 2, 3, 4 and 5 show invalid relations for
each type of invalidity, “Error in the Category In-
stances”, “Semantic Ambiguity”, “Semantically
Incomplete Relations” and “Illogical Relations”
respectively. More specifically, Table 2 shows a
sample of relations generated due to an entity be-
ing labeled incorrectly as to belong to a category.
The incorrect category instances are in italics.
</bodyText>
<tableCaption confidence="0.939518666666667">
Table 3 presents a sample of relations which
were generated because of semantic ambiguity.
Instances with ambiguity are in italics.
Table 4 shows some of the generated relations
which are semantically incomplete.
Table 5 presents samples of illogical relations
which do not establish any concrete fact.
Table 2. Examples of Incorrect category in-
stances.
</tableCaption>
<table confidence="0.999028111111111">
name(category1 Relation Seed
-main context- Contexts Instances
category2)
SportsGame `beating&apos; &amp;quot;tournament,Sri Lanka&amp;quot;
-Beating- &amp;quot;champions, France&amp;quot;
Country &amp;quot;match, canada&amp;quot;
Animal `will eat&apos; &amp;quot;wolf, sheep&amp;quot;
-will eat- `eating&apos; &amp;quot;fox, rabbit&amp;quot;
Condiment &amp;quot;lion, lamb&amp;quot;
</table>
<page confidence="0.552601">
1453
</page>
<tableCaption confidence="0.9672495">
Table 3. Examples of Semantically Ambiguous
relations.
</tableCaption>
<table confidence="0.999851875">
Name Relation Seed
Contexts Instances
Bird „play‟ &amp;quot;Cardinals, Atlanta&amp;quot;
-play- &amp;quot;Ravens, Miami&amp;quot;
City &amp;quot;Eagles, Chicago&amp;quot;
BakedGood „baking‟ &amp;quot;time, cakes&amp;quot;
-baking- &amp;quot;people, cookies&amp;quot;
Magazine
</table>
<tableCaption confidence="0.982161">
Table 4. Examples of semantically incomplete
relations.
</tableCaption>
<table confidence="0.9859465">
Name Relation Seed
Contexts Instances
Personus &amp;quot;m obama, tues-
acknowledged acknowledged‟ day&amp;quot;
Date „warned‟
„met‟ &amp;quot;george w . bush,
tuesday&amp;quot;
&amp;quot;al gore, thursday&amp;quot;
NewsPaper „is reporting &amp;quot;financial times,
-is reporting apple&amp;quot;
that- that‟ &amp;quot;wall street jour-
Company „writes that‟ nal, gm&amp;quot;
„reported that‟ &amp;quot;wall street jour-
nal, yahoo&amp;quot;
</table>
<tableCaption confidence="0.9961635">
Table 5. Examples of relations representing
facts that are not concrete.
</tableCaption>
<table confidence="0.9851528">
Name Relation Seed
Contexts Instances
Emotion „of living in‟ &amp;quot;joy, california&amp;quot;
-of living in- &amp;quot;excitement, colora-
StateOrPro do&amp;quot;
vince &amp;quot;fear, iowa&amp;quot;
BodyPart „to keep‟ “hand, eye”
-to keep- „guard‟ “nose, throat”
BodyPart “eye, brain”
“elbow, hand”
</table>
<subsectionHeader confidence="0.984057">
4.3 Relation Classification:
</subsectionHeader>
<bodyText confidence="0.999959619047619">
To determine the feasibility of automatically classi-
fying OntExt‟s proposed relations as valid or inva-
lid, we trained and tested a classifier using the fea-
tures described above, using manually assigned
class label for some of the generated relations (252
relations) as valid or invalid (the criteria for which
was explained before). 115 of these 252 relations
were found to be valid by manual evaluation. This
shows the need for a machine learning classifier to
identify valid/invalid relations. The various fea-
tures described earlier (such as normalized fre-
quency count, relationship characteristics, pattern
context features, distribution of extraction patterns)
were generated for each relation. Ten-fold cross
validation experiments were carried out with vari-
ous classifiers. A Random Forest classifier per-
formed the best. Precision, recall and ROC-area is
shown in the table below (ROC area is the area
under the ROC curve which plots the classifier
performance by having the True Positive Rate on
the Y-axis and False Positive Rate on the X-axis).
</bodyText>
<tableCaption confidence="0.920815">
Table 6. Classifier performance.
</tableCaption>
<table confidence="0.970713">
RelationType Precision Recall ROC Area
Valid 71.6 72.2 0.804
Invalid 76.5 75.9 0.804
Weighted 74.2 74.2 0.804
Avg.
</table>
<bodyText confidence="0.998873166666667">
These results indicate that the system is able to
learn to identify semantically valid relations with-
out using any manually input information. The val-
id relations generated can be input to NELL, al-
lowing it to iteratively learn additional instances
for each proposed relation.
</bodyText>
<sectionHeader confidence="0.998038" genericHeader="conclusions">
5 Conclusion and Future work:
</sectionHeader>
<bodyText confidence="0.9994194">
Open Relation Extraction and Traditional Relation
Extraction have their respective strengths and
weaknesses. The OntExt system proposed in this
work combines the strengths of both of those
methods. The relation predicates automatically
generated by our approach are typed, have a mean-
ingful name identifying the relation, and are ac-
companied by suggested context patterns and seed
instances. These relations can be input to NELL to
learn more instances for the relation. We propose
in the future to integrate this relation generation
system into NELL, to iteratively extend NELL‟s
initial ontology, providing an ongoing stream of
new learning tasks. After every fixed set of
NELL‟s iterations, its growing knowledge base
would be input to the relation generation system
which will in turn feed NELL with new relation
predicates. One additional area for future research
is to extend OntExt to discover new categories in
addition to new relations.
</bodyText>
<page confidence="0.993175">
1454
</page>
<sectionHeader confidence="0.9991" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.985860333333333">
We gratefully acknowledge support for this re-
search from Darpa, Google, Yahoo! and the Brazil-
ian research agency CNPq. We also gratefully
acknowledge Dr. Madhavi Ganapathiraju (at Uni-
versity of Pittsburgh) for her support and encour-
agement.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777265306123">
Agichtein, E. and L. Gravano (2000). &amp;quot;Snowball: Ex-
tracting relations from large plain-text collections.&amp;quot;
Procs. of the Fifth ACM International Conference on
Digital Libraries.
Banko, M., M. Cararella, et al. (2007). &amp;quot;Open infor-
mation extraction from the web.&amp;quot; In Procs. of IJCAI.
Banko, M. and O. Etzioni (2008). &amp;quot;The Tradeoffs Be-
tween Open and Traditional Relation Extraction.&amp;quot; In
Proceedings of ACL-08.
Callan, J., and Hoy, M. (2009). Clueweb09 data set.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Carlson, A., J. Betteridge, et al. (2009). &amp;quot;Coupling
Semi-Supervised Learning of Categories and Rela-
tions.&amp;quot; Proceedings of the NAACL HLT 2009 Work-
shop on Semi-supervised Learning for Natural Lan-
guage Processing.
A. Carlson, J. Betteridge, et al. (2010). “Coupled Semi-
Supervised Learning for Information Extraction,”
Proceedings of the ACM International Conference on
Web Search and Data Mining (WSDM), 2010.
A. Carlson, J. Betteridge, et al., (2010b). “Toward an
Architecture for Never-Ending Language Learning,”
Proceedings of the Conference on Artificial Intelli-
gence (AAAI), 2010.
Etzioni, O., M. Cafarella, et al. (2005). &amp;quot;Unsupervised
named-entity extraction from the web: An experi-
mental study.&amp;quot; Artificial Intelligence.
Hasegawa, T., S. Sekine, et al. (2004). &amp;quot;Discovering
relations among named entities from large corpora.&amp;quot;
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics
Zhang, M., J. Su, et al. (2005). &amp;quot;Discovering Relations
between Named Entities from a Large Raw Corpus
Using Tree Similarity-based Clustering.&amp;quot; IJCNLP 05.
Hasegawa, T., S. Sekine, et al. (2004). &amp;quot;Discovering
relations among named entities from large corpora.&amp;quot;
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics
Zhang, M., J. Su, et al. (2005). &amp;quot;Discovering Relations
between Named Entities from a Large Raw Corpus
Using Tree Similarity-based Clustering.&amp;quot; IJCNL
Hearst, M. (1992) Automatic Acquisition of Hyponyms
from Large Text Corpora. Proc. of the Fourteenth In-
ternational Conference on Computational Linguistics,
Nantes, F
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, Ian H. Witten (2009);
The WEKA Data Mining Software: An Update;
SIGKDD Explorations, Volume 11, Issue 1.
</reference>
<page confidence="0.992976">
1455
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.952802">
<title confidence="0.999885">Discovering Relations between Noun Categories</title>
<author confidence="0.994847">Tom M Mitchell</author>
<affiliation confidence="0.999946">University Of Pittsburgh Federal University of Sao Carlos Carnegie Mellon University</affiliation>
<email confidence="0.999385">estevam@cs.cmu.edutom.mitchell@cs.cmu.edu</email>
<abstract confidence="0.998439851851852">Traditional approaches to Relation Extraction from text require manually defining the relations to be extracted. We propose here an approach to automatically discovering relevant relations, given a large text corpus plus an initial ontology defining hundreds of noun categories (e.g., Athlete, Musician, Instrument). Our approach discovers frequently stated relations between pairs of these categories, using a two step process. For each pair of categories (e.g., Musician and Instrument) it first coclusters the text contexts that connect known instances of the two categories, generating a candidate relation for each resulting cluster. It then applies a trained classifier to determine which of these candidate relations is semantically valid. Our experiments apply this to a text corpus containing approximately 200 million web pages and an ontology containing 122 categories from the NELL system [Carlson et al., 2010b], producing a set of 781 proposed candidate relations, approximately half of which are semantically valid. We conclude this is a useful approach to semi-automatic extension of the ontology for large-scale information extraction systems such as NELL.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.&amp;quot;</title>
<date>2000</date>
<booktitle>Procs. of the Fifth ACM International Conference on Digital Libraries.</booktitle>
<contexts>
<context position="5661" citStr="Agichtein and Gravano 2000" startWordPosition="824" endWordPosition="827">l-caused by- „is the causa- la” Bacteria tive agent of‟ “mastitis, staphylococ„is the cause cus aureus” of‟ “gonorrhea, neisseriagonorrhoeae” Disease „destroys‟ &amp;quot;alzheimer, brain cells&amp;quot; -destroys- „attacks‟ “vitiligo&amp;quot;, melanoCellType cytes&amp;quot; &amp;quot;aids, lymphocytes&amp;quot; County „county‟ &amp;quot;sufolk, massachusetts&amp;quot; -county- „county of‟ &amp;quot;marin, california&amp;quot; StateOrProvince „county in‟ &amp;quot;sussex, delaware&amp;quot; &amp;quot;osceola, michigan&amp;quot; 2 Background Traditional Relation Extraction We define Traditional RE systems as those that require the user to specify information about the relations to be learned. For instance, SnowBall (Agichtein and Gravano 2000) &amp; CPL (Carlson et al. 2009) are bootstrapped learning systems that require manual input of relation predicates. In these systems, for each relation predicate, the relation name (e.g. City „Capital of‟ Country), the seed instances and the category type (e.g. City, Country, Celebrity etc) are provided (for domain and range). In CPL (Carlson et al. 2009), learning of relation/category instances is coupled by using constraints such as mutual exclusion relationships among the predicates. The authors show that this coupling reduces semantic drift, which commonly occurs with bootstrapping systems, t</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Agichtein, E. and L. Gravano (2000). &amp;quot;Snowball: Extracting relations from large plain-text collections.&amp;quot; Procs. of the Fifth ACM International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cararella</author>
</authors>
<title>Open information extraction from the web.&amp;quot; In Procs. of IJCAI.</title>
<date>2007</date>
<marker>Banko, Cararella, 2007</marker>
<rawString>Banko, M., M. Cararella, et al. (2007). &amp;quot;Open information extraction from the web.&amp;quot; In Procs. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>O Etzioni</author>
</authors>
<title>The Tradeoffs Between Open and Traditional Relation Extraction.&amp;quot;</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08.</booktitle>
<contexts>
<context position="8415" citStr="Banko and Etzioni, 2008" startWordPosition="1262" endWordPosition="1265">se concrete facts 88% were estimated to be correct. For instance, (Mountain View, headquarters of, Google) is a tuple representing a valid concrete fact. The remaining 90% of the tuples are abstract or do not have well-formed arguments or well-formed relations. For instance, (Einstein, derived, theory) is an abstract tuple as it does not 1448 have enough information to indicate a concrete fact (Banko, Cararella et al. 2007) because the specific theory which Einstein derived is missing in that tuple. In the tuple (45, `went to&apos;, `Boston&apos;), one of the arguments (i.e. 45) is not well formed. In (Banko and Etzioni, 2008) a Conditional Random Field (CRF) classifier is used to perform Open Relation Extraction which improves by more than 60% the F-score achieved by the Naive Bayes model in the TextRunner system. However the CRF approach does not solve the problem associated with extraction of abstract/non-well formed tuples. Further, in the same work, it is shown that Open RE has a much lower recall in comparison to Traditional RE systems. On four common relations (Acquisition, Birthplace, InvetorOf, WonAward), Open RE attained a recall of 18.4% in comparison to 58.4% achieved by Traditional RE (Banko and Etzion</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Banko, M. and O. Etzioni (2008). &amp;quot;The Tradeoffs Between Open and Traditional Relation Extraction.&amp;quot; In Proceedings of ACL-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Callan</author>
<author>M Hoy</author>
</authors>
<date>2009</date>
<note>Clueweb09 data set. http://boston.lti.cs.cmu.edu/Data/clueweb09/.</note>
<contexts>
<context position="13724" citStr="Callan and Hoy, 2009" startWordPosition="2119" endWordPosition="2122">h valid relations from semantically invalid relations. OntExt has 3 components. 1) It starts exploring a large web corpus and 2) category instances extracted by CPL to generate new relations. After the relations are generated, 3) a classifier is developed to classify semantically valid relations. 3.1 Pre-processing Following along the same strategy used in [Carlson et al., 2010], OntExt uses as input a corpus of 2 billion sentences, which was generated by using the OpenNLP2 package to extract, tokenize, and POS-tag sentences from the 500 million web page English portion of the ClueWeb09 data [Callan and Hoy, 2009]. Before performing relation extraction, this corpus is preprocessed. First, sentences which contain a pair of known category instances are retrieved (e.g. the sentence “Ottawa is the capital of Canada.”, where `Ottawa&apos; is a known instance of the `City&apos; category and `Canada&apos; is a known instance of `Country&apos;). For every category pair (e.g. &lt;City, Country&gt;) the sentences containing known instances of both categories are grouped into a set S. The text between the two instances is called the `context pattern&apos; (e.g. `is the capital of&apos; is a context pattern). Three types of pruning are done on this</context>
</contexts>
<marker>Callan, Hoy, 2009</marker>
<rawString>Callan, J., and Hoy, M. (2009). Clueweb09 data set. http://boston.lti.cs.cmu.edu/Data/clueweb09/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
</authors>
<title>Coupling Semi-Supervised Learning of Categories and Relations.&amp;quot;</title>
<date>2009</date>
<booktitle>Proceedings of the NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing.</booktitle>
<marker>Carlson, Betteridge, 2009</marker>
<rawString>Carlson, A., J. Betteridge, et al. (2009). &amp;quot;Coupling Semi-Supervised Learning of Categories and Relations.&amp;quot; Proceedings of the NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
</authors>
<title>Coupled SemiSupervised Learning for Information Extraction,”</title>
<date>2010</date>
<booktitle>Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM),</booktitle>
<marker>Carlson, Betteridge, 2010</marker>
<rawString>A. Carlson, J. Betteridge, et al. (2010). “Coupled SemiSupervised Learning for Information Extraction,” Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
</authors>
<title>Toward an Architecture for Never-Ending Language Learning,”</title>
<date>2010</date>
<booktitle>Proceedings of the Conference on Artificial Intelligence (AAAI),</booktitle>
<marker>Carlson, Betteridge, 2010</marker>
<rawString>A. Carlson, J. Betteridge, et al., (2010b). “Toward an Architecture for Never-Ending Language Learning,” Proceedings of the Conference on Artificial Intelligence (AAAI), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.&amp;quot;</title>
<date>2005</date>
<journal>Artificial Intelligence.</journal>
<marker>Etzioni, Cafarella, 2005</marker>
<rawString>Etzioni, O., M. Cafarella, et al. (2005). &amp;quot;Unsupervised named-entity extraction from the web: An experimental study.&amp;quot; Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
</authors>
<title>Discovering relations among named entities from large corpora.&amp;quot;</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</booktitle>
<marker>Hasegawa, Sekine, 2004</marker>
<rawString>Hasegawa, T., S. Sekine, et al. (2004). &amp;quot;Discovering relations among named entities from large corpora.&amp;quot; Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Su</author>
</authors>
<title>Discovering Relations between Named Entities from a Large Raw Corpus Using Tree Similarity-based Clustering.&amp;quot;</title>
<date>2005</date>
<journal>IJCNLP</journal>
<volume>05</volume>
<marker>Zhang, Su, 2005</marker>
<rawString>Zhang, M., J. Su, et al. (2005). &amp;quot;Discovering Relations between Named Entities from a Large Raw Corpus Using Tree Similarity-based Clustering.&amp;quot; IJCNLP 05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
</authors>
<title>Discovering relations among named entities from large corpora.&amp;quot;</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</booktitle>
<marker>Hasegawa, Sekine, 2004</marker>
<rawString>Hasegawa, T., S. Sekine, et al. (2004). &amp;quot;Discovering relations among named entities from large corpora.&amp;quot; Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Su</author>
</authors>
<title>Discovering Relations between Named Entities from a Large Raw Corpus Using Tree Similarity-based Clustering.&amp;quot;</title>
<date>2005</date>
<publisher>IJCNL</publisher>
<marker>Zhang, Su, 2005</marker>
<rawString>Zhang, M., J. Su, et al. (2005). &amp;quot;Discovering Relations between Named Entities from a Large Raw Corpus Using Tree Similarity-based Clustering.&amp;quot; IJCNL</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>Proc. of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<location>Nantes, F</location>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M. (1992) Automatic Acquisition of Hyponyms from Large Text Corpora. Proc. of the Fourteenth International Conference on Computational Linguistics, Nantes, F</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update;</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<contexts>
<context position="16701" citStr="Hall et al., 2009" startWordPosition="2623" endWordPosition="2626">m 1: Relation Generator Input: One pair of Categories (C1, C2) and set of sentences, each containing a pair of instances known to belong to C1 and C2. The phrase connecting the instances in the sentence is the context. Output: Relations and their seed instances Steps: 1. From the input sentences, build a Context by Context co-occurrence matrix (Shown in figure 1). The matrix is then normalized. 2. Apply K-means clustering on the matrix to cluster the related contexts together. Each cluster corresponds to a possible new relation between the two input categories. (Weka Machine Learning package [Hall et al., 2009] was used to perform K-means clustering. The value of K was set to 5 based on trial and error experiments.) 1450 3. Rank the known instance pairs (belonging to C1,C2) for each cluster and take the top 50 as seed instances for the relation The key data structure used by OntExt is a cooccurrence matrix of the contexts for each category pair, as shown in Figure 1. In this matrix, each cell corresponds to the number of pairs of category instances that both contexts co-occur with (e.g. the sentences “Vioxx can cure Arthritis” and “Vioxx is a treatment for Arthritis” provide a case where the 2 cont</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, Ian H. Witten (2009); The WEKA Data Mining Software: An Update; SIGKDD Explorations, Volume 11, Issue 1.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>