<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9981875">
Topical Clustering of MRD Senses Based
on Information Retrieval Techniques
</title>
<author confidence="0.999557">
Jen Nan Chen* Jason S. Chang*
</author>
<affiliation confidence="0.96091">
National Tsing Hua University National Tsing Hua University
</affiliation>
<bodyText confidence="0.999577277777778">
This paper describes a heuristic approach capable of automatically clustering senses in a machine-
readable dictionary (MRD). Including these clusters in the MRD-based lexical database offers
several positive benefits for word sense disambiguation (WSD). First, the clusters can be used
as a coarser sense division, so unnecessarily fine sense distinction can be avoided. The clustered
entries in the MRD can also be used as materials for supervised training to develop a WSD
system. Furthermore, if the algorithm is run on several MRDs, the clusters also provide a means
of linking different senses across multiple MRDs to create an integrated lexical database. An
implementation of the method for clustering definition sentences in the Longman Dictionary
of Contemporary English (LDOCE) is described. To this end, the topical word lists and topical
cross-references in the Longman Lexicon of Contemporary English (LLOCE) are used. Nearly
half of the senses in the LDOCE can be linked precisely to a relevant LLOCE topic using a simple
heuristic. With the definitions of senses linked to the same topic viewed as a document, topical
clustering of the MRD senses bears a striking resemblance to retrieval of relevant documents for
a given query in information retrieval (IR) research. Relatively well-established IR techniques of
weighting terms and ranking document relevancy are applied to find the topical clusters that are
most relevant to the definition of each MRD sense. Finally, we describe an implemented version
of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed
approach in a series of experiments and evaluations.
</bodyText>
<sectionHeader confidence="0.989005" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999730533333334">
Word sense disambiguation (WSD) has been found useful in many natural language
processing (NLP) applications, including information retrieval (Krovetz and Croft 1992;
McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991;
Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received in-
creasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze
1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995;
Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running
text, the task of WSD involves examining contextual information to determine the in-
tended sense from a set of predetermined candidates. It is a nontrivial task to divide
the senses of a word and determine this set, for word sense is an abstract concept
frequently based on subjective and subtle distinctions in topic, register, dialect, collo-
cation, part of speech, and valency (McRoy 1992). Various approaches to word sense
division have been proposed in the literature on WSD, including (1) sense numbers in
every-day dictionaries (Lesk 1986; Cowie, Guthrie, and Guthrie 1992), (2) automatic
or hand-crafted clusters of dictionary senses (Dolan 1994; Bruce and Wiebe 1995; Luk
</bodyText>
<footnote confidence="0.4847455">
* Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC. E-mail:
dr818314@cs.nthu.edu.tw; jschang@cs.nthu.edu.tw.
</footnote>
<note confidence="0.8780545">
C) 1998 Association for Computational Linguistics
Computational Linguistics Volume 24, Number 1
</note>
<bodyText confidence="0.999537882352941">
1995), (3) thesaurus categories (Yarowsky 1992; Chen and Chang 1994), (4) translation
in another language (Gale, Church, and Yarowsky 1992; Dagan, Itai, and Schwa11 1991;
Dagan and Itai 1994), (5) automatically induced clusters with sublexical representation
(Schtitze 1992), and (6) hand-crafted lexicons (McRoy 1992).
This paper is motivated by the observation that directly using dictionary senses
for sense division offers several advantages. Sense distinction according to a dictionary
is readily available from machine-readable dictionaries (MRDs) such as the Longman
Dictionary of Contemporary English (LDOCE) (Proctor 1978). A dictionary such as the
LDOCE has broad coverage of word senses, useful for WSD. Furthermore, indicative
words and concepts for each sense are directly available in numbered definitions and
examples. Lesk (1986) describes the first MRD-based WSD method that relies on the
extent of overlap between words in a dictionary definition and words in the local
context of the word to be disambiguated. The author reports that WSD performance
ranges from 50% to 70% and his method works well for senses strongly associated
with specific collocations, such as ice-cream cone and pine cone.
Unfortunately, using MRDs as the knowledge source for sense division and disam-
biguation leads to some problems. Zernik (1992) notes that the dictionary dichotomy
of senses is inadequate for WSD, because it is defined along grammatical, not seman-
tic, lines. Furthermore, as pointed out in Dolan (1994), the sense division in an MRD
is frequently too fine-grained for the purpose of WSD. A WSD system based on dic-
tionary senses often faces unnecessary and difficult &amp;quot;forced-choices.&amp;quot; Dolan proposes
a heuristic algorithm for forming unlabeled clusters of closely related senses in the
LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the
proposed algorithm was only described in a few examples and was not developed
further. Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk
1995; Yarowsky 1995) still resort to human intervention to identify and group closely
related senses in an MRD.
Using thesaurus categories directly as a coarse sense division may seem to be a vi-
able alternative (Yarowsky 1995). However, typical thesauri, such as Roget&apos;s Thesaurus
(1987), suffer sense gaps and, occasionally, are too fine-grained. Yarowsky (1992) re-
ports that there are uses not listed in Roget&apos;s for 3 of 12 nouns in his WSD study, while
uses which a native speaker might consider as a single sense are often encoded in
several Roget&apos;s categories.
As an alternative approach to word sense division, this paper presents an algo-
rithm capable of automatically clustering senses in an MRD based on topical informa-
tion in a thesaurus. We refer to the algorithm as TopSense (Topical clustering of Senses).
The current implementation of TopSense uses the topical information in the Longman
Lexicon of Contemporary English (LLOCE) (McArthur 1992) to cluster LDOCE senses. The
method makes use of none of the idiosyncratic information in either the LLOCE or the
LDOCE. Therefore, the TopSense algorithm is quite general and is expected to produce
comparable results for other MRDs and thesauri. TopSense is tested on 20 words exten-
sively investigated in recent WSD literature (Schtitze 1992; Yarowsky 1992; Luk 1995).
According to the experimental results, the automatically derived topical clusters can
be used to good effect without any human intervention as a coarse sense division for
WSD.
The rest of the paper is organized as follows. Section 2 starts out with a description
of the MRDs and thesauri used in the computational lexicography and WSD literature,
followed by some observations to justify the topic-based approach to word sense
division. Section 3 describes the LinkSense algorithm for linking senses between an
MRD and a thesaurus. Section 4 shows how the TopSense algorithm based on the IR
model may be used to cluster the senses in an MRD. Examples are given in both
</bodyText>
<page confidence="0.997386">
62
</page>
<note confidence="0.749312">
Chen and Chang Topical Clustering
</note>
<bodyText confidence="0.9998925">
Sections 3 and 4 to illustrate how the algorithms work. Section 4 also describes an
implementation of the algorithms for the LDOCE and the LLOCE and reports the
evaluation results for both algorithms based on a 20-word test set. Section 5 analyzes
the experimental results to demonstrate the strengths and limitations of the method.
The implication of TopSense to WSD and other issues related to lexical semantics are
also touched upon. Section 6 compares the proposed method with other approaches in
the computational linguistics literature. Finally, conclusions are made and directions
for further research are pointed out in Section 7.
</bodyText>
<sectionHeader confidence="0.499412" genericHeader="method">
2. Word Senses in Machine-Readable Dictionaries and Thesauri
</sectionHeader>
<bodyText confidence="0.9999663">
In this section, we look at two knowledge sources of word sense division, which are
currently widely available, namely, the dictionary and the thesaurus. A good-sized
dictionary usually has a large vocabulary and broad coverage of word senses, both of
which are useful for WSD. However, a dictionary&apos;s division of senses for a given word
is often too fine for the task of WSD. On the other hand, a thesaurus organizes word
senses into a fixed set of coarse semantic categories, making it more appropriate for
WSD. However, thesauri tend to have a smaller vocabulary and a narrower coverage of
word senses. To get the best of both worlds of dictionary and thesaurus, we propose to
cluster MRD definitions to yield a broad-coverage sense division with the granularity
of a thesaurus. Therefore, a short description of MRDs and thesauri is in order.
</bodyText>
<subsectionHeader confidence="0.99847">
2.1 Fine-Grained Senses in an MRD
</subsectionHeader>
<bodyText confidence="0.99997">
Interest in MRD-based research has increased over the years; in particular, the LDOCE
and Webster&apos;s Seventh New Collegiate Dictionary (W7) (1967) have drawn much attention.
Much of the MRD-based research has focused on the analysis and exploitation of
the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi,
Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the
definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or
a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations
are then used for various tasks, ranging from the interpretation of a noun sequence
(Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural
ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan
1994). Besides the definition itself, there is an abundance of information listed in a
dictionary entry, including part of speech, subcategory, examples, collocations, and
typical arguments, which is potentially useful for WSD. In this regard, the LDOCE is
particularly appropriate since it uses a reduced, controlled vocabulary of some 2,000
words to define over 60,000 word senses representing a comprehensive vocabulary
and broad coverage of word senses.
It is arguable that the dictionary division of senses for a given word is too fine-
grained, thus inadequate for WSD. For instance, it might not always be necessary or
easy to distinguish between two LDOCE senses bank.1.n.1 (river bank) and bank.1.n.5
(sandbank) shown in Table 1. Hence, dictionary senses can be used to good effect
for WSD only if such closely related senses are merged and treated as one. There is
more than one way to merge dictionary senses. In the following sections, we describe
one such approach, under which MRD senses are merged according to the sense
granularity of a typical thesaurus.
</bodyText>
<subsectionHeader confidence="0.999544">
2.2 Coarse Senses in Thesauri: WordNet, Roget&apos;s, and LLOCE
</subsectionHeader>
<bodyText confidence="0.999841">
One of the most potentially valuable aspects of the thesaurus, as a knowledge source
for word sense division, is the organization of word senses into a limited number of
</bodyText>
<page confidence="0.998507">
63
</page>
<note confidence="0.856294">
Computational Linguistics Volume 24, Number 1
</note>
<tableCaption confidence="0.988397">
Table 1
</tableCaption>
<bodyText confidence="0.53340275">
The sense entries for bank in the LDOCE.
Sense Entries
land along the side of a river, lake, etc.
earth which is heaped up in a field or garden, often making a border or division.
</bodyText>
<figure confidence="0.806926793103448">
a mass of snow, clouds, mud, etc.
a slope made at bends in a road or race-track, so that they are safer for cars to go
round.
= SANDBANK. (a high underwater bank of sand in a river, harbour, etc.)
(of a car or aircraft) to move with one side higher than the other, esp. when
making a turn
a row, esp. of OARs in an ancient boat or KEYs on a TYPEWRITER.
a place in which money is kept and paid out on demand, and where related
activities go on.
(usu. in comb.) a place where something is held ready for use, esp. ORGANIC
products of human origin for medical use.
(a person who keeps) a supply of money or pieces for payment or use in a game
of chance.
to put or keep (money) in a bank.
[esp. with] to keep one&apos;s money (esp. in the stated bank)
Sense ID
bank.1.n.1
bank.1.n.2
bank.1.n.3
bank.l.n.4
bank.l.n.5
bank.2.v.1
bank.3.n.1
bank.4.n.1
bank.4.n.2
bank.4.n.3
bank.5.v.1
bank.5.v.2
Note: Sense ID = Root + Homonym No. + Part-of-speech + Sense No.
</figure>
<tableCaption confidence="0.801667">
Table 2
</tableCaption>
<table confidence="0.717500333333333">
Roget&apos;s semantic classes and categories.
Class Categories Gloss for Classes
A 1-182 Abstract relations
</table>
<listItem confidence="0.9739984">
• 183-318 Space
• 319-446 Matter
D 447-594 Intellect: the exercise of the mind
• 595-816 Volition: the exercise of the will
• 817-990 Emotion, religion and morality
</listItem>
<bodyText confidence="0.990221823529412">
coarse semantic categories. We briefly describe the on-line thesauri, WordNet (Miller
et al. 1993), Roget&apos;s Thesaurus, and LLOCE, which have been used as word sense di-
visions in the computational linguistics literature. WordNet is organized as a set of
hierarchical, conceptual taxonomies of nouns, verbs, adjectives, and adverbs called
synsets. The synsets are too fine-grained from the WSD perspective; WordNet con-
tains 24,825 noun synsets for 32,264 distinct nouns with a total of 43,136 senses in its
noun taxonomy alone. It would be difficult to acquire WSD knowledge for making
such fine distinctions even from a substantial body of training materials.
Roget&apos;s Thesaurus arranges words in a three-layer hierarchy and organizes over
30,000 distinct words into some 1,000 categories on the bottom layer. These categories
are divided into 39 middle-layer sections that are further organized as 6 top-layer
classes. Each category is given a three-digit reference code. To make the hierarchical
structure explicit, an uppercase letter from A to F is added to the reference code to de-
note the top-layer class for each category, as indicated in Table 2. Similarly, the middle
layer is denoted with a lowercase reference letter. The sections related to class B (Space)
are shown in Table 3. Therefore, the reference code for each category is denoted by
an uppercase class letter, a lowercase section letter, and a three-digit category number.
</bodyText>
<page confidence="0.997415">
64
</page>
<note confidence="0.674683">
Chen and Chang Topical Clustering
</note>
<tableCaption confidence="0.755855">
Table 3
</tableCaption>
<figure confidence="0.900594571428571">
Sections related to the Space class in Roget&apos;s.
Class Sections Gloss of Section Examples
B 183-194 a Space in general surface, heavens, room, kitchen, abode
B 195-242 b Dimensions weight, proximity, clothes, wear, hall
B 243-264 c Form idea, distortion, flat, plug, yawn, subway
B 265-318 d Motion rocket, transposition, carrier, entrance
Roget&apos;s
</figure>
<figureCaption confidence="0.990362">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.611886">
Roget&apos;s categorization scheme.
</subsectionHeader>
<bodyText confidence="0.99943945">
For instance, the word bank listed under Category 209 in Roget&apos;s will be prefixed an
additional letter B to denote the class Space, plus a lowercase letter b to denote the
section Dimensions; the reference code 209 is replaced with Bb209. Figure 1 shows the
information for the word bank in Roget&apos;s.
WordNet and Roget&apos;s to a lesser degree present word senses that are too fine-
grained for WSD. Often, uses that a native speaker might consider as a single sense are
encoded in several Roget&apos;s categories or WordNet synsets. For instance, a single LDOCE
sense bank.4.n.1 shown in Table 1 corresponds to two WordNet synsets Depository
financial institution and Bank building and two Roget&apos;s categories, 799 (Treasurer) and
784 (Lending). Similarly, the Roget&apos;s lists two categories 234 (Edge) and 344 (Land) for a
concept treated as one word sense, bank.1.n.1 in the LDOCE. Table 4 provides further
details.
The LLOCE is a hierarchical thesaurus that organizes word senses primarily ac-
cording to subject matter. The LLOCE contains over 23,000 different senses for some
15,000 distinct words. The coarser senses in LLOCE are organized into approximately
2,500 topical word sets. These sets are divided into 129 topics and these topics are
further organized as fourteen subjects. The subjects are denoted with alphabetical ref-
erence letters from A to N (see Table 5). Thus the LLOCE subject, topic, and topical set
constitutes a three-level hierarchy, in which each subject contains 7 to 12 topics and
each topic contains 10 to 50 sets of related words. Table 6 displays the topics related
</bodyText>
<figure confidence="0.9994436">
Land
Treasurer
411}
. bank .. ... bank ...
Abstract
relatio
atter
Emotion,
religion
d morality
Intellect olition
0
Inorganic Prospective Possessive
matter volition relations
o....0
a
class
0
section
0
Dimen
Laterality
Edge
Obliquit
Suppor
Height
. bank ..
)
category
. bank ..
</figure>
<page confidence="0.933641">
65
</page>
<note confidence="0.37184">
Computational Linguistics Volume 24, Number 1
</note>
<tableCaption confidence="0.991308">
Table 4
</tableCaption>
<table confidence="0.921491142857143">
Comparison of MRD and thesaurus treatment of bank senses.
bank.3.n.1 Array
bank.4.n.1 Depository/Bank building 799 (Treasurer)/784 (Lending) Je104 (Finance)
bank.4.n.2 Supply 632 (Storage)
bank.4.n.3 Supply
bank.5.v.1 Deposit 799 (Treasurer) Je106 (Deposit)
bank.5.v.2 Keep money/Deposit 799 (Treasurer) Je106 (Deposit)
</table>
<tableCaption confidence="0.996254">
Table 5
</tableCaption>
<table confidence="0.939903333333333">
LLOCE subjects and their reference letters.
Subject Set Gloss for Subjects
A 1-158 Life and living things
B 1-181 Body; its function and welfare
C 1-357 People and family
D 1-186 Buildings, houses, home, clothes, belongings, personal care
</table>
<listItem confidence="0.97843975">
• 1-143 Food, drink, and farming
• 1-283 Feeling, emotions, attitudes, and sensations
G 1-293 Thought, communication, language, and grammar
H 1-252 Substance, materials, objects, and equipment
1-148 Arts/Crafts, science/technology, industry/education
1-240 Numbers, measurement, money, and commerce
K 1-207 Entailment, sports and games
• 1-273 Space and time
</listItem>
<equation confidence="0.306034">
M 1-225 Movement, location, travel, and transportation
N 1-367 General and abstract terms
</equation>
<bodyText confidence="0.999067714285715">
to subject L (Space and time). Each topical set is given a three-digit reference code;
however, this code does not explicitly reflect the topic. To make use of the informa-
tion related to a topic, we have designated a lowercase letter to each topic. Therefore,
each set is denoted by an uppercase &amp;quot;subject&amp;quot; letter, a lowercase &amp;quot;topic&amp;quot; letter, and a
three-digit &amp;quot;topical set&amp;quot; number. For instance, the word bank listed under L99 in the
LLOCE will be given an additional reference letter d to denote the topic Geography; the
reference code L99 is replaced with Ld099. The LLOCE also provides cross-references
between sets and topics to indicate various intersense relations not captured within
the same topic. For instance, topic Ld (Geography) has a cross-reference to topic Me
(Place). Figure 2 shows LLOCE&apos;s topical classification and cross-references related to
the word bank.
The LLOCE, and, to a lesser degree, Roget&apos;s, are based on coarse, topical seman-
tic classes, making them more appropriate for WSD than the finer-grained WordNet
synsets. The 129 topics in the LLOCE or 990 categories in Roget&apos;s appear to be suffi-
</bodyText>
<figure confidence="0.952947692307692">
LDOCE WordNet Sense Roget&apos;s Sense
bank.1.n.1 Ridge 234 (Edge)/344 (Land)
bank.1.n.2 Ridge 234 (Edge)
bank.1.n.3 Array
bank.1.n.4 Slope 239 (Laterality)
bank.1.n.5 Ridge 344 (Land)
bank.2.v.1 Tip laterally 239 (Laterality)
LLOCE Sense
Ld099 (River bank)
Ld099 (River bank)
Nj295 (To bend)
66
Chen and Chang Topical Clustering
</figure>
<tableCaption confidence="0.9394715">
Table 6
Topics related to subject L in LLOCE.
</tableCaption>
<table confidence="0.9994867">
Subject Range Gloss
L 001-019 a The universe
L 020-039 Light and color
L 040-079 Weather and temperature
L 080-129 Geography
L 130-169 Time generally
L 170-199 Beginning and ending
L 200-219 Old, new, and young
L 220-249 Period/Measure of time
L 250-273 Function words (time)
</table>
<tableCaption confidence="0.774941">
Examples
sun, moon, star, left, right, etc.
light, dark, ray, color, white, black, etc.
weather, sky, rain, snow, rain, ice, etc.
stream, sea, lake, flood, to flow, etc.
time, history, frequent, permanent, etc.
start, stop, late, last, etc.
ancient, modern, future, age, etc.
day, night, second, minute, etc.
now, soon, always, ever, after, etc.
</tableCaption>
<figure confidence="0.98537825">
LLOCE
People Buildin Materia Money Space Location
Subject 0 0
Organization Government Owning Material Banking Geograph Travel Place
0 Topic • • • •
---
C) Sets
- - - So- cross-reference
</figure>
<figureCaption confidence="0.848618">
Figure 2
LLOCE&apos;s topical organization of word sense.
</figureCaption>
<bodyText confidence="0.999961777777778">
cient for representing the distinction we would want to make for the task of WSD.
Roget&apos;s has been used as the sense division in two recent WSD works (Yarowsky 1992;
Luk 1995) more or less as is, except for a small number of senses added to fill gaps.
We contend that a sense division based on the LLOCE topics will offer more or less
the same kind of granularity, suitable for WSD. For instance, in Yarowsky (1992), the
senses of star are divided into three Roget&apos;s categories, which roughly correspond to
five LDOCE star senses labeled with LLOCE topics. In the same study, six Roget&apos;s cat-
egories are sufficient to distinguish the senses of slug. These six categories correspond
to five relevant LLOCE topics. Table 7 provides further details.
</bodyText>
<subsectionHeader confidence="0.999977">
2.3 Combining Word Sense Information from an MRD and a Thesaurus
</subsectionHeader>
<bodyText confidence="0.9998708">
It should be clear by now that combining a dictionary and a thesaurus leads to a
broad-coverage sense division with a suitable granularity for WSD. The obvious way
to combine the two would be to disambiguate and link a sense definition D of a
headword h in the dictionary to an entry relevant to D in the thesaurus. This amounts
to a special case of WSD with respect to thesaurus senses. There is no simple solution
</bodyText>
<figure confidence="0.9155995">
Je104 Ld099
... bank ..
</figure>
<page confidence="0.864675">
67
</page>
<note confidence="0.342321">
Computational Linguistics Volume 24, Number 1
</note>
<tableCaption confidence="0.977074">
Table 7
</tableCaption>
<table confidence="0.930470692307692">
Roget&apos;s and LLOCE classifiers for two sample words.
Word Roget&apos;s (three-layer representation) LLOCE (two-layer representation)
star 321 (Universe) La (Universe)
594 (Entertainer) Kd (Drama)
729 (Insignia) Jb (Mathematics)
Dg (Personal belongings)
Nb (Chance)
slug 365 (Animal/Insect) Ag (Insect)
587 (Printing) Gd (Communication)
359 (Impulse/Impact)
797 (Money) Jd (Money)
723 (Arms) Hh (Weapon)
322 (Weight) Hc (Specific substances)
</table>
<bodyText confidence="0.995542529411765">
to the general WSD problem for unrestricted text, but we will show that this special
case of disambiguating MRD definitions is significantly easier, for several reasons.
First, the words used in a definition sentence are limited primarily to a small set;
in the case of the LDOCE, the controlled vocabulary consists of some 2,000 words.
For instance, in the first five LDOCE senses of bank shown in Table 1, all defining
words are in the controlled vocabulary, except for the word SANDBANK, shown in
capital letters. Obtaining WSD information for this small set of words obviously is
much easier than it would be for a large, open set.
Second, dictionary definitions adhere to rather rigid patterns under which only
words with predictable semantic relations show up. A dictionary definition, in general,
begins with a genus term (that is, conceptual ancestor of the sense), followed by a set
of differentiae that are words semantically related to the sense to provide the specifics.
The semantic relations between the sense, the genus, and differentiae are reflected in
what are termed categorical, functional, and situational clusters in McRoy (1992).
The semantic relations and clusters have been shown to be very effective knowledge
sources for such NLP tasks as WSD (McRoy 1992) and interpretation of noun sequences
(Vanderwende 1994). For instance, in the first four definitions of bank in Table 1, the
genus terms land, earth, mass, and slope are categorically related to the respective bank
senses. On the other hand, the differentiae river, lake, field, garden, bend, road, and race-
track have a LocationOf situational relation with bank. Other differentiae, snow, cloud,
and mud, are related functionally to bank.1.n.3 through the MakeOf relation.
Third, for the most part these relations are captured implicitly in a typical the-
saurus. The LLOCE and Roget&apos;s conveniently contain information on the relations in
the form of word lists under a topic (category) or cross-referencing to other topics.
Therefore, an MRD sense definition can be effectively disambiguated based on the
word lists and cross-references in a thesaurus. A simple heuristic relying on the sim-
ilarity between a sense&apos;s defining keywords and thesaurus word lists suffices to link
an MRD sense to its relevant sense in the thesaurus. For instance, the differentiae
(land, side, river, lake) of bank.1.n.1 is sufficiently similar to the word list of Ld-topic
(Geography) to warrant the link between LDOCE sense bank.1.n.1 and LLOCE sense
bank-Ld099.
The topics and cross-references of LLOCE in general capture the Generic/Specific
relation; therefore, a sense definition is often disambiguated through the genus. Thus,
the task of linking MRD and thesaurus senses is closely related to the extraction and
</bodyText>
<page confidence="0.997597">
68
</page>
<note confidence="0.310687">
Chen and Chang Topical Clustering
</note>
<bodyText confidence="0.999823857142857">
disambiguation of the genus. For instance, in the above example, linking bank.1.n.1 to
bank-Ld099 has, as a by-product, the disambiguation of the genus land to land-Ld084
(Geography) rather than land-Ce078 (Social organization in groups and place). Details of
extraction and disambiguation of the genus can be found in previous works (Guthrie
et al. 1990; Klavans, Chodorow, and Wacholder 1990; Copestake 1990; Ageno et al.
1992). Disambiguated genus and differentiae terms can be used to construct a better
taxonomy of word senses.
Since the dictionary usually has broader coverage of word senses than the the-
saurus, not all MRD senses of a headword h correspond to one of h&apos;s predefined
senses in the thesaurus. For instance, LDOCE sense bank.1.n.3 (a mass of cloud, snow,
or mud, etc.) corresponds to LLOCE topic Hb (Object generally) rather than any of the
predefined LLOCE senses for bank. Therefore, such entries represent sense gaps in the
thesaurus and should be left unlinked. Nevertheless, the linked entries are enough
training material for topical clustering of MRD senses, as described in Section 4.
</bodyText>
<subsectionHeader confidence="0.770065">
3. Linking an MRD to a Thesaurus
</subsectionHeader>
<bodyText confidence="0.995117666666667">
This section describes how to establish a link between an MRD sense and its relevant
word sense in a thesaurus, if such a link exists. We start with the preprocessing steps
for the sense definition, which are necessary for the algorithm to obtain good results.
Then we describe the linking algorithm step by step. Finally, we show illustrative
examples to give some idea how the proposed algorithm works for the LLOCE and
Roget&apos;s.
</bodyText>
<subsectionHeader confidence="0.999988">
3.1 Preprocessing Steps
</subsectionHeader>
<bodyText confidence="0.999283909090909">
Although only simple words are usually used in sense definitions, most of these words
are also highly ambiguous. For instance, the two instances of lies listed in the two
following LDOCE sense definitions differ in meaning:
couch.2.n.2 a bed-like piece of furniture on which a person lies when being ex-
amined by a doctor.
lie detector an instrument that is supposed to show when a person is telling lies.
Notably, their parts-of-speech are also different. Determining the part of speech of
each instance allows us to limit the range of possible meanings. The first instance
of lies is a verb that means &amp;quot;to be in a flat resting position&amp;quot; or &amp;quot;to tell a lie.&amp;quot; On
the other hand, the second instance is a nominal with a unique meaning &amp;quot;a false
statement purposely made to deceive.&amp;quot; By tagging the definition with part-of-speech
information, the degree of sense ambiguity in the definition can be reduced, thereby
increasing the chance of successful linking.
Part-of-Speech Tagging. Various methods for POS tagging have been proposed in re-
cent years. For simplicity, we adopted the method proposed by Church (1988) to tag
definition sentences. Experiments indicated an average error rate for tagging of less
than 10%. Tagging errors have limited negative impact, because words in the LLOCE
are organized primarily according to topic, not part of speech. The POS information
is used to remove function words, as well as to look up words in the LLOCE with
matching POS. The part-of-speech preprocessing phase is mandatory for the algorithm
to exclude some inappropriate candidates for topics. See Table 8 for some examples
of tagged LDOCE definition sentences.
</bodyText>
<page confidence="0.996226">
69
</page>
<note confidence="0.407515">
Computational Linguistics Volume 24, Number 1
</note>
<tableCaption confidence="0.992368">
Table 8
</tableCaption>
<table confidence="0.907062875">
Some tagged LDOCE definition sentences for the headword bank.
bank.1.n.1 land/n along/prep the/det side/n of/prep a/det river/n ,/, lake/n ,/, etc/adv
bank.1.n.2 earth/n which/det is/v heaped/v up/adv in/prep a/det field/n or/conj gar-
den/n ,/, often/adv making/v a/det border/n or/conj division/n
bank.1.n.3 a/det mass/n of/prep snow/n ,/, clouds/n ,/, mud/n ,/, etc/adv
bank.1.n.4 a/det slope/n made/v at/prep bends/n in/prep a/det road/n or/conj race-
track/n ,/, so/conj that/conj they/pron are/v safer/adj for/conj cars/n to/* go/v
round/adv
</table>
<tableCaption confidence="0.994458">
Table 9
</tableCaption>
<figure confidence="0.243635">
Examples of keywords extracted from tagged definition sentences.
bank.l.n.1 land/n side/n river/n lake/n
bank.l.n.2 earth/n heap/v field/n garden/n border/n division/n
bank.1.n.3 mass/n snow/n clouds/n mud/n
bank.1.n.4 slope/n bend/n road/n race-track/n cars/n
</figure>
<bodyText confidence="0.994431">
Removal of Stopwords. In general, function words in the definition are only marginally
relevant to the meaning being defined. This is also true of words used in many defi-
nitions. For this reason, IR systems commonly exclude stopwords from the process of
indexing and query. This also applies to our situation of retrieving topics relevant to
the meaning of a sense based on the words in its definition. The list of all the stop-
words is specifically designed to remove pronouns, determiners, prepositions, and
conjunctions. Table 9 shows that the meaning of some definitions of bank is found to
be quite intact, even after stopwords are removed.
Calculating Similarity between Definition and Thesaurus Class. When viewing the definition
of a headword h as a set of words, it becomes easy to compare and measure their
similarity to thesaurus word classes containing h. By word classes, we mean any
supersets of synonym sets in a thesaurus that capture the semantic relations and
semantic clusters that are effective for disambiguation as described in Section 2.3. The
word classes are so chosen that they contain enough words to overlap with the sense
definition in question. But each class should not be so big as to cover more than one
thesaurus sense for h, blurring the distinction we want to make in the first place. Topics
in the LLOCE and categories or sections in Roget&apos;s are good choices for such classes.
Similarity between the defining keywords and a class of words reflects how closely
the definition is related to the class. As a simple heuristic, the intended meaning of a
dictionary definition D for h is disambiguated in favor of a relevant sense T for h in a
thesaurus class C with the highest similarity to D. When such a sense T is found, we
say that the dictionary sense D is linked to the thesaurus sense T or that D is linked
to the thesaurus class C (containing T.)
For a headword h, let DEFh denote the definitions of h and let CLASSh be the word
classes in a thesaurus that contain h. For a definition D e DEFh, our problem amounts
to finding C E CLASSh that is relevant to D. With these terms, the unweighted Dice
coefficient can be adopted to measure similarity between a definition D and a class C
as follows:
</bodyText>
<equation confidence="0.989707333333333">
EdEKEyD 2 x wd x In(d, C)
Sim(D, C) =
1KEYDI + ICI
</equation>
<bodyText confidence="0.991119">
where KEYD = the set of words in definition D E DEFh, IKEYDI = number of words in
</bodyText>
<page confidence="0.945176">
70
</page>
<figure confidence="0.909669">
Chen and Chang Topical Clustering
KEYD, C E CLASSh --= a relevant class to h in the thesaurus, Wk = 1
</figure>
<figureCaption confidence="0.323926">
degree of ambiguity of lc&apos;
</figureCaption>
<bodyText confidence="0.940188166666667">
In(a, B) = 1, when a E B, and In(a, B) = 0, when a ,% B.
The above similarity measure may be improved by taking into consideration spe-
cific features of a particular thesaurus. For instance, the cross-reference features in the
LLOCE or the intersense relations in Roget&apos;s are very effective in reflecting semantic re-
latedness; thus, they should be included in this similarity measure. Let REFc represent
the cross-referenced classes for the word class C in the thesaurus. Thus, we have
</bodyText>
<equation confidence="0.996029">
EdEKEYD 2 wd x (In(d, C) + -y In(d, REFc))
Sim&apos;(D,C) =
</equation>
<bodyText confidence="0.999958">
where -y = relevancy of cross-references to a classl, and IREFc I = the number of classes
in REFc.
</bodyText>
<subsectionHeader confidence="0.999972">
3.2 The LinkSense Algorithm
</subsectionHeader>
<bodyText confidence="0.9995295">
We sum up the above description and outline the procedure for labeling senses on a
dictionary entry as follows:
</bodyText>
<subsectionHeader confidence="0.670016">
Algorithm LinkSense
</subsectionHeader>
<bodyText confidence="0.703540375">
Linking fine-grained MRD senses to their relevant thesaurus classes.
Step 1: Given a head word h, read its definition, DEFh, from the MRD.
Step 2: For each definition D in DEFh, tag each word in D with POS information.
Step 3: Remove all stop words in D to obtain a list of keyword-POS pairs, KEYD.
Step 4: Look up the headword h in the thesaurus to obtain CLASSh.
Step 5: Compute Sim(D, C) for all C E CLASSh.
Step 6: Link D to C such that Sim(D, C) is the largest and Sim(D, C) is greater
than a preset threshold, O.
</bodyText>
<subsectionHeader confidence="0.999808">
3.3 Illustrative Examples: Linking LDOCE to LLOCE and Roget&apos;s
</subsectionHeader>
<bodyText confidence="0.997158428571429">
Two examples are given in this subsection to illustrate how LinkSense works to establish
linkage between a typical dictionary and thesaurus. Example 1 shows, step by step,
how LinkSense links up an LDOCE sense, interest.1.n.2 (a share in a company business
etc.) with the relevant LLOCE sense interestie (Banking).2 Example 2 is intended to
show that LinkSense is quite general and applies to thesauri other than the LLOCE.
The same LDOCE senses will be shown to links to a relevant Roget&apos;s sense interest-El
(Possessive relation).
</bodyText>
<subsectionHeader confidence="0.759882">
Example 1
</subsectionHeader>
<bodyText confidence="0.931368">
Linking an LDOCE sense interest.1.n.3 to its relevant LLOCE sense.
</bodyText>
<equation confidence="0.6890375">
Step 1: D = &amp;quot;a share in a company, business, etc.&amp;quot;
Step 2: POSD = {a/ det, share /n, in/prep, a/ det, company/n, business /n, etc. / adv}
</equation>
<footnote confidence="0.89361825">
1 For simplicity, the parameter -y is set to 1 in our experiment.
2 Je represents the class of words related to the topic of Banking, Wealth, and Investment listed under
LLOCE topical sets Je100 through 127. The reference code e is added in accordance with the coding
scheme described in Section 2.2.
</footnote>
<figure confidence="0.486707333333333">
IKEYDI + ICI + 7IREFc1
71
Computational Linguistics Volume 24, Number 1
</figure>
<figureCaption confidence="0.3243795">
Step 3: KEYD = {share/n, company/n, business/n}, IKEYDI = 3.
Step 4: Using LLOCE topics as word classes in LinkSense, we have
</figureCaption>
<equation confidence="0.594288714285714">
CLASSinterest = {Fj (Excitement), Fb (Liking), Je (Banking), Ka (Entertainment)},
The LLOCE lists the following cross references relevant to CLASSinterest:
REF, {Ka (Entertainment), Kb (Music and related activity),. ,
Kb (Outdoor games)},
REF Fb {Cc (Friendship)}, = {De (Getting and giving)},
REFica = {Fj (Excitement)}, IFj1 = 1, IFb1 = 1, IJel =- 1, IKal = 1,
IREFFj I = 8, IREFFbI = 1, IREFjel = 1, IREFKa I = 1.
</equation>
<bodyText confidence="0.916163333333333">
All three keywords appear in three different topics but only the following
classes are relevant to CLASSinterest: De (share), Je (share), Cc (company)
Thus, we have
</bodyText>
<equation confidence="0.9751015">
In(share, De) = 1, In(share, Je) = 1, In(company, Cc) = 1.
Wshare/n = Wcompany/n = Wbusiness/n = 1/3.
</equation>
<bodyText confidence="0.8846465">
Step 5: Similarity values are calculated as follows:
Sim&apos; (D,Je) = 2 X Wshare X (In (share, Je) + In (share, REFje))
j{share, company, business} I + I {Je} I + IREFje I
2 X Wcompany X (In (company, Je) + In (company, REFje))
I {share, company, business} I + I {ie} + IRjeI
+ 2 x wbusiness x (In (business, Je) + In (business, REFie))
I {share, company, business} I + I {Je} I + IREF)e I
2x x (1 + 1) + 2 x x (0 + 0) + 2 x A x (0 + 0)
3 + 1 + 1
L33
= 0.267,
5
</bodyText>
<equation confidence="0.99724425">
2 x 1 x (0 + 1)
Sim&apos;(D,Fb) = 33+ 1 + 1 = 0.133,
Sim&apos; (D, Fj) = 0,
Sim&apos; (D, Ka) = 0.
</equation>
<bodyText confidence="0.568172">
Step 6: The LDOCE sense, interest.1.n.3 is linked to the LLOCE sense, interest-Je.
</bodyText>
<subsectionHeader confidence="0.675754">
Example 2
</subsectionHeader>
<bodyText confidence="0.980952333333333">
Linking an LDOCE sense interest.1.n.3 to its relevant Roget&apos;s sense.
Step 1-3: The first three steps are independent of the thesaurus used, therefore
the same results as in Example 1 should be obtained.
</bodyText>
<page confidence="0.968803">
72
</page>
<figure confidence="0.687460857142857">
Chen and Chang Topical Clustering
Step 4: Using Roget&apos;s categories as word classes in LinkSense, we have:
CLASS interest = {Ab (Dimensions), Cb (Inorganic matter),
Eb (Prospective volition), Ei (Possessive relations)},
The keywords share, company, and business appear in many Roget&apos;s sections,
but only the following sections are relevant to CLASSinterest: Ei (share and
business), Eb (business)
Therefore we have:
Wshare/n = 1/4, Wcompany/n = 1/5, wbusinessin = 1/7.
Step 5: For simplicity, we ignore the cross-reference information in Roget&apos;s and
base our similarity calculation solely on the CLASS information. Thus, we
have:
Sim(D, Bb) = 0,
Sim(D, Cb) = 0,
2x12
Sim(D, Eb) = 3 + 1
4 28 0.071,
Sim(D, Ei)
2 x(+ 11 .196.
-= -=
3 1 4 56
</figure>
<figureCaption confidence="0.692638">
Step 6: The LDOCE sense interest.1.n.3 is linked to Roget&apos;s sense interest-Ei.
</figureCaption>
<subsectionHeader confidence="0.99654">
3.4 Performance Evaluation of LinkSense
</subsectionHeader>
<bodyText confidence="0.999990333333333">
An experiment involving the LDOCE and the LLOCE was carried out to assess the
effectiveness of the LinkSense algorithm (see Table 10). To evaluate the performance of
algorithms, we define the ratios of applicability A and precision P as follows:
</bodyText>
<table confidence="0.99314675">
A= # (all labeled definitions)
# (all definitions)
# (correct labeled definitions)
# (all labeled definitions) •
</table>
<bodyText confidence="0.998185">
Nearly half of the nominal LDOCE senses for a set of highly polysemous words are
linked to their relevant LLOCE sense and topics, with a surprisingly high precision
rate of 93%. For the other half, LinkSense does not find sufficiently high similarity to
warrant a link. That is due primarily (approximately two-thirds) to sense gaps in the
LLOCE, rather than inconsistency among the LDOCE definitions.
</bodyText>
<sectionHeader confidence="0.698452" genericHeader="method">
4. Topical Clustering of MRD Senses as Information Retrieval
</sectionHeader>
<bodyText confidence="0.999679833333333">
In this section, we will describe TopSense, an algorithm for clustering dictionary senses.
TopSense clusters closely related senses by applying IR techniques on the results of
running LinkSense on an MRD. After LinkSense links a substantial portion of MRD
senses to thesaurus sense classes, we put all definitions of the senses linked to a
particular class together in a document. With such a document of collective definitions,
topical clustering of all MRD senses bears a striking resemblance to the IR task of
</bodyText>
<page confidence="0.996033">
73
</page>
<note confidence="0.408456">
Computational Linguistics Volume 24, Number 1
</note>
<tableCaption confidence="0.994249">
Table 10
</tableCaption>
<table confidence="0.97866592">
Performance of LinkSense algorithm.
Headword # of Definitions Linking to the LLOCE
in LDOCE
Correct Incorrect Unknown Applicability Precision
bass 5 2 1 2 67% 100%
bow 5 4 0 1 80% 100%
cone 3 3 0 0 100% 100%
country 5 5 0 0 100% 100%
crane 2 2 0 0 100% 100%
duty 2 1 0 1 50% 100%
galley 4 2 0 2 50% 100%
interest 6 4 0 2 67% 100%
issue 8 2 1 5 38% 50%
mole 3 2 0 1 67% 100%
plant 6 1 0 5 17% 100%
position 10 7 0 3 70% 100%
sentence 2 2 0 0 100% 100%
slug 5 2 0 3 40% 100%
space 8 2 0 6 25% 100%
star 9 3 2 4 56% 60%
suit 6 3 0 3 50% 100%
table 7 2 0 5 29% 100%
tank 3 1 0 2 33% 100%
taste 6 1 0 5 17% 100%
total 105 51 4 50 52% 93%
</table>
<bodyText confidence="0.999410909090909">
retrieving relevant documents for a given query. We observe that the defining words
of a sense S frequently recur in documents relevant to S. For instance, consider the
following LDOCE sense:
star.1.n.5 a piece of metal in this shape for wearing as a mark of office, rank,
honour, etc.
We observe that most entries in the LDOCE are like star.1.n.5, in that they contain
defining words that are also recurring terms in a relevant document. For instance,
headwords such as apron, bracelet, necklace, and tie are defined by using terms in a
document corresponding the LLOCE word class for the topic Dg (Clothes and personal
belongings). Table 11 shows the Dg class, including such senses as apron, bracelet, neck-
lace, and tie, which are indeed relevant to star.1.n.5.
</bodyText>
<subsectionHeader confidence="0.999614">
4.1 The Clustered Senses as Documents
</subsectionHeader>
<bodyText confidence="0.999982222222222">
With topical clustering of MRD senses cast as an IR task, a wealth of well-understood
IR techniques can be utilized, including stopword removal, case folding, stemming,
term weighting, and document ranking (Witten, Moffat, and Bell 1994). Using the IR
analogy, topical clustering of an MRD sense S is finding relevant documents (topical
clusters), given a query (S, the sense definition). With this in mind, we treat the col-
lective definitions of each topical cluster as a virtual document (VD) and reduce the
clustering task to ranking relevancy based on terms in the sense definition as well as
those in the VDs. For simplicity, we adopt a common scheme of tf x idf to weight terms
in the documents. Each defining term in a VD is associated with a term frequency (tf)
</bodyText>
<page confidence="0.987049">
74
</page>
<note confidence="0.513157">
Chen and Chang Topical Clustering
</note>
<bodyText confidence="0.940198214285714">
and document frequency (df). Let tf,j represent the frequency of term tj in document
VD,, and dfj represent the number of VDs where term tj appears. The relevancy of VD,
to the sense S according to term tj is therefore given by the following weight:
Wij = tfij x idfj = tfij x log (N/dfj),
where N is the number of documents in the collection. The relevancy of a VD; to a
query Q is obtained by summing up the weights of all terms tj in Q:
Etf, x log(N/dfj).
Table 11 shows the LDOCE senses and their definitions that are linked to relevant
LLOCE senses under a certain topic. An implementation of LinkSense links 455 LDOCE
senses including accessory, bracelet, and tie to a Dg class in the LLOCE. The definitions
of these senses in the Dg class form the virtual document Dog. As shown in Table 12,
significantly topical terms are used within a VD consistently. For instance, the term
angle appears consistently in 10 senses in Djb with a weight of 23.82. Table 12 displays
heavily weighted terms in some VDs and their associated values of tf, df, and weight.
</bodyText>
<subsectionHeader confidence="0.999166">
4.2 The TopSense Algorithm
</subsectionHeader>
<bodyText confidence="0.6790765">
We sum up the above descriptions and outline the TopSense algorithm here.
Algorithm TopSense: Topical clustering of MRD senses.
</bodyText>
<figureCaption confidence="0.8199215">
Step 1: Run LinkSense on the MRD and thesaurus and collect terms to form VDs.
Step 2: Read sense definition S from the MRD.
Step 3: Remove all stopwords in S and produce a list, Q, of stemmed keywords
with part of speech.
</figureCaption>
<bodyText confidence="0.8981444">
Step 4: For each term tj in Q, look up the corresponding IN for all virtual docu-
ments Dc, C E CLASS, the set of all word classes in the thesaurus.
Step 5: For C E CLASS, calculate Sim(Q, = Et, wii, where Wij = tfij x log(N/dfj).
Step 6: Assign S to the class C such that Sim(Q, Dc) is the largest for all C E CLASS
and passes a preset threshold O.
</bodyText>
<subsectionHeader confidence="0.998476">
4.3 Illustrative Examples: Topical Clustering of LDOCE Senses
</subsectionHeader>
<bodyText confidence="0.986328">
Two examples are given in this subsection to illustrate how TopSense works. Example 3
shows a calculation done in TopSense to find the most relevant topics for another star
sense (a 5-or more pointed figure). Example 4 shows the same calculation done for the
sense of star (a piece of metal in this shape for wearing as a mark of office, rank, honour etc.)
discussed above at the beginning of Section 4. Despite very ambiguous terms such as
to wear (to dress or to rub) and figure (body, shape, or number) present in both definitions,
the weighting scheme of TopSense seems to work well enough to determine the relevant
topics, Dg (Clothes and personal belongings) and Jb (Mathematics), respectively.
</bodyText>
<page confidence="0.987998">
75
</page>
<table confidence="0.469385">
Computational Linguistics Volume 24, Number 1
</table>
<tableCaption confidence="0.987386">
Table 11
</tableCaption>
<table confidence="0.9698574">
Partial list of LDOCE senses linked to LLOCE classes by LinkSense.
Cluster/ Size Headword Sense Definition
Dg / 455 • accessory • something which is not a necessary part of something
(Clothes and per- larger but which makes it more beautiful, useful, effec-
sonal belongings) tive etc.
</table>
<listItem confidence="0.883818461538462">
• apron • a simple garment worn over the front part of one&apos;s
clothes to keep them clean while working or doing
something dirty or esp. while cooking.
• bracelet • a band or ring, usu. of metal, worn round the wrist or
arm as an ornament.
• coat • an outer garment with long SLEEVEs, often fastened at
the front with buttons, and usu. worn to keep warm or
for protection.
• necklace • a string of jewels, BEADs, PEARLs, etc., or a chain of
gold, silver, etc., worn around the neck as an ornament
esp. by women.
• tie • a band of cloth worn round the neck, usu. inside a shirt
collar and tied in a knot at the front.
</listItem>
<bodyText confidence="0.57281075">
Jb / 212 • cross • a figure or mark formed by one straight line crossing
another, as X, often used.
(Math) • diameter • a straight line going from side to side through the centre
of a circle or other curved figure.
</bodyText>
<listItem confidence="0.999408">
• pyramid • a solid figure with a flat usu. square base and straight
flat 3-angled sides that slope upwards to meet at a point.
• rectangle • a figure with 4 straight sides forming 4 right angles.
• square • a figure with 4 equal sides and 4 right angles.
• triangle • a flat figure with 3 straight sides and 3 angles.
</listItem>
<equation confidence="0.7859695">
Ld / 524 • bank • land along the side of a river, lake, etc.
(Geography) • bay • a wide opening along a coast; part of the sea or of a
</equation>
<bodyText confidence="0.852994">
large lake enclosed in a curve of the land.
</bodyText>
<listItem confidence="0.988023071428572">
• beach • a shore of an ocean, sea, or lake or the bank of a river
covered by sand, smooth stones, or larger pieces of rock.
• lake • a large mass of water surrounded by land.
• cascade • a steep high usu. small waterfall, esp. one part of a
bigger waterfall.
Je / 181 • account • a record or statement of money received and paid out,
(Banking) as by bank or business, esp. for a particular period or
at a particular date.
• asset • something such as a house or furniture, that has value
and that may be sold to pay a debt.
• bank • a place in which money is kept and paid out on demand,
and where related activities go on.
• capital • wealth, esp. when used to produce more wealth.
• stock • money lent to a government at a fixed rate of interest.
</listItem>
<tableCaption confidence="0.6653576">
Example 3
Clustering an LDOCE sense star.1.n.3.
Step 1: Refer to Table 11 for some of the results of running LinkSense.
Step 2: S = &amp;quot;a 5-or more pointed figure.&amp;quot;
Step 3: Q = {pointed/a, figure/n}
</tableCaption>
<page confidence="0.599649">
76
</page>
<tableCaption confidence="0.466504">
Chen and Chang Topical Clustering
Table 12
Some examples of virtual documents, terms, tf, df, and weight.
</tableCaption>
<table confidence="0.990026440677966">
VD Terms tf df Weight VD Terms tf df Weight
Dg garment 43 12 102.45 Ld sea 38 23 65.81
wear 60 27 94.30 land 47 40 55.39
dress 21 5 68.42 mountain 16 7 46.74
woman 49 36 62.91 water 44 47 44.76
coat 19 7 55.51 river 17 15 36.71
trouser 11 2 45.91 tide 7 1 34.07
shirt 12 3 45.22 valley 8 2 33.39
undergarment 8 2 33.39 ocean 9 4 31.33
shoe 12 11 29.63 lake 10 8 27.88
skirt 6 1 29.20 shore 8 4 27.84
cloth 16 25 26.37 earth 16 26 25.75
waist 8 5 26.06 island 6 2 25.04
jacket 6 2 25.04 rock 11 14 24.51
sleeve 5 1 24.33 wave 9 13 20.72
glove 5 2 20.87 hill 8 10 20.51
sock 5 2 20.87 deep 12 25 19.78
neck 10 17 20.34 coast 6 5 19.54
underpants 4 1 19.47 slope 7 8 19.51
woolen 5 4 17.40 cliff 5 3 18.84
tie 7 14 15.59 map 7 9 18.69
Jb mathematics 15 4 52.21 Je money 42 39 50.56
multiply 8 3 30.15 account 16 9 42.72
figure 13 19 25.00 bank 16 12 38.12
straight 12 17 24.41 pay 17 31 24.37
angle 10 12 23.82 lend 7 5 22.80
line 21 44 22.75 interest 12 25 19.78
circle 9 11 22.22 debt 5 3 18.84
geometry 5 3 18.84 sum 6 10 15.38
calculate 7 9 18.69 wealth 5 6 15.37
add 8 13 18.42 credit 3 1 14.60
subtract 3 1 14.60 property 5 9 13.35
curved 7 5 11.54 deposit 2 1 9.73
perpendicular 2 1 9.73 savings 2 1 9.73
proportion 2 1 9.73 payment 4 14 8.91
right-angled 2 1 9.73 share 4 15 8.63
triangle 2 1 9.73 record 4 16 8.37
edge 6 26 9.65 spend 3 11 7.40
arc 2 2 8.34 business 6 40 7.07
curve 3 9 8.01 supply 4 25 6.59
cross 3 11 7.40 amount 6 46 6.23
Step 4: For each term in Q, we have: Wpointed,Hd = 0,
Wpointed,r3 = 0, Wpointed,Kf 0, Wpointed,Gd = 0, Wfigure,Hd = 5.77.
WfigureJb - 25.00, Wfigure,Kf = 9.62, Wfigure,Gd = 7.69,
Step 5: Adding up the weights for each VD, we get
Sim(Q, Jb) = 25.00,
Sim(Q, Kf) = 9.62,
Sim(Q, Gd) = 7.69,
77
Computational Linguistics Volume 24, Number 1
Sim (Q, Hd) = 5.77.
Step 6: For the most relevant topics to S, we get the following ranked list
Jb (Mathematics) ,
Kf (Indoor games) ,
Gd (Communicating),
Hd (Equipment, machines, and instruments).
Example 4
Clustering an LDOCE sense star.1.n.4.
Step 1: See Table 11.
</table>
<equation confidence="0.946969666666667">
Step 2: S = &amp;quot;a piece of metal in this shape for wearing as a mark of office, rank, honour
etc.&amp;quot;
Step 3: Q = {metal/n, shape/n, wear/v, mark/n, office/n, rank/n, honour/n}
Step 4: For each term in Q, we have:
Wmetal,Dg = 3.77,
Wshapewg = 5.98,
Wwear,Dg = 94.30,
Wmark,Dg = 1.11,
Woffice,Dg = 0,
Wrank,Dg = 1.69,
Whonour,Dg = 0,
Knetauir = 62.83,
WWspape,Hc = 8.97, wsmheatpac
eleci = 0,
i = 0,
Wwear,Hc = 0, Wwear,Ct = 0,
Wmark,Hc = 3.32, Wmark,Ci = 0,
Woffice,Hc = 1.61, Woffice,Ci = 3.22,
Wrank,Hr = 0, Wrank,Ci = 72.65,
Whdnour,Dc = 0, Whonour,Ci = 2.30,
Wmetal,Hb = 22.62,
Wshape,Hb = 7.97,
Wwear,Hb = 4.72,
wWrnoffairckextib = 4,
b = 61..661,
Wrank,Hb = 0,
Whonour,Hb -= 2.30.
Step 5: Adding up the weights for each VD, we get
Sim(Q,Dg) = 115.16,
Sim(Q,Hc) = 100.29,
Sim(Q,Hb) = 88.83,
Sim(Q, Ci) = 78.17.
Step 6: For the most relevant topics to S, we get the following ranked list:
</equation>
<bodyText confidence="0.410426">
Dg (Clothes and personal belongings),
Hc (Specific substances and materials),
Hb (Object generally),
</bodyText>
<subsectionHeader confidence="0.7013665">
Ci (Social classifications and situations).
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999145">
An experiment was conducted to assess the effectiveness of the LinkSense and TopSense
algorithms. The experimental results show that the LinkSense links nearly 11,045 of
some 39,000 nominal LDOCE senses to a topical sense in the LLOCE. Evaluation
based on a 20-word test set shows that, on the average, 50% of the LDOCE instances
linked to an LLOCE sense, and, of these links, 95% are correct. These linked LDOCE
senses establish 129 topical clusters, one for each LLOCE topic. When the proposed
</bodyText>
<page confidence="0.982857">
78
</page>
<note confidence="0.512458">
Chen and Chang Topical Clustering
</note>
<bodyText confidence="0.999819444444444">
LinkSense algorithm is applied to assign sense definitions in LDOCE with relevant
topical labels, it obtains very high precision but low coverage. TopSense is design
specifically to improve coverage by providing a reliable method for clustering MRD
entries left unlabeled by LinkSense.&apos; A document of defining terms is then formed from
MRD senses in each of these clusters. Subsequently, TopSense runs on the nominal
LDOCE sense, attempting to merge it to one of the topical clusters.
The thresholds for LinkSense and TopSense are selected according to random sam-
pling from definitions in the LDOCE. Assume 9 is the threshold and 0 is an estimator
of 0, and B is the bound on the error of estimation. The problem is to limit the error of
estimation below B with probability 1 — a. This can be stated as P(10 —01 &lt; B) = 1— a,
since the number of definitions is large enough to permit estimation of population
parameter O. Considering Central Limit Theory, the parameter o tends to have ap-
proximately a normal distribution. We will usually select B = 2o-o, and hence 1 — a
will be approximately 0.95 for normal distribution. To estimate d, a simple random
sample of 100 definitions (about 350 senses) is used. Thus, the estimate of threshold
is 0.12 for LinkSense. Similar estimation was done for the threshold used in TopSense.
Evaluation was done on a set of 20 polysemous words that have been used in
recent literature on WSD. These words focus on the more difficult cases of sense
ambiguity, as can be seen by the degree of ambiguity as recorded in the LDOCE.
These words have 5.3 senses on the average, as opposed to the average of 2.6 senses
for all words in the LDOCE.
The evaluation is based on the relevancy assessment by two human judges. The
Appendix gives a sense-by-sense rundown of all senses tested and evaluated. Table 13
summarizes the word-by-word applicability and precision of TopSense. Although not
all senses are clustered and not all clustered senses are correct, applicability and pre-
cision are rather high, which seems to indicate that the resulting sense division is
directly usable in WSD, and thus, eliminates the need for human intervention.
</bodyText>
<sectionHeader confidence="0.993223" genericHeader="method">
5. Discussion
</sectionHeader>
<bodyText confidence="0.99995075">
In this section, we thoroughly analyze the experimental results, in particular, the cases
for which TopSense fails. These cases reveal the strengths and limitations of TopSense
and hint at possible improvements to the algorithm. In addition, we also point out
several uses of the topical clusters.
</bodyText>
<subsectionHeader confidence="0.999617">
5.1 Failure of the TopSense Algorithm
</subsectionHeader>
<bodyText confidence="0.966084090909091">
Failure of TopSense can be attributed to a number of factors, including vagueness of
definitions, inappropriate definition lengths (too short or too long), metaphoric or
metonymic senses, and deictic references. Table 14 shows some examples of the failed
cases. For instance, the sense interest.1.n.3 (a readiness to give attention) is too vague and
short for correct clustering to occur. On the other hand, long definitions including too
many non-essential differentiae also give rise to erroneous clustering. We notice that
the definitions of such senses have been radically changed and made more specific
in the third edition of the LDOCE. The reason behind the changes may be that these
sense definitions are also difficult for humans to grasp.
Metonymic senses sometimes lead to problems for the proposed algorithms.
TopSense successfully puts start.1.n.3 (a piece of metal in this shape for wearing as a mark
</bodyText>
<footnote confidence="0.9773045">
3 It seems that precision may be lower if TopSense is run on the unlabeled entries, but we suspect the
difference is very small.
</footnote>
<page confidence="0.996547">
79
</page>
<tableCaption confidence="0.5858275">
Computational Linguistics Volume 24, Number 1
Table 13
</tableCaption>
<table confidence="0.99487768">
Evaluation of the TopSense algorithm.
Headword #of Definitions Labeling with Expanded Candidate Set
in LDOCE
Correct Incorrect Unknown Applicability Precision
bass 5 5 0 0 100% 100%
bow 5 5 0 0 100% 100%
cone 3 3 0 0 100% 100%
country 5 5 0 0 100% 100%
crane 2 2 0 0 100% 100%
duty 2 2 0 0 100% 100%
galley 4 4 0 0 100% 100%
interest 6 4 2 0 100% 67%
issue 8 3 2 3 63% 60%
mole 3 3 0 0 100% 100%
plant 6 5 1 0 100% 83%
position 10 9 1 0 100% 90%
sentence 2 2 0 0 100% 100%
slug 5 5 0 0 100% 100%
space 8 7 1 0 100% 88%
star 9 8 1 0 100% 90%
suit 6 5 1 0 100% 83%
table 7 6 1 0 100% 86%
tank 3 3 0 0 100% 100%
taste 6 5 0 1 83% 100%
total 105 91 10 4 96% 90%
</table>
<bodyText confidence="0.999857263157895">
of office, rank, honour, etc.) in the Dg class (Clothes and personal belongings). On the other
hand, the metonymic meaning, Nb (Chance) of another star sense (a heavenly body re-
garded as determining one&apos;s fate) comes out second to the &amp;quot;primary&amp;quot; sense, La (Heavenly
body). By considering cue phrases such as regarded as or as a mark of, we might be able
to handle metaphoric and metonymic senses more successfully.
Krovetz (1992) observes that the LDOCE indicates explicit sense shifts via the de-
ictic reference, which is a link to the previous sense created by such terms as this, these,
that, those, its, itself, such a, and such an. The author identifies many systematic sense
shifts indicated by such references including Substance/Product (lemon, tree or fruit),
Substance/Color (jade, amber), Object/Shape (pyramid), Animal/Food (chicken), Count-
noun/Mass-noun (blasphemy), Language/People (Spanish), Animal/Skin-fur (crocodile),
and Music/Dance (waltz). Such shifts indicated through a deictic reference are so per-
vasive in the MRD that they show up more than once in our small 20-word test
set. For instance, the LDOCE sense issue.1.n.2 (an example of this) indicates a Count-
noun/Mass-noun shift from its previous sense issue.1.n.1 (the act of coming out) through
the deictic reference of this. Since these specific patterns of definition are not taken into
consideration in TopSense, the algorithm often fails in such cases. Further work must
be undertaken to cope with direct and deictic references, so that such definitions can
be appropriately clustered.
</bodyText>
<subsectionHeader confidence="0.999615">
5.2 Clustered Definitions and Examples as a Knowledge Source for WSD
</subsectionHeader>
<bodyText confidence="0.999218333333333">
Many studies have shown that MRD definitions and example sentences are a good
knowledge source for WSD. As described in the introduction, Lesk (1986) shows that
defining words are especially effective for disambiguating senses strongly associated
</bodyText>
<page confidence="0.987264">
80
</page>
<note confidence="0.504345">
Chen and Chang Topical Clustering
</note>
<tableCaption confidence="0.789258">
Table 14
</tableCaption>
<figure confidence="0.822797">
Analysis of failure by error types.
Error Type TopSense Output Sense Definition
vague definition *Gd (communicating)
long definition *Ca (people)
metonynym * La (universe)
short, vague definition *Ge (communication)
*Bj (medicine)
— (unknown)
— (unknown)
deictic reference *Hb (object)
— (unknown)
</figure>
<figureCaption confidence="0.963767125">
interest - an activity, subject, etc., which one
gives time and attention to
table - also multiplication table; a list which
young children repeat to learn what number
results when a number from 1 to 12 is mul-
tiplied by any of the numbers from 1 to 12
star - a heavenly body regarded as determin-
ing one&apos;s fate
</figureCaption>
<bodyText confidence="0.901620333333333">
suit - a set (of armour)
interest - a readiness to give attention
issue - the act of coming out
issue - something which comes or is given
out
space - a quantity or bit of this for a particular
purpose
issue - an example of this
with specific collocations, such as cone in ice-cream cone and pine cone. Wilks et al.
(1990) call the defining words in the LDOCE definition semantic primitives (SP) and
suggest that a semantic network constructed on the strength of co-occurrence of SPs
in definitions can be useful for a variety of NLP tasks, ranging from WSD, to machine
translation, to message understanding. Along the same lines, Luk (1995) terms SP
the definition-based concept (DBC) and proposes using DBC co-occurrence (DBCC)
trained on a large corpus to disambiguate word senses. However, the effectiveness
of SPs or DBCs to represent a word sense and its indicative context is hampered
by ambiguity and data sparseness. For instance, earth, one of the SPs in bank.1.n.2
is ambiguous (either as the planet Earth or soil) thus possibly leading to problems in
WSD. Although these SPs are drawn from a small, controlled vocabulary in most
MRDs, nevertheless, it is difficult to find SPs of a polysemous sense overlapping the
SPs of its context. For instance, consider the problem of disambiguating the word bank
in the context of an LDOCE example, He sat down and rested on a mossy bank in the woods.
When working on the level of the SPs of an individual MRD sense, we are hard
pressed to find a match between the SPs of the intended sense:
SP(bank.1.n.2) = {earth, heap, field, garden, make, border, division}
and the SPs of its context:
SP(sit)
SP(rest)
SP(moss)
SP(wood.1.n.1)
SP(wood.1.n.2)
- {rest, position, upper, body, upright, support, bottom, chair, seat} ,
- {take, rest},
{small,flat, green, yellow,flowerless, plants, grow, thick, furry, wet,
soil, surface},
- {material, trunk, branch, tree, cut, dry, form, burn, paper, furniture} ,
- {place, tree, grow, small, forest} .
The clusters of MRD senses produced by TopSense give us an advantage in this
respect. By matching the context against the clustered semantic primitives (CSP) of the
</bodyText>
<page confidence="0.99563">
81
</page>
<note confidence="0.638257">
Computational Linguistics Volume 24, Number 1
</note>
<bodyText confidence="0.998595666666667">
related senses, we have a better chance of a match. For instance, the following CSPs
of the relevant bank senses contains more words, therefore are more likely to recur in
the SPs of contextual words:
</bodyText>
<equation confidence="0.9932895">
CSP(bank-Ld) = SP(bank.1.n.1) U SP(bank.1.n.2) U SP(bank.1.n.4)
U SP(bank.1.n.5)
</equation>
<bodyText confidence="0.999503428571429">
= {hand, side, river, stream, lake, earth, heap, field, make, border,
division, slope, bend, road, race-track, safe, car, go round,
sandbank}
If data sparseness still gets in the way, as in the case of this example, one can go
one step further and adopt a class-based approach. Under such an approach, the SPs of
the context are matched against the SPs of a class of senses related to the polysemous
sense in question. To this end, we can make use of the topical clusters of MRD senses
produced by TopSense. By taking the collective defining terms of all the senses in a
topical cluster, we obtain the virtual document of SPs described in Section 4.1. To cope
with the problem caused by ambiguous SPs, it is a good idea to weight terms according
to tf and idf, , as in the TopSense algorithm. Under such a class-based approach, we will
be matching the contextual information against the unweighted or weighted terms in
a class relevant to the intended sense. For instance, to resolve the sense of bank in the
above example to the Ld sense, we look for a match of contextual information with
</bodyText>
<equation confidence="0.8439695">
VLd •
VLd CSP(bank-Ld) U CSP(forest-Ld) U CSP(valley-Ld) U • -
</equation>
<bodyText confidence="0.9985763">
{land, side, river, stream, lake, earth, heap, field, make, border, division,
slope, bend, road, race-track, safe, car, go round, sandbank, large,
area, land, thick, cover, tree, bush, grow, wild, plant,
purpose,..
VLd {sea (65.81), land (55.39), mountain (46.74), water (44.76), river (36.71),
lake (27.88), earth (25.76), tree (21.87),..
Notice that for this example, the relevant VD is now large enough to overlap the
contextual information; the term tree appears in SP (wood.1.n.1) as well as the relevant
document VLd. Although the relevant VLd is very large, it contains mostly words that
are nevertheless consistently related to geography.
</bodyText>
<subsectionHeader confidence="0.999744">
5.3 Systematic Sense Shift
</subsectionHeader>
<bodyText confidence="0.999838846153846">
Ostler and Atkins (1991) contend that there is strong evidence to suggest that a large
part of word sense ambiguity is not arbitrary but follows regular patterns. Moreover,
gaps frequently arise in dictionaries and thesauri in specifying this kind of polysemy.
Encoding regularity of the extended usage of a sense makes it possible to resolve
word sense ambiguity for word entries that are underspecified in this respect. This so-
called virtual polysemy can be illustrated through some examples. For instance, many
verbs for moving and action, such as move and strike, can be used polysemously in the
sense of emotion. Chodorow, Byrd, and Heidom (1985) observe that many instances
of intersense relations can be found in W7 that are not idiosyncratic, but rather exist
among senses of many words. Those relations include Process/Result, Food/Plant, and
Container/Volume. Virtual polysemy and recurring intersense relations are closely
related to polymorphic senses that can support coercion in semantic typing under
Putstejovsky&apos;s (1991) theory of the generative lexicon.
</bodyText>
<page confidence="0.993637">
82
</page>
<note confidence="0.515678">
Chen and Chang Topical Clustering
</note>
<bodyText confidence="0.999829923076923">
Dolan (1994) maintains the position that intersense relations are mostly idiosyn-
cratical, thereby making it difficult to characterize them in a general way so as to
identify them. The author cites the example of two senses of to moult, one a bird be-
havior and the other an animal behavior, to stress that polysemy primarily reflects fine
distinctions that do not recur systematically throughout the English lexicon. However,
our experimental results indicate that (a) it is exactly senses with fine distinction that
are merged together and (b) there is a greater concentration of recurring intersense
relations emerging from condensed senses. For instance, the distinction between the
bird and animal behavior of moulting would be eliminated, since both are likely to be
clustered and labeled as Ha (Making things) by TopSense. Relations among senses in
the same topical clusters are mostly systematic. Many of those relations are reflected
in the cross-reference information in the LLOCE. For instance, the LLOCE lists the
following cross-references for the topic of Eb (Food):
</bodyText>
<figure confidence="0.877840428571429">
Ac (Animals/Mammals),
Ad (Birds),
Af (Fish and other water creatures),
Ah (Parts of animal),
Ai (Kinds of parts of plants),
Aj (Plant in general),
Jg (Shopkeepers and shops selling food).
</figure>
<bodyText confidence="0.975611727272727">
Most of those cross-references are systematic intersense relations similar to the
abovementioned Food/Plant relation. Indeed, words involved in such intersense rela-
tions are frequently underspecified. For instance, chicken is listed under both topic Eb
and topic Ad, while duck is listed under Ad but not Eb.
By characterizing some 200 cross-references in LLOCE, most systematic sense shifts
can be easily identified among the senses across topical clusters. The topical clusters of
MRD senses, coupled with the topical description of sense-shift knowledge, can sup-
port and realize automatic sense extension, as advocated in Putstejovsky and Bouillon
(1994), and prevent a proliferation of senses in the semantic lexicon. For instance, the
sense of duck in the Ad cluster can be coerced into an Eb sense, in some context, based
on the knowledge of a systematic sense shift from Ad (Birds) to Eb (Food).
</bodyText>
<sectionHeader confidence="0.806371" genericHeader="method">
6. Other Approaches
</sectionHeader>
<bodyText confidence="0.999953125">
Sanfilippo and Poznanski (1992) propose a so-called dictionary correlation kit (DCK)
in a dialogue-based environment for correlating word senses across a pair of MRDs
such as the LDOCE and the LLOCE. The approach taken in DCK is essentially a
heuristic one, based on a correlation in the headwords, grammar codes, definition, and
examples between the senses in LDOCE and LLOCE. The authors indicate that for the
heuristics to yield optimum results, the degree of overlap in the examples should be
weighted twice as heavily as all other factors. However, they do not elaborate on how
the comparisons are done, or on how effective the program is.
</bodyText>
<footnote confidence="0.79002525">
Dolan (1994) describes a heuristic approach to forming unlabeled clusters of closely
related senses in an MRD. The clustering program relies on LDOCE domain code,
grammar code, and 25 types of semantic relations extracted from definitions such as
Hypernym, Location, Manner, Purpose, Part0f, and Ingredient0f. Matching two senses
</footnote>
<page confidence="0.996824">
83
</page>
<note confidence="0.729493">
Computational Linguistics Volume 24, Number 1
</note>
<bodyText confidence="0.999821923076923">
involves comparing any values that have been identified for each of the semantic re-
lation types. The author reports that straightforwardly comparing the values of the
same semantic relation types, particularly the Hypern yin relation, for two senses would
be quite effective. In addition to such a comparison, a number of &amp;quot;scrambled&amp;quot; com-
parisons between values of different types of semantic relations are also helpful. For
instance, in comparing the two senses of coffee, the value &amp;quot;drink&amp;quot; in the sense, &amp;quot;the
coffee as a drink&amp;quot; is compared with that of the Ingredient Of relation in another sense,
&amp;quot;the powder as an ingredient of the drink.&amp;quot;
Yarowsky (1992) describes a WSD method and an implementation based on Ro-
get&apos;s Thesaurus and a very large corpus, the 10-million-word Grolier&apos;s Encyclopedia. He
suggests that the method can be applied to disambiguation and merging of MRD defi-
nitions as well, and gives the results of applying the method to the senses of the word
crane for the COBUILD and Collins dictionaries using Roget&apos;s categories as an example.
It is not known how the method fares for words other than crane. Contrary to our
approach, the method requires substantial data for training.
In most of the above-mentioned works, experimental results are reported only for
some senses of a few words. In this study, we have evaluated our method using all
senses for 20 words that have been studied in WSD literature. This evaluation provides
an overall picture of the expected success rate of the method when applied to all word
senses in the MRD. Direct comparison of methods is often difficult, but it is clear that,
as compared to other methods discussed above, our algorithm is very simple, requires
minimal preprocessing, and does not rely on information idiosyncratic to the MRD,
such as the LDOCE subject code or grammar code. Thus, the algorithm described
in this paper can be readily applied to other MRDs besides LDOCE. Although our
algorithm makes use of defining words in various semantic relations with the sense,
those relations need not be explicitly computed through an elaborated parsing and
extraction process.
Finally, it is interesting to compare our method with some aspects of the program
for induction of sense division of SchUtze (1992). As mentioned in the introduction,
the program uses distributional similarity of lexical co-occurrence to partition word
instances into clusters that are likely to be related to sense division. Drawing on the
work of latent semantic indexing in IR research, words and contexts are represented as
vectors in a multidimensional space. Regression techniques of singular value decom-
position are used to reduce the representation to a lower dimensional space. After that,
sense division is derived through unsupervised clustering of these word instances. Our
method, on the other hand, relies primarily on co-occurrence in an existing set of top-
ical clusters, the topics in LLOCE or Roget&apos;s. The sense in question is simply merged
to the nearest topical cluster. Low-cost distance calculation is done according to the
overlap between words in a definition and a topical cluster.
</bodyText>
<sectionHeader confidence="0.575323" genericHeader="evaluation">
7. Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999928777777778">
This paper presents the issues of WSD using machine-readable dictionaries. It de-
scribes simple but effective algorithms for disambiguating and clustering dictionary
senses to create a sense division for WSD. The proposed algorithms are effective for
specific linguistic reasons. Although word sense is an abstract concept that relies on
the subjective and subtle distinction of many factors, coarse word sense division can
be attributed primarily to the subject and topic. This is evident from the observation
that very topical genus and differentiae show up in dictionary definitions in rather
rigid patterns. Therefore, an MRD coupled with a thesaurus organized according to
subjects and topics is very effective for acquisition of sense division for WSD.
</bodyText>
<page confidence="0.993368">
84
</page>
<note confidence="0.642459">
Chen and Chang Topical Clustering
</note>
<bodyText confidence="0.9999208">
In a broader context, this paper presents an approach to automatic construction of
semantic lexicons through integration of lexicographic resources such as MRDs and
thesauri. As noted in Dolan (1994), it is possible to run a sense-clustering algorithm
on several MRDs to build an integrated lexical database with more complete coverage
of word senses. If TopSense is run on several bilingual MRDs, there is a potential
for creating an integrated multilingual lexicon enriched with thesaurus concepts as
language-neutral signs to support knowledge-based machine translation. A similar
idea has been put forward by Okumura and Hovy (1994).
The TopSense algorithm&apos;s performance could definitely be improved by handling
deictic, metonymic, and metaphoric sense definitions more appropriately Neverthe-
less, the algorithm already produces clustered MRD sense entries that not only are
exploitable as a workable sense division but also are likely to be an effective knowl-
edge source for many NLP tasks related to semantic processing, such as WSD. In
summary, this paper presents a functional core for automatic construction of the se-
mantic lexicon.
</bodyText>
<sectionHeader confidence="0.951762" genericHeader="conclusions">
Appendix
</sectionHeader>
<bodyText confidence="0.945296">
The following table shows the experimental results of running TopSense on the LDOCE
senses in a test set of 20 highly polysemous words.
bass
</bodyText>
<listItem confidence="0.98152504">
Topical Clustering Definition Sentences Applicability Precision
• Eb (food) • any of many kinds of fresh-water 100% 100%
• Gd or salt-water fish that have prickly
(communicating) skins and that can be eaten.
• Kb (music) • the lowest part in written music.
• Kb (music) • the lowest male singing voice.
• Kb (music) • a deep voice.
• = DOUBLE BASS.
bow
Topical Clustering Definition Sentences Applicability Precision
• Dg (clothes • a knot formed by doubling a line 100% 100%
and personal into 2 or more round or curved
belongings) pieces, and used for ornament in
• Hc (substances the hair, in tying shoes, etc.
and materials) • a piece of wood held in a curve
• Kb (music) by a tight string and used for
• Ma (moving) shooting arrows.
• Mf (shipping) • a long thin piece of wood with a
tight string fastened along it,
used for playing musical
• instruments that have strings.
• a bending forward of the upper
part of the body to show respect
or yielding.
• the forward part of a ship.
</listItem>
<page confidence="0.995003">
85
</page>
<figure confidence="0.492653">
Computational Linguistics Volume 24, Number 1
cone
Topical Clustering Definition Sentences Applicability Precision
</figure>
<listItem confidence="0.948068777777778">
• Aj (plants) • the fruit of a PINE or FIR, 100% 100%
• Hb (objects) consisting of several partly
• Hf (containers) separate seed-containing pieces
laid over each other, shaped
rather like this.
• a hollow or solid object
shaped like this.
• a solid object with a round base
and a point at the top.
country
Topical Clustering Definition Sentences Applicability Precision
• Ld (geography) • a nation or state with its land or 100% 100%
• Ce (organization) population.
• Ce (organization) • the nation or state of one&apos;s birth or
• Ld (geography) citizenship.
• Ld (geography) • the people of a nation or state.
• land with a special nature
or character
• the land outside cities or towns;
land used for farming or left unused.
crane
Topical Clustering Definition Sentences Applicability Precision
• Hd (equipments) • a machine for lifting and 100% 100%
• Ad (birds) moving heavy objects by means of a
very strong rope or wire fastened to a
movable arm (JIB).
• a type of large tall bird with
very long legs and neck, which
spends much time walking in water
catching fish in its very long beak.
duty
Topical Clustering Definition Sentences Applicability Precision
• Jf (commerce) • any of various types of tax. 100% 100%
• Jh (work) • what one must do either because
of one&apos;s job or because one thinks
it right
</listItem>
<page confidence="0.96614">
86
</page>
<figure confidence="0.359406333333333">
Chen and Chang Topical Clustering
galley
Topical Clustering Definition Sentences Applicability Precision
</figure>
<listItem confidence="0.925267955555556">
• Gd • a long flat container used by a 100% 100%
(communicating) printer to hold the letters (TYPE)
• Gd • which have been arranged for
(communicating) the first stage of printing.
• Mf (shipping) • =GALLEY PROOF.
• Mf (shipping) • a ship which was rowed along
by slaves.
a ship&apos;s kitchen.
interest
Topical Clustering Definition Sentences Applicability Precision
• *Bj (medicine) • a readiness to give attention. 100% 67%
• *Gd • an activity, subject, etc., which
(communicating) one gives time and attention to.
• Je (banking) • advantage, advancement, or favour
• Je (banking) (esp. in the phrs. in the interest of
• Jf (commerce) (something)/in someone&apos;s interest).
• Na (being, • money paid for the use of money.
becoming) • a share (in a company, business, etc.
• a quality of causing attention to
be given.
issue
Topical Clustering Definition Sentences Applicability Precision
• - (unknown)
• - (unknown)
• - (unknown)
• Ca (people)
• *Ck
(courts of law)
• *Cl (police,
crime)
• Gd
(communicating)
• Nf (causing)
• the act of coming out.
• something which comes or is
given out.
• an important point.
• old use and law children (esp.
in the phr. die without issue).
• the act of bringing out something
in a new form.
• an example of this.
• something, esp. something printed,
brought out again or in a new form.
• the result.
</listItem>
<figure confidence="0.4398034">
63% 60%
87
Computational Linguistics Volume 24, Number 1
mole
Topical Clustering Definition Sentences Applicability Precision
</figure>
<listItem confidence="0.97025325">
• Ac (animals) • a small, dark brown, slightly raised 100% 100%
• Hc (specific mark on a person&apos;s skin, usu. there
substances and since birth.
materials) • a type of small insect-eating animal
• Ld (geography) with very small eyes and soft dark
fur, which digs holes and passage under-
ground and makes its home in them.
• a stone wall of great strength built
out into the sea from the land as a
defense against the force of the waves,
or to act as a road.
plant
Topical Clustering Definition Sentences Applicability Precision
• Ai (plants) • a living thing that has leaves and 100% 83%
• Hd (equipment) roots, and grows usu. in earth, esp.
• Id (industry) the kind smaller than trees.
• *Md (vehicles) • a machine; apparatus.
• Ce (organization in • a factory ().
groups) • machinery.
• Cl (crime) • a person who is placed in a group
</listItem>
<bodyText confidence="0.7819336">
of people thought to be criminals in
order to discover facts about them.
• a thing, esp. stolen goods,
hidden on a person so that he
will seem guilty.
</bodyText>
<page confidence="0.953379">
88
</page>
<figure confidence="0.369425">
Chen and Chang Topical Clustering
position
Topical Clustering Definition Sentences Applicability Precision
</figure>
<listItem confidence="0.987808777777778">
• Me (places) • the place where someone or 100% 90%
• Me (places) something is or stands, esp. in
• Me (places) relation to other objects,
• Cn (fighting) places, etc.
• Ma (moving) • the place where someone or
• *Ca (people) something is (in the phr. in
• Ci (classifications) position).
• Cf (government) • the place where someone or
• Jh (work) something is supposed to be; the
• Ga (thinking) proper place.
• the place of advantage in a
struggle (in the phrs. manoeuvre/
jockey for position).
• the way or manner in which
someone or something is placed or
moves, stands, sits, etc.
• a condition or state, esp. in relation
to that of someone or something else.
• a particular place or rank in a group.
• high rank in society, government, or
business.
• a job; employment.
• an opinion or judgment on a matter.
sentence
Topical Clustering Definition Sentences Applicability Precision
• Ck (courts of law) • a punishment for a criminal found 100% 100%
• Gd guilty in court.
</listItem>
<bodyText confidence="0.95646">
(communicating) • a group of words that forms a
statement, command, EXCLAMATION,
or question, usu. contains a subject
and a verb, and (in writing) begins
with a capital letter and ends with
one of the marks &amp;quot;.!?&amp;quot;
</bodyText>
<page confidence="0.99325">
89
</page>
<figure confidence="0.7365182">
Computational Linguistics Volume 24, Number 1
slug
Topical Clustering Definition Sentences Applicability Precision
• Ab (living • any of several types of small 100% 100%
creatures) limbless plant-eating creature, related
</figure>
<listItem confidence="0.920738290322581">
• Gd to the SNAIL but with no shell,
(communicating) that often do damage to gardens.
• Hc (specific • a machine-made piece of metal
substances) with a row of letters along the edge
• Hd (equipment) for printing.
• Hh (weapons) • a machine-made piece of metal
with a row of letters along the edge
for printing.
• a coin-shaped object unlawfully
put into a machine in place
of a coin.
• a bullet.
space
Topical Clustering Definition Sentences Applicability Precision
• Jc (measurement) • something limited and measurable 100% 88%
• *Hb (objects) in length, width, or depth and regarded
• La (universe) as not filled up; distance, area, or
• La (universe) VOLUME (3); room.
• Ld (geography) • a quantity or bit of this for a
• Le (time) particular purpose.
• Gd • that which surrounds all objects and
(communicating) continues outward in all directions.
• Gd • what is outside the earth&apos;s air; where
(communicating) other heavenly bodies move.
• land not built on (esp. in the phr. open
space).
• a period of time.
• an area or distance left between
written or printed words, lines etc.
• the width of a letter on a
TYPEWRITER.
</listItem>
<page confidence="0.799521">
90
</page>
<figure confidence="0.489566666666667">
Chen and Chang Topical Clustering
star
Topical Clustering Definition Sentences Applicability Precision
</figure>
<listItem confidence="0.957931666666667">
• Dg (personal • a piece of metal in this shape for 100% 90%
belongings) wearing as a mark of office, rank,
• Jb (mathematics) honour, etc.
• Kd (drama) • a 5- or more-pointed figure.
• La (universe) • a famous or very skillful performer.
• La (universe) • STARS.
• La (universe) • a brightly-burning heavenly body
• *La (universe) of great size, such as the sun but esp.
• La (universe) one very far away.
• Nb (possibility) • any heavenly body (such as a
PLANET) that appears as a bright
point in the sky.
• a heavenly body regarded as
determining one&apos;s fate.
• a sign used with numbers from usu.
1 to 5 in various systems, and in the
imagination, to judge quality.
• one&apos;s success or fame or chance of
getting it.
suit
Topical Clustering Definition Sentences Applicability Precision
• Dg (clothes)
• a set of outer clothes which
match, usu. including a short
coat (JACKET) with trousers or
skirt.
• a garment or set of garments
for a special purpose.
• a set (of armour) (in the phrs.
suit of armour/mail).
• one of the 4 sets of cards used
in games.
• fml a request 0.
• old use the act of asking a
woman to marry (esp. in the
phrs. plead/press one&apos;s suit).
</listItem>
<figure confidence="0.9271045">
100% 83%
• Dg (clothes)
• *Ge
(communication)
• Kf (indoor games
• Gc
(communicating)
• Cb (courting)
91
Computational Linguistics Volume 24, Number 1
tank
Topical Clustering Definition Sentences Applicability Precision
</figure>
<listItem confidence="0.978322208333333">
• Hf (containers) • a large container for storing 100% 100%
• Md(vehicles) liquid or gas.
• Ld(geography) • an enclosed heavily armed
armoured vehicle that moves on
2 endless metal belts.
• esp. Ind &amp; PakE a large man-
made pool for storing water.
table
Topical Clustering Definition Sentences Applicability Precision
• Hb (objects) • a piece of furniture with a flat top 100% 86%
• Df (furniture) supported by one or more upright
• Kf (indoor games) legs.
• Ea (food) • made to be placed and used on
• Ca (people) such a piece of furniture.
• Gd • such a piece of furniture specially
(communicating) made for the playing of various
• *Ca (people) games.
• the food served at a meal.
• the people sitting at a table.
• a printed or written collection
of figures, facts, or information
arranged in orderly rows across and
down the page.
• also multiplication table a list which
</listItem>
<bodyText confidence="0.95879625">
young children repeat to learn what
number results when a number from
1 to 12 is multiplied by any of the
numbers from 1 to 12.
</bodyText>
<page confidence="0.823595">
92
</page>
<figure confidence="0.510383666666667">
Chen and Chang Topical Clustering
taste
Topical Clustering Definition Sentences Applicability Precision
</figure>
<listItem confidence="0.995055">
• Bg (bodily states) • an experience. 83% 100%
• Ea (food generally) • a small quantity of food or drink.
• Eb (food) • the special sense by which a person
or animal knows one food from
another by its sweet, bitter,
salty, etc.
• Eb (food) • the sensation that is produced
when food or drink is put in the
mouth and that makes it different
from other foods or drinks by its
salty, sweet, bitter, etc.
• Kb(music) • the ability to enjoy and judge
beauty, style, art, music, etc.;
ability to choose and use the best
manners, behaviour, fashions, etc.
• - (unknown) • a personal liking for something.
</listItem>
<sectionHeader confidence="0.981845" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999818222222222">
This work is partially supported by ROC
NSC grants 84-2213-E-007-023 and NSC
85-2213-E-007-042. We are grateful to Betty
Teng and Nora Liu from Longman Asia
Limited for the permission to use their
lexicographical resources for research
purposes. Finally, we would like to thank
the anonymous reviewers for many
constructive and insightful suggestions.
</bodyText>
<sectionHeader confidence="0.992015" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999326690909091">
Ageno, A., I. Castellon, M. A. Marti,
G. Rigau, F. Ribas, H. Rodriguez, M.
Taule and E Verdejo. 1992. SEISD: An
environment for extraction of semantic
information from on-line dictionaries. In
Proceedings of the 3rd Conference on Applied
Natural Language Processing, pages
253-254, Trento, Italy.
Ahlswede, Thomas and Martha Evens. 1988.
Parsing vs. text processing in the analysis
of dictionary definitions. In Proceedings of
the 26th Annual Meeting, pages 217-224.
Association for Computational
Linguistics.
Alshawi, H. 1987. Processing dictionary
definitions with phrasal pattern
hierarchies. American Journal of
Computational Linguistics, 13(3):195-202.
Alshawi, H., B. Boguraev, and D. Carter.
1989. Placing the dictionary on-line. In
B. Boguraev and Briscoe, editors,
Computational Lexicography for Natural
Language Processing, Longman, London,
pages 41-63.
Amsler, Robert A. 1984a. Machine-readable
dictionaries. Annual Review of Information
Science and Technology, 19:161-209.
Amsler, Robert A. 1984b. Lexical knowledge
bases, Panel session on machine-readable
dictionaries. In Proceedings of the Tenth
International Congress on Computational
Linguistics, pages 458-459, Stanford, CA.
Amsler, Robert A. 1987. Words and words.
In Proceedings of the Third Workshop on
Theoretical Issues in Natural Language
Processing, pages 7-9, New Mexico State
University at Las Cruces, NM.
Brown, P. E, S. A. Della Pietra, V. J. Della
Pietra, and R. L. Mercer. 1991. Word sense
disambiguation using statistical methods.
In Proceedings of the 29th Annual Meeting,
pages 264-270. Association for
Computational Linguistics.
Bruce, Rebecca and Janyce Wiebe. 1995.
Word sense disambiguation using
decomposable models. In Proceedings of the
33rd Annual Meeting, pages 139-145.
Association for Computational
Linguistics.
Chang, Jason S., J. N. Chen, H. H. Sheng,
and S. J. Ker. 1996. Combining machine
readable lexical resources and bilingual
corpora for broad word sense
disambiguation. In Proceedings of the
Second Conference of the Association for
</reference>
<page confidence="0.965561">
93
</page>
<note confidence="0.352108">
Computational Linguistics Volume 24, Number 1
</note>
<reference confidence="0.999754196721312">
Machine Translation, pages 115-124,
Montreal, Quebec, Canada.
Chen, J. N. and Jason S. Chang. 1994.
Towards generality and modularity in
statistical word sense disambiguation. In
Proceedings of the 8th Asian Conference on
Language, Information and Computation,
pages 45-48.
Chodorow, Martin S., Roy J. Byrd and
George E. Heidom. 1985. Extracting
semantic hierarchies from a large on-line
dictionary. In Proceedings of the 23rd Annual
Meeting, pages 299-304. Association for
Computational Linguistics.
Church, K. W. 1988. A stochastic parts
program and noun phrase parser for
unrestricted text. In Proceedings of the 2nd
Conference on Applied Natural Language
Processing, pages 136-143, Austin, TX.
Copestake, A. 1990. An approach to
building the hierarchical element of a
lexical knowledge base from a machine
readable dictionary. In Proceedings of the
First International Workshop on Inheritance in
Natural Language Processing, pages 19-29,
Tilburg, The Netherlands.
Cowie, Jim, Joe Guthrie and Louise Guthrie.
1992. Lexical disambiguation using
simulated annealing. In Proceedings of the
14th International Conference on
Computational Linguistics, pages 359-365.
Dagan, Ido and Alon Itai. 1994. Word sense
disambiguation using a second language
monolingual corpus. Computational
Linguistics, 20(4):563-596.
Dagan, Ido, Alon Itai, and Ulrike Schwall.
1991. Two languages are more informative
than one. In Proceedings of the 29th Annual
Meeting, pages 130-137. Association for
Computational Linguistics.
Dolan, William B. 1994. Word sense
disambiguation: Clustering related senses.
In Proceedings of the 15th International
Conference on Computational Linguistics,
pages 712-716.
Gale, W., K. W. Church, and D. Yarowsky.
1992. Using bilingual materials to develop
word sense disambiguation methods. In
Proceedings of the 4th International Conference
on Theoretical and Methodological Issues in
Machine Translation, pages 101-112.
Guthrie, Louise, Brian M. Slator, Yorick
Wilks, and Rebecca Bruce. 1990. Is there
contents in empty heads? In Proceedings of
the 13th International Conference on
Computational Linguistics, pages 138-143.
Jenson, Karen and Jean Louis Binot. 1987.
Disambiguating prepositional phrase
attachments by using on-line dictionary
definitions. Computational Linguistics,
13(3-4):251-260.
Klavans, J. L., M. S. Chodorow, and N.
Wacholder. 1990. From dictionary to
knowledge via taxonomy. In Proceedings of
the Sixth Conference of the University of
Waterloo Centre for the New Oxford English
Dictionary and Text Research: Electronic Text
Research, University of Waterloo,
Waterloo, Canada.
Krovetz, Robert. 1992. Sense-linking in a
machine readable dictionary. In
Proceedings of the 30th Annual Meeting,
pages 330-332. Association for
Computational Linguistics.
Krovetz, R. and W. B. Croft. 1992. Lexical
ambiguity and information retrieval.
ACM Transaction on Information Systems,
pages 115-141.
Lesk, Michael E. 1986. Automatic sense
disambiguation using machine-readable
dictionaries: How to tell a pine cone from
an ice-cream cone. In Proceedings of the
ACM SIGDOC Conference, Toronto,
Ontario, pages 24-26.
Proctor, P., editor. 1978. Longman Dictionary
of Contemporary English. Longman Group,
London.
Luk, Alpha K. 1995. Statistical sense
disambiguation with relatively small
corpora using dictionary definitions. In
Proceedings of the 33rd Annual Meeting,
pages 181-188. Association for
Computational Linguistics.
McArthur, Tom. 1992. Longman Lexicon of
Contemporary English. Longman Group
(Far East) Ltd., Hong Kong.
McRoy, S. 1992. Using multiple knowledge
sources for word sense discrimination.
Computational Linguistics, 18(1):1-30.
Miller, G. A., R. Beckwith, C. Fellbaum,
D. Gross, and K. Miller. 1993. Introduction
to WordNet: An on-line lexical database.
CSL 43, Cognitive Science Laboratory,
Princeton University, Princeton, NJ.
Montemagni, S. and L. Vanderwende. 1992.
Structural pattern vs. string pattern for
extracting semantic information from
dictionaries. In Proceedings of the Fifteenth
International Conference on Computational
Linguistics, pages 546-552.
Ng, Hwee Tou and Hian Beng Lee. 1996.
Integrating multiple knowledge sources
to disambiguate word sense: An
examplar-based approach. In Proceedings
of the 34th Annual Meeting, pages 40-47,
Santa Cruz, CA. Association for
Computational Linguistics.
Okumura, A. and Eduard Hovy. 1994.
Lexicon-to-ontology concept association
using a bilingual dictionary. In Proceedings
of the First Conference of the Association for
Machine Translation in the Americans,
</reference>
<page confidence="0.977918">
94
</page>
<reference confidence="0.991701581081081">
Chen and Chang Topical Clustering
pages 177-184. Columbia, MD.
Ostler, Nicholas and B. T. S. Atkins. 1991.
Predictable meaning shift: Some linguistic
properties of lexical implication rules. In
Proceedings of the 1991 ACL Workshop on
Lexical Semantics and Knowledge
Representation, pages 76-87.
Putstejovsky, James. 1991. The generative
lexicon. Computational Linguistics,
(17)4:409-441.
Putstejovsky, James and Pierrette Bouillon.
1994. On the proper role of coercion in
semantic typing. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 706-711.
Ravin, Yael. 1990. Disambiguating and
interpreting verb definitions. In
Proceedings of the 28th Annual Meeting,
pages 260-267. Association for
Computational Linguistics.
Roget&apos;s Thesaurus of English Words and Phrases.
1987. Longman Group UK Limited,
London.
Sanfilippo, A. and V. Poznanski. 1992. The
acquisition of lexical knowledge from
combined machine-readable dictionary
sources. In Proceedings of the 3rd Conference
on Applied Natural Language Processing
(ANLP-92), pages 80-87, Trento, Italy.
Schutze, Hinrich. 1992. Word sense
disambiguation with sublexical
representations. In Proceedings of the 1992
AAAI Workshop on Statistically-based
Natural Language Programming Techniques,
pages 100-104.
Vanderwende, L. 1994. Interpretation of
noun sequences. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 454-460.
Vossen, P., W. Meijs, and M. den Broeder.
1989. Meaning and structure in dictionary
definitions. In Computational Lexicography
for Natural Language Processing. Branimir
Boguraev and Ted Briscoe, editors,
Longman Group UK Limited, London,
pages 171-190.
Webster&apos;s Seventh New Collegiate Dictionary.
1967. C. and C. Merriam Company,
Springfield, MA.
Wilks, Y. A., D. C. Fass, C. Ming Guo,
J. E. McDonald, T. Plate, and B. M. Slator.
1990. Providing tractable dictionary tools.
Machine Translation, 5:99-154.
Witten, Ian H., Alistair Moffat, and Timothy
C. Bell. 1994. Managing Gigabytes. Van
Nostrand Reinhold, New York.
Yarowsky, David. 1992. Word-sense
disambiguation using statistical models of
Roget&apos;s categories trained on large
corpora. In Proceedings of the 14th
International Conference on Computational
Linguistics, pages 454-460, Nantes, France.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting, pages 189-196. Association for
Computational Linguistics.
Zernik, Uri. 1992. Trainl vs. Train2: Tagging
word senses in corpus. In U. Zemik,
editor, Lexical Acquisition: Exploiting
On-line Resources to Build a Lexicon.
Lawrence Erlbaum Associates,
pages 91-112.
</reference>
<page confidence="0.99906">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994713">Topical Clustering of MRD Senses Based on Information Retrieval Techniques</title>
<author confidence="0.999626">Jen Nan Chen Jason S Chang</author>
<affiliation confidence="0.996753">National Tsing Hua University National Tsing Hua University</affiliation>
<abstract confidence="0.979724261146497">This paper describes a heuristic approach capable of automatically clustering senses in a machinereadable dictionary (MRD). Including these clusters in the MRD-based lexical database offers several positive benefits for word sense disambiguation (WSD). First, the clusters can be used as a coarser sense division, so unnecessarily fine sense distinction can be avoided. The clustered entries in the MRD can also be used as materials for supervised training to develop a WSD system. Furthermore, if the algorithm is run on several MRDs, the clusters also provide a means of linking different senses across multiple MRDs to create an integrated lexical database. An implementation of the method for clustering definition sentences in the Longman Dictionary of Contemporary English (LDOCE) is described. To this end, the topical word lists and topical cross-references in the Longman Lexicon of Contemporary English (LLOCE) are used. Nearly half of the senses in the LDOCE can be linked precisely to a relevant LLOCE topic using a simple heuristic. With the definitions of senses linked to the same topic viewed as a document, topical clustering of the MRD senses bears a striking resemblance to retrieval of relevant documents for a given query in information retrieval (IR) research. Relatively well-established IR techniques of weighting terms and ranking document relevancy are applied to find the topical clusters that are most relevant to the definition of each MRD sense. Finally, we describe an implemented version of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations. Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic, register, dialect, collocation, part of speech, and valency (McRoy 1992). Various approaches to word sense division have been proposed in the literature on WSD, including (1) sense numbers in every-day dictionaries (Lesk 1986; Cowie, Guthrie, and Guthrie 1992), (2) automatic or hand-crafted clusters of dictionary senses (Dolan 1994; Bruce and Wiebe 1995; Luk * Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC. E-mail: dr818314@cs.nthu.edu.tw; jschang@cs.nthu.edu.tw. C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 thesaurus categories (Yarowsky 1992; Chen and Chang 1994), (4) translation in another language (Gale, Church, and Yarowsky 1992; Dagan, Itai, and Schwa11 1991; Dagan and Itai 1994), (5) automatically induced clusters with sublexical representation (Schtitze 1992), and (6) hand-crafted lexicons (McRoy 1992). This paper is motivated by the observation that directly using dictionary senses for sense division offers several advantages. Sense distinction according to a dictionary readily available from machine-readable dictionaries (MRDs) such as the of Contemporary English (Proctor 1978). A dictionary such as the LDOCE has broad coverage of word senses, useful for WSD. Furthermore, indicative words and concepts for each sense are directly available in numbered definitions and examples. Lesk (1986) describes the first MRD-based WSD method that relies on the extent of overlap between words in a dictionary definition and words in the local context of the word to be disambiguated. The author reports that WSD performance ranges from 50% to 70% and his method works well for senses strongly associated specific collocations, such as cone cone. Unfortunately, using MRDs as the knowledge source for sense division and disambiguation leads to some problems. Zernik (1992) notes that the dictionary dichotomy of senses is inadequate for WSD, because it is defined along grammatical, not semantic, lines. Furthermore, as pointed out in Dolan (1994), the sense division in an MRD is frequently too fine-grained for the purpose of WSD. A WSD system based on dictionary senses often faces unnecessary and difficult &amp;quot;forced-choices.&amp;quot; Dolan proposes a heuristic algorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the proposed algorithm was only described in a few examples and was not developed further. Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk 1995; Yarowsky 1995) still resort to human intervention to identify and group closely in an MRD. Using thesaurus categories directly as a coarse sense division may seem to be a vialternative (Yarowsky 1995). However, typical thesauri, such as Thesaurus (1987), suffer sense gaps and, occasionally, are too fine-grained. Yarowsky (1992) rethat there are uses not listed in 3 of 12 nouns in his WSD study, while uses which a native speaker might consider as a single sense are often encoded in As an alternative approach to word sense division, this paper presents an algorithm capable of automatically clustering senses in an MRD based on topical informain a thesaurus. We refer to the algorithm as clustering of current implementation of the topical information in the of Contemporary English (McArthur 1992) to cluster LDOCE senses. The method makes use of none of the idiosyncratic information in either the LLOCE or the Therefore, the is quite general and is expected to produce results for other MRDs and thesauri. tested on 20 words extensively investigated in recent WSD literature (Schtitze 1992; Yarowsky 1992; Luk 1995). According to the experimental results, the automatically derived topical clusters can be used to good effect without any human intervention as a coarse sense division for WSD. The rest of the paper is organized as follows. Section 2 starts out with a description of the MRDs and thesauri used in the computational lexicography and WSD literature, followed by some observations to justify the topic-based approach to word sense Section 3 describes the for linking senses between an and a thesaurus. Section 4 shows how the based on the IR model may be used to cluster the senses in an MRD. Examples are given in both 62 Chen and Chang Topical Clustering Sections 3 and 4 to illustrate how the algorithms work. Section 4 also describes an implementation of the algorithms for the LDOCE and the LLOCE and reports the evaluation results for both algorithms based on a 20-word test set. Section 5 analyzes the experimental results to demonstrate the strengths and limitations of the method. implication of WSD and other issues related to lexical semantics are also touched upon. Section 6 compares the proposed method with other approaches in the computational linguistics literature. Finally, conclusions are made and directions for further research are pointed out in Section 7. Word Senses in Dictionaries and Thesauri In this section, we look at two knowledge sources of word sense division, which are currently widely available, namely, the dictionary and the thesaurus. A good-sized dictionary usually has a large vocabulary and broad coverage of word senses, both of which are useful for WSD. However, a dictionary&apos;s division of senses for a given word is often too fine for the task of WSD. On the other hand, a thesaurus organizes word senses into a fixed set of coarse semantic categories, making it more appropriate for WSD. However, thesauri tend to have a smaller vocabulary and a narrower coverage of word senses. To get the best of both worlds of dictionary and thesaurus, we propose to cluster MRD definitions to yield a broad-coverage sense division with the granularity of a thesaurus. Therefore, a short description of MRDs and thesauri is in order. Senses in an MRD Interest in MRD-based research has increased over the years; in particular, the LDOCE Seventh New Collegiate Dictionary (1967) have drawn much attention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of information listed in a dictionary entry, including part of speech, subcategory, examples, collocations, and typical arguments, which is potentially useful for WSD. In this regard, the LDOCE is particularly appropriate since it uses a reduced, controlled vocabulary of some 2,000 words to define over 60,000 word senses representing a comprehensive vocabulary and broad coverage of word senses. It is arguable that the dictionary division of senses for a given word is too finegrained, thus inadequate for WSD. For instance, it might not always be necessary or to distinguish between two LDOCE senses bank) in Table 1. Hence, dictionary senses can be used to good effect for WSD only if such closely related senses are merged and treated as one. There is more than one way to merge dictionary senses. In the following sections, we describe one such approach, under which MRD senses are merged according to the sense granularity of a typical thesaurus. Coarse Senses in Thesauri: WordNet, LLOCE One of the most potentially valuable aspects of the thesaurus, as a knowledge source for word sense division, is the organization of word senses into a limited number of 63 Computational Linguistics Volume 24, Number 1 Table 1 sense entries for the LDOCE. Sense Entries land along the side of a river, lake, etc. earth which is heaped up in a field or garden, often making a border or division. a mass of snow, clouds, mud, etc. a slope made at bends in a road or race-track, so that they are safer for cars to go round. = SANDBANK. (a high underwater bank of sand in a river, harbour, etc.) (of a car or aircraft) to move with one side higher than the other, esp. when making a turn a row, esp. of OARs in an ancient boat or KEYs on a TYPEWRITER. a place in which money is kept and paid out on demand, and where related activities go on. (usu. in comb.) a place where something is held ready for use, esp. ORGANIC products of human origin for medical use. (a person who keeps) a supply of money or pieces for payment or use in a game of chance. to put or keep (money) in a bank.</abstract>
<note confidence="0.802807652173913">[esp. with] to keep one&apos;s money (esp. in the stated bank) Sense ID bank.1.n.1 bank.1.n.2 bank.1.n.3 bank.l.n.4 bank.l.n.5 bank.2.v.1 bank.3.n.1 bank.4.n.1 bank.4.n.2 bank.4.n.3 bank.5.v.1 bank.5.v.2 Note: Sense ID = Root + Homonym No. + Part-of-speech + Sense No. Table 2 classes and categories. Class Categories Gloss for Classes A 1-182 Abstract relations • 183-318 Space • 319-446 Matter D 447-594 Intellect: the exercise of the mind Volition: the exercise of the will Emotion, religion and morality • 595-816</note>
<abstract confidence="0.965475888888889">817-990 semantic categories. We describe the on-line thesauri, WordNet (Miller al. 1993), Thesaurus, LLOCE, which have been used as word sense divisions in the computational linguistics literature. WordNet is organized as a set of hierarchical, conceptual taxonomies of nouns, verbs, adjectives, and adverbs called synsets are too fine-grained from the WSD perspective; WordNet contains 24,825 noun synsets for 32,264 distinct nouns with a total of 43,136 senses in its noun taxonomy alone. It would be difficult to acquire WSD knowledge for making such fine distinctions even from a substantial body of training materials. Thesaurus words in a three-layer hierarchy and organizes over 30,000 distinct words into some 1,000 categories on the bottom layer. These categories are divided into 39 middle-layer sections that are further organized as 6 top-layer classes. Each category is given a three-digit reference code. To make the hierarchical explicit, an uppercase letter from A to added to the reference code to denote the top-layer class for each category, as indicated in Table 2. Similarly, the middle is denoted with a lowercase reference letter. The sections related to class are shown in Table 3. Therefore, the reference code for each category is denoted by an uppercase class letter, a lowercase section letter, and a three-digit category number.</abstract>
<note confidence="0.8159891">64 Chen and Chang Topical Clustering related to the in Class Sections Gloss of Section Examples B 183-194 a Space in general surface, heavens, room, kitchen, abode B 195-242 b Dimensions weight, proximity, clothes, wear, hall B 243-264 c Form idea, distortion, flat, plug, yawn, subway B 265-318 d Motion rocket, transposition, carrier, entrance Roget&apos;s Figure 1</note>
<abstract confidence="0.937753511363637">scheme. instance, the word under Category be prefixed an letter denote the class a lowercase letter denote the reference code replaced with Bb209. Figure 1 shows the for the word and a lesser degree present word senses that are too finegrained for WSD. Often, uses that a native speaker might consider as a single sense are in several or WordNet synsets. For instance, a single LDOCE in Table 1 corresponds to two WordNet synsets institution building two the two categories a concept treated as one word sense, bank.1.n.1 in the LDOCE. Table 4 provides further details. The LLOCE is a hierarchical thesaurus that organizes word senses primarily according to subject matter. The LLOCE contains over 23,000 different senses for some 15,000 distinct words. The coarser senses in LLOCE are organized into approximately 2,500 topical word sets. These sets are divided into 129 topics and these topics are further organized as fourteen subjects. The subjects are denoted with alphabetical refletters from Table 5). Thus the LLOCE subject, topic, and topical set constitutes a three-level hierarchy, in which each subject contains 7 to 12 topics and each topic contains 10 to 50 sets of related words. Table 6 displays the topics related Land Treasurer 411} . bank .. ... bank ... Abstract relatio atter Emotion, religion d morality Intellect olition 0 Inorganic Prospective Possessive matter volition relations a class section 0 Dimen Laterality Edge Obliquit Suppor Height . bank .. ) category . bank .. 65 Computational Linguistics Volume 24, Number 1 Table 4 of MRD and thesaurus treatment of building 799 (Treasurer)/784 (Lending) Je104 (Finance) 632 799 Je106 money/Deposit 799 (Treasurer) Je106 (Deposit) Table 5 LLOCE subjects and their reference letters. Subject Set Gloss for Subjects and living things B its function and welfare and family D houses, home, clothes, belongings, personal care • drink, and farming • emotions, attitudes, and sensations G communication, language, and grammar H materials, objects, and equipment science/technology, industry/education measurement, money, and commerce K sports and games • and time location, travel, and transportation N and abstract terms subject topical set is given a three-digit reference code; however, this code does not explicitly reflect the topic. To make use of the information related to a topic, we have designated a lowercase letter to each topic. Therefore, each set is denoted by an uppercase &amp;quot;subject&amp;quot; letter, a lowercase &amp;quot;topic&amp;quot; letter, and a &amp;quot;topical set&amp;quot; number. For instance, the word under the will be given an additional reference letter denote the topic code replaced with LLOCE also provides cross-references between sets and topics to indicate various intersense relations not captured within same topic. For instance, topic a cross-reference to topic 2 shows LLOCE&apos;s topical classification and cross-references related to word LLOCE, and, to a lesser degree, based on coarse, topical semantic classes, making them more appropriate for WSD than the finer-grained WordNet The 129 topics in the LLOCE or 990 categories in to be suffi-</abstract>
<affiliation confidence="0.555978">WordNet Sense</affiliation>
<address confidence="0.8434134">234 (Edge)/344 (Land) 234 (Edge) 239 (Laterality) 344 (Land) laterally 239 (Laterality)</address>
<affiliation confidence="0.420112">LLOCE Sense</affiliation>
<note confidence="0.798040875">Ld099 (River bank) Ld099 (River bank) Nj295 (To bend) 66 Chen and Chang Topical Clustering Table 6 Topics related to subject L in LLOCE. Subject Range Gloss</note>
<abstract confidence="0.86511775">L 001-019 a The universe L 020-039 Light and color L 040-079 Weather and temperature L 080-129 Geography L 130-169 Time generally L 170-199 Beginning and ending L 200-219 Old, new, and young L 220-249 Period/Measure of time L 250-273 Function words (time) Examples sun, moon, star, left, right, etc. light, dark, ray, color, white, black, etc.</abstract>
<keyword confidence="0.792330142857143">weather, sky, rain, snow, rain, ice, etc. stream, sea, lake, flood, to flow, etc. time, history, frequent, permanent, etc. start, stop, late, last, etc. ancient, modern, future, age, etc. day, night, second, minute, etc. now, soon, always, ever, after, etc.</keyword>
<title confidence="0.518868">LLOCE People Buildin Materia Money Space Location</title>
<note confidence="0.922139">Subject 0 0</note>
<title confidence="0.602402">Organization Government Owning Material Banking Geograph Travel Place</title>
<abstract confidence="0.980109016638936">Figure 2 LLOCE&apos;s topical organization of word sense. cient for representing the distinction we would want to make for the task of WSD. been used as the sense division in two recent WSD works (Yarowsky 1992; Luk 1995) more or less as is, except for a small number of senses added to fill gaps. We contend that a sense division based on the LLOCE topics will offer more or less the same kind of granularity, suitable for WSD. For instance, in Yarowsky (1992), the of divided into three which roughly correspond to LDOCE labeled with LLOCE topics. In the same study, six catare sufficient to distinguish the senses of six categories correspond to five relevant LLOCE topics. Table 7 provides further details. 2.3 Combining Word Sense Information from an MRD and a Thesaurus It should be clear by now that combining a dictionary and a thesaurus leads to a broad-coverage sense division with a suitable granularity for WSD. The obvious way combine the two would be to disambiguate and link a sense definition a the dictionary to an entry relevant to the thesaurus. This amounts to a special case of WSD with respect to thesaurus senses. There is no simple solution ... bank .. 67 Computational Linguistics Volume 24, Number 1 Table 7 LLOCE classifiers for two sample words. representation) LLOCE (two-layer representation) belongings) 365 substances) to the general WSD problem for unrestricted text, but we will show that this special case of disambiguating MRD definitions is significantly easier, for several reasons. First, the words used in a definition sentence are limited primarily to a small set; in the case of the LDOCE, the controlled vocabulary consists of some 2,000 words. instance, in the first five LDOCE senses of in Table 1, all defining are in the controlled vocabulary, except for the word in capital letters. Obtaining WSD information for this small set of words obviously is much easier than it would be for a large, open set. Second, dictionary definitions adhere to rather rigid patterns under which only words with predictable semantic relations show up. A dictionary definition, in general, with a term is, conceptual ancestor of the sense), followed by a set are words semantically related to the sense to provide the specifics. The semantic relations between the sense, the genus, and differentiae are reflected in are termed functional, in McRoy (1992). The semantic relations and clusters have been shown to be very effective knowledge sources for such NLP tasks as WSD (McRoy 1992) and interpretation of noun sequences 1994). For instance, in the first four definitions of Table 1, the terms earth, mass, categorically related to the respective On the other hand, the differentiae lake, field, garden, bend, road, racea relation with differentiae, cloud, related functionally to the Third, for the most part these relations are captured implicitly in a typical the- The LLOCE and contain information on the relations in form of word lists under a topic (category) or to other Therefore, an MRD sense definition can be effectively disambiguated based on the word lists and cross-references in a thesaurus. A simple heuristic relying on the similarity between a sense&apos;s defining keywords and thesaurus word lists suffices to link an MRD sense to its relevant sense in the thesaurus. For instance, the differentiae side, river, lake) sufficiently similar to the word list of Ld-topic warrant the link between LDOCE sense LLOCE sense topics and cross-references of LLOCE in general capture the relation; therefore, a sense definition is often disambiguated through the genus. Thus, the task of linking MRD and thesaurus senses is closely related to the extraction and 68 Chen and Chang Topical Clustering of the genus. For instance, in the above example, linking as a by-product, the disambiguation of the genus than organization in groups and place). of extraction and disambiguation of the genus can be found in previous works (Guthrie et al. 1990; Klavans, Chodorow, and Wacholder 1990; Copestake 1990; Ageno et al. 1992). Disambiguated genus and differentiae terms can be used to construct a better taxonomy of word senses. Since the dictionary usually has broader coverage of word senses than the thenot all MRD senses of a headword to one of h&apos;s predefined in the thesaurus. For instance, LDOCE sense mass of cloud, snow, mud, etc.) to LLOCE topic generally) than any of the LLOCE senses for such entries represent sense gaps in the thesaurus and should be left unlinked. Nevertheless, the linked entries are enough training material for topical clustering of MRD senses, as described in Section 4. 3. Linking an MRD to a Thesaurus This section describes how to establish a link between an MRD sense and its relevant word sense in a thesaurus, if such a link exists. We start with the preprocessing steps for the sense definition, which are necessary for the algorithm to obtain good results. Then we describe the linking algorithm step by step. Finally, we show illustrative examples to give some idea how the proposed algorithm works for the LLOCE and Roget&apos;s. 3.1 Preprocessing Steps Although only simple words are usually used in sense definitions, most of these words also highly ambiguous. For instance, the two instances of in the two following LDOCE sense definitions differ in meaning: bed-like piece of furniture on which a person being examined by a doctor. detector instrument that is supposed to show when a person is telling Notably, their parts-of-speech are also different. Determining the part of speech of each instance allows us to limit the range of possible meanings. The first instance a verb that means &amp;quot;to be in a flat resting position&amp;quot; or &amp;quot;to tell a lie.&amp;quot; On the other hand, the second instance is a nominal with a unique meaning &amp;quot;a false statement purposely made to deceive.&amp;quot; By tagging the definition with part-of-speech information, the degree of sense ambiguity in the definition can be reduced, thereby increasing the chance of successful linking. Tagging. methods for POS tagging have been proposed in recent years. For simplicity, we adopted the method proposed by Church (1988) to tag definition sentences. Experiments indicated an average error rate for tagging of less than 10%. Tagging errors have limited negative impact, because words in the LLOCE are organized primarily according to topic, not part of speech. The POS information is used to remove function words, as well as to look up words in the LLOCE with matching POS. The part-of-speech preprocessing phase is mandatory for the algorithm to exclude some inappropriate candidates for topics. See Table 8 for some examples of tagged LDOCE definition sentences. 69 Computational Linguistics Volume 24, Number 1 Table 8 tagged LDOCE definition sentences for the headword along/prep the/det side/n of/prep a/det river/n ,/, lake/n ,/, etc/adv which/det is/v heaped/v up/adv in/prep a/det field/n or/conj garden/n ,/, often/adv making/v a/det border/n or/conj division/n mass/n of/prep snow/n ,/, clouds/n ,/, mud/n ,/, etc/adv slope/n made/v at/prep bends/n in/prep a/det road/n or/conj racetrack/n ,/, so/conj that/conj they/pron are/v safer/adj for/conj cars/n to/* go/v round/adv Table 9 Examples of keywords extracted from tagged definition sentences. side/n river/n lake/n heap/v field/n garden/n border/n division/n snow/n clouds/n mud/n bend/n road/n race-track/n cars/n of Stopwords. general, function words in the definition are only marginally relevant to the meaning being defined. This is also true of words used in many definitions. For this reason, IR systems commonly exclude stopwords from the process of indexing and query. This also applies to our situation of retrieving topics relevant to the meaning of a sense based on the words in its definition. The list of all the stopwords is specifically designed to remove pronouns, determiners, prepositions, and Table 9 shows that the meaning of some definitions of found to be quite intact, even after stopwords are removed. Similarity between Definition and Thesaurus Class. viewing the definition a headword a set of words, it becomes easy to compare and measure their to thesaurus word classes containing word classes, we mean any supersets of synonym sets in a thesaurus that capture the semantic relations and semantic clusters that are effective for disambiguation as described in Section 2.3. The word classes are so chosen that they contain enough words to overlap with the sense definition in question. But each class should not be so big as to cover more than one sense for the distinction we want to make in the first place. Topics the LLOCE and categories or sections in good choices for such classes. Similarity between the defining keywords and a class of words reflects how closely the definition is related to the class. As a simple heuristic, the intended meaning of a definition disambiguated in favor of a relevant sense T for a class the highest similarity to such a sense found, we that the dictionary sense linked to the thesaurus sense T or that linked the thesaurus class a headword the definitions of let the word in a thesaurus that contain a definition e DEFh, problem amounts finding is relevant to these terms, the unweighted Dice can be adopted to measure similarity between a definition a class as follows: 2x wd x In(d, 1KEYDI + ICI = set of words in definition IKEYDI = of words in 70 Chen and Chang Topical Clustering C relevant class to the thesaurus, Wk = of ambiguity of B) = 1, and B) = when ,% B. The above similarity measure may be improved by taking into consideration specific features of a particular thesaurus. For instance, the cross-reference features in the or the intersense relations in very effective in reflecting semantic rethus, they should be included in this similarity measure. Let the cross-referenced classes for the word class C in the thesaurus. Thus, we have 2wd x C) + = of cross-references to a and IREFc I = the number of classes The We sum up the above description and outline the procedure for labeling senses on a dictionary entry as follows: Linking fine-grained MRD senses to their relevant thesaurus classes. 1: Given a head word its definition, the MRD. 2: For each definition each word in POS information. 3: Remove all stop words in obtain a list of keyword-POS pairs, KEYD. 4: Look up the headword the thesaurus to obtain 5: Compute Sim(D, all 6: Link that Sim(D, the largest and Sim(D, greater a preset threshold, Illustrative Examples: Linking LDOCE to LLOCE and examples are given in this subsection to illustrate how to establish linkage between a typical dictionary and thesaurus. Example 1 shows, step by step, up an LDOCE sense, share in a company business the relevant LLOCE sense Example 2 is intended to that quite general and applies to thesauri other than the LLOCE. same LDOCE senses will be shown to links to a relevant (Possessive relation). Example 1 an LDOCE sense its relevant LLOCE sense. 1: = &amp;quot;a share in a company, business, etc.&amp;quot; 2: = det, share /n, in/prep, a/ det, company/n, business /n, etc. / adv} 1 For simplicity, the parameter -y is set to 1 in our experiment. Je represents the class of words related to the topic of Wealth, and Investment under LLOCE topical sets Je100 through 127. The reference code e is added in accordance with the coding scheme described in Section 2.2. + ICI + 71 Computational Linguistics Volume 24, Number 1 3: = {share/n, company/n, business/n}, IKEYDI = 3. 4: LLOCE topics as word classes in have = The LLOCE lists the following cross references relevant to CLASSinterest: {Ka and related activity),. , games)}, = and = IFj1 = 1, IFb1 = =- 1, IKal = I = 8, = 1, = 1, I = 1. All three keywords appear in three different topics but only the following are relevant to Cc (company) Thus, we have = In(share, Je) = 1, In(company, Cc) = 1. = Wcompany/n = Wbusiness/n = 5: values are calculated as follows: (D,Je) WshareX Je) In(share, business} I + I {Je} I + I Wcompany X Je) + {share, company, business} I + I {ie} + x wbusiness x Je)+ {share, company, business} I + I {Je}I + IREF)e I x (1 +1)+ 2 x x (0+ 0)+2 x A x (0 + 0) 3 + 1 + 1 L33 = 5 x 1x (0 + 1) 1 + 1 0.133, Fj) = 0, Sim&apos; (D, Ka) = 0. 6: The LDOCE is linked the LLOCE sense, Example 2 an LDOCE its relevant 1-3: The first three are independent of the thesaurus used, therefore the same results as in Example 1 should be obtained. 72 Chen and Chang Topical Clustering 4: as word classes in have: interest matter), volition), relations)}, keywords company, in many only the following sections are relevant to Therefore we have: = = wbusinessin = 5: simplicity, we ignore the cross-reference information in our similarity calculation solely on the Thus, we have: = 0, Sim(D, Cb) = 0, Eb) = 3+1 4 28 0.071, Sim(D, Ei) 2x(+ 11 -= -= 3 4 56 6: LDOCE sense linked to Performance Evaluation of An experiment involving the LDOCE and the LLOCE was carried out to assess the of the (see Table 10). To evaluate the performance of we define the ratios of applicability precision follows: A= (all labeled # (all definitions) (correct labeled definitions) Nearly half of the nominal LDOCE senses for a set of highly polysemous words are linked to their relevant LLOCE sense and topics, with a surprisingly high precision of the other half, not find sufficiently high similarity to warrant a link. That is due primarily (approximately two-thirds) to sense gaps in the LLOCE, rather than inconsistency among the LDOCE definitions. 4. Topical Clustering of MRD Senses as Information Retrieval section, we will describe algorithm for clustering dictionary senses. closely related senses by applying IR techniques on the results of an MRD. After a substantial portion of MRD senses to thesaurus sense classes, we put all definitions of the senses linked to a particular class together in a document. With such a document of collective definitions, topical clustering of all MRD senses bears a striking resemblance to the IR task of 73 Computational Linguistics Volume 24, Number 1 Table 10 of Headword # of Definitions in LDOCE Linking to the LLOCE Correct Incorrect Unknown Applicability Precision bass 5 2 1 2 67% 100% bow 5 4 0 1 80% 100% cone 3 3 0 0 100% 100% country 5 5 0 0 100% 100% crane 2 2 0 0 100% 100% duty 2 1 0 1 50% 100% galley 4 2 0 2 50% 100% interest 6 4 0 2 67% 100% issue 8 2 1 5 38% 50% mole 3 2 0 1 67% 100% plant 6 1 0 5 17% 100% position 10 7 0 3 70% 100% sentence 2 2 0 0 100% 100% slug 5 2 0 3 40% 100% space 8 2 0 6 25% 100% star 9 3 2 4 56% 60% suit 6 3 0 3 50% 100% table 7 2 0 5 29% 100% tank 3 1 0 2 33% 100% taste 6 1 0 5 17% 100% total 105 51 4 50 52% 93% retrieving relevant documents for a given query. We observe that the defining words a sense S frequently recur in documents relevant to instance, consider the following LDOCE sense: piece of this shape for a mark of office, rank, honour, etc. observe that most entries in the LDOCE are like that they contain defining words that are also recurring terms in a relevant document. For instance, such as bracelet, necklace, defined by using terms in a corresponding the LLOCE word class for the topic and personal 11 shows the including such senses as bracelet, neckare indeed relevant to 4.1 The Clustered Senses as Documents With topical clustering of MRD senses cast as an IR task, a wealth of well-understood IR techniques can be utilized, including stopword removal, case folding, stemming, term weighting, and document ranking (Witten, Moffat, and Bell 1994). Using the IR analogy, topical clustering of an MRD sense S is finding relevant documents (topical clusters), given a query (S, the sense definition). With this in mind, we treat the collective definitions of each topical cluster as a virtual document (VD) and reduce the clustering task to ranking relevancy based on terms in the sense definition as well as in the VDs. For simplicity, we adopt a common scheme of x idf weight terms the documents. Each defining term in a VD is associated with a term frequency 74 Chen and Chang Topical Clustering document frequency represent the frequency of term tj in document represent the number of VDs where term appears. The relevancy of the sense to term is therefore given by the following weight: = x x log the number of documents in the collection. The relevancy of a a Q is obtained by summing up the weights of all terms in Q: Table 11 shows the LDOCE senses and their definitions that are linked to relevant senses under a certain topic. An implementation of 455 LDOCE including bracelet, a in the LLOCE. The definitions these senses in the form the virtual document shown in Table 12, significantly topical terms are used within a VD consistently. For instance, the term consistently in 10 senses in a weight of 23.82. Table 12 displays weighted terms in some VDs and their associated values of df, weight. The sum up the above descriptions and outline the here. clustering of MRD senses. 1: Run the MRD and thesaurus and collect terms to form VDs. 2: Read sense definition the MRD. 3: Remove all stopwords in produce a list, Q, of stemmed keywords with part of speech. Step 4: For each term tj in Q, look up the corresponding IN for all virtual docu- C set of all word classes in the thesaurus. 5: For Sim(Q, = tfij x log(N/dfj). 6: Assign S to the class that Sim(Q, the largest for all passes a preset threshold 4.3 Illustrative Examples: Topical Clustering of LDOCE Senses examples are given in this subsection to illustrate how Example a calculation done in find the most relevant topics for another 5-or more pointed figure). 4 shows the same calculation done for the of (a piece of metal in this shape for wearing as a mark of office, rank, honour etc.) discussed above at the beginning of Section 4. Despite very ambiguous terms such as wear (to dress rub) (body, shape, in both definitions, weighting scheme of to work well enough to determine the relevant and personal belongings) 75 Computational Linguistics Volume 24, Number 1 Table 11 list of LDOCE senses linked to LLOCE classes by Cluster/ Size Headword Sense Definition / • accessory • something which is not a necessary part of something larger but which makes it more beautiful, useful, effec-tive etc. (Clothes and per-sonal belongings) • apron • a simple garment worn over the front part of clothes to keep them clean while working or doing something dirty or esp. while cooking. bracelet • a band or ring, usu. of worn the wrist arm as an ornament. • coat • an outer garment with long SLEEVEs, often fastened the front with buttons, and usu. worn to keep warm or for protection. • necklace • a string of jewels, BEADs, PEARLs, etc., or a chain silver, etc., the neck as an ornament esp. by women. tie • a band of cloth the neck, usu. inside a collar and tied in a knot at the front. / • cross • a figure or mark formed by one straight line crossing another, as X, often used. • • a straight line going from side to side through the of a circle or other curved figure. • pyramid • a solid figure with a flat usu. square base and flat 3-angled sides that slope upwards to meet at a point. rectangle • a figure with 4 straight sides forming 4 right square • a figure with 4 equal sides and 4 right triangle • a flat figure with 3 straight sides and 3 / • bank • land along the side of a river, lake, etc. • • a wide opening along a coast; part of the sea or of a large lake enclosed in a curve of the land. • beach • a shore of an ocean, sea, or lake or the bank of a covered by sand, smooth stones, or larger pieces of rock. • lake • a large mass of water surrounded by land. • cascade • a steep high usu. small waterfall, esp. one part of bigger waterfall. / • account • a record or statement of money received and paid (Banking) as by bank or business, esp. for a particular period or at a particular date. • asset • something such as a house or furniture, that has and that may be sold to pay a debt. • bank • a place in which money is kept and paid out on and where related activities go on. • capital • wealth, esp. when used to produce more wealth. • stock • money lent to a government at a fixed rate of interest. Example 3 Clustering an LDOCE sense star.1.n.3. to Table 11 for some of the results of running 2: = 5-or more pointed figure.&amp;quot; 3: Q = figure/n} 76 Chen and Chang Topical Clustering Table 12 examples of virtual documents, terms, df, weight. VD Terms tf df Weight VD Terms tf df Weight Dg garment 43 12 102.45 Ld sea 38 23 65.81 wear 60 27 94.30 land 47 40 55.39 dress 21 5 68.42 mountain 16 7 46.74 woman 49 36 62.91 water 44 47 44.76 coat 19 7 55.51 river 17 15 36.71 trouser 11 2 45.91 tide 7 1 34.07 shirt 12 3 45.22 valley 8 2 33.39 undergarment 8 2 33.39 ocean 9 4 31.33 shoe 12 11 29.63 lake 10 8 27.88 skirt 6 1 29.20 shore 8 4 27.84 cloth 16 25 26.37 earth 16 26 25.75 waist 8 5 26.06 island 6 2 25.04 jacket 6 2 25.04 rock 11 14 24.51 sleeve 5 1 24.33 wave 9 13 20.72 glove 5 2 20.87 hill 8 10 20.51 sock 5 2 20.87 deep 12 25 19.78 neck 10 17 20.34 coast 6 5 19.54 underpants 4 1 19.47 slope 7 8 19.51 woolen 5 4 17.40 cliff 5 3 18.84 tie 7 14 15.59 map 7 9 18.69 Jb mathematics 15 4 52.21 Je money 42 39 50.56 multiply 8 3 30.15 account 16 9 42.72 figure 13 19 25.00 bank 16 12 38.12 straight 12 17 24.41 pay 17 31 24.37 angle 10 12 23.82 lend 7 5 22.80 line 21 44 22.75 interest 12 25 19.78 circle 9 11 22.22 debt 5 3 18.84 geometry 5 3 18.84 sum 6 10 15.38 calculate 7 9 18.69 wealth 5 6 15.37 add 8 13 18.42 credit 3 1 14.60 subtract 3 1 14.60 property 5 9 13.35 curved 7 5 11.54 deposit 2 1 9.73 perpendicular 2 1 9.73 savings 2 1 9.73 proportion 2 1 9.73 payment 4 14 8.91 right-angled 2 1 9.73 share 4 15 8.63 triangle 2 1 9.73 record 4 16 8.37 edge 6 26 9.65 spend 3 11 7.40 arc 2 2 8.34 business 6 40 7.07 curve 3 9 8.01 supply 4 25 6.59 cross 3 11 7.40 amount 6 46 6.23 4: For each term in have: Wpointed,Hd = 0, = Wpointed,r3 = 0, Wpointed,Kf 0, Wpointed,Gd = 0, - = = Step 5: Adding up the weights for each VD, we get = Sim(Q, Kf) = 9.62, = 77 Computational Linguistics Volume 24, Number 1 Sim (Q, Hd) = 5.77. 6: the most relevant topics to S, we get the following ranked list , games) , machines, and instruments). Example 4 an LDOCE 1: Table 11. 2: = &amp;quot;a piece of metal in this shape for wearing as a mark of office, rank, honour etc.&amp;quot; 3: Q = shape/n, wear/v, mark/n, office/n, rank/n, honour/n} 4: each term in Q, we have: = = 5.98, = Wmark,Dg = 1.11, Woffice,Dg = 0, = = 0, Knetauir = 62.83, = wsmheatpac = 0, i= 0, Wwear,Hc = 0, Wwear,Ct = 0, = = 0, = = = 0, = = 0, = 22.62, = = = 4, b= Wrank,Hb = 0, -= 5: up the weights for each VD, we get Sim(Q, Ci) = 78.17. 6: the most relevant topics to S, we get the following ranked list: and personal belongings), substances and materials), generally), classifications and situations). 4.4 Experimental Results experiment was conducted to assess the effectiveness of the The experimental results show that the nearly 11,045 of some 39,000 nominal LDOCE senses to a topical sense in the LLOCE. Evaluation based on a 20-word test set shows that, on the average, 50% of the LDOCE instances linked to an LLOCE sense, and, of these links, 95% are correct. These linked LDOCE senses establish 129 topical clusters, one for each LLOCE topic. When the proposed 78 Chen and Chang Topical Clustering is applied to assign sense definitions in LDOCE with relevant labels, it obtains very high precision but low coverage. design specifically to improve coverage by providing a reliable method for clustering MRD left unlabeled by document of defining terms is then formed from senses in each of these clusters. Subsequently, on the nominal LDOCE sense, attempting to merge it to one of the topical clusters. thresholds for selected according to random sampling from definitions in the LDOCE. Assume 9 is the threshold and 0 is an estimator the bound on the error of estimation. The problem is to limit the error of below probability 1 — can be stated as &lt; = since the number of definitions is large enough to permit estimation of population Central Limit Theory, the parameter to have apa normal distribution. We will usually select = hence 1 — a will be approximately 0.95 for normal distribution. To estimate d, a simple random sample of 100 definitions (about 350 senses) is used. Thus, the estimate of threshold 0.12 for estimation was done for the threshold used in Evaluation was done on a set of 20 polysemous words that have been used in recent literature on WSD. These words focus on the more difficult cases of sense ambiguity, as can be seen by the degree of ambiguity as recorded in the LDOCE. These words have 5.3 senses on the average, as opposed to the average of 2.6 senses for all words in the LDOCE. The evaluation is based on the relevancy assessment by two human judges. The Appendix gives a sense-by-sense rundown of all senses tested and evaluated. Table 13 the word-by-word applicability and precision of not all senses are clustered and not all clustered senses are correct, applicability and precision are rather high, which seems to indicate that the resulting sense division is directly usable in WSD, and thus, eliminates the need for human intervention. 5. Discussion In this section, we thoroughly analyze the experimental results, in particular, the cases which These cases reveal the strengths and limitations of and hint at possible improvements to the algorithm. In addition, we also point out several uses of the topical clusters. Failure of the of be attributed to a number of factors, including vagueness of definitions, inappropriate definition lengths (too short or too long), metaphoric or metonymic senses, and deictic references. Table 14 shows some examples of the failed For instance, the sense readiness to give attention) too vague and short for correct clustering to occur. On the other hand, long definitions including too many non-essential differentiae also give rise to erroneous clustering. We notice that the definitions of such senses have been radically changed and made more specific in the third edition of the LDOCE. The reason behind the changes may be that these sense definitions are also difficult for humans to grasp. Metonymic senses sometimes lead to problems for the proposed algorithms. puts piece of metal in this shape for wearing as a mark It seems that precision may be lower if run on the unlabeled entries, but we suspect the difference is very small. 79 Computational Linguistics Volume 24, Number 1 Table 13 of the Headword #of Labeling with Expanded Candidate Set in LDOCE Correct Incorrect Unknown Applicability Precision bass 5 5 0 0 100% 100% bow 5 5 0 0 100% 100% cone 3 3 0 0 100% 100% country 5 5 0 0 100% 100% crane 2 2 0 0 100% 100% duty 2 2 0 0 100% 100% galley 4 4 0 0 100% 100% interest 6 4 2 0 100% 67% issue 8 3 2 3 63% 60% mole 3 3 0 0 100% 100% plant 6 5 1 0 100% 83% position 10 9 1 0 100% 90% sentence 2 2 0 0 100% 100% slug 5 5 0 0 100% 100% space 8 7 1 0 100% 88% star 9 8 1 0 100% 90% suit 6 5 1 0 100% 83% table 7 6 1 0 100% 86% tank 3 3 0 0 100% 100% taste 6 5 0 1 83% 100% total 105 91 10 4 96% 90% office, rank, honour, etc.) the Dg class and personal belongings). the other the metonymic meaning, another heavenly body reas determining one&apos;s fate) second to the &amp;quot;primary&amp;quot; sense, La considering cue phrases such as as a mark of, might be able to handle metaphoric and metonymic senses more successfully. Krovetz (1992) observes that the LDOCE indicates explicit sense shifts via the dereference, which is a link to the previous sense created by such terms as these, those, its, itself, such a, an. author identifies many systematic sense indicated by such references including tree or fruit), amber), Countshifts indicated through a deictic reference are so pervasive in the MRD that they show up more than once in our small 20-word test For instance, the LDOCE sense example of this) a Countfrom its previous sense act of coming out) deictic reference of these specific patterns of definition are not taken into in algorithm often fails in such cases. Further work must be undertaken to cope with direct and deictic references, so that such definitions can be appropriately clustered. 5.2 Clustered Definitions and Examples as a Knowledge Source for WSD Many studies have shown that MRD definitions and example sentences are a good</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Ageno</author>
<author>I Castellon</author>
<author>M A Marti</author>
<author>G Rigau</author>
<author>F Ribas</author>
<author>H Rodriguez</author>
<author>M Taule</author>
<author>E Verdejo</author>
</authors>
<title>SEISD: An environment for extraction of semantic information from on-line dictionaries.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd Conference on Applied Natural Language Processing,</booktitle>
<pages>253--254</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="24754" citStr="Ageno et al. 1992" startWordPosition="3914" endWordPosition="3917"> sense definition is often disambiguated through the genus. Thus, the task of linking MRD and thesaurus senses is closely related to the extraction and 68 Chen and Chang Topical Clustering disambiguation of the genus. For instance, in the above example, linking bank.1.n.1 to bank-Ld099 has, as a by-product, the disambiguation of the genus land to land-Ld084 (Geography) rather than land-Ce078 (Social organization in groups and place). Details of extraction and disambiguation of the genus can be found in previous works (Guthrie et al. 1990; Klavans, Chodorow, and Wacholder 1990; Copestake 1990; Ageno et al. 1992). Disambiguated genus and differentiae terms can be used to construct a better taxonomy of word senses. Since the dictionary usually has broader coverage of word senses than the thesaurus, not all MRD senses of a headword h correspond to one of h&apos;s predefined senses in the thesaurus. For instance, LDOCE sense bank.1.n.3 (a mass of cloud, snow, or mud, etc.) corresponds to LLOCE topic Hb (Object generally) rather than any of the predefined LLOCE senses for bank. Therefore, such entries represent sense gaps in the thesaurus and should be left unlinked. Nevertheless, the linked entries are enough</context>
</contexts>
<marker>Ageno, Castellon, Marti, Rigau, Ribas, Rodriguez, Taule, Verdejo, 1992</marker>
<rawString>Ageno, A., I. Castellon, M. A. Marti, G. Rigau, F. Ribas, H. Rodriguez, M. Taule and E Verdejo. 1992. SEISD: An environment for extraction of semantic information from on-line dictionaries. In Proceedings of the 3rd Conference on Applied Natural Language Processing, pages 253-254, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Ahlswede</author>
<author>Martha Evens</author>
</authors>
<title>Parsing vs. text processing in the analysis of dictionary definitions.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting,</booktitle>
<pages>217--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9554" citStr="Ahlswede and Evens 1988" startWordPosition="1479" endWordPosition="1482">description of MRDs and thesauri is in order. 2.1 Fine-Grained Senses in an MRD Interest in MRD-based research has increased over the years; in particular, the LDOCE and Webster&apos;s Seventh New Collegiate Dictionary (W7) (1967) have drawn much attention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of information listed in a dictionary entry, including part of speech, subcategory, examples, collocations, and typical arguments, which is potentially useful for WSD. In this regard, the LDOCE is particularly appropriate since it uses a reduced, contr</context>
</contexts>
<marker>Ahlswede, Evens, 1988</marker>
<rawString>Ahlswede, Thomas and Martha Evens. 1988. Parsing vs. text processing in the analysis of dictionary definitions. In Proceedings of the 26th Annual Meeting, pages 217-224. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Processing dictionary definitions with phrasal pattern hierarchies.</title>
<date>1987</date>
<journal>American Journal of Computational Linguistics,</journal>
<pages>13--3</pages>
<contexts>
<context position="9332" citStr="Alshawi 1987" startWordPosition="1448" endWordPosition="1449">f word senses. To get the best of both worlds of dictionary and thesaurus, we propose to cluster MRD definitions to yield a broad-coverage sense division with the granularity of a thesaurus. Therefore, a short description of MRDs and thesauri is in order. 2.1 Fine-Grained Senses in an MRD Interest in MRD-based research has increased over the years; in particular, the LDOCE and Webster&apos;s Seventh New Collegiate Dictionary (W7) (1967) have drawn much attention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of information listed in a dic</context>
</contexts>
<marker>Alshawi, 1987</marker>
<rawString>Alshawi, H. 1987. Processing dictionary definitions with phrasal pattern hierarchies. American Journal of Computational Linguistics, 13(3):195-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>B Boguraev</author>
<author>D Carter</author>
</authors>
<title>Placing the dictionary on-line.</title>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing,</booktitle>
<pages>41--63</pages>
<editor>In B. Boguraev and Briscoe, editors,</editor>
<location>Longman, London,</location>
<marker>Alshawi, Boguraev, Carter, 1989</marker>
<rawString>Alshawi, H., B. Boguraev, and D. Carter. 1989. Placing the dictionary on-line. In B. Boguraev and Briscoe, editors, Computational Lexicography for Natural Language Processing, Longman, London, pages 41-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Amsler</author>
</authors>
<title>Machine-readable dictionaries.</title>
<date>1984</date>
<booktitle>Annual Review of Information Science and Technology,</booktitle>
<pages>19--161</pages>
<contexts>
<context position="9304" citStr="Amsler 1984" startWordPosition="1444" endWordPosition="1445">y and a narrower coverage of word senses. To get the best of both worlds of dictionary and thesaurus, we propose to cluster MRD definitions to yield a broad-coverage sense division with the granularity of a thesaurus. Therefore, a short description of MRDs and thesauri is in order. 2.1 Fine-Grained Senses in an MRD Interest in MRD-based research has increased over the years; in particular, the LDOCE and Webster&apos;s Seventh New Collegiate Dictionary (W7) (1967) have drawn much attention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of</context>
</contexts>
<marker>Amsler, 1984</marker>
<rawString>Amsler, Robert A. 1984a. Machine-readable dictionaries. Annual Review of Information Science and Technology, 19:161-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Amsler</author>
</authors>
<title>Lexical knowledge bases, Panel session on machine-readable dictionaries.</title>
<date>1984</date>
<booktitle>In Proceedings of the Tenth International Congress on Computational Linguistics,</booktitle>
<pages>458--459</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="9304" citStr="Amsler 1984" startWordPosition="1444" endWordPosition="1445">y and a narrower coverage of word senses. To get the best of both worlds of dictionary and thesaurus, we propose to cluster MRD definitions to yield a broad-coverage sense division with the granularity of a thesaurus. Therefore, a short description of MRDs and thesauri is in order. 2.1 Fine-Grained Senses in an MRD Interest in MRD-based research has increased over the years; in particular, the LDOCE and Webster&apos;s Seventh New Collegiate Dictionary (W7) (1967) have drawn much attention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of</context>
</contexts>
<marker>Amsler, 1984</marker>
<rawString>Amsler, Robert A. 1984b. Lexical knowledge bases, Panel session on machine-readable dictionaries. In Proceedings of the Tenth International Congress on Computational Linguistics, pages 458-459, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Amsler</author>
</authors>
<title>Words and words.</title>
<date>1987</date>
<booktitle>In Proceedings of the Third Workshop on Theoretical Issues in Natural Language Processing,</booktitle>
<pages>7--9</pages>
<institution>Mexico State University at Las</institution>
<location>New</location>
<marker>Amsler, 1987</marker>
<rawString>Amsler, Robert A. 1987. Words and words. In Proceedings of the Third Workshop on Theoretical Issues in Natural Language Processing, pages 7-9, New Mexico State University at Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>Word sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<pages>264--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2061" citStr="Brown et al. 1991" startWordPosition="310" endWordPosition="313">ch. Relatively well-established IR techniques of weighting terms and ranking document relevancy are applied to find the topical clusters that are most relevant to the definition of each MRD sense. Finally, we describe an implemented version of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Brown, P. E, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1991. Word sense disambiguation using statistical methods. In Proceedings of the 29th Annual Meeting, pages 264-270. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>Word sense disambiguation using decomposable models.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting,</booktitle>
<pages>139--145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2343" citStr="Bruce and Wiebe 1995" startWordPosition="352" endWordPosition="355">e LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic, register, dialect, collocation, part of speech, and valency (McRoy 1992). Various approaches to word sense division have been proposed in the literature on WSD, including (1) sense numbers in every-</context>
<context position="5390" citStr="Bruce and Wiebe 1995" startWordPosition="811" endWordPosition="814">it is defined along grammatical, not semantic, lines. Furthermore, as pointed out in Dolan (1994), the sense division in an MRD is frequently too fine-grained for the purpose of WSD. A WSD system based on dictionary senses often faces unnecessary and difficult &amp;quot;forced-choices.&amp;quot; Dolan proposes a heuristic algorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the proposed algorithm was only described in a few examples and was not developed further. Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk 1995; Yarowsky 1995) still resort to human intervention to identify and group closely related senses in an MRD. Using thesaurus categories directly as a coarse sense division may seem to be a viable alternative (Yarowsky 1995). However, typical thesauri, such as Roget&apos;s Thesaurus (1987), suffer sense gaps and, occasionally, are too fine-grained. Yarowsky (1992) reports that there are uses not listed in Roget&apos;s for 3 of 12 nouns in his WSD study, while uses which a native speaker might consider as a single sense are often encoded in several Roget&apos;s categories. As an alternative approach t</context>
</contexts>
<marker>Bruce, Wiebe, 1995</marker>
<rawString>Bruce, Rebecca and Janyce Wiebe. 1995. Word sense disambiguation using decomposable models. In Proceedings of the 33rd Annual Meeting, pages 139-145. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason S Chang</author>
<author>J N Chen</author>
<author>H H Sheng</author>
<author>S J Ker</author>
</authors>
<title>Combining machine readable lexical resources and bilingual corpora for broad word sense disambiguation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Second Conference of the Association for Machine Translation,</booktitle>
<pages>115--124</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="2390" citStr="Chang et al. 1996" startWordPosition="362" endWordPosition="365">d approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic, register, dialect, collocation, part of speech, and valency (McRoy 1992). Various approaches to word sense division have been proposed in the literature on WSD, including (1) sense numbers in every-day dictionaries (Lesk 1986; Cowie, Guthrie, an</context>
</contexts>
<marker>Chang, Chen, Sheng, Ker, 1996</marker>
<rawString>Chang, Jason S., J. N. Chen, H. H. Sheng, and S. J. Ker. 1996. Combining machine readable lexical resources and bilingual corpora for broad word sense disambiguation. In Proceedings of the Second Conference of the Association for Machine Translation, pages 115-124, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Chen</author>
<author>Jason S Chang</author>
</authors>
<title>Towards generality and modularity in statistical word sense disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the 8th Asian Conference on Language, Information and Computation,</booktitle>
<pages>45--48</pages>
<contexts>
<context position="3420" citStr="Chen and Chang 1994" startWordPosition="513" endWordPosition="516">ency (McRoy 1992). Various approaches to word sense division have been proposed in the literature on WSD, including (1) sense numbers in every-day dictionaries (Lesk 1986; Cowie, Guthrie, and Guthrie 1992), (2) automatic or hand-crafted clusters of dictionary senses (Dolan 1994; Bruce and Wiebe 1995; Luk * Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC. E-mail: dr818314@cs.nthu.edu.tw; jschang@cs.nthu.edu.tw. C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 1995), (3) thesaurus categories (Yarowsky 1992; Chen and Chang 1994), (4) translation in another language (Gale, Church, and Yarowsky 1992; Dagan, Itai, and Schwa11 1991; Dagan and Itai 1994), (5) automatically induced clusters with sublexical representation (Schtitze 1992), and (6) hand-crafted lexicons (McRoy 1992). This paper is motivated by the observation that directly using dictionary senses for sense division offers several advantages. Sense distinction according to a dictionary is readily available from machine-readable dictionaries (MRDs) such as the Longman Dictionary of Contemporary English (LDOCE) (Proctor 1978). A dictionary such as the LDOCE has </context>
</contexts>
<marker>Chen, Chang, 1994</marker>
<rawString>Chen, J. N. and Jason S. Chang. 1994. Towards generality and modularity in statistical word sense disambiguation. In Proceedings of the 8th Asian Conference on Language, Information and Computation, pages 45-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin S Chodorow</author>
<author>Roy J Byrd</author>
<author>George E Heidom</author>
</authors>
<title>Extracting semantic hierarchies from a large on-line dictionary.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting,</booktitle>
<pages>299--304</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Chodorow, Byrd, Heidom, 1985</marker>
<rawString>Chodorow, Martin S., Roy J. Byrd and George E. Heidom. 1985. Extracting semantic hierarchies from a large on-line dictionary. In Proceedings of the 23rd Annual Meeting, pages 299-304. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the 2nd Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<location>Austin, TX.</location>
<contexts>
<context position="27036" citStr="Church (1988)" startWordPosition="4294" endWordPosition="4295">stance allows us to limit the range of possible meanings. The first instance of lies is a verb that means &amp;quot;to be in a flat resting position&amp;quot; or &amp;quot;to tell a lie.&amp;quot; On the other hand, the second instance is a nominal with a unique meaning &amp;quot;a false statement purposely made to deceive.&amp;quot; By tagging the definition with part-of-speech information, the degree of sense ambiguity in the definition can be reduced, thereby increasing the chance of successful linking. Part-of-Speech Tagging. Various methods for POS tagging have been proposed in recent years. For simplicity, we adopted the method proposed by Church (1988) to tag definition sentences. Experiments indicated an average error rate for tagging of less than 10%. Tagging errors have limited negative impact, because words in the LLOCE are organized primarily according to topic, not part of speech. The POS information is used to remove function words, as well as to look up words in the LLOCE with matching POS. The part-of-speech preprocessing phase is mandatory for the algorithm to exclude some inappropriate candidates for topics. See Table 8 for some examples of tagged LDOCE definition sentences. 69 Computational Linguistics Volume 24, Number 1 Table </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. W. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the 2nd Conference on Applied Natural Language Processing, pages 136-143, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>An approach to building the hierarchical element of a lexical knowledge base from a machine readable dictionary.</title>
<date>1990</date>
<booktitle>In Proceedings of the First International Workshop on Inheritance in Natural Language Processing,</booktitle>
<pages>pages</pages>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="24734" citStr="Copestake 1990" startWordPosition="3912" endWordPosition="3913">on; therefore, a sense definition is often disambiguated through the genus. Thus, the task of linking MRD and thesaurus senses is closely related to the extraction and 68 Chen and Chang Topical Clustering disambiguation of the genus. For instance, in the above example, linking bank.1.n.1 to bank-Ld099 has, as a by-product, the disambiguation of the genus land to land-Ld084 (Geography) rather than land-Ce078 (Social organization in groups and place). Details of extraction and disambiguation of the genus can be found in previous works (Guthrie et al. 1990; Klavans, Chodorow, and Wacholder 1990; Copestake 1990; Ageno et al. 1992). Disambiguated genus and differentiae terms can be used to construct a better taxonomy of word senses. Since the dictionary usually has broader coverage of word senses than the thesaurus, not all MRD senses of a headword h correspond to one of h&apos;s predefined senses in the thesaurus. For instance, LDOCE sense bank.1.n.3 (a mass of cloud, snow, or mud, etc.) corresponds to LLOCE topic Hb (Object generally) rather than any of the predefined LLOCE senses for bank. Therefore, such entries represent sense gaps in the thesaurus and should be left unlinked. Nevertheless, the linke</context>
</contexts>
<marker>Copestake, 1990</marker>
<rawString>Copestake, A. 1990. An approach to building the hierarchical element of a lexical knowledge base from a machine readable dictionary. In Proceedings of the First International Workshop on Inheritance in Natural Language Processing, pages 19-29, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Cowie</author>
<author>Joe Guthrie</author>
<author>Louise Guthrie</author>
</authors>
<title>Lexical disambiguation using simulated annealing.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>359--365</pages>
<marker>Cowie, Guthrie, Guthrie, 1992</marker>
<rawString>Cowie, Jim, Joe Guthrie and Louise Guthrie. 1992. Lexical disambiguation using simulated annealing. In Proceedings of the 14th International Conference on Computational Linguistics, pages 359-365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Word sense disambiguation using a second language monolingual corpus.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="2114" citStr="Dagan and Itai 1994" startWordPosition="319" endWordPosition="322">eighting terms and ranking document relevancy are applied to find the topical clusters that are most relevant to the definition of each MRD sense. Finally, we describe an implemented version of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and</context>
<context position="3543" citStr="Dagan and Itai 1994" startWordPosition="532" endWordPosition="535">se numbers in every-day dictionaries (Lesk 1986; Cowie, Guthrie, and Guthrie 1992), (2) automatic or hand-crafted clusters of dictionary senses (Dolan 1994; Bruce and Wiebe 1995; Luk * Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC. E-mail: dr818314@cs.nthu.edu.tw; jschang@cs.nthu.edu.tw. C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 1995), (3) thesaurus categories (Yarowsky 1992; Chen and Chang 1994), (4) translation in another language (Gale, Church, and Yarowsky 1992; Dagan, Itai, and Schwa11 1991; Dagan and Itai 1994), (5) automatically induced clusters with sublexical representation (Schtitze 1992), and (6) hand-crafted lexicons (McRoy 1992). This paper is motivated by the observation that directly using dictionary senses for sense division offers several advantages. Sense distinction according to a dictionary is readily available from machine-readable dictionaries (MRDs) such as the Longman Dictionary of Contemporary English (LDOCE) (Proctor 1978). A dictionary such as the LDOCE has broad coverage of word senses, useful for WSD. Furthermore, indicative words and concepts for each sense are directly avail</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>Dagan, Ido and Alon Itai. 1994. Word sense disambiguation using a second language monolingual corpus. Computational Linguistics, 20(4):563-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
<author>Ulrike Schwall</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<pages>130--137</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Dagan, Itai, Schwall, 1991</marker>
<rawString>Dagan, Ido, Alon Itai, and Ulrike Schwall. 1991. Two languages are more informative than one. In Proceedings of the 29th Annual Meeting, pages 130-137. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
</authors>
<title>Word sense disambiguation: Clustering related senses.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>712--716</pages>
<contexts>
<context position="3078" citStr="Dolan 1994" startWordPosition="472" endWordPosition="473">contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic, register, dialect, collocation, part of speech, and valency (McRoy 1992). Various approaches to word sense division have been proposed in the literature on WSD, including (1) sense numbers in every-day dictionaries (Lesk 1986; Cowie, Guthrie, and Guthrie 1992), (2) automatic or hand-crafted clusters of dictionary senses (Dolan 1994; Bruce and Wiebe 1995; Luk * Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC. E-mail: dr818314@cs.nthu.edu.tw; jschang@cs.nthu.edu.tw. C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 1995), (3) thesaurus categories (Yarowsky 1992; Chen and Chang 1994), (4) translation in another language (Gale, Church, and Yarowsky 1992; Dagan, Itai, and Schwa11 1991; Dagan and Itai 1994), (5) automatically induced clusters with sublexical representation (Schtitze 1992), and (6) hand-crafted lexicons (McRoy 1992). This p</context>
<context position="4867" citStr="Dolan (1994)" startWordPosition="732" endWordPosition="733">he extent of overlap between words in a dictionary definition and words in the local context of the word to be disambiguated. The author reports that WSD performance ranges from 50% to 70% and his method works well for senses strongly associated with specific collocations, such as ice-cream cone and pine cone. Unfortunately, using MRDs as the knowledge source for sense division and disambiguation leads to some problems. Zernik (1992) notes that the dictionary dichotomy of senses is inadequate for WSD, because it is defined along grammatical, not semantic, lines. Furthermore, as pointed out in Dolan (1994), the sense division in an MRD is frequently too fine-grained for the purpose of WSD. A WSD system based on dictionary senses often faces unnecessary and difficult &amp;quot;forced-choices.&amp;quot; Dolan proposes a heuristic algorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the proposed algorithm was only described in a few examples and was not developed further. Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk 1995; Yarowsky 1995) still resort to human intervention to identify and</context>
<context position="9847" citStr="Dolan 1994" startWordPosition="1524" endWordPosition="1525">lysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of information listed in a dictionary entry, including part of speech, subcategory, examples, collocations, and typical arguments, which is potentially useful for WSD. In this regard, the LDOCE is particularly appropriate since it uses a reduced, controlled vocabulary of some 2,000 words to define over 60,000 word senses representing a comprehensive vocabulary and broad coverage of word senses. It is arguable that the dictionary division of senses for a given word is too finegrained, thus inadequate for WSD. For instance, it might not alwa</context>
<context position="60757" citStr="Dolan (1994)" startWordPosition="10264" endWordPosition="10265"> many verbs for moving and action, such as move and strike, can be used polysemously in the sense of emotion. Chodorow, Byrd, and Heidom (1985) observe that many instances of intersense relations can be found in W7 that are not idiosyncratic, but rather exist among senses of many words. Those relations include Process/Result, Food/Plant, and Container/Volume. Virtual polysemy and recurring intersense relations are closely related to polymorphic senses that can support coercion in semantic typing under Putstejovsky&apos;s (1991) theory of the generative lexicon. 82 Chen and Chang Topical Clustering Dolan (1994) maintains the position that intersense relations are mostly idiosyncratical, thereby making it difficult to characterize them in a general way so as to identify them. The author cites the example of two senses of to moult, one a bird behavior and the other an animal behavior, to stress that polysemy primarily reflects fine distinctions that do not recur systematically throughout the English lexicon. However, our experimental results indicate that (a) it is exactly senses with fine distinction that are merged together and (b) there is a greater concentration of recurring intersense relations e</context>
<context position="63592" citStr="Dolan (1994)" startWordPosition="10712" endWordPosition="10713">called dictionary correlation kit (DCK) in a dialogue-based environment for correlating word senses across a pair of MRDs such as the LDOCE and the LLOCE. The approach taken in DCK is essentially a heuristic one, based on a correlation in the headwords, grammar codes, definition, and examples between the senses in LDOCE and LLOCE. The authors indicate that for the heuristics to yield optimum results, the degree of overlap in the examples should be weighted twice as heavily as all other factors. However, they do not elaborate on how the comparisons are done, or on how effective the program is. Dolan (1994) describes a heuristic approach to forming unlabeled clusters of closely related senses in an MRD. The clustering program relies on LDOCE domain code, grammar code, and 25 types of semantic relations extracted from definitions such as Hypernym, Location, Manner, Purpose, Part0f, and Ingredient0f. Matching two senses 83 Computational Linguistics Volume 24, Number 1 involves comparing any values that have been identified for each of the semantic relation types. The author reports that straightforwardly comparing the values of the same semantic relation types, particularly the Hypern yin relation</context>
<context position="68128" citStr="Dolan (1994)" startWordPosition="11427" endWordPosition="11428"> factors, coarse word sense division can be attributed primarily to the subject and topic. This is evident from the observation that very topical genus and differentiae show up in dictionary definitions in rather rigid patterns. Therefore, an MRD coupled with a thesaurus organized according to subjects and topics is very effective for acquisition of sense division for WSD. 84 Chen and Chang Topical Clustering In a broader context, this paper presents an approach to automatic construction of semantic lexicons through integration of lexicographic resources such as MRDs and thesauri. As noted in Dolan (1994), it is possible to run a sense-clustering algorithm on several MRDs to build an integrated lexical database with more complete coverage of word senses. If TopSense is run on several bilingual MRDs, there is a potential for creating an integrated multilingual lexicon enriched with thesaurus concepts as language-neutral signs to support knowledge-based machine translation. A similar idea has been put forward by Okumura and Hovy (1994). The TopSense algorithm&apos;s performance could definitely be improved by handling deictic, metonymic, and metaphoric sense definitions more appropriately Nevertheles</context>
</contexts>
<marker>Dolan, 1994</marker>
<rawString>Dolan, William B. 1994. Word sense disambiguation: Clustering related senses. In Proceedings of the 15th International Conference on Computational Linguistics, pages 712-716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>101--112</pages>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W., K. W. Church, and D. Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 101-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise Guthrie</author>
<author>Brian M Slator</author>
<author>Yorick Wilks</author>
<author>Rebecca Bruce</author>
</authors>
<title>Is there contents in empty heads?</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<pages>138--143</pages>
<contexts>
<context position="24679" citStr="Guthrie et al. 1990" startWordPosition="3903" endWordPosition="3906">nces of LLOCE in general capture the Generic/Specific relation; therefore, a sense definition is often disambiguated through the genus. Thus, the task of linking MRD and thesaurus senses is closely related to the extraction and 68 Chen and Chang Topical Clustering disambiguation of the genus. For instance, in the above example, linking bank.1.n.1 to bank-Ld099 has, as a by-product, the disambiguation of the genus land to land-Ld084 (Geography) rather than land-Ce078 (Social organization in groups and place). Details of extraction and disambiguation of the genus can be found in previous works (Guthrie et al. 1990; Klavans, Chodorow, and Wacholder 1990; Copestake 1990; Ageno et al. 1992). Disambiguated genus and differentiae terms can be used to construct a better taxonomy of word senses. Since the dictionary usually has broader coverage of word senses than the thesaurus, not all MRD senses of a headword h correspond to one of h&apos;s predefined senses in the thesaurus. For instance, LDOCE sense bank.1.n.3 (a mass of cloud, snow, or mud, etc.) corresponds to LLOCE topic Hb (Object generally) rather than any of the predefined LLOCE senses for bank. Therefore, such entries represent sense gaps in the thesaur</context>
</contexts>
<marker>Guthrie, Slator, Wilks, Bruce, 1990</marker>
<rawString>Guthrie, Louise, Brian M. Slator, Yorick Wilks, and Rebecca Bruce. 1990. Is there contents in empty heads? In Proceedings of the 13th International Conference on Computational Linguistics, pages 138-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Jenson</author>
<author>Jean Louis Binot</author>
</authors>
<title>Disambiguating prepositional phrase attachments by using on-line dictionary definitions.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<pages>13--3</pages>
<contexts>
<context position="9796" citStr="Jenson and Binot 1987" startWordPosition="1514" endWordPosition="1517">tention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of information listed in a dictionary entry, including part of speech, subcategory, examples, collocations, and typical arguments, which is potentially useful for WSD. In this regard, the LDOCE is particularly appropriate since it uses a reduced, controlled vocabulary of some 2,000 words to define over 60,000 word senses representing a comprehensive vocabulary and broad coverage of word senses. It is arguable that the dictionary division of senses for a given word is too finegrained, thus </context>
</contexts>
<marker>Jenson, Binot, 1987</marker>
<rawString>Jenson, Karen and Jean Louis Binot. 1987. Disambiguating prepositional phrase attachments by using on-line dictionary definitions. Computational Linguistics, 13(3-4):251-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Klavans</author>
<author>M S Chodorow</author>
<author>N Wacholder</author>
</authors>
<title>From dictionary to knowledge via taxonomy.</title>
<date>1990</date>
<booktitle>In Proceedings of the Sixth Conference of the University of Waterloo Centre for the New Oxford English Dictionary and Text Research: Electronic</booktitle>
<institution>Text Research, University of Waterloo,</institution>
<location>Waterloo, Canada.</location>
<marker>Klavans, Chodorow, Wacholder, 1990</marker>
<rawString>Klavans, J. L., M. S. Chodorow, and N. Wacholder. 1990. From dictionary to knowledge via taxonomy. In Proceedings of the Sixth Conference of the University of Waterloo Centre for the New Oxford English Dictionary and Text Research: Electronic Text Research, University of Waterloo, Waterloo, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Krovetz</author>
</authors>
<title>Sense-linking in a machine readable dictionary.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting,</booktitle>
<pages>330--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="53218" citStr="Krovetz (1992)" startWordPosition="9052" endWordPosition="9053"> slug 5 5 0 0 100% 100% space 8 7 1 0 100% 88% star 9 8 1 0 100% 90% suit 6 5 1 0 100% 83% table 7 6 1 0 100% 86% tank 3 3 0 0 100% 100% taste 6 5 0 1 83% 100% total 105 91 10 4 96% 90% of office, rank, honour, etc.) in the Dg class (Clothes and personal belongings). On the other hand, the metonymic meaning, Nb (Chance) of another star sense (a heavenly body regarded as determining one&apos;s fate) comes out second to the &amp;quot;primary&amp;quot; sense, La (Heavenly body). By considering cue phrases such as regarded as or as a mark of, we might be able to handle metaphoric and metonymic senses more successfully. Krovetz (1992) observes that the LDOCE indicates explicit sense shifts via the deictic reference, which is a link to the previous sense created by such terms as this, these, that, those, its, itself, such a, and such an. The author identifies many systematic sense shifts indicated by such references including Substance/Product (lemon, tree or fruit), Substance/Color (jade, amber), Object/Shape (pyramid), Animal/Food (chicken), Countnoun/Mass-noun (blasphemy), Language/People (Spanish), Animal/Skin-fur (crocodile), and Music/Dance (waltz). Such shifts indicated through a deictic reference are so pervasive in</context>
</contexts>
<marker>Krovetz, 1992</marker>
<rawString>Krovetz, Robert. 1992. Sense-linking in a machine readable dictionary. In Proceedings of the 30th Annual Meeting, pages 330-332. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
<author>W B Croft</author>
</authors>
<title>Lexical ambiguity and information retrieval.</title>
<date>1992</date>
<journal>ACM Transaction on Information Systems,</journal>
<pages>115--141</pages>
<contexts>
<context position="2008" citStr="Krovetz and Croft 1992" startWordPosition="302" endWordPosition="305">nts for a given query in information retrieval (IR) research. Relatively well-established IR techniques of weighting terms and ranking document relevancy are applied to find the topical clusters that are most relevant to the definition of each MRD sense. Finally, we describe an implemented version of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses </context>
</contexts>
<marker>Krovetz, Croft, 1992</marker>
<rawString>Krovetz, R. and W. B. Croft. 1992. Lexical ambiguity and information retrieval. ACM Transaction on Information Systems, pages 115-141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine-readable dictionaries: How to tell a pine cone from an ice-cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the ACM SIGDOC Conference,</booktitle>
<pages>24--26</pages>
<location>Toronto, Ontario,</location>
<contexts>
<context position="2252" citStr="Lesk 1986" startWordPosition="340" endWordPosition="341">nally, we describe an implemented version of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic, register, dialect, collocation, part of speech, and valency (McRoy 1992). Various approaches to word sense </context>
<context position="4197" citStr="Lesk (1986)" startWordPosition="624" endWordPosition="625">ublexical representation (Schtitze 1992), and (6) hand-crafted lexicons (McRoy 1992). This paper is motivated by the observation that directly using dictionary senses for sense division offers several advantages. Sense distinction according to a dictionary is readily available from machine-readable dictionaries (MRDs) such as the Longman Dictionary of Contemporary English (LDOCE) (Proctor 1978). A dictionary such as the LDOCE has broad coverage of word senses, useful for WSD. Furthermore, indicative words and concepts for each sense are directly available in numbered definitions and examples. Lesk (1986) describes the first MRD-based WSD method that relies on the extent of overlap between words in a dictionary definition and words in the local context of the word to be disambiguated. The author reports that WSD performance ranges from 50% to 70% and his method works well for senses strongly associated with specific collocations, such as ice-cream cone and pine cone. Unfortunately, using MRDs as the knowledge source for sense division and disambiguation leads to some problems. Zernik (1992) notes that the dictionary dichotomy of senses is inadequate for WSD, because it is defined along grammat</context>
<context position="54573" citStr="Lesk (1986)" startWordPosition="9256" endWordPosition="9257">s a Countnoun/Mass-noun shift from its previous sense issue.1.n.1 (the act of coming out) through the deictic reference of this. Since these specific patterns of definition are not taken into consideration in TopSense, the algorithm often fails in such cases. Further work must be undertaken to cope with direct and deictic references, so that such definitions can be appropriately clustered. 5.2 Clustered Definitions and Examples as a Knowledge Source for WSD Many studies have shown that MRD definitions and example sentences are a good knowledge source for WSD. As described in the introduction, Lesk (1986) shows that defining words are especially effective for disambiguating senses strongly associated 80 Chen and Chang Topical Clustering Table 14 Analysis of failure by error types. Error Type TopSense Output Sense Definition vague definition *Gd (communicating) long definition *Ca (people) metonynym * La (universe) short, vague definition *Ge (communication) *Bj (medicine) — (unknown) — (unknown) deictic reference *Hb (object) — (unknown) interest - an activity, subject, etc., which one gives time and attention to table - also multiplication table; a list which young children repeat to learn wh</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk, Michael E. 1986. Automatic sense disambiguation using machine-readable dictionaries: How to tell a pine cone from an ice-cream cone. In Proceedings of the ACM SIGDOC Conference, Toronto, Ontario, pages 24-26.</rawString>
</citation>
<citation valid="true">
<title>Longman Dictionary of Contemporary English.</title>
<date>1978</date>
<editor>Proctor, P., editor.</editor>
<publisher>Longman Group,</publisher>
<location>London.</location>
<marker>1978</marker>
<rawString>Proctor, P., editor. 1978. Longman Dictionary of Contemporary English. Longman Group, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alpha K Luk</author>
</authors>
<title>Statistical sense disambiguation with relatively small corpora using dictionary definitions.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting,</booktitle>
<pages>181--188</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2353" citStr="Luk 1995" startWordPosition="356" endWordPosition="357"> performance of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic, register, dialect, collocation, part of speech, and valency (McRoy 1992). Various approaches to word sense division have been proposed in the literature on WSD, including (1) sense numbers in every-day dictio</context>
<context position="5400" citStr="Luk 1995" startWordPosition="815" endWordPosition="816">ammatical, not semantic, lines. Furthermore, as pointed out in Dolan (1994), the sense division in an MRD is frequently too fine-grained for the purpose of WSD. A WSD system based on dictionary senses often faces unnecessary and difficult &amp;quot;forced-choices.&amp;quot; Dolan proposes a heuristic algorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the proposed algorithm was only described in a few examples and was not developed further. Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk 1995; Yarowsky 1995) still resort to human intervention to identify and group closely related senses in an MRD. Using thesaurus categories directly as a coarse sense division may seem to be a viable alternative (Yarowsky 1995). However, typical thesauri, such as Roget&apos;s Thesaurus (1987), suffer sense gaps and, occasionally, are too fine-grained. Yarowsky (1992) reports that there are uses not listed in Roget&apos;s for 3 of 12 nouns in his WSD study, while uses which a native speaker might consider as a single sense are often encoded in several Roget&apos;s categories. As an alternative approach to word sen</context>
<context position="6721" citStr="Luk 1995" startWordPosition="1026" endWordPosition="1027">cal information in a thesaurus. We refer to the algorithm as TopSense (Topical clustering of Senses). The current implementation of TopSense uses the topical information in the Longman Lexicon of Contemporary English (LLOCE) (McArthur 1992) to cluster LDOCE senses. The method makes use of none of the idiosyncratic information in either the LLOCE or the LDOCE. Therefore, the TopSense algorithm is quite general and is expected to produce comparable results for other MRDs and thesauri. TopSense is tested on 20 words extensively investigated in recent WSD literature (Schtitze 1992; Yarowsky 1992; Luk 1995). According to the experimental results, the automatically derived topical clusters can be used to good effect without any human intervention as a coarse sense division for WSD. The rest of the paper is organized as follows. Section 2 starts out with a description of the MRDs and thesauri used in the computational lexicography and WSD literature, followed by some observations to justify the topic-based approach to word sense division. Section 3 describes the LinkSense algorithm for linking senses between an MRD and a thesaurus. Section 4 shows how the TopSense algorithm based on the IR model m</context>
<context position="19939" citStr="Luk 1995" startWordPosition="3152" endWordPosition="3153">ood, to flow, etc. time, history, frequent, permanent, etc. start, stop, late, last, etc. ancient, modern, future, age, etc. day, night, second, minute, etc. now, soon, always, ever, after, etc. LLOCE People Buildin Materia Money Space Location Subject 0 0 Organization Government Owning Material Banking Geograph Travel Place 0 Topic • • • • --- C) Sets - - - So- cross-reference Figure 2 LLOCE&apos;s topical organization of word sense. cient for representing the distinction we would want to make for the task of WSD. Roget&apos;s has been used as the sense division in two recent WSD works (Yarowsky 1992; Luk 1995) more or less as is, except for a small number of senses added to fill gaps. We contend that a sense division based on the LLOCE topics will offer more or less the same kind of granularity, suitable for WSD. For instance, in Yarowsky (1992), the senses of star are divided into three Roget&apos;s categories, which roughly correspond to five LDOCE star senses labeled with LLOCE topics. In the same study, six Roget&apos;s categories are sufficient to distinguish the senses of slug. These six categories correspond to five relevant LLOCE topics. Table 7 provides further details. 2.3 Combining Word Sense Info</context>
<context position="55965" citStr="Luk (1995)" startWordPosition="9490" endWordPosition="9491">est - a readiness to give attention issue - the act of coming out issue - something which comes or is given out space - a quantity or bit of this for a particular purpose issue - an example of this with specific collocations, such as cone in ice-cream cone and pine cone. Wilks et al. (1990) call the defining words in the LDOCE definition semantic primitives (SP) and suggest that a semantic network constructed on the strength of co-occurrence of SPs in definitions can be useful for a variety of NLP tasks, ranging from WSD, to machine translation, to message understanding. Along the same lines, Luk (1995) terms SP the definition-based concept (DBC) and proposes using DBC co-occurrence (DBCC) trained on a large corpus to disambiguate word senses. However, the effectiveness of SPs or DBCs to represent a word sense and its indicative context is hampered by ambiguity and data sparseness. For instance, earth, one of the SPs in bank.1.n.2 is ambiguous (either as the planet Earth or soil) thus possibly leading to problems in WSD. Although these SPs are drawn from a small, controlled vocabulary in most MRDs, nevertheless, it is difficult to find SPs of a polysemous sense overlapping the SPs of its con</context>
</contexts>
<marker>Luk, 1995</marker>
<rawString>Luk, Alpha K. 1995. Statistical sense disambiguation with relatively small corpora using dictionary definitions. In Proceedings of the 33rd Annual Meeting, pages 181-188. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom McArthur</author>
</authors>
<date>1992</date>
<institution>Longman Lexicon of Contemporary English. Longman Group (Far East) Ltd., Hong Kong.</institution>
<contexts>
<context position="6352" citStr="McArthur 1992" startWordPosition="967" endWordPosition="968">ined. Yarowsky (1992) reports that there are uses not listed in Roget&apos;s for 3 of 12 nouns in his WSD study, while uses which a native speaker might consider as a single sense are often encoded in several Roget&apos;s categories. As an alternative approach to word sense division, this paper presents an algorithm capable of automatically clustering senses in an MRD based on topical information in a thesaurus. We refer to the algorithm as TopSense (Topical clustering of Senses). The current implementation of TopSense uses the topical information in the Longman Lexicon of Contemporary English (LLOCE) (McArthur 1992) to cluster LDOCE senses. The method makes use of none of the idiosyncratic information in either the LLOCE or the LDOCE. Therefore, the TopSense algorithm is quite general and is expected to produce comparable results for other MRDs and thesauri. TopSense is tested on 20 words extensively investigated in recent WSD literature (Schtitze 1992; Yarowsky 1992; Luk 1995). According to the experimental results, the automatically derived topical clusters can be used to good effect without any human intervention as a coarse sense division for WSD. The rest of the paper is organized as follows. Sectio</context>
</contexts>
<marker>McArthur, 1992</marker>
<rawString>McArthur, Tom. 1992. Longman Lexicon of Contemporary English. Longman Group (Far East) Ltd., Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McRoy</author>
</authors>
<title>Using multiple knowledge sources for word sense discrimination.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--1</pages>
<contexts>
<context position="2021" citStr="McRoy 1992" startWordPosition="306" endWordPosition="307"> information retrieval (IR) research. Relatively well-established IR techniques of weighting terms and ranking document relevancy are applied to find the topical clusters that are most relevant to the definition of each MRD sense. Finally, we describe an implemented version of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and</context>
<context position="3670" citStr="McRoy 1992" startWordPosition="549" endWordPosition="550"> senses (Dolan 1994; Bruce and Wiebe 1995; Luk * Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC. E-mail: dr818314@cs.nthu.edu.tw; jschang@cs.nthu.edu.tw. C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 1995), (3) thesaurus categories (Yarowsky 1992; Chen and Chang 1994), (4) translation in another language (Gale, Church, and Yarowsky 1992; Dagan, Itai, and Schwa11 1991; Dagan and Itai 1994), (5) automatically induced clusters with sublexical representation (Schtitze 1992), and (6) hand-crafted lexicons (McRoy 1992). This paper is motivated by the observation that directly using dictionary senses for sense division offers several advantages. Sense distinction according to a dictionary is readily available from machine-readable dictionaries (MRDs) such as the Longman Dictionary of Contemporary English (LDOCE) (Proctor 1978). A dictionary such as the LDOCE has broad coverage of word senses, useful for WSD. Furthermore, indicative words and concepts for each sense are directly available in numbered definitions and examples. Lesk (1986) describes the first MRD-based WSD method that relies on the extent of ov</context>
<context position="22658" citStr="McRoy (1992)" startWordPosition="3592" endWordPosition="3593">ion for this small set of words obviously is much easier than it would be for a large, open set. Second, dictionary definitions adhere to rather rigid patterns under which only words with predictable semantic relations show up. A dictionary definition, in general, begins with a genus term (that is, conceptual ancestor of the sense), followed by a set of differentiae that are words semantically related to the sense to provide the specifics. The semantic relations between the sense, the genus, and differentiae are reflected in what are termed categorical, functional, and situational clusters in McRoy (1992). The semantic relations and clusters have been shown to be very effective knowledge sources for such NLP tasks as WSD (McRoy 1992) and interpretation of noun sequences (Vanderwende 1994). For instance, in the first four definitions of bank in Table 1, the genus terms land, earth, mass, and slope are categorically related to the respective bank senses. On the other hand, the differentiae river, lake, field, garden, bend, road, and racetrack have a LocationOf situational relation with bank. Other differentiae, snow, cloud, and mud, are related functionally to bank.1.n.3 through the MakeOf relat</context>
</contexts>
<marker>McRoy, 1992</marker>
<rawString>McRoy, S. 1992. Using multiple knowledge sources for word sense discrimination. Computational Linguistics, 18(1):1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1993</date>
<tech>CSL 43,</tech>
<institution>Cognitive Science Laboratory, Princeton University,</institution>
<location>Princeton, NJ.</location>
<contexts>
<context position="12701" citStr="Miller et al. 1993" startWordPosition="2009" endWordPosition="2012"> one&apos;s money (esp. in the stated bank) Sense ID bank.1.n.1 bank.1.n.2 bank.1.n.3 bank.l.n.4 bank.l.n.5 bank.2.v.1 bank.3.n.1 bank.4.n.1 bank.4.n.2 bank.4.n.3 bank.5.v.1 bank.5.v.2 Note: Sense ID = Root + Homonym No. + Part-of-speech + Sense No. Table 2 Roget&apos;s semantic classes and categories. Class Categories Gloss for Classes A 1-182 Abstract relations • 183-318 Space • 319-446 Matter D 447-594 Intellect: the exercise of the mind • 595-816 Volition: the exercise of the will • 817-990 Emotion, religion and morality coarse semantic categories. We briefly describe the on-line thesauri, WordNet (Miller et al. 1993), Roget&apos;s Thesaurus, and LLOCE, which have been used as word sense divisions in the computational linguistics literature. WordNet is organized as a set of hierarchical, conceptual taxonomies of nouns, verbs, adjectives, and adverbs called synsets. The synsets are too fine-grained from the WSD perspective; WordNet contains 24,825 noun synsets for 32,264 distinct nouns with a total of 43,136 senses in its noun taxonomy alone. It would be difficult to acquire WSD knowledge for making such fine distinctions even from a substantial body of training materials. Roget&apos;s Thesaurus arranges words in a t</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1993. Introduction to WordNet: An on-line lexical database. CSL 43, Cognitive Science Laboratory, Princeton University, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Montemagni</author>
<author>L Vanderwende</author>
</authors>
<title>Structural pattern vs. string pattern for extracting semantic information from dictionaries.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>546--552</pages>
<contexts>
<context position="9507" citStr="Montemagni and Vanderwende 1992" startWordPosition="1471" endWordPosition="1474">ith the granularity of a thesaurus. Therefore, a short description of MRDs and thesauri is in order. 2.1 Fine-Grained Senses in an MRD Interest in MRD-based research has increased over the years; in particular, the LDOCE and Webster&apos;s Seventh New Collegiate Dictionary (W7) (1967) have drawn much attention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of information listed in a dictionary entry, including part of speech, subcategory, examples, collocations, and typical arguments, which is potentially useful for WSD. In this regard, the LDOCE is particul</context>
</contexts>
<marker>Montemagni, Vanderwende, 1992</marker>
<rawString>Montemagni, S. and L. Vanderwende. 1992. Structural pattern vs. string pattern for extracting semantic information from dictionaries. In Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 546-552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An examplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting,</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="2370" citStr="Ng and Lee 1996" startWordPosition="358" endWordPosition="361">ce of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic, register, dialect, collocation, part of speech, and valency (McRoy 1992). Various approaches to word sense division have been proposed in the literature on WSD, including (1) sense numbers in every-day dictionaries (Lesk 1986</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An examplar-based approach. In Proceedings of the 34th Annual Meeting, pages 40-47, Santa Cruz, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Okumura</author>
<author>Eduard Hovy</author>
</authors>
<title>Lexicon-to-ontology concept association using a bilingual dictionary.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americans, Chen and Chang Topical Clustering</booktitle>
<pages>177--184</pages>
<location>Columbia, MD.</location>
<contexts>
<context position="68565" citStr="Okumura and Hovy (1994)" startWordPosition="11491" endWordPosition="11494">context, this paper presents an approach to automatic construction of semantic lexicons through integration of lexicographic resources such as MRDs and thesauri. As noted in Dolan (1994), it is possible to run a sense-clustering algorithm on several MRDs to build an integrated lexical database with more complete coverage of word senses. If TopSense is run on several bilingual MRDs, there is a potential for creating an integrated multilingual lexicon enriched with thesaurus concepts as language-neutral signs to support knowledge-based machine translation. A similar idea has been put forward by Okumura and Hovy (1994). The TopSense algorithm&apos;s performance could definitely be improved by handling deictic, metonymic, and metaphoric sense definitions more appropriately Nevertheless, the algorithm already produces clustered MRD sense entries that not only are exploitable as a workable sense division but also are likely to be an effective knowledge source for many NLP tasks related to semantic processing, such as WSD. In summary, this paper presents a functional core for automatic construction of the semantic lexicon. Appendix The following table shows the experimental results of running TopSense on the LDOCE s</context>
</contexts>
<marker>Okumura, Hovy, 1994</marker>
<rawString>Okumura, A. and Eduard Hovy. 1994. Lexicon-to-ontology concept association using a bilingual dictionary. In Proceedings of the First Conference of the Association for Machine Translation in the Americans, Chen and Chang Topical Clustering pages 177-184. Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Ostler</author>
<author>B T S Atkins</author>
</authors>
<title>Predictable meaning shift: Some linguistic properties of lexical implication rules.</title>
<date>1991</date>
<booktitle>In Proceedings of the 1991 ACL Workshop on Lexical Semantics and Knowledge Representation,</booktitle>
<pages>76--87</pages>
<contexts>
<context position="59661" citStr="Ostler and Atkins (1991)" startWordPosition="10095" endWordPosition="10098">on, slope, bend, road, race-track, safe, car, go round, sandbank, large, area, land, thick, cover, tree, bush, grow, wild, plant, purpose,.. VLd {sea (65.81), land (55.39), mountain (46.74), water (44.76), river (36.71), lake (27.88), earth (25.76), tree (21.87),.. Notice that for this example, the relevant VD is now large enough to overlap the contextual information; the term tree appears in SP (wood.1.n.1) as well as the relevant document VLd. Although the relevant VLd is very large, it contains mostly words that are nevertheless consistently related to geography. 5.3 Systematic Sense Shift Ostler and Atkins (1991) contend that there is strong evidence to suggest that a large part of word sense ambiguity is not arbitrary but follows regular patterns. Moreover, gaps frequently arise in dictionaries and thesauri in specifying this kind of polysemy. Encoding regularity of the extended usage of a sense makes it possible to resolve word sense ambiguity for word entries that are underspecified in this respect. This socalled virtual polysemy can be illustrated through some examples. For instance, many verbs for moving and action, such as move and strike, can be used polysemously in the sense of emotion. Chodor</context>
</contexts>
<marker>Ostler, Atkins, 1991</marker>
<rawString>Ostler, Nicholas and B. T. S. Atkins. 1991. Predictable meaning shift: Some linguistic properties of lexical implication rules. In Proceedings of the 1991 ACL Workshop on Lexical Semantics and Knowledge Representation, pages 76-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Putstejovsky</author>
</authors>
<title>The generative lexicon.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--4</pages>
<marker>Putstejovsky, 1991</marker>
<rawString>Putstejovsky, James. 1991. The generative lexicon. Computational Linguistics, (17)4:409-441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Putstejovsky</author>
<author>Pierrette Bouillon</author>
</authors>
<title>On the proper role of coercion in semantic typing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>706--711</pages>
<contexts>
<context position="62669" citStr="Putstejovsky and Bouillon (1994)" startWordPosition="10554" endWordPosition="10557">ences are systematic intersense relations similar to the abovementioned Food/Plant relation. Indeed, words involved in such intersense relations are frequently underspecified. For instance, chicken is listed under both topic Eb and topic Ad, while duck is listed under Ad but not Eb. By characterizing some 200 cross-references in LLOCE, most systematic sense shifts can be easily identified among the senses across topical clusters. The topical clusters of MRD senses, coupled with the topical description of sense-shift knowledge, can support and realize automatic sense extension, as advocated in Putstejovsky and Bouillon (1994), and prevent a proliferation of senses in the semantic lexicon. For instance, the sense of duck in the Ad cluster can be coerced into an Eb sense, in some context, based on the knowledge of a systematic sense shift from Ad (Birds) to Eb (Food). 6. Other Approaches Sanfilippo and Poznanski (1992) propose a so-called dictionary correlation kit (DCK) in a dialogue-based environment for correlating word senses across a pair of MRDs such as the LDOCE and the LLOCE. The approach taken in DCK is essentially a heuristic one, based on a correlation in the headwords, grammar codes, definition, and exam</context>
</contexts>
<marker>Putstejovsky, Bouillon, 1994</marker>
<rawString>Putstejovsky, James and Pierrette Bouillon. 1994. On the proper role of coercion in semantic typing. In Proceedings of the 15th International Conference on Computational Linguistics, pages 706-711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yael Ravin</author>
</authors>
<title>Disambiguating and interpreting verb definitions.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting,</booktitle>
<pages>260--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9737" citStr="Ravin 1990" startWordPosition="1508" endWordPosition="1509">egiate Dictionary (W7) (1967) have drawn much attention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of information listed in a dictionary entry, including part of speech, subcategory, examples, collocations, and typical arguments, which is potentially useful for WSD. In this regard, the LDOCE is particularly appropriate since it uses a reduced, controlled vocabulary of some 2,000 words to define over 60,000 word senses representing a comprehensive vocabulary and broad coverage of word senses. It is arguable that the dictionary di</context>
</contexts>
<marker>Ravin, 1990</marker>
<rawString>Ravin, Yael. 1990. Disambiguating and interpreting verb definitions. In Proceedings of the 28th Annual Meeting, pages 260-267. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>Roget&apos;s Thesaurus of English Words and Phrases.</title>
<date>1987</date>
<publisher>Longman Group UK Limited,</publisher>
<location>London.</location>
<contexts>
<context position="5683" citStr="(1987)" startWordPosition="860" endWordPosition="860">lgorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the proposed algorithm was only described in a few examples and was not developed further. Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk 1995; Yarowsky 1995) still resort to human intervention to identify and group closely related senses in an MRD. Using thesaurus categories directly as a coarse sense division may seem to be a viable alternative (Yarowsky 1995). However, typical thesauri, such as Roget&apos;s Thesaurus (1987), suffer sense gaps and, occasionally, are too fine-grained. Yarowsky (1992) reports that there are uses not listed in Roget&apos;s for 3 of 12 nouns in his WSD study, while uses which a native speaker might consider as a single sense are often encoded in several Roget&apos;s categories. As an alternative approach to word sense division, this paper presents an algorithm capable of automatically clustering senses in an MRD based on topical information in a thesaurus. We refer to the algorithm as TopSense (Topical clustering of Senses). The current implementation of TopSense uses the topical information i</context>
</contexts>
<marker>1987</marker>
<rawString>Roget&apos;s Thesaurus of English Words and Phrases. 1987. Longman Group UK Limited, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sanfilippo</author>
<author>V Poznanski</author>
</authors>
<title>The acquisition of lexical knowledge from combined machine-readable dictionary sources.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd Conference on Applied Natural Language Processing (ANLP-92),</booktitle>
<pages>80--87</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="62966" citStr="Sanfilippo and Poznanski (1992)" startWordPosition="10606" endWordPosition="10609">ing some 200 cross-references in LLOCE, most systematic sense shifts can be easily identified among the senses across topical clusters. The topical clusters of MRD senses, coupled with the topical description of sense-shift knowledge, can support and realize automatic sense extension, as advocated in Putstejovsky and Bouillon (1994), and prevent a proliferation of senses in the semantic lexicon. For instance, the sense of duck in the Ad cluster can be coerced into an Eb sense, in some context, based on the knowledge of a systematic sense shift from Ad (Birds) to Eb (Food). 6. Other Approaches Sanfilippo and Poznanski (1992) propose a so-called dictionary correlation kit (DCK) in a dialogue-based environment for correlating word senses across a pair of MRDs such as the LDOCE and the LLOCE. The approach taken in DCK is essentially a heuristic one, based on a correlation in the headwords, grammar codes, definition, and examples between the senses in LDOCE and LLOCE. The authors indicate that for the heuristics to yield optimum results, the degree of overlap in the examples should be weighted twice as heavily as all other factors. However, they do not elaborate on how the comparisons are done, or on how effective th</context>
</contexts>
<marker>Sanfilippo, Poznanski, 1992</marker>
<rawString>Sanfilippo, A. and V. Poznanski. 1992. The acquisition of lexical knowledge from combined machine-readable dictionary sources. In Proceedings of the 3rd Conference on Applied Natural Language Processing (ANLP-92), pages 80-87, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Word sense disambiguation with sublexical representations.</title>
<date>1992</date>
<booktitle>In Proceedings of the 1992 AAAI Workshop on Statistically-based Natural Language Programming Techniques,</booktitle>
<pages>100--104</pages>
<marker>Schutze, 1992</marker>
<rawString>Schutze, Hinrich. 1992. Word sense disambiguation with sublexical representations. In Proceedings of the 1992 AAAI Workshop on Statistically-based Natural Language Programming Techniques, pages 100-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
</authors>
<title>Interpretation of noun sequences.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>454--460</pages>
<contexts>
<context position="9698" citStr="Vanderwende 1994" startWordPosition="1502" endWordPosition="1503">lar, the LDOCE and Webster&apos;s Seventh New Collegiate Dictionary (W7) (1967) have drawn much attention. Much of the MRD-based research has focused on the analysis and exploitation of the sense definitions in MRDs (Amsler 1984a, 1984b, 1987; Alshawi 1987; Alshawi, Boguraev, and Carter 1989; Vossen, Meijs, and denBroeder 1989). In these works, the definitions are analyzed using either a parser (Montemagni and Vanderwende 1992) or a pattern matcher (Ahlswede and Evens 1988) into semantic relations. These relations are then used for various tasks, ranging from the interpretation of a noun sequence (Vanderwende 1994) or a prepositional phrase (Ravin 1990), to resolving structural ambiguity (Jenson and Binot 1987), to merging dictionary senses for WSD (Dolan 1994). Besides the definition itself, there is an abundance of information listed in a dictionary entry, including part of speech, subcategory, examples, collocations, and typical arguments, which is potentially useful for WSD. In this regard, the LDOCE is particularly appropriate since it uses a reduced, controlled vocabulary of some 2,000 words to define over 60,000 word senses representing a comprehensive vocabulary and broad coverage of word senses</context>
<context position="22845" citStr="Vanderwende 1994" startWordPosition="3621" endWordPosition="3622"> with predictable semantic relations show up. A dictionary definition, in general, begins with a genus term (that is, conceptual ancestor of the sense), followed by a set of differentiae that are words semantically related to the sense to provide the specifics. The semantic relations between the sense, the genus, and differentiae are reflected in what are termed categorical, functional, and situational clusters in McRoy (1992). The semantic relations and clusters have been shown to be very effective knowledge sources for such NLP tasks as WSD (McRoy 1992) and interpretation of noun sequences (Vanderwende 1994). For instance, in the first four definitions of bank in Table 1, the genus terms land, earth, mass, and slope are categorically related to the respective bank senses. On the other hand, the differentiae river, lake, field, garden, bend, road, and racetrack have a LocationOf situational relation with bank. Other differentiae, snow, cloud, and mud, are related functionally to bank.1.n.3 through the MakeOf relation. Third, for the most part these relations are captured implicitly in a typical thesaurus. The LLOCE and Roget&apos;s conveniently contain information on the relations in the form of word l</context>
</contexts>
<marker>Vanderwende, 1994</marker>
<rawString>Vanderwende, L. 1994. Interpretation of noun sequences. In Proceedings of the 15th International Conference on Computational Linguistics, pages 454-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>W Meijs</author>
<author>M den Broeder</author>
</authors>
<title>Meaning and structure in dictionary definitions.</title>
<date>1989</date>
<booktitle>In Computational Lexicography for Natural Language Processing. Branimir Boguraev</booktitle>
<pages>171--190</pages>
<editor>and Ted Briscoe, editors,</editor>
<publisher>Longman Group UK Limited,</publisher>
<location>London,</location>
<marker>Vossen, Meijs, den Broeder, 1989</marker>
<rawString>Vossen, P., W. Meijs, and M. den Broeder. 1989. Meaning and structure in dictionary definitions. In Computational Lexicography for Natural Language Processing. Branimir Boguraev and Ted Briscoe, editors, Longman Group UK Limited, London, pages 171-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C</author>
<author>C Merriam</author>
</authors>
<title>Webster&apos;s Seventh New Collegiate Dictionary.</title>
<date>1967</date>
<location>Springfield, MA.</location>
<marker>C, Merriam, 1967</marker>
<rawString>Webster&apos;s Seventh New Collegiate Dictionary. 1967. C. and C. Merriam Company, Springfield, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A Wilks</author>
<author>D C Fass</author>
<author>C Ming Guo</author>
<author>J E McDonald</author>
<author>T Plate</author>
<author>B M Slator</author>
</authors>
<title>Providing tractable dictionary tools.</title>
<date>1990</date>
<journal>Machine Translation,</journal>
<pages>5--99</pages>
<contexts>
<context position="55646" citStr="Wilks et al. (1990)" startWordPosition="9437" endWordPosition="9440">n activity, subject, etc., which one gives time and attention to table - also multiplication table; a list which young children repeat to learn what number results when a number from 1 to 12 is multiplied by any of the numbers from 1 to 12 star - a heavenly body regarded as determining one&apos;s fate suit - a set (of armour) interest - a readiness to give attention issue - the act of coming out issue - something which comes or is given out space - a quantity or bit of this for a particular purpose issue - an example of this with specific collocations, such as cone in ice-cream cone and pine cone. Wilks et al. (1990) call the defining words in the LDOCE definition semantic primitives (SP) and suggest that a semantic network constructed on the strength of co-occurrence of SPs in definitions can be useful for a variety of NLP tasks, ranging from WSD, to machine translation, to message understanding. Along the same lines, Luk (1995) terms SP the definition-based concept (DBC) and proposes using DBC co-occurrence (DBCC) trained on a large corpus to disambiguate word senses. However, the effectiveness of SPs or DBCs to represent a word sense and its indicative context is hampered by ambiguity and data sparsene</context>
</contexts>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1990</marker>
<rawString>Wilks, Y. A., D. C. Fass, C. Ming Guo, J. E. McDonald, T. Plate, and B. M. Slator. 1990. Providing tractable dictionary tools. Machine Translation, 5:99-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Alistair Moffat</author>
<author>Timothy C Bell</author>
</authors>
<title>Managing Gigabytes. Van Nostrand Reinhold,</title>
<date>1994</date>
<location>New York.</location>
<marker>Witten, Moffat, Bell, 1994</marker>
<rawString>Witten, Ian H., Alistair Moffat, and Timothy C. Bell. 1994. Managing Gigabytes. Van Nostrand Reinhold, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>454--460</pages>
<location>Nantes, France.</location>
<contexts>
<context position="2152" citStr="Yarowsky 1992" startWordPosition="326" endWordPosition="327">y are applied to find the topical clusters that are most relevant to the definition of each MRD sense. Finally, we describe an implemented version of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations. 1. Introduction Word sense disambiguation (WSD) has been found useful in many natural language processing (NLP) applications, including information retrieval (Krovetz and Croft 1992; McRoy 1992), machine translation (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994), and speech synthesis (Yarowsky 1992). WSD has received increasing attention in recent literature on computational linguistics (Lesk 1986; Schtitze 1992; Gale, Church, and Yarowsky 1992; Yarowsky 1992, 1995; Bruce and Wiebe 1995; Luk 1995; Ng and Lee 1996; Chang et al. 1996). Given a polysemous word in running text, the task of WSD involves examining contextual information to determine the intended sense from a set of predetermined candidates. It is a nontrivial task to divide the senses of a word and determine this set, for word sense is an abstract concept frequently based on subjective and subtle distinctions in topic, registe</context>
<context position="3398" citStr="Yarowsky 1992" startWordPosition="511" endWordPosition="512">speech, and valency (McRoy 1992). Various approaches to word sense division have been proposed in the literature on WSD, including (1) sense numbers in every-day dictionaries (Lesk 1986; Cowie, Guthrie, and Guthrie 1992), (2) automatic or hand-crafted clusters of dictionary senses (Dolan 1994; Bruce and Wiebe 1995; Luk * Department of Computer Science, National Tsing Hua University, Hsinchu 30043, Taiwan, ROC. E-mail: dr818314@cs.nthu.edu.tw; jschang@cs.nthu.edu.tw. C) 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 1995), (3) thesaurus categories (Yarowsky 1992; Chen and Chang 1994), (4) translation in another language (Gale, Church, and Yarowsky 1992; Dagan, Itai, and Schwa11 1991; Dagan and Itai 1994), (5) automatically induced clusters with sublexical representation (Schtitze 1992), and (6) hand-crafted lexicons (McRoy 1992). This paper is motivated by the observation that directly using dictionary senses for sense division offers several advantages. Sense distinction according to a dictionary is readily available from machine-readable dictionaries (MRDs) such as the Longman Dictionary of Contemporary English (LDOCE) (Proctor 1978). A dictionary </context>
<context position="5759" citStr="Yarowsky (1992)" startWordPosition="869" endWordPosition="870">n the LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the proposed algorithm was only described in a few examples and was not developed further. Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk 1995; Yarowsky 1995) still resort to human intervention to identify and group closely related senses in an MRD. Using thesaurus categories directly as a coarse sense division may seem to be a viable alternative (Yarowsky 1995). However, typical thesauri, such as Roget&apos;s Thesaurus (1987), suffer sense gaps and, occasionally, are too fine-grained. Yarowsky (1992) reports that there are uses not listed in Roget&apos;s for 3 of 12 nouns in his WSD study, while uses which a native speaker might consider as a single sense are often encoded in several Roget&apos;s categories. As an alternative approach to word sense division, this paper presents an algorithm capable of automatically clustering senses in an MRD based on topical information in a thesaurus. We refer to the algorithm as TopSense (Topical clustering of Senses). The current implementation of TopSense uses the topical information in the Longman Lexicon of Contemporary English (LLOCE) (McArthur 1992) to clu</context>
<context position="19928" citStr="Yarowsky 1992" startWordPosition="3150" endWordPosition="3151">, sea, lake, flood, to flow, etc. time, history, frequent, permanent, etc. start, stop, late, last, etc. ancient, modern, future, age, etc. day, night, second, minute, etc. now, soon, always, ever, after, etc. LLOCE People Buildin Materia Money Space Location Subject 0 0 Organization Government Owning Material Banking Geograph Travel Place 0 Topic • • • • --- C) Sets - - - So- cross-reference Figure 2 LLOCE&apos;s topical organization of word sense. cient for representing the distinction we would want to make for the task of WSD. Roget&apos;s has been used as the sense division in two recent WSD works (Yarowsky 1992; Luk 1995) more or less as is, except for a small number of senses added to fill gaps. We contend that a sense division based on the LLOCE topics will offer more or less the same kind of granularity, suitable for WSD. For instance, in Yarowsky (1992), the senses of star are divided into three Roget&apos;s categories, which roughly correspond to five LDOCE star senses labeled with LLOCE topics. In the same study, six Roget&apos;s categories are sufficient to distinguish the senses of slug. These six categories correspond to five relevant LLOCE topics. Table 7 provides further details. 2.3 Combining Word</context>
<context position="64617" citStr="Yarowsky (1992)" startWordPosition="10872" endWordPosition="10873">n identified for each of the semantic relation types. The author reports that straightforwardly comparing the values of the same semantic relation types, particularly the Hypern yin relation, for two senses would be quite effective. In addition to such a comparison, a number of &amp;quot;scrambled&amp;quot; comparisons between values of different types of semantic relations are also helpful. For instance, in comparing the two senses of coffee, the value &amp;quot;drink&amp;quot; in the sense, &amp;quot;the coffee as a drink&amp;quot; is compared with that of the Ingredient Of relation in another sense, &amp;quot;the powder as an ingredient of the drink.&amp;quot; Yarowsky (1992) describes a WSD method and an implementation based on Roget&apos;s Thesaurus and a very large corpus, the 10-million-word Grolier&apos;s Encyclopedia. He suggests that the method can be applied to disambiguation and merging of MRD definitions as well, and gives the results of applying the method to the senses of the word crane for the COBUILD and Collins dictionaries using Roget&apos;s categories as an example. It is not known how the method fares for words other than crane. Contrary to our approach, the method requires substantial data for training. In most of the above-mentioned works, experimental result</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, David. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. In Proceedings of the 14th International Conference on Computational Linguistics, pages 454-460, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5416" citStr="Yarowsky 1995" startWordPosition="817" endWordPosition="818"> not semantic, lines. Furthermore, as pointed out in Dolan (1994), the sense division in an MRD is frequently too fine-grained for the purpose of WSD. A WSD system based on dictionary senses often faces unnecessary and difficult &amp;quot;forced-choices.&amp;quot; Dolan proposes a heuristic algorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the proposed algorithm was only described in a few examples and was not developed further. Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk 1995; Yarowsky 1995) still resort to human intervention to identify and group closely related senses in an MRD. Using thesaurus categories directly as a coarse sense division may seem to be a viable alternative (Yarowsky 1995). However, typical thesauri, such as Roget&apos;s Thesaurus (1987), suffer sense gaps and, occasionally, are too fine-grained. Yarowsky (1992) reports that there are uses not listed in Roget&apos;s for 3 of 12 nouns in his WSD study, while uses which a native speaker might consider as a single sense are often encoded in several Roget&apos;s categories. As an alternative approach to word sense division, thi</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting, pages 189-196. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Zernik</author>
</authors>
<title>Trainl vs. Train2: Tagging word senses in corpus.</title>
<date>1992</date>
<pages>91--112</pages>
<editor>In U. Zemik, editor, Lexical</editor>
<contexts>
<context position="4692" citStr="Zernik (1992)" startWordPosition="704" endWordPosition="705"> indicative words and concepts for each sense are directly available in numbered definitions and examples. Lesk (1986) describes the first MRD-based WSD method that relies on the extent of overlap between words in a dictionary definition and words in the local context of the word to be disambiguated. The author reports that WSD performance ranges from 50% to 70% and his method works well for senses strongly associated with specific collocations, such as ice-cream cone and pine cone. Unfortunately, using MRDs as the knowledge source for sense division and disambiguation leads to some problems. Zernik (1992) notes that the dictionary dichotomy of senses is inadequate for WSD, because it is defined along grammatical, not semantic, lines. Furthermore, as pointed out in Dolan (1994), the sense division in an MRD is frequently too fine-grained for the purpose of WSD. A WSD system based on dictionary senses often faces unnecessary and difficult &amp;quot;forced-choices.&amp;quot; Dolan proposes a heuristic algorithm for forming unlabeled clusters of closely related senses in the LDOCE to eliminate distinctions that are unnecessarily fine for WSD. Regrettably, the proposed algorithm was only described in a few examples </context>
</contexts>
<marker>Zernik, 1992</marker>
<rawString>Zernik, Uri. 1992. Trainl vs. Train2: Tagging word senses in corpus. In U. Zemik, editor, Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon. Lawrence Erlbaum Associates, pages 91-112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>