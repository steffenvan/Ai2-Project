<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.989193">
Interactive Topic Modeling
</title>
<author confidence="0.998824">
Yuening Hu Jordan Boyd-Graber Brianna Satinoff
</author>
<affiliation confidence="0.998935">
Department of Computer Science iSchool Department of Computer Science
University of Maryland University of Maryland University of Maryland
</affiliation>
<email confidence="0.998389">
ynhu@cs.umd.edu jbg@umiacs.umd.edu bsonrisa@cs.umd.edu
</email>
<sectionHeader confidence="0.993896" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99922855">
Topic models have been used extensively as a
tool for corpus exploration, and a cottage in-
dustry has developed to tweak topic models
to better encode human intuitions or to better
model data. However, creating such extensions
requires expertise in machine learning unavail-
able to potential end-users of topic modeling
software. In this work, we develop a frame-
work for allowing users to iteratively refine
the topics discovered by models such as la-
tent Dirichlet allocation (LDA) by adding con-
straints that enforce that sets of words must ap-
pear together in the same topic. We incorporate
these constraints interactively by selectively
removing elements in the state of a Markov
Chain used for inference; we investigate a va-
riety of methods for incorporating this infor-
mation and demonstrate that these interactively
added constraints improve topic usefulness for
simulated and actual user sessions.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995468125">
Probabilistic topic models, as exemplified by prob-
abilistic latent semantic indexing (Hofmann, 1999)
and latent Dirichlet allocation (LDA) (Blei et al.,
2003) are unsupervised statistical techniques to dis-
cover the thematic topics that permeate a large cor-
pus of text documents. Topic models have had con-
siderable application beyond natural language pro-
cessing in computer vision (Rob et al., 2005), bi-
ology (Shringarpure and Xing, 2008), and psychol-
ogy (Landauer et al., 2006) in addition to their canon-
ical application to text.
For text, one of the few real-world applications
of topic models is corpus exploration. Unannotated,
noisy, and ever-growing corpora are the norm rather
than the exception, and topic models offer a way to
quickly get the gist a large corpus.1
</bodyText>
<footnote confidence="0.98002">
1For examples, see Rexa http://rexa.info/, JSTOR
</footnote>
<bodyText confidence="0.976922108108108">
Contrary to the impression given by the tables
shown in topic modeling papers, topics discovered
by topic modeling don’t always make sense to os-
tensible end users. Part of the problem is that the
objective function of topic models doesn’t always cor-
relate with human judgements (Chang et al., 2009).
Another issue is that topic models — with their bag-
of-words vision of the world — simply lack the nec-
essary information to create the topics as end-users
expect.
There has been a thriving cottage industry adding
more and more information to topic models to cor-
rect these shortcomings; either by modeling perspec-
tive (Paul and Girju, 2010; Lin et al., 2006), syn-
tax (Wallach, 2006; Gruber et al., 2007), or author-
ship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Sim-
ilarly, there has been an effort to inject human knowl-
edge into topic models (Boyd-Graber et al., 2007;
Andrzejewski et al., 2009; Petterson et al., 2010).
However, these are a priori fixes. They don’t help
a frustrated consumer of topic models staring at a
collection of topics that don’t make sense. In this
paper, we propose interactive topic modeling (ITM),
an in situ method for incorporating human knowl-
edge into topic models. In Section 2, we review prior
work on creating probabilistic models that incorpo-
rate human knowledge, which we extend in Section 3
to apply to ITM sessions. Section 4 discusses the
implementation of this process during the inference
process. Via a motivating example in Section 5, simu-
lated ITM sessions in Section 6, and a real interactive
test in Section 7, we demonstrate that our approach is
able to focus a user’s desires in a topic model, better
capture the key properties of a corpus, and capture
diverse interests from users on the web.
http://showcase.jstor.org/blei/, and the NIH
https://app.nihmaps.org/nih/.
</bodyText>
<page confidence="0.93886">
248
</page>
<note confidence="0.986921">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 248–257,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.543244" genericHeader="introduction">
2 Putting Knowledge in Topic Models
</sectionHeader>
<bodyText confidence="0.999971511627907">
At a high level, topic models such as LDA take as
input a number of topics K and a corpus. As output,
a topic model discovers K distributions over words
— the namesake topics — and associations between
documents and topics. In LDA both of these out-
puts are multinomial distributions; typically they are
presented to users in summary form by listing the
elements with highest probability. For an example
of topics discovered from a 20-topic model of New
York Times editorials, see Table 1.
When presented with poor topics learned from
data, users can offer a number of complaints:2
these documents should have similar topics but
don’t (Daum´e III, 2009); this topic should have syn-
tactic coherence (Gruber et al., 2007; Boyd-Graber
and Blei, 2008); this topic doesn’t make any sense
at all (Newman et al., 2010); this topic shouldn’t be
associated with this document but is (Ramage et al.,
2009); these words shouldn’t be the in same topic
but are (Andrzejewski et al., 2009); or these words
should be in the same topic but aren’t (Andrzejewski
et al., 2009).
Many of these complaints can be addressed by
using “must-link” constraints on topics, retaining An-
drzejewski et al’s (2009) terminology borrowed from
the database literature. A “must-link” constraint is a
group of words whose probability must be correlated
in the topic. For example, Figure 1 shows an example
constraint: {plant, factory}. After this constraint is
added, the probabilities of “plant” and “factory” in
each topic are likely to both be high or both be low.
It’s unlikely for “plant” to have high probability in a
topic and “factory” to have a low probability. In the
next section, we demonstrate how such constraints
can be built into a model and how they can even be
added while inference is underway.
In this paper, we view constraints as transitive; if
“plant” is in a constraint with “factory” and “factory”
is in a constraint with “production,” then “plant” is
in a constraint with “production.” Making this as-
sumption can simplify inference slightly, which we
take advantage of in Section 3.1, but the real reason
for this assumption is because not doing so would
</bodyText>
<footnote confidence="0.989848">
2Citations in this litany of complaints are offline solutions for
addressing the problem; the papers also give motivation why
such complaints might arise.
</footnote>
<figure confidence="0.5352665">
Constraints Prior Structure
plant factory
</figure>
<figureCaption confidence="0.999427818181818">
Figure 1: How adding constraints (left) creates new topic
priors (right). The trees represent correlated distributions
(assuming 77 &gt;&gt; 0). After the {plant, factory} constraint
is added, it is now highly unlikely for a topic drawn from
the distribution to have a high probability for “plant” and
a low probability for “factory” or vice versa. The bottom
panel adds an additional constraint, so now dog-related
words are also correlated. Notice that the two constraints
themselves are uncorrelated. It’s possible for both, either,
or none of “bark” and “plant” (for instance) to have high
probability in a topic.
</figureCaption>
<bodyText confidence="0.9984526">
introduce ambiguity over the path associated with an
observed token in the generative process. As long as
a word is either in a single constraint or in the general
vocabulary, there is only a single path. The details of
this issue are further discussed in Section 4.
</bodyText>
<sectionHeader confidence="0.989322" genericHeader="method">
3 Constraints Shape Topics
</sectionHeader>
<bodyText confidence="0.9998713125">
As discussed above, LDA views topics as distribu-
tions over words, and each document expresses an
admixture of these topics. For “vanilla” LDA (no con-
straints), these are symmetric Dirichlet distributions.
A document is composed of a number of observed
words, which we call tokens to distinguish specific
observations from the more abstract word (type) as-
sociated with each token. Because LDA assumes
a document’s tokens are interchangeable, it treats
the document as a bag-of-words, ignoring potential
relations between words.
This problem with vanilla LDA can be solved by
encoding constraints, which will “guide” different
words into the same topic. Constraints can be added
to vanilla LDA by replacing the multinomial distri-
bution over words for each topic with a collection of
</bodyText>
<figure confidence="0.995449333333333">
{}
6 6 6 6 6 6
bark dog tree plant factory leash
{plant, factory}
6 6 6
266
bark dog tree
leash
n
n
{plant, factory}
{dog, bark, leash}
36
6 26
tree
n n
n nn
bark dog leash plant factory
</figure>
<page confidence="0.997828">
249
</page>
<bodyText confidence="0.999675838709677">
tree-structured multinomial distributions drawn from
a prior as depicted in Figure 1. By encoding word
distributions as a tree, we can preserve conjugacy
and relatively simple inference while encouraging
correlations between related concepts (Boyd-Graber
et al., 2007; Andrzejewski et al., 2009; Boyd-Graber
and Resnik, 2010). Each topic has a top-level dis-
tribution over words and constraints, and each con-
straint in each topic has second-level distribution
over the words in the constraint. Critically, the per-
constraint distribution over words is engineered to be
non-sparse and close to uniform. The top level distri-
bution encodes which constraints (and unconstrained
words) to include; the lower-level distribution forces
the probabilities to be correlated for each of the con-
straints.
In LDA, a document’s token is produced in the
generative process by choosing a topic z and sam-
pling a word from the multinomial distribution 0z of
topic z. For a constrained topic, the process now can
take two steps. First, a first-level node in the tree is
selected from 0z. If that is an unconstrained word,
the word is emitted and the generative process for
that token is done. Otherwise, if the first level node
is constraint l, then choose a word to emit from the
constraint’s distribution over words 7rz,l.
More concretely, suppose for a corpus with M
documents we have a set of constraints SZ. The prior
structure has B branches (one branch for each word
not in a constraint and one for each constraint). Then
the generative process for constrained LDA is:
</bodyText>
<listItem confidence="0.936928769230769">
1. For each topic i E {1,... K1:
(a) draw a distribution over the B branches (words and
constraints) Oi — Dir((j), and
(b) for each constraint Qj E n, draw a distribution over
the words in the constraint 7ri,j — Dir(77), where
iri,j is a distribution over the words in Qj
2. Then for each document d E {1, ... M1:
(a) first draw a distribution over topics Bd — Dir(α),
(b) then for each token n E {1, ... Nd1:
i. choose a topic assignment zd,n — Mult(Bd),
and then
ii. choose either a constraint or word from
Mult(0zd,n):
</listItem>
<bodyText confidence="0.631942428571429">
A. if we chose a word, emit that word wd,n
B. otherwise if we chose a constraint index ld,n,
emit a word wd,n from the constraint’s dis-
tribution over words in topic zd,n: wd,n —
Mult(1rzd,n,ld,n).
In this model, α, Q, and q are Dirichlet hyperpa-
rameters set by the user; their role is explained below.
</bodyText>
<subsectionHeader confidence="0.999637">
3.1 Gibbs Sampling for Topic Models
</subsectionHeader>
<bodyText confidence="0.999620692307692">
In topic modeling, collapsed Gibbs sampling (Grif-
fiths and Steyvers, 2004) is a standard procedure for
obtaining a Markov chain over the latent variables
in the model. Given certain technical conditions,
the stationary distribution of the Markov chain is
the posterior (Neal, 1993). Given M documents the
state of a Gibbs sampler for LDA consists of topic
assignments for each token in the corpus and is rep-
resented as Z = {z1,1 ... z1,N1, z2,1,... zM,NMI. In
each iteration, every token’s topic assignment zd,n
is resampled based on topic assignments for all the
tokens except for zd,n. (This subset of the state is
denoted Z−(d,n)). The sampling equation for zd,n is
</bodyText>
<equation confidence="0.998776333333333">
//�� Td,k + α Pk,wd,n + Q
p(zd,n = k |Z−cd,n), α, N) a Pk,· + V� (1)
Td,· + Kα
</equation>
<bodyText confidence="0.995946347826087">
where Td,k is the number of times topic k is used in
document d, Pk,wd,n is the number of times the type
wd,n is assigned to topic k, and α, Q are the hyperpa-
rameters of the two Dirichlet distributions, and B is
the number of top-level branches (this is the vocab-
ulary size for vanilla LDA). When a dot replaces a
subscript of a count, it represents the marginal sum
over all possible topics or words, e.g. Td,· = Ek Td,k.
The count statistics P and T provide summaries of
the state. Typically, these only change based on as-
signments of latent variables in the sampler; in Sec-
tion 4 we describe how changes in the model’s struc-
ture (in addition to the latent state) can be reflected
in these count statistics.
Contrasting with the above inference is the infer-
ence for a constrained model. (For a derivation, see
Boyd-Graber, Blei, and Zhu (2007) for the general
case or Andrzejewski, Zhu, and Craven (2009) for
the specific case of constraints.) In this case the
sampling equation for zd,n is changed to p(zd,n =
kIZ−(d,n), α, Q, q)
where Pk,wd,n is the number of times the uncon-
strained word wd,n appears in topic k; Pk,l is the
</bodyText>
<figure confidence="0.9962745">
Pk,l+Clβ
Pk,·+V β
Wk,l,·+Clη wd,n E Ql
Pk,wd,n +β
if dl, wd,n E� �l
Pk,·+V β
Wk,l,wd,n+η
, (2)
⎧
⎨⎪
⎪⎩
Td,k+α
Td,·+Kα
IX
Td,k+α
Td,·+Kα
</figure>
<page confidence="0.950851">
250
</page>
<bodyText confidence="0.988395555555555">
number of times any word of constraint Ql appears in Previous New
topic k; Wk,l,11d,n is the number of times word wd,n
appears in constraint Ql in topic k; V is the vocabu-
lary size; Cl is the number of words in constraint Ql.
Note the differences between these two samplers for
constrained words; however, for unconstrained LDA
and for unconstrained words in constrained LDA, the
conditional probability is the same.
In order to make the constraints effective, we set
the constraint word-distribution hyperparameter q
to be much larger than the hyperparameter for the
distribution over constraints and vocabulary Q. This
gives the constraints higher weight. Normally, esti-
mating hyperparameters is important for topic mod-
eling (Wallach et al., 2009). However, in ITM, sam-
pling hyperparameters often (but not always) undoes
the constraints (by making q comparable to Q), so we
keep the hyperparameters fixed.
</bodyText>
<table confidence="0.910754928571429">
4 Interactively adding constraints
[bark:2, dog:3, leash:3 dog:2] [bark:-1, dog:-1, leash:-1 dog:-1]
All [bark:2, bark:2, plant:2, tree:3] [bark:-1, bark:-1, plant:-1, tree:-1]
[tree:2,play:2,forest:1,leash:2] [tree:-1,play:-1,forest:-1,leash:-1]
[bark:2, dog:3, leash:3 dog:2] [bark:-1, dog:-1, leash:-1 dog:-1]
Doc [bark:2, bark:2, plant:2, tree:3] [bark:-1, bark:-1, plant:-1, tree:-1]
[tree:2,play:2,forest:1,leash:2] [tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:3] [bark:-1, dog:-1, leash:3 dog:-1]
Term [bark:2, bark:2, plant:2, tree:3] [bark:-1, bark:-1, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2] [tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2] [bark:2, dog:3, leash:3 dog:2]
None [bark:2, bark:2, plant:2, tree:3] [bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2] [tree:2,play:2,forest:1,leash:2]
Figure 2: Four different strategies for state ablation after
</table>
<bodyText confidence="0.979838571428571">
the words “dog” and “bark” are added to the constraint
{“leash,” “puppy”} to make the constraint {“dog,” “bark,”
“leash,” “puppy”}. The state is represented by showing the
current topic assignment after each word (e.g. “leash” in
the first document has topic 3, while “forest” in the third
document has topic 1). On the left are the assignments
before words were added to constraints, and on the right
are the ablated assignments. Unassigned words are given
the new topic assignment -1 and are highlighted in red.
For a static model, inference in ITM is the same as All We could revoke all state assignments, essen-
in previous models (Andrzejewski et al., 2009). In tially starting the sampler from scratch. This does
this section, we detail how interactively changing not allow interactive refinement, as there is nothing
constraints can be accommodated in ITM, smoothly to enforce that the new topics will be in any way
transitioning from unconstrained LDA (n.b. Equa- consistent with the existing topics. Once the topic
tion 1) to constrained LDA (n.b. Equation 2) with one assignments of all states are revoked, the counts for
constraint, to constrained LDA with two constraints, T, P and W (as described in Section 3.1) will be
etc. zero, retaining no information about the state the user
A central tool that we will use is the strategic unas- observed.
signment of states, which we call ablation (distinct
from feature ablation in supervised learning). As
described in the previous section, a sampler stores
the topic assignment of each token. In the implemen-
tation of a Gibbs sampler, unassignment is done by
setting a token’s topic assignment to an invalid topic
(e.g. -1, as we use here) and decrementing any counts
associated with that word.
The constraints created by users implicitly signal
that words in constraints don’t belong in a given
topic. In other models, this input is sometimes used
to “fix,” i.e. deterministically hold constant topic as-
signments (Ramage et al., 2009). Instead, we change
the underlying model, using the current topic assign-
ments as a starting position for a new Markov chain
with some states strategically unassigned. How much
of the existing topic assignments we use leads to four
</bodyText>
<table confidence="0.890507789473684">
different options, which are illustrated in Figure 2.
251
Doc Because topic models treat the document con-
text as exchangeable, a document is a natural context
for partial state ablation. Thus if a user adds a set of
words S to constraints, then we have reason to sus-
pect that all documents containing any one of S may
have incorrect topic assignments. This is reflected
in the state of the sampler by performing the UNAS-
SIGN (Algorithm 1) operation for each word in any
document containing a word added to a constraint.
Algorithm 1 UNASSIGN(d, n, wd,n, zd,n = k)
1: T : Td,k ← Td,k − 1
2: If Wd,n ∈� nold,
P : Pk,wd n ← Pk,wd n − 1
3: Else: suppose Wd,n ∈ nold
m ,
P : Pk,m ← Pk,m − 1
W : Wk,m,wd n ← Wk,m,wd n − 1
</table>
<bodyText confidence="0.999841625">
This is equivalent to the Gibbs2 sampler of Yao
et al. (2009) for incorporating new documents in
a streaming context. Viewed in this light, a user
is using words to select documents that should be
treated as “new” for this refined model.
Term Another option is to perform ablation only
on the topic assignments of tokens whose words have
added to a constraint. This applies the unassignment
operation (Algorithm 1) only to tokens whose corre-
sponding word appears in added constraints (i.e. a
subset of the Doc strategy). This makes it less likely
that other tokens in similar contexts will follow the
words explicitly included in the constraints to new
topic assignments.
None The final option is to move words into con-
straints but keep the topic assignments fixed. Thus,
P and W change, but not T, as described in Algo-
rithm 2.3 This is arguably the simplest option, and
in principle is sufficient, as the Markov chain should
find a stationary distribution regardless of the starting
position. In practice, however, this strategy is less
interactive, as users don’t feel that their constraints
are actually incorporated in the model, and inertia
can keep the chain from reflecting the constraints.
</bodyText>
<construct confidence="0.693512">
Algorithm 2 MOVE(d, n, Wd,n, zd,n = k, Ql)
</construct>
<listItem confidence="0.49851">
1: If Wd,n ∈� nold,
</listItem>
<equation confidence="0.920854714285714">
P : Pk,wd n ← Pk,wd n − 1, Pk,l ← Pk,l + 1
W : Wk,l,wd n ← Wk,l,wd n + 1
2: Else, suppose Wd,n ∈ nold
m ,
P : Pk,m ← Pk,m − 1, Pk,l ← Pk,l + 1
W : Wk,m,wd n ← Wk,m,wd n − 1
Wk,l,wd n ← Wk,l,wd n + 1
</equation>
<bodyText confidence="0.889115888888889">
Regardless of what ablation scheme is used, after
the state of the Markov chain is altered, the next
step is to actually run inference forward, sampling
assignments for the unassigned tokens for the “first”
time and changing the topic assignment of previously
assigned tokens. How many additional iterations are
3This assumes that there is only one possible path in the con-
straint tree that can generate a word; in other words, this as-
sumes that constraints are transitive, as discussed at the end of
Section 2. In the more general case, when words lack a unique
path in the constraint tree, an additional latent variable specifies
which possible paths in the constraint tree produced the word;
this would have to be sampled. All other updating strategies
are immune to this complication, as the assignments are left
unassigned.
required after adding constraints is a delicate tradeoff
between interactivity and effectiveness, which we
investigate further in the next sections.
</bodyText>
<sectionHeader confidence="0.948959" genericHeader="method">
5 Motivating Example
</sectionHeader>
<bodyText confidence="0.999932692307692">
To examine the viability of ITM, we begin with a
qualitative demonstration that shows the potential
usefulness of ITM. For this task, we used a corpus
of about 2000 New York Times editorials from the
years 1987 to 1996. We started by finding 20 initial
topics with no constraints, as shown in Table 1 (left).
Notice that topics 1 and 20 both deal with Russia.
Topic 20 seems to be about the Soviet Union, with
topic 1 about the post-Soviet years. We wanted to
combine the two into a single topic, so we created a
constraint with all of the clearly Russian or Soviet
words (boris, communist, gorbachev, mikhail, russia,
russian, soviet, union, yeltsin ). Running inference
forward 100 iterations with the Doc ablation strat-
egy yields the topics in Table 1 (right). The two
Russia topics were combined into Topic 20. This
combination also pulled in other relevant words that
not near the top of either topic before: “moscow”
and “relations.” Topic 1 is now more about elections
in countries other than Russia. The other 18 topics
changed little.
While we combined the Russian topics, other re-
searchers analyzing large corpora might preserve the
Soviet vs. post-Soviet distinction but combine topics
about American government. ITM allows tuning for
specific tasks.
</bodyText>
<sectionHeader confidence="0.989427" genericHeader="method">
6 Simulation Experiment
</sectionHeader>
<bodyText confidence="0.9999805">
Next, we consider a process for evaluating our ITM
using automatically derived constraints. These con-
straints are meant to simulate a user with a predefined
list of categories (e.g. reviewers for journal submis-
sions, e-mail folders, etc.). The categories grow more
and more specific during the session as the simulated
users add more constraint words.
To test the ability of ITM to discover relevant
subdivisions in a corpus, we use a dataset with pre-
defined, intrinsic labels and assess how well the dis-
covered latent topic structure can reproduce the cor-
pus’s inherent structure. Specifically, for a corpus
with M classes, we use the per-document topic dis-
tribution as a feature vector in a supervised classi-
</bodyText>
<page confidence="0.985414">
252
</page>
<bodyText confidence="0.852972424242424">
Topic Words
election, yeltsin, russian, political, party, democratic, russia, presi-
dent, democracy, boris, country, south, years, month, government, vote,
since, leader, presidential, military
new, york, city, state, mayor, budget, giuliani, council, cuomo, gov,
plan, year, rudolph, dinkins, lead, need, governor, legislature, pataki,
david
nuclear, arms, weapon, defense, treaty, missile, world, unite, yet, soviet,
lead, secretary, would, control, korea, intelligence, test, nation, country,
testing
president, bush, administration, clinton, american, force, reagan, war,
4 unite, lead, economic, iraq, congress, america, iraqi, policy, aid, inter-
national, military, see
...
soviet, lead, gorbachev, union, west, mikhail, reform, change, europe,
leaders, poland, communist, know, old, right, human, washington,
western, bring, party
Topic Words
election, democratic, south, country, president, party, africa, lead, even,
democracy, leader, presidential, week, politics, minister, percent, voter,
last, month, years
new, york, city, state, mayor, budget, council, giuliani, gov, cuomo,
year, rudolph, dinkins, legislature, plan, david, governor, pataki, need,
cut
nuclear, arms, weapon, treaty, defense, war, missile, may, come, test,
american, world, would, need, lead, get, join, yet, clinton, nation
president, administration, bush, clinton, war, unite, force, reagan, amer-
4 ican, america, make, nation, military, iraq, iraqi, troops, international,
country, yesterday, plan
...
soviet, union, economic, reform, yeltsin, russian, lead, russia, gor-
bachev, leaders, west, president, boris, moscow, europe, poland,
mikhail, communist, power, relations
</bodyText>
<figure confidence="0.984038375">
1
2
3
20
1
2
3
20
</figure>
<tableCaption confidence="0.96973275">
Table 1: Five topics from a 20 topic topic model on the editorials from the New York times before adding a constraint
(left) and after (right). After the constraint was added, which encouraged Russian and Soviet terms to be in the same
topic, non-Russian terms gained increased prominence in Topic 1, and “Moscow” (which was not part of the constraint)
appeared in Topic 20.
</tableCaption>
<bodyText confidence="0.999855333333333">
fier (Hall et al., 2009). The lower the classification
error rate, the better the model has captured the struc-
ture of the corpus.4
</bodyText>
<subsectionHeader confidence="0.999788">
6.1 Generating automatic constraints
</subsectionHeader>
<bodyText confidence="0.963039884615385">
We used the 20 Newsgroups corpus, which contains
18846 documents divided into 20 constituent news-
groups. We use these newsgroups as ground-truth
labels.5
We simulate a user’s constraints by ranking words
in the training split by their information gain (IG).6
After ranking the top 200 words for each class
by IG, we delete words associated with multiple
labels to prevent constraints for different labels
from merging. The smallest class had 21 words
remaining after removing duplicates (due to high
4Our goal is to understand the phenomena of ITM, not classifica-
tion, so these classification results are well below state of the
art. However, adding interactively selected topics to the state
of the art features (tf-idf unigrams) gives a relative error reduc-
tion of 5.1%, while just adding topics from vanilla LDA gives
a relative error reduction of 1.1%. Both measurements were
obtained without tuning or weighting features, so presumably
better results are possible.
5http://people.csail.mit.edu/jrennie/20Newsgroups/
In preprocessing, we deleted short documents, leaving 15160
documents, including 9131 training documents and 6029 test
documents (default split). Tokenization, lemmatization, and
stopword removal was performed using the Natural Language
Toolkit (Loper and Bird, 2002). Topic modeling was performed
using the most frequent 5000 lemmas as the vocabulary.
</bodyText>
<footnote confidence="0.9902835">
6IG is computed by the Rainbow toolbox
http://www.cs.umass.edu/ mccallum/bow/rainbow/
</footnote>
<bodyText confidence="0.999187090909091">
overlaps of 125 words between “talk.religion.misc”
and “soc.religion.christian,” and 110 words between
“talk.religion.misc” and “alt.atheism”), so the top 21
words for each class were the ingredients for our
simulated constraints. For example, for the class
“soc.religion.christian,” the 21 constraint words in-
clude “catholic, scripture, resurrection, pope, sab-
bath, spiritual, pray, divine, doctrine, orthodox.” We
simulate a user’s ITM session by adding a word to
each of the 20 constraints until each of the constraints
has 21 words.
</bodyText>
<subsectionHeader confidence="0.999834">
6.2 Simulation scheme
</subsectionHeader>
<bodyText confidence="0.999946823529412">
Starting with 100 base iterations, we perform suc-
cessive rounds of refinement. In each round a new
constraint is added corresponding to the newsgroup
labels. Next, we perform one of the strategies for
state ablation, add additional iterations of Gibbs sam-
pling, use the newly obtained topic distribution of
each document as the feature vector, and perform
classification on the test / train split. We do this for
21 rounds until each label has 21 constraint words.
The number of LDA topics is set to 20 to match the
number of newsgroups. The hyperparameters for all
experiments are α = 0.1, Q = 0.01, and q = 100.
At 100 iterations, the chain is clearly not con-
verged. However, we chose this number of iterations
because it more closely matches the likely use case as
users do not wait for convergence. Moreover, while
investigations showed that the patterns shown in Fig-
</bodyText>
<page confidence="0.996379">
253
</page>
<bodyText confidence="0.9999856">
ure 4 were broadly consistent with larger numbers
of iterations, such configurations sometimes had too
much inertia to escape from local extrema. More iter-
ations make it harder for the constraints to influence
the topic assignment.
</bodyText>
<subsectionHeader confidence="0.997126">
6.3 Investigating Ablation Strategies
</subsectionHeader>
<bodyText confidence="0.999978365853658">
First, we investigate which ablation strategy best al-
lows constraints to be incorporated. Figure 3 shows
the classification error of six different ablation strate-
gies based on the number of words in each constraint,
ranging from 0 to 21. Each is averaged over five dif-
ferent chains using 10 additional iterations of Gibbs
sampling per round (other numbers of iterations are
discussed in Section 6.4). The model runs forward 10
iterations after the first round, another 10 iterations
after the second round, etc. In general, as the number
of words per constraint increases, the error decreases
as models gain more information about the classes.
Strategy Null is the non-interactive baseline that
contains no constraints (vanilla LDA), but runs infer-
ence for a comparable number of rounds. All Initial
and All Full are non-interactive baselines with all
constraints known a priori. All Initial runs the model
for the only the initial number of iterations (100 it-
erations in this experiment), while All Full runs the
model for the total number of iterations added for the
interactive version. (That is, if there were 21 rounds
and each round of interactive modeling added 10 iter-
ations, All Full would have 210 iterations more than
All Initial).
While Null sees no constraints, it serves as an
upper baseline for the error rate (lower error being
better) but shows the effect of additional inference.
All Full is a lower baseline for the error rate since
it both sees the constraints at the beginning and also
runs for the maximum number of total iterations. All
Initial sees the constraints before the other ablation
techniques but it has fewer total iterations.
The Null strategy does not perform as well as
the interactive versions, especially with larger con-
straints. Both All Initial and All Full, however, show
a larger variance (as denoted by error bands around
the average trends) than the interactive schemes. This
can be viewed as akin to simulated annealing, as the
interactive search has more freedom to explore in
early rounds. As more constraint words are added
each round, the model is less free to explore.
</bodyText>
<figure confidence="0.945100777777778">
Strategy
All Full
All Initial
Doc
None
Null
Term
0 5 10 15 20
Words per constraint
</figure>
<figureCaption confidence="0.999403625">
Figure 3: Error rate (y-axis, lower is better) using different
ablation strategies as additional constraints are added (x-
axis). Null represents standard LDA, as the unconstrained
baseline. All Initial and All Full are non-interactive, con-
strained baselines. The results of None, Term, Doc are
more stable (as denoted by the error bars), and the error
rate is reduced gradually as more constraint words are
added.
</figureCaption>
<bodyText confidence="0.999815222222222">
The error rate of each interactive ablation strategy
is (as expected) between the lower and upper base-
lines. Generally, the constraints will influence not
only the topics of the constraint words, but also the
topics of the constraint words’ context in the same
document. Doc ablation gives more freedom for the
constraints to overcome the inertia of the old topic
distribution and move towards a new one influenced
by the constraints.
</bodyText>
<subsectionHeader confidence="0.994554">
6.4 How many iterations do users have to wait?
</subsectionHeader>
<bodyText confidence="0.99976075">
Figure 4 shows the effect of using different numbers
of Gibbs sampling iterations after changing a con-
straint. For each of the ablation strategies, we run
110, 20, 30, 50,100} additional Gibbs sampling iter-
ations. As expected, more iterations reduce error,
although improvements diminish beyond 100 itera-
tions. With more constraints, the impact of additional
iterations is lessened, as the model has more a priori
knowledge to draw upon.
For all numbers of additional iterations, while the
Null serves as the upper baseline on the error rate
in all cases, the Doc ablation clearly outperforms
the other ablation schemes, consistently yielding a
lower error rate. Thus, there is a benefit when the
model has a chance to relearn the document context
when constraints are added. The difference is even
larger with more iterations, suggesting Doc needs
more iterations to “recover” from unassignment.
The luxury of having hundreds or thousands of
additional iterations for each constraint would be im-
</bodyText>
<figure confidence="0.998734375">
0.50
0.48
0.46
Error
0.44
0.42
0.40
0.38
</figure>
<page confidence="0.522297">
254
</page>
<figure confidence="0.998873826086957">
Error
0.50
0.48
0.46
0.44
0.42
0.40
0 5 10 15 20
10
0 5 10 15 20
20
0 5 10 15 20
30
0 5 10 15 20
50
0 5 10 15 20
100
Strategy
Doc
None
Null
Term
Words per constraint
</figure>
<figureCaption confidence="0.990588333333333">
Figure 4: Classification accuracy by strategy and number of additional iterations. The Doc ablation strategy performs
best, suggesting that the document context is important for ablation constraints. While more iterations are better, there
is a tradeoff with interactivity.
</figureCaption>
<bodyText confidence="0.999901">
practical. For even moderately sized datasets, even
one iteration per second can tax the patience of in-
dividuals who want to use the system interactively.
Based on these results and an ad hoc qualitative ex-
amination of the resulting topics, we found that 30
additional iterations of inference was acceptable; this
is used in later experiments.
</bodyText>
<sectionHeader confidence="0.556059" genericHeader="method">
7 Getting Humans in the Loop
</sectionHeader>
<bodyText confidence="0.999924333333333">
To move beyond using simulated users adding the
same words regardless of what topics were discov-
ered by the model, we needed to expose the model
to human users. We solicited approximately 200
judgments from Mechanical Turk, a popular crowd-
sourcing platform that has been used to gather lin-
guistic annotations (Snow et al., 2008), measure topic
quality (Chang et al., 2009), and supplement tradi-
tional inference techniques for topic models (Chang,
2010). After presenting our interface for collecting
judgments, we examine the results from these ITM
sessions both quantitatively and qualitatively.
</bodyText>
<sectionHeader confidence="0.584997" genericHeader="method">
7.1 Interface for soliciting refinements
</sectionHeader>
<bodyText confidence="0.999922090909091">
Figure 5 shows the interface used in the Mechanical
Turk tests. The left side of the screen shows the
current topics in a scrollable list, with the top 30
words displayed for each topic.
Users create constraints by clicking on words from
the topic word lists. The word lists use a color-coding
scheme to help the users keep track of which words
they are currently grouping into constraints. The right
side of the screen displays the existing constraints.
Users can click on icons to edit or delete each one.
The constraint currently being built is also shown.
</bodyText>
<figureCaption confidence="0.993255">
Figure 5: Interface for Mechanical Turk experiments.
Users see the topics discovered by the model and select
words (by clicking on them) to build constraints to be
added to the model.
</figureCaption>
<bodyText confidence="0.9992699">
Clicking on a word will remove that word from the
current constraint.
As in Section 6, we can compute the classification
error for these users as they add words to constraints.
The best users, who seemed to understand the task
well, were able to decrease classification error. (Fig-
ure 6). The median user, however, had an error re-
duction indistinguishable from zero. Despite this, we
can examine the users’ behavior to better understand
their goals and how they interact with the system.
</bodyText>
<subsectionHeader confidence="0.998568">
7.2 Untrained users and ITM
</subsectionHeader>
<bodyText confidence="0.99962425">
Most of the large (10+ word) user-created constraints
corresponded to the themes of the individual news-
groups, which users were able to infer from the
discovered topics. Common constraint themes that
</bodyText>
<page confidence="0.991849">
255
</page>
<figure confidence="0.993991142857143">
Best Session
10 Topics
20 Topics
50 Topics
75 Topics
0 1 2 3 4
Round
</figure>
<figureCaption confidence="0.9641595">
Figure 6: The relative error rate (using round 0 as a base-
line) of the best Mechanical Turk user session for each of
the four numbers of topics. While the 10-topic model does
not provide enough flexibility to create good constraints,
the best users could clearly improve classification with
more topics.
</figureCaption>
<bodyText confidence="0.999965842105263">
matched specific newsgroups included religion, space
exploration, graphics, and encryption. Other com-
mon themes were broader than individual news-
groups (e.g. sports, government and computers). Oth-
ers matched sub-topics of a single newsgroup, such
as homosexuality, Israel or computer programming.
Some users created inscrutable constraints, like
(“better, people, right, take, things”) and (“fbi, let,
says”). They may have just clicked random words to
finish the task quickly. While subsequent users could
delete poor constraints, most chose not to. Because
we wanted to understand broader behavior we made
no effort to squelch such responses.
The two-word constraints illustrate an interesting
contrast. Some pairs are linked together in the corpus,
like (“jesus, christ”) and (“solar, sun”). With others,
like (“even, number”) and (“book, list”), the users
seem to be encouraging collocations to be in the
same topic. However, the collocations may not be in
any document in this corpus. Another user created a
constraint consisting of male first names. A topic did
emerge with these words, but the rest of the words
in that topic seemed random, as male first names are
not likely to co-occur in the same document.
Not all sensible constraints led to successful topic
changes. Many users grouped “mac” and “windows”
together, but they were almost never placed in the
same topic. The corpus includes separate newsgroups
for Macintosh and Windows hardware, and divergent
contexts of “mac” and “windows” overpowered the
prior distribution.
The constraint size ranged from one word to over
40. In general, the more words in the constraint,
the more likely it was to noticeably affect the topic
distribution. This observation makes sense given
our ablation method. A constraint with more words
will cause the topic assignments to be reset for more
documents.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999982304347826">
In this work, we introduced a means for end-users
to refine and improve the topics discovered by topic
models. ITM offers a paradigm for non-specialist
consumers of machine learning algorithms to refine
models to better reflect their interests and needs. We
demonstrated that even novice users are able to under-
stand and build constraints using a simple interface
and that their constraints can improve the model’s
ability to capture the latent structure of a corpus.
As presented here, the technique for incorporating
constraints is closely tied to inference with Gibbs
sampling. However, most inference techniques are
essentially optimization problems. As long as it is
possible to define a transition on the state space that
moves from one less-constrained model to another
more-constrained model, other inference procedures
can also be used.
We hope to engage these algorithms with more
sophisticated users than those on Mechanical Turk
to measure how these models can help them better
explore and understand large, uncurated data sets. As
we learn their needs, we can add more avenues for
interacting with topic models.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999629">
We would like to thank the anonymous reviewers, Ed-
mund Talley, Jonathan Chang, and Philip Resnik for
their helpful comments on drafts of this paper. This
work was supported by NSF grant #0705832. Jordan
Boyd-Graber is also supported by the Army Research
Laboratory through ARL Cooperative Agreement
W911NF-09-2-0072 and by NSF grant #1018625.
Any opinions, findings, conclusions, or recommenda-
tions expressed are the authors’ and do not necessar-
ily reflect those of the sponsors.
</bodyText>
<figure confidence="0.994293">
1.00
Relative Error
0.98
0.96
0.94
</figure>
<page confidence="0.994536">
256
</page>
<sectionHeader confidence="0.995429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997094">
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In Proceedings of
International Conference of Machine Learning.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine Learn-
ing Research, 3:993–1022.
Jordan Boyd-Graber and David M. Blei. 2008. Syntactic
topic models. In Proceedings of Advances in Neural
Information Processing Systems.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet allocation. In Proceedings of
Emperical Methods in Natural Language Processing.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean
Gerrish, and David M. Blei. 2009. Reading tea leaves:
How humans interpret topic models. In Neural Infor-
mation Processing Systems.
Jonathan Chang. 2010. Not-so-latent Dirichlet allocation:
Collapsed Gibbs sampling using human judgments. In
NAACL Workshop: Creating Speech and Language
Data With Amazon’ss Mechanical Turk.
Hal Daum´e III. 2009. Markov random topic fields. In
Proceedings of Artificial Intelligence and Statistics.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In Pro-
ceedings ofInternational Conference of Machine Learn-
ing.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Proceedings of the National Academy
of Sciences, 101(Suppl 1):5228–5235.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic Markov models. In Artificial Intelligence
and Statistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1):10–18.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence.
Thomas K. Landauer, Danielle S. McNamara, Dennis S.
Marynick, and Walter Kintsch, editors. 2006. Proba-
bilistic Topic Models. Laurence Erlbaum.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-
der Hauptmann. 2006. Which side are you on? identi-
fying perspectives at the document and sentence levels.
In Proceedings of the Conference on Natural Language
Learning (CoNLL).
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching.
Radford M. Neal. 1993. Probabilistic inference using
Markov chain Monte Carlo methods. Technical Report
CRG-TR-93-1, University of Toronto.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Conference of the North American Chapter of
the Association for Computational Linguistics.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In Association for the Advancement of
Artificial Intelligence.
James Petterson, Smola Alex, Tiberio Caetano, Wray Bun-
tine, and Narayanamurthy Shravan. 2010. Word fea-
tures for latent Dirichlet allocation. In Neural Informa-
tion Processing Systems.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of Emperical Methods
in Natural Language Processing.
Fergus Rob, Li Fei-Fei, Perona Pietro, and Zisserman An-
drew. 2005. Learning object categories from Google’s
image search. In International Conference on Com-
puter Vision.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model for
authors and documents. In Proceedings of Uncertainty
in Artificial Intelligence.
Suyash Shringarpure and Eric P. Xing. 2008. mStruct:
a new admixture model for inference of population
structure in light of both genetic admixing and allele
mutations. In Proceedings of International Conference
of Machine Learning.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast—but is it good? Evalu-
ating non-expert annotations for natural language tasks.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Hanna Wallach, David Mimno, and Andrew McCallum.
2009. Rethinking LDA: Why priors matter. In Pro-
ceedings ofAdvances in Neural Information Processing
Systems.
Hanna M. Wallach. 2006. Topic modeling: Beyond bag-
of-words. In Proceedings of International Conference
of Machine Learning.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In Knowledge Discovery and
Data Mining.
</reference>
<page confidence="0.997168">
257
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985750">
<title confidence="0.999952">Interactive Topic Modeling</title>
<author confidence="0.998674">Yuening Hu Jordan Boyd-Graber Brianna Satinoff</author>
<affiliation confidence="0.9999005">Department of Computer Science iSchool Department of Computer Science University of Maryland University of Maryland University of</affiliation>
<email confidence="0.997975">ynhu@cs.umd.edujbg@umiacs.umd.edubsonrisa@cs.umd.edu</email>
<abstract confidence="0.999450428571429">Topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data. However, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software. In this work, we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via Dirichlet forest priors.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="2909" citStr="Andrzejewski et al., 2009" startWordPosition="453" endWordPosition="456">gements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on creating probabilistic models that incorporate human knowledge, which we extend in Section 3 to apply to ITM sessions. Section 4 discusses the implementation of this process during the inference process. Via a motivating example in Section 5, simulated ITM s</context>
<context position="5036" citStr="Andrzejewski et al., 2009" startWordPosition="799" endWordPosition="802"> the elements with highest probability. For an example of topics discovered from a 20-topic model of New York Times editorials, see Table 1. When presented with poor topics learned from data, users can offer a number of complaints:2 these documents should have similar topics but don’t (Daum´e III, 2009); this topic should have syntactic coherence (Gruber et al., 2007; Boyd-Graber and Blei, 2008); this topic doesn’t make any sense at all (Newman et al., 2010); this topic shouldn’t be associated with this document but is (Ramage et al., 2009); these words shouldn’t be the in same topic but are (Andrzejewski et al., 2009); or these words should be in the same topic but aren’t (Andrzejewski et al., 2009). Many of these complaints can be addressed by using “must-link” constraints on topics, retaining Andrzejewski et al’s (2009) terminology borrowed from the database literature. A “must-link” constraint is a group of words whose probability must be correlated in the topic. For example, Figure 1 shows an example constraint: {plant, factory}. After this constraint is added, the probabilities of “plant” and “factory” in each topic are likely to both be high or both be low. It’s unlikely for “plant” to have high prob</context>
<context position="8575" citStr="Andrzejewski et al., 2009" startWordPosition="1377" endWordPosition="1380">ame topic. Constraints can be added to vanilla LDA by replacing the multinomial distribution over words for each topic with a collection of {} 6 6 6 6 6 6 bark dog tree plant factory leash {plant, factory} 6 6 6 266 bark dog tree leash n n {plant, factory} {dog, bark, leash} 36 6 26 tree n n n nn bark dog leash plant factory 249 tree-structured multinomial distributions drawn from a prior as depicted in Figure 1. By encoding word distributions as a tree, we can preserve conjugacy and relatively simple inference while encouraging correlations between related concepts (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Boyd-Graber and Resnik, 2010). Each topic has a top-level distribution over words and constraints, and each constraint in each topic has second-level distribution over the words in the constraint. Critically, the perconstraint distribution over words is engineered to be non-sparse and close to uniform. The top level distribution encodes which constraints (and unconstrained words) to include; the lower-level distribution forces the probabilities to be correlated for each of the constraints. In LDA, a document’s token is produced in the generative process by choosing a topic z and sampling a w</context>
<context position="15205" citStr="Andrzejewski et al., 2009" startWordPosition="2463" endWordPosition="2466">ed to the constraint {“leash,” “puppy”} to make the constraint {“dog,” “bark,” “leash,” “puppy”}. The state is represented by showing the current topic assignment after each word (e.g. “leash” in the first document has topic 3, while “forest” in the third document has topic 1). On the left are the assignments before words were added to constraints, and on the right are the ablated assignments. Unassigned words are given the new topic assignment -1 and are highlighted in red. For a static model, inference in ITM is the same as All We could revoke all state assignments, essenin previous models (Andrzejewski et al., 2009). In tially starting the sampler from scratch. This does this section, we detail how interactively changing not allow interactive refinement, as there is nothing constraints can be accommodated in ITM, smoothly to enforce that the new topics will be in any way transitioning from unconstrained LDA (n.b. Equa- consistent with the existing topics. Once the topic tion 1) to constrained LDA (n.b. Equation 2) with one assignments of all states are revoked, the counts for constraint, to constrained LDA with two constraints, T, P and W (as described in Section 3.1) will be etc. zero, retaining no info</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. Incorporating domain knowledge into topic modeling via Dirichlet forest priors. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1350" citStr="Blei et al., 2003" startWordPosition="193" endWordPosition="196">richlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions. 1 Introduction Probabilistic topic models, as exemplified by probabilistic latent semantic indexing (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003) are unsupervised statistical techniques to discover the thematic topics that permeate a large corpus of text documents. Topic models have had considerable application beyond natural language processing in computer vision (Rob et al., 2005), biology (Shringarpure and Xing, 2008), and psychology (Landauer et al., 2006) in addition to their canonical application to text. For text, one of the few real-world applications of topic models is corpus exploration. Unannotated, noisy, and ever-growing corpora are the norm rather than the exception, and topic models offer a way to quickly get the gist a </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Syntactic topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="4808" citStr="Boyd-Graber and Blei, 2008" startWordPosition="759" endWordPosition="762">covers K distributions over words — the namesake topics — and associations between documents and topics. In LDA both of these outputs are multinomial distributions; typically they are presented to users in summary form by listing the elements with highest probability. For an example of topics discovered from a 20-topic model of New York Times editorials, see Table 1. When presented with poor topics learned from data, users can offer a number of complaints:2 these documents should have similar topics but don’t (Daum´e III, 2009); this topic should have syntactic coherence (Gruber et al., 2007; Boyd-Graber and Blei, 2008); this topic doesn’t make any sense at all (Newman et al., 2010); this topic shouldn’t be associated with this document but is (Ramage et al., 2009); these words shouldn’t be the in same topic but are (Andrzejewski et al., 2009); or these words should be in the same topic but aren’t (Andrzejewski et al., 2009). Many of these complaints can be addressed by using “must-link” constraints on topics, retaining Andrzejewski et al’s (2009) terminology borrowed from the database literature. A “must-link” constraint is a group of words whose probability must be correlated in the topic. For example, Fig</context>
</contexts>
<marker>Boyd-Graber, Blei, 2008</marker>
<rawString>Jordan Boyd-Graber and David M. Blei. 2008. Syntactic topic models. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8606" citStr="Boyd-Graber and Resnik, 2010" startWordPosition="1381" endWordPosition="1384">be added to vanilla LDA by replacing the multinomial distribution over words for each topic with a collection of {} 6 6 6 6 6 6 bark dog tree plant factory leash {plant, factory} 6 6 6 266 bark dog tree leash n n {plant, factory} {dog, bark, leash} 36 6 26 tree n n n nn bark dog leash plant factory 249 tree-structured multinomial distributions drawn from a prior as depicted in Figure 1. By encoding word distributions as a tree, we can preserve conjugacy and relatively simple inference while encouraging correlations between related concepts (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Boyd-Graber and Resnik, 2010). Each topic has a top-level distribution over words and constraints, and each constraint in each topic has second-level distribution over the words in the constraint. Critically, the perconstraint distribution over words is engineered to be non-sparse and close to uniform. The top level distribution encodes which constraints (and unconstrained words) to include; the lower-level distribution forces the probabilities to be correlated for each of the constraints. In LDA, a document’s token is produced in the generative process by choosing a topic z and sampling a word from the multinomial distri</context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2882" citStr="Boyd-Graber et al., 2007" startWordPosition="449" endWordPosition="452">s correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on creating probabilistic models that incorporate human knowledge, which we extend in Section 3 to apply to ITM sessions. Section 4 discusses the implementation of this process during the inference process. Via a motivating example in</context>
<context position="8548" citStr="Boyd-Graber et al., 2007" startWordPosition="1373" endWordPosition="1376">different words into the same topic. Constraints can be added to vanilla LDA by replacing the multinomial distribution over words for each topic with a collection of {} 6 6 6 6 6 6 bark dog tree plant factory leash {plant, factory} 6 6 6 266 bark dog tree leash n n {plant, factory} {dog, bark, leash} 36 6 26 tree n n n nn bark dog leash plant factory 249 tree-structured multinomial distributions drawn from a prior as depicted in Figure 1. By encoding word distributions as a tree, we can preserve conjugacy and relatively simple inference while encouraging correlations between related concepts (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Boyd-Graber and Resnik, 2010). Each topic has a top-level distribution over words and constraints, and each constraint in each topic has second-level distribution over the words in the constraint. Critically, the perconstraint distribution over words is engineered to be non-sparse and close to uniform. The top level distribution encodes which constraints (and unconstrained words) to include; the lower-level distribution forces the probabilities to be correlated for each of the constraints. In LDA, a document’s token is produced in the generative process by choosing</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Chong Wang</author>
<author>Sean Gerrish</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2312" citStr="Chang et al., 2009" startWordPosition="349" endWordPosition="352">nonical application to text. For text, one of the few real-world applications of topic models is corpus exploration. Unannotated, noisy, and ever-growing corpora are the norm rather than the exception, and topic models offer a way to quickly get the gist a large corpus.1 1For examples, see Rexa http://rexa.info/, JSTOR Contrary to the impression given by the tables shown in topic modeling papers, topics discovered by topic modeling don’t always make sense to ostensible end users. Part of the problem is that the objective function of topic models doesn’t always correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; P</context>
<context position="32645" citStr="Chang et al., 2009" startWordPosition="5308" endWordPosition="5311">se the system interactively. Based on these results and an ad hoc qualitative examination of the resulting topics, we found that 30 additional iterations of inference was acceptable; this is used in later experiments. 7 Getting Humans in the Loop To move beyond using simulated users adding the same words regardless of what topics were discovered by the model, we needed to expose the model to human users. We solicited approximately 200 judgments from Mechanical Turk, a popular crowdsourcing platform that has been used to gather linguistic annotations (Snow et al., 2008), measure topic quality (Chang et al., 2009), and supplement traditional inference techniques for topic models (Chang, 2010). After presenting our interface for collecting judgments, we examine the results from these ITM sessions both quantitatively and qualitatively. 7.1 Interface for soliciting refinements Figure 5 shows the interface used in the Mechanical Turk tests. The left side of the screen shows the current topics in a scrollable list, with the top 30 words displayed for each topic. Users create constraints by clicking on words from the topic word lists. The word lists use a color-coding scheme to help the users keep track of w</context>
</contexts>
<marker>Chang, Boyd-Graber, Wang, Gerrish, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
</authors>
<title>Not-so-latent Dirichlet allocation: Collapsed Gibbs sampling using human judgments.</title>
<date>2010</date>
<booktitle>In NAACL Workshop: Creating Speech and Language Data With Amazon’ss Mechanical Turk.</booktitle>
<contexts>
<context position="32725" citStr="Chang, 2010" startWordPosition="5321" endWordPosition="5322">on of the resulting topics, we found that 30 additional iterations of inference was acceptable; this is used in later experiments. 7 Getting Humans in the Loop To move beyond using simulated users adding the same words regardless of what topics were discovered by the model, we needed to expose the model to human users. We solicited approximately 200 judgments from Mechanical Turk, a popular crowdsourcing platform that has been used to gather linguistic annotations (Snow et al., 2008), measure topic quality (Chang et al., 2009), and supplement traditional inference techniques for topic models (Chang, 2010). After presenting our interface for collecting judgments, we examine the results from these ITM sessions both quantitatively and qualitatively. 7.1 Interface for soliciting refinements Figure 5 shows the interface used in the Mechanical Turk tests. The left side of the screen shows the current topics in a scrollable list, with the top 30 words displayed for each topic. Users create constraints by clicking on words from the topic word lists. The word lists use a color-coding scheme to help the users keep track of which words they are currently grouping into constraints. The right side of the s</context>
</contexts>
<marker>Chang, 2010</marker>
<rawString>Jonathan Chang. 2010. Not-so-latent Dirichlet allocation: Collapsed Gibbs sampling using human judgments. In NAACL Workshop: Creating Speech and Language Data With Amazon’ss Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Markov random topic fields.</title>
<date>2009</date>
<booktitle>In Proceedings of Artificial Intelligence and Statistics.</booktitle>
<marker>Daum´e, 2009</marker>
<rawString>Hal Daum´e III. 2009. Markov random topic fields. In Proceedings of Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Dietz</author>
<author>Steffen Bickel</author>
<author>Tobias Scheffer</author>
</authors>
<title>Unsupervised prediction of citation influences.</title>
<date>2007</date>
<booktitle>In Proceedings ofInternational Conference of Machine Learning.</booktitle>
<contexts>
<context position="2775" citStr="Dietz et al., 2007" startWordPosition="430" endWordPosition="433">ostensible end users. Part of the problem is that the objective function of topic models doesn’t always correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on creating probabilistic models that incorporate human knowledge, which we extend in Section 3 to apply to ITM sessions. Secti</context>
</contexts>
<marker>Dietz, Bickel, Scheffer, 2007</marker>
<rawString>Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007. Unsupervised prediction of citation influences. In Proceedings ofInternational Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="10770" citStr="Griffiths and Steyvers, 2004" startWordPosition="1761" endWordPosition="1765"> ... M1: (a) first draw a distribution over topics Bd — Dir(α), (b) then for each token n E {1, ... Nd1: i. choose a topic assignment zd,n — Mult(Bd), and then ii. choose either a constraint or word from Mult(0zd,n): A. if we chose a word, emit that word wd,n B. otherwise if we chose a constraint index ld,n, emit a word wd,n from the constraint’s distribution over words in topic zd,n: wd,n — Mult(1rzd,n,ld,n). In this model, α, Q, and q are Dirichlet hyperparameters set by the user; their role is explained below. 3.1 Gibbs Sampling for Topic Models In topic modeling, collapsed Gibbs sampling (Griffiths and Steyvers, 2004) is a standard procedure for obtaining a Markov chain over the latent variables in the model. Given certain technical conditions, the stationary distribution of the Markov chain is the posterior (Neal, 1993). Given M documents the state of a Gibbs sampler for LDA consists of topic assignments for each token in the corpus and is represented as Z = {z1,1 ... z1,N1, z2,1,... zM,NMI. In each iteration, every token’s topic assignment zd,n is resampled based on topic assignments for all the tokens except for zd,n. (This subset of the state is denoted Z−(d,n)). The sampling equation for zd,n is //�� </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michael Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden topic Markov models.</title>
<date>2007</date>
<booktitle>In Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="2715" citStr="Gruber et al., 2007" startWordPosition="419" endWordPosition="422">pics discovered by topic modeling don’t always make sense to ostensible end users. Part of the problem is that the objective function of topic models doesn’t always correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on creating probabilistic models that incorporate human knowledge, </context>
<context position="4779" citStr="Gruber et al., 2007" startWordPosition="755" endWordPosition="758">ut, a topic model discovers K distributions over words — the namesake topics — and associations between documents and topics. In LDA both of these outputs are multinomial distributions; typically they are presented to users in summary form by listing the elements with highest probability. For an example of topics discovered from a 20-topic model of New York Times editorials, see Table 1. When presented with poor topics learned from data, users can offer a number of complaints:2 these documents should have similar topics but don’t (Daum´e III, 2009); this topic should have syntactic coherence (Gruber et al., 2007; Boyd-Graber and Blei, 2008); this topic doesn’t make any sense at all (Newman et al., 2010); this topic shouldn’t be associated with this document but is (Ramage et al., 2009); these words shouldn’t be the in same topic but are (Andrzejewski et al., 2009); or these words should be in the same topic but aren’t (Andrzejewski et al., 2009). Many of these complaints can be addressed by using “must-link” constraints on topics, retaining Andrzejewski et al’s (2009) terminology borrowed from the database literature. A “must-link” constraint is a group of words whose probability must be correlated i</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007. Hidden topic Markov models. In Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="24013" citStr="Hall et al., 2009" startWordPosition="3916" endWordPosition="3919">, country, yesterday, plan ... soviet, union, economic, reform, yeltsin, russian, lead, russia, gorbachev, leaders, west, president, boris, moscow, europe, poland, mikhail, communist, power, relations 1 2 3 20 1 2 3 20 Table 1: Five topics from a 20 topic topic model on the editorials from the New York times before adding a constraint (left) and after (right). After the constraint was added, which encouraged Russian and Soviet terms to be in the same topic, non-Russian terms gained increased prominence in Topic 1, and “Moscow” (which was not part of the constraint) appeared in Topic 20. fier (Hall et al., 2009). The lower the classification error rate, the better the model has captured the structure of the corpus.4 6.1 Generating automatic constraints We used the 20 Newsgroups corpus, which contains 18846 documents divided into 20 constituent newsgroups. We use these newsgroups as ground-truth labels.5 We simulate a user’s constraints by ranking words in the training split by their information gain (IG).6 After ranking the top 200 words for each class by IG, we delete words associated with multiple labels to prevent constraints for different labels from merging. The smallest class had 21 words remai</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="1292" citStr="Hofmann, 1999" startWordPosition="186" endWordPosition="187">fine the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions. 1 Introduction Probabilistic topic models, as exemplified by probabilistic latent semantic indexing (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003) are unsupervised statistical techniques to discover the thematic topics that permeate a large corpus of text documents. Topic models have had considerable application beyond natural language processing in computer vision (Rob et al., 2005), biology (Shringarpure and Xing, 2008), and psychology (Landauer et al., 2006) in addition to their canonical application to text. For text, one of the few real-world applications of topic models is corpus exploration. Unannotated, noisy, and ever-growing corpora are the norm rather than the exceptio</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Danielle S McNamara</author>
</authors>
<title>Probabilistic Topic Models. Laurence Erlbaum.</title>
<date>2006</date>
<editor>Dennis S. Marynick, and Walter Kintsch, editors.</editor>
<marker>Landauer, McNamara, 2006</marker>
<rawString>Thomas K. Landauer, Danielle S. McNamara, Dennis S. Marynick, and Walter Kintsch, editors. 2006. Probabilistic Topic Models. Laurence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on? identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="2670" citStr="Lin et al., 2006" startWordPosition="411" endWordPosition="414"> tables shown in topic modeling papers, topics discovered by topic modeling don’t always make sense to ostensible end users. Part of the problem is that the objective function of topic models doesn’t always correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on creating probabilis</context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on? identifying perspectives at the document and sentence levels. In Proceedings of the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: the natural language toolkit. In Tools and methodologies for teaching.</title>
<date>2002</date>
<contexts>
<context position="25445" citStr="Loper and Bird, 2002" startWordPosition="4127" endWordPosition="4130">ected topics to the state of the art features (tf-idf unigrams) gives a relative error reduction of 5.1%, while just adding topics from vanilla LDA gives a relative error reduction of 1.1%. Both measurements were obtained without tuning or weighting features, so presumably better results are possible. 5http://people.csail.mit.edu/jrennie/20Newsgroups/ In preprocessing, we deleted short documents, leaving 15160 documents, including 9131 training documents and 6029 test documents (default split). Tokenization, lemmatization, and stopword removal was performed using the Natural Language Toolkit (Loper and Bird, 2002). Topic modeling was performed using the most frequent 5000 lemmas as the vocabulary. 6IG is computed by the Rainbow toolbox http://www.cs.umass.edu/ mccallum/bow/rainbow/ overlaps of 125 words between “talk.religion.misc” and “soc.religion.christian,” and 110 words between “talk.religion.misc” and “alt.atheism”), so the top 21 words for each class were the ingredients for our simulated constraints. For example, for the class “soc.religion.christian,” the 21 constraint words include “catholic, scripture, resurrection, pope, sabbath, spiritual, pray, divine, doctrine, orthodox.” We simulate a u</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: the natural language toolkit. In Tools and methodologies for teaching.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Probabilistic inference using Markov chain Monte Carlo methods.</title>
<date>1993</date>
<tech>Technical Report CRG-TR-93-1,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="10977" citStr="Neal, 1993" startWordPosition="1796" endWordPosition="1797"> chose a word, emit that word wd,n B. otherwise if we chose a constraint index ld,n, emit a word wd,n from the constraint’s distribution over words in topic zd,n: wd,n — Mult(1rzd,n,ld,n). In this model, α, Q, and q are Dirichlet hyperparameters set by the user; their role is explained below. 3.1 Gibbs Sampling for Topic Models In topic modeling, collapsed Gibbs sampling (Griffiths and Steyvers, 2004) is a standard procedure for obtaining a Markov chain over the latent variables in the model. Given certain technical conditions, the stationary distribution of the Markov chain is the posterior (Neal, 1993). Given M documents the state of a Gibbs sampler for LDA consists of topic assignments for each token in the corpus and is represented as Z = {z1,1 ... z1,N1, z2,1,... zM,NMI. In each iteration, every token’s topic assignment zd,n is resampled based on topic assignments for all the tokens except for zd,n. (This subset of the state is denoted Z−(d,n)). The sampling equation for zd,n is //�� Td,k + α Pk,wd,n + Q p(zd,n = k |Z−cd,n), α, N) a Pk,· + V� (1) Td,· + Kα where Td,k is the number of times topic k is used in document d, Pk,wd,n is the number of times the type wd,n is assigned to topic k,</context>
</contexts>
<marker>Neal, 1993</marker>
<rawString>Radford M. Neal. 1993. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report CRG-TR-93-1, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4872" citStr="Newman et al., 2010" startWordPosition="771" endWordPosition="774">ons between documents and topics. In LDA both of these outputs are multinomial distributions; typically they are presented to users in summary form by listing the elements with highest probability. For an example of topics discovered from a 20-topic model of New York Times editorials, see Table 1. When presented with poor topics learned from data, users can offer a number of complaints:2 these documents should have similar topics but don’t (Daum´e III, 2009); this topic should have syntactic coherence (Gruber et al., 2007; Boyd-Graber and Blei, 2008); this topic doesn’t make any sense at all (Newman et al., 2010); this topic shouldn’t be associated with this document but is (Ramage et al., 2009); these words shouldn’t be the in same topic but are (Andrzejewski et al., 2009); or these words should be in the same topic but aren’t (Andrzejewski et al., 2009). Many of these complaints can be addressed by using “must-link” constraints on topics, retaining Andrzejewski et al’s (2009) terminology borrowed from the database literature. A “must-link” constraint is a group of words whose probability must be correlated in the topic. For example, Figure 1 shows an example constraint: {plant, factory}. After this </context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Roxana Girju</author>
</authors>
<title>A twodimensional topic-aspect model for discovering multifaceted topics.</title>
<date>2010</date>
<booktitle>In Association for the Advancement of Artificial Intelligence.</booktitle>
<contexts>
<context position="2651" citStr="Paul and Girju, 2010" startWordPosition="407" endWordPosition="410">mpression given by the tables shown in topic modeling papers, topics discovered by topic modeling don’t always make sense to ostensible end users. Part of the problem is that the objective function of topic models doesn’t always correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on </context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>Michael Paul and Roxana Girju. 2010. A twodimensional topic-aspect model for discovering multifaceted topics. In Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Petterson</author>
<author>Smola Alex</author>
<author>Tiberio Caetano</author>
<author>Wray Buntine</author>
<author>Narayanamurthy Shravan</author>
</authors>
<title>Word features for latent Dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2934" citStr="Petterson et al., 2010" startWordPosition="457" endWordPosition="460">). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on creating probabilistic models that incorporate human knowledge, which we extend in Section 3 to apply to ITM sessions. Section 4 discusses the implementation of this process during the inference process. Via a motivating example in Section 5, simulated ITM sessions in Section 6, and</context>
</contexts>
<marker>Petterson, Alex, Caetano, Buntine, Shravan, 2010</marker>
<rawString>James Petterson, Smola Alex, Tiberio Caetano, Wray Buntine, and Narayanamurthy Shravan. 2010. Word features for latent Dirichlet allocation. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4956" citStr="Ramage et al., 2009" startWordPosition="785" endWordPosition="788">ibutions; typically they are presented to users in summary form by listing the elements with highest probability. For an example of topics discovered from a 20-topic model of New York Times editorials, see Table 1. When presented with poor topics learned from data, users can offer a number of complaints:2 these documents should have similar topics but don’t (Daum´e III, 2009); this topic should have syntactic coherence (Gruber et al., 2007; Boyd-Graber and Blei, 2008); this topic doesn’t make any sense at all (Newman et al., 2010); this topic shouldn’t be associated with this document but is (Ramage et al., 2009); these words shouldn’t be the in same topic but are (Andrzejewski et al., 2009); or these words should be in the same topic but aren’t (Andrzejewski et al., 2009). Many of these complaints can be addressed by using “must-link” constraints on topics, retaining Andrzejewski et al’s (2009) terminology borrowed from the database literature. A “must-link” constraint is a group of words whose probability must be correlated in the topic. For example, Figure 1 shows an example constraint: {plant, factory}. After this constraint is added, the probabilities of “plant” and “factory” in each topic are li</context>
<context position="16536" citStr="Ramage et al., 2009" startWordPosition="2680" endWordPosition="2683">tates, which we call ablation (distinct from feature ablation in supervised learning). As described in the previous section, a sampler stores the topic assignment of each token. In the implementation of a Gibbs sampler, unassignment is done by setting a token’s topic assignment to an invalid topic (e.g. -1, as we use here) and decrementing any counts associated with that word. The constraints created by users implicitly signal that words in constraints don’t belong in a given topic. In other models, this input is sometimes used to “fix,” i.e. deterministically hold constant topic assignments (Ramage et al., 2009). Instead, we change the underlying model, using the current topic assignments as a starting position for a new Markov chain with some states strategically unassigned. How much of the existing topic assignments we use leads to four different options, which are illustrated in Figure 2. 251 Doc Because topic models treat the document context as exchangeable, a document is a natural context for partial state ablation. Thus if a user adds a set of words S to constraints, then we have reason to suspect that all documents containing any one of S may have incorrect topic assignments. This is reflecte</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fergus Rob</author>
<author>Li Fei-Fei</author>
<author>Perona Pietro</author>
<author>Zisserman Andrew</author>
</authors>
<title>Learning object categories from Google’s image search.</title>
<date>2005</date>
<booktitle>In International Conference on Computer Vision.</booktitle>
<contexts>
<context position="1590" citStr="Rob et al., 2005" startWordPosition="231" endWordPosition="234">ference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions. 1 Introduction Probabilistic topic models, as exemplified by probabilistic latent semantic indexing (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003) are unsupervised statistical techniques to discover the thematic topics that permeate a large corpus of text documents. Topic models have had considerable application beyond natural language processing in computer vision (Rob et al., 2005), biology (Shringarpure and Xing, 2008), and psychology (Landauer et al., 2006) in addition to their canonical application to text. For text, one of the few real-world applications of topic models is corpus exploration. Unannotated, noisy, and ever-growing corpora are the norm rather than the exception, and topic models offer a way to quickly get the gist a large corpus.1 1For examples, see Rexa http://rexa.info/, JSTOR Contrary to the impression given by the tables shown in topic modeling papers, topics discovered by topic modeling don’t always make sense to ostensible end users. Part of the </context>
</contexts>
<marker>Rob, Fei-Fei, Pietro, Andrew, 2005</marker>
<rawString>Fergus Rob, Li Fei-Fei, Perona Pietro, and Zisserman Andrew. 2005. Learning object categories from Google’s image search. In International Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="2754" citStr="Rosen-Zvi et al., 2004" startWordPosition="426" endWordPosition="429">’t always make sense to ostensible end users. Part of the problem is that the objective function of topic models doesn’t always correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on creating probabilistic models that incorporate human knowledge, which we extend in Section 3 to apply t</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, and Padhraic Smyth. 2004. The author-topic model for authors and documents. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suyash Shringarpure</author>
<author>Eric P Xing</author>
</authors>
<title>mStruct: a new admixture model for inference of population structure in light of both genetic admixing and allele mutations.</title>
<date>2008</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="1629" citStr="Shringarpure and Xing, 2008" startWordPosition="237" endWordPosition="240">riety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions. 1 Introduction Probabilistic topic models, as exemplified by probabilistic latent semantic indexing (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003) are unsupervised statistical techniques to discover the thematic topics that permeate a large corpus of text documents. Topic models have had considerable application beyond natural language processing in computer vision (Rob et al., 2005), biology (Shringarpure and Xing, 2008), and psychology (Landauer et al., 2006) in addition to their canonical application to text. For text, one of the few real-world applications of topic models is corpus exploration. Unannotated, noisy, and ever-growing corpora are the norm rather than the exception, and topic models offer a way to quickly get the gist a large corpus.1 1For examples, see Rexa http://rexa.info/, JSTOR Contrary to the impression given by the tables shown in topic modeling papers, topics discovered by topic modeling don’t always make sense to ostensible end users. Part of the problem is that the objective function </context>
</contexts>
<marker>Shringarpure, Xing, 2008</marker>
<rawString>Suyash Shringarpure and Eric P. Xing. 2008. mStruct: a new admixture model for inference of population structure in light of both genetic admixing and allele mutations. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Rethinking LDA: Why priors matter.</title>
<date>2009</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="13463" citStr="Wallach et al., 2009" startWordPosition="2238" endWordPosition="2241">n constraint Ql in topic k; V is the vocabulary size; Cl is the number of words in constraint Ql. Note the differences between these two samplers for constrained words; however, for unconstrained LDA and for unconstrained words in constrained LDA, the conditional probability is the same. In order to make the constraints effective, we set the constraint word-distribution hyperparameter q to be much larger than the hyperparameter for the distribution over constraints and vocabulary Q. This gives the constraints higher weight. Normally, estimating hyperparameters is important for topic modeling (Wallach et al., 2009). However, in ITM, sampling hyperparameters often (but not always) undoes the constraints (by making q comparable to Q), so we keep the hyperparameters fixed. 4 Interactively adding constraints [bark:2, dog:3, leash:3 dog:2] [bark:-1, dog:-1, leash:-1 dog:-1] All [bark:2, bark:2, plant:2, tree:3] [bark:-1, bark:-1, plant:-1, tree:-1] [tree:2,play:2,forest:1,leash:2] [tree:-1,play:-1,forest:-1,leash:-1] [bark:2, dog:3, leash:3 dog:2] [bark:-1, dog:-1, leash:-1 dog:-1] Doc [bark:2, bark:2, plant:2, tree:3] [bark:-1, bark:-1, plant:-1, tree:-1] [tree:2,play:2,forest:1,leash:2] [tree:2,play:2,fore</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking LDA: Why priors matter. In Proceedings ofAdvances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: Beyond bagof-words.</title>
<date>2006</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="2693" citStr="Wallach, 2006" startWordPosition="417" endWordPosition="418">ling papers, topics discovered by topic modeling don’t always make sense to ostensible end users. Part of the problem is that the objective function of topic models doesn’t always correlate with human judgements (Chang et al., 2009). Another issue is that topic models — with their bagof-words vision of the world — simply lack the necessary information to create the topics as end-users expect. There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). However, these are a priori fixes. They don’t help a frustrated consumer of topic models staring at a collection of topics that don’t make sense. In this paper, we propose interactive topic modeling (ITM), an in situ method for incorporating human knowledge into topic models. In Section 2, we review prior work on creating probabilistic models that incorpo</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach. 2006. Topic modeling: Beyond bagof-words. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<booktitle>In Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="17544" citStr="Yao et al. (2009)" startWordPosition="2876" endWordPosition="2879">for partial state ablation. Thus if a user adds a set of words S to constraints, then we have reason to suspect that all documents containing any one of S may have incorrect topic assignments. This is reflected in the state of the sampler by performing the UNASSIGN (Algorithm 1) operation for each word in any document containing a word added to a constraint. Algorithm 1 UNASSIGN(d, n, wd,n, zd,n = k) 1: T : Td,k ← Td,k − 1 2: If Wd,n ∈� nold, P : Pk,wd n ← Pk,wd n − 1 3: Else: suppose Wd,n ∈ nold m , P : Pk,m ← Pk,m − 1 W : Wk,m,wd n ← Wk,m,wd n − 1 This is equivalent to the Gibbs2 sampler of Yao et al. (2009) for incorporating new documents in a streaming context. Viewed in this light, a user is using words to select documents that should be treated as “new” for this refined model. Term Another option is to perform ablation only on the topic assignments of tokens whose words have added to a constraint. This applies the unassignment operation (Algorithm 1) only to tokens whose corresponding word appears in added constraints (i.e. a subset of the Doc strategy). This makes it less likely that other tokens in similar contexts will follow the words explicitly included in the constraints to new topic as</context>
</contexts>
<marker>Yao, Mimno, McCallum, 2009</marker>
<rawString>Limin Yao, David Mimno, and Andrew McCallum. 2009. Efficient methods for topic model inference on streaming document collections. In Knowledge Discovery and Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>