<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.991353">
Named Entity Extraction Based on A Maximum Entropy
Model and Transformation Rules
</title>
<author confidence="0.808984">
Kiyotaka Uchimoto, Qing Ma, Masaki Murata,
Hiromi Ozaku and Hitoshi Isahara
</author>
<affiliation confidence="0.96968">
Communications Research Laboratory
Ministry of Posts and Telecommunications
</affiliation>
<address confidence="0.639291">
588-2, Iwaoka, Iwaoka-cho, Nishi-ku
Kobe, Hyogo, 651-2492, Japan
[uchimot o , qma , murat a , romi , i sahara] @cr1 . go . jp
</address>
<sectionHeader confidence="0.863747" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998">
This paper describes named entity
(NE) extraction based on a max-
imum entropy (M.E.) model and
transformation rules. There are two
types of named entities when focus-
ing on the relationship between mor-
phemes and NEs as defined in the
NE task of the IREX competition
held in 1999. Each NE consists of
one or more morphemes, or includes
a substring of a morpheme. We ex-
tract the former type of NE by using
the M.E. model. We then extract
the latter type of NE by applying
transformation rules to the text.
</bodyText>
<sectionHeader confidence="0.995581" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944072727273">
Named entity (NE) extraction is one basic
technique used in information extraction. It
can also help to improve the accuracy of mor-
phological and syntactic analysis. The com-
petition held at the MUG (Message Under-
standing Conference) (SAIC, 1998) since 1980
in the U.S. has helped to improve the tech-
nique. In Japan, the &amp;quot;IREX (Information
Retrieval and Extraction Exercise)&amp;quot; project
began sponsoring a similar competition in
1998. NE extraction is one of two tasks in
the competition. The targets for extraction
in this task are the names of organizations
such as &amp;quot;0 i#-W (the Ministry of Posts and
Telecommunications),&amp;quot; people&apos;s names such
as &amp;quot;/[141A a (Yasunari Kawabata),&amp;quot; names
of locations such as &amp;quot;*F&apos;t (Kobe),&amp;quot; names of
artifacts such as &amp;quot;t a — , (Toyota&apos;s Corolla
car),&amp;quot; and expressions which represent dates,
times, sums of money, and percentages, such
as &amp;quot;9 E 28 li (September 28th),&amp;quot;&amp;quot;I&apos;k 3 *
(3 p. m.),&amp;quot; &amp;quot;100 EN (one million yen),&amp;quot; and
&amp;quot;10%.&amp;quot; There are many and various NEs, and
new ones are produced all of the time, so it is
impossible to add all of them to a dictionary.
There are also ambiguities in usage so that
a given expression may be used as a location
name in one context and as a person&apos;s name
in another context. Therefore, it is not easy
to identify NEs, and to identify the type of
each NE, in a given sentence.
There are two main approaches to extract-
ing NEs, one based on hand-crafted rules
and the other based on a machine-learning.
The former approach is costly because defini-
tions differ across applications, and the rules
have to be changed according to the applica-
tion. The machine-learning approach requires
a training corpus, but a high accuracy can
be achieved without requiring a large amount
of data if we use a learning model which in-
cludes ways of overcoming the data sparseness
problem. Therefore, we have taken the lat-
ter approach. Many methods based on max-
imum entropy (M.E.) models have been very
accurate (Ratnaparkhi, 1996; Ratnaparkhi,
1997; Borthwick et al., 1998a; Uchimoto et
al., 1999), and the M.E. model can be adapted
to deal with the data sparseness problem ef-
fectively. We have thus used the M.E. model
to extract NEs. After identifying NEs in a
given text by applying our model, we ap-
ply transformation rules which have been ac-
quired by an error-driven learning method to
the text.
</bodyText>
<sectionHeader confidence="0.8527595" genericHeader="method">
2 Named Entity Extraction
Algorithm
</sectionHeader>
<bodyText confidence="0.999892615384615">
We have used the definition of NEs which
is used in the IREX-NE task (IREX Execu-
tive Committee, 1999). Eight types of NE,
&amp;quot;ORGANIZATION&amp;quot;, &amp;quot;PERSON&amp;quot;, &amp;quot;LOCA-
TION&amp;quot;, &amp;quot;ARTIFACT&amp;quot;, &amp;quot;DATE&amp;quot;, &amp;quot;TIME&amp;quot;,
&amp;quot;MONEY&amp;quot;, and &amp;quot;PERCENT&amp;quot; are defined.
This section describes the method of identi-
fying NEs in a given text and assigning one
of eight SGML tags which represent the type
of NE to each one.
Each NE consists of one or more mor-
phemes, or includes a substring of a mor-
pheme. We define 40 NE labels, as explained
below, and extract an NE which consists of
one or more morphemes by estimating the
appropriate NE labels according to an M.E.
model. The trained M.E. model detects the
relationship between features and the NE la-
bels assigned to morphemes. The features are
clues used for estimating the labels. After es-
timating the NE labels according to the M.E.
model, we extract an NE, which includes a
substring of a morpheme, by using transfor-
mation rules that will be explained later.
In detail, the following steps are used to
extract NEs.
</bodyText>
<listItem confidence="0.888541">
1. Morphological analysis of a given text.
</listItem>
<bodyText confidence="0.994740555555556">
We used JUMAN (Kurohashi and Na-
gao, 1998) for morphological analysis.
For example, the phrase &amp;quot;A)KA Itr AIN
is di-
vided into the morphemes shown in the
first line of Table 1, and morphological
information as shown in the second and
third lines of Table 1 is assigned to each
morpheme.
</bodyText>
<listItem confidence="0.545306">
2. Assigning NE labels to each morpheme.
</listItem>
<bodyText confidence="0.986097092592593">
We defined the following 40 NE labels,
and the rules for connectivity between
the labels, which we call connectivity
rules, as shown in Table 2.
(a) We added an &amp;quot;OPTIONAL&amp;quot; tag
to the eight NE tags, then di-
vided each into four types of sub
labels which represented the begin-
ning, middle, and end of an NE,
or an NE which consisted of a sin-
gle morpheme. We thus defined
9x4=36 NE labels. For exam-
ple, the &amp;quot;PERSON&amp;quot; tag was divided
into &amp;quot;PERSON:BEGIN&amp;quot;, &amp;quot;PER-
SON:MIDDLE&amp;quot;, &amp;quot;PERSON:END&amp;quot;,
and &amp;quot;PERSON:SINGLE&amp;quot;. We di-
vided the NE tags into four types
because several morphemes can con-
stitute a single NE.
The &amp;quot;OPTIONAL&amp;quot; tag was defined
because, in some cases, even a hu-
man judge would find it difficult to
decide which tag should be assigned
to a string, or whether a string
is or is not an NE. For example,
should &amp;quot;* JP: AA (The Tokyo high
court)&amp;quot; be tagged as &amp;quot;LOCATION&amp;quot;
or &amp;quot;ORGANIZATION&amp;quot;? Should &amp;quot;
ig (Nikkei, the abbreviation of the
name of a newspaper publishing
company in Japan)&amp;quot; in the expres-
sion &amp;quot; 11i (Nikkei stock av-
erage)&amp;quot; be tagged as &amp;quot;ORGANIZA-
TION&amp;quot; or not? In these cases, &amp;quot;*TP:
AA&amp;quot; and &amp;quot; RI&amp;quot; are tagged as &amp;quot;OP-
TIONAL&amp;quot;, and are not extracted as
NEs. The definition of the &amp;quot;OP-
TIONAL&amp;quot; tag is also the same as
that which is used in the IREX-NE
task. We defined the tag to learn its
characteristics and to avoid assign-
ing NE tags to strings in such diffi-
cult cases as those explained above.
(b) We defined three more NE labels,
&amp;quot;PRE&amp;quot;, &amp;quot;POST&amp;quot;, and &amp;quot;MID&amp;quot;, to
distinguish morphemes to the left
and right, and between NEs, respec-
tively, from the other morphemes.
For example, &amp;quot;3`&lt; PR (Osaka)&amp;quot; and
&amp;quot;*Fi (Kobe)&amp;quot; in the phrase &amp;quot;Ellf
3`&lt;13R L#1)=1&amp;quot;e (Yesterday in Osaka
and Kobe...)&amp;quot; are the names of loca-
tions, so the whole phrase is tagged
in the following way.
</bodyText>
<equation confidence="0.4425">
Ef (PRE) PR (LOCATION:SINGLE)
/ (MID) /WP. (LOCATION:SINGLE)
</equation>
<tableCaption confidence="0.996124">
Table 1: Example of the assignment of NE labels by the M.E. model.
</tableCaption>
<bodyText confidence="0.901242857142857">
Entry ?`1* (zaibei, At (jos ei, (wo) is Ilt■ (chuushin, (ni) A (jinken,
staying in the U.S.) women) center) human rights)
POS (major) noun noun post positional noun post positional special noun
POS (minor) common common particle common particle beginning common noun
noun noun case marker noun case marker of brackets
Candidate 1 OTHER OTHER OTHER OTHER OTHER PRE ORG:BEGIN
of label 2 OTHER OTHER OTHER OTHER OTHER PRE ART:SINGLE
</bodyText>
<figure confidence="0.988415805555556">
• • • • • • • • •
(In this table, &amp;quot;ORG&amp;quot; and &amp;quot;ART&amp;quot; indicate &amp;quot;ORGANIZATION&amp;quot; and &amp;quot;ARTIFACT,&amp;quot; respectively.)
x kangaeru, (kai,
to think) meeting)
noun
common noun
ORG:MIDDLE ORG:END
OTHER OTHER
• • • • • •
special
end of brackets
POST
OTHER
(ga)
post positional
particle
case marker
OTHER
OTHER
(deki,
organized)
verb special
comma
OTHER OTHER
OTHER OTHER
• • • • • •
Score
0.8
0.7
post positional verb
particle
case marker
ORG:MIDDLE
POST
(wo)
/-e (POST) •••&amp;quot;
</figure>
<figureCaption confidence="0.858914">
(The labels in parentheses indicate the
candidate NE labels assigned to the
strings to their left.)
</figureCaption>
<bodyText confidence="0.99907778125">
These three labels are used in partic-
ular to distinguish morphemes to the
left or right of an NE from those to
which the &amp;quot;OTHER&amp;quot; tag, explained
immediately below, is assigned, be-
cause morphemes such as suffixes
can be clues which assist in finding
NEs.
(c) We defined the label &amp;quot;OTHER&amp;quot; to
designate morphemes to which none
of the labels defined above can be
assigned.
Given tokenization of a test corpus, the
extraction of named entities can be re-
duced to the problem of assigning one
NE label to each morpheme in each sen-
tence. The 40 NE labels form the space
of &amp;quot;futures&amp;quot; in the M.E. formulation of
our problem of extracting named enti-
ties. The M.E. model, as well as other
similar models, allows the computation
of P(flh) for any f in the space of pos-
sible futures, F, and for every h in the
space of possible histories, H. A &amp;quot;his-
tory&amp;quot; in maximum entropy is all of the
conditioning data that enable us to make
a decision in the space of futures. In the
problem of extracting named entities, we
could reformulate this in terms of finding
the probability of f associated with the
relationship at index t in the test corpus
as:
</bodyText>
<equation confidence="0.629695">
P(f iht) = P (,f &apos;Information derivable
</equation>
<bodyText confidence="0.915202272727273">
from the test corpus
related to relationship t)
The computation of P(f1h) in M.E. is
dependent on a set of &amp;quot;features&amp;quot; which
should be helpful in making a prediction
about the future. Like most current M.E.
models in computational linguistics, our
model is restricted to the features which
are binary functions of the history and
the future. For instance, one of our fea-
tures is
</bodyText>
<equation confidence="0.9991554">
g(h, f) = {1
0 : x = &amp;quot;POS(major)(0) : verb&amp;quot;
if has(h, x) = true,
otherwise.
f = 1 (1)
</equation>
<bodyText confidence="0.998784142857143">
Here &amp;quot;has(h,x)&amp;quot; is a binary function
which returns true if the history h has
the attribute x. g(h, f) in Eq. (1) can
return 1 when the major part-of-speech
of the target morpheme is verb. We use
the following information as features on
the target morpheme: a lexical item and
the parts-of-speech it belongs to, and the
same information on the four closest mor-
phemes, the two on the left and the two
on the right of the target morpheme. In
our experiments, we used 12,368 lexical
items that appeared five times or more in
the training corpus. The part-of-speech
</bodyText>
<tableCaption confidence="0.969803">
Table 2: Connectivity rules
</tableCaption>
<table confidence="0.997506727272727">
NE label labels connectable to the left labels connectable to the right
x #(BOS), x, y, x:END, y:END, PRE, MID $(E0S), x, y, x:BEGIN, y:BEGIN, POST, MID
x:BEGIN #(BOS), x, y, x:END, y:END, PRE, MID x:MIDDLE, x:END
x:MIDDLE x:BEGIN, x:MIDDLE x:MIDDLE, x:END
x:END x:BEGIN, x:MIDDLE S(E0S), x, y, x:BEGIN, y:BEGIN, POST, MID
MID x, x:END x, x:BEGIN
PRE #(BOS), POST, OTHER x, x:BEGIN
POST x, x:END S(E0S), PRE, OTHER
OTHER #(BOS), POST, OTHER S(E0S), PRE, OTHER
S(E0S) x, x:END, POST, OTHER
#(BOS) x, x:BEGIN, PRE, OTHER
</table>
<bodyText confidence="0.941932142857143">
(BOS and EOS indicate &amp;quot;beginning of sentence&amp;quot; and &amp;quot;end of sentence,&amp;quot; respectively. x and y correspond to
&amp;quot;OPTIONAL&amp;quot; or the other eight tags defined for the IREX-NE task.)
categories are the same as those used by
JUMAN. We used 27,370 features that
were found three times or more in the
training corpus.
Given a set of features and some training
data, the maximum entropy estimation
process produces a model in which every
feature g, has associated with it a param-
eter a,. This allows us to compute the
conditional probability as follows (Berger
et al., 1996):
ni a si (h,f)
</bodyText>
<equation confidence="0.758514666666667">
Z A(h)
Ell aigi (h,f
f
</equation>
<bodyText confidence="0.999829166666667">
The maximum entropy estimation tech-
nique guarantees that for every feature
g„ the expected value of 9, according to
the M.E. model will equal the empirical
expectation of g, in the training corpus.
In other words:
</bodyText>
<equation confidence="0.998221333333333">
E -15(h, f) • gi(h, f)
h,f
= E p(h) • E Pm.E.( f IN • gi(h, f ). (4)
</equation>
<bodyText confidence="0.987433809523809">
Here 15 is an empirical probability and
PALE, is the probability assigned by the
M.E. model.
Let us assume that a given sentence con-
sists of n morphemes. One of the NE
labels as defined above is assigned to
each morpheme rn,(1 &lt; i &lt; n) by using
the morphological information acquired
in the first step of the process we are de-
scribing. The NE label assigned to the
i-th morpheme m, is selected according
to probabilities estimated by a trained
M.E. model. We call the probability of
a particular NE label being assigned to a
morpheme, the labeling probability. The
labeling probability is represented by Eq.
(2). We assume that a labeling proba-
bility for a whole sentence can be deter-
mined as the product of all labeling prob-
abilities in the sentence. We employ the
Viterbi algorithm to find the optimal set
of assigned NE labels in a sentence with
the condition that the placement of la-
bels satisfies connectivity rules shown in
Table 2.
3. Post-processing by using transformation
rules.
The boundaries between morphemes
which result from analysis by JUMAN
do not always correspond to the bound-
aries between named entities as defined
in the IREX-NE task. So after the NEs
have been labeled in the second step, we
use transformation rules which are au-
tomatically determined to extract NEs
with boundaries that are not same as
those between morphemes. Transforma-
tion rules are acquired by an error-driven
learning method which is similar to that
used by Brill (Brill, 1995) for POS tag-
ging. The difference between our method
of rule acquisition and Brill&apos;s is that Brill
</bodyText>
<equation confidence="0.937587">
P(flh)
Z(h)
</equation>
<bodyText confidence="0.99986856">
uses templates to acquire rules and we
do not. In our method, rules are auto-
matically acquired by investigating the
difference between two sets of data, NE
labels in a tagged corpus and those ex-
tracted during the previous step from the
same corpus without tags. We extract
all of the differences in places where the
two data sets are broken up into a differ-
ent number of units or morphemes even
though the strings are the same, and use
them as transformation rules. For exam-
ple, the rule shown in Figure 1 was ac-
quired. The antecedent and consequent
interpretations are from the result of the
previous step and a tagged corpus, re-
spectively. If several different rules have
the same antecedent part, only the rule
with the highest frequency is chosen. If
several rules share the highest frequency,
all of the rules are removed from transfor-
mation rules. Furthermore, if there are
rules which decrease the accuracy of the
method on the training corpus, they are
removed.
</bodyText>
<listItem confidence="0.630565">
4. Transforming NE labels to NE tags.
</listItem>
<bodyText confidence="0.685768875">
After transforming NE labels to NE tags,
the &amp;quot;OPTIONAL&amp;quot; tag is removed be-
cause it is not a target of the task.
For example, &amp;quot;A (OTHER)&amp;quot; on the
first candidate in Table 1 is transformed to
&amp;quot;A (PRE) (LOCATION:SINGLE)&amp;quot; in the
third step. We get the following output after
transforming NE labels to NE tags.
</bodyText>
<listItem confidence="0.5196592">
&amp;quot;?`1 &lt;LOCATION&gt; * &lt; /LOCATION&gt; Ait WAIN 4,1
r &lt;ORGANIZATION&gt; A* A. .A• &lt; /ORGANIZATION&gt;i
&amp;quot;
3 Experiments and Discussion
3.1 Data Used in Our Experiments
</listItem>
<bodyText confidence="0.999315571428572">
For training, we used the CRL (Communica-
tions Research Laboratory) NE data, IREX-
NE dry-run training data, IREX-NE dry-
run data, and IREX-NE formal-run domain-
specific data. The total number of sentences
is about 12,000, and the total number of mor-
phemes is about 303,200. All data consist of
articles from the Mainichi newspaper, and are
tagged with the nine NE tags in SGML for-
mat. We used these data after morphologi-
cally analyzing the text and transforming the
NE tags into our new NE labels. For testing,
we used the IREX-NE formal-run data, which
consists of articles of two kinds, 71 (about 400
sentences) in a general domain and 20 (about
100 sentences) in a specific domain, the topic
being an arrest. They were selected from the
Mainichi newspaper articles which appeared
from April 14th to May 13th in 1999, and
were also tagged with NE tags 1. The defini-
tion of tags is that of the IREX-NE task.
</bodyText>
<subsectionHeader confidence="0.990877">
3.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999917888888889">
The results are shown in Table 3. The first
and second columns show the results for the
specific domain (ARREST) and the general
domain (GENERAL), respectively. We did
not tune our model to either domain. Com-
paring the results with those of experiments
carried out without transformation rules, we
found the accuracy for the formal experiments
had an F-measure, for both domains, one or
two points better than those without trans-
formation rules, as shown in Table 3.
In the IREX-NE formal-run, any tags as-
signed by a system within the region tagged
&amp;quot;OPTIONAL&amp;quot; in the formal-run data are
ignored in the evaluation. When a region
tagged by a system and the region tagged
&amp;quot;OPTIONAL&amp;quot; overlap, it is counted as an er-
ror. Our evaluation followed this standard.
</bodyText>
<subsectionHeader confidence="0.78339">
3.3 Transformation Rules and
Accuracy
</subsectionHeader>
<bodyText confidence="0.943632">
We applied the transformation rules to NEs
which included a substring of a morpheme.
The rules were applied to 18 such NEs in the
specific domain, and 79 in the general do-
main. Each of the figures represents about
5% of the NEs in the formal-run data, for
each domain. 362 rules were automatically
acquired from the training corpus. Nine rules
were applied eleven times in processing of the
specific domain data, with one error. The re-
&apos;All data are available on the IREX web site (IREX
Executive Committee, 1999).
</bodyText>
<figure confidence="0.990234923076923">
Antecedent part Consequent part
Ef (zainichi, staying in Japan)
noun
SAHEN noun
OTHER
(zai, staying) Ef (nichi, Japan)
noun noun
common noun common noun
PRE LOCATION: SINGLE
Entry
POS (major)
POS (minor)
Label
</figure>
<figureCaption confidence="0.995893">
Figure 1: Example of transformation rules.
</figureCaption>
<tableCaption confidence="0.777863">
Table 3: Results for extraction of named entities.
</tableCaption>
<table confidence="0.997283142857143">
With transformation rules Without transformation rules
Named entity ARREST GENERAL ARREST GENERAL
Recall Precision Recall Precision Recall Precision Recall Precision
(%) (%) (%) (%) (%) (%) (%) (%)
ORGANIZATION 59.46 81.48 59.28 79.55 59.46 81.48 58.73 81.85
PERSON 84.54 84.54 76.92 83.87 84.54 84.54 76.92 83.87
LOCATION 83.02 81.48 76.27 84.45 73.58 77.23 69.73 82.52
ARTIFACT 61.54 66.67 35.42 50.00 61.54 66.67 35.42 50.00
DATE 97.22 97.22 91.15 94.80 97.22 97.22 90.38 94.76
TIME 94.74 100.00 87.04 94.00 94.74 100.00 87.04 94.00
MONEY 100.00 100.00 93.33 93.33 100.00 100.00 93.33 93.33
PERCENT - - 100.00 95.45 - - 80.95 94.44
Total 81.75 86.18 74.50 85.03 79.18 85.08 72.19 84.96
F-measure 83.91 79.42 82.02 78.05
</table>
<bodyText confidence="0.957289571428571">
call and precision were 56% (10/18) and 91%
(10/11), respectively. Twelve rules were ap-
plied 42 times in processing of the general do-
main data. There were 10 errors. The re-
call and precision were 41% (32/79) and 76%
(32/42), respectively. We found the following
two types of errors.
</bodyText>
<listItem confidence="0.711935">
• A substring of an NE was extracted as
</listItem>
<bodyText confidence="0.976657266666667">
an NE by mistake in one case.
The substring (nichi, Japan)&amp;quot; was
extracted as LOCATION from the
name of a location &amp;quot;AB )KW C4 EH ft
(zainichi_beigun_yokota_kichi, an Amer-
ican military base in Yokota).&amp;quot; The
whole of &amp;quot;A Mfg EH t&amp;quot; should have
been extracted as LOCATION accord-
ing to the IREX-NE definition. The
M.E. model was not able, however, to
achieve this. Consequently, a transfor-
mation rule was applied to the whole
string, and the substring was extracted
by mistake. To reduce such errors, the
M.E. model needs to be improved.
</bodyText>
<listItem confidence="0.977862333333333">
• Definitions assigned in the test data dif-
fered from those in the training data (10
cases).
</listItem>
<bodyText confidence="0.987337387096774">
&amp;quot;Ai&amp;quot; in the word &amp;quot;IA (houjin, Japa-
nese)&amp;quot; and &amp;quot;n&amp;quot; in the word &amp;quot;A
(gaisou_kaidan, Foreign Office Minister
conference)&amp;quot; were defined as LOCATION
and ORGANIZATION, respectively, in
the training corpus while they were not
NEs in the test data. To reduce such er-
rors, maintenance of the training corpus
is essential.
We obtained an improvement of about two
points in the F-measure for the specific do-
main, and about 1.5 points in the F-measure
for the general domain, by applying transfor-
mation rules. In our experiments, the sys-
tem automatically acquired rules with conse-
quent parts that always have NEs which in-
clude a substring of a morpheme, but did not
acquire rules with consequent parts that do
not have NEs which include a substring of a
morpheme. So we carried out the experiments
with all of the rules. We then obtained F-
measures of 72.23 for the specific domain and
73.12 for the general domain. For the specific
domain the results were ten points worse, and
for the general domain five points worse, than
the accuracies of the experimental results ob-
tained without transformation rules. This re-
sult shows that the transformation rules ac-
quired for any types of NEs do not have the
ability to correctly revise NE labels assigned
by our M.E. model. However, our rule ac-
</bodyText>
<tableCaption confidence="0.977812">
Table 4: Accuracy with all feature sets, single feature sets, and one set omitted (with transfor-
mation rules).
</tableCaption>
<table confidence="0.9999204">
Feature set ARREST GENERAL
Recall Precision F Difference Recall Precision F Difference
(%) (%) (%) (%)
All 81.75 86.18 83.91 0 74.50 85.03 79.42 0
Lexical items alone 73.26 80.97 76.92 -6.99 62.58 74.29 67.94 -11.48
POS (major) alone 5.40 70.00 10.02 -73.89 2.85 42.16 5.33 -74.09
POS (minor) alone 51.41 62.50 56.42 -27.49 45.23 61.31 52.06 -27.36
No lexical items 51.41 63.49 56.82 -27.09 46.16 65.45 54.14 -25.28
No POS (major) 80.46 85.99 83.13 -0.78 72.91 82.29 77.32 -2.10
No POS (minor) 76.09 87.57 81.43 -2.48 66.89 82.72 73.97 -5.45
</table>
<tableCaption confidence="0.9941785">
Table 5: Accuracy with features of the target morpheme plus those of additional surrounding
morphemes (with transformation rules).
</tableCaption>
<table confidence="0.989283428571429">
Feature set ARREST GENERAL
Recall Precision F Difference Recall Precision F Difference
(%) (%) (%) (%)
On only (0) 31.11 48.79 37.99 -45.92 35.56 70.57 47.29 -32.13
On (-1)(0)(1) 76.86 84.46 80.48 -3.43 72.32 85.11 78.20 -1.22
On (-2) to (2) 81.75 86.18 83.91 0 74.50 85.03 79.42 0
On (-3) to (3) 80.72 85.09 82.85 -1.06 73.38 84.19 78.41 -1.01
</table>
<bodyText confidence="0.99940675">
quisition method is simple and we obtained
good results with the rules acquired for NEs
which include a substring of a morpheme. So
we can conclude that the transformation rules
acquired by our method are effective in ex-
tracting NEs which include a substring of a
morpheme, which cannot be extracted by our
M.E. model.
</bodyText>
<subsectionHeader confidence="0.92438">
3.4 Features and Accuracy
</subsectionHeader>
<bodyText confidence="0.999992894736842">
This section describes how much each feature
set contributes to improving the accuracy.
We carried out the experiments with each
feature set alone, and with all feature sets but
one, omitting each in turn. We used trans-
formation rules in those experiments. Ta-
ble 4 shows the performance under these con-
ditions. In this table, &amp;quot;F&amp;quot; indicates the F-
measure and &amp;quot;Difference&amp;quot; indicates the degra-
dation from the results for the formal experi-
ment. We achieved high accuracy with lex-
ical items, and the accuracy decreased sig-
nificantly when lexical items were not used.
This result shows that the lexical items are
the most important features for improving the
accuracy.
Table 5 is a comparison with performance
of the analysis for features of the target mor-
pheme alone, and for performance with the
features of surrounding morphemes as well.
In this table, &amp;quot;On only (0)&amp;quot; indicates that we
used features of the target morpheme alone,
&amp;quot;On (-1) to (1)&amp;quot; indicates that we used fea-
tures of the target morpheme and two adja-
cent morphemes. &amp;quot;On (-2) to (2)&amp;quot; indicates
that we used features of the target morpheme
and four other morphemes, the two on the left
and the two on the right of the target. &amp;quot;On
(-3) to (3)&amp;quot; indicates that we used features
of the target morpheme and the six nearest
morphemes, i.e., the three on the left and the
three on the right. The best accuracy was
achieved when we used the features of the
target morpheme and the four nearest mor-
phemes. The accuracy decreased when we
used the features of the target morpheme and
the six nearest morphemes. We believe that
it is due to the data sparseness problem.
</bodyText>
<subsectionHeader confidence="0.903226">
3.5 Amount of Training Data and
Accuracy
</subsectionHeader>
<bodyText confidence="0.984222142857143">
Figure 2 shows the relationship between the
amount of training data (the number of sen-
tences) and accuracy. The horizontal axis in-
dicates the number of sentences in training
data, and the vertical axis indicates the F-
measure. In this figure, the notation &amp;quot;arrest&amp;quot;
and &amp;quot;general&amp;quot; are used to indicate the results
</bodyText>
<figure confidence="0.984529684210527">
&amp;quot;arrest.with_rules&amp;quot;
&amp;quot;arrest.without_rules&amp;quot;
&amp;quot;general.with_rules&amp;quot;
&amp;quot;general.without_rules&amp;quot;
85
80
75
70
65
60
0 2000 4000 6000 8000 10000 12000
Number of sentences
F-measure
a
❾
❾
100
95
90
</figure>
<bodyText confidence="0.99949125">
(Borthwick et al., 1998a), collocation statis-
tics (Lin, 1998), and a transformation-based
error-driven learning model (Aberdeen et al.,
1995) have been proposed so far. In the MUG
competition, the highest accuracy has been
achieved by a system called Nymble (Bikel et
al., 1997) which is based on an HMM. This
system extracts NEs by applying the follow-
ing procedure. A finite-state transition net-
work is prepared. Each state of the net-
work represents an NE defined in the MUG-
NE task, such as PERSON or ORGANIZA-
TION, or represents NOT-A-NAME which
means the word is not a defined NE. Each
transition has a transition probability, which
represents the transition&apos;s conditional proba-
bility for a given input word. The analysis
is a search for the optimal path in the net-
work which uses the Viterbi algorithm. The
states in the optimal path give us NEs. In the
other systems, named entities are extracted
by a similar procedure, except that the way
of estimating the probability varies. Borth-
wick and his coworkers selected several sys-
tems which obtained a high accuracy in the
MUG-NE task from among those based on
statistical methods and those based on hand-
crafted rules, and obtained better results than
any of the individual systems by integrating
them on the basis of the M.E. model (Borth-
wick et al., 1998a). They reported that a
good accuracy which surpassed human per-
formance could be obtained for a certain data
set by integrating several systems (Borthwick
et al., 1998b).
With regard to named entity extraction
from Japanese sentences, similar statistical
methods have been proposed, including meth-
ods based on an HMM (Shinnou, 1999), a
decision tree model (Sekine et al., 1998; No-
bata, 1999), and an M.E. model (Borthwick,
1999). Borthwick&apos;s approach is similar to ours
except that he used hand-crafted transforma-
tion rules while we use automatically acquired
rules alone. The accuracy we reported in
Section 3.6 is better than that which Borth-
wick obtained. Our method is more accurate
than any other system based on a statistical
method that participated in the last IREX-
NE workshop, and is close to that obtained
by the system which obtained the highest ac-
curacy for the IREX-NE task.
</bodyText>
<sectionHeader confidence="0.995813" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.991307279069767">
This paper described the extraction of named
entities on the basis of an M.E. (maximum en-
tropy) model and transformation rules. Eight
types of NE are defined by IREX-NE, and
each NE consists of one or more morphemes,
or includes a substring of a morpheme. We
defined 40 NE labels to indicate the begin-
ning, middle, and end of NEs, and extract
NEs which consist of one or more morphemes
by estimating the labels according to an M.E.
model. After this estimation, we extract NEs,
which include a substring of a morpheme, by
using transformation rules. These rules are
automatically acquired by investigating the
difference between NE labels in a tagged cor-
pus and those extracted from the same corpus
without tags by our system.
Through our experiments, we found that
the transformation rules contribute to an im-
proved accuracy, lexical items are the most
important features, and the best accuracy was
achieved when we used the features of the tar-
get morpheme and the four morphemes clos-
est to it, i.e., the two on the left and the
two on the right, when a training corpus with
12,000 sentences was used. These results were
obtained with the information in the training
corpus alone. When we used an NE dictio-
nary which is available on the web as well, we
achieved an F-measure of 85.75 for a specific
domain, and 80.17 for a general domain, for
IREX-NE formal-run data.
There are several possible future directions.
In particular, we are interested in the follow-
ing issues.
• Finding effective features
We expect that we can achieve higher ac-
curacy by using information that we are
not using at the moment, such as infor-
mation on dependencies between phrasal
units called &apos;bunsetsu&apos;, anaphoric rela-
tions, and the information given in the
process of analyzing text.
</bodyText>
<listItem confidence="0.990857">
• Corpus revision and an NE dictionary
</listItem>
<bodyText confidence="0.999990636363636">
We found that errors in a training corpus
will lead to a lower accuracy, and that
dictionary information helps to improve
the accuracy. Therefore, corpus revision
should be actively studied, and larger NE
dictionaries will also be helpful.
We may be able to tune the model to
a particular domain by preparing an NE
dictionary adapted to the domain. We
would like to try this, and see how well
an adapted dictionary works.
</bodyText>
<sectionHeader confidence="0.997683" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998455">
The authors would like to thank Satoshi
Sekine and Andrew Borthwic for fruitful com-
ments and helpful discussions during the
progress of this work.
</bodyText>
<sectionHeader confidence="0.997374" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822804597701">
John Aberdeen, John Burger, David Day, Lynette
Hirschman, Patricia Robinson, and Marc Vilain.
1995. MITRE: Description of the ALEMBIC
System used for MUC-6. In Proceedings of the
Sixth Message Understanding Conference (MUC-
6), pages 141-155.
Adam L Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy
Approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39-71.
Daniel NI. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a High-
Performance Learning Name-finder. In Proceedings
of the Fifth Conference on Applied Natural Lan-
guage Processing, pages 194-201.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998a. Exploiting Di-
verse Knowledge Sources via Maximum Entropy in
Named Entity Recognition. In Proceedings of the
Sixth Workshop on Very Large Corpora, pages 152-
160.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998b. NYU: Descrip-
tion of the MENE Named Entity System as
Used in MUC-7. In Proceedings of the Sev-
enth Message Understanding Conference (MUC-7).
http://www.muc.saic.com/.
Andrew Borthwick. 1999. A Japanese Named En-
tity Recognizer Constructed by a Non-Speaker of
Japanese. In Proceedings of the IREX Workshop,
pages 187-193.
Eric Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A
Case Study in Part-of-Speech Tagging. Computa-
tional Linguistics, 21(4):543-565.
Jim Cowie. 1995. CRL/NMSU Description of the
CRL/NMSU System Used for MUC-6. In Proceed-
ings of the Sixth Message Understanding Confer-
ence (MUC-6), pages 157-166.
IREX Executive Committee. 1999. IREX homepage.
http://cs.nyu.edu/cs/projects/proteus/irex/.
Sadao Kurohashi and Makoto Nagao, 1998. Japanese
Morphological Analysis System JUMAN Version
3.6. Department of Informatics, Kyoto University.
Dekang Lin. 1998. Using Collocation Statistics in
Information Extraction. In Proceedings of the Sev-
enth Message Understanding Conference (MUC-7).
http://www.muc.saic.com/.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that Learn to Extract Infor-
mation BBN: Description of the Sift System as
Used for MUC-7. In Proceedings of the Sev-
enth Message Understanding Conference (MUC-7).
http://www.muc.saic.com/.
Chikashi Nobata. 1999. Named Entity Tagging Sys-
tem Based on a Decision Tree Model. In Proceed-
ings of the IREX Workshop, pages 201-206. (in
Japanese).
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Conference
on Empirical Methods in Natural Language Process-
ing, pages 133-142.
Adwait Ratnaparkhi. 1997. A Linear Observed
Time Statistical Parser Based on Maximum En-
tropy Models. In Conference on Empirical Methods
in Natural Language Processing.
SAIC. 1998. MUC homepage. http://www.muc.saic.
com/.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A Decision Tree Method for Finding
and Classifying Names in Japanese Texts. In Pro-
ceedings of the Sixth Workshop on Very Large Cor-
pora, pages 171-178.
Satoshi Sekine. 1999. Satoshi Sekine homepage.
http://www.cs.nyu.edu/cs/projects/proteus/sekine/.
Hiroyuki Shinnou. 1999. Extraction of Proper Nouns
through Extended Character Based HIVINI. In Pro-
ceedings of the IREX Workshop, pages 151-157. (in
Japanese).
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese Dependency Structure Anal-
ysis Based on Maximum Entropy Models. In Pro-
ceedings of the Ninth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL&apos;99), pages 196-203.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.343256">
<title confidence="0.999135">Named Entity Extraction Based on A Maximum Entropy Model and Transformation Rules</title>
<author confidence="0.989654">Kiyotaka Uchimoto</author>
<author confidence="0.989654">Qing Ma</author>
<author confidence="0.989654">Masaki Murata</author>
<affiliation confidence="0.829397">Hiromi Ozaku and Hitoshi Isahara Communications Research Laboratory Ministry of Posts and Telecommunications</affiliation>
<address confidence="0.9781515">588-2, Iwaoka, Iwaoka-cho, Nishi-ku Kobe, Hyogo, 651-2492, Japan</address>
<email confidence="0.721355">o,qma,murata,romi,isahara]@cr1..jp</email>
<abstract confidence="0.9992178125">This paper describes named entity (NE) extraction based on a maximum entropy (M.E.) model and transformation rules. There are two types of named entities when focusing on the relationship between morphemes and NEs as defined in the NE task of the IREX competition held in 1999. Each NE consists of one or more morphemes, or includes a substring of a morpheme. We extract the former type of NE by using the M.E. model. We then extract the latter type of NE by applying transformation rules to the text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Aberdeen</author>
<author>John Burger</author>
<author>David Day</author>
<author>Lynette Hirschman</author>
<author>Patricia Robinson</author>
<author>Marc Vilain</author>
</authors>
<title>MITRE: Description of the ALEMBIC System used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC6),</booktitle>
<pages>141--155</pages>
<contexts>
<context position="23440" citStr="Aberdeen et al., 1995" startWordPosition="4060" endWordPosition="4063">he relationship between the amount of training data (the number of sentences) and accuracy. The horizontal axis indicates the number of sentences in training data, and the vertical axis indicates the Fmeasure. In this figure, the notation &amp;quot;arrest&amp;quot; and &amp;quot;general&amp;quot; are used to indicate the results &amp;quot;arrest.with_rules&amp;quot; &amp;quot;arrest.without_rules&amp;quot; &amp;quot;general.with_rules&amp;quot; &amp;quot;general.without_rules&amp;quot; 85 80 75 70 65 60 0 2000 4000 6000 8000 10000 12000 Number of sentences F-measure a   100 95 90 (Borthwick et al., 1998a), collocation statistics (Lin, 1998), and a transformation-based error-driven learning model (Aberdeen et al., 1995) have been proposed so far. In the MUG competition, the highest accuracy has been achieved by a system called Nymble (Bikel et al., 1997) which is based on an HMM. This system extracts NEs by applying the following procedure. A finite-state transition network is prepared. Each state of the network represents an NE defined in the MUGNE task, such as PERSON or ORGANIZATION, or represents NOT-A-NAME which means the word is not a defined NE. Each transition has a transition probability, which represents the transition&apos;s conditional probability for a given input word. The analysis is a search for t</context>
</contexts>
<marker>Aberdeen, Burger, Day, Hirschman, Robinson, Vilain, 1995</marker>
<rawString>John Aberdeen, John Burger, David Day, Lynette Hirschman, Patricia Robinson, and Marc Vilain. 1995. MITRE: Description of the ALEMBIC System used for MUC-6. In Proceedings of the Sixth Message Understanding Conference (MUC6), pages 141-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="10819" citStr="Berger et al., 1996" startWordPosition="1896" endWordPosition="1899">ER S(E0S) x, x:END, POST, OTHER #(BOS) x, x:BEGIN, PRE, OTHER (BOS and EOS indicate &amp;quot;beginning of sentence&amp;quot; and &amp;quot;end of sentence,&amp;quot; respectively. x and y correspond to &amp;quot;OPTIONAL&amp;quot; or the other eight tags defined for the IREX-NE task.) categories are the same as those used by JUMAN. We used 27,370 features that were found three times or more in the training corpus. Given a set of features and some training data, the maximum entropy estimation process produces a model in which every feature g, has associated with it a parameter a,. This allows us to compute the conditional probability as follows (Berger et al., 1996): ni a si (h,f) Z A(h) Ell aigi (h,f f The maximum entropy estimation technique guarantees that for every feature g„ the expected value of 9, according to the M.E. model will equal the empirical expectation of g, in the training corpus. In other words: E -15(h, f) • gi(h, f) h,f = E p(h) • E Pm.E.( f IN • gi(h, f ). (4) Here 15 is an empirical probability and PALE, is the probability assigned by the M.E. model. Let us assume that a given sentence consists of n morphemes. One of the NE labels as defined above is assigned to each morpheme rn,(1 &lt; i &lt; n) by using the morphological information acq</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a HighPerformance Learning Name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="23577" citStr="Bikel et al., 1997" startWordPosition="4084" endWordPosition="4087">ences in training data, and the vertical axis indicates the Fmeasure. In this figure, the notation &amp;quot;arrest&amp;quot; and &amp;quot;general&amp;quot; are used to indicate the results &amp;quot;arrest.with_rules&amp;quot; &amp;quot;arrest.without_rules&amp;quot; &amp;quot;general.with_rules&amp;quot; &amp;quot;general.without_rules&amp;quot; 85 80 75 70 65 60 0 2000 4000 6000 8000 10000 12000 Number of sentences F-measure a   100 95 90 (Borthwick et al., 1998a), collocation statistics (Lin, 1998), and a transformation-based error-driven learning model (Aberdeen et al., 1995) have been proposed so far. In the MUG competition, the highest accuracy has been achieved by a system called Nymble (Bikel et al., 1997) which is based on an HMM. This system extracts NEs by applying the following procedure. A finite-state transition network is prepared. Each state of the network represents an NE defined in the MUGNE task, such as PERSON or ORGANIZATION, or represents NOT-A-NAME which means the word is not a defined NE. Each transition has a transition probability, which represents the transition&apos;s conditional probability for a given input word. The analysis is a search for the optimal path in the network which uses the Viterbi algorithm. The states in the optimal path give us NEs. In the other systems, named </context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1997</marker>
<rawString>Daniel NI. Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. Nymble: a HighPerformance Learning Name-finder. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 194-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
<author>John Sterling</author>
<author>Eugene Agichtein</author>
<author>Ralph Grishman</author>
</authors>
<title>Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>152--160</pages>
<contexts>
<context position="2897" citStr="Borthwick et al., 1998" startWordPosition="491" endWordPosition="494">e based on hand-crafted rules and the other based on a machine-learning. The former approach is costly because definitions differ across applications, and the rules have to be changed according to the application. The machine-learning approach requires a training corpus, but a high accuracy can be achieved without requiring a large amount of data if we use a learning model which includes ways of overcoming the data sparseness problem. Therefore, we have taken the latter approach. Many methods based on maximum entropy (M.E.) models have been very accurate (Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998a; Uchimoto et al., 1999), and the M.E. model can be adapted to deal with the data sparseness problem effectively. We have thus used the M.E. model to extract NEs. After identifying NEs in a given text by applying our model, we apply transformation rules which have been acquired by an error-driven learning method to the text. 2 Named Entity Extraction Algorithm We have used the definition of NEs which is used in the IREX-NE task (IREX Executive Committee, 1999). Eight types of NE, &amp;quot;ORGANIZATION&amp;quot;, &amp;quot;PERSON&amp;quot;, &amp;quot;LOCATION&amp;quot;, &amp;quot;ARTIFACT&amp;quot;, &amp;quot;DATE&amp;quot;, &amp;quot;TIME&amp;quot;, &amp;quot;MONEY&amp;quot;, and &amp;quot;PERCENT&amp;quot; are defined. This section</context>
<context position="23322" citStr="Borthwick et al., 1998" startWordPosition="4045" endWordPosition="4048">s. We believe that it is due to the data sparseness problem. 3.5 Amount of Training Data and Accuracy Figure 2 shows the relationship between the amount of training data (the number of sentences) and accuracy. The horizontal axis indicates the number of sentences in training data, and the vertical axis indicates the Fmeasure. In this figure, the notation &amp;quot;arrest&amp;quot; and &amp;quot;general&amp;quot; are used to indicate the results &amp;quot;arrest.with_rules&amp;quot; &amp;quot;arrest.without_rules&amp;quot; &amp;quot;general.with_rules&amp;quot; &amp;quot;general.without_rules&amp;quot; 85 80 75 70 65 60 0 2000 4000 6000 8000 10000 12000 Number of sentences F-measure a   100 95 90 (Borthwick et al., 1998a), collocation statistics (Lin, 1998), and a transformation-based error-driven learning model (Aberdeen et al., 1995) have been proposed so far. In the MUG competition, the highest accuracy has been achieved by a system called Nymble (Bikel et al., 1997) which is based on an HMM. This system extracts NEs by applying the following procedure. A finite-state transition network is prepared. Each state of the network represents an NE defined in the MUGNE task, such as PERSON or ORGANIZATION, or represents NOT-A-NAME which means the word is not a defined NE. Each transition has a transition probabi</context>
<context position="24606" citStr="Borthwick et al., 1998" startWordPosition="4262" endWordPosition="4266">y for a given input word. The analysis is a search for the optimal path in the network which uses the Viterbi algorithm. The states in the optimal path give us NEs. In the other systems, named entities are extracted by a similar procedure, except that the way of estimating the probability varies. Borthwick and his coworkers selected several systems which obtained a high accuracy in the MUG-NE task from among those based on statistical methods and those based on handcrafted rules, and obtained better results than any of the individual systems by integrating them on the basis of the M.E. model (Borthwick et al., 1998a). They reported that a good accuracy which surpassed human performance could be obtained for a certain data set by integrating several systems (Borthwick et al., 1998b). With regard to named entity extraction from Japanese sentences, similar statistical methods have been proposed, including methods based on an HMM (Shinnou, 1999), a decision tree model (Sekine et al., 1998; Nobata, 1999), and an M.E. model (Borthwick, 1999). Borthwick&apos;s approach is similar to ours except that he used hand-crafted transformation rules while we use automatically acquired rules alone. The accuracy we reported i</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>Andrew Borthwick, John Sterling, Eugene Agichtein, and Ralph Grishman. 1998a. Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 152-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
<author>John Sterling</author>
<author>Eugene Agichtein</author>
<author>Ralph Grishman</author>
</authors>
<date>1998</date>
<booktitle>NYU: Description of the MENE Named Entity System as Used in MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7). http://www.muc.saic.com/.</booktitle>
<contexts>
<context position="2897" citStr="Borthwick et al., 1998" startWordPosition="491" endWordPosition="494">e based on hand-crafted rules and the other based on a machine-learning. The former approach is costly because definitions differ across applications, and the rules have to be changed according to the application. The machine-learning approach requires a training corpus, but a high accuracy can be achieved without requiring a large amount of data if we use a learning model which includes ways of overcoming the data sparseness problem. Therefore, we have taken the latter approach. Many methods based on maximum entropy (M.E.) models have been very accurate (Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998a; Uchimoto et al., 1999), and the M.E. model can be adapted to deal with the data sparseness problem effectively. We have thus used the M.E. model to extract NEs. After identifying NEs in a given text by applying our model, we apply transformation rules which have been acquired by an error-driven learning method to the text. 2 Named Entity Extraction Algorithm We have used the definition of NEs which is used in the IREX-NE task (IREX Executive Committee, 1999). Eight types of NE, &amp;quot;ORGANIZATION&amp;quot;, &amp;quot;PERSON&amp;quot;, &amp;quot;LOCATION&amp;quot;, &amp;quot;ARTIFACT&amp;quot;, &amp;quot;DATE&amp;quot;, &amp;quot;TIME&amp;quot;, &amp;quot;MONEY&amp;quot;, and &amp;quot;PERCENT&amp;quot; are defined. This section</context>
<context position="23322" citStr="Borthwick et al., 1998" startWordPosition="4045" endWordPosition="4048">s. We believe that it is due to the data sparseness problem. 3.5 Amount of Training Data and Accuracy Figure 2 shows the relationship between the amount of training data (the number of sentences) and accuracy. The horizontal axis indicates the number of sentences in training data, and the vertical axis indicates the Fmeasure. In this figure, the notation &amp;quot;arrest&amp;quot; and &amp;quot;general&amp;quot; are used to indicate the results &amp;quot;arrest.with_rules&amp;quot; &amp;quot;arrest.without_rules&amp;quot; &amp;quot;general.with_rules&amp;quot; &amp;quot;general.without_rules&amp;quot; 85 80 75 70 65 60 0 2000 4000 6000 8000 10000 12000 Number of sentences F-measure a   100 95 90 (Borthwick et al., 1998a), collocation statistics (Lin, 1998), and a transformation-based error-driven learning model (Aberdeen et al., 1995) have been proposed so far. In the MUG competition, the highest accuracy has been achieved by a system called Nymble (Bikel et al., 1997) which is based on an HMM. This system extracts NEs by applying the following procedure. A finite-state transition network is prepared. Each state of the network represents an NE defined in the MUGNE task, such as PERSON or ORGANIZATION, or represents NOT-A-NAME which means the word is not a defined NE. Each transition has a transition probabi</context>
<context position="24606" citStr="Borthwick et al., 1998" startWordPosition="4262" endWordPosition="4266">y for a given input word. The analysis is a search for the optimal path in the network which uses the Viterbi algorithm. The states in the optimal path give us NEs. In the other systems, named entities are extracted by a similar procedure, except that the way of estimating the probability varies. Borthwick and his coworkers selected several systems which obtained a high accuracy in the MUG-NE task from among those based on statistical methods and those based on handcrafted rules, and obtained better results than any of the individual systems by integrating them on the basis of the M.E. model (Borthwick et al., 1998a). They reported that a good accuracy which surpassed human performance could be obtained for a certain data set by integrating several systems (Borthwick et al., 1998b). With regard to named entity extraction from Japanese sentences, similar statistical methods have been proposed, including methods based on an HMM (Shinnou, 1999), a decision tree model (Sekine et al., 1998; Nobata, 1999), and an M.E. model (Borthwick, 1999). Borthwick&apos;s approach is similar to ours except that he used hand-crafted transformation rules while we use automatically acquired rules alone. The accuracy we reported i</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>Andrew Borthwick, John Sterling, Eugene Agichtein, and Ralph Grishman. 1998b. NYU: Description of the MENE Named Entity System as Used in MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7). http://www.muc.saic.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Japanese Named Entity Recognizer Constructed by a Non-Speaker of Japanese.</title>
<date>1999</date>
<booktitle>In Proceedings of the IREX Workshop,</booktitle>
<pages>187--193</pages>
<contexts>
<context position="25035" citStr="Borthwick, 1999" startWordPosition="4333" endWordPosition="4334">cal methods and those based on handcrafted rules, and obtained better results than any of the individual systems by integrating them on the basis of the M.E. model (Borthwick et al., 1998a). They reported that a good accuracy which surpassed human performance could be obtained for a certain data set by integrating several systems (Borthwick et al., 1998b). With regard to named entity extraction from Japanese sentences, similar statistical methods have been proposed, including methods based on an HMM (Shinnou, 1999), a decision tree model (Sekine et al., 1998; Nobata, 1999), and an M.E. model (Borthwick, 1999). Borthwick&apos;s approach is similar to ours except that he used hand-crafted transformation rules while we use automatically acquired rules alone. The accuracy we reported in Section 3.6 is better than that which Borthwick obtained. Our method is more accurate than any other system based on a statistical method that participated in the last IREXNE workshop, and is close to that obtained by the system which obtained the highest accuracy for the IREX-NE task. 4 Conclusion This paper described the extraction of named entities on the basis of an M.E. (maximum entropy) model and transformation rules.</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Japanese Named Entity Recognizer Constructed by a Non-Speaker of Japanese. In Proceedings of the IREX Workshop, pages 187-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="12613" citStr="Brill, 1995" startWordPosition="2217" endWordPosition="2218">on that the placement of labels satisfies connectivity rules shown in Table 2. 3. Post-processing by using transformation rules. The boundaries between morphemes which result from analysis by JUMAN do not always correspond to the boundaries between named entities as defined in the IREX-NE task. So after the NEs have been labeled in the second step, we use transformation rules which are automatically determined to extract NEs with boundaries that are not same as those between morphemes. Transformation rules are acquired by an error-driven learning method which is similar to that used by Brill (Brill, 1995) for POS tagging. The difference between our method of rule acquisition and Brill&apos;s is that Brill P(flh) Z(h) uses templates to acquire rules and we do not. In our method, rules are automatically acquired by investigating the difference between two sets of data, NE labels in a tagged corpus and those extracted during the previous step from the same corpus without tags. We extract all of the differences in places where the two data sets are broken up into a different number of units or morphemes even though the strings are the same, and use them as transformation rules. For example, the rule sh</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Cowie</author>
</authors>
<title>CRL/NMSU Description of the CRL/NMSU System Used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>157--166</pages>
<marker>Cowie, 1995</marker>
<rawString>Jim Cowie. 1995. CRL/NMSU Description of the CRL/NMSU System Used for MUC-6. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 157-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IREX Executive Committee</author>
</authors>
<date>1999</date>
<note>IREX homepage. http://cs.nyu.edu/cs/projects/proteus/irex/.</note>
<contexts>
<context position="3362" citStr="Committee, 1999" startWordPosition="576" endWordPosition="577">ter approach. Many methods based on maximum entropy (M.E.) models have been very accurate (Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998a; Uchimoto et al., 1999), and the M.E. model can be adapted to deal with the data sparseness problem effectively. We have thus used the M.E. model to extract NEs. After identifying NEs in a given text by applying our model, we apply transformation rules which have been acquired by an error-driven learning method to the text. 2 Named Entity Extraction Algorithm We have used the definition of NEs which is used in the IREX-NE task (IREX Executive Committee, 1999). Eight types of NE, &amp;quot;ORGANIZATION&amp;quot;, &amp;quot;PERSON&amp;quot;, &amp;quot;LOCATION&amp;quot;, &amp;quot;ARTIFACT&amp;quot;, &amp;quot;DATE&amp;quot;, &amp;quot;TIME&amp;quot;, &amp;quot;MONEY&amp;quot;, and &amp;quot;PERCENT&amp;quot; are defined. This section describes the method of identifying NEs in a given text and assigning one of eight SGML tags which represent the type of NE to each one. Each NE consists of one or more morphemes, or includes a substring of a morpheme. We define 40 NE labels, as explained below, and extract an NE which consists of one or more morphemes by estimating the appropriate NE labels according to an M.E. model. The trained M.E. model detects the relationship between features and the NE</context>
<context position="16423" citStr="Committee, 1999" startWordPosition="2879" endWordPosition="2880">it is counted as an error. Our evaluation followed this standard. 3.3 Transformation Rules and Accuracy We applied the transformation rules to NEs which included a substring of a morpheme. The rules were applied to 18 such NEs in the specific domain, and 79 in the general domain. Each of the figures represents about 5% of the NEs in the formal-run data, for each domain. 362 rules were automatically acquired from the training corpus. Nine rules were applied eleven times in processing of the specific domain data, with one error. The re&apos;All data are available on the IREX web site (IREX Executive Committee, 1999). Antecedent part Consequent part Ef (zainichi, staying in Japan) noun SAHEN noun OTHER (zai, staying) Ef (nichi, Japan) noun noun common noun common noun PRE LOCATION: SINGLE Entry POS (major) POS (minor) Label Figure 1: Example of transformation rules. Table 3: Results for extraction of named entities. With transformation rules Without transformation rules Named entity ARREST GENERAL ARREST GENERAL Recall Precision Recall Precision Recall Precision Recall Precision (%) (%) (%) (%) (%) (%) (%) (%) ORGANIZATION 59.46 81.48 59.28 79.55 59.46 81.48 58.73 81.85 PERSON 84.54 84.54 76.92 83.87 84.5</context>
</contexts>
<marker>Committee, 1999</marker>
<rawString>IREX Executive Committee. 1999. IREX homepage. http://cs.nyu.edu/cs/projects/proteus/irex/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<date>1998</date>
<booktitle>Japanese Morphological Analysis System JUMAN Version 3.6.</booktitle>
<institution>Department of Informatics, Kyoto University.</institution>
<contexts>
<context position="4368" citStr="Kurohashi and Nagao, 1998" startWordPosition="748" endWordPosition="752">s explained below, and extract an NE which consists of one or more morphemes by estimating the appropriate NE labels according to an M.E. model. The trained M.E. model detects the relationship between features and the NE labels assigned to morphemes. The features are clues used for estimating the labels. After estimating the NE labels according to the M.E. model, we extract an NE, which includes a substring of a morpheme, by using transformation rules that will be explained later. In detail, the following steps are used to extract NEs. 1. Morphological analysis of a given text. We used JUMAN (Kurohashi and Nagao, 1998) for morphological analysis. For example, the phrase &amp;quot;A)KA Itr AIN is divided into the morphemes shown in the first line of Table 1, and morphological information as shown in the second and third lines of Table 1 is assigned to each morpheme. 2. Assigning NE labels to each morpheme. We defined the following 40 NE labels, and the rules for connectivity between the labels, which we call connectivity rules, as shown in Table 2. (a) We added an &amp;quot;OPTIONAL&amp;quot; tag to the eight NE tags, then divided each into four types of sub labels which represented the beginning, middle, and end of an NE, or an NE wh</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morphological Analysis System JUMAN Version 3.6. Department of Informatics, Kyoto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Using Collocation Statistics in Information Extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7). http://www.muc.saic.com/.</booktitle>
<contexts>
<context position="23360" citStr="Lin, 1998" startWordPosition="4052" endWordPosition="4053">s problem. 3.5 Amount of Training Data and Accuracy Figure 2 shows the relationship between the amount of training data (the number of sentences) and accuracy. The horizontal axis indicates the number of sentences in training data, and the vertical axis indicates the Fmeasure. In this figure, the notation &amp;quot;arrest&amp;quot; and &amp;quot;general&amp;quot; are used to indicate the results &amp;quot;arrest.with_rules&amp;quot; &amp;quot;arrest.without_rules&amp;quot; &amp;quot;general.with_rules&amp;quot; &amp;quot;general.without_rules&amp;quot; 85 80 75 70 65 60 0 2000 4000 6000 8000 10000 12000 Number of sentences F-measure a   100 95 90 (Borthwick et al., 1998a), collocation statistics (Lin, 1998), and a transformation-based error-driven learning model (Aberdeen et al., 1995) have been proposed so far. In the MUG competition, the highest accuracy has been achieved by a system called Nymble (Bikel et al., 1997) which is based on an HMM. This system extracts NEs by applying the following procedure. A finite-state transition network is prepared. Each state of the network represents an NE defined in the MUGNE task, such as PERSON or ORGANIZATION, or represents NOT-A-NAME which means the word is not a defined NE. Each transition has a transition probability, which represents the transition&apos;</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Using Collocation Statistics in Information Extraction. In Proceedings of the Seventh Message Understanding Conference (MUC-7). http://www.muc.saic.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Michael Crystal</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Richard Schwartz</author>
<author>Rebecca Stone</author>
<author>Ralph Weischedel</author>
</authors>
<title>and the Annotation Group.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7). http://www.muc.saic.com/.</booktitle>
<marker>Miller, Crystal, Fox, Ramshaw, Schwartz, Stone, Weischedel, 1998</marker>
<rawString>Scott Miller, Michael Crystal, Heidi Fox, Lance Ramshaw, Richard Schwartz, Rebecca Stone, Ralph Weischedel, and the Annotation Group. 1998. Algorithms that Learn to Extract Information BBN: Description of the Sift System as Used for MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7). http://www.muc.saic.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikashi Nobata</author>
</authors>
<title>Named Entity Tagging System Based on a Decision Tree Model.</title>
<date>1999</date>
<booktitle>In Proceedings of the IREX Workshop,</booktitle>
<pages>201--206</pages>
<note>(in Japanese).</note>
<contexts>
<context position="24998" citStr="Nobata, 1999" startWordPosition="4326" endWordPosition="4328">from among those based on statistical methods and those based on handcrafted rules, and obtained better results than any of the individual systems by integrating them on the basis of the M.E. model (Borthwick et al., 1998a). They reported that a good accuracy which surpassed human performance could be obtained for a certain data set by integrating several systems (Borthwick et al., 1998b). With regard to named entity extraction from Japanese sentences, similar statistical methods have been proposed, including methods based on an HMM (Shinnou, 1999), a decision tree model (Sekine et al., 1998; Nobata, 1999), and an M.E. model (Borthwick, 1999). Borthwick&apos;s approach is similar to ours except that he used hand-crafted transformation rules while we use automatically acquired rules alone. The accuracy we reported in Section 3.6 is better than that which Borthwick obtained. Our method is more accurate than any other system based on a statistical method that participated in the last IREXNE workshop, and is close to that obtained by the system which obtained the highest accuracy for the IREX-NE task. 4 Conclusion This paper described the extraction of named entities on the basis of an M.E. (maximum ent</context>
</contexts>
<marker>Nobata, 1999</marker>
<rawString>Chikashi Nobata. 1999. Named Entity Tagging System Based on a Decision Tree Model. In Proceedings of the IREX Workshop, pages 201-206. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="2854" citStr="Ratnaparkhi, 1996" startWordPosition="487" endWordPosition="488"> main approaches to extracting NEs, one based on hand-crafted rules and the other based on a machine-learning. The former approach is costly because definitions differ across applications, and the rules have to be changed according to the application. The machine-learning approach requires a training corpus, but a high accuracy can be achieved without requiring a large amount of data if we use a learning model which includes ways of overcoming the data sparseness problem. Therefore, we have taken the latter approach. Many methods based on maximum entropy (M.E.) models have been very accurate (Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998a; Uchimoto et al., 1999), and the M.E. model can be adapted to deal with the data sparseness problem effectively. We have thus used the M.E. model to extract NEs. After identifying NEs in a given text by applying our model, we apply transformation rules which have been acquired by an error-driven learning method to the text. 2 Named Entity Extraction Algorithm We have used the definition of NEs which is used in the IREX-NE task (IREX Executive Committee, 1999). Eight types of NE, &amp;quot;ORGANIZATION&amp;quot;, &amp;quot;PERSON&amp;quot;, &amp;quot;LOCATION&amp;quot;, &amp;quot;ARTIFACT&amp;quot;, &amp;quot;DATE&amp;quot;, &amp;quot;TIME&amp;quot;, &amp;quot;MONE</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Conference on Empirical Methods in Natural Language Processing, pages 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Linear Observed Time Statistical Parser Based on Maximum Entropy Models.</title>
<date>1997</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2873" citStr="Ratnaparkhi, 1997" startWordPosition="489" endWordPosition="490"> extracting NEs, one based on hand-crafted rules and the other based on a machine-learning. The former approach is costly because definitions differ across applications, and the rules have to be changed according to the application. The machine-learning approach requires a training corpus, but a high accuracy can be achieved without requiring a large amount of data if we use a learning model which includes ways of overcoming the data sparseness problem. Therefore, we have taken the latter approach. Many methods based on maximum entropy (M.E.) models have been very accurate (Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998a; Uchimoto et al., 1999), and the M.E. model can be adapted to deal with the data sparseness problem effectively. We have thus used the M.E. model to extract NEs. After identifying NEs in a given text by applying our model, we apply transformation rules which have been acquired by an error-driven learning method to the text. 2 Named Entity Extraction Algorithm We have used the definition of NEs which is used in the IREX-NE task (IREX Executive Committee, 1999). Eight types of NE, &amp;quot;ORGANIZATION&amp;quot;, &amp;quot;PERSON&amp;quot;, &amp;quot;LOCATION&amp;quot;, &amp;quot;ARTIFACT&amp;quot;, &amp;quot;DATE&amp;quot;, &amp;quot;TIME&amp;quot;, &amp;quot;MONEY&amp;quot;, and &amp;quot;PERCENT&amp;quot; a</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A Linear Observed Time Statistical Parser Based on Maximum Entropy Models. In Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SAIC</author>
</authors>
<date>1998</date>
<note>MUC homepage. http://www.muc.saic. com/.</note>
<contexts>
<context position="1131" citStr="SAIC, 1998" startWordPosition="188" endWordPosition="189">es when focusing on the relationship between morphemes and NEs as defined in the NE task of the IREX competition held in 1999. Each NE consists of one or more morphemes, or includes a substring of a morpheme. We extract the former type of NE by using the M.E. model. We then extract the latter type of NE by applying transformation rules to the text. 1 Introduction Named entity (NE) extraction is one basic technique used in information extraction. It can also help to improve the accuracy of morphological and syntactic analysis. The competition held at the MUG (Message Understanding Conference) (SAIC, 1998) since 1980 in the U.S. has helped to improve the technique. In Japan, the &amp;quot;IREX (Information Retrieval and Extraction Exercise)&amp;quot; project began sponsoring a similar competition in 1998. NE extraction is one of two tasks in the competition. The targets for extraction in this task are the names of organizations such as &amp;quot;0 i#-W (the Ministry of Posts and Telecommunications),&amp;quot; people&apos;s names such as &amp;quot;/[141A a (Yasunari Kawabata),&amp;quot; names of locations such as &amp;quot;*F&apos;t (Kobe),&amp;quot; names of artifacts such as &amp;quot;t a — , (Toyota&apos;s Corolla car),&amp;quot; and expressions which represent dates, times, sums of money, and p</context>
</contexts>
<marker>SAIC, 1998</marker>
<rawString>SAIC. 1998. MUC homepage. http://www.muc.saic. com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
<author>Hiroyuki Shinnou</author>
</authors>
<title>A Decision Tree Method for Finding and Classifying Names in Japanese Texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>171--178</pages>
<contexts>
<context position="24983" citStr="Sekine et al., 1998" startWordPosition="4322" endWordPosition="4325">y in the MUG-NE task from among those based on statistical methods and those based on handcrafted rules, and obtained better results than any of the individual systems by integrating them on the basis of the M.E. model (Borthwick et al., 1998a). They reported that a good accuracy which surpassed human performance could be obtained for a certain data set by integrating several systems (Borthwick et al., 1998b). With regard to named entity extraction from Japanese sentences, similar statistical methods have been proposed, including methods based on an HMM (Shinnou, 1999), a decision tree model (Sekine et al., 1998; Nobata, 1999), and an M.E. model (Borthwick, 1999). Borthwick&apos;s approach is similar to ours except that he used hand-crafted transformation rules while we use automatically acquired rules alone. The accuracy we reported in Section 3.6 is better than that which Borthwick obtained. Our method is more accurate than any other system based on a statistical method that participated in the last IREXNE workshop, and is close to that obtained by the system which obtained the highest accuracy for the IREX-NE task. 4 Conclusion This paper described the extraction of named entities on the basis of an M.</context>
</contexts>
<marker>Sekine, Grishman, Shinnou, 1998</marker>
<rawString>Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou. 1998. A Decision Tree Method for Finding and Classifying Names in Japanese Texts. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 171-178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<date>1999</date>
<note>Satoshi Sekine homepage. http://www.cs.nyu.edu/cs/projects/proteus/sekine/.</note>
<marker>Sekine, 1999</marker>
<rawString>Satoshi Sekine. 1999. Satoshi Sekine homepage. http://www.cs.nyu.edu/cs/projects/proteus/sekine/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shinnou</author>
</authors>
<title>Extraction of Proper Nouns through Extended Character Based HIVINI.</title>
<date>1999</date>
<booktitle>In Proceedings of the IREX Workshop,</booktitle>
<pages>151--157</pages>
<note>(in Japanese).</note>
<contexts>
<context position="24939" citStr="Shinnou, 1999" startWordPosition="4316" endWordPosition="4317">l systems which obtained a high accuracy in the MUG-NE task from among those based on statistical methods and those based on handcrafted rules, and obtained better results than any of the individual systems by integrating them on the basis of the M.E. model (Borthwick et al., 1998a). They reported that a good accuracy which surpassed human performance could be obtained for a certain data set by integrating several systems (Borthwick et al., 1998b). With regard to named entity extraction from Japanese sentences, similar statistical methods have been proposed, including methods based on an HMM (Shinnou, 1999), a decision tree model (Sekine et al., 1998; Nobata, 1999), and an M.E. model (Borthwick, 1999). Borthwick&apos;s approach is similar to ours except that he used hand-crafted transformation rules while we use automatically acquired rules alone. The accuracy we reported in Section 3.6 is better than that which Borthwick obtained. Our method is more accurate than any other system based on a statistical method that participated in the last IREXNE workshop, and is close to that obtained by the system which obtained the highest accuracy for the IREX-NE task. 4 Conclusion This paper described the extrac</context>
</contexts>
<marker>Shinnou, 1999</marker>
<rawString>Hiroyuki Shinnou. 1999. Extraction of Proper Nouns through Extended Character Based HIVINI. In Proceedings of the IREX Workshop, pages 151-157. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Japanese Dependency Structure Analysis Based on Maximum Entropy Models.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;99),</booktitle>
<pages>196--203</pages>
<contexts>
<context position="2922" citStr="Uchimoto et al., 1999" startWordPosition="495" endWordPosition="498">ules and the other based on a machine-learning. The former approach is costly because definitions differ across applications, and the rules have to be changed according to the application. The machine-learning approach requires a training corpus, but a high accuracy can be achieved without requiring a large amount of data if we use a learning model which includes ways of overcoming the data sparseness problem. Therefore, we have taken the latter approach. Many methods based on maximum entropy (M.E.) models have been very accurate (Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998a; Uchimoto et al., 1999), and the M.E. model can be adapted to deal with the data sparseness problem effectively. We have thus used the M.E. model to extract NEs. After identifying NEs in a given text by applying our model, we apply transformation rules which have been acquired by an error-driven learning method to the text. 2 Named Entity Extraction Algorithm We have used the definition of NEs which is used in the IREX-NE task (IREX Executive Committee, 1999). Eight types of NE, &amp;quot;ORGANIZATION&amp;quot;, &amp;quot;PERSON&amp;quot;, &amp;quot;LOCATION&amp;quot;, &amp;quot;ARTIFACT&amp;quot;, &amp;quot;DATE&amp;quot;, &amp;quot;TIME&amp;quot;, &amp;quot;MONEY&amp;quot;, and &amp;quot;PERCENT&amp;quot; are defined. This section describes the method of </context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese Dependency Structure Analysis Based on Maximum Entropy Models. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;99), pages 196-203.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>