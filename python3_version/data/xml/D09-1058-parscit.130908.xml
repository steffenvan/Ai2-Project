<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.9992265">
An Empirical Study of Semi-supervised Structured Conditional Models
for Dependency Parsing
</title>
<note confidence="0.780170333333333">
Jun Suzuki, Hideki Isozaki Xavier Carreras, and Michael Collins
NTT CS Lab., NTT Corp. MIT CSAIL
Kyoto, 619-0237, Japan Cambridge, MA 02139, USA
</note>
<email confidence="0.987735">
jun@cslab.kecl.ntt.co.jp carreras@csail.mit.edu
isozaki@cslab.kecl.ntt.co.jp mcollins@csail.mit.edu
</email>
<sectionHeader confidence="0.994698" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918230769231">
This paper describes an empirical study
of high-performance dependency parsers
based on a semi-supervised learning ap-
proach. We describe an extension of semi-
supervised structured conditional models
(SS-SCMs) to the dependency parsing
problem, whose framework is originally
proposed in (Suzuki and Isozaki, 2008).
Moreover, we introduce two extensions re-
lated to dependency parsing: The first ex-
tension is to combine SS-SCMs with an-
other semi-supervised approach, described
in (Koo et al., 2008). The second exten-
sion is to apply the approach to second-
order parsing models, such as those de-
scribed in (Carreras, 2007), using a two-
stage semi-supervised learning approach.
We demonstrate the effectiveness of our
proposed methods on dependency parsing
experiments using two widely used test
collections: the Penn Treebank for En-
glish, and the Prague Dependency Tree-
bank for Czech. Our best results on
test data in the above datasets achieve
93.79% parent-prediction accuracy for En-
glish, and 88.05% for Czech.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999479538461538">
Recent work has successfully developed depen-
dency parsing models for many languages us-
ing supervised learning algorithms (Buchholz and
Marsi, 2006; Nivre et al., 2007). Semi-supervised
learning methods, which make use of unlabeled
data in addition to labeled examples, have the po-
tential to give improved performance over purely
supervised methods for dependency parsing. It
is often straightforward to obtain large amounts
of unlabeled data, making semi-supervised ap-
proaches appealing; previous work on semi-
supervised methods for dependency parsing in-
cludes (Smith and Eisner, 2007; Koo et al., 2008;
Wang et al., 2008).
In particular, Koo et al. (2008) describe a
semi-supervised approach that makes use of clus-
ter features induced from unlabeled data, and gives
state-of-the-art results on the widely used depen-
dency parsing test collections: the Penn Tree-
bank (PTB) for English and the Prague Depen-
dency Treebank (PDT) for Czech. This is a very
simple approach, but provided significant perfor-
mance improvements comparing with the state-
of-the-art supervised dependency parsers such as
(McDonald and Pereira, 2006).
This paper introduces an alternative method for
semi-supervised learning for dependency parsing.
Our approach basically follows a framework pro-
posed in (Suzuki and Isozaki, 2008). We extend it
for dependency parsing, which we will refer to as
a Semi-supervised Structured Conditional Model
(SS-SCM). In this framework, a structured condi-
tional model is constructed by incorporating a se-
ries of generative models, whose parameters are
estimated from unlabeled data. This paper de-
scribes a basic method for learning within this ap-
proach, and in addition describes two extensions.
The first extension is to combine our method with
the cluster-based semi-supervised method of (Koo
et al., 2008). The second extension is to apply the
approach to second-order parsing models, more
specifically the model of (Carreras, 2007), using
a two-stage semi-supervised learning approach.
We conduct experiments on dependency parsing
of English (on Penn Treebank data) and Czech (on
the Prague Dependency Treebank). Our experi-
ments investigate the effectiveness of: 1) the basic
SS-SCM for dependency parsing; 2) a combina-
tion of the SS-SCM with Koo et al. (2008)’s semi-
supervised approach (even in the case we used the
same unlabeled data for both methods); 3) the two-
stage semi-supervised learning approach that in-
</bodyText>
<page confidence="0.970842">
551
</page>
<note confidence="0.9956865">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551–560,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<equation confidence="0.891743">
E
+
(h,m,l)Ey
k
E
j=1
vjqj(x, h, m, l). (2)
</equation>
<bodyText confidence="0.999935260869565">
In this model v1, ... , vk are scalar parameters that
may be positive or negative; q1 ... qk are func-
tions (in fact, generative models), that are trained
on unlabeled data. The vj parameters will dictate
the relative strengths of the functions q1 ... qk, and
will be trained on labeled data.
For convenience, we will use v to refer to the
vector of parameters v1 ... vk, and q to refer to the
set of generative models q1 ... qk. The full model
is specified by values for w, v, and q. We will
write p(y|x; w, v, q) to refer to the conditional
distribution under parameter values w, v, q.
We will describe a three-step parameter estima-
tion method that: 1) initializes the q functions
(generative models) to be uniform distributions,
and estimates parameter values w and v from la-
beled data; 2) induces new functions q1 ... qk from
unlabeled data, based on the distribution defined
by the w, v, q values from step (1); 3) re-estimates
w and v on the labeled examples, keeping the
q1 . . . qk from step (2) fixed. The end result is a
model that combines supervised training with gen-
erative models induced from unlabeled data.
</bodyText>
<subsectionHeader confidence="0.999471">
2.2 The Generative Models
</subsectionHeader>
<bodyText confidence="0.998665782608696">
We now describe how the generative models
q1 . . . qk are defined, and how they are induced
from unlabeled data. These models make direct
use of the feature-vector definition f(x, y) used in
the original, fully supervised, dependency parser.
The first step is to partition the d fea-
tures in f(x, y) into k separate feature vectors,
r1(x, y) ... rk(x, y) (with the result that f is the
concatenation of the k feature vectors r1 ... rk). In
our experiments on dependency parsing, we parti-
tioned f into up to over 140 separate feature vec-
tors corresponding to different feature types. For
example, one feature vector rj might include only
those features corresponding to word bigrams in-
volved in dependencies (i.e., indicator functions
tied to the word bigram (xm, xh) involved in a de-
pendency (x, h, m, l)).
We then define a generative model that assigns
a probability
corporates a second-order parsing model. In ad-
dition, we evaluate the SS-SCM for English de-
pendency parsing with large amounts (up to 3.72
billion tokens) of unlabeled data.
</bodyText>
<figure confidence="0.771967333333333">
2 Semi-supervised Structured
Conditional Models for Dependency
Parsing
</figure>
<figureCaption confidence="0.917310875">
Suzuki et al. (2008) describe a semi-supervised
learning method for conditional random fields
(CRFs) (Lafferty et al., 2001). In this paper we
extend this method to the dependency parsing
problem. We will refer to this extended method
as Semi-supervised Structured Conditional Mod-
els (SS-SCMs). The remainder of this section de-
scribes our approach.
</figureCaption>
<subsectionHeader confidence="0.929767">
2.1 The Basic Model
</subsectionHeader>
<bodyText confidence="0.99917775">
Throughout this paper we will use x to denote an
input sentence, and y to denote a labeled depen-
dency structure. Given a sentence x with n words,
a labeled dependency structure y is a set of n de-
pendencies of the form (h, m, l), where h is the
index of the head-word in the dependency, m is
the index of the modifier word, and l is the label
of the dependency. We use h = 0 for the root of
the sentence. We assume access to a set of labeled
training examples, {xz, yz}Z_&apos;1, and in addition a
set of unlabeled examples, {xz}M1.
In conditional log-linear models for dependency
parsing (which are closely related to conditional
random fields (Lafferty et al., 2001)), a distribu-
tion over dependency structures for a sentence x
is defined as follows:
</bodyText>
<equation confidence="0.847902833333333">
1
p(y|x) = Z(x) exp{g(x,y)}, (1)
where Z(x) is the partition function, w is a pa-
rameter vector, and
g(x, y) = E w · f(x, h, m, l)
(h,m,l)Ey
</equation>
<bodyText confidence="0.996775714285714">
Here f(x, h, m, l) is a feature vector represent-
ing the dependency (h, m, l) in the context of the
sentence x (see for example (McDonald et al.,
2005a)).
In this paper we extend the definition of g(x, y)
to include features that are induced from unlabeled
data. Specifically, we define
</bodyText>
<equation confidence="0.995052555555555">
g(x, y) = E w · f(x, h, m, l)
(h,m,l)Ey
dj
H
a=1
θrj,a(x,h,m,l)
j,a 3)
(
q�(x, h, m, l) =
</equation>
<bodyText confidence="0.99915">
to the dj-dimensional feature vector rj(x, h, m, l).
The parameters of this model are θj,1 ... θj,dj;
</bodyText>
<page confidence="0.995802">
552
</page>
<bodyText confidence="0.9967275">
they form a multinomial distribution, with the con-
straints that θj,a &gt; 0, and Pa θj,a = 1. This
model can be viewed as a very simple (naive-
Bayes) model that defines a distribution over fea-
ture vectors rj E Rdj. The next section describes
how the parameters θj,a are trained on unlabeled
data.
Given parameters θj,a, we can simply define the
functions q1 ... qk to be log probabilities under the
generative model:
</bodyText>
<equation confidence="0.979396">
qj(x, h, m, l) = log q0j(x, h, m, l)
= Xdj rj,a(x, h, m, l) log θj,a.
a=1
</equation>
<bodyText confidence="0.999752125">
We modify this definition slightly, be introducing
scaling factors cj,a &gt; 0, and defining
In our experiments, cj,a is simply a count of the
number of times the feature indexed by (j, a) ap-
pears in unlabeled data. Thus more frequent fea-
tures have their contribution down-weighted in the
model. We have found this modification to be ben-
eficial.
</bodyText>
<subsectionHeader confidence="0.9996505">
2.3 Estimating the Parameters of the
Generative Models
</subsectionHeader>
<bodyText confidence="0.999017875">
We now describe the method for estimating the
parameters θj,a of the generative models. We
assume initial parameters w, v, q, which define
a distribution p(y|x0i; w, v, q) over dependency
structures for each unlabeled example x0i. We will
re-estimate the generative models q, based on un-
labeled examples. The likelihood function on un-
labeled data is defined as
</bodyText>
<equation confidence="0.989509">
p(y|x0 i; w, v, q) X log q0j(x0i, h, m, l),
(h,m,l)∈Y
(5)
</equation>
<bodyText confidence="0.957836777777778">
where q0 j is as defined in Eq. 3. This function re-
sembles the Q function used in the EM algorithm,
where the hidden labels (in our case, dependency
structures), are filled in using the conditional dis-
tribution p(y|x0i; w, v, q).
It is simple to show that the estimates θj,a that
maximize the function in Eq. 5 can be defined as
follows. First, define a vector of expected counts
based on w, v, q as
</bodyText>
<equation confidence="0.984003">
p(y|x0 i; w, v, q) X rj(x0i, h, m, l).
(h,m,l)∈Y
</equation>
<bodyText confidence="0.999402777777778">
Note that it is straightforward to calculate these ex-
pected counts using a variant of the inside-outside
algorithm (Baker, 1979) applied to the (Eisner,
1996) dependency-parsing data structures (Paskin,
2001) for projective dependency structures, or the
matrix-tree theorem (Koo et al., 2007; Smith and
Smith, 2007; McDonald and Satta, 2007) for non-
projective dependency structures.
The estimates that maximize Eq. 5 are then
</bodyText>
<equation confidence="0.946999">
rj,a
dj
Pa=1 rj,a
</equation>
<bodyText confidence="0.999954">
In a slight modification, we employ the follow-
ing estimates in our model, where η &gt; 1 is a pa-
rameter of the model:
</bodyText>
<equation confidence="0.999636">
θ _ (η − 1) + Tj,a (6)
7&apos;a — d-
dj X (η − 1) + PaL 1 rj,a
</equation>
<bodyText confidence="0.999069">
This corresponds to a MAP estimate under a
Dirichlet prior over the θj,a parameters.
</bodyText>
<subsectionHeader confidence="0.992777">
2.4 The Complete Parameter-Estimation
Method
</subsectionHeader>
<bodyText confidence="0.996811111111111">
This section describes the full parameter estima-
tion method. The input to the algorithm is a set
of labeled examples {xi, yi}Ni=1, a set of unla-
beled examples {x0i}Mi=1, a feature-vector defini-
tion f(x, y), and a partition of f into k feature vec-
tors r1 ... rk which underly the generative mod-
els. The output from the algorithm is a parameter
vector w, a set of generative models q1 ... qk, and
parameters v1 ... vk, which define a probabilistic
dependency parsing model through Eqs. 1 and 2.
The learning algorithm proceeds in three steps:
Step 1: Estimation of a Fully Supervised
Model. We choose the initial value q0 of the
generative models to be the uniform distribution,
i.e., we set θj,a = 1/dj for all j, a. We then de-
fine the regularized log-likelihood function for the
labeled examples, with the generative model fixed
at q0, to be:
</bodyText>
<equation confidence="0.999603823529412">
L(w, v; q0) = Xn log p(yi|xi; w, v, q0)
i=1
C 3||w||2 + ||v||2´
2
qj(x, h, m, l) = Xdj θj,a (4)
a=1 rj,a(x, h, m, l) log
cj,a
XM
i=1
X
Y
X
Y
rj =
XM
i=1
θj,a =
</equation>
<page confidence="0.977336">
553
</page>
<bodyText confidence="0.9916216">
This is a conventional regularized log-likelihood
function, as commonly used in CRF models. The
parameter C &gt; 0 dictates the level of regular-
ization in the model. We define the initial pa-
rameters (w0, v0) = arg max,,v L(w, v; q0).
These parameters can be found using conventional
methods for estimating the parameters of regu-
larized log-likelihood functions (in our case we
use LBFGS (Liu and Nocedal, 1989)). Note that
the gradient of the log-likelihood function can be
calculated using the inside-outside algorithm ap-
plied to projective dependency parse structures, or
the matrix-tree theorem applied to non-projective
structures.
Step 2: Estimation of the Generative Mod-
els. In this step, expected count vectors r1 ... rk
are first calculated, based on the distribution
p(y|x; w0, v0, q0). Generative model parameters
Oj,a are calculated through the definition in Eq. 6;
these estimates define updated generative models
q1j for j = 1... k through Eq. 4. We refer to the
new values for the generative models as q1.
Step 3: Re-estimation of w and v. In
the final step, w1 and v1 are estimated as
arg max,,v L(w, v; q1) where L(w, v; q1) is de-
fined in an analogous way to L(w, v; q0). Thus w
and v are re-estimated to optimize log-likelihood
of the labeled examples, with the generative mod-
els q1 estimated in step 2.
The final output from the algorithm is the set of
parameters (w1, v1, q1). Note that it is possible to
iterate the method—steps 2 and 3 can be repeated
multiple times (Suzuki and Isozaki, 2008)—but
in our experiments we only performed these steps
once.
</bodyText>
<sectionHeader confidence="0.999356" genericHeader="introduction">
3 Extensions
</sectionHeader>
<subsectionHeader confidence="0.999976">
3.1 Incorporating Cluster-Based Features
</subsectionHeader>
<bodyText confidence="0.999898958333333">
Koo et al. (2008) describe a semi-supervised
approach that incorporates cluster-based features,
and that gives competitive results on dependency
parsing benchmarks. The method is a two-stage
approach. First, hierarchical word clusters are de-
rived from unlabeled data using the Brown et al.
clustering algorithm (Brown et al., 1992). Sec-
ond, a new feature set is constructed by represent-
ing words by bit-strings of various lengths, corre-
sponding to clusters at different levels of the hier-
archy. These features are combined with conven-
tional features based on words and part-of-speech
tags. The new feature set is then used within a
conventional discriminative, supervised approach,
such as the averaged perceptron algorithm.
The important point is that their approach uses
unlabeled data only for the construction of a new
feature set, and never affects to learning algo-
rithms. It is straightforward to incorporate cluster-
based features within the SS-SCM approach de-
scribed in this paper. We simply use the cluster-
based feature-vector representation f(x, y) intro-
duced by (Koo et al., 2008) as the basis of our ap-
proach.
</bodyText>
<subsectionHeader confidence="0.999557">
3.2 Second-order Parsing Models
</subsectionHeader>
<bodyText confidence="0.9998982">
Previous work (McDonald and Pereira, 2006; Car-
reras, 2007) has shown that second-order parsing
models, which include information from “sibling”
or “grandparent” relationships between dependen-
cies, can give significant improvements in accu-
racy over first-order parsing models. In principle
it would be straightforward to extend the SS-SCM
approach that we have described to second-order
parsing models. In practice, however, a bottle-
neck for the method would be the estimation of
the generative models on unlabeled data. This
step requires calculation of marginals on unlabeled
data. Second-order parsing models generally re-
quire more costly inference methods for the cal-
culation of marginals, and this increased cost may
be prohibitive when large quantities of unlabeled
data are employed.
We instead make use of a simple ‘two-stage’ ap-
proach for extending the SS-SCM approach to the
second-order parsing model of (Carreras, 2007).
In the first stage, we use a first-order parsing
model to estimate generative models q1 ... qk from
unlabeled data. In the second stage, we incorpo-
rate these generative models as features within a
second-order parsing model. More precisely, in
our approach, we first train a first-order parsing
model by Step 1 and 2, exactly as described in
Section 2.4, to estimate w0, v0 and q1. Then,
we substitute Step 3 as a supervised learning such
as MIRA with a second-order parsing model (Mc-
Donald et al., 2005a), which incorporates q1 as a
real-values features. We refer this two-stage ap-
proach to as two-stage SS-SCM.
In our experiments we use the 1-best MIRA
algorithm (McDonald and Pereira, 2006)1 as a
</bodyText>
<footnote confidence="0.952187333333333">
1We used a slightly modified version of 1-best MIRA,
whose difference can be found in the third line in Eq. 7,
namely, including L(yi, y).
</footnote>
<page confidence="0.976241">
554
</page>
<table confidence="0.997471333333333">
(a) English dependency parsing
Data set (WSJ Sec. IDs) # of sentences # of tokens
Training (02–21) 39,832 950,028
Development (22) 1,700 40,117
Test (23) 2,012 47,377
Unlabeled 1,796,379 43,380,315
(b) Czech dependency parsing
Data set # of sentences # of tokens
Training 73,088 1,255,590
Development 7,507 126,030
Test 7,319 125,713
Unlabeled 2,349,224 39,336,570
Corpus article name (mm/yy) # of sent. # of tokens
BLLIP wsj 00/87–00/89 1,796,379 43,380,315
Tipster wsj 04/90–03/92 1,550,026 36,583,547
North wsj 07/94–12/96 2,748,803 62,937,557
American reu 04/94–07/96 4,773,701 110,001,109
Reuters reu 09/96–08/97 12,969,056 214,708,766
English afp 05/94–12/06 21,231,470 513,139,928
Gigaword apw 11/94–12/06 46,978,725 960,733,303
ltw 04/94–12/06 10,524,545 230,370,454
nyt 07/94–12/06 60,752,363 1,266,531,274
xin 01/95–12/06 12,624,835 283,579,330
total 175,949,903 3,721,965,583
</table>
<tableCaption confidence="0.999155">
Table 1: Details of training, development, test data
</tableCaption>
<bodyText confidence="0.834546666666667">
(labeled data sets) and unlabeled data used in our
experiments
parameter-estimation method for the second-order
parsing model. In particular, we perform the fol-
lowing optimizations on each update t = 1, ..., T
for re-estimating w and v:
</bodyText>
<equation confidence="0.999547">
min ||w(t+1) − w(t) ||+ ||v(t+1) − v(t)||
s.t. S(xi, yi) − S(xi, Y) ? L(yi, Y) (7)
y� = arg maxy S(xi, y) + L(yi, y),
</equation>
<bodyText confidence="0.999582">
where L(yi, y) represents the loss between correct
output of i’th sample yi and y. Then, the scoring
function S for each y can be defined as follows:
</bodyText>
<equation confidence="0.903871666666667">
S(x, y) = w - (f1(x, y) + f2(x, y))
(8)
vjqj(x,y),
</equation>
<bodyText confidence="0.999875333333333">
where B represents a tunable scaling factor, and
f1 and f2 represent the feature vectors of first and
second-order parsing parts, respectively.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999965833333333">
We now describe experiments investigating the ef-
fectiveness of the SS-SCM approach for depen-
dency parsing. The experiments test basic, first-
order parsing models, as well as the extensions
to cluster-based features and second-order parsing
models described in the previous section.
</bodyText>
<subsectionHeader confidence="0.995305">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.99503225">
We conducted experiments on both English and
Czech data. We used the Wall Street Journal
sections of the Penn Treebank (PTB) III (Mar-
cus et al., 1994) as a source of labeled data for
English, and the Prague Dependency Treebank
(PDT) 1.0 (Hajiˇc, 1998) for Czech. To facili-
tate comparisons with previous work, we used ex-
actly the same training, development and test sets
</bodyText>
<tableCaption confidence="0.604293">
Table 2: Details of the larger unlabeled data set
used in English dependency parsing: sentences ex-
ceeding 128 tokens in length were excluded for
computational reasons.
</tableCaption>
<bodyText confidence="0.999428818181818">
as those described in (McDonald et al., 2005a;
McDonald et al., 2005b; McDonald and Pereira,
2006; Koo et al., 2008). The English dependency-
parsing data sets were constructed using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to dependency tree repre-
sentations. We split the data into three parts:
sections 02-21 for training, section 22 for de-
velopment and section 23 for test. The Czech
data sets were obtained from the predefined train-
ing/development/test partition in the PDT. The un-
labeled data for English was derived from the
Brown Laboratory for Linguistic Information Pro-
cessing (BLLIP) Corpus (LDC2000T43)2, giving
a total of 1,796,379 sentences and 43,380,315
tokens. The raw text section of the PDT was
used for Czech, giving 2,349,224 sentences and
39,336,570 tokens. These data sets are identical
to the unlabeled data used in (Koo et al., 2008),
and are disjoint from the training, development
and test sets. The datasets used in our experiments
are summarized in Table 1.
In addition, we will describe experiments that
make use of much larger amounts of unlabeled
data. Unfortunately, we have no data available
other than PDT for Czech, this is done only for
English dependency parsing. Table 2 shows the
detail of the larger unlabeled data set used in our
experiments, where we eliminated sentences that
have more than 128 tokens for computational rea-
sons. Note that the total size of the unlabeled data
reaches 3.72G (billion) tokens, which is approxi-
</bodyText>
<footnote confidence="0.678940333333333">
2We ensured that the sentences used in the PTB were
excluded from the unlabeled data, since sentences used in
BLLIP corpus are a super-set of the PTB.
</footnote>
<equation confidence="0.59149525">
k
E
j=1
+B
</equation>
<page confidence="0.990021">
555
</page>
<bodyText confidence="0.994134">
mately 4,000 times larger than the size of labeled
training data.
</bodyText>
<subsectionHeader confidence="0.664501">
4.2 Features
4.2.1 Baseline Features
</subsectionHeader>
<bodyText confidence="0.9999582">
In general we will assume that the input sentences
include both words and part-of-speech (POS) tags.
Our baseline features (“baseline”) are very simi-
lar to those described in (McDonald et al., 2005a;
Koo et al., 2008): these features track word and
POS bigrams, contextual features surrounding de-
pendencies, distance features, and so on. En-
glish POS tags were assigned by MXPOST (Rat-
naparkhi, 1996), which was trained on the train-
ing data described in Section 4.1. Czech POS tags
were obtained by the following two steps: First,
we used ‘feature-based tagger’ included with the
PDT3, and then, we used the method described in
(Collins et al., 1999) to convert the assigned rich
POS tags into simplified POS tags.
</bodyText>
<subsubsectionHeader confidence="0.814087">
4.2.2 Cluster-based Features
</subsubsectionHeader>
<bodyText confidence="0.999972454545455">
In a second set of experiments, we make use of the
feature set used in the semi-supervised approach
of (Koo et al., 2008). We will refer to this as the
“cluster-based feature set” (CL). The BLLIP (43M
tokens) and PDT (39M tokens) unlabeled data sets
shown in Table 1 were used to construct the hierar-
chical clusterings used within the approach. Note
that when this feature set is used within the SS-
SCM approach, the same set of unlabeled data is
used to both induce the clusters, and to estimate
the generative models within the SS-SCM model.
</bodyText>
<subsectionHeader confidence="0.973635">
4.2.3 Constructing the Generative Models
</subsectionHeader>
<bodyText confidence="0.999893">
As described in section 2.2, the generative mod-
els in the SS-SCM approach are defined through
a partition of the original feature vector f(x, y)
into k feature vectors r1(x, y) ... rk(x, y). We
follow a similar approach to that of (Suzuki and
Isozaki, 2008) in partitioning f(x, y), where the
k different feature vectors correspond to different
feature types or feature templates. Note that, in
general, we are not necessary to do as above, this
is one systematic way of a feature design for this
approach.
</bodyText>
<subsectionHeader confidence="0.998644">
4.3 Other Experimental Settings
</subsectionHeader>
<bodyText confidence="0.996286525">
All results presented in our experiments are given
in terms of parent-prediction accuracy on unla-
3Training, development, and test data in PDT already con-
tains POS tags assigned by the ‘feature-based tagger’.
beled dependency parsing. We ignore the parent-
predictions of punctuation tokens for English,
while we retain all the punctuation tokens for
Czech. These settings match the evaluation setting
in previous work such as (McDonald et al., 2005a;
Koo et al., 2008).
We used the method proposed by (Carreras,
2007) for our second-order parsing model. Since
this method only considers projective dependency
structures, we “projectivized” the PDT training
data in the same way as (Koo et al., 2008). We
used a non-projective model, trained using an ap-
plication of the matrix-tree theorem (Koo et al.,
2007; Smith and Smith, 2007; McDonald and
Satta, 2007) for the first-order Czech models, and
projective parsers for all other models.
As shown in Section 2, SS-SCMs with 1st-order
parsing models have two tunable parameters, C
and q, corresponding to the regularization con-
stant, and the Dirichlet prior for the generative
models. We selected a fixed value q = 2, which
was found to work well in preliminary experi-
ments.4 The value of C was chosen to optimize
performance on development data. Note that C
for supervised SCMs were also tuned on develop-
ment data. For the two-stage SS-SCM for incor-
porating second-order parsing model, we have ad-
ditional one tunable parameter B shown in Eq. 8.
This was also chosen by the value that provided
the best performance on development data.
In addition to providing results for models
trained on the full training sets, we also performed
experiments with smaller labeled training sets.
These training sets were either created through
random sampling or by using a predefined subset
of document IDs from the labeled training data.
</bodyText>
<sectionHeader confidence="0.998604" genericHeader="method">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9998895">
Table 3 gives results for the SS-SCM method un-
der various configurations: for first and second-
order parsing models, with and without the clus-
ter features of (Koo et al., 2008), and for varying
amounts of labeled data. The remainder of this
section discusses these results in more detail.
</bodyText>
<subsectionHeader confidence="0.998763">
5.1 Effects of the Quantity of Labeled Data
</subsectionHeader>
<bodyText confidence="0.99063">
We can see from the results in Table 3 that our
semi-supervised approach consistently gives gains
</bodyText>
<footnote confidence="0.99938">
4An intuitive meaning of rl = 2 is that this adds one
pseudo expected count to every feature when estimating new
parameter values.
</footnote>
<page confidence="0.991371">
556
</page>
<table confidence="0.98360465">
(a) English dependency parsing: w/ 43M token unlabeled data (BLLIP)
WSJ sec. IDs wsj 21 random selection random selection wsj 15–18 wsj 02-21(all)
# of sentences / tokens 1,671 / 40,039 2,000 / 48,577 8,000 / 190,958 8,936 / 211,727 39,832 / 950,028
feature type baseline CL baseline CL baseline CL baseline CL baseline CL
Supervised SCM (1od) 85.63 86.80 87.02 88.05 89.23 90.45 89.43 90.85 91.21 92.53
SS-SCM (1od) 87.16 88.40 88.07 89.55 90.06 91.45 90.23 91.63 91.72 93.01
(gain over Sup. SCM) (+1.53) (+1.60) (+1.05) (+1.50) (+0.83) (+1.00) (+0.80) (+0.78) (+0.51) (+0.48)
Supervised MIRA (2od) 87.99 89.05 89.20 90.06 91.20 91.75 91.50 92.14 93.02 93.54
2-stage SS-SCM(+MIRA) (2od) 88.88 89.94 90.03 90.90 91.73 92.51 91.95 92.73 93.45 94.13
(gain over Sup. MIRA) (+0.89) (+0.89) (+0.83) (+0.84) (+0.53) (+0.76) (+0.45) (+0.59) (+0.43) (+0.59)
(b) Czech dependency parsing: w/ 39M token unlabeled data (PDT)
PDT Doc. IDs random selection c[0-9]* random selection l[a-i]* (all)
# of sentences / tokens 2,000 / 34,722 3,526 / 53,982 8,000 / 140,423 14,891 / 261,545 73,008 /1,225,590
feature type baseline CL baseline CL baseline CL baseline CL baseline CL
Supervised SCM (1od) 75.67 77.82 76.88 79.24 80.61 82.85 81.94 84.47 84.43 86.72
SS-SCM (1od) 76.47 78.96 77.61 80.28 81.30 83.49 82.74 84.91 85.00 87.03
(gain over Sup. SCM) (+0.80) (+1.14) (+0.73) (+1.04) (+0.69) (+0.64) (+0.80) (+0.44) (+0.57) (+0.31)
Supervised MIRA (2od) 78.19 79.60 79.58 80.77 83.15 84.39 84.27 85.75 86.82 87.76
2-stage SS-SCM(+MIRA) (2od) 78.71 80.09 80.37 81.40 83.61 84.87 84.95 86.00 87.03 88.03
(gain over Sup. MIRA) (+0.52) (+0.49) (+0.79) (+0.63) (+0.46) (+0.48) (+0.68) (+0.25) (+0.21) (+0.27)
</table>
<tableCaption confidence="0.997523">
Table 3: Dependency parsing results for the SS-SCM method with different amounts of labeled training
</tableCaption>
<bodyText confidence="0.9827756">
data. Supervised SCM (1od) and Supervised MIRA (2od) are the baseline first and second-order ap-
proaches; SS-SCM (1od) and 2-stage SS-SCM(+MIRA) (2od) are the first and second-order approaches
described in this paper. “Baseline” refers to models without cluster-based features, “CL” refers to models
which make use of cluster-based features.
in performance under various sizes of labeled data.
Note that the baseline methods that we have used
in these experiments are strong baselines. It is
clear that the gains from our method are larger for
smaller labeled data sizes, a tendency that was also
observed in (Koo et al., 2008).
</bodyText>
<subsectionHeader confidence="0.9984275">
5.2 Impact of Combining SS-SCM with
Cluster Features
</subsectionHeader>
<bodyText confidence="0.99996675">
One important observation from the results in Ta-
ble 3 is that SS-SCMs can successfully improve
the performance over a baseline method that uses
the cluster-based feature set (CL). This is in spite
of the fact that the generative models within the
SS-SCM approach were trained on the same un-
labeled data used to induce the cluster-based fea-
tures.
</bodyText>
<subsectionHeader confidence="0.998583">
5.3 Impact of the Two-stage Approach
</subsectionHeader>
<bodyText confidence="0.99941925">
Table 3 also shows the effectiveness of the two-
stage approach (described in Section 3.2) that inte-
grates the SS-SCM method within a second-order
parser. This suggests that the SS-SCM method
can be effective in providing features (generative
models) used within a separate learning algorithm,
providing that this algorithm can make use of real-
valued features.
</bodyText>
<figure confidence="0.958286333333333">
10 100 1,000 (Mega tokens)
10000
Unlabeled data size: [Log-scale]
</figure>
<figureCaption confidence="0.815524666666667">
Figure 1: Impact of unlabeled data size for the SS-
SCM on development data of English dependency
parsing.
</figureCaption>
<subsectionHeader confidence="0.817581">
5.4 Impact of the Amount of Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.998965454545455">
Figure 1 shows the dependency parsing accuracy
on English as a function of the amount of unla-
beled data used within the SS-SCM approach. (As
described in Section 4.1, we have no unlabeled
data other than PDT for Czech, hence this section
only considers English dependency parsing.) We
can see that performance does improve as more
unlabeled data is added; this trend is seen both
with and without cluster-based features. In addi-
tion, Table 4 shows the performance of our pro-
posed method using 3.72 billion tokens of unla-
</bodyText>
<figure confidence="0.902157555555556">
93.5
93.0
92.5
92.0
91.5
(BLLIP)
43.4M 143M 468M 1.3813 3.7213
CL
baseline
557
CL
baseline
feature set
CL
baseline
93.23
92.23
92.70
(+1.02) (+0.70)
SS-SCM (1st-order)
91.89
(a) English dependency parsing: w/ 3.72G token ULD
feature type
SS-SCM (1st-order)
(gain over Sup. SCM)
94.26
93.68
(+0.66) (+0.72)
(+0.92) (+0.58)
93.79
93.41
2-stage SS-SCM(+MIRA) (2nd-order)
(gain over Sup. MIRA)
(gain over Sup. SCM)
2-stage SS-SCM(+MIRA) (2nd-order)
(gain over Sup. MIRA) (+0.65) (+0.48)
</figure>
<tableCaption confidence="0.734195666666667">
Table 4: Parent-prediction accuracies on develop-
ment data with 3.72G tokens unlabeled data for
English dependency parsing.
</tableCaption>
<table confidence="0.550614333333333">
(b) Czech dependency parsing: w/ 39M token ULD (PDT)
feature set baseline CL
SS-SCM (1st-order) 84.98 87.14
(gain over Sup. SCM) (+0.58) (+0.39)
2-stage SS-SCM(+MIRA) (2nd-order) 86.90 88.05
(gain over Sup. MIRA) (+0.15) (+0.36)
</table>
<bodyText confidence="0.992968">
beled data. Note, however, that the gain in perfor-
mance as unlabeled data is added is not as sharp
as might be hoped, with a relatively modest dif-
ference in performance for 43.4 million tokens vs.
3.72 billion tokens of unlabeled data.
</bodyText>
<subsectionHeader confidence="0.974307">
5.5 Computational Efficiency
</subsectionHeader>
<bodyText confidence="0.999937857142857">
The main computational challenge in our ap-
proach is the estimation of the generative mod-
els q = (qi ... qk) from unlabeled data, partic-
ularly when the amount of unlabeled data used
is large. In our implementation, on the 43M to-
ken BLLIP corpus, using baseline features, it takes
about 5 hours to compute the expected counts re-
quired to estimate the parameters of the generative
models on a single 2.93GHz Xeon processor. It
takes roughly 18 days of computation to estimate
the generative models from the larger (3.72 billion
word) corpus. Fortunately it is simple to paral-
lelize this step; our method takes a few hours on
the larger data set when parallelized across around
300 separate processes.
Note that once the generative models have been
estimated, decoding with the model, or train-
ing the model on labeled data, is relatively in-
expensive, essentially taking the same amount of
computation as standard dependency-parsing ap-
proaches.
</bodyText>
<subsectionHeader confidence="0.699145">
5.6 Results on Test Data
</subsectionHeader>
<bodyText confidence="0.97744075">
Finally, Table 5 displays the final results on test
data. There results are obtained using the best
setting in terms of the development data perfor-
mance. Note that the English dependency pars-
ing results shown in the table were achieved us-
ing 3.72 billion tokens of unlabeled data. The im-
provements on test data are similar to those ob-
served on the development data. To determine
statistical significance, we tested the difference of
parent-prediction error-rates at the sentence level
using a paired Wilcoxon signed rank test. All eight
comparisons shown in Table 5 are significant with
</bodyText>
<tableCaption confidence="0.937195666666667">
Table 5: Parent-prediction accuracies on test data
using the best setting in terms of development data
performances in each condition.
</tableCaption>
<table confidence="0.9928565">
(a) English dependency parsers on PTB
dependency parser test description
(McDonald et al., 2005a) 90.9 1od
(McDonald and Pereira, 2006) 91.5 2od
(Koo et al., 2008) 92.23 1od, 43M ULD
SS-SCM (w/ CL) 92.70 1od, 3.72G ULD
(Koo et al., 2008) 93.16 2od, 43M ULD
2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD
(b) Czech dependency parsers on PDT
dependency parser test description
(McDonald et al., 2005b) 84.4 1od
(McDonald and Pereira, 2006) 85.2 2od
(Koo et al., 2008) 86.07 1od, 39M ULD
(Koo et al., 2008) 87.13 2od, 39M ULD
SS-SCM (w/ CL) 87.14 1od, 39M ULD
2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD
</table>
<tableCaption confidence="0.957925333333333">
Table 6: Comparisons with the previous top sys-
tems: (1od, 2od: 1st- and 2nd-order parsing
model, ULD: unlabeled data).
</tableCaption>
<bodyText confidence="0.921657">
p &lt; 0.01.
</bodyText>
<sectionHeader confidence="0.944503" genericHeader="method">
6 Comparison with Previous Methods
</sectionHeader>
<bodyText confidence="0.999933375">
Table 6 shows the performance of a number of
state-of-the-art approaches on the English and
Czech data sets. For both languages our ap-
proach gives the best reported figures on these
datasets. Our results yield relative error reduc-
tions of roughly 27% (English) and 20% (Czech)
over McDonald and Pereira (2006)’s second-order
supervised dependency parsers, and roughly 9%
(English) and 7% (Czech) over the previous best
results provided by Koo et. al. (2008)’s second-
order semi-supervised dependency parsers.
Note that there are some similarities between
our two-stage semi-supervised learning approach
and the semi-supervised learning method intro-
duced by (Blitzer et al., 2006), which is an exten-
sion of the method described by (Ando and Zhang,
</bodyText>
<page confidence="0.992345">
558
</page>
<bodyText confidence="0.999582">
2005). In particular, both methods use a two-stage
approach; They first train generative models or
auxiliary problems from unlabeled data, and then,
they incorporate these trained models into a super-
vised learning algorithm as real valued features.
Moreover, both methods make direct use of exist-
ing feature-vector definitions f(x, y) in inducing
representations from unlabeled data.
</bodyText>
<sectionHeader confidence="0.998083" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999992727272727">
This paper has described an extension of the
semi-supervised learning approach of (Suzuki and
Isozaki, 2008) to the dependency parsing problem.
In addition, we have described extensions that in-
corporate the cluster-based features of Koo et al.
(2008), and that allow the use of second-order
parsing models. We have described experiments
that show that the approach gives significant im-
provements over state-of-the-art methods for de-
pendency parsing; performance improves when
the amount of unlabeled data is increased from
43.8 million tokens to 3.72 billion tokens. The ap-
proach should be relatively easily applied to lan-
guages other than English or Czech.
We stress that the SS-SCM approach requires
relatively little hand-engineering: it makes di-
rect use of the existing feature-vector representa-
tion f(x, y) used in a discriminative model, and
does not require the design of new features. The
main choice in the approach is the partitioning
of f(x, y) into components r1(x, y) ... rk(x, y),
which in our experience is straightforward.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908056338028">
R. Kubota Ando and T. Zhang. 2005. A Framework for
Learning Predictive Structures from Multiple Tasks
and Unlabeled Data. Journal of Machine Learning
Research, 6:1817–1853.
J. K. Baker. 1979. Trainable Grammars for Speech
Recognition. In Speech Communication Papers for
the 97th Meeting of the Acoustical Society of Amer-
ica, pages 547–550.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
Adaptation with Structural Correspondence Learn-
ing. In Proc. of EMNLP-2006, pages 120–128.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram
Models of Natural Language. Computational Lin-
guistics, 18(4):467–479.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
of CoNLL-X, pages 149–164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of EMNLP-
CoNLL, pages 957–961.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann.
1999. A Statistical Parser for Czech. In Proc. of
ACL, pages 505–512.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proc. of
COLING-96, pages 340–345.
Jan Hajiˇc. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning. Studies in Honor of
Jarmila Panevov´a, pages 12–19. Prague Karolinum,
Charles University Press.
T. Koo, A. Globerson, X. Carreras, and M. Collins.
2007. Structured Prediction Models via the Matrix-
Tree Theorem. In Proc. of EMNLP-CoNLL, pages
141–150.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
Semi-supervised Dependency Parsing. In Proc. of
ACL-08: HLT, pages 595–603.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 282–289.
D. C. Liu and J. Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Math. Programming, Ser. B, 45(3):503–528.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
R. McDonald and F. Pereira. 2006. Online Learning of
Approximate Dependency Parsing Algorithms. In
Proc. of EACL, pages 81–88.
R. McDonald and G. Satta. 2007. On the Com-
plexity of Non-Projective Data-Driven Dependency
Parsing. In Proc. of IWPT, pages 121–132.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line Large-margin Training of Dependency Parsers.
In Proc. of ACL, pages 91–98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc.
2005b. Non-projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proc. of HLT-
EMNLP, pages 523–530.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proc. of
EMNLP-CoNLL, pages 915–932.
Mark A. Paskin. 2001. Cubic-time Parsing and Learn-
ing Algorithms for Grammatical Bigram. Technical
report, University of California at Berkeley, Berke-
ley, CA, USA.
</reference>
<page confidence="0.984189">
559
</page>
<reference confidence="0.999895578947368">
A. Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proc. of EMNLP,
pages 133–142.
D. A. Smith and J. Eisner. 2007. Bootstrapping
Feature-Rich Dependency Parsers with Entropic Pri-
ors. In Proc. of EMNLP-CoNLL, pages 667–677.
D. A. Smith and N. A. Smith. 2007. Probabilis-
tic Models of Nonprojective Dependency Trees. In
Proc. of EMNLP-CoNLL, pages 132–140.
J. Suzuki and H. Isozaki. 2008. Semi-supervised
Sequential Labeling and Segmentation Using Giga-
Word Scale Unlabeled Data. In Proc. of ACL-08:
HLT, pages 665–673.
Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi-
supervised Convex Training for Dependency Pars-
ing. In Proc. of ACL-08: HLT, pages 532–540.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
</reference>
<page confidence="0.997026">
560
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341693">
<title confidence="0.9965065">An Empirical Study of Semi-supervised Structured Conditional for Dependency Parsing</title>
<author confidence="0.997335">Jun Suzuki</author>
<author confidence="0.997335">Hideki Isozaki Xavier Carreras</author>
<author confidence="0.997335">Michael Collins</author>
<affiliation confidence="0.991932">NTT CS Lab., NTT Corp. MIT CSAIL</affiliation>
<address confidence="0.998942">Kyoto, 619-0237, Japan Cambridge, MA 02139, USA</address>
<email confidence="0.980285">jun@cslab.kecl.ntt.co.jpcarreras@csail.mit.eduisozaki@cslab.kecl.ntt.co.jpmcollins@csail.mit.edu</email>
<abstract confidence="0.998975961538461">This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semisupervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for En-</abstract>
<intro confidence="0.365788">glish, and 88.05% for Czech.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Kubota Ando</author>
<author>T Zhang</author>
</authors>
<title>A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<marker>Ando, Zhang, 2005</marker>
<rawString>R. Kubota Ando and T. Zhang. 2005. A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data. Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable Grammars for Speech Recognition.</title>
<date>1979</date>
<booktitle>In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<contexts>
<context position="9865" citStr="Baker, 1979" startWordPosition="1653" endWordPosition="1654">g q0j(x0i, h, m, l), (h,m,l)∈Y (5) where q0 j is as defined in Eq. 3. This function resembles the Q function used in the EM algorithm, where the hidden labels (in our case, dependency structures), are filled in using the conditional distribution p(y|x0i; w, v, q). It is simple to show that the estimates θj,a that maximize the function in Eq. 5 can be defined as follows. First, define a vector of expected counts based on w, v, q as p(y|x0 i; w, v, q) X rj(x0i, h, m, l). (h,m,l)∈Y Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then rj,a dj Pa=1 rj,a In a slight modification, we employ the following estimates in our model, where η &gt; 1 is a parameter of the model: θ _ (η − 1) + Tj,a (6) 7&apos;a — ddj X (η − 1) + PaL 1 rj,a This corresponds to a MAP estimate under a Dirichlet prior over the θj,a parameters. 2.4 The Complete Parameter-E</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable Grammars for Speech Recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, pages 547–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Domain Adaptation with Structural Correspondence Learning.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP-2006,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="32878" citStr="Blitzer et al., 2006" startWordPosition="5414" endWordPosition="5417">-art approaches on the English and Czech data sets. For both languages our approach gives the best reported figures on these datasets. Our results yield relative error reductions of roughly 27% (English) and 20% (Czech) over McDonald and Pereira (2006)’s second-order supervised dependency parsers, and roughly 9% (English) and 7% (Czech) over the previous best results provided by Koo et. al. (2008)’s secondorder semi-supervised dependency parsers. Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al., 2006), which is an extension of the method described by (Ando and Zhang, 558 2005). In particular, both methods use a two-stage approach; They first train generative models or auxiliary problems from unlabeled data, and then, they incorporate these trained models into a supervised learning algorithm as real valued features. Moreover, both methods make direct use of existing feature-vector definitions f(x, y) in inducing representations from unlabeled data. 7 Conclusion This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency pars</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain Adaptation with Structural Correspondence Learning. In Proc. of EMNLP-2006, pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>V J Della Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="13445" citStr="Brown et al., 1992" startWordPosition="2278" endWordPosition="2281">om the algorithm is the set of parameters (w1, v1, q1). Note that it is possible to iterate the method—steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008)—but in our experiments we only performed these steps once. 3 Extensions 3.1 Incorporating Cluster-Based Features Koo et al. (2008) describe a semi-supervised approach that incorporates cluster-based features, and that gives competitive results on dependency parsing benchmarks. The method is a two-stage approach. First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992). Second, a new feature set is constructed by representing words by bit-strings of various lengths, corresponding to clusters at different levels of the hierarchy. These features are combined with conventional features based on words and part-of-speech tags. The new feature set is then used within a conventional discriminative, supervised approach, such as the averaged perceptron algorithm. The important point is that their approach uses unlabeled data only for the construction of a new feature set, and never affects to learning algorithms. It is straightforward to incorporate clusterbased fea</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J. C. Lai. 1992. Class-based n-gram Models of Natural Language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL-X,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="1513" citStr="Buchholz and Marsi, 2006" startWordPosition="212" endWordPosition="215"> secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech. 1 Introduction Recent work has successfully developed dependency parsing models for many languages using supervised learning algorithms (Buchholz and Marsi, 2006; Nivre et al., 2007). Semi-supervised learning methods, which make use of unlabeled data in addition to labeled examples, have the potential to give improved performance over purely supervised methods for dependency parsing. It is often straightforward to obtain large amounts of unlabeled data, making semi-supervised approaches appealing; previous work on semisupervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008). In particular, Koo et al. (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabe</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proc. of CoNLL-X, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a Higher-Order Projective Dependency Parser.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="961" citStr="Carreras, 2007" startWordPosition="131" endWordPosition="132">his paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semisupervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech. 1 Introduction Recent work has successfully developed dependency parsing models for many languages using supervised learning algorithms (Buchholz and Marsi, 2006; Nivre et al., 2007). Semi-supervised learning </context>
<context position="3304" citStr="Carreras, 2007" startWordPosition="486" endWordPosition="487">dency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approach that in551 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551–560, Singapore, </context>
<context position="14325" citStr="Carreras, 2007" startWordPosition="2420" endWordPosition="2422">w feature set is then used within a conventional discriminative, supervised approach, such as the averaged perceptron algorithm. The important point is that their approach uses unlabeled data only for the construction of a new feature set, and never affects to learning algorithms. It is straightforward to incorporate clusterbased features within the SS-SCM approach described in this paper. We simply use the clusterbased feature-vector representation f(x, y) introduced by (Koo et al., 2008) as the basis of our approach. 3.2 Second-order Parsing Models Previous work (McDonald and Pereira, 2006; Carreras, 2007) has shown that second-order parsing models, which include information from “sibling” or “grandparent” relationships between dependencies, can give significant improvements in accuracy over first-order parsing models. In principle it would be straightforward to extend the SS-SCM approach that we have described to second-order parsing models. In practice, however, a bottleneck for the method would be the estimation of the generative models on unlabeled data. This step requires calculation of marginals on unlabeled data. Second-order parsing models generally require more costly inference methods</context>
<context position="22725" citStr="Carreras, 2007" startWordPosition="3780" endWordPosition="3781">, this is one systematic way of a feature design for this approach. 4.3 Other Experimental Settings All results presented in our experiments are given in terms of parent-prediction accuracy on unla3Training, development, and test data in PDT already contains POS tags assigned by the ‘feature-based tagger’. beled dependency parsing. We ignore the parentpredictions of punctuation tokens for English, while we retain all the punctuation tokens for Czech. These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008). We used the method proposed by (Carreras, 2007) for our second-order parsing model. Since this method only considers projective dependency structures, we “projectivized” the PDT training data in the same way as (Koo et al., 2008). We used a non-projective model, trained using an application of the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for the first-order Czech models, and projective parsers for all other models. As shown in Section 2, SS-SCMs with 1st-order parsing models have two tunable parameters, C and q, corresponding to the regularization constant, and the Dirichlet prior for the gene</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a Higher-Order Projective Dependency Parser. In Proc. of EMNLPCoNLL, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Hajic</author>
<author>L Ramshaw</author>
<author>C Tillmann</author>
</authors>
<title>A Statistical Parser for Czech.</title>
<date>1999</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="20993" citStr="Collins et al., 1999" startWordPosition="3489" endWordPosition="3492">hat the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3, and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown in Table 1 were used to construct the hierarchical clusterings used within the approach. Note that when this feature set is used within the SSSCM approach, the same set of unlabeled data is used to both induce the clusters, and to estimate the genera</context>
</contexts>
<marker>Collins, Hajic, Ramshaw, Tillmann, 1999</marker>
<rawString>M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999. A Statistical Parser for Czech. In Proc. of ACL, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING-96,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="9895" citStr="Eisner, 1996" startWordPosition="1658" endWordPosition="1659">Y (5) where q0 j is as defined in Eq. 3. This function resembles the Q function used in the EM algorithm, where the hidden labels (in our case, dependency structures), are filled in using the conditional distribution p(y|x0i; w, v, q). It is simple to show that the estimates θj,a that maximize the function in Eq. 5 can be defined as follows. First, define a vector of expected counts based on w, v, q as p(y|x0 i; w, v, q) X rj(x0i, h, m, l). (h,m,l)∈Y Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then rj,a dj Pa=1 rj,a In a slight modification, we employ the following estimates in our model, where η &gt; 1 is a parameter of the model: θ _ (η − 1) + Tj,a (6) 7&apos;a — ddj X (η − 1) + PaL 1 rj,a This corresponds to a MAP estimate under a Dirichlet prior over the θj,a parameters. 2.4 The Complete Parameter-Estimation Method This section </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. In Proc. of COLING-96, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Building a Syntactically Annotated Corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>In Issues of Valency and Meaning. Studies in Honor of Jarmila Panevov´a,</booktitle>
<pages>12--19</pages>
<publisher>Prague Karolinum, Charles University Press.</publisher>
<marker>Hajiˇc, 1998</marker>
<rawString>Jan Hajiˇc. 1998. Building a Syntactically Annotated Corpus: The Prague Dependency Treebank. In Issues of Valency and Meaning. Studies in Honor of Jarmila Panevov´a, pages 12–19. Prague Karolinum, Charles University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A Globerson</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Structured Prediction Models via the MatrixTree Theorem.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="10028" citStr="Koo et al., 2007" startWordPosition="1673" endWordPosition="1676">(in our case, dependency structures), are filled in using the conditional distribution p(y|x0i; w, v, q). It is simple to show that the estimates θj,a that maximize the function in Eq. 5 can be defined as follows. First, define a vector of expected counts based on w, v, q as p(y|x0 i; w, v, q) X rj(x0i, h, m, l). (h,m,l)∈Y Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then rj,a dj Pa=1 rj,a In a slight modification, we employ the following estimates in our model, where η &gt; 1 is a parameter of the model: θ _ (η − 1) + Tj,a (6) 7&apos;a — ddj X (η − 1) + PaL 1 rj,a This corresponds to a MAP estimate under a Dirichlet prior over the θj,a parameters. 2.4 The Complete Parameter-Estimation Method This section describes the full parameter estimation method. The input to the algorithm is a set of labeled examples {xi, yi}Ni=1, a set of unlabe</context>
<context position="23014" citStr="Koo et al., 2007" startWordPosition="3824" endWordPosition="3827">ture-based tagger’. beled dependency parsing. We ignore the parentpredictions of punctuation tokens for English, while we retain all the punctuation tokens for Czech. These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008). We used the method proposed by (Carreras, 2007) for our second-order parsing model. Since this method only considers projective dependency structures, we “projectivized” the PDT training data in the same way as (Koo et al., 2008). We used a non-projective model, trained using an application of the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for the first-order Czech models, and projective parsers for all other models. As shown in Section 2, SS-SCMs with 1st-order parsing models have two tunable parameters, C and q, corresponding to the regularization constant, and the Dirichlet prior for the generative models. We selected a fixed value q = 2, which was found to work well in preliminary experiments.4 The value of C was chosen to optimize performance on development data. Note that C for supervised SCMs were also tuned on development data. For the two-stage SS-SCM for incorporating </context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007. Structured Prediction Models via the MatrixTree Theorem. In Proc. of EMNLP-CoNLL, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="839" citStr="Koo et al., 2008" startWordPosition="107" endWordPosition="110">MA 02139, USA jun@cslab.kecl.ntt.co.jp carreras@csail.mit.edu isozaki@cslab.kecl.ntt.co.jp mcollins@csail.mit.edu Abstract This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semisupervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech. 1 Introduction Recent work has successfully developed dependency parsing models for man</context>
<context position="3177" citStr="Koo et al., 2008" startWordPosition="465" endWordPosition="468">for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approac</context>
<context position="13131" citStr="Koo et al. (2008)" startWordPosition="2234" endWordPosition="2237">Re-estimation of w and v. In the final step, w1 and v1 are estimated as arg max,,v L(w, v; q1) where L(w, v; q1) is defined in an analogous way to L(w, v; q0). Thus w and v are re-estimated to optimize log-likelihood of the labeled examples, with the generative models q1 estimated in step 2. The final output from the algorithm is the set of parameters (w1, v1, q1). Note that it is possible to iterate the method—steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008)—but in our experiments we only performed these steps once. 3 Extensions 3.1 Incorporating Cluster-Based Features Koo et al. (2008) describe a semi-supervised approach that incorporates cluster-based features, and that gives competitive results on dependency parsing benchmarks. The method is a two-stage approach. First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992). Second, a new feature set is constructed by representing words by bit-strings of various lengths, corresponding to clusters at different levels of the hierarchy. These features are combined with conventional features based on words and part-of-speech tags. The new feature set is then</context>
<context position="18645" citStr="Koo et al., 2008" startWordPosition="3100" endWordPosition="3103">ish and Czech data. We used the Wall Street Journal sections of the Penn Treebank (PTB) III (Marcus et al., 1994) as a source of labeled data for English, and the Prague Dependency Treebank (PDT) 1.0 (Hajiˇc, 1998) for Czech. To facilitate comparisons with previous work, we used exactly the same training, development and test sets Table 2: Details of the larger unlabeled data set used in English dependency parsing: sentences exceeding 128 tokens in length were excluded for computational reasons. as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations. We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test. The Czech data sets were obtained from the predefined training/development/test partition in the PDT. The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing (BLLIP) Corpus (LDC2000T43)2, giving a total of 1,</context>
<context position="20562" citStr="Koo et al., 2008" startWordPosition="3418" endWordPosition="3421">8 tokens for computational reasons. Note that the total size of the unlabeled data reaches 3.72G (billion) tokens, which is approxi2We ensured that the sentences used in the PTB were excluded from the unlabeled data, since sentences used in BLLIP corpus are a super-set of the PTB. k E j=1 +B 555 mately 4,000 times larger than the size of labeled training data. 4.2 Features 4.2.1 Baseline Features In general we will assume that the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3, and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the </context>
<context position="22676" citStr="Koo et al., 2008" startWordPosition="3770" endWordPosition="3773">at, in general, we are not necessary to do as above, this is one systematic way of a feature design for this approach. 4.3 Other Experimental Settings All results presented in our experiments are given in terms of parent-prediction accuracy on unla3Training, development, and test data in PDT already contains POS tags assigned by the ‘feature-based tagger’. beled dependency parsing. We ignore the parentpredictions of punctuation tokens for English, while we retain all the punctuation tokens for Czech. These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008). We used the method proposed by (Carreras, 2007) for our second-order parsing model. Since this method only considers projective dependency structures, we “projectivized” the PDT training data in the same way as (Koo et al., 2008). We used a non-projective model, trained using an application of the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for the first-order Czech models, and projective parsers for all other models. As shown in Section 2, SS-SCMs with 1st-order parsing models have two tunable parameters, C and q, corresponding to the regularizati</context>
<context position="24280" citStr="Koo et al., 2008" startWordPosition="4035" endWordPosition="4038">e tunable parameter B shown in Eq. 8. This was also chosen by the value that provided the best performance on development data. In addition to providing results for models trained on the full training sets, we also performed experiments with smaller labeled training sets. These training sets were either created through random sampling or by using a predefined subset of document IDs from the labeled training data. 5 Results and Discussion Table 3 gives results for the SS-SCM method under various configurations: for first and secondorder parsing models, with and without the cluster features of (Koo et al., 2008), and for varying amounts of labeled data. The remainder of this section discusses these results in more detail. 5.1 Effects of the Quantity of Labeled Data We can see from the results in Table 3 that our semi-supervised approach consistently gives gains 4An intuitive meaning of rl = 2 is that this adds one pseudo expected count to every feature when estimating new parameter values. 556 (a) English dependency parsing: w/ 43M token unlabeled data (BLLIP) WSJ sec. IDs wsj 21 random selection random selection wsj 15–18 wsj 02-21(all) # of sentences / tokens 1,671 / 40,039 2,000 / 48,577 8,000 / 1</context>
<context position="27110" citStr="Koo et al., 2008" startWordPosition="4479" endWordPosition="4482">ervised SCM (1od) and Supervised MIRA (2od) are the baseline first and second-order approaches; SS-SCM (1od) and 2-stage SS-SCM(+MIRA) (2od) are the first and second-order approaches described in this paper. “Baseline” refers to models without cluster-based features, “CL” refers to models which make use of cluster-based features. in performance under various sizes of labeled data. Note that the baseline methods that we have used in these experiments are strong baselines. It is clear that the gains from our method are larger for smaller labeled data sizes, a tendency that was also observed in (Koo et al., 2008). 5.2 Impact of Combining SS-SCM with Cluster Features One important observation from the results in Table 3 is that SS-SCMs can successfully improve the performance over a baseline method that uses the cluster-based feature set (CL). This is in spite of the fact that the generative models within the SS-SCM approach were trained on the same unlabeled data used to induce the cluster-based features. 5.3 Impact of the Two-stage Approach Table 3 also shows the effectiveness of the twostage approach (described in Section 3.2) that integrates the SS-SCM method within a second-order parser. This sugg</context>
<context position="31591" citStr="Koo et al., 2008" startWordPosition="5205" endWordPosition="5208">abeled data. The improvements on test data are similar to those observed on the development data. To determine statistical significance, we tested the difference of parent-prediction error-rates at the sentence level using a paired Wilcoxon signed rank test. All eight comparisons shown in Table 5 are significant with Table 5: Parent-prediction accuracies on test data using the best setting in terms of development data performances in each condition. (a) English dependency parsers on PTB dependency parser test description (McDonald et al., 2005a) 90.9 1od (McDonald and Pereira, 2006) 91.5 2od (Koo et al., 2008) 92.23 1od, 43M ULD SS-SCM (w/ CL) 92.70 1od, 3.72G ULD (Koo et al., 2008) 93.16 2od, 43M ULD 2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD (b) Czech dependency parsers on PDT dependency parser test description (McDonald et al., 2005b) 84.4 1od (McDonald and Pereira, 2006) 85.2 2od (Koo et al., 2008) 86.07 1od, 39M ULD (Koo et al., 2008) 87.13 2od, 39M ULD SS-SCM (w/ CL) 87.14 1od, 39M ULD 2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD Table 6: Comparisons with the previous top systems: (1od, 2od: 1st- and 2nd-order parsing model, ULD: unlabeled data). p &lt; 0.01. 6 Comparison with Previous</context>
<context position="33597" citStr="Koo et al. (2008)" startWordPosition="5524" endWordPosition="5527">hods use a two-stage approach; They first train generative models or auxiliary problems from unlabeled data, and then, they incorporate these trained models into a supervised learning algorithm as real valued features. Moreover, both methods make direct use of existing feature-vector definitions f(x, y) in inducing representations from unlabeled data. 7 Conclusion This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem. In addition, we have described extensions that incorporate the cluster-based features of Koo et al. (2008), and that allow the use of second-order parsing models. We have described experiments that show that the approach gives significant improvements over state-of-the-art methods for dependency parsing; performance improves when the amount of unlabeled data is increased from 43.8 million tokens to 3.72 billion tokens. The approach should be relatively easily applied to languages other than English or Czech. We stress that the SS-SCM approach requires relatively little hand-engineering: it makes direct use of the existing feature-vector representation f(x, y) used in a discriminative model, and do</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proc. of ACL-08: HLT, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML-2001,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="6370" citStr="Lafferty et al., 2001" startWordPosition="1011" endWordPosition="1014">rj might include only those features corresponding to word bigrams involved in dependencies (i.e., indicator functions tied to the word bigram (xm, xh) involved in a dependency (x, h, m, l)). We then define a generative model that assigns a probability corporates a second-order parsing model. In addition, we evaluate the SS-SCM for English dependency parsing with large amounts (up to 3.72 billion tokens) of unlabeled data. 2 Semi-supervised Structured Conditional Models for Dependency Parsing Suzuki et al. (2008) describe a semi-supervised learning method for conditional random fields (CRFs) (Lafferty et al., 2001). In this paper we extend this method to the dependency parsing problem. We will refer to this extended method as Semi-supervised Structured Conditional Models (SS-SCMs). The remainder of this section describes our approach. 2.1 The Basic Model Throughout this paper we will use x to denote an input sentence, and y to denote a labeled dependency structure. Given a sentence x with n words, a labeled dependency structure y is a set of n dependencies of the form (h, m, l), where h is the index of the head-word in the dependency, m is the index of the modifier word, and l is the label of the depend</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. of ICML-2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the Limited Memory BFGS Method for Large Scale Optimization.</title>
<date>1989</date>
<journal>Math. Programming, Ser. B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="11896" citStr="Liu and Nocedal, 1989" startWordPosition="2023" endWordPosition="2026">les, with the generative model fixed at q0, to be: L(w, v; q0) = Xn log p(yi|xi; w, v, q0) i=1 C 3||w||2 + ||v||2´ 2 qj(x, h, m, l) = Xdj θj,a (4) a=1 rj,a(x, h, m, l) log cj,a XM i=1 X Y X Y rj = XM i=1 θj,a = 553 This is a conventional regularized log-likelihood function, as commonly used in CRF models. The parameter C &gt; 0 dictates the level of regularization in the model. We define the initial parameters (w0, v0) = arg max,,v L(w, v; q0). These parameters can be found using conventional methods for estimating the parameters of regularized log-likelihood functions (in our case we use LBFGS (Liu and Nocedal, 1989)). Note that the gradient of the log-likelihood function can be calculated using the inside-outside algorithm applied to projective dependency parse structures, or the matrix-tree theorem applied to non-projective structures. Step 2: Estimation of the Generative Models. In this step, expected count vectors r1 ... rk are first calculated, based on the distribution p(y|x; w0, v0, q0). Generative model parameters Oj,a are calculated through the definition in Eq. 6; these estimates define updated generative models q1j for j = 1... k through Eq. 4. We refer to the new values for the generative mode</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the Limited Memory BFGS Method for Large Scale Optimization. Math. Programming, Ser. B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="18141" citStr="Marcus et al., 1994" startWordPosition="3016" endWordPosition="3020">) (8) vjqj(x,y), where B represents a tunable scaling factor, and f1 and f2 represent the feature vectors of first and second-order parsing parts, respectively. 4 Experiments We now describe experiments investigating the effectiveness of the SS-SCM approach for dependency parsing. The experiments test basic, firstorder parsing models, as well as the extensions to cluster-based features and second-order parsing models described in the previous section. 4.1 Data Sets We conducted experiments on both English and Czech data. We used the Wall Street Journal sections of the Penn Treebank (PTB) III (Marcus et al., 1994) as a source of labeled data for English, and the Prague Dependency Treebank (PDT) 1.0 (Hajiˇc, 1998) for Czech. To facilitate comparisons with previous work, we used exactly the same training, development and test sets Table 2: Details of the larger unlabeled data set used in English dependency parsing: sentences exceeding 128 tokens in length were excluded for computational reasons. as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selectio</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2485" citStr="McDonald and Pereira, 2006" startWordPosition="360" endWordPosition="363">vious work on semisupervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008). In particular, Koo et al. (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension </context>
<context position="14308" citStr="McDonald and Pereira, 2006" startWordPosition="2416" endWordPosition="2419"> part-of-speech tags. The new feature set is then used within a conventional discriminative, supervised approach, such as the averaged perceptron algorithm. The important point is that their approach uses unlabeled data only for the construction of a new feature set, and never affects to learning algorithms. It is straightforward to incorporate clusterbased features within the SS-SCM approach described in this paper. We simply use the clusterbased feature-vector representation f(x, y) introduced by (Koo et al., 2008) as the basis of our approach. 3.2 Second-order Parsing Models Previous work (McDonald and Pereira, 2006; Carreras, 2007) has shown that second-order parsing models, which include information from “sibling” or “grandparent” relationships between dependencies, can give significant improvements in accuracy over first-order parsing models. In principle it would be straightforward to extend the SS-SCM approach that we have described to second-order parsing models. In practice, however, a bottleneck for the method would be the estimation of the generative models on unlabeled data. This step requires calculation of marginals on unlabeled data. Second-order parsing models generally require more costly </context>
<context position="15892" citStr="McDonald and Pereira, 2006" startWordPosition="2667" endWordPosition="2670"> generative models q1 ... qk from unlabeled data. In the second stage, we incorporate these generative models as features within a second-order parsing model. More precisely, in our approach, we first train a first-order parsing model by Step 1 and 2, exactly as described in Section 2.4, to estimate w0, v0 and q1. Then, we substitute Step 3 as a supervised learning such as MIRA with a second-order parsing model (McDonald et al., 2005a), which incorporates q1 as a real-values features. We refer this two-stage approach to as two-stage SS-SCM. In our experiments we use the 1-best MIRA algorithm (McDonald and Pereira, 2006)1 as a 1We used a slightly modified version of 1-best MIRA, whose difference can be found in the third line in Eq. 7, namely, including L(yi, y). 554 (a) English dependency parsing Data set (WSJ Sec. IDs) # of sentences # of tokens Training (02–21) 39,832 950,028 Development (22) 1,700 40,117 Test (23) 2,012 47,377 Unlabeled 1,796,379 43,380,315 (b) Czech dependency parsing Data set # of sentences # of tokens Training 73,088 1,255,590 Development 7,507 126,030 Test 7,319 125,713 Unlabeled 2,349,224 39,336,570 Corpus article name (mm/yy) # of sent. # of tokens BLLIP wsj 00/87–00/89 1,796,379 43</context>
<context position="18626" citStr="McDonald and Pereira, 2006" startWordPosition="3096" endWordPosition="3099">ted experiments on both English and Czech data. We used the Wall Street Journal sections of the Penn Treebank (PTB) III (Marcus et al., 1994) as a source of labeled data for English, and the Prague Dependency Treebank (PDT) 1.0 (Hajiˇc, 1998) for Czech. To facilitate comparisons with previous work, we used exactly the same training, development and test sets Table 2: Details of the larger unlabeled data set used in English dependency parsing: sentences exceeding 128 tokens in length were excluded for computational reasons. as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations. We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test. The Czech data sets were obtained from the predefined training/development/test partition in the PDT. The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing (BLLIP) Corpus (LDC2000T43)2, g</context>
<context position="31563" citStr="McDonald and Pereira, 2006" startWordPosition="5199" endWordPosition="5202">ieved using 3.72 billion tokens of unlabeled data. The improvements on test data are similar to those observed on the development data. To determine statistical significance, we tested the difference of parent-prediction error-rates at the sentence level using a paired Wilcoxon signed rank test. All eight comparisons shown in Table 5 are significant with Table 5: Parent-prediction accuracies on test data using the best setting in terms of development data performances in each condition. (a) English dependency parsers on PTB dependency parser test description (McDonald et al., 2005a) 90.9 1od (McDonald and Pereira, 2006) 91.5 2od (Koo et al., 2008) 92.23 1od, 43M ULD SS-SCM (w/ CL) 92.70 1od, 3.72G ULD (Koo et al., 2008) 93.16 2od, 43M ULD 2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD (b) Czech dependency parsers on PDT dependency parser test description (McDonald et al., 2005b) 84.4 1od (McDonald and Pereira, 2006) 85.2 2od (Koo et al., 2008) 86.07 1od, 39M ULD (Koo et al., 2008) 87.13 2od, 39M ULD SS-SCM (w/ CL) 87.14 1od, 39M ULD 2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD Table 6: Comparisons with the previous top systems: (1od, 2od: 1st- and 2nd-order parsing model, ULD: unlabeled data). p &lt; 0.01</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proc. of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>G Satta</author>
</authors>
<title>On the Complexity of Non-Projective Data-Driven Dependency Parsing.</title>
<date>2007</date>
<booktitle>In Proc. of IWPT,</booktitle>
<pages>121--132</pages>
<contexts>
<context position="10078" citStr="McDonald and Satta, 2007" startWordPosition="1681" endWordPosition="1684"> filled in using the conditional distribution p(y|x0i; w, v, q). It is simple to show that the estimates θj,a that maximize the function in Eq. 5 can be defined as follows. First, define a vector of expected counts based on w, v, q as p(y|x0 i; w, v, q) X rj(x0i, h, m, l). (h,m,l)∈Y Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then rj,a dj Pa=1 rj,a In a slight modification, we employ the following estimates in our model, where η &gt; 1 is a parameter of the model: θ _ (η − 1) + Tj,a (6) 7&apos;a — ddj X (η − 1) + PaL 1 rj,a This corresponds to a MAP estimate under a Dirichlet prior over the θj,a parameters. 2.4 The Complete Parameter-Estimation Method This section describes the full parameter estimation method. The input to the algorithm is a set of labeled examples {xi, yi}Ni=1, a set of unlabeled examples {x0i}Mi=1, a feature-vector definitio</context>
<context position="23064" citStr="McDonald and Satta, 2007" startWordPosition="3832" endWordPosition="3835">ing. We ignore the parentpredictions of punctuation tokens for English, while we retain all the punctuation tokens for Czech. These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008). We used the method proposed by (Carreras, 2007) for our second-order parsing model. Since this method only considers projective dependency structures, we “projectivized” the PDT training data in the same way as (Koo et al., 2008). We used a non-projective model, trained using an application of the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for the first-order Czech models, and projective parsers for all other models. As shown in Section 2, SS-SCMs with 1st-order parsing models have two tunable parameters, C and q, corresponding to the regularization constant, and the Dirichlet prior for the generative models. We selected a fixed value q = 2, which was found to work well in preliminary experiments.4 The value of C was chosen to optimize performance on development data. Note that C for supervised SCMs were also tuned on development data. For the two-stage SS-SCM for incorporating second-order parsing model, we have additional one</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>R. McDonald and G. Satta. 2007. On the Complexity of Non-Projective Data-Driven Dependency Parsing. In Proc. of IWPT, pages 121–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online Large-margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="7651" citStr="McDonald et al., 2005" startWordPosition="1248" endWordPosition="1251">ccess to a set of labeled training examples, {xz, yz}Z_&apos;1, and in addition a set of unlabeled examples, {xz}M1. In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: 1 p(y|x) = Z(x) exp{g(x,y)}, (1) where Z(x) is the partition function, w is a parameter vector, and g(x, y) = E w · f(x, h, m, l) (h,m,l)Ey Here f(x, h, m, l) is a feature vector representing the dependency (h, m, l) in the context of the sentence x (see for example (McDonald et al., 2005a)). In this paper we extend the definition of g(x, y) to include features that are induced from unlabeled data. Specifically, we define g(x, y) = E w · f(x, h, m, l) (h,m,l)Ey dj H a=1 θrj,a(x,h,m,l) j,a 3) ( q�(x, h, m, l) = to the dj-dimensional feature vector rj(x, h, m, l). The parameters of this model are θj,1 ... θj,dj; 552 they form a multinomial distribution, with the constraints that θj,a &gt; 0, and Pa θj,a = 1. This model can be viewed as a very simple (naiveBayes) model that defines a distribution over feature vectors rj E Rdj. The next section describes how the parameters θj,a are t</context>
<context position="15702" citStr="McDonald et al., 2005" startWordPosition="2636" endWordPosition="2640"> simple ‘two-stage’ approach for extending the SS-SCM approach to the second-order parsing model of (Carreras, 2007). In the first stage, we use a first-order parsing model to estimate generative models q1 ... qk from unlabeled data. In the second stage, we incorporate these generative models as features within a second-order parsing model. More precisely, in our approach, we first train a first-order parsing model by Step 1 and 2, exactly as described in Section 2.4, to estimate w0, v0 and q1. Then, we substitute Step 3 as a supervised learning such as MIRA with a second-order parsing model (McDonald et al., 2005a), which incorporates q1 as a real-values features. We refer this two-stage approach to as two-stage SS-SCM. In our experiments we use the 1-best MIRA algorithm (McDonald and Pereira, 2006)1 as a 1We used a slightly modified version of 1-best MIRA, whose difference can be found in the third line in Eq. 7, namely, including L(yi, y). 554 (a) English dependency parsing Data set (WSJ Sec. IDs) # of sentences # of tokens Training (02–21) 39,832 950,028 Development (22) 1,700 40,117 Test (23) 2,012 47,377 Unlabeled 1,796,379 43,380,315 (b) Czech dependency parsing Data set # of sentences # of toke</context>
<context position="18573" citStr="McDonald et al., 2005" startWordPosition="3088" endWordPosition="3091">in the previous section. 4.1 Data Sets We conducted experiments on both English and Czech data. We used the Wall Street Journal sections of the Penn Treebank (PTB) III (Marcus et al., 1994) as a source of labeled data for English, and the Prague Dependency Treebank (PDT) 1.0 (Hajiˇc, 1998) for Czech. To facilitate comparisons with previous work, we used exactly the same training, development and test sets Table 2: Details of the larger unlabeled data set used in English dependency parsing: sentences exceeding 128 tokens in length were excluded for computational reasons. as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations. We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test. The Czech data sets were obtained from the predefined training/development/test partition in the PDT. The unlabeled data for English was derived from the Brown Laboratory for Linguistic I</context>
<context position="20542" citStr="McDonald et al., 2005" startWordPosition="3414" endWordPosition="3417">s that have more than 128 tokens for computational reasons. Note that the total size of the unlabeled data reaches 3.72G (billion) tokens, which is approxi2We ensured that the sentences used in the PTB were excluded from the unlabeled data, since sentences used in BLLIP corpus are a super-set of the PTB. k E j=1 +B 555 mately 4,000 times larger than the size of labeled training data. 4.2 Features 4.2.1 Baseline Features In general we will assume that the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3, and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feat</context>
<context position="22656" citStr="McDonald et al., 2005" startWordPosition="3766" endWordPosition="3769">ature templates. Note that, in general, we are not necessary to do as above, this is one systematic way of a feature design for this approach. 4.3 Other Experimental Settings All results presented in our experiments are given in terms of parent-prediction accuracy on unla3Training, development, and test data in PDT already contains POS tags assigned by the ‘feature-based tagger’. beled dependency parsing. We ignore the parentpredictions of punctuation tokens for English, while we retain all the punctuation tokens for Czech. These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008). We used the method proposed by (Carreras, 2007) for our second-order parsing model. Since this method only considers projective dependency structures, we “projectivized” the PDT training data in the same way as (Koo et al., 2008). We used a non-projective model, trained using an application of the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for the first-order Czech models, and projective parsers for all other models. As shown in Section 2, SS-SCMs with 1st-order parsing models have two tunable parameters, C and q, corresponding</context>
<context position="31523" citStr="McDonald et al., 2005" startWordPosition="5193" endWordPosition="5196">esults shown in the table were achieved using 3.72 billion tokens of unlabeled data. The improvements on test data are similar to those observed on the development data. To determine statistical significance, we tested the difference of parent-prediction error-rates at the sentence level using a paired Wilcoxon signed rank test. All eight comparisons shown in Table 5 are significant with Table 5: Parent-prediction accuracies on test data using the best setting in terms of development data performances in each condition. (a) English dependency parsers on PTB dependency parser test description (McDonald et al., 2005a) 90.9 1od (McDonald and Pereira, 2006) 91.5 2od (Koo et al., 2008) 92.23 1od, 43M ULD SS-SCM (w/ CL) 92.70 1od, 3.72G ULD (Koo et al., 2008) 93.16 2od, 43M ULD 2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD (b) Czech dependency parsers on PDT dependency parser test description (McDonald et al., 2005b) 84.4 1od (McDonald and Pereira, 2006) 85.2 2od (Koo et al., 2008) 86.07 1od, 39M ULD (Koo et al., 2008) 87.13 2od, 39M ULD SS-SCM (w/ CL) 87.14 1od, 39M ULD 2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD Table 6: Comparisons with the previous top systems: (1od, 2od: 1st- and 2nd-order parsi</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online Large-margin Training of Dependency Parsers. In Proc. of ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLTEMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005b. Non-projective Dependency Parsing using Spanning Tree Algorithms. In Proc. of HLTEMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>Shared Task on Dependency Parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In Proc. of EMNLP-CoNLL, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Paskin</author>
</authors>
<title>Cubic-time Parsing and Learning Algorithms for Grammatical Bigram.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>University of California at Berkeley,</institution>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="9945" citStr="Paskin, 2001" startWordPosition="1663" endWordPosition="1664">tion resembles the Q function used in the EM algorithm, where the hidden labels (in our case, dependency structures), are filled in using the conditional distribution p(y|x0i; w, v, q). It is simple to show that the estimates θj,a that maximize the function in Eq. 5 can be defined as follows. First, define a vector of expected counts based on w, v, q as p(y|x0 i; w, v, q) X rj(x0i, h, m, l). (h,m,l)∈Y Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then rj,a dj Pa=1 rj,a In a slight modification, we employ the following estimates in our model, where η &gt; 1 is a parameter of the model: θ _ (η − 1) + Tj,a (6) 7&apos;a — ddj X (η − 1) + PaL 1 rj,a This corresponds to a MAP estimate under a Dirichlet prior over the θj,a parameters. 2.4 The Complete Parameter-Estimation Method This section describes the full parameter estimation method. Th</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>Mark A. Paskin. 2001. Cubic-time Parsing and Learning Algorithms for Grammatical Bigram. Technical report, University of California at Berkeley, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="20743" citStr="Ratnaparkhi, 1996" startWordPosition="3447" endWordPosition="3449">re excluded from the unlabeled data, since sentences used in BLLIP corpus are a super-set of the PTB. k E j=1 +B 555 mately 4,000 times larger than the size of labeled training data. 4.2 Features 4.2.1 Baseline Features In general we will assume that the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3, and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Proc. of EMNLP, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Bootstrapping Feature-Rich Dependency Parsers with Entropic Priors.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>667--677</pages>
<contexts>
<context position="1950" citStr="Smith and Eisner, 2007" startWordPosition="276" endWordPosition="279">, and 88.05% for Czech. 1 Introduction Recent work has successfully developed dependency parsing models for many languages using supervised learning algorithms (Buchholz and Marsi, 2006; Nivre et al., 2007). Semi-supervised learning methods, which make use of unlabeled data in addition to labeled examples, have the potential to give improved performance over purely supervised methods for dependency parsing. It is often straightforward to obtain large amounts of unlabeled data, making semi-supervised approaches appealing; previous work on semisupervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008). In particular, Koo et al. (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised</context>
</contexts>
<marker>Smith, Eisner, 2007</marker>
<rawString>D. A. Smith and J. Eisner. 2007. Bootstrapping Feature-Rich Dependency Parsers with Entropic Priors. In Proc. of EMNLP-CoNLL, pages 667–677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic Models of Nonprojective Dependency Trees.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>132--140</pages>
<contexts>
<context position="10051" citStr="Smith and Smith, 2007" startWordPosition="1677" endWordPosition="1680">ndency structures), are filled in using the conditional distribution p(y|x0i; w, v, q). It is simple to show that the estimates θj,a that maximize the function in Eq. 5 can be defined as follows. First, define a vector of expected counts based on w, v, q as p(y|x0 i; w, v, q) X rj(x0i, h, m, l). (h,m,l)∈Y Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then rj,a dj Pa=1 rj,a In a slight modification, we employ the following estimates in our model, where η &gt; 1 is a parameter of the model: θ _ (η − 1) + Tj,a (6) 7&apos;a — ddj X (η − 1) + PaL 1 rj,a This corresponds to a MAP estimate under a Dirichlet prior over the θj,a parameters. 2.4 The Complete Parameter-Estimation Method This section describes the full parameter estimation method. The input to the algorithm is a set of labeled examples {xi, yi}Ni=1, a set of unlabeled examples {x0i}Mi=1,</context>
<context position="23037" citStr="Smith and Smith, 2007" startWordPosition="3828" endWordPosition="3831">. beled dependency parsing. We ignore the parentpredictions of punctuation tokens for English, while we retain all the punctuation tokens for Czech. These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008). We used the method proposed by (Carreras, 2007) for our second-order parsing model. Since this method only considers projective dependency structures, we “projectivized” the PDT training data in the same way as (Koo et al., 2008). We used a non-projective model, trained using an application of the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for the first-order Czech models, and projective parsers for all other models. As shown in Section 2, SS-SCMs with 1st-order parsing models have two tunable parameters, C and q, corresponding to the regularization constant, and the Dirichlet prior for the generative models. We selected a fixed value q = 2, which was found to work well in preliminary experiments.4 The value of C was chosen to optimize performance on development data. Note that C for supervised SCMs were also tuned on development data. For the two-stage SS-SCM for incorporating second-order parsing mo</context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>D. A. Smith and N. A. Smith. 2007. Probabilistic Models of Nonprojective Dependency Trees. In Proc. of EMNLP-CoNLL, pages 132–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
</authors>
<title>Semi-supervised Sequential Labeling and Segmentation Using GigaWord Scale Unlabeled Data.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>665--673</pages>
<contexts>
<context position="656" citStr="Suzuki and Isozaki, 2008" startWordPosition="78" endWordPosition="81">rvised Structured Conditional Models for Dependency Parsing Jun Suzuki, Hideki Isozaki Xavier Carreras, and Michael Collins NTT CS Lab., NTT Corp. MIT CSAIL Kyoto, 619-0237, Japan Cambridge, MA 02139, USA jun@cslab.kecl.ntt.co.jp carreras@csail.mit.edu isozaki@cslab.kecl.ntt.co.jp mcollins@csail.mit.edu Abstract This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semisupervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in </context>
<context position="2665" citStr="Suzuki and Isozaki, 2008" startWordPosition="385" endWordPosition="388">upervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more spe</context>
<context position="13000" citStr="Suzuki and Isozaki, 2008" startWordPosition="2215" endWordPosition="2218">tes define updated generative models q1j for j = 1... k through Eq. 4. We refer to the new values for the generative models as q1. Step 3: Re-estimation of w and v. In the final step, w1 and v1 are estimated as arg max,,v L(w, v; q1) where L(w, v; q1) is defined in an analogous way to L(w, v; q0). Thus w and v are re-estimated to optimize log-likelihood of the labeled examples, with the generative models q1 estimated in step 2. The final output from the algorithm is the set of parameters (w1, v1, q1). Note that it is possible to iterate the method—steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008)—but in our experiments we only performed these steps once. 3 Extensions 3.1 Incorporating Cluster-Based Features Koo et al. (2008) describe a semi-supervised approach that incorporates cluster-based features, and that gives competitive results on dependency parsing benchmarks. The method is a two-stage approach. First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992). Second, a new feature set is constructed by representing words by bit-strings of various lengths, corresponding to clusters at different levels of the hi</context>
<context position="21928" citStr="Suzuki and Isozaki, 2008" startWordPosition="3651" endWordPosition="3654"> (39M tokens) unlabeled data sets shown in Table 1 were used to construct the hierarchical clusterings used within the approach. Note that when this feature set is used within the SSSCM approach, the same set of unlabeled data is used to both induce the clusters, and to estimate the generative models within the SS-SCM model. 4.2.3 Constructing the Generative Models As described in section 2.2, the generative models in the SS-SCM approach are defined through a partition of the original feature vector f(x, y) into k feature vectors r1(x, y) ... rk(x, y). We follow a similar approach to that of (Suzuki and Isozaki, 2008) in partitioning f(x, y), where the k different feature vectors correspond to different feature types or feature templates. Note that, in general, we are not necessary to do as above, this is one systematic way of a feature design for this approach. 4.3 Other Experimental Settings All results presented in our experiments are given in terms of parent-prediction accuracy on unla3Training, development, and test data in PDT already contains POS tags assigned by the ‘feature-based tagger’. beled dependency parsing. We ignore the parentpredictions of punctuation tokens for English, while we retain a</context>
<context position="33455" citStr="Suzuki and Isozaki, 2008" startWordPosition="5502" endWordPosition="5505">ning method introduced by (Blitzer et al., 2006), which is an extension of the method described by (Ando and Zhang, 558 2005). In particular, both methods use a two-stage approach; They first train generative models or auxiliary problems from unlabeled data, and then, they incorporate these trained models into a supervised learning algorithm as real valued features. Moreover, both methods make direct use of existing feature-vector definitions f(x, y) in inducing representations from unlabeled data. 7 Conclusion This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem. In addition, we have described extensions that incorporate the cluster-based features of Koo et al. (2008), and that allow the use of second-order parsing models. We have described experiments that show that the approach gives significant improvements over state-of-the-art methods for dependency parsing; performance improves when the amount of unlabeled data is increased from 43.8 million tokens to 3.72 billion tokens. The approach should be relatively easily applied to languages other than English or Czech. We stress that the SS-SCM approach requires relati</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>J. Suzuki and H. Isozaki. 2008. Semi-supervised Sequential Labeling and Segmentation Using GigaWord Scale Unlabeled Data. In Proc. of ACL-08: HLT, pages 665–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q I Wang</author>
<author>D Schuurmans</author>
<author>D Lin</author>
</authors>
<title>Semisupervised Convex Training for Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>532--540</pages>
<contexts>
<context position="1988" citStr="Wang et al., 2008" startWordPosition="284" endWordPosition="287">ent work has successfully developed dependency parsing models for many languages using supervised learning algorithms (Buchholz and Marsi, 2006; Nivre et al., 2007). Semi-supervised learning methods, which make use of unlabeled data in addition to labeled examples, have the potential to give improved performance over purely supervised methods for dependency parsing. It is often straightforward to obtain large amounts of unlabeled data, making semi-supervised approaches appealing; previous work on semisupervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008). In particular, Koo et al. (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our </context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2008</marker>
<rawString>Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semisupervised Convex Training for Dependency Parsing. In Proc. of ACL-08: HLT, pages 532–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="18777" citStr="Yamada and Matsumoto, 2003" startWordPosition="3120" endWordPosition="3124">rce of labeled data for English, and the Prague Dependency Treebank (PDT) 1.0 (Hajiˇc, 1998) for Czech. To facilitate comparisons with previous work, we used exactly the same training, development and test sets Table 2: Details of the larger unlabeled data set used in English dependency parsing: sentences exceeding 128 tokens in length were excluded for computational reasons. as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations. We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test. The Czech data sets were obtained from the predefined training/development/test partition in the PDT. The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing (BLLIP) Corpus (LDC2000T43)2, giving a total of 1,796,379 sentences and 43,380,315 tokens. The raw text section of the PDT was used for Czech, giving 2,349,224 sentences and 39,336,5</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In Proc. of IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>