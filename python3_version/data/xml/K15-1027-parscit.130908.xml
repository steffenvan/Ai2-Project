<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9949115">
Task-Oriented Learning of Word Embeddings
for Semantic Relation Classification
</title>
<author confidence="0.894197">
Kazuma Hashimoto†, Pontus Stenetorp‡, Makoto Miwa§, and Yoshimasa Tsuruoka††The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
</author>
<email confidence="0.792768">
{hassy,tsuruoka}@logos.t.u-tokyo.ac.jp
</email>
<affiliation confidence="0.733070333333333">
‡University College London, London, United Kingdom
pontus@stenetorp.se
§Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan
</affiliation>
<email confidence="0.860254">
makoto-miwa@toyota-ti.ac.jp
</email>
<sectionHeader confidence="0.990062" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970166666667">
We present a novel learning method for
word embeddings designed for relation
classification. Our word embeddings are
trained by predicting words between noun
pairs using lexical relation-specific fea-
tures on a large unlabeled corpus. This al-
lows us to explicitly incorporate relation-
specific information into the word embed-
dings. The learned word embeddings are
then used to construct feature vectors for
a relation classification model. On a well-
established semantic relation classification
task, our method significantly outperforms
a baseline based on a previously intro-
duced word embedding method, and com-
pares favorably to previous state-of-the-art
models that use syntactic information or
manually constructed external resources.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999831423076923">
Automatic classification of semantic relations has
a variety of applications, such as information ex-
traction and the construction of semantic net-
works (Girju et al., 2007; Hendrickx et al., 2010).
A traditional approach to relation classification is
to train classifiers using various kinds of features
with class labels annotated by humans. Carefully
crafted features derived from lexical, syntactic,
and semantic resources play a significant role in
achieving high accuracy for semantic relation clas-
sification (Rink and Harabagiu, 2010).
In recent years there has been an increasing in-
terest in using word embeddings as an alternative
to traditional hand-crafted features. Word embed-
dings are represented as real-valued vectors and
capture syntactic and semantic similarity between
words. For example, word2vec&apos; (Mikolov et al.,
2013b) is a well-established tool for learning word
embeddings. Although word2vec has successfully
been used to learn word embeddings, these kinds
of word embeddings capture only co-occurrence
relationships between words (Levy and Gold-
berg, 2014). While simply adding word embed-
dings trained using window-based contexts as ad-
ditional features to existing systems has proven
valuable (Turian et al., 2010), more recent studies
have focused on how to tune and enhance word
embeddings for specific tasks (Bansal et al., 2014;
Boros et al., 2014; Chen et al., 2014; Guo et al.,
2014; Nguyen and Grishman, 2014) and we con-
tinue this line of research for the task of relation
classification.
In this work we present a learning method for
word embeddings specifically designed to be use-
ful for relation classification. The overview of
our system and the embedding learning process
are shown in Figure 1. First we train word em-
beddings by predicting each of the words between
noun pairs using lexical relation-specific features
on a large unlabeled corpus. We then use the word
embeddings to construct lexical feature vectors for
relation classification. Lastly, the feature vectors
are used to train a relation classification model.
We evaluate our method on a well-established
semantic relation classification task and compare
it to a baseline based on word2vec embeddings
and previous state-of-the-art models that rely on
either manually crafted features, syntactic parses
or external semantic resources. Our method sig-
nificantly outperforms the word2vec-based base-
line, and compares favorably with previous state-
of-the-art models, despite relying only on lexi-
</bodyText>
<footnote confidence="0.54464">
lhttps://code.google.com/p/word2vec/.
</footnote>
<page confidence="0.890636">
268
</page>
<note confidence="0.9928955">
Proceedings of the 19th Conference on Computational Language Learning, pages 268–278,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.990329">
Figure 1: The overview of our system (a) and the embedding learning method (b). In the example
sentence, each of are, caused, and by is treated as a target word to be predicted during training.
</figureCaption>
<bodyText confidence="0.975566">
cal level features and no external annotated re-
sources. Furthermore, our qualitative analysis of
the learned embeddings shows that n-grams of our
embeddings capture salient syntactic patterns sim-
ilar to semantic relation types.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999976409090909">
A traditional approach to relation classification is
to train classifiers in a supervised fashion using a
variety of features. These features include lexical
bag-of-words features and features based on syn-
tactic parse trees. For syntactic parse trees, the
paths between the target entities on constituency
and dependency trees have been demonstrated to
be useful (Bunescu and Mooney, 2005; Zhang et
al., 2006). On the shared task introduced by Hen-
drickx et al. (2010), Rink and Harabagiu (2010)
achieved the best score using a variety of hand-
crafted features which were then used to train a
Support Vector Machine (SVM).
Recently, word embeddings have become popu-
lar as an alternative to hand-crafted features (Col-
lobert et al., 2011). However, one of the limita-
tions is that word embeddings are usually learned
by predicting a target word in its context, leading
to only local co-occurrence information being cap-
tured (Levy and Goldberg, 2014). Thus, several
recent studies have focused on overcoming this
limitation. Le and Mikolov (2014) integrated para-
graph information into a word2vec-based model,
which allowed them to capture paragraph-level in-
formation. For dependency parsing, Bansal et
al. (2014) and Chen et al. (2014) found ways to
improve performance by integrating dependency-
based context information into their embeddings.
Bansal et al. (2014) trained embeddings by defin-
ing parent and child nodes in dependency trees as
contexts. Chen et al. (2014) introduced the con-
cept of feature embeddings induced by parsing a
large unannotated corpus and then learning em-
beddings for the manually crafted features. For
information extraction, Boros et al. (2014) trained
word embeddings relevant for event role extrac-
tion, and Nguyen and Grishman (2014) employed
word embeddings for domain adaptation of rela-
tion extraction. Another kind of task-specific word
embeddings was proposed by Tang et al. (2014),
which used sentiment labels on tweets to adapt
word embeddings for a sentiment analysis tasks.
However, such an approach is only feasible when
a large amount of labeled data is available.
</bodyText>
<sectionHeader confidence="0.992458" genericHeader="method">
3 Relation Classification Using Word
Embedding-based Features
</sectionHeader>
<bodyText confidence="0.999875375">
We propose a novel method for learning word
embeddings designed for relation classification.
The word embeddings are trained by predicting
each word between noun pairs, given the corre-
sponding low-level features for relation classifi-
cation. In general, to classify relations between
pairs of nouns the most important features come
from the pairs themselves and the words between
and around the pairs (Hendrickx et al., 2010). For
example, in the sentence in Figure 1 (b) there is
a cause-effect relationship between the two nouns
conflicts and players. To classify the relation, the
most common features are the noun pair (conflicts,
players), the words between the noun pair (are,
caused, by), the words before the pair (the, exter-
nal), and the words after the pair (playing, tiles,
</bodyText>
<page confidence="0.996675">
269
</page>
<bodyText confidence="0.999828764705882">
to, ...). As shown by Rink and Harabagiu (2010),
the words between the noun pairs are the most ef-
fective among these features. Our main idea is to
treat the most important features (the words be-
tween the noun pairs) as the targets to be predicted
and other lexical features (noun pairs, words out-
side them) as their contexts. Due to this, we expect
our embeddings to capture relevant features for
relation classification better than previous models
which only use window-based contexts.
In this section we first describe the learning pro-
cess for the word embeddings, focusing on lexical
features for relation classification (Figure 1 (b)).
We then propose a simple and powerful technique
to construct features which serve as input for a
softmax classifier. The overview of our proposed
system is shown in Figure 1 (a).
</bodyText>
<subsectionHeader confidence="0.999525">
3.1 Learning Word Embeddings
</subsectionHeader>
<bodyText confidence="0.8970695">
Assume that there is a noun pair n = (n1, n2) in
a sentence with Min words between the pair and
Mout words before and after the pair:
in in
</bodyText>
<listItem confidence="0.859639333333333">
•
win = (w1 , ... , wMin) ,
• wbef = (wbef
1 , ... , wbef Mout) , and
aft aft
• waft = (w1 , ... , wMout) .
</listItem>
<bodyText confidence="0.998514041666667">
Our method predicts each target word win
i E win
using three kinds of information: n, words around
win
i in win, and words in wbef and waft. Words
are embedded in a d-dimensional vector space and
we refer to these vectors as word embeddings. To
discriminate between words in n from those in
win, wbef, and waft, we have two sets of word
embeddings: N E Rd×|N |and W E Rd×|W|. W
is a set of words and N is also a set of words but
contains only nouns. Hence, the word cause has
two embeddings: one in N and another in W. In
general cause is used as a noun and a verb, and
thus we expect the noun embeddings to capture
the meanings focusing on their noun usage. This
is inspired by some recent work on word represen-
tations that explicitly assigns an independent rep-
resentation for each word usage according to its
part-of-speech tag (Baroni and Zamparelli, 2010;
Grefenstette and Sadrzadeh, 2011; Hashimoto et
al., 2013; Hashimoto et al., 2014; Kartsaklis and
Sadrzadeh, 2013).
A feature vector f E R2d(2+c)×1 is constructed
</bodyText>
<equation confidence="0.955102333333333">
to predict win
i by concatenating word embeddings:
f = [N(n1); N(n2); W(win
i−1); ... ; W(win
i−c);
W(win
i+1);...; W(win
i+c);
1 Mout
W (wbef );
Mout
j=1
</equation>
<bodyText confidence="0.988775875">
N(•) and W(•) E Rd×1 corresponds to each word
and c is the context size. A special NULL token is
used if i − j is smaller than 1 or i + j is larger than
Min for each j E 11, 2, ... , c}.
Our method then estimates a conditional prob-
ability p(w|f) that the target word is a word w
given the feature vector f, using a logistic regres-
sion model:
</bodyText>
<equation confidence="0.9895">
p(w|f) = a( W(w) • f + b(w)) , (2)
1+e−x
</equation>
<bodyText confidence="0.9739256">
is the logistic function. Each column vector in
W E R2d(c+1)×|W |corresponds to a word. That
is, we assign a logistic regression model for each
word, and we can train the embeddings using the
one-versus-rest approach to make p(win
</bodyText>
<equation confidence="0.77261">
i |f) larger
than p(w′|f) for w′ =� win
</equation>
<bodyText confidence="0.998589818181818">
i . However, naively opti-
mizing the parameters of those logistic regression
models would lead to prohibitive computational
cost since it grows linearly with the size of the vo-
cabulary.
When training we employ several procedures
introduced by Mikolov et al. (2013b), namely,
negative sampling, a modified unigram noise dis-
tribution and subsampling. For negative sampling
the model parameters N, W, W, and b are learned
by maximizing the objective function Junlabeled:
</bodyText>
<equation confidence="0.777651">

log(1 − p(w′j|f)) ,
(3)
</equation>
<bodyText confidence="0.99991725">
where w′jis a word randomly drawn from the uni-
gram noise distribution weighted by an exponent
of 0.75. Maximizing Junlabeled means that our
method can discriminate between each target word
and k noise words given the target word’s context.
This approach is much less computationally ex-
pensive than the one-versus-rest approach and has
proven effective in learning word embeddings.
</bodyText>
<figure confidence="0.857394">
1 Mout∑
j=1
Mout
W(waft
j )� .
(1)
where �W(w) E R2d(2+c)×1 is a weight vector for
w, b(w) E R is a bias for w, and a(x) = 1
∑ Min∑  ∑k
n i=1 log(p(win j=1
i |f)) +
</figure>
<page confidence="0.968663">
270
</page>
<bodyText confidence="0.997864">
To reduce redundancy during training we use
subsampling. A training sample, whose tar-
</bodyText>
<equation confidence="0.861387333333333">
get word is w, is discarded with the probability
� t
Pd(w) = 1− p(w), where t is a threshold which
</equation>
<bodyText confidence="0.992285375">
is set to 10−5 and p(w) is a probability corre-
sponding to the frequency of w in the training cor-
pus. The more frequent a target word is, the more
likely it is to be discarded. To further emphasize
infrequent words, we apply the subsampling ap-
proach not only to target words, but also to noun
pairs; concretely, by drawing two random numbers
r1 and r2, a training sample whose noun pair is
</bodyText>
<equation confidence="0.681057">
(n1, n2) is discarded if Pd(n1) is larger than r1 or
Pd(n2) is larger than r2.
</equation>
<bodyText confidence="0.999964375">
Since the feature vector f is constructed as de-
fined in Eq. (1), at each training step, �W(w) is
updated based on information about what pair of
nouns surrounds w, what word n-grams appear in
a small window around w, and what words appear
outside the noun pair. Hence, the weight vector
�W(w) captures rich information regarding the tar-
get word w.
</bodyText>
<subsectionHeader confidence="0.999938">
3.2 Constructing Feature Vectors
</subsectionHeader>
<bodyText confidence="0.999415076923077">
Once the word embeddings are trained, we can use
them for relation classification. Given a noun pair
n = (n1, n2) with its context words win, wbef,
and waft, we construct a feature vector to classify
the relation between n1 and n2 by concatenating
three kinds of feature vectors:
gn the word embeddings of the noun pair,
gin the averaged n-gram embeddings between the
pair, and
gout the concatenation of the averaged word em-
beddings in wbef and waft.
The feature vector gn E R2d×1 is the concate-
nation of N(n1) and N(n2):
</bodyText>
<equation confidence="0.985106">
gn = [N(n1); N(n2)] . (4)
</equation>
<bodyText confidence="0.99994575">
Words between the noun pair contribute to clas-
sifying the relation, and one of the most common
ways to incorporate an arbitrary number of words
is treating them as a bag of words. However, word
order information is lost for bag-of-words features
such as averaged word embeddings. To incorpo-
rate the word order information, we first define n-
gram embeddings hi E R4d(1+c)×1 between the
</bodyText>
<equation confidence="0.804947333333333">
noun pair:
(5)
W(win
i+1); ... ; W(win
i+c); �W(win
i )] .
</equation>
<bodyText confidence="0.995558913043478">
Note that W can also be used and that the value
used for n is (2c+1). As described in Section 3.1,
W captures meaningful information about each
word and after the first embedding learning step
we can treat the embeddings in W as features for
the words. Mnih and Kavukcuoglu (2013) have
demonstrated that using embeddings like those in
W is useful in representing the words. We then
compute the feature vector gin by averaging hi:
hi . (6)
We use the averaging approach since Min depends
on each instance. The feature vector gin allows us
to represent word sequences of arbitrary lengths as
fixed-length feature vectors using the simple oper-
ations: concatenation and averaging.
The words before and after the noun pair are
sometimes important in classifying the relation.
For example, in the phrase “pour n1 into n2”, the
word pour should be helpful in classifying the re-
lation. As with Eq. (1), we use the concatenation
of the averaged word embeddings of words before
and after the noun pair to compute the feature vec-
torgout E R2d×1:
</bodyText>
<equation confidence="0.766368333333333">
W(waft
j )] .
(7)
</equation>
<bodyText confidence="0.999963444444444">
As described above, the overall feature vector
e E R4d(2+c)×1 is constructed by concatenating
gn, gin, and gout. We would like to emphasize
that we only use simple operations: averaging and
concatenating the learned word embeddings. The
feature vector e is then used as input for a soft-
max classifier, without any complex transforma-
tion such as matrix multiplication with non-linear
functions.
</bodyText>
<subsectionHeader confidence="0.999281">
3.3 Supervised Learning
</subsectionHeader>
<bodyText confidence="0.9999842">
Given a relation classification task we train a soft-
max classifier using the feature vector e described
in Section 3.2. For each k-th training sample with
a corresponding label lk among L predefined la-
bels, we compute a conditional probability given
</bodyText>
<equation confidence="0.987810857142857">
hi = [W(win i−1); ... ; W(win
i−c);
1 Min
gin = Min i=1
gout = 1 Mout W(wbef Mout
[ j=1 j ); j=1
Mout
</equation>
<page confidence="0.958621">
271
</page>
<bodyText confidence="0.959436">
its feature vector ek:
</bodyText>
<equation confidence="0.979091">
exp(o(lk))
p(lk|ek) = (8)
ELi=1 exp(o(i))
</equation>
<bodyText confidence="0.99897925">
where o E RL×1 is defined as o = Sek + s, and
S E RL×4d(2+c) and s E RL×1 are the softmax
parameters. o(i) is the i-th element of o. We then
define the objective function as:
</bodyText>
<equation confidence="0.8944795">
λ
log(p(lk|ek)) − 211θ112 . (9)
</equation>
<bodyText confidence="0.995385823529412">
K is the number of training samples and λ con-
trols the L-2 regularization. θ = (N, W, ˜W, S, s)
is the set of parameters and Jlabeled is maximized
using AdaGrad (Duchi et al., 2011). We have
found that dropout (Hinton et al., 2012) is help-
ful in preventing our model from overfitting. Con-
cretely, elements in e are randomly omitted with a
probability of 0.5 at each training step. Recently
dropout has been applied to deep neural network
models for natural language processing tasks and
proven effective (Irsoy and Cardie, 2014; Paulus
et al., 2014).
In what follows, we refer to the above method
as RelEmb. While RelEmb uses only low-level
features, a variety of useful features have been
proposed for relation classification. Among them,
we use dependency path features (Bunescu and
Mooney, 2005) based on the untyped binary de-
pendencies of the Stanford parser to find the short-
est path between target nouns. The dependency
path features are computed by averaging word em-
beddings from W on the shortest path, and are
then concatenated to the feature vector e. Fur-
thermore, we directly incorporate semantic infor-
mation using word-level semantic features from
Named Entity (NE) tags and WordNet hypernyms,
as used in previous work (Rink and Harabagiu,
2010; Socher et al., 2012; Yu et al., 2014). We
refer to this extended method as RelEmbFULL.
Concretely, RelEmbFULL uses the same binary
features as in Socher et al. (2012). The features
come from NE tags and WordNet hypernym tags
of target nouns provided by a sense tagger (Cia-
ramita and Altun, 2006).
</bodyText>
<sectionHeader confidence="0.998366" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<subsectionHeader confidence="0.990844">
4.1 Training Data
</subsectionHeader>
<bodyText confidence="0.920582">
For pre-training we used a snapshot of the En-
glish Wikipedia2 from November 2013. First,
</bodyText>
<footnote confidence="0.687253">
2http://dumps.wikimedia.org/enwiki/.
</footnote>
<bodyText confidence="0.999870714285714">
we extracted 80 million sentences from the orig-
inal Wikipedia file, and then used Enju3 (Miyao
and Tsujii, 2008) to automatically assign part-of-
speech (POS) tags. From the POS tags we used
NN, NNS, NNP, or NNPS to locate noun pairs in
the corpus. We then collected training data by list-
ing pairs of nouns and the words between, before,
and after the noun pairs. A noun pair was omit-
ted if the number of words between the pair was
larger than 10 and we consequently collected 1.4
billion pairs of nouns and their contexts 4. We used
the 300,000 most frequent words and the 300,000
most frequent nouns and treated out-of-vocabulary
words as a special UNK token.
</bodyText>
<subsectionHeader confidence="0.946511">
4.2 Initialization and Optimization
</subsectionHeader>
<bodyText confidence="0.999983681818182">
We initialized the embedding matrices N and W
with zero-mean gaussian noise with a variance of
W˜ and b were zero-initialized. The model pa-
rameters were optimized by maximizing the ob-
jective function in Eq. (3) using stochastic gradi-
ent ascent. The learning rate was set to α and lin-
early decreased to 0 during training, as described
in Mikolov et al. (2013a). The hyperparameters
are the embedding dimensionality d, the context
size c, the number of negative samples k, the initial
learning rate α, and Mout, the number of words
outside the noun pairs. For hyperparameter tun-
ing, we first fixed α to 0.025 and Mout to 5, and
then set d to 150, 100, 3001, c to 11, 2, 31, and
k to 15, 15, 251.
At the supervised learning step, we initialized S
and s with zeros. The hyperparameters, the learn-
ing rate for AdaGrad, λ, Mout, and the number of
iterations, were determined via 10-fold cross val-
idation on the training set for each setting. Note
that Mout can be tuned at the supervised learning
step, adapting to a specific dataset.
</bodyText>
<sectionHeader confidence="0.999717" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.996649">
5.1 Evaluation Dataset
</subsectionHeader>
<bodyText confidence="0.999835333333333">
We evaluated our method on the SemEval 2010
Task 8 data sets (Hendrickx et al., 2010), which
involves predicting the semantic relations between
</bodyText>
<footnote confidence="0.974663888888889">
3Despite Enju being a syntactic parser we only use the
POS tagger component. The accuracy of the POS tagger is
about 97.2% on the WSJ corpus.
4The training data, the training code, and the learned
model parameters used in this paper are publicly available at
http://www.logos.t.u-tokyo.ac.jp/˜hassy/
publications/conll2015/
5http://docs.google.com/View?docid=
dfvxd49s_36c28v9pmw.
</footnote>
<equation confidence="0.9167972">
K
Jlabeled =
k=1
1
d.
</equation>
<page confidence="0.984726">
272
</page>
<bodyText confidence="0.999396428571429">
noun pairs in their contexts. The dataset, contain-
ing 8,000 training and 2,717 test samples, defines
nine classes (Cause-Effect, Entity-Origin, etc.) for
ordered relations and one class (Other) for other
relations. Thus, the task can be treated as a 19-
class classification task. Two examples from the
training set are shown below.
</bodyText>
<listItem confidence="0.83339975">
(a) Financial [stress]E1 is one of the main causes
of [divorce]E2
(b) The [burst]E1 has been caused by water ham-
mer [pressure]E2
</listItem>
<bodyText confidence="0.9956585">
Training example (a) is classified as Cause-
Effect(E1, E2) which denotes that E2 is an effect
caused by E1, while training example (b) is classi-
fied as Cause-Effect(E2, E1) which is the inverse
of Cause-Effect(E1, E2). We report the official
macro-averaged F1 scores and accuracy.
</bodyText>
<subsectionHeader confidence="0.993168">
5.2 Models
</subsectionHeader>
<bodyText confidence="0.999951">
To empirically investigate the performance of our
proposed method we compared it to several base-
lines and previously proposed models.
</bodyText>
<subsectionHeader confidence="0.516718">
5.2.1 Random and word2vec Initialization
</subsectionHeader>
<bodyText confidence="0.999979863636364">
Rand-Init. The first baseline is RelEmb itself,
but without applying the learning method on the
unlabeled corpus. In other words, we train the
softmax classifier from Section 3.3 on the labeled
training data with randomly initialized model pa-
rameters.
W2V-Init. The second baseline is RelEmb us-
ing word embeddings learned by word2vec. More
specifically, we initialize the embedding matrices
N and W with the word2vec embeddings. Re-
lated to our method, word2vec has a set of weight
vectors similar to W when trained with negative
sampling and we use these weight vectors as a re-
placement for W. We trained the word2vec em-
beddings using the CBOW model with subsam-
pling on the full Wikipedia corpus. As with our
experimental settings, we fix the learning rate to
0.025, and investigate several hyperparameter set-
tings. For hyperparameter tuning we set the em-
bedding dimensionality d to {50, 100, 300}, the
context size c to {1, 3, 9}, and the number of neg-
ative samples k to {5, 15, 25}.
</bodyText>
<subsubsectionHeader confidence="0.507015">
5.2.2 SVM-Based Systems
</subsubsectionHeader>
<bodyText confidence="0.999817833333333">
A simple approach to the relation classification
task is to use SVMs with standard binary bag-
of-words features. The bag-of-words features in-
cluded the noun pairs and words between, before,
and after the pairs, and we used LIBLINEAR6 as
our classifier.
</bodyText>
<subsectionHeader confidence="0.579727">
5.2.3 Neural Network Models
</subsectionHeader>
<bodyText confidence="0.99993532">
Socher et al. (2012) used Recursive Neural Net-
work (RNN) models to classify the relations.
Subsequently, Ebrahimi and Dou (2015) and
Hashimoto et al. (2013) proposed RNN models to
better handle the relations. These methods rely on
syntactic parse trees.
Yu et al. (2014) introduced their novel Factor-
based Compositional Model (FCM) and presented
results from several model variants, the best per-
forming being FCMEMB and FCMFULL. The for-
mer only uses word embedding information and
the latter relies on dependency paths and NE fea-
tures, in addition to word embeddings.
Zeng et al. (2014) used a Convolutional Neu-
ral Network (CNN) with WordNet hypernyms.
Noteworthy in relation to the RNN-based meth-
ods, the CNN model does not rely on parse trees.
More recently, dos Santos et al. (2015) have in-
troduced CR-CNN by extending the CNN model
and achieved the best result to date. The key point
of CR-CNN is that it improves the classification
score by omitting the noisy class “Other” in the
dataset described in Section 5.1. We call CR-CNN
using the “Other” class CR-CNNOther and CR-
CNN omitting the class CR-CNNBeA.
</bodyText>
<subsectionHeader confidence="0.886266">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9999365">
The scores on the test set for SemEval 2010 Task 8
are shown in Table 1. RelEmb achieves 82.8% of
F1 which is better than those of almost all models
compared and comparable to that of the previous
state of the art, except for CR-CNNBe,t. Note that
RelEmb does not rely on external semantic fea-
tures and syntactic parse features7. Furthermore,
RelEmbFULL achieves 83.5% of F1. We calcu-
lated a confidence interval (82.0, 84.9) (p &lt; 0.05)
using bootstrap resampling (Noreen, 1989).
</bodyText>
<subsectionHeader confidence="0.529412">
5.3.1 Comparison with the Baselines
</subsectionHeader>
<bodyText confidence="0.8987805">
RelEmb significantly outperforms not only the
Rand-Init baseline, but also the W2V-Init baseline.
</bodyText>
<footnote confidence="0.9976818">
6http://www.csie.ntu.edu.tw/˜cjlin/
liblinear/.
7While we use a POS tagger to locate noun pairs, RelEmb
does not explicitly use POS features at the supervised learn-
ing step.
</footnote>
<page confidence="0.994445">
273
</page>
<table confidence="0.9981076">
Features for classifiers F1 / ACC (%)
RelEmbFULL embeddings, dependency paths, WordNet, NE 83.5 / 79.9
RelEmb embeddings 82.8 / 78.9
RelEmb (W2V-Init) embeddings 81.8 / 77.7
RelEmb (Rand-Init) embeddings 78.2 / 73.5
</table>
<note confidence="0.966714090909091">
SVM bag of words 76.5 / 72.0
SVM bag of words, POS, dependency paths, WordNet, 82.2 / 77.9
(Rink and Harabagiu, 2010) paraphrases, TextRunner, Google n-grams, etc.
CR-CNNBe3t (dos Santos et al., 2015) embeddings, word position embeddings 84.1 / n/a
FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.0 / n/a
CR-CNNOther (dos Santos et al., 2015) embeddings, word position embeddings 82.7 / n/a
CRNN (Ebrahimi and Dou, 2015) embeddings, parse trees, WordNet, NE, POS 82.7 / n/a
CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a
MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a
FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a
RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc. 79.4 / n/a
</note>
<tableCaption confidence="0.99889">
Table 1: Scores on the test set for SemEval 2010 Task 8.
</tableCaption>
<bodyText confidence="0.99986025">
These results show that our task-specific word em-
beddings are more useful than those trained using
window-based contexts. A point that we would
like to emphasize is that the baselines are un-
expectedly strong. As was noted by Wang and
Manning (2012), we should carefully implement
strong baselines and see whether complex models
can outperform these baselines.
</bodyText>
<subsectionHeader confidence="0.914327">
5.3.2 Comparison with SVM-Based Systems
</subsectionHeader>
<bodyText confidence="0.999992777777778">
RelEmb performs much better than the bag-of-
words-based SVM. This is not surprising given
that we use a large unannotated corpus and embed-
dings with a large number of parameters. RelEmb
also outperforms the SVM system of Rink and
Harabagiu (2010), which demonstrates the effec-
tiveness of our task-specific word embeddings, de-
spite our only requirement being a large unanno-
tated corpus and a POS tagger.
</bodyText>
<subsectionHeader confidence="0.9006755">
5.3.3 Comparison with Neural Network
Models
</subsectionHeader>
<bodyText confidence="0.999971473684211">
RelEmb outperforms the RNN models. In our pre-
liminary experiments, we have found some un-
desirable parse trees when computing vector rep-
resentations using RNN-based models and such
parsing errors might hamper the performance of
the RNN models.
FCMFULL, which relies on dependency paths
and NE features, achieves a better score than that
of RElEmb. Without such features, RelEmb out-
performs FCMEMB by a large margin. By incor-
porating external resources, RelEmbFULL outper-
forms FCMFULL.
RelEmb compares favorably to CR-CNNOther,
despite our method being less computationally ex-
pensive than CR-CNNOther. When classifying an
instance, the number of the floating number mul-
tiplications is 4d(2 + c)L in our method since
our method requires only one matrix-vector prod-
uct for the softmax classifier as described in Sec-
tion 3.3. c is the window size, d is the word
embedding dimensionality, and L is the number
of the classes. In CR-CNNOther, the number is
(Dc(d + 2d′)N + DL), where D is the dimen-
sionality of the convolution layer, d′ is the posi-
tion embedding dimensionality, and N is the av-
erage length of the input sentences. Here, we
omit the cost of the hyperbolic tangent function
in CR-CNNOther for simplicity. Using the best
hyperparameter settings, the number is roughly
3.8 × 104 in our method, and 1.6 × 107 in CR-
CNNOther assuming N is 10. dos Santos et al.
(2015) also boosted the score of CR-CNNOther
by omitting the noisy class “Other” by a ranking-
based classifier, and achieved the best score (CR-
CNNBeA). Our results may also be improved by
using the same technique, but the technique is
dataset-dependent, so we did not incorporate the
technique.
</bodyText>
<subsectionHeader confidence="0.999003">
5.4 Analysis on Training Settings
</subsectionHeader>
<bodyText confidence="0.9983455">
We perform analysis of the training procedure fo-
cusing on RelEmb.
</bodyText>
<subsectionHeader confidence="0.802968">
5.4.1 Effects of Tuning Hyperparameters
</subsectionHeader>
<bodyText confidence="0.9990355">
In Tables 2 and 3, we show how tuning the hyper-
parameters of our method and word2vec affects
</bodyText>
<page confidence="0.993979">
274
</page>
<table confidence="0.998129875">
c d k = 5 k = 15 k = 25
1 50 80.5 81.0 80.9
100 80.9 81.3 81.2
2 50 80.9 81.3 81.3
100 81.3 81.6 81.7
50 81.0 81.0 81.5
3 100 81.3 81.9 82.2
300 - - 82.0
</table>
<tableCaption confidence="0.847842">
Table 2: Cross-validation results for RelEmb.
</tableCaption>
<table confidence="0.9966565">
c d k = 5 k = 15 k = 25
50 80.5 80.7 80.9
1 100 81.1 81.2 81.0
300 81.2 81.3 81.2
3 50 80.4 80.7 80.8
100 81.0 81.0 80.9
9 50 80.0 79.8 80.2
100 80.3 80.4 80.1
</table>
<tableCaption confidence="0.999534">
Table 3: Cross-validation results for the W2V-Init.
</tableCaption>
<bodyText confidence="0.982648733333333">
the classification results using 10-fold cross vali-
dation on the training set. The same split is used
for each setting, so all results are comparable to
each other. The best settings for the cross vali-
dation are used to produce the results reported in
Table 1.
Table 2 shows F1 scores obtained by RelEmb.
The results for d = 50, 100 show that RelEmb
benefits from relatively large context sizes. The
n-gram embeddings in RelEmb capture richer in-
formation by setting c to 3 compared to setting c
to 1. Relatively large numbers of negative sam-
ples also slightly boost the scores. As opposed
to these trends, the score does not improve using
d = 300. We use the best setting (c = 3, d = 100,
k = 25) for the remaining analysis. We note that
RelEmbFULL achieves an F1-score of 82.5.
We also performed similar experiments for the
W2V-Init baseline, and the results are shown in
Table 3. In this case, the number of negative sam-
ples does not affect the scores, and the best score
is achieved by c = 1. As discussed in Bansal et al.
(2014), the small context size captures the syntac-
tic similarity between words rather than the top-
ical similarity. This result indicates that syntactic
similarity is more important than topical similarity
for this task. Compared to the word2vec embed-
dings, our embeddings capture not only local con-
text information using word order, but also long-
gn gin g′in gn, gin gn, gin, gout
</bodyText>
<note confidence="0.37247">
61.8 70.2 68.2 81.1 82.2
</note>
<tableCaption confidence="0.887536">
Table 4: Cross-validation results for ablation tests.
</tableCaption>
<table confidence="0.98086875">
Method Score
RelEmb N 0.690
RelEmb W 0.599
W2V-Init 0.687
</table>
<tableCaption confidence="0.999789">
Table 5: Evaluation on the WordSim-353 dataset.
</tableCaption>
<bodyText confidence="0.964713">
range co-occurrence information by being tailored
for the specific task.
</bodyText>
<subsectionHeader confidence="0.844455">
5.4.2 Ablation Tests
</subsectionHeader>
<bodyText confidence="0.985738">
As described in Section 3.2, we concatenate three
kinds of feature vectors, gn, gin, and gout, for
supervised learning. Table 4 shows classification
scores for ablation tests using 10-fold cross val-
idation. We also provide a score using a sim-
plified version of gin, where the feature vector
g′in is computed by averaging the word embed-
dings [W(w�n
</bodyText>
<equation confidence="0.87175">
� ); �W(w�n
</equation>
<bodyText confidence="0.999240875">
� )] of the words between
the noun pairs. This feature vector g′in then serves
as a bag-of-words feature.
Table 4 clearly shows that the averaged n-gram
embeddings contribute the most to the semantic
relation classification performance. The differ-
ence between the scores of gin and g′in shows the
effectiveness of our averaged n-gram embeddings.
</bodyText>
<subsectionHeader confidence="0.983283">
5.4.3 Effects of Dropout
</subsectionHeader>
<bodyText confidence="0.99996775">
At the supervised learning step we use dropout to
regularize our model. Without dropout, our per-
formance drops from 82.2% to 81.3% of F1 on the
training set using 10-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.808772">
5.4.4 Performance on a Word Similarity Task
</subsectionHeader>
<bodyText confidence="0.999902538461538">
As described in Section 3.1, we have the noun-
specific embeddings N as well as the standard
word embeddings W. We evaluated the learned
embeddings using a word-level semantic evalua-
tion task called WordSim-353 (Finkelstein et al.,
2001). This dataset consists of 353 pairs of nouns
and each pair has an averaged human rating which
corresponds to a semantic similarity score. Evalu-
ation is performed by measuring Spearman’s rank
correlation between the human ratings and the co-
sine similarity scores of the embeddings. Table 5
shows the evaluation results. We used the best set-
tings reported in Table 2 and 3 since our method
</bodyText>
<page confidence="0.993334">
275
</page>
<table confidence="0.854995166666667">
Cause-Effect(E1,E2) Content-Container(E1,E2) Message-Topic(E1,E2)
resulted poverty caused the inside was inside a discuss magazines relating to
caused stability caused the in was in a explaining to discuss aspects
generated coast resulted in hidden hidden in a discussing concerned about NULL
cause fire caused due was was inside the relating interview relates to
causes that resulted in stored was hidden in describing to discuss the
Cause-Effect(E2,E1) Content-Container(E2,E1) Message-Topic(E2,E1)
after caused by radiation full NULL full of subject were related in
from caused by infection included was full of related was related in
caused stomach caused by contains a full NULL discussed been discussed in
triggered caused by genetic contained a full and documented is related through
due anger caused by stored a full forty received the subject of
</table>
<tableCaption confidence="0.987015">
Table 6: Top five unigrams and trigrams with the highest scores for six classes.
</tableCaption>
<bodyText confidence="0.9999395">
is designed for relation classification and it is not
clear how to tune the hyperparameters for the word
similarity task. As shown in the result table, the
noun-specific embeddings perform better than the
standard embeddings in our method, which indi-
cates the noun-specific embeddings capture more
useful information in measuring the semantic sim-
ilarity between nouns. The performance of the
noun-specific embeddings is roughly the same as
that of the word2vec embeddings.
</bodyText>
<subsectionHeader confidence="0.99017">
5.5 Qualitative Analysis on the Embeddings
</subsectionHeader>
<bodyText confidence="0.9999720625">
Using the n-gram embeddings hi in Eq. (5), we in-
spect which n-grams are relevant to each relation
class after the supervised learning step of RelEmb.
When the context size c is 3, we can use at most
7-grams. The learned weight matrix S in Sec-
tion 3.3 is used to detect the most relevant n-grams
for each class. More specifically, for each n-gram
embedding (n = 1, 3) in the training set, we com-
pute the dot product between the n-gram embed-
ding and the corresponding components in S. We
then select the pairs of n-grams and class labels
with the highest scores. In Table 6 we show the top
five n-grams for six classes. These results clearly
show that the n-gram embeddings capture salient
syntactic patterns which are useful for the relation
classification task.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999755">
We have presented a method for learning word em-
beddings specifically designed for relation classi-
fication. The word embeddings are trained using
large unlabeled corpora to capture lexical features
for relation classification. On a well-established
semantic relation classification task our method
significantly outperforms the baseline based on
word2vec. Our method also compares favorably to
previous state-of-the-art models that rely on syn-
tactic parsers and external semantic resources, de-
spite our method requiring only access to an unan-
notated corpus and a POS tagger. For future work,
we will investigate how well our method performs
on other domains and datasets and how relation la-
bels can help when learning embeddings in a semi-
supervised learning setting.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994075">
We thank the anonymous reviewers for their help-
ful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999538580882353">
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring Continuous Word Representations
for Dependency Parsing. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
809–815.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are Vectors, Adjectives are Matrices: Representing
Adjective-Noun Constructions in Semantic Space.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183–1193.
Emanuela Boros, Romaric Besanc¸on, Olivier Ferret,
and Brigitte Grau. 2014. Event Role Extrac-
tion using Domain-Relevant Word Representations.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1852–1857.
Razvan Bunescu and Raymond Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Ex-
traction. In Proceedings of Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
724–731.
Wenliang Chen, Yue Zhang, and Min Zhang. 2014.
Feature Embedding for Dependency Parsing. In
Proceedings of COLING 2014, the 25th Interna-
tional Conference on Computational Linguistics:
Technical Papers, pages 816–826.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-Coverage Sense Disambiguation and Infor-
mation Extraction with a Supersense Sequence Tag-
ger. In Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Processing,
pages 594–602.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493–2537.
Cicero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying Relations by Ranking with
Convolutional Neural Networks. In Proceedings
of the Joint Conference of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing. to appear.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121–2159.
Javid Ebrahimi and Dejing Dou. 2015. Chain Based
RNN for Relation Classification. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1244–1249.
Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi,
Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-
pin Eytan. 2001. Placing Search in Context: The
Concept Revisited. In Proceedings of the Tenth In-
ternational World Wide Web Conference.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 13–18.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394–
1404.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting Embedding Features for Sim-
ple Semi-supervised Learning. In Proceedings of
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 110–120.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple Cus-
tomization of Recursive Neural Networks for Se-
mantic Relation Classification. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1372–1376.
Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,
and Yoshimasa Tsuruoka. 2014. Jointly Learning
Word Representations and Composition Functions
Using Predicate-Argument Structures. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1544–1555.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. SemEval-2010 Task 8:
Multi-Way Classification of Semantic Relations be-
tween Pairs of Nominals. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 33–38.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.
Ozan Irsoy and Claire Cardie. 2014. Deep Recursive
Neural Networks for Compositionality in Language.
In Advances in Neural Information Processing Sys-
tems 27, pages 2096–2104.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior Disambiguation of Word Tensors for Con-
structing Sentence Vectors. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1590–1601.
Quoc Le and Tomas Mikolov. 2014. Distributed
Representations of Sentences and Documents. In
Proceedings of the 31st International Conference
on Machine Learning (ICML-14), ICML ’14, pages
1188–1196.
Omer Levy and Yoav Goldberg. 2014. Neural Word
Embedding as Implicit Matrix Factorization. In Ad-
vances in Neural Information Processing Systems
27, pages 2177–2185.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at the International Conference on Learning
Representations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature For-
est Models for Probabilistic HPSG Parsing. Compu-
tational Linguistics, 34(1):35–80, March.
</reference>
<page confidence="0.959363">
277
</page>
<reference confidence="0.99968225">
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in Neural Information Pro-
cessing Systems 26, pages 2265–2273.
Thien Huu Nguyen and Ralph Grishman. 2014. Em-
ploying Word Representations and Regularization
for Domain Adaptation of Relation Extraction. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 68–74.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Romain Paulus, Richard Socher, and Christopher D
Manning. 2014. Global Belief Recursive Neural
Networks. In Advances in Neural Information Pro-
cessing Systems 27, pages 2888–2896.
Bryan Rink and Sanda Harabagiu. 2010. UTD: Clas-
sifying Semantic Relations by Combining Lexical
and Semantic Resources. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 256–259.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1201–1211.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentiment
Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1555–
1565.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384–394.
Sida Wang and Christopher Manning. 2012. Baselines
and Bigrams: Simple, Good Sentiment and Topic
Classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 90–94.
Mo Yu, Matthew R. Gormley, and Mark Dredze. 2014.
Factor-based Compositional Embedding Models. In
Proceedings of Workshop on Learning Semantics at
the 2014 Conference on Neural Information Pro-
cessing Systems.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation Classification via
Convolutional Deep Neural Network. In Proceed-
ings of COLING 2014, the 25th International Con-
ference on Computational Linguistics: Technical
Papers, pages 2335–2344.
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Fea-
tures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 825–832.
</reference>
<page confidence="0.996929">
278
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.297575">
<title confidence="0.9904435">Task-Oriented Learning of Word for Semantic Relation Classification</title>
<author confidence="0.721553">Pontus Makoto</author>
<author confidence="0.721553">Yoshimasa University of Tokyo</author>
<address confidence="0.519963">College London, London, United Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya,</address>
<email confidence="0.98231">makoto-miwa@toyota-ti.ac.jp</email>
<abstract confidence="0.999469684210526">We present a novel learning method for word embeddings designed for relation classification. Our word embeddings are trained by predicting words between noun pairs using lexical relation-specific features on a large unlabeled corpus. This allows us to explicitly incorporate relationspecific information into the word embeddings. The learned word embeddings are then used to construct feature vectors for a relation classification model. On a wellestablished semantic relation classification task, our method significantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models that use syntactic information or manually constructed external resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring Continuous Word Representations for Dependency Parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>809--815</pages>
<contexts>
<context position="2544" citStr="Bansal et al., 2014" startWordPosition="347" endWordPosition="350"> capture syntactic and semantic similarity between words. For example, word2vec&apos; (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for</context>
<context position="5532" citStr="Bansal et al. (2014)" startWordPosition="804" endWordPosition="807">in a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this limitation. Le and Mikolov (2014) integrated paragraph information into a word2vec-based model, which allowed them to capture paragraph-level information. For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependencybased context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation </context>
<context position="28770" citStr="Bansal et al. (2014)" startWordPosition="4808" endWordPosition="4811">dings in RelEmb capture richer information by setting c to 3 compared to setting c to 1. Relatively large numbers of negative samples also slightly boost the scores. As opposed to these trends, the score does not improve using d = 300. We use the best setting (c = 3, d = 100, k = 25) for the remaining analysis. We note that RelEmbFULL achieves an F1-score of 82.5. We also performed similar experiments for the W2V-Init baseline, and the results are shown in Table 3. In this case, the number of negative samples does not affect the scores, and the best score is achieved by c = 1. As discussed in Bansal et al. (2014), the small context size captures the syntactic similarity between words rather than the topical similarity. This result indicates that syntactic similarity is more important than topical similarity for this task. Compared to the word2vec embeddings, our embeddings capture not only local context information using word order, but also longgn gin g′in gn, gin gn, gin, gout 61.8 70.2 68.2 81.1 82.2 Table 4: Cross-validation results for ablation tests. Method Score RelEmb N 0.690 RelEmb W 0.599 W2V-Init 0.687 Table 5: Evaluation on the WordSim-353 dataset. range co-occurrence information by being </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring Continuous Word Representations for Dependency Parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 809–815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="9230" citStr="Baroni and Zamparelli, 2010" startWordPosition="1442" endWordPosition="1445">mbeddings. To discriminate between words in n from those in win, wbef, and waft, we have two sets of word embeddings: N E Rd×|N |and W E Rd×|W|. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). A feature vector f E R2d(2+c)×1 is constructed to predict win i by concatenating word embeddings: f = [N(n1); N(n2); W(win i−1); ... ; W(win i−c); W(win i+1);...; W(win i+c); 1 Mout W (wbef ); Mout j=1 N(•) and W(•) E Rd×1 corresponds to each word and c is the context size. A special NULL token is used if i − j is smaller than 1 or i + j is larger than Min for each j E 11, 2, ... , c}. Our method then estimates a conditional probability p(w|f) that the target word is a word w gi</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuela Boros</author>
<author>Romaric Besanc¸on</author>
<author>Olivier Ferret</author>
<author>Brigitte Grau</author>
</authors>
<title>Event Role Extraction using Domain-Relevant Word Representations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1852--1857</pages>
<marker>Boros, Besanc¸on, Ferret, Grau, 2014</marker>
<rawString>Emanuela Boros, Romaric Besanc¸on, Olivier Ferret, and Brigitte Grau. 2014. Event Role Extraction using Domain-Relevant Word Representations. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A Shortest Path Dependency Kernel for Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<contexts>
<context position="4714" citStr="Bunescu and Mooney, 2005" startWordPosition="674" endWordPosition="677">el features and no external annotated resources. Furthermore, our qualitative analysis of the learned embeddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-words features and features based on syntactic parse trees. For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused </context>
<context position="16170" citStr="Bunescu and Mooney, 2005" startWordPosition="2681" endWordPosition="2684">., 2011). We have found that dropout (Hinton et al., 2012) is helpful in preventing our model from overfitting. Concretely, elements in e are randomly omitted with a probability of 0.5 at each training step. Recently dropout has been applied to deep neural network models for natural language processing tasks and proven effective (Irsoy and Cardie, 2014; Paulus et al., 2014). In what follows, we refer to the above method as RelEmb. While RelEmb uses only low-level features, a variety of useful features have been proposed for relation classification. Among them, we use dependency path features (Bunescu and Mooney, 2005) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns. The dependency path features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL. Concretely, RelEmbFULL uses the same binary features </context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A Shortest Path Dependency Kernel for Relation Extraction. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Yue Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Feature Embedding for Dependency Parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>816--826</pages>
<contexts>
<context position="2583" citStr="Chen et al., 2014" startWordPosition="355" endWordPosition="358">y between words. For example, word2vec&apos; (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for relation classification. Lastly, the f</context>
<context position="5555" citStr="Chen et al. (2014)" startWordPosition="809" endWordPosition="812">ne (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this limitation. Le and Mikolov (2014) integrated paragraph information into a word2vec-based model, which allowed them to capture paragraph-level information. For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependencybased context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation of relation extraction.</context>
</contexts>
<marker>Chen, Zhang, Zhang, 2014</marker>
<rawString>Wenliang Chen, Yue Zhang, and Min Zhang. 2014. Feature Embedding for Dependency Parsing. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 816–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>594--602</pages>
<contexts>
<context position="16925" citStr="Ciaramita and Altun, 2006" startWordPosition="2807" endWordPosition="2811">ath features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL. Concretely, RelEmbFULL uses the same binary features as in Socher et al. (2012). The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger (Ciaramita and Altun, 2006). 4 Experimental Settings 4.1 Training Data For pre-training we used a snapshot of the English Wikipedia2 from November 2013. First, 2http://dumps.wikimedia.org/enwiki/. we extracted 80 million sentences from the original Wikipedia file, and then used Enju3 (Miyao and Tsujii, 2008) to automatically assign part-ofspeech (POS) tags. From the POS tags we used NN, NNS, NNP, or NNPS to locate noun pairs in the corpus. We then collected training data by listing pairs of nouns and the words between, before, and after the noun pairs. A noun pair was omitted if the number of words between the pair was </context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 594–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (Almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="5060" citStr="Collobert et al., 2011" startWordPosition="732" endWordPosition="736">ty of features. These features include lexical bag-of-words features and features based on syntactic parse trees. For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this limitation. Le and Mikolov (2014) integrated paragraph information into a word2vec-based model, which allowed them to capture paragraph-level information. For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependencybased context information into their embeddin</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero Nogueira dos Santos</author>
<author>Bing Xiang</author>
<author>Bowen Zhou</author>
</authors>
<title>Classifying Relations by Ranking with Convolutional Neural Networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the Joint Conference of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing.</booktitle>
<note>to appear.</note>
<contexts>
<context position="22409" citStr="Santos et al. (2015)" startWordPosition="3712" endWordPosition="3715"> RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL. The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rely on parse trees. More recently, dos Santos et al. (2015) have introduced CR-CNN by extending the CNN model and achieved the best result to date. The key point of CR-CNN is that it improves the classification score by omitting the noisy class “Other” in the dataset described in Section 5.1. We call CR-CNN using the “Other” class CR-CNNOther and CRCNN omitting the class CR-CNNBeA. 5.3 Results and Discussion The scores on the test set for SemEval 2010 Task 8 are shown in Table 1. RelEmb achieves 82.8% of F1 which is better than those of almost all models compared and comparable to that of the previous state of the art, except for CR-CNNBe,t. Note that</context>
<context position="23969" citStr="Santos et al., 2015" startWordPosition="3964" endWordPosition="3967">so the W2V-Init baseline. 6http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/. 7While we use a POS tagger to locate noun pairs, RelEmb does not explicitly use POS features at the supervised learning step. 273 Features for classifiers F1 / ACC (%) RelEmbFULL embeddings, dependency paths, WordNet, NE 83.5 / 79.9 RelEmb embeddings 82.8 / 78.9 RelEmb (W2V-Init) embeddings 81.8 / 77.7 RelEmb (Rand-Init) embeddings 78.2 / 73.5 SVM bag of words 76.5 / 72.0 SVM bag of words, POS, dependency paths, WordNet, 82.2 / 77.9 (Rink and Harabagiu, 2010) paraphrases, TextRunner, Google n-grams, etc. CR-CNNBe3t (dos Santos et al., 2015) embeddings, word position embeddings 84.1 / n/a FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.0 / n/a CR-CNNOther (dos Santos et al., 2015) embeddings, word position embeddings 82.7 / n/a CRNN (Ebrahimi and Dou, 2015) embeddings, parse trees, WordNet, NE, POS 82.7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc. 79.4 / n/a Table 1: Scores on the test set for SemE</context>
<context position="26801" citStr="Santos et al. (2015)" startWordPosition="4433" endWordPosition="4436">-vector product for the softmax classifier as described in Section 3.3. c is the window size, d is the word embedding dimensionality, and L is the number of the classes. In CR-CNNOther, the number is (Dc(d + 2d′)N + DL), where D is the dimensionality of the convolution layer, d′ is the position embedding dimensionality, and N is the average length of the input sentences. Here, we omit the cost of the hyperbolic tangent function in CR-CNNOther for simplicity. Using the best hyperparameter settings, the number is roughly 3.8 × 104 in our method, and 1.6 × 107 in CRCNNOther assuming N is 10. dos Santos et al. (2015) also boosted the score of CR-CNNOther by omitting the noisy class “Other” by a rankingbased classifier, and achieved the best score (CRCNNBeA). Our results may also be improved by using the same technique, but the technique is dataset-dependent, so we did not incorporate the technique. 5.4 Analysis on Training Settings We perform analysis of the training procedure focusing on RelEmb. 5.4.1 Effects of Tuning Hyperparameters In Tables 2 and 3, we show how tuning the hyperparameters of our method and word2vec affects 274 c d k = 5 k = 15 k = 25 1 50 80.5 81.0 80.9 100 80.9 81.3 81.2 2 50 80.9 81</context>
</contexts>
<marker>Santos, Xiang, Zhou, 2015</marker>
<rawString>Cicero Nogueira dos Santos, Bing Xiang, and Bowen Zhou. 2015. Classifying Relations by Ranking with Convolutional Neural Networks. In Proceedings of the Joint Conference of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="15553" citStr="Duchi et al., 2011" startWordPosition="2581" endWordPosition="2584">edefined labels, we compute a conditional probability given hi = [W(win i−1); ... ; W(win i−c); 1 Min gin = Min i=1 gout = 1 Mout W(wbef Mout [ j=1 j ); j=1 Mout 271 its feature vector ek: exp(o(lk)) p(lk|ek) = (8) ELi=1 exp(o(i)) where o E RL×1 is defined as o = Sek + s, and S E RL×4d(2+c) and s E RL×1 are the softmax parameters. o(i) is the i-th element of o. We then define the objective function as: λ log(p(lk|ek)) − 211θ112 . (9) K is the number of training samples and λ controls the L-2 regularization. θ = (N, W, ˜W, S, s) is the set of parameters and Jlabeled is maximized using AdaGrad (Duchi et al., 2011). We have found that dropout (Hinton et al., 2012) is helpful in preventing our model from overfitting. Concretely, elements in e are randomly omitted with a probability of 0.5 at each training step. Recently dropout has been applied to deep neural network models for natural language processing tasks and proven effective (Irsoy and Cardie, 2014; Paulus et al., 2014). In what follows, we refer to the above method as RelEmb. While RelEmb uses only low-level features, a variety of useful features have been proposed for relation classification. Among them, we use dependency path features (Bunescu </context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javid Ebrahimi</author>
<author>Dejing Dou</author>
</authors>
<title>Chain Based RNN for Relation Classification.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1244--1249</pages>
<contexts>
<context position="21752" citStr="Ebrahimi and Dou (2015)" startWordPosition="3603" endWordPosition="3606">erparameter settings. For hyperparameter tuning we set the embedding dimensionality d to {50, 100, 300}, the context size c to {1, 3, 9}, and the number of negative samples k to {5, 15, 25}. 5.2.2 SVM-Based Systems A simple approach to the relation classification task is to use SVMs with standard binary bagof-words features. The bag-of-words features included the noun pairs and words between, before, and after the pairs, and we used LIBLINEAR6 as our classifier. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL. The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rel</context>
<context position="24203" citStr="Ebrahimi and Dou, 2015" startWordPosition="4001" endWordPosition="4004"> / ACC (%) RelEmbFULL embeddings, dependency paths, WordNet, NE 83.5 / 79.9 RelEmb embeddings 82.8 / 78.9 RelEmb (W2V-Init) embeddings 81.8 / 77.7 RelEmb (Rand-Init) embeddings 78.2 / 73.5 SVM bag of words 76.5 / 72.0 SVM bag of words, POS, dependency paths, WordNet, 82.2 / 77.9 (Rink and Harabagiu, 2010) paraphrases, TextRunner, Google n-grams, etc. CR-CNNBe3t (dos Santos et al., 2015) embeddings, word position embeddings 84.1 / n/a FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.0 / n/a CR-CNNOther (dos Santos et al., 2015) embeddings, word position embeddings 82.7 / n/a CRNN (Ebrahimi and Dou, 2015) embeddings, parse trees, WordNet, NE, POS 82.7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc. 79.4 / n/a Table 1: Scores on the test set for SemEval 2010 Task 8. These results show that our task-specific word embeddings are more useful than those trained using window-based contexts. A point that we would like to emphasize is that the baselines are unexpectedly strong. As was n</context>
</contexts>
<marker>Ebrahimi, Dou, 2015</marker>
<rawString>Javid Ebrahimi and Dejing Dou. 2015. Chain Based RNN for Relation Classification. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1244–1249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Gabrilovich Evgenly</author>
<author>Matias Yossi</author>
<author>Rivlin Ehud</author>
<author>Solan Zach</author>
<author>Wolfman Gadi</author>
<author>Ruppin Eytan</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth International World Wide Web Conference.</booktitle>
<contexts>
<context position="30622" citStr="Finkelstein et al., 2001" startWordPosition="5108" endWordPosition="5111">ification performance. The difference between the scores of gin and g′in shows the effectiveness of our averaged n-gram embeddings. 5.4.3 Effects of Dropout At the supervised learning step we use dropout to regularize our model. Without dropout, our performance drops from 82.2% to 81.3% of F1 on the training set using 10-fold cross validation. 5.4.4 Performance on a Word Similarity Task As described in Section 3.1, we have the nounspecific embeddings N as well as the standard word embeddings W. We evaluated the learned embeddings using a word-level semantic evaluation task called WordSim-353 (Finkelstein et al., 2001). This dataset consists of 353 pairs of nouns and each pair has an averaged human rating which corresponds to a semantic similarity score. Evaluation is performed by measuring Spearman’s rank correlation between the human ratings and the cosine similarity scores of the embeddings. Table 5 shows the evaluation results. We used the best settings reported in Table 2 and 3 since our method 275 Cause-Effect(E1,E2) Content-Container(E1,E2) Message-Topic(E1,E2) resulted poverty caused the inside was inside a discuss magazines relating to caused stability caused the in was in a explaining to discuss a</context>
</contexts>
<marker>Finkelstein, Evgenly, Yossi, Ehud, Zach, Gadi, Eytan, 2001</marker>
<rawString>Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi, Rivlin Ehud, Solan Zach, Wolfman Gadi, and Ruppin Eytan. 2001. Placing Search in Context: The Concept Revisited. In Proceedings of the Tenth International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>SemEval-2007 Task 04: Classification of Semantic Relations between Nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>13--18</pages>
<contexts>
<context position="1361" citStr="Girju et al., 2007" startWordPosition="173" endWordPosition="176"> into the word embeddings. The learned word embeddings are then used to construct feature vectors for a relation classification model. On a wellestablished semantic relation classification task, our method significantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models that use syntactic information or manually constructed external resources. 1 Introduction Automatic classification of semantic relations has a variety of applications, such as information extraction and the construction of semantic networks (Girju et al., 2007; Hendrickx et al., 2010). A traditional approach to relation classification is to train classifiers using various kinds of features with class labels annotated by humans. Carefully crafted features derived from lexical, syntactic, and semantic resources play a significant role in achieving high accuracy for semantic relation classification (Rink and Harabagiu, 2010). In recent years there has been an increasing interest in using word embeddings as an alternative to traditional hand-crafted features. Word embeddings are represented as real-valued vectors and capture syntactic and semantic simi</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. SemEval-2007 Task 04: Classification of Semantic Relations between Nominals. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental Support for a Categorical Compositional Distributional Model of Meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<contexts>
<context position="9264" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1446" endWordPosition="1449">tween words in n from those in win, wbef, and waft, we have two sets of word embeddings: N E Rd×|N |and W E Rd×|W|. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). A feature vector f E R2d(2+c)×1 is constructed to predict win i by concatenating word embeddings: f = [N(n1); N(n2); W(win i−1); ... ; W(win i−c); W(win i+1);...; W(win i+c); 1 Mout W (wbef ); Mout j=1 N(•) and W(•) E Rd×1 corresponds to each word and c is the context size. A special NULL token is used if i − j is smaller than 1 or i + j is larger than Min for each j E 11, 2, ... , c}. Our method then estimates a conditional probability p(w|f) that the target word is a word w given the feature vector f, using a </context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental Support for a Categorical Compositional Distributional Model of Meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394– 1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting Embedding Features for Simple Semi-supervised Learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>110--120</pages>
<contexts>
<context position="2601" citStr="Guo et al., 2014" startWordPosition="359" endWordPosition="362">r example, word2vec&apos; (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for relation classification. Lastly, the feature vectors are</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting Embedding Features for Simple Semi-supervised Learning. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuma Hashimoto</author>
<author>Makoto Miwa</author>
<author>Yoshimasa Tsuruoka</author>
<author>Takashi Chikayama</author>
</authors>
<title>Simple Customization of Recursive Neural Networks for Semantic Relation Classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1372--1376</pages>
<contexts>
<context position="9288" citStr="Hashimoto et al., 2013" startWordPosition="1450" endWordPosition="1453">, wbef, and waft, we have two sets of word embeddings: N E Rd×|N |and W E Rd×|W|. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). A feature vector f E R2d(2+c)×1 is constructed to predict win i by concatenating word embeddings: f = [N(n1); N(n2); W(win i−1); ... ; W(win i−c); W(win i+1);...; W(win i+c); 1 Mout W (wbef ); Mout j=1 N(•) and W(•) E Rd×1 corresponds to each word and c is the context size. A special NULL token is used if i − j is smaller than 1 or i + j is larger than Min for each j E 11, 2, ... , c}. Our method then estimates a conditional probability p(w|f) that the target word is a word w given the feature vector f, using a logistic regression mode</context>
<context position="21780" citStr="Hashimoto et al. (2013)" startWordPosition="3608" endWordPosition="3611">perparameter tuning we set the embedding dimensionality d to {50, 100, 300}, the context size c to {1, 3, 9}, and the number of negative samples k to {5, 15, 25}. 5.2.2 SVM-Based Systems A simple approach to the relation classification task is to use SVMs with standard binary bagof-words features. The bag-of-words features included the noun pairs and words between, before, and after the pairs, and we used LIBLINEAR6 as our classifier. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL. The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rely on parse trees. More recen</context>
<context position="24468" citStr="Hashimoto et al., 2013" startWordPosition="4048" endWordPosition="4051">t, 82.2 / 77.9 (Rink and Harabagiu, 2010) paraphrases, TextRunner, Google n-grams, etc. CR-CNNBe3t (dos Santos et al., 2015) embeddings, word position embeddings 84.1 / n/a FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.0 / n/a CR-CNNOther (dos Santos et al., 2015) embeddings, word position embeddings 82.7 / n/a CRNN (Ebrahimi and Dou, 2015) embeddings, parse trees, WordNet, NE, POS 82.7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc. 79.4 / n/a Table 1: Scores on the test set for SemEval 2010 Task 8. These results show that our task-specific word embeddings are more useful than those trained using window-based contexts. A point that we would like to emphasize is that the baselines are unexpectedly strong. As was noted by Wang and Manning (2012), we should carefully implement strong baselines and see whether complex models can outperform these baselines. 5.3.2 Comparison with SVM-Based Systems RelEmb performs much better than the bag-ofwords-based SVM. This is not surprising</context>
</contexts>
<marker>Hashimoto, Miwa, Tsuruoka, Chikayama, 2013</marker>
<rawString>Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple Customization of Recursive Neural Networks for Semantic Relation Classification. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372–1376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuma Hashimoto</author>
<author>Pontus Stenetorp</author>
<author>Makoto Miwa</author>
<author>Yoshimasa Tsuruoka</author>
</authors>
<title>Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1544--1555</pages>
<contexts>
<context position="9312" citStr="Hashimoto et al., 2014" startWordPosition="1454" endWordPosition="1457">e two sets of word embeddings: N E Rd×|N |and W E Rd×|W|. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). A feature vector f E R2d(2+c)×1 is constructed to predict win i by concatenating word embeddings: f = [N(n1); N(n2); W(win i−1); ... ; W(win i−c); W(win i+1);...; W(win i+c); 1 Mout W (wbef ); Mout j=1 N(•) and W(•) E Rd×1 corresponds to each word and c is the context size. A special NULL token is used if i − j is smaller than 1 or i + j is larger than Min for each j E 11, 2, ... , c}. Our method then estimates a conditional probability p(w|f) that the target word is a word w given the feature vector f, using a logistic regression model: p(w|f) = a( W(w) • f </context>
</contexts>
<marker>Hashimoto, Stenetorp, Miwa, Tsuruoka, 2014</marker>
<rawString>Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa, and Yoshimasa Tsuruoka. 2014. Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1544–1555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>33--38</pages>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2012</date>
<location>CoRR, abs/1207.0580.</location>
<marker>Hinton, Srivastava, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep Recursive Neural Networks for Compositionality in Language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="15899" citStr="Irsoy and Cardie, 2014" startWordPosition="2638" endWordPosition="2641">e i-th element of o. We then define the objective function as: λ log(p(lk|ek)) − 211θ112 . (9) K is the number of training samples and λ controls the L-2 regularization. θ = (N, W, ˜W, S, s) is the set of parameters and Jlabeled is maximized using AdaGrad (Duchi et al., 2011). We have found that dropout (Hinton et al., 2012) is helpful in preventing our model from overfitting. Concretely, elements in e are randomly omitted with a probability of 0.5 at each training step. Recently dropout has been applied to deep neural network models for natural language processing tasks and proven effective (Irsoy and Cardie, 2014; Paulus et al., 2014). In what follows, we refer to the above method as RelEmb. While RelEmb uses only low-level features, a variety of useful features have been proposed for relation classification. Among them, we use dependency path features (Bunescu and Mooney, 2005) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns. The dependency path features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using w</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep Recursive Neural Networks for Compositionality in Language. In Advances in Neural Information Processing Systems 27, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Prior Disambiguation of Word Tensors for Constructing Sentence Vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1590--1601</pages>
<contexts>
<context position="9345" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="1458" endWordPosition="1461">dings: N E Rd×|N |and W E Rd×|W|. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). A feature vector f E R2d(2+c)×1 is constructed to predict win i by concatenating word embeddings: f = [N(n1); N(n2); W(win i−1); ... ; W(win i−c); W(win i+1);...; W(win i+c); 1 Mout W (wbef ); Mout j=1 N(•) and W(•) E Rd×1 corresponds to each word and c is the context size. A special NULL token is used if i − j is smaller than 1 or i + j is larger than Min for each j E 11, 2, ... , c}. Our method then estimates a conditional probability p(w|f) that the target word is a word w given the feature vector f, using a logistic regression model: p(w|f) = a( W(w) • f + b(w)) , (2) 1+e−x is the logist</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, 2013</marker>
<rawString>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013. Prior Disambiguation of Word Tensors for Constructing Sentence Vectors. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590–1601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed Representations of Sentences and Documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML-14), ICML ’14,</booktitle>
<pages>1188--1196</pages>
<contexts>
<context position="5366" citStr="Mikolov (2014)" startWordPosition="783" endWordPosition="784">ask introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this limitation. Le and Mikolov (2014) integrated paragraph information into a word2vec-based model, which allowed them to capture paragraph-level information. For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependencybased context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extr</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), ICML ’14, pages 1188–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural Word Embedding as Implicit Matrix Factorization.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="2269" citStr="Levy and Goldberg, 2014" startWordPosition="302" endWordPosition="306">hieving high accuracy for semantic relation classification (Rink and Harabagiu, 2010). In recent years there has been an increasing interest in using word embeddings as an alternative to traditional hand-crafted features. Word embeddings are represented as real-valued vectors and capture syntactic and semantic similarity between words. For example, word2vec&apos; (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the</context>
<context position="5270" citStr="Levy and Goldberg, 2014" startWordPosition="767" endWordPosition="770"> trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this limitation. Le and Mikolov (2014) integrated paragraph information into a word2vec-based model, which allowed them to capture paragraph-level information. For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependencybased context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannot</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding as Implicit Matrix Factorization. In Advances in Neural Information Processing Systems 27, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at the International Conference on Learning Representations.</booktitle>
<contexts>
<context position="2027" citStr="Mikolov et al., 2013" startWordPosition="269" endWordPosition="272">ach to relation classification is to train classifiers using various kinds of features with class labels annotated by humans. Carefully crafted features derived from lexical, syntactic, and semantic resources play a significant role in achieving high accuracy for semantic relation classification (Rink and Harabagiu, 2010). In recent years there has been an increasing interest in using word embeddings as an alternative to traditional hand-crafted features. Word embeddings are represented as real-valued vectors and capture syntactic and semantic similarity between words. For example, word2vec&apos; (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 201</context>
<context position="10465" citStr="Mikolov et al. (2013" startWordPosition="1670" endWordPosition="1673"> vector f, using a logistic regression model: p(w|f) = a( W(w) • f + b(w)) , (2) 1+e−x is the logistic function. Each column vector in W E R2d(c+1)×|W |corresponds to a word. That is, we assign a logistic regression model for each word, and we can train the embeddings using the one-versus-rest approach to make p(win i |f) larger than p(w′|f) for w′ =� win i . However, naively optimizing the parameters of those logistic regression models would lead to prohibitive computational cost since it grows linearly with the size of the vocabulary. When training we employ several procedures introduced by Mikolov et al. (2013b), namely, negative sampling, a modified unigram noise distribution and subsampling. For negative sampling the model parameters N, W, W, and b are learned by maximizing the objective function Junlabeled:  log(1 − p(w′j|f)) , (3) where w′jis a word randomly drawn from the unigram noise distribution weighted by an exponent of 0.75. Maximizing Junlabeled means that our method can discriminate between each target word and k noise words given the target word’s context. This approach is much less computationally expensive than the one-versus-rest approach and has proven effective in learning word</context>
<context position="18148" citStr="Mikolov et al. (2013" startWordPosition="3015" endWordPosition="3018">ger than 10 and we consequently collected 1.4 billion pairs of nouns and their contexts 4. We used the 300,000 most frequent words and the 300,000 most frequent nouns and treated out-of-vocabulary words as a special UNK token. 4.2 Initialization and Optimization We initialized the embedding matrices N and W with zero-mean gaussian noise with a variance of W˜ and b were zero-initialized. The model parameters were optimized by maximizing the objective function in Eq. (3) using stochastic gradient ascent. The learning rate was set to α and linearly decreased to 0 during training, as described in Mikolov et al. (2013a). The hyperparameters are the embedding dimensionality d, the context size c, the number of negative samples k, the initial learning rate α, and Mout, the number of words outside the noun pairs. For hyperparameter tuning, we first fixed α to 0.025 and Mout to 5, and then set d to 150, 100, 3001, c to 11, 2, 31, and k to 15, 15, 251. At the supervised learning step, we initialized S and s with zeros. The hyperparameters, the learning rate for AdaGrad, λ, Mout, and the number of iterations, were determined via 10-fold cross validation on the training set for each setting. Note that Mout can be</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at the International Conference on Learning Representations.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<booktitle>2013b. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature Forest Models for Probabilistic HPSG Parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="17207" citStr="Miyao and Tsujii, 2008" startWordPosition="2849" endWordPosition="2852">d in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL. Concretely, RelEmbFULL uses the same binary features as in Socher et al. (2012). The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger (Ciaramita and Altun, 2006). 4 Experimental Settings 4.1 Training Data For pre-training we used a snapshot of the English Wikipedia2 from November 2013. First, 2http://dumps.wikimedia.org/enwiki/. we extracted 80 million sentences from the original Wikipedia file, and then used Enju3 (Miyao and Tsujii, 2008) to automatically assign part-ofspeech (POS) tags. From the POS tags we used NN, NNS, NNP, or NNPS to locate noun pairs in the corpus. We then collected training data by listing pairs of nouns and the words between, before, and after the noun pairs. A noun pair was omitted if the number of words between the pair was larger than 10 and we consequently collected 1.4 billion pairs of nouns and their contexts 4. We used the 300,000 most frequent words and the 300,000 most frequent nouns and treated out-of-vocabulary words as a special UNK token. 4.2 Initialization and Optimization We initialized t</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. Computational Linguistics, 34(1):35–80, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>2265--2273</pages>
<contexts>
<context position="13553" citStr="Mnih and Kavukcuoglu (2013)" startWordPosition="2225" endWordPosition="2228">ays to incorporate an arbitrary number of words is treating them as a bag of words. However, word order information is lost for bag-of-words features such as averaged word embeddings. To incorporate the word order information, we first define ngram embeddings hi E R4d(1+c)×1 between the noun pair: (5) W(win i+1); ... ; W(win i+c); �W(win i )] . Note that W can also be used and that the value used for n is (2c+1). As described in Section 3.1, W captures meaningful information about each word and after the first embedding learning step we can treat the embeddings in W as features for the words. Mnih and Kavukcuoglu (2013) have demonstrated that using embeddings like those in W is useful in representing the words. We then compute the feature vector gin by averaging hi: hi . (6) We use the averaging approach since Min depends on each instance. The feature vector gin allows us to represent word sequences of arbitrary lengths as fixed-length feature vectors using the simple operations: concatenation and averaging. The words before and after the noun pair are sometimes important in classifying the relation. For example, in the phrase “pour n1 into n2”, the word pour should be helpful in classifying the relation. As</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in Neural Information Processing Systems 26, pages 2265–2273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thien Huu Nguyen</author>
<author>Ralph Grishman</author>
</authors>
<title>Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>68--74</pages>
<contexts>
<context position="2629" citStr="Nguyen and Grishman, 2014" startWordPosition="363" endWordPosition="366">c&apos; (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for relation classification. Lastly, the feature vectors are used to train a relation cl</context>
<context position="6084" citStr="Nguyen and Grishman (2014)" startWordPosition="890" endWordPosition="893"> paragraph-level information. For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependencybased context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation of relation extraction. Another kind of task-specific word embeddings was proposed by Tang et al. (2014), which used sentiment labels on tweets to adapt word embeddings for a sentiment analysis tasks. However, such an approach is only feasible when a large amount of labeled data is available. 3 Relation Classification Using Word Embedding-based Features We propose a novel method for learning word embeddings designed for relation classification. The word embeddings are trained by predicting each word between noun pairs, given the corresponding low</context>
</contexts>
<marker>Nguyen, Grishman, 2014</marker>
<rawString>Thien Huu Nguyen and Ralph Grishman. 2014. Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>WileyInterscience.</publisher>
<contexts>
<context position="23239" citStr="Noreen, 1989" startWordPosition="3856" endWordPosition="3857">d in Section 5.1. We call CR-CNN using the “Other” class CR-CNNOther and CRCNN omitting the class CR-CNNBeA. 5.3 Results and Discussion The scores on the test set for SemEval 2010 Task 8 are shown in Table 1. RelEmb achieves 82.8% of F1 which is better than those of almost all models compared and comparable to that of the previous state of the art, except for CR-CNNBe,t. Note that RelEmb does not rely on external semantic features and syntactic parse features7. Furthermore, RelEmbFULL achieves 83.5% of F1. We calculated a confidence interval (82.0, 84.9) (p &lt; 0.05) using bootstrap resampling (Noreen, 1989). 5.3.1 Comparison with the Baselines RelEmb significantly outperforms not only the Rand-Init baseline, but also the W2V-Init baseline. 6http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/. 7While we use a POS tagger to locate noun pairs, RelEmb does not explicitly use POS features at the supervised learning step. 273 Features for classifiers F1 / ACC (%) RelEmbFULL embeddings, dependency paths, WordNet, NE 83.5 / 79.9 RelEmb embeddings 82.8 / 78.9 RelEmb (W2V-Init) embeddings 81.8 / 77.7 RelEmb (Rand-Init) embeddings 78.2 / 73.5 SVM bag of words 76.5 / 72.0 SVM bag of words, POS, dependency paths, </context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Romain Paulus</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Global Belief Recursive Neural Networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>2888--2896</pages>
<contexts>
<context position="15921" citStr="Paulus et al., 2014" startWordPosition="2642" endWordPosition="2645">then define the objective function as: λ log(p(lk|ek)) − 211θ112 . (9) K is the number of training samples and λ controls the L-2 regularization. θ = (N, W, ˜W, S, s) is the set of parameters and Jlabeled is maximized using AdaGrad (Duchi et al., 2011). We have found that dropout (Hinton et al., 2012) is helpful in preventing our model from overfitting. Concretely, elements in e are randomly omitted with a probability of 0.5 at each training step. Recently dropout has been applied to deep neural network models for natural language processing tasks and proven effective (Irsoy and Cardie, 2014; Paulus et al., 2014). In what follows, we refer to the above method as RelEmb. While RelEmb uses only low-level features, a variety of useful features have been proposed for relation classification. Among them, we use dependency path features (Bunescu and Mooney, 2005) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns. The dependency path features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using word-level semantic fea</context>
</contexts>
<marker>Paulus, Socher, Manning, 2014</marker>
<rawString>Romain Paulus, Richard Socher, and Christopher D Manning. 2014. Global Belief Recursive Neural Networks. In Advances in Neural Information Processing Systems 27, pages 2888–2896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Rink</author>
<author>Sanda Harabagiu</author>
</authors>
<title>UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>256--259</pages>
<contexts>
<context position="1730" citStr="Rink and Harabagiu, 2010" startWordPosition="225" endWordPosition="228"> use syntactic information or manually constructed external resources. 1 Introduction Automatic classification of semantic relations has a variety of applications, such as information extraction and the construction of semantic networks (Girju et al., 2007; Hendrickx et al., 2010). A traditional approach to relation classification is to train classifiers using various kinds of features with class labels annotated by humans. Carefully crafted features derived from lexical, syntactic, and semantic resources play a significant role in achieving high accuracy for semantic relation classification (Rink and Harabagiu, 2010). In recent years there has been an increasing interest in using word embeddings as an alternative to traditional hand-crafted features. Word embeddings are represented as real-valued vectors and capture syntactic and semantic similarity between words. For example, word2vec&apos; (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-ba</context>
<context position="4820" citStr="Rink and Harabagiu (2010)" startWordPosition="693" endWordPosition="696">eddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-words features and features based on syntactic parse trees. For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this limitation. Le and Mikolov (2014) integrated paragraph information into a word2vec-base</context>
<context position="7323" citStr="Rink and Harabagiu (2010)" startWordPosition="1086" endWordPosition="1089">s for relation classification. In general, to classify relations between pairs of nouns the most important features come from the pairs themselves and the words between and around the pairs (Hendrickx et al., 2010). For example, in the sentence in Figure 1 (b) there is a cause-effect relationship between the two nouns conflicts and players. To classify the relation, the most common features are the noun pair (conflicts, players), the words between the noun pair (are, caused, by), the words before the pair (the, external), and the words after the pair (playing, tiles, 269 to, ...). As shown by Rink and Harabagiu (2010), the words between the noun pairs are the most effective among these features. Our main idea is to treat the most important features (the words between the noun pairs) as the targets to be predicted and other lexical features (noun pairs, words outside them) as their contexts. Due to this, we expect our embeddings to capture relevant features for relation classification better than previous models which only use window-based contexts. In this section we first describe the learning process for the word embeddings, focusing on lexical features for relation classification (Figure 1 (b)). We then</context>
<context position="16628" citStr="Rink and Harabagiu, 2010" startWordPosition="2756" endWordPosition="2759">ow-level features, a variety of useful features have been proposed for relation classification. Among them, we use dependency path features (Bunescu and Mooney, 2005) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns. The dependency path features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL. Concretely, RelEmbFULL uses the same binary features as in Socher et al. (2012). The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger (Ciaramita and Altun, 2006). 4 Experimental Settings 4.1 Training Data For pre-training we used a snapshot of the English Wikipedia2 from November 2013. First, 2http://dumps.wikimedia.org/enwiki/. we extracted 80 million sentences from the original Wikipedia file, and then used Enju3 (Miyao and Tsujii, 2008) to automatically ass</context>
<context position="23886" citStr="Rink and Harabagiu, 2010" startWordPosition="3953" endWordPosition="3956">h the Baselines RelEmb significantly outperforms not only the Rand-Init baseline, but also the W2V-Init baseline. 6http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/. 7While we use a POS tagger to locate noun pairs, RelEmb does not explicitly use POS features at the supervised learning step. 273 Features for classifiers F1 / ACC (%) RelEmbFULL embeddings, dependency paths, WordNet, NE 83.5 / 79.9 RelEmb embeddings 82.8 / 78.9 RelEmb (W2V-Init) embeddings 81.8 / 77.7 RelEmb (Rand-Init) embeddings 78.2 / 73.5 SVM bag of words 76.5 / 72.0 SVM bag of words, POS, dependency paths, WordNet, 82.2 / 77.9 (Rink and Harabagiu, 2010) paraphrases, TextRunner, Google n-grams, etc. CR-CNNBe3t (dos Santos et al., 2015) embeddings, word position embeddings 84.1 / n/a FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.0 / n/a CR-CNNOther (dos Santos et al., 2015) embeddings, word position embeddings 82.7 / n/a CRNN (Ebrahimi and Dou, 2015) embeddings, parse trees, WordNet, NE, POS 82.7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse</context>
<context position="25231" citStr="Rink and Harabagiu (2010)" startWordPosition="4172" endWordPosition="4175">hat our task-specific word embeddings are more useful than those trained using window-based contexts. A point that we would like to emphasize is that the baselines are unexpectedly strong. As was noted by Wang and Manning (2012), we should carefully implement strong baselines and see whether complex models can outperform these baselines. 5.3.2 Comparison with SVM-Based Systems RelEmb performs much better than the bag-ofwords-based SVM. This is not surprising given that we use a large unannotated corpus and embeddings with a large number of parameters. RelEmb also outperforms the SVM system of Rink and Harabagiu (2010), which demonstrates the effectiveness of our task-specific word embeddings, despite our only requirement being a large unannotated corpus and a POS tagger. 5.3.3 Comparison with Neural Network Models RelEmb outperforms the RNN models. In our preliminary experiments, we have found some undesirable parse trees when computing vector representations using RNN-based models and such parsing errors might hamper the performance of the RNN models. FCMFULL, which relies on dependency paths and NE features, achieves a better score than that of RElEmb. Without such features, RelEmb outperforms FCMEMB by </context>
</contexts>
<marker>Rink, Harabagiu, 2010</marker>
<rawString>Bryan Rink and Sanda Harabagiu. 2010. UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 256–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="16649" citStr="Socher et al., 2012" startWordPosition="2760" endWordPosition="2763">ty of useful features have been proposed for relation classification. Among them, we use dependency path features (Bunescu and Mooney, 2005) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns. The dependency path features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL. Concretely, RelEmbFULL uses the same binary features as in Socher et al. (2012). The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger (Ciaramita and Altun, 2006). 4 Experimental Settings 4.1 Training Data For pre-training we used a snapshot of the English Wikipedia2 from November 2013. First, 2http://dumps.wikimedia.org/enwiki/. we extracted 80 million sentences from the original Wikipedia file, and then used Enju3 (Miyao and Tsujii, 2008) to automatically assign part-ofspeech (PO</context>
<context position="21644" citStr="Socher et al. (2012)" startWordPosition="3587" endWordPosition="3590">corpus. As with our experimental settings, we fix the learning rate to 0.025, and investigate several hyperparameter settings. For hyperparameter tuning we set the embedding dimensionality d to {50, 100, 300}, the context size c to {1, 3, 9}, and the number of negative samples k to {5, 15, 25}. 5.2.2 SVM-Based Systems A simple approach to the relation classification task is to use SVMs with standard binary bagof-words features. The bag-of-words features included the noun pairs and words between, before, and after the pairs, and we used LIBLINEAR6 as our classifier. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL. The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Netwo</context>
<context position="24339" citStr="Socher et al., 2012" startWordPosition="4025" endWordPosition="4028"> / 77.7 RelEmb (Rand-Init) embeddings 78.2 / 73.5 SVM bag of words 76.5 / 72.0 SVM bag of words, POS, dependency paths, WordNet, 82.2 / 77.9 (Rink and Harabagiu, 2010) paraphrases, TextRunner, Google n-grams, etc. CR-CNNBe3t (dos Santos et al., 2015) embeddings, word position embeddings 84.1 / n/a FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.0 / n/a CR-CNNOther (dos Santos et al., 2015) embeddings, word position embeddings 82.7 / n/a CRNN (Ebrahimi and Dou, 2015) embeddings, parse trees, WordNet, NE, POS 82.7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc. 79.4 / n/a Table 1: Scores on the test set for SemEval 2010 Task 8. These results show that our task-specific word embeddings are more useful than those trained using window-based contexts. A point that we would like to emphasize is that the baselines are unexpectedly strong. As was noted by Wang and Manning (2012), we should carefully implement strong baselines and see whether complex models can outperform these base</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1555--1565</pages>
<contexts>
<context position="6236" citStr="Tang et al. (2014)" startWordPosition="913" endWordPosition="916">ed context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation of relation extraction. Another kind of task-specific word embeddings was proposed by Tang et al. (2014), which used sentiment labels on tweets to adapt word embeddings for a sentiment analysis tasks. However, such an approach is only feasible when a large amount of labeled data is available. 3 Relation Classification Using Word Embedding-based Features We propose a novel method for learning word embeddings designed for relation classification. The word embeddings are trained by predicting each word between noun pairs, given the corresponding low-level features for relation classification. In general, to classify relations between pairs of nouns the most important features come from the pairs th</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1555– 1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-Supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="2427" citStr="Turian et al., 2010" startWordPosition="327" endWordPosition="330">ngs as an alternative to traditional hand-crafted features. Word embeddings are represented as real-valued vectors and capture syntactic and semantic similarity between words. For example, word2vec&apos; (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-sp</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-Supervised Learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher Manning</author>
</authors>
<title>Baselines and Bigrams: Simple, Good Sentiment and Topic Classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>90--94</pages>
<contexts>
<context position="24834" citStr="Wang and Manning (2012)" startWordPosition="4111" endWordPosition="4114">ngs, parse trees, WordNet, NE, POS 82.7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc. 79.4 / n/a Table 1: Scores on the test set for SemEval 2010 Task 8. These results show that our task-specific word embeddings are more useful than those trained using window-based contexts. A point that we would like to emphasize is that the baselines are unexpectedly strong. As was noted by Wang and Manning (2012), we should carefully implement strong baselines and see whether complex models can outperform these baselines. 5.3.2 Comparison with SVM-Based Systems RelEmb performs much better than the bag-ofwords-based SVM. This is not surprising given that we use a large unannotated corpus and embeddings with a large number of parameters. RelEmb also outperforms the SVM system of Rink and Harabagiu (2010), which demonstrates the effectiveness of our task-specific word embeddings, despite our only requirement being a large unannotated corpus and a POS tagger. 5.3.3 Comparison with Neural Network Models Re</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher Manning. 2012. Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 90–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Matthew R Gormley</author>
<author>Mark Dredze</author>
</authors>
<title>Factor-based Compositional Embedding Models.</title>
<date>2014</date>
<booktitle>In Proceedings of Workshop on Learning Semantics at the 2014 Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="16667" citStr="Yu et al., 2014" startWordPosition="2764" endWordPosition="2767"> have been proposed for relation classification. Among them, we use dependency path features (Bunescu and Mooney, 2005) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns. The dependency path features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL. Concretely, RelEmbFULL uses the same binary features as in Socher et al. (2012). The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger (Ciaramita and Altun, 2006). 4 Experimental Settings 4.1 Training Data For pre-training we used a snapshot of the English Wikipedia2 from November 2013. First, 2http://dumps.wikimedia.org/enwiki/. we extracted 80 million sentences from the original Wikipedia file, and then used Enju3 (Miyao and Tsujii, 2008) to automatically assign part-ofspeech (POS) tags. From the </context>
<context position="21894" citStr="Yu et al. (2014)" startWordPosition="3627" endWordPosition="3630">ber of negative samples k to {5, 15, 25}. 5.2.2 SVM-Based Systems A simple approach to the relation classification task is to use SVMs with standard binary bagof-words features. The bag-of-words features included the noun pairs and words between, before, and after the pairs, and we used LIBLINEAR6 as our classifier. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL. The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rely on parse trees. More recently, dos Santos et al. (2015) have introduced CR-CNN by extending the CNN model and achieved the best result to da</context>
<context position="24043" citStr="Yu et al., 2014" startWordPosition="3976" endWordPosition="3979">le we use a POS tagger to locate noun pairs, RelEmb does not explicitly use POS features at the supervised learning step. 273 Features for classifiers F1 / ACC (%) RelEmbFULL embeddings, dependency paths, WordNet, NE 83.5 / 79.9 RelEmb embeddings 82.8 / 78.9 RelEmb (W2V-Init) embeddings 81.8 / 77.7 RelEmb (Rand-Init) embeddings 78.2 / 73.5 SVM bag of words 76.5 / 72.0 SVM bag of words, POS, dependency paths, WordNet, 82.2 / 77.9 (Rink and Harabagiu, 2010) paraphrases, TextRunner, Google n-grams, etc. CR-CNNBe3t (dos Santos et al., 2015) embeddings, word position embeddings 84.1 / n/a FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.0 / n/a CR-CNNOther (dos Santos et al., 2015) embeddings, word position embeddings 82.7 / n/a CRNN (Ebrahimi and Dou, 2015) embeddings, parse trees, WordNet, NE, POS 82.7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc. 79.4 / n/a Table 1: Scores on the test set for SemEval 2010 Task 8. These results show that our task-specific word embeddings</context>
</contexts>
<marker>Yu, Gormley, Dredze, 2014</marker>
<rawString>Mo Yu, Matthew R. Gormley, and Mark Dredze. 2014. Factor-based Compositional Embedding Models. In Proceedings of Workshop on Learning Semantics at the 2014 Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daojian Zeng</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
</authors>
<title>Relation Classification via Convolutional Deep Neural Network.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>2335--2344</pages>
<contexts>
<context position="22210" citStr="Zeng et al. (2014)" startWordPosition="3678" endWordPosition="3681">er. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL. The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rely on parse trees. More recently, dos Santos et al. (2015) have introduced CR-CNN by extending the CNN model and achieved the best result to date. The key point of CR-CNN is that it improves the classification score by omitting the noisy class “Other” in the dataset described in Section 5.1. We call CR-CNN using the “Other” class CR-CNNOther and CRCNN omitting the class CR-CNNBeA. 5.3 Results and Discussion The scores on the test set for SemEval 2010 Task</context>
<context position="24280" citStr="Zeng et al., 2014" startWordPosition="4015" endWordPosition="4018"> embeddings 82.8 / 78.9 RelEmb (W2V-Init) embeddings 81.8 / 77.7 RelEmb (Rand-Init) embeddings 78.2 / 73.5 SVM bag of words 76.5 / 72.0 SVM bag of words, POS, dependency paths, WordNet, 82.2 / 77.9 (Rink and Harabagiu, 2010) paraphrases, TextRunner, Google n-grams, etc. CR-CNNBe3t (dos Santos et al., 2015) embeddings, word position embeddings 84.1 / n/a FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.0 / n/a CR-CNNOther (dos Santos et al., 2015) embeddings, word position embeddings 82.7 / n/a CRNN (Ebrahimi and Dou, 2015) embeddings, parse trees, WordNet, NE, POS 82.7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc. 79.4 / n/a Table 1: Scores on the test set for SemEval 2010 Task 8. These results show that our task-specific word embeddings are more useful than those trained using window-based contexts. A point that we would like to emphasize is that the baselines are unexpectedly strong. As was noted by Wang and Manning (2012), we should carefully implement strong baselin</context>
</contexts>
<marker>Zeng, Liu, Lai, Zhou, Zhao, 2014</marker>
<rawString>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation Classification via Convolutional Deep Neural Network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335–2344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>GuoDong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>825--832</pages>
<contexts>
<context position="4735" citStr="Zhang et al., 2006" startWordPosition="678" endWordPosition="681">l annotated resources. Furthermore, our qualitative analysis of the learned embeddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-words features and features based on syntactic parse trees. For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this li</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 825–832.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>