<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9996415">
Inducing Gazetteers for Named Entity Recognition
by Large-scale Clustering of Dependency Relations
</title>
<author confidence="0.994907">
Jun’ichi Kazama
</author>
<affiliation confidence="0.8036715">
Japan Advanced Institute of
Science and Technology (JAIST),
</affiliation>
<address confidence="0.9463605">
Asahidai 1-1, Nomi,
Ishikawa, 923-1292 Japan
</address>
<email confidence="0.999404">
kazama@jaist.ac.jp
</email>
<author confidence="0.983453">
Kentaro Torisawa
</author>
<affiliation confidence="0.9358035">
National Institute of Information and
Communications Technology (NICT),
</affiliation>
<address confidence="0.9836515">
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto, 619-0289 Japan
</address>
<email confidence="0.999392">
torisawa@nict.go.jp
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999315454545454">
We propose using large-scale clustering of de-
pendency relations between verbs and multi-
word nouns (MNs) to construct a gazetteer for
named entity recognition (NER). Since depen-
dency relations capture the semantics of MNs
well, the MN clusters constructed by using
dependency relations should serve as a good
gazetteer. However, the high level of computa-
tional cost has prevented the use of clustering
for constructing gazetteers. We parallelized
a clustering algorithm based on expectation-
maximization (EM) and thus enabled the con-
struction of large-scale MN clusters. We
demonstrated with the IREX dataset for the
Japanese NER that using the constructed clus-
ters as a gazetteer (cluster gazetteer) is a effec-
tive way of improving the accuracy of NER.
Moreover, we demonstrate that the combina-
tion of the cluster gazetteer and a gazetteer ex-
tracted from Wikipedia, which is also useful
for NER, can further improve the accuracy in
several cases.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993085">
Gazetteers, or entity dictionaries, are important for
performing named entity recognition (NER) accu-
rately. Since building and maintaining high-quality
gazetteers by hand is very expensive, many meth-
ods have been proposed for automatic extraction of
gazetteers from texts (Riloff and Jones, 1999; The-
len and Riloff, 2002; Etzioni et al., 2005; Shinzato et
al., 2006; Talukdar et al., 2006; Nadeau et al., 2006).
Most studies using gazetteers for NER are based
on the assumption that a gazetteer is a mapping
from a multi-word noun (MN)1 to named en-
tity categories such as “Tokyo Stock Exchange →
{ORGANIZATION}”.2 However, since the corre-
spondence between the labels and the NE categories
can be learned by tagging models, a gazetteer will be
useful as long as it returns consistent labels even if
those returned are not the NE categories. By chang-
ing the perspective in such a way, we can explore
more broad classes of gazetteers. For example, we
can use automatically extracted hyponymy relations
(Hearst, 1992; Shinzato and Torisawa, 2004), or au-
tomatically induced MN clusters (Rooth et al., 1999;
Torisawa, 2001).
For instance, Kazama and Torisawa (2007) used
the hyponymy relations extracted from Wikipedia
for the English NER, and reported improved accu-
racies with such a gazetteer.
We focused on the automatically induced clus-
ters of multi-word nouns (MNs) as the source of
gazetteers. We call the constructed gazetteers clus-
ter gazetteers. In the context of tagging, there are
several studies that utilized word clusters to prevent
the data sparseness problem (Kazama et al., 2001;
Miller et al., 2004). However, these methods cannot
produce the MN clusters required for constructing
gazetteers. In addition, the clustering methods used,
such as HMMs and Brown’s algorithm (Brown et
al., 1992), seem unable to adequately capture the se-
mantics of MNs since they are based only on the
information of adjacent words. We utilized richer
</bodyText>
<footnote confidence="0.9996986">
1We used the term, “multi-word”, to emphasize that a
gazetteer includes not only one-word expressions but also
multi-word expressions.
2Although several categories can be associated in general,
we assume that only one category is associated.
</footnote>
<page confidence="0.923177">
407
</page>
<note confidence="0.671135">
Proceedings ofACL-08: HLT, pages 407–415,
</note>
<page confidence="0.310451">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.985605357142857">
p(c|n). This
syntactic/semantic structures, i.e., verb-MN depen-
dencies to make clean MN clusters. Rooth et al.
(1999) and Torisawa (2001) showed that the EM-
based clustering using verb-MN dependencies can
produce semantically clean MN clusters. However,
the clustering algorithms, especially the EM-based
algorithms, are computationally expensive. There-
fore, performing the clustering with a vocabulary
that is large enough to cover the many named entities
required to improve the accuracy of NER is difficult.
We enabled such large-scale clustering by paralleliz-
ing the clustering algorithm, and we demonstrate the
usefulness of the gazetteer constructed.
We parallelized the algorithm of (Torisawa, 2001)
using the Message Passing Interface (MPI), with the
prime goal being to distribute parameters and thus
enable clustering with a large vocabulary. Apply-
ing the parallelized clustering to a large set of de-
pendencies collected from Web documents enabled
us to construct gazetteers with up to 500,000 entries
and 3,000 classes.
In our experiments, we used the IREX dataset
(Sekine and Isahara, 2000) to demonstrate the use-
fulness of cluster gazetteers. We also compared
the cluster gazetteers with the Wikipedia gazetteer
constructed by following the method of (Kazama
and Torisawa, 2007). The improvement was larger
for the cluster gazetteer than for the Wikipedia
gazetteer. We also investigated whether these
gazetteers improve the accuracies further when they
are used in combination. The experimental results
indicated that the accuracy improved further in sev-
eral cases and showed that these gazetteers comple-
ment each other.
The paper is organized as follows. In Section 2,
we explain the construction of cluster gazetteers and
its parallelization, along with a brief explanation of
the construction of the Wikipedia gazetteer. In Sec-
tion 3, we explain how to use these gazetteers as fea-
tures in an NE tagger. Our experimental results are
reported in Section 4.
</bodyText>
<sectionHeader confidence="0.997652" genericHeader="method">
2 Gazetteer Induction
</sectionHeader>
<subsectionHeader confidence="0.997871">
2.1 Induction by MN Clustering
</subsectionHeader>
<bodyText confidence="0.924588">
Assume we have a probabilistic model of a multi-
word noun (MN) and its class: p(n, c) =
p(n|c)p(c), where n E N is an MN and c E C is a
class. We can use this model to construct a gazetteer
in several ways. The method we used in this study
constructs a gazetteer: n —* argmax
</bodyText>
<equation confidence="0.841251">
c
computation can be re-written by the Bayes rule as
p(n|c)p(c) using p(n|c) and p(c).
</equation>
<bodyText confidence="0.9999462">
Note that we do not exclude non-NEs when we
construct the gazetteer. We expect that tagging
models (CRFs in our case) can learn an appropri-
ate weight for each gazetteer match regardless of
whether it is an NE or not.
</bodyText>
<subsectionHeader confidence="0.971632">
2.2 EM-based Clustering using Dependency
Relations
</subsectionHeader>
<bodyText confidence="0.98945">
To learn p(n|c) and p(c) for Japanese, we use the
EM-based clustering method presented by Torisawa
(2001). This method assumes a probabilistic model
of verb-MN dependencies with hidden semantic
classes:3
</bodyText>
<equation confidence="0.993648">
�p(v, r, n) = p((v, r)|c)p(n|c)p(c), (1)
c
</equation>
<bodyText confidence="0.9999464">
where v E V is a verb and n E N is an MN that
depends on verb v with relation r. A relation, r,
is represented by Japanese postpositions attached to
n. For example, from the following Japanese sen-
tence, we extract the following dependency: v =
</bodyText>
<equation confidence="0.978467666666667">
Mt (drink), r = (”wo” postposition), n =
t�—)L (beer).
t�—)L (beer) � (wo) Mt (drink) (Pz� drink beer)
</equation>
<bodyText confidence="0.9997443125">
In the following, we let vt = (v, r) E V7 for the
simplicity of explanation.
To be precise, we attach various auxiliary verb
suffixes, such as “*Lra (reru)”, which is for pas-
sivization, into v, since these greatly change the type
of n in the dependent position. In addition, we also
treated the MN-MN expressions, “MN1 0) MN2”
(Pz� “MN2 of MN1”), as dependencies v = MN2,
r = 0), n = MN1, since these expressions also
characterize the dependent MNs well.
Given L training examples of verb-MN depen-
dencies {(vti, ni, fi)}Li=1, where fi is the number
of dependency (vti, ni) in a corpus, the EM-based
clustering tries to find p(vt|c), p(n|c), and p(c) that
maximize the (log)-likelihood of the training exam-
ples:
</bodyText>
<equation confidence="0.663248833333333">
LL(p) = � fi log( � p(vti|c)p(ni|c)p(c)). (2)
i c
3This formulation is based on the formulation presented in
Rooth et al. (1999) for English.
argmax
c
</equation>
<page confidence="0.992063">
408
</page>
<bodyText confidence="0.999776777777778">
We iteratively update the probabilities using the EM
algorithm. For the update procedures used, see Tori-
sawa (2001).
The corpus we used for collecting dependencies
was a large set (76 million) of Web documents,
that were processed by a dependency parser, KNP
(Kurohashi and Kawahara, 2005).4 From this cor-
pus, we extracted about 380 million dependencies
of the form {(vti, ni, fi)gLi .
</bodyText>
<subsectionHeader confidence="0.994388">
2.3 Parallelization for Large-scale Data
</subsectionHeader>
<bodyText confidence="0.998351787878788">
The disadvantage of the clustering algorithm de-
scribed above is the computational costs. The space
requirements are O(|VT ||C|+|N||C|+|C|) for stor-
ing the parameters, p(vt|c), p(n|c), and p(c)5, plus
O(L) for storing the training examples. The time
complexity is mainly O(L x |C |x I), where I is
the number of update iterations. The space require-
ments are the main limiting factor. Assume that a
floating-point number consumes 8 bytes. With the
setting, |N |= 500, 000, |VT  |= 500, 000, and
|C |= 3, 000, the algorithm requires more than 44
GB for the parameters and 4 GB of memory for the
training examples. A machine with more than 48
GB of memory is not widely available even today.
Therefore, we parallelized the clustering algo-
rithm, to make it suitable for running on a cluster
of PCs with a moderate amount of memory (e.g., 8
GB). First, we decided to store the training examples
on a file since otherwise each node would need to
store all the examples when we use the data splitting
described below, and having every node consume 4
GB of memory is memory-consuming. Since the ac-
cess to the training data is sequential, this does not
slow down the execution when we use a buffering
technique appropriately.6
We then split the matrix for the model parameters,
p(n|c) and p(vt|c), along with the class coordinate.
That is, each cluster node is responsible for storing
only a part of classes Cl, i.e., 1/|P |of the parame-
ter matrix, where P is the number of cluster nodes.
This data splitting enables linear scalability of mem-
ory sizes. However, doing so complicates the update
procedure and, in terms of execution speed, may
</bodyText>
<footnote confidence="0.98516425">
4Acknowledgements: This corpus was provided by Dr.
Daisuke Kawahara of NICT.
5To be precise, we need two copies of these.
6Each node has a copy of the training data on a local disk.
</footnote>
<figureCaption confidence="0.999572">
Figure 1: Parallelized inner-most routine of EM cluster-
ing algorithm. Each node executes this code in parallel.
</figureCaption>
<bodyText confidence="0.9774998">
offset the advantage of parallelization because each
node needs to receive information about the classes
that are not on the node in the inner-most routine of
the update procedure.
The inner-most routine should compute:
</bodyText>
<equation confidence="0.8325015">
p(c|vti,ni) = p(vti|c)p(ni|c)p(c)/Z, (3)
for each class c, where Z = Ec p(vti|c)p(ni|c)p(c)
</equation>
<bodyText confidence="0.999794296296296">
is a normalizing constant. However, Z cannot be
calculated without knowing the results of other clus-
ter nodes. Thus, if we use MPI for parallelization,
the parallelized version of this routine should re-
semble the algorithm shown in Figure 1. This rou-
tine first computes p(vti|cl)p(ni|cl)p(cl) for each
cl E Cl, and stores the sum of these values as localZ.
The routine uses an MPI function, MPI Allreduce,
to sum up localZ of the all cluster nodes and to
set Z with the resulting sum. We can compute
p(cl|vti, ni) by using this Z to normalize the value.
Although the above is the essence of our paralleliza-
tion, invoking MPI Allreduce in the inner-most loop
is very expensive because the communication setup
is not so cheap. Therefore, our implementation cal-
culates p(cl|vti, ni) in batches of B examples and
calls MPI Allreduce at every B examples.7 We used
a value of B = 4, 096 in this study.
By using this parallelization, we successfully per-
formed the clustering with |N |= 500, 000, |VT  |=
500, 000, |C |= 3, 000, and I = 150, on 8 clus-
ter nodes with a 2.6 GHz Opteron processor and 8
GB of memory. This clustering took about a week.
To our knowledge, no one else has performed EM-
based clustering of this type on this scale. The re-
sulting MN clusters are shown in Figure 2. In terms
of speed, our experiments are still at a preliminary
</bodyText>
<footnote confidence="0.755682">
7MPI Allreduce can also take array arguments and apply the
operation to each element of the array in one call.
</footnote>
<figure confidence="0.7707965">
Algorithm 2.1: Compute p(cl|vti, ni)
localZ = 0,Z = 0
for cl E Cl do
d = p(vti|c)p(ni|c)p(c)
p(cl|vti, ni) = d
localZ += d
MPI Allreduce( localZ, Z, 1, MPI DOUBLE,
MPI SUM, MPI COMM WORLD)
for cl E Cl do p(cl|vti, ni) /= Z
�
�
�
</figure>
<page confidence="0.982588">
409
</page>
<table confidence="0.985712153846154">
Class 791 Class 2760
ウィン ダ ム マリン/スタジアム
(WINDOM) (Chiba Marine Stadium [abb.])
カ ム リ 大阪/ドーム
(CAMRY) (Osaka Dome)
ディア マ ン テ ナ ゴ/ド
(DIAMANTE) (Nagoya Dome [abb.])
オ デッセ イ 福岡/ドーム
(ODYSSEY) (Fukuoka Dome)
インスパイア 大阪/球場
(INSPIRE) (Osaka Stadium)
ス イ フ ト ハマ/スタ
(SWIFT) (Yokohama Stadium [abb.])
</table>
<figureCaption confidence="0.989336166666667">
Figure 2: Clean MN clusters with named entity entries
(Left: car brand names. Right: stadium names). Names
are sorted on the basis of p(c1n). Stadium names are
examples of multi-word nouns (word boundaries are in-
dicated by “/”) and also include abbreviated expressions
(marked by [abb.]) .
</figureCaption>
<bodyText confidence="0.926007333333333">
stage. We have observed 5 times faster execution,
when using 8 cluster nodes with a relatively small
setting, |N |= |VT  |= 50, 000, |C |= 2, 000.
</bodyText>
<subsectionHeader confidence="0.991901">
2.4 Induction from Wikipedia
</subsectionHeader>
<bodyText confidence="0.99991784">
Defining sentences in a dictionary or an encyclope-
dia have long been used as a source of hyponymy re-
lations (Tsurumaru et al., 1991; Herbelot and Copes-
take, 2006).
Kazama and Torisawa (2007) extracted hy-
ponymy relations from the first sentences (i.e., defin-
ing sentences) of Wikipedia articles and then used
them as a gazetteer for NER. We used this method
to construct the Wikipedia gazetteer.
The method described by Kazama and Torisawa
(2007) is to first extract the first (base) noun phrase
after the first “is”, “was”, “are”, or “were” in the first
sentence of a Wikipedia article. The last word in the
noun phase is then extracted and becomes the hyper-
nym of the entity described by the article. For exam-
ple, from the following defining sentence, it extracts
“guitarist” as the hypernym for “Jimi Hendrix”.
Jimi Hendrix (November 27, 1942) was an Ameri-
can guitarist, singer and songwriter.
The second noun phrase is used when the first noun
phrase ends with “one”, “kind”, “sort”, or “type”,
or it ended with “name” followed by “of”. This
rule is for treating expressions like “... is one of
the landlocked countries.” By applying this method
of extraction to all the articles in Wikipedia, we
</bodyText>
<table confidence="0.9989005">
# instances
page titles processed 550,832
articles found 547,779
(found by redirection) (189,222)
first sentences found 545,577
hypernyms extracted 482,599
</table>
<tableCaption confidence="0.999417">
Table 1: Wikipedia gazetteer extraction
</tableCaption>
<bodyText confidence="0.999732173913043">
construct a gazetteer that maps an MN (a title of a
Wikipedia article) to its hypernym.8 When the hy-
pernym extraction failed, a special hypernym sym-
bol, e.g., “UNK”, was used.
We modified this method for Japanese. After pre-
processing the first sentence of an article using a
morphological analyzer, MeCab9, we extracted the
last noun after the appearance of Japanese postpo-
sition “は (wa)” (1/4 “is”). As in the English case,
we also refrained from extracting expressions corre-
sponding to “one of” and so on.
From the Japanese Wikipedia entries of April
10, 2007, we extracted 550,832 gazetteer entries
(482,599 entries have hypernyms other than UNK).
Various statistics for this extraction are shown in
Table 1. The number of distinct hypernyms in
the gazetteer was 12,786. Although this Wikipedia
gazetteer is much smaller than the English version
used by Kazama and Torisawa (2007) that has over
2,000,000 entries, it is the largest gazetteer that can
be freely used for Japanese NER. Our experimen-
tal results show that this Wikipedia gazetteer can be
used to improve the accuracy of Japanese NER.
</bodyText>
<sectionHeader confidence="0.82009" genericHeader="method">
3 Using Gazetteers as Features of NER
</sectionHeader>
<bodyText confidence="0.999856833333333">
Since Japanese has no spaces between words, there
are several choices for the token unit used in NER.
Asahara and Motsumoto (2003) proposed using
characters instead of morphemes as the unit to alle-
viate the effect of segmentation errors in morpholog-
ical analysis and we also used their character-based
method. The NER task is then treated as a tagging
task, which assigns IOB tags to each character in
a sentence.10 We use Conditional Random Fields
(CRFs) (Lafferty et al., 2001) to perform this tag-
ging.
The information of a gazetteer is incorporated
</bodyText>
<footnote confidence="0.99428625">
8They handled “redirections” as well by following redirec-
tion links and extracting a hypernym from the article reached.
9http://mecab.sourceforge.net
10Precisely, we use IOB2 tags.
</footnote>
<page confidence="0.960197">
410
</page>
<table confidence="0.918706">
ch tc Y -- — bi 010 A ···
match O B I I O O O · · ·
(w/ class) O B-I�R I-°�R I-°�R O O O · · ·
</table>
<tableCaption confidence="0.5278">
Figure 3: Gazetteer features for Japanese NER. Here, ‘Y
--” means “SONY”, “��” means “company”, and “
DIA” means “to develop”.
</tableCaption>
<bodyText confidence="0.999948">
as features in a CRF-based NE tagger. We follow
the method used by Kazama and Torisawa (2007),
which encodes the matching with a gazetteer entity
using IOB tags, with the modification for Japanese.
They describe using two types of gazetteer features.
The first is a matching-only feature, which uses
bare IOB tags to encode only matching information.
The second uses IOB tags that are augmented with
classes (e.g., B-country and I-country).11 When
there are several possibilities for making a match,
the left-most longest match is selected. The small
differences from their work are: (1) We used char-
acters as the unit as we described above, (2) While
Kazama and Torisawa (2007) checked only the word
sequences that start with a capitalized word and thus
exploited the characteristics of English language, we
checked the matching at every character, (3) We
used a TRIE to make the look-up efficient.
The output of gazetteer features for Japanese NER
are thus as those shown in Figure 3. These annotated
IOB tags can be used in the same way as other fea-
tures in a CRF tagger.
</bodyText>
<sectionHeader confidence="0.999698" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.929064">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9999465">
We used the CRL NE dataset provided in the
IREX competition (Sekine and Isahara, 2000). In
the dataset, 1,174 newspaper articles are annotated
with 8 NE categories: ARTIFACT, DATE, LO-
CATION, MONEY, ORGANIZATION, PERCENT,
PERSON, and TIME.12 We converted the data into
the CoNLL 2003 format, i.e., each row corresponds
to a character in this case. We obtained 11,892 sen-
tences13 with 18,677 named entities. We split this
data into the training set (9,000 sentences), the de-
</bodyText>
<footnote confidence="0.729973714285714">
11Here, we call the value returned by a gazetteer a “class”.
Features are not output when the returned class is UNK in the
case of the Wikipedia gazetteer. We did not observe any signif-
icant change if we also used UNK.
12We ignored OPTIONAL category.
13This number includes the number of -DOCSTART- tokens
in CoNLL 2003 format.
</footnote>
<table confidence="0.895089416666667">
Name Description
ch character itself
ct character type: uppercase alphabet, lower-
case alphabet, katakana, hiragana, Chinese
characters, numbers, numbers in Chinese
characters, and spaces
m mo bare IOB tag indicating boundaries of mor-
phemes
m mm IOB tag augmented by morpheme string,
indicating boundaries and morphemes
m mp IOB tag augmented by morpheme type, in-
dicating boundaries and morpheme types
(POSs)
bm bare IOB tag indicating “bunsetsu” bound-
aries (Bunsetsu is a basic unit in Japanese
and usually contains content words fol-
lowed by function words such as postpo-
sitions)
bi bunsetsu-inner feature. See (Nakano and
Hirai, 2004).
bp adjacent-bunsetsu feature. See (Nakano
and Hirai, 2004).
bh head-of-bunsetsu features. See (Nakano
and Hirai, 2004).
</table>
<tableCaption confidence="0.996477">
Table 2: Atomic features used in baseline model.
</tableCaption>
<bodyText confidence="0.8777365">
velopment set (1,446 sentences), and the testing set
(1,446 sentences).
</bodyText>
<subsectionHeader confidence="0.781807">
4.2 Baseline Model
</subsectionHeader>
<bodyText confidence="0.999994333333333">
We extracted the atomic features listed in Table 2
at each character for our baseline model. Though
there may be slight differences, these features are
based on the standard ones proposed and used in
previous studies on Japanese NER such as those by
Asahara and Motsumoto (2003), Nakano and Hirai
(2004), and Yamada (2007). We used MeCab as a
morphological analyzer and CaboCha14 (Kudo and
Matsumoto, 2002) as the dependency parser to find
the boundaries of the bunsetsu. We generated the
node and the edge features of a CRF model as de-
scribed in Table 3 using these atomic features.
</bodyText>
<subsectionHeader confidence="0.99731">
4.3 Training
</subsectionHeader>
<bodyText confidence="0.9997315">
To train CRF models, we used Taku Kudo’s CRF++
(ver. 0.44) 15 with some modifications.16 We
</bodyText>
<footnote confidence="0.9846565">
14http://chasen.org/∼taku/software/
CaboCha
15http://chasen.org/˜taku/software/CRF++
16We implemented scaling, which is similar to that for
HMMs (Rabiner, 1989), in the forward-backward phase and re-
placed the optimization module in the original package with the
</footnote>
<page confidence="0.992836">
411
</page>
<table confidence="0.999052636363636">
Node features:
1»», x−2, x−1, x0, x+1, x+2} x y0
where x = ch, ct, m mm, m mo, m mp, bi,
bp, and bh
Edge features:
1»»,
x−1, x0, x+1} x y−1 x y0
where x = ch, ct, and m mp
Bigram node features:
1x−2x−1, x−1x0, x0x+1} x y0
x = ch, ct, m mo, m mp, bm, bi, bp, and bh
</table>
<tableCaption confidence="0.875512">
Table 3: Baseline features. Value of node feature is deter-
mined from current tag, y0, and surface feature (combina-
</tableCaption>
<bodyText confidence="0.896399461538462">
tion of atomic features in Table 2). Value of edge feature
is determined by previous tag, y−1, current tag, y0, and
surface feature. Subscripts indicate relative position from
current character.
used Gaussian regularization to prevent overfitting.
The parameter of the Gaussian, σ2, was tuned us-
ing the development set. We tested 10 points:
10.64,1.28, 2.56, 5.12, ... ,163.84, 327.68}. We
stopped training when the relative change in the log-
likelihood became less than a pre-defined threshold,
0.0001. Throughout the experiments, we omitted
the features whose surface part described in Table
3 occurred less than twice in the training corpus.
</bodyText>
<subsectionHeader confidence="0.999585">
4.4 Effect of Gazetteer Features
</subsectionHeader>
<bodyText confidence="0.984196753623189">
We investigated the effect of the cluster gazetteer de-
scribed in Section 2.1 and the Wikipedia gazetteer
described in Section 2.4, by adding each gazetteer
to the baseline model. We added the matching-
only and the class-augmented features, and we gen-
erated the node and the edge features in Table 3.17
For the cluster gazetteer, we made several gazetteers
that had different vocabulary sizes and numbers of
classes. The number of clustering iterations was 150
and the initial parameters were set randomly with a
Dirichlet distribution (αi = 1.0).
The statistics of each gazetteer are summarized
in Table 4. The number of entries in a gazetteer is
given by “# entries”, and “# matches” is the number
of matches that were output for the training set. We
define “# e-matches” as the number of matches that
also match a boundary of a named entity in the train-
ing set, and “# optimal” as the optimal number of “#
e-matches” that can be achieved when we know the
LMVM optimizer of TAO (version 1.9) (Benson et al., 2007)
17Bigram node features were not used for gazetteer features.
oracle of entity boundaries. Note that this cannot
be realized because our matching uses the left-most
longest heuristics. We define “pre.” as the precision
of the output matches (i.e., # e-matches/# matches),
and “rec.” as the recall (i.e., # e-matches/# NEs).
Here, # NEs = 14, 056. Finally, “opt.” is the op-
timal recall (i.e., # optimal/# NEs). “# classes” is
the number of distinct classes in a gazetteer, and
“# used” is the number of classes that were out-
put for the training set. Gazetteers are as follows:
“wikip(m)” is the Wikipedia gazetteer (matching
only), and “wikip(c)” is the Wikipedia gazetteer
(with class-augmentation). A cluster gazetteer,
which is constructed by the clustering with |M |=
|V7  |= X x 1, 000 and |C |= Y x 1, 000, is indi-
cated by “cXk-Y k”. Note that “# entries” is slightly
smaller than the vocabulary size since we removed
some duplications during the conversion to a TRIE.
These gazetteers cover 40 - 50% of the named en-
tities, and the cluster gazetteers have relatively wider
coverage than the Wikipedia gazetteer has. The pre-
cisions are very low because there are many erro-
neous matches, e.g., with a entries for a hiragana
character.18 Although this seems to be a serious
problem, removing such one-character entries does
not affect the accuracy, and in fact, makes it worsen
slightly. We think this shows one of the strengths
of machine learning methods such as CRFs. We can
also see that our current matching method is not an
optimal one. For example, 16% of the matches were
lost as a result of using our left-most longest heuris-
tics for the case of the c500k-2k gazetteer.
A comparison of the effect of these gazetteers is
shown in Table 5. The performance is measured
by the F-measure. First, the Wikipedia gazetteer
improved the accuracy as expected, i.e., it repro-
duced the result of Kazama and Torisawa (2007)
for Japanese NER. The improvement for the test-
ing set was 1.08 points. Second, all the tested clus-
ter gazetteers improved the accuracy. The largest
improvement was 1.55 points with the c300k-3k
gazetteer. This was larger than that of the Wikipedia
gazetteer. The results for c300k-Yk gazetteers show
a peak of the improvement at some number of clus-
ters. In this case, |C |= 3, 000 achieved the best
improvement. The results of cXk-2k gazetteers in-
18Wikipedia contains articles explaining each hiragana char-
acter, e.g., “あ is a hiragana character”.
</bodyText>
<page confidence="0.994143">
412
</page>
<table confidence="0.9995957">
Name # entries # matches # e-matches # optimal pre. (%) rec. (%) opt. rec. (%) # classes # used
wikip(m) 550,054 225,607 6,804 7,602 3.02 48.4 54.1 N/A N/A
wikip(c) 550,054 189,029 5,441 6,064 2.88 38.7 43.1 12,786 1,708
c100k-2k 99,671 193,897 6,822 8,233 3.52 48.5 58.6 2,000 1,910
c300k-2k 295,695 178,220 7,377 9,436 4.14 52.5 67.1 2,000 1,973
c300k-1k T T T T T T T 1,000 982
c300k-3k T T T T T T T 3,000 2,848
c300k-4k T T T T T T T 4,000 3,681
c500k-2k 497,101 174,482 7,470 9,798 4.28 53.1 69.7 2,000 1,951
c500k-3k T T T T T T T 3,000 2,854
</table>
<tableCaption confidence="0.994232">
Table 4: Statistics of various gazetteers.
</tableCaption>
<table confidence="0.9994696">
Model F (dev.) F (test.) best σ2
baseline 87.23 87.42 20.48
+wikip 87.60 88.50 2.56
+c300k-1k 88.74 87.98 40.96
+c300k-2k 88.75 88.01 163.84
+c300k-3k 89.12 88.97 20.48
+c300k-4k 88.99 88.40 327.68
+c100k-2k 88.15 88.06 20.48
+c500k-2k 88.80 88.12 40.96
+c500k-3k 88.75 88.03 20.48
</table>
<tableCaption confidence="0.998274">
Table 5: Comparison of gazetteer features.
</tableCaption>
<table confidence="0.999195714285714">
Model F (dev.) F (test.) best σ2
+wikip+c300k-1k 88.65 *89.32 0.64
+wikip+c300k-2k *89.22 *89.13 10.24
+wikip+c300k-3k 88.69 *89.62 40.96
+wikip+c300k-4k 88.67 *89.19 40.96
+wikip+c500k-2k *89.26 *89.19 2.56
+wikip+c500k-3k *88.80 *88.60 10.24
</table>
<tableCaption confidence="0.9745815">
Table 6: Effect of combination. Figures with * mean that
accuracy was improved by combining gazetteers.
</tableCaption>
<bodyText confidence="0.999989166666667">
dicate that the larger a gazetteer is, the larger the im-
provement. However, the accuracies of the c300k-3k
and c500k-3k gazetteers seem to contradict this ten-
dency. It might be caused by the accidental low qual-
ity of the clustering that results from random initial-
ization. We need to investigate this further.
</bodyText>
<subsectionHeader confidence="0.998064">
4.5 Effect of Combining the Cluster and the
Wikipedia Gazetteers
</subsectionHeader>
<bodyText confidence="0.999908375">
We have observed that using the cluster gazetteer
and the Wikipedia one improves the accuracy of
Japanese NER. The next question is whether these
gazetteers improve the accuracy further when they
are used together. The accuracies of models that
use the Wikipedia gazetteer and one of the cluster
gazetteers at the same time are shown in Table 6.
The accuracy was improved in most cases. How-
</bodyText>
<table confidence="0.999204090909091">
Model F
(Asahara and Motsumoto, 2003) 87.21
(Nakano and Hirai, 2004) 89.03
(Yamada, 2007) 88.33
(Sasano and Kurohashi, 2008) 89.40
proposed (baseline) 87.62
proposed (+wikip) 88.14
proposed (+c300k-3k) 88.45
proposed (+c500k-2k) 88.41
proposed (+wikip+c300k-3k) 88.93
proposed (+wikip+c500k-2k) 88.71
</table>
<tableCaption confidence="0.999982">
Table 7: Comparison with previous studies
</tableCaption>
<bodyText confidence="0.938497666666667">
a 1.77-point improvement fr
om the baseline for the
testing set.
</bodyText>
<subsectionHeader confidence="0.837741">
vious Studies
</subsectionHeader>
<bodyText confidence="0.999972625">
ever, there were some cases where the accuracy for
the development set was degraded. Therefore, we
should state at this point that while the benefit of
combining these gazetteers is not consistent in a
strict sense, it seems to exist. The best performance,
F = 89.26 (dev.) / 89.19 (test.), was achieved when
we combined the Wikipedia gazetteer and the clus-
ter gazetteer, c500k-2k. This means that there was
</bodyText>
<sectionHeader confidence="0.734871" genericHeader="method">
5 Comparison with Pre
</sectionHeader>
<bodyText confidence="0.942846777777778">
Since many previous studies on Japanese NER used
5-fold cross validation for the
dataset, we
also performed it for some our models that had the
best
found in the previous experiments. The re-
sults are listed in Table 7 with references to the re-
sults of recent studies. These results not only re-
confirmed the effects of the gazetteer features shown
in the previous experiments, but they also showed
that our best model is comparable to the state-of-the-
art models. The system recently proposed by Sasano
and Kurohashi (2008) is currently the best system
for the IREX dataset. It uses man
IREX
σ2
y structural fea-
tures that are not used in our model. Incorporating
</bodyText>
<page confidence="0.997876">
413
</page>
<bodyText confidence="0.953729">
such features might improve our model further.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="method">
6 Related Work and Discussion
</sectionHeader>
<bodyText confidence="0.993295493670887">
There are several studies that used automatically ex-
tracted gazetteers for NER (Shinzato et al., 2006;
Talukdar et al., 2006; Nadeau et al., 2006; Kazama
and Torisawa, 2007). Most of the methods (Shin-
zato et al., 2006; Talukdar et al., 2006; Nadeau et
al., 2006) are oriented at the NE category. They
extracted a gazetteer for each NE category and uti-
lized it in a NE tagger. On the other hand, Kazama
and Torisawa (2007) extracted hyponymy relations,
which are independent of the NE categories, from
Wikipedia and utilized it as a gazetteer. The ef-
fectiveness of this method was demonstrated for
Japanese NER as well by this study.
Inducing features for taggers by clustering has
been tried by several researchers (Kazama et al.,
2001; Miller et al., 2004). They constructed word
clusters by using HMMs or Brown’s clustering algo-
rithm (Brown et al., 1992), which utilize only infor-
mation from neighboring words. This study, on the
other hand, utilized MN clustering based on verb-
MN dependencies (Rooth et al., 1999; Torisawa,
2001). We showed that gazetteers created by using
such richer semantic/syntactic structures improves
the accuracy for NER.
The size of the gazetteers is also a novel point of
this study. The previous studies, with the excep-
tion of Kazama and Torisawa (2007), used smaller
gazetteers than ours. Shinzato et al. (2006) con-
structed gazetteers with about 100,000 entries in
total for the “restaurant” domain; Talukdar et al.
(2006) used gazetteers with about 120,000 entries
in total, and Nadeau et al. (2006) used gazetteers
with about 85,000 entries in total. By paralleliz-
ing the clustering algorithm, we successfully con-
structed a cluster gazetteer with up to 500,000 en-
tries from a large amount of dependency relations
in Web documents. To our knowledge, no one else
has performed this type of clustering on such a large
scale. Wikipedia also produced a large gazetteer
of more than 550,000 entries. However, compar-
ing these gazetteers and ours precisely is difficult at
this point because the detailed information such as
the precision and the recall of these gazetteers were
not reported.19 Recently, Inui et al. (2007) investi-
19Shinzato et al. (2006) reported some useful statistics about
gated the relation between the size and the quality of
a gazetteer and its effect. We think this is one of the
important directions of future research.
Parallelization has recently regained attention in
the machine learning community because of the
need for learning from very large sets of data. Chu
et al. (2006) presented the MapReduce framework
for a wide range of machine learning algorithms, in-
cluding the EM algorithm. Newman et al. (2007)
presented parallelized Latent Dirichlet Allocation
(LDA). However, these studies focus on the distri-
bution of the training examples and relevant com-
putation, and ignore the need that we found for the
distribution of model parameters. The exception,
which we noticed recently, is a study by Wolfe et
al. (2007), which describes how each node stores
only those parameters relevant to the training data
on each node. However, some parameters need to
be duplicated and thus their method is less efficient
than ours in terms of memory usage.
We used the left-most longest heuristics to find
the matching gazetteer entries. However, as shown
in Table 4 this is not an optimal method. We need
more sophisticated matching methods that can han-
dle multiple matching possibilities. Using models
such as Semi-Markov CRFs (Sarawagi and Cohen,
2004), which handle the features on overlapping re-
gions, is one possible direction. However, even if
we utilize the current gazetteers optimally, the cov-
erage is upper bounded at 70%. To cover most of
the named entities in the data, we need much larger
gazetteers. A straightforward approach is to increase
the number of Web documents used for the MN clus-
tering and to use larger vocabularies.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.945547666666667">
We demonstrated that a gazetteer obtained by clus-
tering verb-MN dependencies is a useful feature
for a Japanese NER. In addition, we demonstrated
that using the cluster gazetteer and the gazetteer ex-
tracted from Wikipedia (also shown to be useful)
can together further improves the accuracy in sev-
eral cases. Future work will be to refine the match-
ing method and to construct even larger gazetteers.
their gazetteers.
</bodyText>
<page confidence="0.997966">
414
</page>
<sectionHeader confidence="0.995847" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999764821782178">
M. Asahara and Y. Motsumoto. 2003. Japanese named
entity extraction with redundant morphological analy-
sis.
S. Benson, L. C. McInnes, J. Mor´e, T. Munson, and
J. Sarich. 2007. TAO user manual (revision 1.9).
Technical Report ANL/MCS-TM-242, Mathematics
and Computer Science Division, Argonne National
Laboratory. http://www.mcs.anl.gov/tao.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467–479.
C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. Bradski, A. Y.
Ng, and K. Olukotun. 2006. Map-reduce for machine
learning on multicore. In NIPS 2006.
O. Etzioni, M. Cafarella, D. Downey, A. M. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
Web – an experimental study. Artificial Intelligence
Journal.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539–545.
A. Herbelot and A. Copestake. 2006. Acquiring onto-
logical relationships from Wikipedia using RMRS. In
Workshop on Web Content Mining with Human Lan-
guage Technologies ISWC06.
T. Inui, K. Murakami, T. Hashimoto, K. Utsumi, and
M. Ishikawa. 2007. A study on using gazetteers for
organization name recognition. In IPSJ SIG Technical
Report 2007-NL-182 (in Japanese).
J. Kazama and K. Torisawa. 2007. Exploiting Wikipedia
as external knowledge for named entity recognition.
In EMNLP-CoNLL 2007.
J. Kazama, Y. Miyao, and J. Tsujii. 2001. A maxi-
mum entropy tagger with unsupervised hidden Markov
models. In NLPRS 2001.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In CoNLL 2002.
S. Kurohashi and D. Kawahara. 2005. KNP (Kurohashi-
Nagao parser) 2.0 users manual.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML 2001.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In HLT-NAACL04.
D. Nadeau, Peter D. Turney, and Stan Matwin. 2006.
Unsupervised named-entity recognition: Generating
gazetteers and resolving ambiguity. In 19th Canadian
Conference on Artificial Intelligence.
K. Nakano and Y. Hirai. 2004. Japanese named entity
extraction with bunsetsu features. IPSJ Journal (in
Japanese).
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2007. Distributed inference for latent dirichlet allo-
cation. In NIPS 2007.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257–286.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In 16th National Conference on Artificial Intelligence
(AAAI-99).
M. Rooth, S. Riezler, D. Presher, G. Carroll, and F. Beil.
1999. Inducing a semantically annotated lexicon via
EM-based clustering.
S. Sarawagi and W. W. Cohen. 2004. Semi-Markov ran-
dom fields for information extraction. In NIPS 2004.
R. Sasano and S. Kurohashi. 2008. Japanese named en-
tity recognition using structural natural language pro-
cessing. In IJCNLP 2008.
S. Sekine and H. Isahara. 2000. IREX: IR and IE evalu-
ation project in Japanese. In IREX 2000.
K. Shinzato and K. Torisawa. 2004. Acquiring hy-
ponymy relations from Web documents. In HLT-
NAACL 2004.
K. Shinzato, S. Sekine, N. Yoshinaga, and K. Tori-
sawa. 2006. Constructing dictionaries for named en-
tity recognition on specific domains from the Web. In
Web Content Mining with Human Language Technolo-
gies Workshop on the 5th International Semantic Web.
P. P. Talukdar, T. Brants, M. Liberman, and F. Pereira.
2006. A context pattern induction method for named
entity extraction. In CoNLL 2006.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
context. In EMNLP 2002.
K. Torisawa. 2001. An unsupervised method for canoni-
calization of Japanese postpositions. In NLPRS 2001.
H. Tsurumaru, K. Takeshita, K. Iami, T. Yanagawa, and
S. Yoshida. 1991. An approach to thesaurus construc-
tion from Japanese language dictionary. In IPSJ SIG
Notes Natural Language vol.83-16, (in Japanese).
J. Wolfe, A. Haghighi, and D. Klein. 2007. Fully dis-
tributed EM for very large datasets. In NIPS Workshop
on Efficient Machine Learning.
H. Yamada. 2007. Shift-reduce chunking for Japanese
named entity extraction. In ISPJ SIG Technical Report
2007-NL-179.
</reference>
<page confidence="0.998554">
415
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.435367">
<title confidence="0.9649805">Inducing Gazetteers for Named Entity Recognition by Large-scale Clustering of Dependency Relations</title>
<author confidence="0.987235">Jun’ichi Kazama</author>
<affiliation confidence="0.8370355">Japan Advanced Institute of Science and Technology (JAIST),</affiliation>
<address confidence="0.9577195">Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan</address>
<email confidence="0.980224">kazama@jaist.ac.jp</email>
<author confidence="0.804198">Kentaro Torisawa</author>
<affiliation confidence="0.9994945">National Institute of Information and Communications Technology (NICT),</affiliation>
<address confidence="0.9789775">3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan</address>
<email confidence="0.984348">torisawa@nict.go.jp</email>
<abstract confidence="0.998557434782609">We propose using large-scale clustering of dependency relations between verbs and multiword nouns (MNs) to construct a gazetteer for named entity recognition (NER). Since dependency relations capture the semantics of MNs well, the MN clusters constructed by using dependency relations should serve as a good gazetteer. However, the high level of computational cost has prevented the use of clustering for constructing gazetteers. We parallelized a clustering algorithm based on expectationmaximization (EM) and thus enabled the construction of large-scale MN clusters. We demonstrated with the IREX dataset for the Japanese NER that using the constructed clusas a gazetteer is a effective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Asahara</author>
<author>Y Motsumoto</author>
</authors>
<title>Japanese named entity extraction with redundant morphological analysis.</title>
<date>2003</date>
<contexts>
<context position="15569" citStr="Asahara and Motsumoto (2003)" startWordPosition="2573" endWordPosition="2576">than UNK). Various statistics for this extraction are shown in Table 1. The number of distinct hypernyms in the gazetteer was 12,786. Although this Wikipedia gazetteer is much smaller than the English version used by Kazama and Torisawa (2007) that has over 2,000,000 entries, it is the largest gazetteer that can be freely used for Japanese NER. Our experimental results show that this Wikipedia gazetteer can be used to improve the accuracy of Japanese NER. 3 Using Gazetteers as Features of NER Since Japanese has no spaces between words, there are several choices for the token unit used in NER. Asahara and Motsumoto (2003) proposed using characters instead of morphemes as the unit to alleviate the effect of segmentation errors in morphological analysis and we also used their character-based method. The NER task is then treated as a tagging task, which assigns IOB tags to each character in a sentence.10 We use Conditional Random Fields (CRFs) (Lafferty et al., 2001) to perform this tagging. The information of a gazetteer is incorporated 8They handled “redirections” as well by following redirection links and extracting a hypernym from the article reached. 9http://mecab.sourceforge.net 10Precisely, we use IOB2 tag</context>
<context position="19471" citStr="Asahara and Motsumoto (2003)" startWordPosition="3223" endWordPosition="3226">ion words such as postpositions) bi bunsetsu-inner feature. See (Nakano and Hirai, 2004). bp adjacent-bunsetsu feature. See (Nakano and Hirai, 2004). bh head-of-bunsetsu features. See (Nakano and Hirai, 2004). Table 2: Atomic features used in baseline model. velopment set (1,446 sentences), and the testing set (1,446 sentences). 4.2 Baseline Model We extracted the atomic features listed in Table 2 at each character for our baseline model. Though there may be slight differences, these features are based on the standard ones proposed and used in previous studies on Japanese NER such as those by Asahara and Motsumoto (2003), Nakano and Hirai (2004), and Yamada (2007). We used MeCab as a morphological analyzer and CaboCha14 (Kudo and Matsumoto, 2002) as the dependency parser to find the boundaries of the bunsetsu. We generated the node and the edge features of a CRF model as described in Table 3 using these atomic features. 4.3 Training To train CRF models, we used Taku Kudo’s CRF++ (ver. 0.44) 15 with some modifications.16 We 14http://chasen.org/∼taku/software/ CaboCha 15http://chasen.org/˜taku/software/CRF++ 16We implemented scaling, which is similar to that for HMMs (Rabiner, 1989), in the forward-backward pha</context>
<context position="26758" citStr="Asahara and Motsumoto, 2003" startWordPosition="4456" endWordPosition="4459">be caused by the accidental low quality of the clustering that results from random initialization. We need to investigate this further. 4.5 Effect of Combining the Cluster and the Wikipedia Gazetteers We have observed that using the cluster gazetteer and the Wikipedia one improves the accuracy of Japanese NER. The next question is whether these gazetteers improve the accuracy further when they are used together. The accuracies of models that use the Wikipedia gazetteer and one of the cluster gazetteers at the same time are shown in Table 6. The accuracy was improved in most cases. HowModel F (Asahara and Motsumoto, 2003) 87.21 (Nakano and Hirai, 2004) 89.03 (Yamada, 2007) 88.33 (Sasano and Kurohashi, 2008) 89.40 proposed (baseline) 87.62 proposed (+wikip) 88.14 proposed (+c300k-3k) 88.45 proposed (+c500k-2k) 88.41 proposed (+wikip+c300k-3k) 88.93 proposed (+wikip+c500k-2k) 88.71 Table 7: Comparison with previous studies a 1.77-point improvement fr om the baseline for the testing set. vious Studies ever, there were some cases where the accuracy for the development set was degraded. Therefore, we should state at this point that while the benefit of combining these gazetteers is not consistent in a strict sense,</context>
</contexts>
<marker>Asahara, Motsumoto, 2003</marker>
<rawString>M. Asahara and Y. Motsumoto. 2003. Japanese named entity extraction with redundant morphological analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Benson</author>
<author>L C McInnes</author>
<author>J Mor´e</author>
<author>T Munson</author>
<author>J Sarich</author>
</authors>
<title>TAO user manual (revision 1.9).</title>
<date>2007</date>
<tech>Technical Report ANL/MCS-TM-242,</tech>
<institution>Mathematics and Computer Science Division, Argonne National Laboratory.</institution>
<note>http://www.mcs.anl.gov/tao.</note>
<marker>Benson, McInnes, Mor´e, Munson, Sarich, 2007</marker>
<rawString>S. Benson, L. C. McInnes, J. Mor´e, T. Munson, and J. Sarich. 2007. TAO user manual (revision 1.9). Technical Report ANL/MCS-TM-242, Mathematics and Computer Science Division, Argonne National Laboratory. http://www.mcs.anl.gov/tao.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="3180" citStr="Brown et al., 1992" startWordPosition="482" endWordPosition="485">ions extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs and Brown’s algorithm (Brown et al., 1992), seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2Although several categories can be associated in general, we assume that only one category is associated. 407 Proceedings ofACL-08: HLT, pages 407–415, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics p(c|n). This syntactic/semantic structures, i.e., verb-MN dependencies to make clean MN clu</context>
<context position="29171" citStr="Brown et al., 1992" startWordPosition="4853" endWordPosition="4856">006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other hand, Kazama and Torisawa (2007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by several researchers (Kazama et al., 2001; Miller et al., 2004). They constructed word clusters by using HMMs or Brown’s clustering algorithm (Brown et al., 1992), which utilize only information from neighboring words. This study, on the other hand, utilized MN clustering based on verbMN dependencies (Rooth et al., 1999; Torisawa, 2001). We showed that gazetteers created by using such richer semantic/syntactic structures improves the accuracy for NER. The size of the gazetteers is also a novel point of this study. The previous studies, with the exception of Kazama and Torisawa (2007), used smaller gazetteers than ours. Shinzato et al. (2006) constructed gazetteers with about 100,000 entries in total for the “restaurant” domain; Talukdar et al. (2006) u</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-T Chu</author>
<author>S K Kim</author>
<author>Y-A Lin</author>
<author>Y Yu</author>
<author>G Bradski</author>
<author>A Y Ng</author>
<author>K Olukotun</author>
</authors>
<title>Map-reduce for machine learning on multicore.</title>
<date>2006</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="30849" citStr="Chu et al. (2006)" startWordPosition="5124" endWordPosition="5127">0 entries. However, comparing these gazetteers and ours precisely is difficult at this point because the detailed information such as the precision and the recall of these gazetteers were not reported.19 Recently, Inui et al. (2007) investi19Shinzato et al. (2006) reported some useful statistics about gated the relation between the size and the quality of a gazetteer and its effect. We think this is one of the important directions of future research. Parallelization has recently regained attention in the machine learning community because of the need for learning from very large sets of data. Chu et al. (2006) presented the MapReduce framework for a wide range of machine learning algorithms, including the EM algorithm. Newman et al. (2007) presented parallelized Latent Dirichlet Allocation (LDA). However, these studies focus on the distribution of the training examples and relevant computation, and ignore the need that we found for the distribution of model parameters. The exception, which we noticed recently, is a study by Wolfe et al. (2007), which describes how each node stores only those parameters relevant to the training data on each node. However, some parameters need to be duplicated and th</context>
</contexts>
<marker>Chu, Kim, Lin, Yu, Bradski, Ng, Olukotun, 2006</marker>
<rawString>C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. Bradski, A. Y. Ng, and K. Olukotun. 2006. Map-reduce for machine learning on multicore. In NIPS 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A M Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D S Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the Web – an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence Journal.</journal>
<contexts>
<context position="1720" citStr="Etzioni et al., 2005" startWordPosition="247" endWordPosition="250">ster gazetteer) is a effective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For exampl</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. M. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the Web – an experimental study. Artificial Intelligence Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="2390" citStr="Hearst, 1992" startWordPosition="362" endWordPosition="363">al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004).</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of the 14th International Conference on Computational Linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Herbelot</author>
<author>A Copestake</author>
</authors>
<title>Acquiring ontological relationships from Wikipedia using RMRS.</title>
<date>2006</date>
<booktitle>In Workshop on Web Content Mining with Human Language Technologies ISWC06.</booktitle>
<contexts>
<context position="13067" citStr="Herbelot and Copestake, 2006" startWordPosition="2162" endWordPosition="2166">) Figure 2: Clean MN clusters with named entity entries (Left: car brand names. Right: stadium names). Names are sorted on the basis of p(c1n). Stadium names are examples of multi-word nouns (word boundaries are indicated by “/”) and also include abbreviated expressions (marked by [abb.]) . stage. We have observed 5 times faster execution, when using 8 cluster nodes with a relatively small setting, |N |= |VT |= 50, 000, |C |= 2, 000. 2.4 Induction from Wikipedia Defining sentences in a dictionary or an encyclopedia have long been used as a source of hyponymy relations (Tsurumaru et al., 1991; Herbelot and Copestake, 2006). Kazama and Torisawa (2007) extracted hyponymy relations from the first sentences (i.e., defining sentences) of Wikipedia articles and then used them as a gazetteer for NER. We used this method to construct the Wikipedia gazetteer. The method described by Kazama and Torisawa (2007) is to first extract the first (base) noun phrase after the first “is”, “was”, “are”, or “were” in the first sentence of a Wikipedia article. The last word in the noun phase is then extracted and becomes the hypernym of the entity described by the article. For example, from the following defining sentence, it extrac</context>
</contexts>
<marker>Herbelot, Copestake, 2006</marker>
<rawString>A. Herbelot and A. Copestake. 2006. Acquiring ontological relationships from Wikipedia using RMRS. In Workshop on Web Content Mining with Human Language Technologies ISWC06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Inui</author>
<author>K Murakami</author>
<author>T Hashimoto</author>
<author>K Utsumi</author>
<author>M Ishikawa</author>
</authors>
<title>A study on using gazetteers for organization name recognition.</title>
<date>2007</date>
<booktitle>In IPSJ SIG</booktitle>
<tech>Technical Report 2007-NL-182 (in Japanese).</tech>
<contexts>
<context position="30464" citStr="Inui et al. (2007)" startWordPosition="5061" endWordPosition="5064">) used gazetteers with about 85,000 entries in total. By parallelizing the clustering algorithm, we successfully constructed a cluster gazetteer with up to 500,000 entries from a large amount of dependency relations in Web documents. To our knowledge, no one else has performed this type of clustering on such a large scale. Wikipedia also produced a large gazetteer of more than 550,000 entries. However, comparing these gazetteers and ours precisely is difficult at this point because the detailed information such as the precision and the recall of these gazetteers were not reported.19 Recently, Inui et al. (2007) investi19Shinzato et al. (2006) reported some useful statistics about gated the relation between the size and the quality of a gazetteer and its effect. We think this is one of the important directions of future research. Parallelization has recently regained attention in the machine learning community because of the need for learning from very large sets of data. Chu et al. (2006) presented the MapReduce framework for a wide range of machine learning algorithms, including the EM algorithm. Newman et al. (2007) presented parallelized Latent Dirichlet Allocation (LDA). However, these studies f</context>
</contexts>
<marker>Inui, Murakami, Hashimoto, Utsumi, Ishikawa, 2007</marker>
<rawString>T. Inui, K. Murakami, T. Hashimoto, K. Utsumi, and M. Ishikawa. 2007. A study on using gazetteers for organization name recognition. In IPSJ SIG Technical Report 2007-NL-182 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL</booktitle>
<contexts>
<context position="2537" citStr="Kazama and Torisawa (2007)" startWordPosition="382" endWordPosition="385">N)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs</context>
<context position="4979" citStr="Kazama and Torisawa, 2007" startWordPosition="748" endWordPosition="751">lelized the algorithm of (Torisawa, 2001) using the Message Passing Interface (MPI), with the prime goal being to distribute parameters and thus enable clustering with a large vocabulary. Applying the parallelized clustering to a large set of dependencies collected from Web documents enabled us to construct gazetteers with up to 500,000 entries and 3,000 classes. In our experiments, we used the IREX dataset (Sekine and Isahara, 2000) to demonstrate the usefulness of cluster gazetteers. We also compared the cluster gazetteers with the Wikipedia gazetteer constructed by following the method of (Kazama and Torisawa, 2007). The improvement was larger for the cluster gazetteer than for the Wikipedia gazetteer. We also investigated whether these gazetteers improve the accuracies further when they are used in combination. The experimental results indicated that the accuracy improved further in several cases and showed that these gazetteers complement each other. The paper is organized as follows. In Section 2, we explain the construction of cluster gazetteers and its parallelization, along with a brief explanation of the construction of the Wikipedia gazetteer. In Section 3, we explain how to use these gazetteers </context>
<context position="13095" citStr="Kazama and Torisawa (2007)" startWordPosition="2167" endWordPosition="2170">ith named entity entries (Left: car brand names. Right: stadium names). Names are sorted on the basis of p(c1n). Stadium names are examples of multi-word nouns (word boundaries are indicated by “/”) and also include abbreviated expressions (marked by [abb.]) . stage. We have observed 5 times faster execution, when using 8 cluster nodes with a relatively small setting, |N |= |VT |= 50, 000, |C |= 2, 000. 2.4 Induction from Wikipedia Defining sentences in a dictionary or an encyclopedia have long been used as a source of hyponymy relations (Tsurumaru et al., 1991; Herbelot and Copestake, 2006). Kazama and Torisawa (2007) extracted hyponymy relations from the first sentences (i.e., defining sentences) of Wikipedia articles and then used them as a gazetteer for NER. We used this method to construct the Wikipedia gazetteer. The method described by Kazama and Torisawa (2007) is to first extract the first (base) noun phrase after the first “is”, “was”, “are”, or “were” in the first sentence of a Wikipedia article. The last word in the noun phase is then extracted and becomes the hypernym of the entity described by the article. For example, from the following defining sentence, it extracts “guitarist” as the hypern</context>
<context position="15184" citStr="Kazama and Torisawa (2007)" startWordPosition="2507" endWordPosition="2510">n article using a morphological analyzer, MeCab9, we extracted the last noun after the appearance of Japanese postposition “は (wa)” (1/4 “is”). As in the English case, we also refrained from extracting expressions corresponding to “one of” and so on. From the Japanese Wikipedia entries of April 10, 2007, we extracted 550,832 gazetteer entries (482,599 entries have hypernyms other than UNK). Various statistics for this extraction are shown in Table 1. The number of distinct hypernyms in the gazetteer was 12,786. Although this Wikipedia gazetteer is much smaller than the English version used by Kazama and Torisawa (2007) that has over 2,000,000 entries, it is the largest gazetteer that can be freely used for Japanese NER. Our experimental results show that this Wikipedia gazetteer can be used to improve the accuracy of Japanese NER. 3 Using Gazetteers as Features of NER Since Japanese has no spaces between words, there are several choices for the token unit used in NER. Asahara and Motsumoto (2003) proposed using characters instead of morphemes as the unit to alleviate the effect of segmentation errors in morphological analysis and we also used their character-based method. The NER task is then treated as a t</context>
<context position="16491" citStr="Kazama and Torisawa (2007)" startWordPosition="2738" endWordPosition="2741">ditional Random Fields (CRFs) (Lafferty et al., 2001) to perform this tagging. The information of a gazetteer is incorporated 8They handled “redirections” as well by following redirection links and extracting a hypernym from the article reached. 9http://mecab.sourceforge.net 10Precisely, we use IOB2 tags. 410 ch tc Y -- — bi 010 A ··· match O B I I O O O · · · (w/ class) O B-I�R I-°�R I-°�R O O O · · · Figure 3: Gazetteer features for Japanese NER. Here, ‘Y --” means “SONY”, “��” means “company”, and “ DIA” means “to develop”. as features in a CRF-based NE tagger. We follow the method used by Kazama and Torisawa (2007), which encodes the matching with a gazetteer entity using IOB tags, with the modification for Japanese. They describe using two types of gazetteer features. The first is a matching-only feature, which uses bare IOB tags to encode only matching information. The second uses IOB tags that are augmented with classes (e.g., B-country and I-country).11 When there are several possibilities for making a match, the left-most longest match is selected. The small differences from their work are: (1) We used characters as the unit as we described above, (2) While Kazama and Torisawa (2007) checked only t</context>
<context position="24137" citStr="Kazama and Torisawa (2007)" startWordPosition="4019" endWordPosition="4022">uch one-character entries does not affect the accuracy, and in fact, makes it worsen slightly. We think this shows one of the strengths of machine learning methods such as CRFs. We can also see that our current matching method is not an optimal one. For example, 16% of the matches were lost as a result of using our left-most longest heuristics for the case of the c500k-2k gazetteer. A comparison of the effect of these gazetteers is shown in Table 5. The performance is measured by the F-measure. First, the Wikipedia gazetteer improved the accuracy as expected, i.e., it reproduced the result of Kazama and Torisawa (2007) for Japanese NER. The improvement for the testing set was 1.08 points. Second, all the tested cluster gazetteers improved the accuracy. The largest improvement was 1.55 points with the c300k-3k gazetteer. This was larger than that of the Wikipedia gazetteer. The results for c300k-Yk gazetteers show a peak of the improvement at some number of clusters. In this case, |C |= 3, 000 achieved the best improvement. The results of cXk-2k gazetteers in18Wikipedia contains articles explaining each hiragana character, e.g., “あ is a hiragana character”. 412 Name # entries # matches # e-matches # optimal </context>
<context position="28488" citStr="Kazama and Torisawa, 2007" startWordPosition="4736" endWordPosition="4739">y reconfirmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. It uses man IREX σ2 y structural features that are not used in our model. Incorporating 413 such features might improve our model further. 6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other hand, Kazama and Torisawa (2007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by several researchers (Kazama et al., 2001; Miller et al., 2004). They construct</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>J. Kazama and K. Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>A maximum entropy tagger with unsupervised hidden Markov models.</title>
<date>2001</date>
<booktitle>In NLPRS</booktitle>
<contexts>
<context position="2967" citStr="Kazama et al., 2001" startWordPosition="450" endWordPosition="453">extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs and Brown’s algorithm (Brown et al., 1992), seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2Although several categories can be associated in general, we assume that only one category is associated. 407 P</context>
<context position="29050" citStr="Kazama et al., 2001" startWordPosition="4833" endWordPosition="4836">l., 2006; Nadeau et al., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other hand, Kazama and Torisawa (2007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by several researchers (Kazama et al., 2001; Miller et al., 2004). They constructed word clusters by using HMMs or Brown’s clustering algorithm (Brown et al., 1992), which utilize only information from neighboring words. This study, on the other hand, utilized MN clustering based on verbMN dependencies (Rooth et al., 1999; Torisawa, 2001). We showed that gazetteers created by using such richer semantic/syntactic structures improves the accuracy for NER. The size of the gazetteers is also a novel point of this study. The previous studies, with the exception of Kazama and Torisawa (2007), used smaller gazetteers than ours. Shinzato et al</context>
</contexts>
<marker>Kazama, Miyao, Tsujii, 2001</marker>
<rawString>J. Kazama, Y. Miyao, and J. Tsujii. 2001. A maximum entropy tagger with unsupervised hidden Markov models. In NLPRS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In CoNLL</booktitle>
<contexts>
<context position="19599" citStr="Kudo and Matsumoto, 2002" startWordPosition="3243" endWordPosition="3246"> and Hirai, 2004). bh head-of-bunsetsu features. See (Nakano and Hirai, 2004). Table 2: Atomic features used in baseline model. velopment set (1,446 sentences), and the testing set (1,446 sentences). 4.2 Baseline Model We extracted the atomic features listed in Table 2 at each character for our baseline model. Though there may be slight differences, these features are based on the standard ones proposed and used in previous studies on Japanese NER such as those by Asahara and Motsumoto (2003), Nakano and Hirai (2004), and Yamada (2007). We used MeCab as a morphological analyzer and CaboCha14 (Kudo and Matsumoto, 2002) as the dependency parser to find the boundaries of the bunsetsu. We generated the node and the edge features of a CRF model as described in Table 3 using these atomic features. 4.3 Training To train CRF models, we used Taku Kudo’s CRF++ (ver. 0.44) 15 with some modifications.16 We 14http://chasen.org/∼taku/software/ CaboCha 15http://chasen.org/˜taku/software/CRF++ 16We implemented scaling, which is similar to that for HMMs (Rabiner, 1989), in the forward-backward phase and replaced the optimization module in the original package with the 411 Node features: 1»», x−2, x−1, x0, x+1, x+2} x y0 wh</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>T. Kudo and Y. Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In CoNLL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>D Kawahara</author>
</authors>
<date>2005</date>
<note>KNP (KurohashiNagao parser) 2.0 users manual.</note>
<contexts>
<context position="8092" citStr="Kurohashi and Kawahara, 2005" startWordPosition="1287" endWordPosition="1290">where fi is the number of dependency (vti, ni) in a corpus, the EM-based clustering tries to find p(vt|c), p(n|c), and p(c) that maximize the (log)-likelihood of the training examples: LL(p) = � fi log( � p(vti|c)p(ni|c)p(c)). (2) i c 3This formulation is based on the formulation presented in Rooth et al. (1999) for English. argmax c 408 We iteratively update the probabilities using the EM algorithm. For the update procedures used, see Torisawa (2001). The corpus we used for collecting dependencies was a large set (76 million) of Web documents, that were processed by a dependency parser, KNP (Kurohashi and Kawahara, 2005).4 From this corpus, we extracted about 380 million dependencies of the form {(vti, ni, fi)gLi . 2.3 Parallelization for Large-scale Data The disadvantage of the clustering algorithm described above is the computational costs. The space requirements are O(|VT ||C|+|N||C|+|C|) for storing the parameters, p(vt|c), p(n|c), and p(c)5, plus O(L) for storing the training examples. The time complexity is mainly O(L x |C |x I), where I is the number of update iterations. The space requirements are the main limiting factor. Assume that a floating-point number consumes 8 bytes. With the setting, |N |= 5</context>
</contexts>
<marker>Kurohashi, Kawahara, 2005</marker>
<rawString>S. Kurohashi and D. Kawahara. 2005. KNP (KurohashiNagao parser) 2.0 users manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML</booktitle>
<contexts>
<context position="15918" citStr="Lafferty et al., 2001" startWordPosition="2631" endWordPosition="2634">rimental results show that this Wikipedia gazetteer can be used to improve the accuracy of Japanese NER. 3 Using Gazetteers as Features of NER Since Japanese has no spaces between words, there are several choices for the token unit used in NER. Asahara and Motsumoto (2003) proposed using characters instead of morphemes as the unit to alleviate the effect of segmentation errors in morphological analysis and we also used their character-based method. The NER task is then treated as a tagging task, which assigns IOB tags to each character in a sentence.10 We use Conditional Random Fields (CRFs) (Lafferty et al., 2001) to perform this tagging. The information of a gazetteer is incorporated 8They handled “redirections” as well by following redirection links and extracting a hypernym from the article reached. 9http://mecab.sourceforge.net 10Precisely, we use IOB2 tags. 410 ch tc Y -- — bi 010 A ··· match O B I I O O O · · · (w/ class) O B-I�R I-°�R I-°�R O O O · · · Figure 3: Gazetteer features for Japanese NER. Here, ‘Y --” means “SONY”, “��” means “company”, and “ DIA” means “to develop”. as features in a CRF-based NE tagger. We follow the method used by Kazama and Torisawa (2007), which encodes the matchin</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>J Guinness</author>
<author>A Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In HLT-NAACL04.</booktitle>
<contexts>
<context position="2989" citStr="Miller et al., 2004" startWordPosition="454" endWordPosition="457">lations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs and Brown’s algorithm (Brown et al., 1992), seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2Although several categories can be associated in general, we assume that only one category is associated. 407 Proceedings ofACL-08: H</context>
<context position="29072" citStr="Miller et al., 2004" startWordPosition="4837" endWordPosition="4840">l., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other hand, Kazama and Torisawa (2007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by several researchers (Kazama et al., 2001; Miller et al., 2004). They constructed word clusters by using HMMs or Brown’s clustering algorithm (Brown et al., 1992), which utilize only information from neighboring words. This study, on the other hand, utilized MN clustering based on verbMN dependencies (Rooth et al., 1999; Torisawa, 2001). We showed that gazetteers created by using such richer semantic/syntactic structures improves the accuracy for NER. The size of the gazetteers is also a novel point of this study. The previous studies, with the exception of Kazama and Torisawa (2007), used smaller gazetteers than ours. Shinzato et al. (2006) constructed g</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>S. Miller, J. Guinness, and A. Zamanian. 2004. Name tagging with word clusters and discriminative training. In HLT-NAACL04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>Peter D Turney</author>
<author>Stan Matwin</author>
</authors>
<title>Unsupervised named-entity recognition: Generating gazetteers and resolving ambiguity.</title>
<date>2006</date>
<booktitle>In 19th Canadian Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1788" citStr="Nadeau et al., 2006" startWordPosition="259" endWordPosition="262"> Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 19</context>
<context position="28460" citStr="Nadeau et al., 2006" startWordPosition="4732" endWordPosition="4735">These results not only reconfirmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. It uses man IREX σ2 y structural features that are not used in our model. Incorporating 413 such features might improve our model further. 6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other hand, Kazama and Torisawa (2007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by several researchers (Kazama et al., 2001; Miller e</context>
<context position="29847" citStr="Nadeau et al. (2006)" startWordPosition="4961" endWordPosition="4964"> This study, on the other hand, utilized MN clustering based on verbMN dependencies (Rooth et al., 1999; Torisawa, 2001). We showed that gazetteers created by using such richer semantic/syntactic structures improves the accuracy for NER. The size of the gazetteers is also a novel point of this study. The previous studies, with the exception of Kazama and Torisawa (2007), used smaller gazetteers than ours. Shinzato et al. (2006) constructed gazetteers with about 100,000 entries in total for the “restaurant” domain; Talukdar et al. (2006) used gazetteers with about 120,000 entries in total, and Nadeau et al. (2006) used gazetteers with about 85,000 entries in total. By parallelizing the clustering algorithm, we successfully constructed a cluster gazetteer with up to 500,000 entries from a large amount of dependency relations in Web documents. To our knowledge, no one else has performed this type of clustering on such a large scale. Wikipedia also produced a large gazetteer of more than 550,000 entries. However, comparing these gazetteers and ours precisely is difficult at this point because the detailed information such as the precision and the recall of these gazetteers were not reported.19 Recently, I</context>
</contexts>
<marker>Nadeau, Turney, Matwin, 2006</marker>
<rawString>D. Nadeau, Peter D. Turney, and Stan Matwin. 2006. Unsupervised named-entity recognition: Generating gazetteers and resolving ambiguity. In 19th Canadian Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nakano</author>
<author>Y Hirai</author>
</authors>
<title>Japanese named entity extraction with bunsetsu features.</title>
<date>2004</date>
<journal>IPSJ Journal (in Japanese).</journal>
<contexts>
<context position="18931" citStr="Nakano and Hirai, 2004" startWordPosition="3139" endWordPosition="3142">tion ch character itself ct character type: uppercase alphabet, lowercase alphabet, katakana, hiragana, Chinese characters, numbers, numbers in Chinese characters, and spaces m mo bare IOB tag indicating boundaries of morphemes m mm IOB tag augmented by morpheme string, indicating boundaries and morphemes m mp IOB tag augmented by morpheme type, indicating boundaries and morpheme types (POSs) bm bare IOB tag indicating “bunsetsu” boundaries (Bunsetsu is a basic unit in Japanese and usually contains content words followed by function words such as postpositions) bi bunsetsu-inner feature. See (Nakano and Hirai, 2004). bp adjacent-bunsetsu feature. See (Nakano and Hirai, 2004). bh head-of-bunsetsu features. See (Nakano and Hirai, 2004). Table 2: Atomic features used in baseline model. velopment set (1,446 sentences), and the testing set (1,446 sentences). 4.2 Baseline Model We extracted the atomic features listed in Table 2 at each character for our baseline model. Though there may be slight differences, these features are based on the standard ones proposed and used in previous studies on Japanese NER such as those by Asahara and Motsumoto (2003), Nakano and Hirai (2004), and Yamada (2007). We used MeCab </context>
<context position="26789" citStr="Nakano and Hirai, 2004" startWordPosition="4461" endWordPosition="4464">ity of the clustering that results from random initialization. We need to investigate this further. 4.5 Effect of Combining the Cluster and the Wikipedia Gazetteers We have observed that using the cluster gazetteer and the Wikipedia one improves the accuracy of Japanese NER. The next question is whether these gazetteers improve the accuracy further when they are used together. The accuracies of models that use the Wikipedia gazetteer and one of the cluster gazetteers at the same time are shown in Table 6. The accuracy was improved in most cases. HowModel F (Asahara and Motsumoto, 2003) 87.21 (Nakano and Hirai, 2004) 89.03 (Yamada, 2007) 88.33 (Sasano and Kurohashi, 2008) 89.40 proposed (baseline) 87.62 proposed (+wikip) 88.14 proposed (+c300k-3k) 88.45 proposed (+c500k-2k) 88.41 proposed (+wikip+c300k-3k) 88.93 proposed (+wikip+c500k-2k) 88.71 Table 7: Comparison with previous studies a 1.77-point improvement fr om the baseline for the testing set. vious Studies ever, there were some cases where the accuracy for the development set was degraded. Therefore, we should state at this point that while the benefit of combining these gazetteers is not consistent in a strict sense, it seems to exist. The best pe</context>
</contexts>
<marker>Nakano, Hirai, 2004</marker>
<rawString>K. Nakano and Y. Hirai. 2004. Japanese named entity extraction with bunsetsu features. IPSJ Journal (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>A Asuncion</author>
<author>P Smyth</author>
<author>M Welling</author>
</authors>
<title>Distributed inference for latent dirichlet allocation.</title>
<date>2007</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="30981" citStr="Newman et al. (2007)" startWordPosition="5145" endWordPosition="5148">h as the precision and the recall of these gazetteers were not reported.19 Recently, Inui et al. (2007) investi19Shinzato et al. (2006) reported some useful statistics about gated the relation between the size and the quality of a gazetteer and its effect. We think this is one of the important directions of future research. Parallelization has recently regained attention in the machine learning community because of the need for learning from very large sets of data. Chu et al. (2006) presented the MapReduce framework for a wide range of machine learning algorithms, including the EM algorithm. Newman et al. (2007) presented parallelized Latent Dirichlet Allocation (LDA). However, these studies focus on the distribution of the training examples and relevant computation, and ignore the need that we found for the distribution of model parameters. The exception, which we noticed recently, is a study by Wolfe et al. (2007), which describes how each node stores only those parameters relevant to the training data on each node. However, some parameters need to be duplicated and thus their method is less efficient than ours in terms of memory usage. We used the left-most longest heuristics to find the matching </context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2007</marker>
<rawString>D. Newman, A. Asuncion, P. Smyth, and M. Welling. 2007. Distributed inference for latent dirichlet allocation. In NIPS 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="20042" citStr="Rabiner, 1989" startWordPosition="3311" endWordPosition="3312">ch as those by Asahara and Motsumoto (2003), Nakano and Hirai (2004), and Yamada (2007). We used MeCab as a morphological analyzer and CaboCha14 (Kudo and Matsumoto, 2002) as the dependency parser to find the boundaries of the bunsetsu. We generated the node and the edge features of a CRF model as described in Table 3 using these atomic features. 4.3 Training To train CRF models, we used Taku Kudo’s CRF++ (ver. 0.44) 15 with some modifications.16 We 14http://chasen.org/∼taku/software/ CaboCha 15http://chasen.org/˜taku/software/CRF++ 16We implemented scaling, which is similar to that for HMMs (Rabiner, 1989), in the forward-backward phase and replaced the optimization module in the original package with the 411 Node features: 1»», x−2, x−1, x0, x+1, x+2} x y0 where x = ch, ct, m mm, m mo, m mp, bi, bp, and bh Edge features: 1»», x−1, x0, x+1} x y−1 x y0 where x = ch, ct, and m mp Bigram node features: 1x−2x−1, x−1x0, x0x+1} x y0 x = ch, ct, m mo, m mp, bm, bi, bp, and bh Table 3: Baseline features. Value of node feature is determined from current tag, y0, and surface feature (combination of atomic features in Table 2). Value of edge feature is determined by previous tag, y−1, current tag, y0, and</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In 16th National Conference on Artificial Intelligence (AAAI-99).</booktitle>
<contexts>
<context position="1673" citStr="Riloff and Jones, 1999" startWordPosition="238" endWordPosition="241">sing the constructed clusters as a gazetteer (cluster gazetteer) is a effective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explo</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In 16th National Conference on Artificial Intelligence (AAAI-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
<author>S Riezler</author>
<author>D Presher</author>
<author>G Carroll</author>
<author>F Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<contexts>
<context position="2478" citStr="Rooth et al., 1999" startWordPosition="374" endWordPosition="377"> a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazette</context>
<context position="3806" citStr="Rooth et al. (1999)" startWordPosition="574" endWordPosition="577">unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2Although several categories can be associated in general, we assume that only one category is associated. 407 Proceedings ofACL-08: HLT, pages 407–415, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics p(c|n). This syntactic/semantic structures, i.e., verb-MN dependencies to make clean MN clusters. Rooth et al. (1999) and Torisawa (2001) showed that the EMbased clustering using verb-MN dependencies can produce semantically clean MN clusters. However, the clustering algorithms, especially the EM-based algorithms, are computationally expensive. Therefore, performing the clustering with a vocabulary that is large enough to cover the many named entities required to improve the accuracy of NER is difficult. We enabled such large-scale clustering by parallelizing the clustering algorithm, and we demonstrate the usefulness of the gazetteer constructed. We parallelized the algorithm of (Torisawa, 2001) using the M</context>
<context position="7776" citStr="Rooth et al. (1999)" startWordPosition="1236" endWordPosition="1239"> the type of n in the dependent position. In addition, we also treated the MN-MN expressions, “MN1 0) MN2” (Pz� “MN2 of MN1”), as dependencies v = MN2, r = 0), n = MN1, since these expressions also characterize the dependent MNs well. Given L training examples of verb-MN dependencies {(vti, ni, fi)}Li=1, where fi is the number of dependency (vti, ni) in a corpus, the EM-based clustering tries to find p(vt|c), p(n|c), and p(c) that maximize the (log)-likelihood of the training examples: LL(p) = � fi log( � p(vti|c)p(ni|c)p(c)). (2) i c 3This formulation is based on the formulation presented in Rooth et al. (1999) for English. argmax c 408 We iteratively update the probabilities using the EM algorithm. For the update procedures used, see Torisawa (2001). The corpus we used for collecting dependencies was a large set (76 million) of Web documents, that were processed by a dependency parser, KNP (Kurohashi and Kawahara, 2005).4 From this corpus, we extracted about 380 million dependencies of the form {(vti, ni, fi)gLi . 2.3 Parallelization for Large-scale Data The disadvantage of the clustering algorithm described above is the computational costs. The space requirements are O(|VT ||C|+|N||C|+|C|) for sto</context>
<context position="29330" citStr="Rooth et al., 1999" startWordPosition="4879" endWordPosition="4882">zama and Torisawa (2007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by several researchers (Kazama et al., 2001; Miller et al., 2004). They constructed word clusters by using HMMs or Brown’s clustering algorithm (Brown et al., 1992), which utilize only information from neighboring words. This study, on the other hand, utilized MN clustering based on verbMN dependencies (Rooth et al., 1999; Torisawa, 2001). We showed that gazetteers created by using such richer semantic/syntactic structures improves the accuracy for NER. The size of the gazetteers is also a novel point of this study. The previous studies, with the exception of Kazama and Torisawa (2007), used smaller gazetteers than ours. Shinzato et al. (2006) constructed gazetteers with about 100,000 entries in total for the “restaurant” domain; Talukdar et al. (2006) used gazetteers with about 120,000 entries in total, and Nadeau et al. (2006) used gazetteers with about 85,000 entries in total. By parallelizing the clusterin</context>
</contexts>
<marker>Rooth, Riezler, Presher, Carroll, Beil, 1999</marker>
<rawString>M. Rooth, S. Riezler, D. Presher, G. Carroll, and F. Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarawagi</author>
<author>W W Cohen</author>
</authors>
<title>Semi-Markov random fields for information extraction.</title>
<date>2004</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="31817" citStr="Sarawagi and Cohen, 2004" startWordPosition="5279" endWordPosition="5282">on of model parameters. The exception, which we noticed recently, is a study by Wolfe et al. (2007), which describes how each node stores only those parameters relevant to the training data on each node. However, some parameters need to be duplicated and thus their method is less efficient than ours in terms of memory usage. We used the left-most longest heuristics to find the matching gazetteer entries. However, as shown in Table 4 this is not an optimal method. We need more sophisticated matching methods that can handle multiple matching possibilities. Using models such as Semi-Markov CRFs (Sarawagi and Cohen, 2004), which handle the features on overlapping regions, is one possible direction. However, even if we utilize the current gazetteers optimally, the coverage is upper bounded at 70%. To cover most of the named entities in the data, we need much larger gazetteers. A straightforward approach is to increase the number of Web documents used for the MN clustering and to use larger vocabularies. 7 Conclusion We demonstrated that a gazetteer obtained by clustering verb-MN dependencies is a useful feature for a Japanese NER. In addition, we demonstrated that using the cluster gazetteer and the gazetteer e</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>S. Sarawagi and W. W. Cohen. 2004. Semi-Markov random fields for information extraction. In NIPS 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sasano</author>
<author>S Kurohashi</author>
</authors>
<title>Japanese named entity recognition using structural natural language processing.</title>
<date>2008</date>
<booktitle>In IJCNLP</booktitle>
<contexts>
<context position="26845" citStr="Sasano and Kurohashi, 2008" startWordPosition="4469" endWordPosition="4472">alization. We need to investigate this further. 4.5 Effect of Combining the Cluster and the Wikipedia Gazetteers We have observed that using the cluster gazetteer and the Wikipedia one improves the accuracy of Japanese NER. The next question is whether these gazetteers improve the accuracy further when they are used together. The accuracies of models that use the Wikipedia gazetteer and one of the cluster gazetteers at the same time are shown in Table 6. The accuracy was improved in most cases. HowModel F (Asahara and Motsumoto, 2003) 87.21 (Nakano and Hirai, 2004) 89.03 (Yamada, 2007) 88.33 (Sasano and Kurohashi, 2008) 89.40 proposed (baseline) 87.62 proposed (+wikip) 88.14 proposed (+c300k-3k) 88.45 proposed (+c500k-2k) 88.41 proposed (+wikip+c300k-3k) 88.93 proposed (+wikip+c500k-2k) 88.71 Table 7: Comparison with previous studies a 1.77-point improvement fr om the baseline for the testing set. vious Studies ever, there were some cases where the accuracy for the development set was degraded. Therefore, we should state at this point that while the benefit of combining these gazetteers is not consistent in a strict sense, it seems to exist. The best performance, F = 89.26 (dev.) / 89.19 (test.), was achieve</context>
<context position="28094" citStr="Sasano and Kurohashi (2008)" startWordPosition="4669" endWordPosition="4672"> Wikipedia gazetteer and the cluster gazetteer, c500k-2k. This means that there was 5 Comparison with Pre Since many previous studies on Japanese NER used 5-fold cross validation for the dataset, we also performed it for some our models that had the best found in the previous experiments. The results are listed in Table 7 with references to the results of recent studies. These results not only reconfirmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. It uses man IREX σ2 y structural features that are not used in our model. Incorporating 413 such features might improve our model further. 6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On </context>
</contexts>
<marker>Sasano, Kurohashi, 2008</marker>
<rawString>R. Sasano and S. Kurohashi. 2008. Japanese named entity recognition using structural natural language processing. In IJCNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>H Isahara</author>
</authors>
<title>IREX: IR and IE evaluation project in Japanese.</title>
<date>2000</date>
<booktitle>In IREX</booktitle>
<contexts>
<context position="4790" citStr="Sekine and Isahara, 2000" startWordPosition="720" endWordPosition="723">the accuracy of NER is difficult. We enabled such large-scale clustering by parallelizing the clustering algorithm, and we demonstrate the usefulness of the gazetteer constructed. We parallelized the algorithm of (Torisawa, 2001) using the Message Passing Interface (MPI), with the prime goal being to distribute parameters and thus enable clustering with a large vocabulary. Applying the parallelized clustering to a large set of dependencies collected from Web documents enabled us to construct gazetteers with up to 500,000 entries and 3,000 classes. In our experiments, we used the IREX dataset (Sekine and Isahara, 2000) to demonstrate the usefulness of cluster gazetteers. We also compared the cluster gazetteers with the Wikipedia gazetteer constructed by following the method of (Kazama and Torisawa, 2007). The improvement was larger for the cluster gazetteer than for the Wikipedia gazetteer. We also investigated whether these gazetteers improve the accuracies further when they are used in combination. The experimental results indicated that the accuracy improved further in several cases and showed that these gazetteers complement each other. The paper is organized as follows. In Section 2, we explain the con</context>
<context position="17582" citStr="Sekine and Isahara, 2000" startWordPosition="2922" endWordPosition="2925">ces from their work are: (1) We used characters as the unit as we described above, (2) While Kazama and Torisawa (2007) checked only the word sequences that start with a capitalized word and thus exploited the characteristics of English language, we checked the matching at every character, (3) We used a TRIE to make the look-up efficient. The output of gazetteer features for Japanese NER are thus as those shown in Figure 3. These annotated IOB tags can be used in the same way as other features in a CRF tagger. 4 Experiments 4.1 Data We used the CRL NE dataset provided in the IREX competition (Sekine and Isahara, 2000). In the dataset, 1,174 newspaper articles are annotated with 8 NE categories: ARTIFACT, DATE, LOCATION, MONEY, ORGANIZATION, PERCENT, PERSON, and TIME.12 We converted the data into the CoNLL 2003 format, i.e., each row corresponds to a character in this case. We obtained 11,892 sentences13 with 18,677 named entities. We split this data into the training set (9,000 sentences), the de11Here, we call the value returned by a gazetteer a “class”. Features are not output when the returned class is UNK in the case of the Wikipedia gazetteer. We did not observe any significant change if we also used </context>
</contexts>
<marker>Sekine, Isahara, 2000</marker>
<rawString>S. Sekine and H. Isahara. 2000. IREX: IR and IE evaluation project in Japanese. In IREX 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shinzato</author>
<author>K Torisawa</author>
</authors>
<title>Acquiring hyponymy relations from Web documents.</title>
<date>2004</date>
<booktitle>In HLTNAACL</booktitle>
<contexts>
<context position="2420" citStr="Shinzato and Torisawa, 2004" startWordPosition="364" endWordPosition="367">st studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot</context>
</contexts>
<marker>Shinzato, Torisawa, 2004</marker>
<rawString>K. Shinzato and K. Torisawa. 2004. Acquiring hyponymy relations from Web documents. In HLTNAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shinzato</author>
<author>S Sekine</author>
<author>N Yoshinaga</author>
<author>K Torisawa</author>
</authors>
<title>Constructing dictionaries for named entity recognition on specific domains from the Web.</title>
<date>2006</date>
<booktitle>In Web Content Mining with Human Language Technologies Workshop on the 5th International Semantic Web.</booktitle>
<contexts>
<context position="1743" citStr="Shinzato et al., 2006" startWordPosition="251" endWordPosition="254">ffective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatic</context>
<context position="28416" citStr="Shinzato et al., 2006" startWordPosition="4724" endWordPosition="4727"> references to the results of recent studies. These results not only reconfirmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. It uses man IREX σ2 y structural features that are not used in our model. Incorporating 413 such features might improve our model further. 6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other hand, Kazama and Torisawa (2007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by severa</context>
<context position="29658" citStr="Shinzato et al. (2006)" startWordPosition="4931" endWordPosition="4934">a et al., 2001; Miller et al., 2004). They constructed word clusters by using HMMs or Brown’s clustering algorithm (Brown et al., 1992), which utilize only information from neighboring words. This study, on the other hand, utilized MN clustering based on verbMN dependencies (Rooth et al., 1999; Torisawa, 2001). We showed that gazetteers created by using such richer semantic/syntactic structures improves the accuracy for NER. The size of the gazetteers is also a novel point of this study. The previous studies, with the exception of Kazama and Torisawa (2007), used smaller gazetteers than ours. Shinzato et al. (2006) constructed gazetteers with about 100,000 entries in total for the “restaurant” domain; Talukdar et al. (2006) used gazetteers with about 120,000 entries in total, and Nadeau et al. (2006) used gazetteers with about 85,000 entries in total. By parallelizing the clustering algorithm, we successfully constructed a cluster gazetteer with up to 500,000 entries from a large amount of dependency relations in Web documents. To our knowledge, no one else has performed this type of clustering on such a large scale. Wikipedia also produced a large gazetteer of more than 550,000 entries. However, compar</context>
</contexts>
<marker>Shinzato, Sekine, Yoshinaga, Torisawa, 2006</marker>
<rawString>K. Shinzato, S. Sekine, N. Yoshinaga, and K. Torisawa. 2006. Constructing dictionaries for named entity recognition on specific domains from the Web. In Web Content Mining with Human Language Technologies Workshop on the 5th International Semantic Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Talukdar</author>
<author>T Brants</author>
<author>M Liberman</author>
<author>F Pereira</author>
</authors>
<title>A context pattern induction method for named entity extraction. In CoNLL</title>
<date>2006</date>
<contexts>
<context position="1766" citStr="Talukdar et al., 2006" startWordPosition="255" endWordPosition="258">ng the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy</context>
<context position="28439" citStr="Talukdar et al., 2006" startWordPosition="4728" endWordPosition="4731">lts of recent studies. These results not only reconfirmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. It uses man IREX σ2 y structural features that are not used in our model. Incorporating 413 such features might improve our model further. 6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other hand, Kazama and Torisawa (2007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by several researchers (Kazama e</context>
<context position="29769" citStr="Talukdar et al. (2006)" startWordPosition="4948" endWordPosition="4951">thm (Brown et al., 1992), which utilize only information from neighboring words. This study, on the other hand, utilized MN clustering based on verbMN dependencies (Rooth et al., 1999; Torisawa, 2001). We showed that gazetteers created by using such richer semantic/syntactic structures improves the accuracy for NER. The size of the gazetteers is also a novel point of this study. The previous studies, with the exception of Kazama and Torisawa (2007), used smaller gazetteers than ours. Shinzato et al. (2006) constructed gazetteers with about 100,000 entries in total for the “restaurant” domain; Talukdar et al. (2006) used gazetteers with about 120,000 entries in total, and Nadeau et al. (2006) used gazetteers with about 85,000 entries in total. By parallelizing the clustering algorithm, we successfully constructed a cluster gazetteer with up to 500,000 entries from a large amount of dependency relations in Web documents. To our knowledge, no one else has performed this type of clustering on such a large scale. Wikipedia also produced a large gazetteer of more than 550,000 entries. However, comparing these gazetteers and ours precisely is difficult at this point because the detailed information such as the</context>
</contexts>
<marker>Talukdar, Brants, Liberman, Pereira, 2006</marker>
<rawString>P. P. Talukdar, T. Brants, M. Liberman, and F. Pereira. 2006. A context pattern induction method for named entity extraction. In CoNLL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern context.</title>
<date>2002</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="1698" citStr="Thelen and Riloff, 2002" startWordPosition="242" endWordPosition="246">sters as a gazetteer (cluster gazetteer) is a effective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of </context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern context. In EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torisawa</author>
</authors>
<title>An unsupervised method for canonicalization of Japanese postpositions.</title>
<date>2001</date>
<booktitle>In NLPRS</booktitle>
<contexts>
<context position="2495" citStr="Torisawa, 2001" startWordPosition="378" endWordPosition="379">pping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition,</context>
<context position="3826" citStr="Torisawa (2001)" startWordPosition="579" endWordPosition="580">ture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2Although several categories can be associated in general, we assume that only one category is associated. 407 Proceedings ofACL-08: HLT, pages 407–415, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics p(c|n). This syntactic/semantic structures, i.e., verb-MN dependencies to make clean MN clusters. Rooth et al. (1999) and Torisawa (2001) showed that the EMbased clustering using verb-MN dependencies can produce semantically clean MN clusters. However, the clustering algorithms, especially the EM-based algorithms, are computationally expensive. Therefore, performing the clustering with a vocabulary that is large enough to cover the many named entities required to improve the accuracy of NER is difficult. We enabled such large-scale clustering by parallelizing the clustering algorithm, and we demonstrate the usefulness of the gazetteer constructed. We parallelized the algorithm of (Torisawa, 2001) using the Message Passing Inter</context>
<context position="6448" citStr="Torisawa (2001)" startWordPosition="1002" endWordPosition="1003">and c E C is a class. We can use this model to construct a gazetteer in several ways. The method we used in this study constructs a gazetteer: n —* argmax c computation can be re-written by the Bayes rule as p(n|c)p(c) using p(n|c) and p(c). Note that we do not exclude non-NEs when we construct the gazetteer. We expect that tagging models (CRFs in our case) can learn an appropriate weight for each gazetteer match regardless of whether it is an NE or not. 2.2 EM-based Clustering using Dependency Relations To learn p(n|c) and p(c) for Japanese, we use the EM-based clustering method presented by Torisawa (2001). This method assumes a probabilistic model of verb-MN dependencies with hidden semantic classes:3 �p(v, r, n) = p((v, r)|c)p(n|c)p(c), (1) c where v E V is a verb and n E N is an MN that depends on verb v with relation r. A relation, r, is represented by Japanese postpositions attached to n. For example, from the following Japanese sentence, we extract the following dependency: v = Mt (drink), r = (”wo” postposition), n = t�—)L (beer). t�—)L (beer) � (wo) Mt (drink) (Pz� drink beer) In the following, we let vt = (v, r) E V7 for the simplicity of explanation. To be precise, we attach various a</context>
<context position="7918" citStr="Torisawa (2001)" startWordPosition="1260" endWordPosition="1262"> MN2, r = 0), n = MN1, since these expressions also characterize the dependent MNs well. Given L training examples of verb-MN dependencies {(vti, ni, fi)}Li=1, where fi is the number of dependency (vti, ni) in a corpus, the EM-based clustering tries to find p(vt|c), p(n|c), and p(c) that maximize the (log)-likelihood of the training examples: LL(p) = � fi log( � p(vti|c)p(ni|c)p(c)). (2) i c 3This formulation is based on the formulation presented in Rooth et al. (1999) for English. argmax c 408 We iteratively update the probabilities using the EM algorithm. For the update procedures used, see Torisawa (2001). The corpus we used for collecting dependencies was a large set (76 million) of Web documents, that were processed by a dependency parser, KNP (Kurohashi and Kawahara, 2005).4 From this corpus, we extracted about 380 million dependencies of the form {(vti, ni, fi)gLi . 2.3 Parallelization for Large-scale Data The disadvantage of the clustering algorithm described above is the computational costs. The space requirements are O(|VT ||C|+|N||C|+|C|) for storing the parameters, p(vt|c), p(n|c), and p(c)5, plus O(L) for storing the training examples. The time complexity is mainly O(L x |C |x I), wh</context>
<context position="29347" citStr="Torisawa, 2001" startWordPosition="4883" endWordPosition="4884">007) extracted hyponymy relations, which are independent of the NE categories, from Wikipedia and utilized it as a gazetteer. The effectiveness of this method was demonstrated for Japanese NER as well by this study. Inducing features for taggers by clustering has been tried by several researchers (Kazama et al., 2001; Miller et al., 2004). They constructed word clusters by using HMMs or Brown’s clustering algorithm (Brown et al., 1992), which utilize only information from neighboring words. This study, on the other hand, utilized MN clustering based on verbMN dependencies (Rooth et al., 1999; Torisawa, 2001). We showed that gazetteers created by using such richer semantic/syntactic structures improves the accuracy for NER. The size of the gazetteers is also a novel point of this study. The previous studies, with the exception of Kazama and Torisawa (2007), used smaller gazetteers than ours. Shinzato et al. (2006) constructed gazetteers with about 100,000 entries in total for the “restaurant” domain; Talukdar et al. (2006) used gazetteers with about 120,000 entries in total, and Nadeau et al. (2006) used gazetteers with about 85,000 entries in total. By parallelizing the clustering algorithm, we s</context>
</contexts>
<marker>Torisawa, 2001</marker>
<rawString>K. Torisawa. 2001. An unsupervised method for canonicalization of Japanese postpositions. In NLPRS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tsurumaru</author>
<author>K Takeshita</author>
<author>K Iami</author>
<author>T Yanagawa</author>
<author>S Yoshida</author>
</authors>
<title>An approach to thesaurus construction from Japanese language dictionary.</title>
<date>1991</date>
<booktitle>In IPSJ SIG Notes Natural Language</booktitle>
<note>vol.83-16, (in Japanese).</note>
<contexts>
<context position="13036" citStr="Tsurumaru et al., 1991" startWordPosition="2158" endWordPosition="2161">(Yokohama Stadium [abb.]) Figure 2: Clean MN clusters with named entity entries (Left: car brand names. Right: stadium names). Names are sorted on the basis of p(c1n). Stadium names are examples of multi-word nouns (word boundaries are indicated by “/”) and also include abbreviated expressions (marked by [abb.]) . stage. We have observed 5 times faster execution, when using 8 cluster nodes with a relatively small setting, |N |= |VT |= 50, 000, |C |= 2, 000. 2.4 Induction from Wikipedia Defining sentences in a dictionary or an encyclopedia have long been used as a source of hyponymy relations (Tsurumaru et al., 1991; Herbelot and Copestake, 2006). Kazama and Torisawa (2007) extracted hyponymy relations from the first sentences (i.e., defining sentences) of Wikipedia articles and then used them as a gazetteer for NER. We used this method to construct the Wikipedia gazetteer. The method described by Kazama and Torisawa (2007) is to first extract the first (base) noun phrase after the first “is”, “was”, “are”, or “were” in the first sentence of a Wikipedia article. The last word in the noun phase is then extracted and becomes the hypernym of the entity described by the article. For example, from the followi</context>
</contexts>
<marker>Tsurumaru, Takeshita, Iami, Yanagawa, Yoshida, 1991</marker>
<rawString>H. Tsurumaru, K. Takeshita, K. Iami, T. Yanagawa, and S. Yoshida. 1991. An approach to thesaurus construction from Japanese language dictionary. In IPSJ SIG Notes Natural Language vol.83-16, (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wolfe</author>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Fully distributed EM for very large datasets.</title>
<date>2007</date>
<booktitle>In NIPS Workshop on Efficient Machine Learning.</booktitle>
<contexts>
<context position="31291" citStr="Wolfe et al. (2007)" startWordPosition="5194" endWordPosition="5197">ture research. Parallelization has recently regained attention in the machine learning community because of the need for learning from very large sets of data. Chu et al. (2006) presented the MapReduce framework for a wide range of machine learning algorithms, including the EM algorithm. Newman et al. (2007) presented parallelized Latent Dirichlet Allocation (LDA). However, these studies focus on the distribution of the training examples and relevant computation, and ignore the need that we found for the distribution of model parameters. The exception, which we noticed recently, is a study by Wolfe et al. (2007), which describes how each node stores only those parameters relevant to the training data on each node. However, some parameters need to be duplicated and thus their method is less efficient than ours in terms of memory usage. We used the left-most longest heuristics to find the matching gazetteer entries. However, as shown in Table 4 this is not an optimal method. We need more sophisticated matching methods that can handle multiple matching possibilities. Using models such as Semi-Markov CRFs (Sarawagi and Cohen, 2004), which handle the features on overlapping regions, is one possible direct</context>
</contexts>
<marker>Wolfe, Haghighi, Klein, 2007</marker>
<rawString>J. Wolfe, A. Haghighi, and D. Klein. 2007. Fully distributed EM for very large datasets. In NIPS Workshop on Efficient Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
</authors>
<title>Shift-reduce chunking for Japanese named entity extraction.</title>
<date>2007</date>
<booktitle>In ISPJ SIG</booktitle>
<tech>Technical Report 2007-NL-179.</tech>
<contexts>
<context position="19515" citStr="Yamada (2007)" startWordPosition="3232" endWordPosition="3233"> See (Nakano and Hirai, 2004). bp adjacent-bunsetsu feature. See (Nakano and Hirai, 2004). bh head-of-bunsetsu features. See (Nakano and Hirai, 2004). Table 2: Atomic features used in baseline model. velopment set (1,446 sentences), and the testing set (1,446 sentences). 4.2 Baseline Model We extracted the atomic features listed in Table 2 at each character for our baseline model. Though there may be slight differences, these features are based on the standard ones proposed and used in previous studies on Japanese NER such as those by Asahara and Motsumoto (2003), Nakano and Hirai (2004), and Yamada (2007). We used MeCab as a morphological analyzer and CaboCha14 (Kudo and Matsumoto, 2002) as the dependency parser to find the boundaries of the bunsetsu. We generated the node and the edge features of a CRF model as described in Table 3 using these atomic features. 4.3 Training To train CRF models, we used Taku Kudo’s CRF++ (ver. 0.44) 15 with some modifications.16 We 14http://chasen.org/∼taku/software/ CaboCha 15http://chasen.org/˜taku/software/CRF++ 16We implemented scaling, which is similar to that for HMMs (Rabiner, 1989), in the forward-backward phase and replaced the optimization module in t</context>
<context position="26810" citStr="Yamada, 2007" startWordPosition="4466" endWordPosition="4467">lts from random initialization. We need to investigate this further. 4.5 Effect of Combining the Cluster and the Wikipedia Gazetteers We have observed that using the cluster gazetteer and the Wikipedia one improves the accuracy of Japanese NER. The next question is whether these gazetteers improve the accuracy further when they are used together. The accuracies of models that use the Wikipedia gazetteer and one of the cluster gazetteers at the same time are shown in Table 6. The accuracy was improved in most cases. HowModel F (Asahara and Motsumoto, 2003) 87.21 (Nakano and Hirai, 2004) 89.03 (Yamada, 2007) 88.33 (Sasano and Kurohashi, 2008) 89.40 proposed (baseline) 87.62 proposed (+wikip) 88.14 proposed (+c300k-3k) 88.45 proposed (+c500k-2k) 88.41 proposed (+wikip+c300k-3k) 88.93 proposed (+wikip+c500k-2k) 88.71 Table 7: Comparison with previous studies a 1.77-point improvement fr om the baseline for the testing set. vious Studies ever, there were some cases where the accuracy for the development set was degraded. Therefore, we should state at this point that while the benefit of combining these gazetteers is not consistent in a strict sense, it seems to exist. The best performance, F = 89.26 </context>
</contexts>
<marker>Yamada, 2007</marker>
<rawString>H. Yamada. 2007. Shift-reduce chunking for Japanese named entity extraction. In ISPJ SIG Technical Report 2007-NL-179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>