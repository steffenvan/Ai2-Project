<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.658844666666667">
Last Words
Amazon Mechanical Turk:
Gold Mine or Coal Mine?
</title>
<author confidence="0.729625">
Kar¨en Fort*
</author>
<figure confidence="0.788435666666667">
INIST-CNRS/LIPN
Gilles Adda**
LIMSI/CNRS
</figure>
<author confidence="0.826109">
K. Bretonnel Cohen†
</author>
<affiliation confidence="0.9752795">
University of Colorado School of Medicine
and University of Colorado at Boulder
</affiliation>
<bodyText confidence="0.999645">
Recently heard at a tutorial in our field: “It cost me less than one hundred bucks to
annotate this using Amazon Mechanical Turk!” Assertions like this are increasingly
common, but we believe they should not be stated so proudly; they ignore the ethical
consequences of using MTurk (Amazon Mechanical Turk) as a source of labor.
Manually annotating corpora or manually developing any other linguistic resource,
such as a set of judgments about system outputs, represents such a high cost that many
researchers are looking for alternative solutions to the standard approach. MTurk is
becoming a popular one. However, as in any scientific endeavor involving humans,
there is an unspoken ethical dimension involved in resource construction and system
evaluation, and this is especially true of MTurk.
We would like here to raise some questions about the use of MTurk. To do so, we
will define precisely what MTurk is and what it is not, highlighting the issues raised
by the system. We hope that this will point out opportunities for our community to
deliberately value ethics above cost savings.
</bodyText>
<subsectionHeader confidence="0.541856">
What Is MTurk? What Is It Not?
</subsectionHeader>
<bodyText confidence="0.99982275">
MTurk is an on-line crowdsourcing, microworking1 system which enables elementary
tasks to be performed by a huge number of people (typically called “Turkers”) on-line.
Ideally, these tasks are meant to be solved by computers, but they still remain out of
computational reach (for instance, the translation of an English sentence into Urdu).
</bodyText>
<note confidence="0.651493">
* INIST-CNRS/LIPN, 2 all´ee de Brabois, F-54500 Vandoeuvre-l`es-Nancy, France.
</note>
<email confidence="0.632278">
E-mail: karen.fort®inist.fr.
</email>
<note confidence="0.793756">
** LIMSI/CNRS, Rue John von Neumann, Universit´e Paris-Sud F-91403 ORSAY, France.
</note>
<email confidence="0.445688">
E-mail: gilles.adda®limsi.fr.
</email>
<footnote confidence="0.89092">
† Center for Computational Pharmacology, University of Colorado School of Medicine, University of
Colorado at Boulder. E-mail: kevin.cohen®gmail.com.
1 Microworking refers to situations where tasks are cut into small pieces and their execution is paid for.
Crowdsourcing refers to situations where the job is outsourced on the Web and done by many people
(paid or not).
</footnote>
<note confidence="0.7891995">
© 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 2
</note>
<figureCaption confidence="0.985542">
Figure 1
</figureCaption>
<bodyText confidence="0.99604594117647">
Evolution of MTurk usage in NLP publications.
MTurk is composed of two populations: the Requesters, who launch the tasks to be
completed, and the Turkers, who complete these tasks. Requesters create the so-called
“HITs” (Human Intelligence Tasks), which are elementary components of complex
tasks. The art of the Requesters is to split complex tasks into basic steps and to fix a
reward, usually very low (for instance US$0.05 to translate a sentence). Using the MTurk
paradigm, language resources can be produced at a fraction (1/10th at least) of the usual
cost (Callison-Burch and Dredze 2010).
MTurk should therefore not be considered to be a game. Although it is superficially
similar to Phrase Detectives, in that case the gain is not emphasized (only the best
contributors gain a prize, which consists of Amazon vouchers). The same applies to
the French-language JeuxDeMots (“Play on Words”), which does not offer any prize
(Lafourcade 2007), and to Phrase Detectives (Chamberlain, Poesio, and Kruschwitz
2008), in which the gain is not emphasized (only the best contributors gain a prize).
MTurk is not a game or a social network, it is an unregulated labor marketplace:
a system which deliberately does not pay fair wages, does not pay due taxes, and
provides no protections for workers.
</bodyText>
<subsectionHeader confidence="0.992426">
Why Are We Concerned?
</subsectionHeader>
<bodyText confidence="0.999906">
Since its introduction in 2005, there has been a steadily growing use of MTurk in
building or validating NLP resources, and most of the main scientific conferences in our
field include papers involving MTurk. Figure 1 was created by automatically searching
the proceedings of some of the main speech and language processing conferences, as
well as some smaller events specializing in linguistic resources, using the quoted phrase
“Mechanical Turk.” We then manually checked the retrieved articles, source by source,
to identify those which really make use of MTurk, ignoring those which simply talk
about it. (For example, in the LREC 2010 proceedings, eight articles talk about MTurk,
but only five used it, and in 2008, out of two papers citing MTurk, only one used it.)
The present journal, Computational Linguistics (CL), appears in the bar chart with a
</bodyText>
<page confidence="0.996853">
414
</page>
<note confidence="0.7131">
Fort, Adda, and Cohen Amazon Mechanical Turk: Gold Mine or Coal Mine?
</note>
<bodyText confidence="0.999955869565217">
zero count, as none of the articles published in it so far mention MTurk.2 All of the
other sources contained at least one article per year using MTurk. The total number
of publications varies from year to year, since, for example, conferences may accept
different numbers of papers each year, and some conferences, such as LREC, occur only
every two years.
We performed another, less detailed, search, this time in the whole ACL Anthology
(not source by source), using the same quoted phrase “Mechanical Turk“ on 5 Novem-
ber 2010. We examined the hits manually, and out of the 124 resulting hits, 86 were
papers in which the authors actually used MTurk as part of their research methodology.
Interestingly, we noticed that at least one paper that we know to have used MTurk (i.e.,
Biadsy, Hirschberg, and Filatova 2008), was not returned by the search. The published
version of this paper does not explicitly mention MTurk, but the corresponding pre-
sentation at the conference indicated that MTurk was used. This is some evidence that
use of MTurk may be under-reported. It should be noted that these results include a
specialized workshop—the NAACL-HLT 2010 Workshop on Amazon Mechanical Turk
(35 papers)—the existence of which is, in itself, strong evidence of the importance of the
use of MTurk in the domain.
A vast majority of papers present small to medium size experiments where the
authors have been able to produce linguistic resources or perform evaluations at a very
low cost; at least for transcription and translation, the quality is sufficient to train and
evaluate statistical translation/transcription systems (Callison-Burch and Dredze 2010;
Marge, Banerjee, and Rudnicky 2010). Some of these papers, however, bring to light
language resource quality problems. For example, Tratz and Hovy (2010, page 684) note
that the user interface limitations constitute “[t]he first and most significant drawback”
of MTurk, as, in their context of annotating noun compound relations using a large
taxonomy, “it is impossible to force each Turker to label every data point without
putting all the terms onto a single Web page, which is highly impractical for a large
taxonomy. Some Turkers may label every compound, but most do not.” They also
note that ”while we requested that Turkers only work on our task if English was their
first language, we had no method of enforcing this.” Finally, they note that “Turker
annotation quality varies considerably.” Another important point is made in Bhardwaj
et al. (2010), where it is shown that, for their task of word sense disambiguation, a small
number of trained annotators are superior to a larger number of untrained Turkers.
On that point, their results contradict that of Snow et al. (2008), whose task was much
simpler (the number of senses per word was 3 for the latter, versus 9.5 for the former).
The difficulty of having Turkers perform complex tasks also appears in Gillick and Liu
(2010, page 148), an article from the proceedings of the NAACL-HLT 2010 Workshop on
Amazon Mechanical Turk, in which non-expert evaluation of summarization systems
is proved to be “not able to recover system rankings derived from experts.” Even
more interestingly, Wais et al. (2010) show that standard machine learning techniques
(in their case, a naive Bayes classifier) can outperfom the Turkers on a categorization
task (classifying businesses into Automotive, Health, Real Estate, etc.). Therefore, in
some cases, NLP tools already do better than MTurk. Finally, as we said earlier, the
vast majority of papers present only small or medium size experiments. This can be
explained by the fact that, at least according to Ipeirotis (2010a), submitting large jobs
in MTurk results in low quality and unpredictable completion time.
</bodyText>
<footnote confidence="0.551802">
2 Note that one article in this particular issue of CL uses MTurk (Taboada et al., this issue).
</footnote>
<page confidence="0.988724">
415
</page>
<figure confidence="0.356165">
Computational Linguistics Volume 37, Number 2
</figure>
<subsectionHeader confidence="0.62229">
Who Are the Turkers?
</subsectionHeader>
<bodyText confidence="0.999969324324325">
Many people conceive of MTurk as a transposition of Grid Computing to humans, thus
making it possible to benefit from humans’ “spare cycles” to develop a virtual computer
of unlimited power. The assumption is that there is no inconvenience for humans (as it
is not real work), and the power comes from the myriad. This a fiction.
Let us look first at how many Turkers are performing the HITs. This is a quite
difficult task, because Amazon does not give access to many figures about them. We
know that over 500k people are registered as Turkers in the MTurk system. But how
many Turkers are really performing HITs? To evaluate this, we combined two different
sources of information. First, we have access to some surveys about the demographics
of the Turkers (Ross et al. 2009, 2010; Ipeirotis 2010b). These surveys may have a bias
over the real population of Turkers, as some Turkers may be reluctant to respond to
surveys. Because the results of these surveys are quite consistent, and the surveys
are usually easy to complete, not particularly boring, and paid above the usual rate,
we may assume that this bias is minor, and accept what they say as a good picture
of the population of Turkers. In these surveys we see many interesting things. For
instance, there is a growing number of people from India: There were below 10%
in 2008, above 33% in early 2010, and they represented about 50% of the Turkers in
May 2010.3 Even if these surveys show that the populations from India and the U.S.
are quite different, we may take as an approximation that they have about the same
reasons to perform HITs in MTurk, and produce about the same activity. We looked
at how many HITs the 1,000 Turkers who completed the survey in Ipeirotis (2010b)
claim to perform: between 138,654 and 395,106 HITs per week.4 The second source of
information comes from the Mechanical Turk Tracker:5 According to this, 700,000 HITs
are performed each week. But the tracker system neither keeps track of the HITs which
are completed in less than one hour, nor is able to quantify the fact that the same HIT
can be completed by multiple workers and in fact should be, according to regular users
like Callison-Burch and Dredze (2010). Asking the authors of Ipeirotis (2010b), and the
creator of the Mechanical Turk Tracker (who are in fact the same person), he suggested
that we should multiply the number given by the tracker by 1.7 × 5 to take into
account these two factors,6 resulting in the (conjectural) total number of 5,950,000 HITs.
Taking those two data points,7 we are able to hypothesize that the real number of
Turkers is between 15,059 and 42,912. However, from the surveys, we have access to
another figure: Eighty percent (80%) of the HITs are performed by the 20% most active
Turkers (Deneme 2009), who spend more than 15 hours per week in the MTurk system
(Adda and Mariani 2010)—consistent with the Pareto principle which says that 80%
of the effects come from 20% of the causes. We may therefore say that 80% of the
HITs are performed by 3,011 to 8,582 Turkers. These figures represent 0.6–1.7% of the
</bodyText>
<footnote confidence="0.9929629">
3 http://blog.crowdflower.com/2010/05/amazon-mechanical-turk-survey/.
4 The two figures come from the fact that each Turker gave a range of activity rather than an average
number of HITs.
5 This system keeps track of all the HITs posted on MTurk, each hour. http://mturk-tracker.com.
6 Personal communication in the comments of http://behind-the-enemy-lines.blogspot.com/
2010/03/new-demographics-of-mechanical-turk.html, reporting that the tracker is missing ∼70
of the posted HITs, which are posted and completed within less than one hour, and a 5x factor for the
unobserved HIT redundancy.
7 That is, 1,000 Turkers perform between 138,654 and 395,106 HITs per week, and the total number of HITs
in the MTurk system is about 5.95M HITs per week.
</footnote>
<page confidence="0.992122">
416
</page>
<note confidence="0.864337">
Fort, Adda, and Cohen Amazon Mechanical Turk: Gold Mine or Coal Mine?
</note>
<bodyText confidence="0.9992385">
registered Turkers, which in turn is in accord with the “90-9-1” rule8 valid in the
Internet culture.
Another important question is whether activity in MTurk should be considered
as labor or something else (hobby, volunteer work, etc.). The observed mean hourly
wages for performing jobs in the MTurk system is below US$2 (US$1.25 according to
Ross et al. [2009]). Because they accept such low rewards, a common assumption is
that Turkers are U.S. students or stay-at-home mothers who have plenty of leisure time
and are happy to fill their recreation time by making some extra money. According to
recent studies in the social sciences (Ipeirotis 2010b; Ross et al. 2010), it is quite true
that a majority (60%) of Turkers think that MTurk is a fruitful way to spend free time
getting some cash; but it is only 20% (5% of the India Turkers) who say that they use it to
kill time. And these studies also show that 20% (30% of the India Turkers) declare that
they use MTurk “to make basic ends meet.” From these answers, we find that money is
an important motivation for a majority of the Turkers (20% use MTurk as their primary
source of income, and 50% as their secondary source of income), and leisure is important
for only a minority (30%). We cannot conclude from these studies that the activity in
MTurk should be considered as labor for all the Turkers, but we can at least for the
minority (20%) for whom MTurk represents a primary source of income.9 Moreover,
using the survey in Ipeirotis (2010b), we find that this minority is performing more that
one third of all the HITs.
</bodyText>
<subsectionHeader confidence="0.93826">
What Are the Issues with MTurk?
</subsectionHeader>
<bodyText confidence="0.996341238095238">
The very low wages (below US$2 an hour) are a first issue, but the use of Mechanical
Turk raises other ethical issues as well. The position of many prototypical Turkers would
be considered ethically unacceptable in major developed countries. Denied even the
basic workplace right of collective bargaining (unionization), this community has no
recourse to any channels for redress of employer wrongdoing, let alone the normal ones
available to any typical worker in the United States and many other developed nations
(e.g., class action lawsuits, other lawsuits, and complaints to government agencies),
while simultaneously being subjected to egregious vulnerabilities, including the fact
that they have no guarantee of payment for work properly performed.
Legal issues surrounding the use of MTurk have also been encountered. At least
one university legal department was sufficiently concerned that Turkers working for
several months would claim employee status and demand health and other benefits that
they refused to allow grant funds to be expended on MTurk (personal communication,
E. Hovy). A small number of universities have insisted on institutional review board
approval for MTurk experiments (personal communication, K. Cohen). (Institutional
review boards in U.S. universities are independent bodies that review proposed experi-
ments for legal and ethical issues.)
Is MTurk the Future of Linguistic Resource Development?
The implicit belief that the very low cost of MTurk derives from the fact that incentiviz-
ing casual hobbyists requires only minimal payment is a mirage: Once you admit that a
majority of Turkers do not consider MTurk as a hobby, but as a primary or a secondary
</bodyText>
<footnote confidence="0.865996">
8 http://en.wikipedia.org/wiki/1%25 rule (Internet culture).
9 And even for the 50% who are utilizing MTurk as a secondary source of income.
</footnote>
<page confidence="0.986263">
417
</page>
<note confidence="0.485975">
Computational Linguistics Volume 37, Number 2
</note>
<bodyText confidence="0.999947166666667">
source of income, and that one third of the HITs are performed by Turkers who need
MTurk to make basic ends meet, you then have to admit that MTurk is, at least for
them, a labor marketplace. Moreover, the frequent assumption that the low rewards are
a result of the classical law of supply-and-demand (large numbers of Turkers means
more supply of labor and therefore lower acceptable salaries) is false. Firstly, we do
not observe that there are too many Turkers. In fact, there are not enough Turkers. This
can be observed through the difficulty in finding Turkers with certain abilities (e.g.,
understanding a specific language [Novotney and Callison-Burch 2010]), and in the
difficulty in performing very large HIT groups (Ipeirotis 2010a). This is not surprising,
as we have seen that the number of active Turkers is not that large. Secondly, the low
cost is a result of the Requesters’ view of the relation between quality and reward: Many
articles (e.g., Marge, Banerjee, and Rudnicky 2010) relate that there is no correlation
between the reward and the final quality. The reason is that increasing the price is
believed to attract spammers (i.e., Turkers who cheat, not really performing the job,
but using robots or answering randomly), and these are numerous in the MTurk system
because of an inadequate worker reputation system.10 We obtain here a schema which
is very close to what the 2001 economics Nobel prize winner George Akerlof calls “the
market for lemons,” where asymmetric information in a market results in “the bad
driving out the good.” He takes the market for used cars as an example (Akerlof 1970),
where owners of good cars (here, good workers) will not place their cars on the used
car market, because of the existence of many cars in bad shape (here, the spammers),
which encourage the buyer (here, the Requester) to offer a low price (here, the reward)
because he does not know the exact value of the car. After some time, the good workers
leave the market because they are not able to earn enough money given the work done
(and sometimes they are not even paid), which in turn decreases the quality. At the
moment, the system is stable in terms of the number of Turkers, because good workers
are replaced by naive workers.
Amazon’s attitude towards reputational issues has been passive. It maintains that it
is a neutral clearinghouse for labor, in which all else is the responsibility of the two con-
senting parties. This attitude has led to an explosion of micro-crowdsourcing start-ups,
which observed the MTurk flaws and tried to overcome them.11 Some of these start-ups
could become serious alternatives to MTurk (TheQuill 2010), like Samasource,12 which
offers at least a fair wage to workers, who in turn are clearly identified on the Web site,
with their resumes. But others are even worse than MTurk, ethically speaking. MTurk
is ethically questionable enough; as a scientific community with ethical responsibilities
we should seek to minimize the existence of even less-ethical alternatives to it.
</bodyText>
<subsectionHeader confidence="0.499769">
What’s Next?
</subsectionHeader>
<bodyText confidence="0.99961825">
If we persist in claiming that with MTurk we are now able to produce any linguistic
resource or perform any manual evaluation of output at a very low cost, funding
agencies will come to expect it. It is predictable that in assessing projects involving
linguistic resource production or manual evaluation of output, funding agencies will
</bodyText>
<footnote confidence="0.461210666666667">
10 For more details, see http://behind-the-enemy-lines.blogspot.com/2010/10/be-top-mechanical-
turk-worker-you-need.html.
11 For instance, Agent Anything, Clickworker, CloudCrowd, CrowdFlower, DoMyWork, JobBoy, LiveWork,
Microtask, microWorkers, MiniFreelance, MiniJobz, MinuteWorkers, MyEasyTask, MyMicroJob, OpTask,
RapidWorkers, Samasource, ShortTask, SimpleWorkers, SmartSheet, and so forth.
12 http://www.samasource.org.
</footnote>
<page confidence="0.986">
418
</page>
<note confidence="0.893648">
Fort, Adda, and Cohen Amazon Mechanical Turk: Gold Mine or Coal Mine?
</note>
<bodyText confidence="0.999965904761905">
prefer projects which propose to produce 10 or 100 times more data for the same
amount of money. MTurk costs will then become the standard costs, and it will be very
difficult to obtain funding for a project involving linguistic resource production at any
level that would allow for more traditional, non-crowdsourced resource construction
methodologies. Therefore, our community’s use of MTurk not only supports a work-
place model that is unfair and open to abuses of a variety of sorts, but also creates a
de facto standard for the development of linguistic resources that may have long-term
funding consequences.
Non-exploitative methods for decreasing the cost of linguistic resource devel-
opment exist. They include semi-automatic processing, better methodologies and
tools, and games with a purpose, as well as microworking Web sites (like Samasource)
that guarantee workers minimum payment levels. We encourage the computational
linguistics and NLP communities to keep these alternatives in mind when planning
experiments. If a microworking system is considered desirable by the ACL and
ISCA communities, then we also suggest that they explore the creation and use of a
linguistically specialized special-purpose microworking alternative to MTurk that
both ensures linguistic quality and holds itself to the highest ethical standards of
employer/employee relationships. Through our work as grant evaluators and recipi-
ents, we should also encourage funding bodies to require institutional review board
approval for crowdsourced experiments and to insist on adherence to fair labor prac-
tices in such work.
</bodyText>
<sectionHeader confidence="0.987178" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991804">
We would like to thank Sophie Rosset,
Joseph Mariani, Panos Ipeirotis, Eduard
Hovy, and Robert Dale for their suggestions
and encouragements. Any remaining errors
are our own.
</bodyText>
<sectionHeader confidence="0.993597" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996498192982456">
Adda, Gilles and Joseph Mariani. 2010.
Language resources and Amazon
Mechanical Turk: Legal, ethical and
other issues. In LISLR2010, “Legal Issues
for Sharing Language Resources workshop,”
LREC2010, Malta.
Akerlof, George A. 1970. The market for
‘lemons’: Quality uncertainty and the
market mechanism. Quarterly Journal of
Economics, 84(3):488–500.
Bhardwaj, Vikas, Rebecca Passonneau,
Ansaf Salleb-Aouissi, and Nancy Ide.
2010. Anveshan: A tool for analysis of
multiple annotators’ labeling behavior.
In Proceedings of The Fourth Linguistic
Annotation Workshop (LAW IV),
pages 47–55, Uppsala.
Biadsy, Fadi, Julia Hirschberg, and Elena
Filatova. 2008. An unsupervised approach
to biography production using Wikipedia.
In Proceedings of ACL 2008, pages 807–815,
Columbus.
Callison-Burch, Chris and Mark Dredze.
2010. Creating speech and language data
with Amazon’s Mechanical Turk. In
CSLDAMT ’10: Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and
Language Data with Amazon’s Mechanical
Turk, pages 1–12, Morristown, NJ.
Chamberlain, J., M. Poesio, and
U. Kruschwitz. 2008. Phrase detectives:
a Web-based collaborative annotation
game. In Proceedings of the International
Conference on Semantic Systems
(I-Semantics’08), pages 42–49, Graz.
Deneme. 2009. How many Turkers are there?
Available at http://groups.csail.mit.
edu/uid/deneme/?p=502. Accessed
December 2009.
Gillick, Dan and Yang Liu. 2010. Non-expert
evaluation of summarization systems is
risky. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and
Language Data with Amazon’s Mechanical
Turk, CSLDAMT ’10, pages 148–151,
Stroudsburg, PA.
Ipeirotis, Panos. 2010a. Analyzing the
Amazon Mechanical Turk marketplace.
CeDER Working Papers No. CeDER-10-04.
Available at http://hdl.handle.net/
2451/29801.
Ipeirotis, Panos. 2010b. Demographics
of Mechanical Turk. CeDER Working
Papers No. CeDER-10-01. Available at
http://hdl.handle.net/2451/29585.
Lafourcade, Mathieu. 2007. Making people
play for lexical acquisition. In Proceedings
</reference>
<page confidence="0.995198">
419
</page>
<note confidence="0.512232">
Computational Linguistics Volume 37, Number 2
</note>
<reference confidence="0.996188982758621">
of SNLP 2007, 7th Symposium on Natural
Language Processing, Pattaya.
Marge, Matthew, Satanjeev Banerjee, and
Alexander I. Rudnicky. 2010. Using the
Amazon Mechanical Turk for transcription
of spoken language. In IEEE International
Conference on Acoustics Speech and Signal
Processing (ICASSP), pages 5270–5273,
Dallas, TX.
Novotney, Scott and Chris Callison-Burch.
2010. Cheap, fast and good enough:
Automatic speech recognition with
non-expert transcription. In Human
Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
HLT ’10, pages 207–215, Morristown, NJ.
Ross, Joel, Lilly Irani, M. Six Silberman,
Andrew Zaldivar, and Bill Tomlinson.
2010. Who are the crowdworkers? Shifting
demographics in Mechanical Turk. In
Proceedings of the 28th of the International
Conference Extended Abstracts on Human
Factors in Computing Systems, CHI EA ’10,
pages 2863–2872, New York, NY.
Ross, Joel, Andrew Zaldivar, Lilly Irani,
and Bill Tomlinson. 2009. Who are the
Turkers? Worker demographics in
Amazon Mechanical Turk. Social
Code Report 2009-01. Available at
http://www.ics.uci.edu/∼jwross/pubs/
SocialCode-2009-01.pdf.
Snow, Rion, Brendan O’Connor, Daniel
Jurafsky, and Andrew Y. Ng. 2008. Cheap
and fast—but is it good? Evaluating
non-expert annotations for natural
language tasks. In Proceedings of EMNLP
2008, pages 254–263, Waikiki.
TheQuill. 2010. Making money on-line,
part 2: Microworking. Available at
http://thequill.org/personal-
finance/9-making-money-on-
line-part-2-microworking-.html.
Accessed 21 June 2010.
Tratz, Stephen and Eduard Hovy. 2010.
A taxonomy, dataset, and classifier for
automatic noun compound interpretation.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 678–687, Uppsala.
Wais, Paul, Shivaram Lingamneni,
Duncan Cook, Jason Fennell, Benjamin
Goldenberg, Daniel Lubarov, David Marin,
and Hari Simons. 2010. Towards building a
high-quality workforce with Mechanical
Turk. In Proceedings of Computational Social
Science and the Wisdom of Crowds (NIPS),
pages 1–5, Whister.
</reference>
<page confidence="0.99837">
420
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.086064">
<title confidence="0.8260354">Last Words Amazon Mechanical Turk: Gold Mine or Coal Mine? INIST-CNRS/LIPN LIMSI/CNRS</title>
<author confidence="0.761562">Bretonnel</author>
<affiliation confidence="0.993453">University of Colorado School of Medicine and University of Colorado at Boulder</affiliation>
<abstract confidence="0.985802631578947">Recently heard at a tutorial in our field: “It cost me less than one hundred bucks to annotate this using Amazon Mechanical Turk!” Assertions like this are increasingly common, but we believe they should not be stated so proudly; they ignore the ethical consequences of using MTurk (Amazon Mechanical Turk) as a source of labor. Manually annotating corpora or manually developing any other linguistic resource, such as a set of judgments about system outputs, represents such a high cost that many researchers are looking for alternative solutions to the standard approach. MTurk is becoming a popular one. However, as in any scientific endeavor involving humans, there is an unspoken ethical dimension involved in resource construction and system evaluation, and this is especially true of MTurk. We would like here to raise some questions about the use of MTurk. To do so, we will define precisely what MTurk is and what it is not, highlighting the issues raised by the system. We hope that this will point out opportunities for our community to deliberately value ethics above cost savings. What Is MTurk? What Is It Not? is an on-line crowdsourcing, system which enables elementary tasks to be performed by a huge number of people (typically called “Turkers”) on-line. Ideally, these tasks are meant to be solved by computers, but they still remain out of computational reach (for instance, the translation of an English sentence into Urdu).</abstract>
<note confidence="0.5608963">2 all´ee de Brabois, F-54500 Vandoeuvre-l`es-Nancy, France. Rue John von Neumann, Universit´e Paris-Sud F-91403 ORSAY, France. for Computational Pharmacology, University of Colorado School of Medicine, University of at Boulder. E-mail: to situations where tasks are cut into small pieces and their execution is paid for. to situations where the job is outsourced on the Web and done by many people (paid or not). © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 2 Figure 1</note>
<abstract confidence="0.996321823529412">Evolution of MTurk usage in NLP publications. MTurk is composed of two populations: the Requesters, who launch the tasks to be completed, and the Turkers, who complete these tasks. Requesters create the so-called “HITs” (Human Intelligence Tasks), which are elementary components of complex tasks. The art of the Requesters is to split complex tasks into basic steps and to fix a reward, usually very low (for instance US$0.05 to translate a sentence). Using the MTurk paradigm, language resources can be produced at a fraction (1/10th at least) of the usual cost (Callison-Burch and Dredze 2010). MTurk should therefore not be considered to be a game. Although it is superficially similar to Phrase Detectives, in that case the gain is not emphasized (only the best contributors gain a prize, which consists of Amazon vouchers). The same applies to the French-language JeuxDeMots (“Play on Words”), which does not offer any prize (Lafourcade 2007), and to Phrase Detectives (Chamberlain, Poesio, and Kruschwitz 2008), in which the gain is not emphasized (only the best contributors gain a prize). MTurk is not a game or a social network, it is an unregulated labor marketplace: a system which deliberately does not pay fair wages, does not pay due taxes, and provides no protections for workers.</abstract>
<intro confidence="0.799365">Why Are We Concerned?</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gilles Adda</author>
<author>Joseph Mariani</author>
</authors>
<title>Language resources and Amazon Mechanical Turk: Legal, ethical and other issues.</title>
<date>2010</date>
<booktitle>In LISLR2010, “Legal Issues for Sharing Language Resources workshop,” LREC2010,</booktitle>
<contexts>
<context position="11438" citStr="Adda and Mariani 2010" startWordPosition="1861" endWordPosition="1864"> and the creator of the Mechanical Turk Tracker (who are in fact the same person), he suggested that we should multiply the number given by the tracker by 1.7 × 5 to take into account these two factors,6 resulting in the (conjectural) total number of 5,950,000 HITs. Taking those two data points,7 we are able to hypothesize that the real number of Turkers is between 15,059 and 42,912. However, from the surveys, we have access to another figure: Eighty percent (80%) of the HITs are performed by the 20% most active Turkers (Deneme 2009), who spend more than 15 hours per week in the MTurk system (Adda and Mariani 2010)—consistent with the Pareto principle which says that 80% of the effects come from 20% of the causes. We may therefore say that 80% of the HITs are performed by 3,011 to 8,582 Turkers. These figures represent 0.6–1.7% of the 3 http://blog.crowdflower.com/2010/05/amazon-mechanical-turk-survey/. 4 The two figures come from the fact that each Turker gave a range of activity rather than an average number of HITs. 5 This system keeps track of all the HITs posted on MTurk, each hour. http://mturk-tracker.com. 6 Personal communication in the comments of http://behind-the-enemy-lines.blogspot.com/ 201</context>
</contexts>
<marker>Adda, Mariani, 2010</marker>
<rawString>Adda, Gilles and Joseph Mariani. 2010. Language resources and Amazon Mechanical Turk: Legal, ethical and other issues. In LISLR2010, “Legal Issues for Sharing Language Resources workshop,” LREC2010, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Akerlof</author>
</authors>
<title>The market for ‘lemons’: Quality uncertainty and the market mechanism.</title>
<date>1970</date>
<journal>Quarterly Journal of Economics,</journal>
<volume>84</volume>
<issue>3</issue>
<contexts>
<context position="17610" citStr="Akerlof 1970" startWordPosition="2865" endWordPosition="2866">here is no correlation between the reward and the final quality. The reason is that increasing the price is believed to attract spammers (i.e., Turkers who cheat, not really performing the job, but using robots or answering randomly), and these are numerous in the MTurk system because of an inadequate worker reputation system.10 We obtain here a schema which is very close to what the 2001 economics Nobel prize winner George Akerlof calls “the market for lemons,” where asymmetric information in a market results in “the bad driving out the good.” He takes the market for used cars as an example (Akerlof 1970), where owners of good cars (here, good workers) will not place their cars on the used car market, because of the existence of many cars in bad shape (here, the spammers), which encourage the buyer (here, the Requester) to offer a low price (here, the reward) because he does not know the exact value of the car. After some time, the good workers leave the market because they are not able to earn enough money given the work done (and sometimes they are not even paid), which in turn decreases the quality. At the moment, the system is stable in terms of the number of Turkers, because good workers </context>
</contexts>
<marker>Akerlof, 1970</marker>
<rawString>Akerlof, George A. 1970. The market for ‘lemons’: Quality uncertainty and the market mechanism. Quarterly Journal of Economics, 84(3):488–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Bhardwaj</author>
<author>Rebecca Passonneau</author>
<author>Ansaf Salleb-Aouissi</author>
<author>Nancy Ide</author>
</authors>
<title>Anveshan: A tool for analysis of multiple annotators’ labeling behavior.</title>
<date>2010</date>
<booktitle>In Proceedings of The Fourth Linguistic Annotation Workshop (LAW IV),</booktitle>
<pages>47--55</pages>
<location>Uppsala.</location>
<contexts>
<context position="7141" citStr="Bhardwaj et al. (2010)" startWordPosition="1130" endWordPosition="1133">t significant drawback” of MTurk, as, in their context of annotating noun compound relations using a large taxonomy, “it is impossible to force each Turker to label every data point without putting all the terms onto a single Web page, which is highly impractical for a large taxonomy. Some Turkers may label every compound, but most do not.” They also note that ”while we requested that Turkers only work on our task if English was their first language, we had no method of enforcing this.” Finally, they note that “Turker annotation quality varies considerably.” Another important point is made in Bhardwaj et al. (2010), where it is shown that, for their task of word sense disambiguation, a small number of trained annotators are superior to a larger number of untrained Turkers. On that point, their results contradict that of Snow et al. (2008), whose task was much simpler (the number of senses per word was 3 for the latter, versus 9.5 for the former). The difficulty of having Turkers perform complex tasks also appears in Gillick and Liu (2010, page 148), an article from the proceedings of the NAACL-HLT 2010 Workshop on Amazon Mechanical Turk, in which non-expert evaluation of summarization systems is proved </context>
</contexts>
<marker>Bhardwaj, Passonneau, Salleb-Aouissi, Ide, 2010</marker>
<rawString>Bhardwaj, Vikas, Rebecca Passonneau, Ansaf Salleb-Aouissi, and Nancy Ide. 2010. Anveshan: A tool for analysis of multiple annotators’ labeling behavior. In Proceedings of The Fourth Linguistic Annotation Workshop (LAW IV), pages 47–55, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fadi Biadsy</author>
<author>Julia Hirschberg</author>
<author>Elena Filatova</author>
</authors>
<title>An unsupervised approach to biography production using Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>807--815</pages>
<location>Columbus.</location>
<marker>Biadsy, Hirschberg, Filatova, 2008</marker>
<rawString>Biadsy, Fadi, Julia Hirschberg, and Elena Filatova. 2008. An unsupervised approach to biography production using Wikipedia. In Proceedings of ACL 2008, pages 807–815, Columbus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In CSLDAMT ’10: Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="2969" citStr="Callison-Burch and Dredze 2010" startWordPosition="453" endWordPosition="456">stics Volume 37, Number 2 Figure 1 Evolution of MTurk usage in NLP publications. MTurk is composed of two populations: the Requesters, who launch the tasks to be completed, and the Turkers, who complete these tasks. Requesters create the so-called “HITs” (Human Intelligence Tasks), which are elementary components of complex tasks. The art of the Requesters is to split complex tasks into basic steps and to fix a reward, usually very low (for instance US$0.05 to translate a sentence). Using the MTurk paradigm, language resources can be produced at a fraction (1/10th at least) of the usual cost (Callison-Burch and Dredze 2010). MTurk should therefore not be considered to be a game. Although it is superficially similar to Phrase Detectives, in that case the gain is not emphasized (only the best contributors gain a prize, which consists of Amazon vouchers). The same applies to the French-language JeuxDeMots (“Play on Words”), which does not offer any prize (Lafourcade 2007), and to Phrase Detectives (Chamberlain, Poesio, and Kruschwitz 2008), in which the gain is not emphasized (only the best contributors gain a prize). MTurk is not a game or a social network, it is an unregulated labor marketplace: a system which de</context>
<context position="6281" citStr="Callison-Burch and Dredze 2010" startWordPosition="991" endWordPosition="994">ce that use of MTurk may be under-reported. It should be noted that these results include a specialized workshop—the NAACL-HLT 2010 Workshop on Amazon Mechanical Turk (35 papers)—the existence of which is, in itself, strong evidence of the importance of the use of MTurk in the domain. A vast majority of papers present small to medium size experiments where the authors have been able to produce linguistic resources or perform evaluations at a very low cost; at least for transcription and translation, the quality is sufficient to train and evaluate statistical translation/transcription systems (Callison-Burch and Dredze 2010; Marge, Banerjee, and Rudnicky 2010). Some of these papers, however, bring to light language resource quality problems. For example, Tratz and Hovy (2010, page 684) note that the user interface limitations constitute “[t]he first and most significant drawback” of MTurk, as, in their context of annotating noun compound relations using a large taxonomy, “it is impossible to force each Turker to label every data point without putting all the terms onto a single Web page, which is highly impractical for a large taxonomy. Some Turkers may label every compound, but most do not.” They also note that</context>
<context position="10774" citStr="Callison-Burch and Dredze (2010)" startWordPosition="1745" endWordPosition="1748">ame reasons to perform HITs in MTurk, and produce about the same activity. We looked at how many HITs the 1,000 Turkers who completed the survey in Ipeirotis (2010b) claim to perform: between 138,654 and 395,106 HITs per week.4 The second source of information comes from the Mechanical Turk Tracker:5 According to this, 700,000 HITs are performed each week. But the tracker system neither keeps track of the HITs which are completed in less than one hour, nor is able to quantify the fact that the same HIT can be completed by multiple workers and in fact should be, according to regular users like Callison-Burch and Dredze (2010). Asking the authors of Ipeirotis (2010b), and the creator of the Mechanical Turk Tracker (who are in fact the same person), he suggested that we should multiply the number given by the tracker by 1.7 × 5 to take into account these two factors,6 resulting in the (conjectural) total number of 5,950,000 HITs. Taking those two data points,7 we are able to hypothesize that the real number of Turkers is between 15,059 and 42,912. However, from the surveys, we have access to another figure: Eighty percent (80%) of the HITs are performed by the 20% most active Turkers (Deneme 2009), who spend more th</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Callison-Burch, Chris and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In CSLDAMT ’10: Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chamberlain</author>
<author>M Poesio</author>
<author>U Kruschwitz</author>
</authors>
<title>Phrase detectives: a Web-based collaborative annotation game.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Semantic Systems (I-Semantics’08),</booktitle>
<pages>42--49</pages>
<location>Graz.</location>
<marker>Chamberlain, Poesio, Kruschwitz, 2008</marker>
<rawString>Chamberlain, J., M. Poesio, and U. Kruschwitz. 2008. Phrase detectives: a Web-based collaborative annotation game. In Proceedings of the International Conference on Semantic Systems (I-Semantics’08), pages 42–49, Graz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deneme</author>
</authors>
<title>How many Turkers are there? Available at http://groups.csail.mit.</title>
<date>2009</date>
<booktitle>edu/uid/deneme/?p=502. Accessed</booktitle>
<contexts>
<context position="11355" citStr="Deneme 2009" startWordPosition="1847" endWordPosition="1848">allison-Burch and Dredze (2010). Asking the authors of Ipeirotis (2010b), and the creator of the Mechanical Turk Tracker (who are in fact the same person), he suggested that we should multiply the number given by the tracker by 1.7 × 5 to take into account these two factors,6 resulting in the (conjectural) total number of 5,950,000 HITs. Taking those two data points,7 we are able to hypothesize that the real number of Turkers is between 15,059 and 42,912. However, from the surveys, we have access to another figure: Eighty percent (80%) of the HITs are performed by the 20% most active Turkers (Deneme 2009), who spend more than 15 hours per week in the MTurk system (Adda and Mariani 2010)—consistent with the Pareto principle which says that 80% of the effects come from 20% of the causes. We may therefore say that 80% of the HITs are performed by 3,011 to 8,582 Turkers. These figures represent 0.6–1.7% of the 3 http://blog.crowdflower.com/2010/05/amazon-mechanical-turk-survey/. 4 The two figures come from the fact that each Turker gave a range of activity rather than an average number of HITs. 5 This system keeps track of all the HITs posted on MTurk, each hour. http://mturk-tracker.com. 6 Person</context>
</contexts>
<marker>Deneme, 2009</marker>
<rawString>Deneme. 2009. How many Turkers are there? Available at http://groups.csail.mit. edu/uid/deneme/?p=502. Accessed December 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Yang Liu</author>
</authors>
<title>Non-expert evaluation of summarization systems is risky.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10,</booktitle>
<pages>148--151</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="7572" citStr="Gillick and Liu (2010" startWordPosition="1205" endWordPosition="1208"> their first language, we had no method of enforcing this.” Finally, they note that “Turker annotation quality varies considerably.” Another important point is made in Bhardwaj et al. (2010), where it is shown that, for their task of word sense disambiguation, a small number of trained annotators are superior to a larger number of untrained Turkers. On that point, their results contradict that of Snow et al. (2008), whose task was much simpler (the number of senses per word was 3 for the latter, versus 9.5 for the former). The difficulty of having Turkers perform complex tasks also appears in Gillick and Liu (2010, page 148), an article from the proceedings of the NAACL-HLT 2010 Workshop on Amazon Mechanical Turk, in which non-expert evaluation of summarization systems is proved to be “not able to recover system rankings derived from experts.” Even more interestingly, Wais et al. (2010) show that standard machine learning techniques (in their case, a naive Bayes classifier) can outperfom the Turkers on a categorization task (classifying businesses into Automotive, Health, Real Estate, etc.). Therefore, in some cases, NLP tools already do better than MTurk. Finally, as we said earlier, the vast majority</context>
</contexts>
<marker>Gillick, Liu, 2010</marker>
<rawString>Gillick, Dan and Yang Liu. 2010. Non-expert evaluation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10, pages 148–151, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panos Ipeirotis</author>
</authors>
<title>Analyzing the Amazon Mechanical Turk marketplace. CeDER Working Papers No. CeDER-10-04. Available at http://hdl.handle.net/</title>
<date>2010</date>
<pages>2451--29801</pages>
<contexts>
<context position="8307" citStr="Ipeirotis (2010" startWordPosition="1321" endWordPosition="1322">rt evaluation of summarization systems is proved to be “not able to recover system rankings derived from experts.” Even more interestingly, Wais et al. (2010) show that standard machine learning techniques (in their case, a naive Bayes classifier) can outperfom the Turkers on a categorization task (classifying businesses into Automotive, Health, Real Estate, etc.). Therefore, in some cases, NLP tools already do better than MTurk. Finally, as we said earlier, the vast majority of papers present only small or medium size experiments. This can be explained by the fact that, at least according to Ipeirotis (2010a), submitting large jobs in MTurk results in low quality and unpredictable completion time. 2 Note that one article in this particular issue of CL uses MTurk (Taboada et al., this issue). 415 Computational Linguistics Volume 37, Number 2 Who Are the Turkers? Many people conceive of MTurk as a transposition of Grid Computing to humans, thus making it possible to benefit from humans’ “spare cycles” to develop a virtual computer of unlimited power. The assumption is that there is no inconvenience for humans (as it is not real work), and the power comes from the myriad. This a fiction. Let us loo</context>
<context position="10305" citStr="Ipeirotis (2010" startWordPosition="1668" endWordPosition="1669">, and accept what they say as a good picture of the population of Turkers. In these surveys we see many interesting things. For instance, there is a growing number of people from India: There were below 10% in 2008, above 33% in early 2010, and they represented about 50% of the Turkers in May 2010.3 Even if these surveys show that the populations from India and the U.S. are quite different, we may take as an approximation that they have about the same reasons to perform HITs in MTurk, and produce about the same activity. We looked at how many HITs the 1,000 Turkers who completed the survey in Ipeirotis (2010b) claim to perform: between 138,654 and 395,106 HITs per week.4 The second source of information comes from the Mechanical Turk Tracker:5 According to this, 700,000 HITs are performed each week. But the tracker system neither keeps track of the HITs which are completed in less than one hour, nor is able to quantify the fact that the same HIT can be completed by multiple workers and in fact should be, according to regular users like Callison-Burch and Dredze (2010). Asking the authors of Ipeirotis (2010b), and the creator of the Mechanical Turk Tracker (who are in fact the same person), he sug</context>
<context position="13136" citStr="Ipeirotis 2010" startWordPosition="2129" endWordPosition="2130">in accord with the “90-9-1” rule8 valid in the Internet culture. Another important question is whether activity in MTurk should be considered as labor or something else (hobby, volunteer work, etc.). The observed mean hourly wages for performing jobs in the MTurk system is below US$2 (US$1.25 according to Ross et al. [2009]). Because they accept such low rewards, a common assumption is that Turkers are U.S. students or stay-at-home mothers who have plenty of leisure time and are happy to fill their recreation time by making some extra money. According to recent studies in the social sciences (Ipeirotis 2010b; Ross et al. 2010), it is quite true that a majority (60%) of Turkers think that MTurk is a fruitful way to spend free time getting some cash; but it is only 20% (5% of the India Turkers) who say that they use it to kill time. And these studies also show that 20% (30% of the India Turkers) declare that they use MTurk “to make basic ends meet.” From these answers, we find that money is an important motivation for a majority of the Turkers (20% use MTurk as their primary source of income, and 50% as their secondary source of income), and leisure is important for only a minority (30%). We canno</context>
<context position="16727" citStr="Ipeirotis 2010" startWordPosition="2716" endWordPosition="2717">to admit that MTurk is, at least for them, a labor marketplace. Moreover, the frequent assumption that the low rewards are a result of the classical law of supply-and-demand (large numbers of Turkers means more supply of labor and therefore lower acceptable salaries) is false. Firstly, we do not observe that there are too many Turkers. In fact, there are not enough Turkers. This can be observed through the difficulty in finding Turkers with certain abilities (e.g., understanding a specific language [Novotney and Callison-Burch 2010]), and in the difficulty in performing very large HIT groups (Ipeirotis 2010a). This is not surprising, as we have seen that the number of active Turkers is not that large. Secondly, the low cost is a result of the Requesters’ view of the relation between quality and reward: Many articles (e.g., Marge, Banerjee, and Rudnicky 2010) relate that there is no correlation between the reward and the final quality. The reason is that increasing the price is believed to attract spammers (i.e., Turkers who cheat, not really performing the job, but using robots or answering randomly), and these are numerous in the MTurk system because of an inadequate worker reputation system.10</context>
</contexts>
<marker>Ipeirotis, 2010</marker>
<rawString>Ipeirotis, Panos. 2010a. Analyzing the Amazon Mechanical Turk marketplace. CeDER Working Papers No. CeDER-10-04. Available at http://hdl.handle.net/ 2451/29801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panos Ipeirotis</author>
</authors>
<title>Demographics of Mechanical Turk.</title>
<date>2010</date>
<tech>CeDER Working Papers No. CeDER-10-01.</tech>
<note>Available at http://hdl.handle.net/2451/29585.</note>
<contexts>
<context position="8307" citStr="Ipeirotis (2010" startWordPosition="1321" endWordPosition="1322">rt evaluation of summarization systems is proved to be “not able to recover system rankings derived from experts.” Even more interestingly, Wais et al. (2010) show that standard machine learning techniques (in their case, a naive Bayes classifier) can outperfom the Turkers on a categorization task (classifying businesses into Automotive, Health, Real Estate, etc.). Therefore, in some cases, NLP tools already do better than MTurk. Finally, as we said earlier, the vast majority of papers present only small or medium size experiments. This can be explained by the fact that, at least according to Ipeirotis (2010a), submitting large jobs in MTurk results in low quality and unpredictable completion time. 2 Note that one article in this particular issue of CL uses MTurk (Taboada et al., this issue). 415 Computational Linguistics Volume 37, Number 2 Who Are the Turkers? Many people conceive of MTurk as a transposition of Grid Computing to humans, thus making it possible to benefit from humans’ “spare cycles” to develop a virtual computer of unlimited power. The assumption is that there is no inconvenience for humans (as it is not real work), and the power comes from the myriad. This a fiction. Let us loo</context>
<context position="10305" citStr="Ipeirotis (2010" startWordPosition="1668" endWordPosition="1669">, and accept what they say as a good picture of the population of Turkers. In these surveys we see many interesting things. For instance, there is a growing number of people from India: There were below 10% in 2008, above 33% in early 2010, and they represented about 50% of the Turkers in May 2010.3 Even if these surveys show that the populations from India and the U.S. are quite different, we may take as an approximation that they have about the same reasons to perform HITs in MTurk, and produce about the same activity. We looked at how many HITs the 1,000 Turkers who completed the survey in Ipeirotis (2010b) claim to perform: between 138,654 and 395,106 HITs per week.4 The second source of information comes from the Mechanical Turk Tracker:5 According to this, 700,000 HITs are performed each week. But the tracker system neither keeps track of the HITs which are completed in less than one hour, nor is able to quantify the fact that the same HIT can be completed by multiple workers and in fact should be, according to regular users like Callison-Burch and Dredze (2010). Asking the authors of Ipeirotis (2010b), and the creator of the Mechanical Turk Tracker (who are in fact the same person), he sug</context>
<context position="13136" citStr="Ipeirotis 2010" startWordPosition="2129" endWordPosition="2130">in accord with the “90-9-1” rule8 valid in the Internet culture. Another important question is whether activity in MTurk should be considered as labor or something else (hobby, volunteer work, etc.). The observed mean hourly wages for performing jobs in the MTurk system is below US$2 (US$1.25 according to Ross et al. [2009]). Because they accept such low rewards, a common assumption is that Turkers are U.S. students or stay-at-home mothers who have plenty of leisure time and are happy to fill their recreation time by making some extra money. According to recent studies in the social sciences (Ipeirotis 2010b; Ross et al. 2010), it is quite true that a majority (60%) of Turkers think that MTurk is a fruitful way to spend free time getting some cash; but it is only 20% (5% of the India Turkers) who say that they use it to kill time. And these studies also show that 20% (30% of the India Turkers) declare that they use MTurk “to make basic ends meet.” From these answers, we find that money is an important motivation for a majority of the Turkers (20% use MTurk as their primary source of income, and 50% as their secondary source of income), and leisure is important for only a minority (30%). We canno</context>
<context position="16727" citStr="Ipeirotis 2010" startWordPosition="2716" endWordPosition="2717">to admit that MTurk is, at least for them, a labor marketplace. Moreover, the frequent assumption that the low rewards are a result of the classical law of supply-and-demand (large numbers of Turkers means more supply of labor and therefore lower acceptable salaries) is false. Firstly, we do not observe that there are too many Turkers. In fact, there are not enough Turkers. This can be observed through the difficulty in finding Turkers with certain abilities (e.g., understanding a specific language [Novotney and Callison-Burch 2010]), and in the difficulty in performing very large HIT groups (Ipeirotis 2010a). This is not surprising, as we have seen that the number of active Turkers is not that large. Secondly, the low cost is a result of the Requesters’ view of the relation between quality and reward: Many articles (e.g., Marge, Banerjee, and Rudnicky 2010) relate that there is no correlation between the reward and the final quality. The reason is that increasing the price is believed to attract spammers (i.e., Turkers who cheat, not really performing the job, but using robots or answering randomly), and these are numerous in the MTurk system because of an inadequate worker reputation system.10</context>
</contexts>
<marker>Ipeirotis, 2010</marker>
<rawString>Ipeirotis, Panos. 2010b. Demographics of Mechanical Turk. CeDER Working Papers No. CeDER-10-01. Available at http://hdl.handle.net/2451/29585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathieu Lafourcade</author>
</authors>
<title>Making people play for lexical acquisition.</title>
<date>2007</date>
<booktitle>In Proceedings of SNLP 2007, 7th Symposium on Natural Language Processing,</booktitle>
<location>Pattaya.</location>
<contexts>
<context position="3321" citStr="Lafourcade 2007" startWordPosition="511" endWordPosition="512">lit complex tasks into basic steps and to fix a reward, usually very low (for instance US$0.05 to translate a sentence). Using the MTurk paradigm, language resources can be produced at a fraction (1/10th at least) of the usual cost (Callison-Burch and Dredze 2010). MTurk should therefore not be considered to be a game. Although it is superficially similar to Phrase Detectives, in that case the gain is not emphasized (only the best contributors gain a prize, which consists of Amazon vouchers). The same applies to the French-language JeuxDeMots (“Play on Words”), which does not offer any prize (Lafourcade 2007), and to Phrase Detectives (Chamberlain, Poesio, and Kruschwitz 2008), in which the gain is not emphasized (only the best contributors gain a prize). MTurk is not a game or a social network, it is an unregulated labor marketplace: a system which deliberately does not pay fair wages, does not pay due taxes, and provides no protections for workers. Why Are We Concerned? Since its introduction in 2005, there has been a steadily growing use of MTurk in building or validating NLP resources, and most of the main scientific conferences in our field include papers involving MTurk. Figure 1 was created</context>
</contexts>
<marker>Lafourcade, 2007</marker>
<rawString>Lafourcade, Mathieu. 2007. Making people play for lexical acquisition. In Proceedings of SNLP 2007, 7th Symposium on Natural Language Processing, Pattaya.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Marge</author>
<author>Satanjeev Banerjee</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Using the Amazon Mechanical Turk for transcription of spoken language.</title>
<date>2010</date>
<booktitle>In IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP),</booktitle>
<pages>5270--5273</pages>
<location>Dallas, TX.</location>
<marker>Marge, Banerjee, Rudnicky, 2010</marker>
<rawString>Marge, Matthew, Satanjeev Banerjee, and Alexander I. Rudnicky. 2010. Using the Amazon Mechanical Turk for transcription of spoken language. In IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), pages 5270–5273, Dallas, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Novotney</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Cheap, fast and good enough: Automatic speech recognition with non-expert transcription.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>207--215</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="16650" citStr="Novotney and Callison-Burch 2010" startWordPosition="2702" endWordPosition="2705">ird of the HITs are performed by Turkers who need MTurk to make basic ends meet, you then have to admit that MTurk is, at least for them, a labor marketplace. Moreover, the frequent assumption that the low rewards are a result of the classical law of supply-and-demand (large numbers of Turkers means more supply of labor and therefore lower acceptable salaries) is false. Firstly, we do not observe that there are too many Turkers. In fact, there are not enough Turkers. This can be observed through the difficulty in finding Turkers with certain abilities (e.g., understanding a specific language [Novotney and Callison-Burch 2010]), and in the difficulty in performing very large HIT groups (Ipeirotis 2010a). This is not surprising, as we have seen that the number of active Turkers is not that large. Secondly, the low cost is a result of the Requesters’ view of the relation between quality and reward: Many articles (e.g., Marge, Banerjee, and Rudnicky 2010) relate that there is no correlation between the reward and the final quality. The reason is that increasing the price is believed to attract spammers (i.e., Turkers who cheat, not really performing the job, but using robots or answering randomly), and these are nume</context>
</contexts>
<marker>Novotney, Callison-Burch, 2010</marker>
<rawString>Novotney, Scott and Chris Callison-Burch. 2010. Cheap, fast and good enough: Automatic speech recognition with non-expert transcription. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 207–215, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Ross</author>
<author>Lilly Irani</author>
<author>M Six Silberman</author>
<author>Andrew Zaldivar</author>
<author>Bill Tomlinson</author>
</authors>
<title>Who are the crowdworkers? Shifting demographics in Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the 28th of the International Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA ’10,</booktitle>
<pages>2863--2872</pages>
<location>New York, NY.</location>
<contexts>
<context position="13156" citStr="Ross et al. 2010" startWordPosition="2131" endWordPosition="2134">e “90-9-1” rule8 valid in the Internet culture. Another important question is whether activity in MTurk should be considered as labor or something else (hobby, volunteer work, etc.). The observed mean hourly wages for performing jobs in the MTurk system is below US$2 (US$1.25 according to Ross et al. [2009]). Because they accept such low rewards, a common assumption is that Turkers are U.S. students or stay-at-home mothers who have plenty of leisure time and are happy to fill their recreation time by making some extra money. According to recent studies in the social sciences (Ipeirotis 2010b; Ross et al. 2010), it is quite true that a majority (60%) of Turkers think that MTurk is a fruitful way to spend free time getting some cash; but it is only 20% (5% of the India Turkers) who say that they use it to kill time. And these studies also show that 20% (30% of the India Turkers) declare that they use MTurk “to make basic ends meet.” From these answers, we find that money is an important motivation for a majority of the Turkers (20% use MTurk as their primary source of income, and 50% as their secondary source of income), and leisure is important for only a minority (30%). We cannot conclude from thes</context>
</contexts>
<marker>Ross, Irani, Silberman, Zaldivar, Tomlinson, 2010</marker>
<rawString>Ross, Joel, Lilly Irani, M. Six Silberman, Andrew Zaldivar, and Bill Tomlinson. 2010. Who are the crowdworkers? Shifting demographics in Mechanical Turk. In Proceedings of the 28th of the International Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA ’10, pages 2863–2872, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Ross</author>
<author>Andrew Zaldivar</author>
<author>Lilly Irani</author>
<author>Bill Tomlinson</author>
</authors>
<title>Who are the Turkers? Worker demographics in Amazon Mechanical Turk. Social Code Report 2009-01. Available at http://www.ics.uci.edu/∼jwross/pubs/</title>
<date>2009</date>
<pages>2009--01</pages>
<contexts>
<context position="9343" citStr="Ross et al. 2009" startWordPosition="1496" endWordPosition="1499">l computer of unlimited power. The assumption is that there is no inconvenience for humans (as it is not real work), and the power comes from the myriad. This a fiction. Let us look first at how many Turkers are performing the HITs. This is a quite difficult task, because Amazon does not give access to many figures about them. We know that over 500k people are registered as Turkers in the MTurk system. But how many Turkers are really performing HITs? To evaluate this, we combined two different sources of information. First, we have access to some surveys about the demographics of the Turkers (Ross et al. 2009, 2010; Ipeirotis 2010b). These surveys may have a bias over the real population of Turkers, as some Turkers may be reluctant to respond to surveys. Because the results of these surveys are quite consistent, and the surveys are usually easy to complete, not particularly boring, and paid above the usual rate, we may assume that this bias is minor, and accept what they say as a good picture of the population of Turkers. In these surveys we see many interesting things. For instance, there is a growing number of people from India: There were below 10% in 2008, above 33% in early 2010, and they rep</context>
</contexts>
<marker>Ross, Zaldivar, Irani, Tomlinson, 2009</marker>
<rawString>Ross, Joel, Andrew Zaldivar, Lilly Irani, and Bill Tomlinson. 2009. Who are the Turkers? Worker demographics in Amazon Mechanical Turk. Social Code Report 2009-01. Available at http://www.ics.uci.edu/∼jwross/pubs/ SocialCode-2009-01.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Snow, Rion, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP 2008, pages 254–263, Waikiki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TheQuill</author>
</authors>
<title>Making money on-line, part 2: Microworking. Available at http://thequill.org/personalfinance/9-making-money-online-part-2-microworking-.html.</title>
<date>2010</date>
<journal>Accessed</journal>
<volume>21</volume>
<contexts>
<context position="18650" citStr="TheQuill 2010" startWordPosition="3042" endWordPosition="3043">done (and sometimes they are not even paid), which in turn decreases the quality. At the moment, the system is stable in terms of the number of Turkers, because good workers are replaced by naive workers. Amazon’s attitude towards reputational issues has been passive. It maintains that it is a neutral clearinghouse for labor, in which all else is the responsibility of the two consenting parties. This attitude has led to an explosion of micro-crowdsourcing start-ups, which observed the MTurk flaws and tried to overcome them.11 Some of these start-ups could become serious alternatives to MTurk (TheQuill 2010), like Samasource,12 which offers at least a fair wage to workers, who in turn are clearly identified on the Web site, with their resumes. But others are even worse than MTurk, ethically speaking. MTurk is ethically questionable enough; as a scientific community with ethical responsibilities we should seek to minimize the existence of even less-ethical alternatives to it. What’s Next? If we persist in claiming that with MTurk we are now able to produce any linguistic resource or perform any manual evaluation of output at a very low cost, funding agencies will come to expect it. It is predictab</context>
</contexts>
<marker>TheQuill, 2010</marker>
<rawString>TheQuill. 2010. Making money on-line, part 2: Microworking. Available at http://thequill.org/personalfinance/9-making-money-online-part-2-microworking-.html. Accessed 21 June 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A taxonomy, dataset, and classifier for automatic noun compound interpretation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>678--687</pages>
<location>Uppsala.</location>
<contexts>
<context position="6435" citStr="Tratz and Hovy (2010" startWordPosition="1014" endWordPosition="1017">rk (35 papers)—the existence of which is, in itself, strong evidence of the importance of the use of MTurk in the domain. A vast majority of papers present small to medium size experiments where the authors have been able to produce linguistic resources or perform evaluations at a very low cost; at least for transcription and translation, the quality is sufficient to train and evaluate statistical translation/transcription systems (Callison-Burch and Dredze 2010; Marge, Banerjee, and Rudnicky 2010). Some of these papers, however, bring to light language resource quality problems. For example, Tratz and Hovy (2010, page 684) note that the user interface limitations constitute “[t]he first and most significant drawback” of MTurk, as, in their context of annotating noun compound relations using a large taxonomy, “it is impossible to force each Turker to label every data point without putting all the terms onto a single Web page, which is highly impractical for a large taxonomy. Some Turkers may label every compound, but most do not.” They also note that ”while we requested that Turkers only work on our task if English was their first language, we had no method of enforcing this.” Finally, they note that </context>
</contexts>
<marker>Tratz, Hovy, 2010</marker>
<rawString>Tratz, Stephen and Eduard Hovy. 2010. A taxonomy, dataset, and classifier for automatic noun compound interpretation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 678–687, Uppsala.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Paul Wais</author>
</authors>
<institution>Shivaram Lingamneni,</institution>
<marker>Wais, </marker>
<rawString>Wais, Paul, Shivaram Lingamneni,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duncan Cook</author>
<author>Jason Fennell</author>
<author>Benjamin Goldenberg</author>
<author>Daniel Lubarov</author>
<author>David Marin</author>
<author>Hari Simons</author>
</authors>
<title>Towards building a high-quality workforce with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of Computational Social Science and the Wisdom of Crowds (NIPS),</booktitle>
<pages>1--5</pages>
<location>Whister.</location>
<marker>Cook, Fennell, Goldenberg, Lubarov, Marin, Simons, 2010</marker>
<rawString>Duncan Cook, Jason Fennell, Benjamin Goldenberg, Daniel Lubarov, David Marin, and Hari Simons. 2010. Towards building a high-quality workforce with Mechanical Turk. In Proceedings of Computational Social Science and the Wisdom of Crowds (NIPS), pages 1–5, Whister.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>