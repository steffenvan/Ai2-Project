<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000516">
<title confidence="0.9979445">
Learning Synchronous Grammars for Semantic Parsing with
Lambda Calculus
</title>
<author confidence="0.932269">
Yuk Wah Wong and Raymond J. Mooney
</author>
<affiliation confidence="0.9700315">
Department of Computer Sciences
The University of Texas at Austin
</affiliation>
<email confidence="0.998862">
{ywwong,mooney}@cs.utexas.edu
</email>
<sectionHeader confidence="0.9948" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999715181818182">
This paper presents the first empirical results
to our knowledge on learning synchronous
grammars that generate logical forms. Using
statistical machine translation techniques, a
semantic parser based on a synchronous
context-free grammar augmented with A-
operators is learned given a set of training
sentences and their correct logical forms.
The resulting parser is shown to be the best-
performing system so far in a database query
domain.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990050877193">
Originally developed as a theory of compiling pro-
gramming languages (Aho and Ullman, 1972), syn-
chronous grammars have seen a surge of interest re-
cently in the statistical machine translation (SMT)
community as a way of formalizing syntax-based
translation models between natural languages (NL).
In generating multiple parse trees in a single deriva-
tion, synchronous grammars are ideal for model-
ing syntax-based translation because they describe
not only the hierarchical structures of a sentence
and its translation, but also the exact correspon-
dence between their sub-parts. Among the gram-
mar formalisms successfully put into use in syntax-
based SMT are synchronous context-free gram-
mars (SCFG) (Wu, 1997) and synchronous tree-
substitution grammars (STSG) (Yamada and Knight,
2001). Both formalisms have led to SMT sys-
tems whose performance is state-of-the-art (Chiang,
2005; Galley et al., 2006).
Synchronous grammars have also been used in
other NLP tasks, most notably semantic parsing,
which is the construction of a complete, formal
meaning representation (MR) of an NL sentence. In
our previous work (Wong and Mooney, 2006), se-
mantic parsing is cast as a machine translation task,
where an SCFG is used to model the translation
of an NL into a formal meaning-representation lan-
guage (MRL). Our algorithm, WASP, uses statistical
models developed for syntax-based SMT for lexical
learning and parse disambiguation. The result is a
robust semantic parser that gives good performance
in various domains. More recently, we show that
our SCFG-based parser can be inverted to produce a
state-of-the-art NL generator, where a formal MRL
is translated into an NL (Wong and Mooney, 2007).
Currently, the use of learned synchronous gram-
mars in semantic parsing and NL generation is lim-
ited to simple MRLs that are free of logical vari-
ables. This is because grammar formalisms such as
SCFG do not have a principled mechanism for han-
dling logical variables. This is unfortunate because
most existing work on computational semantics is
based on predicate logic, where logical variables
play an important role (Blackburn and Bos, 2005).
For some domains, this problem can be avoided by
transforming a logical language into a variable-free,
functional language (e.g. the GEOQUERY functional
query language in Wong and Mooney (2006)). How-
ever, development of such a functional language is
non-trivial, and as we will see, logical languages can
be more appropriate for certain domains.
On the other hand, most existing methods for
mapping NL sentences to logical forms involve sub-
stantial hand-written components that are difficult
to maintain (Joshi and Vijay-Shanker, 2001; Bayer
et al., 2004; Bos, 2005). Zettlemoyer and Collins
(2005) present a statistical method that is consider-
</bodyText>
<page confidence="0.936796">
960
</page>
<note confidence="0.926238">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960–967,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9983164">
ably more robust, but it still relies on hand-written
rules for lexical acquisition, which can create a per-
formance bottleneck.
In this work, we show that methods developed for
SMT can be brought to bear on tasks where logical
forms are involved, such as semantic parsing. In par-
ticular, we extend the WASP semantic parsing algo-
rithm by adding variable-binding λ-operators to the
underlying SCFG. The resulting synchronous gram-
mar generates logical forms using λ-calculus (Mon-
tague, 1970). A semantic parser is learned given a
set of sentences and their correct logical forms us-
ing SMT methods. The new algorithm is called λ-
WASP, and is shown to be the best-performing sys-
tem so far in the GEOQUERY domain.
</bodyText>
<sectionHeader confidence="0.83489" genericHeader="method">
2 Test Domain
</sectionHeader>
<bodyText confidence="0.999630666666667">
In this work, we mainly consider the GEOQUERY
domain, where a query language based on Prolog is
used to query a database on U.S. geography (Zelle
and Mooney, 1996). The query language consists
of logical forms augmented with meta-predicates
for concepts such as smallest and count. Figure 1
shows two sample logical forms and their English
glosses. Throughout this paper, we use the notation
x1, x2,... for logical variables.
Although Prolog logical forms are the main focus
of this paper, our algorithm makes minimal assump-
tions about the target MRL. The only restriction on
the MRL is that it be defined by an unambiguous
context-free grammar (CFG) that divides a logical
form into subformulas (and terms into subterms).
Figure 2(a) shows a sample parse tree of a logical
form, where each CFG production corresponds to a
subformula.
</bodyText>
<sectionHeader confidence="0.961995" genericHeader="method">
3 The Semantic Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.99987825">
Our work is based on the WASP semantic parsing al-
gorithm (Wong and Mooney, 2006), which translates
NL sentences into MRs using an SCFG. In WASP,
each SCFG production has the following form:
</bodyText>
<equation confidence="0.958328">
A —* (α, β) (1)
</equation>
<bodyText confidence="0.9999165">
where α is an NL phrase and β is the MR translation
of α. Both α and β are strings of terminal and non-
terminal symbols. Each non-terminal in α appears
in β exactly once. We use indices to show the cor-
respondence between non-terminals in α and β. All
derivations start with a pair of co-indexed start sym-
bols, (S1 , S1 ). Each step of a derivation involves
the rewriting of a pair of co-indexed non-terminals
by the same SCFG production. The yield of a deriva-
tion is a pair of terminal strings, (e, f), where e is
an NL sentence and f is the MR translation of e.
For convenience, we call an SCFG production a rule
throughout this paper.
While WASP works well for target MRLs that
are free of logical variables such as CLANG (Wong
and Mooney, 2006), it cannot easily handle various
kinds of logical forms used in computational seman-
tics, such as predicate logic. The problem is that
WASP lacks a principled mechanism for handling
logical variables. In this work, we extend the WASP
algorithm by adding a variable-binding mechanism
based on λ-calculus, which allows for compositional
semantics for logical forms.
This work is based on an extended version of
SCFG, which we call λ-SCFG, where each rule has
the following form:
</bodyText>
<equation confidence="0.886045">
A —* (α, λx1 ... λxk.β) (2)
</equation>
<bodyText confidence="0.999551909090909">
where α is an NL phrase and β is the MR trans-
lation of α. Unlike (1), β is a string of termi-
nals, non-terminals, and logical variables. The
variable-binding operator λ binds occurrences of
the logical variables x1, ... , xk in β, which makes
λx1 ... λxk.β a λ-function of arity k. When ap-
plied to a list of arguments, (xi1, ... , xik), the λ-
function gives βσ, where σ is a substitution oper-
ator, {x1/xi1,..., xk/xik}, that replaces all bound
occurrences of xj in β with xij. If any of the ar-
guments xij appear in β as a free variable (i.e. not
bound by any λ), then those free variables in β must
be renamed before function application takes place.
Each non-terminal Aj in β is followed by a list
of arguments, xj _ (xj1, ... , xjkj ). During pars-
ing, Aj must be rewritten by a λ-function fj of ar-
ity kj. Like SCFG, a derivation starts with a pair
of co-indexed start symbols and ends when all non-
terminals have been rewritten. To compute the yield
of a derivation, each fj is applied to its correspond-
ing arguments xj to obtain an MR string free of λ-
operators with logical variables properly named.
</bodyText>
<page confidence="0.993977">
961
</page>
<figure confidence="0.9969284">
(a) answer(x1,smallest(x2,(state(x1),area(x1,x2))))
What is the smallest state by area?
(b) answer(x1,count(x2,(city(x2),major(x2),loc(x2,x3),next to(x3,x4),state(x3),
equal(x4,stateid(texas)))))
How many major cities are in states bordering Texas?
</figure>
<figureCaption confidence="0.991926">
Figure 1: Sample logical forms in the GEOQUERY domain and their English glosses.
</figureCaption>
<figure confidence="0.86025625">
QUERY QUERY
answer(x1,FORM) answer(x1,FORM(x1))
(a) smallest(x2,(FORM,FORM)) (b) λx1.smallest(x2,(FORM(x1),FORM(x1, x2)))
state(x1) area(x1,x2) λx1.state(x1) λx1.λx2.area(x1,x2)
</figure>
<figureCaption confidence="0.99824">
Figure 2: Parse trees of the logical form in Figure 1(a).
</figureCaption>
<bodyText confidence="0.999868052631579">
As a concrete example, Figure 2(b) shows an
MR parse tree that corresponds to the English
parse, [What is the [smallest [state] [by area]]],
based on the A-SCFG rules in Figure 3. To
compute the yield of this MR parse tree, we start
from the leaf nodes: apply Ax1.state(x1) to
the argument (x1), and Ax1.Ax2.area(x1,x2)
to the arguments (x1, x2). This results in two
MR strings: state(x1) and area(x1,x2).
Substituting these MR strings for the FORM non-
terminals in the parent node gives the A-function
Ax1.smallest(x2,(state(x1),area(x1,x2))).
Applying this A-function to (x1) gives the MR
string smallest(x2,(state(x1),area(x1,x2))).
Substituting this MR string for the FORM non-
terminal in the grandparent node in turn gives the
logical form in Figure 1(a). This is the yield of the
MR parse tree, since the root node of the parse tree
is reached.
</bodyText>
<subsectionHeader confidence="0.999327">
3.1 Lexical Acquisition
</subsectionHeader>
<bodyText confidence="0.952978857142857">
Given a set of training sentences paired with their
correct logical forms, {(ei, fi)}, the main learning
task is to find a A-SCFG, G, that covers the train-
ing data. Like most existing work on syntax-based
SMT (Chiang, 2005; Galley et al., 2006), we con-
struct G using rules extracted from word alignments.
We use the K = 5 most probable word alignments
for the training set given by GIZA++ (Och and Ney,
2003), with variable names ignored to reduce spar-
sity. Rules are then extracted from each word align-
ment as follows.
To ground our discussion, we use the word align-
ment in Figure 4 as an example. To represent
the logical form in Figure 4, we use its linearized
parse—a list of MRL productions that generate the
logical form, in top-down, left-most order (cf. Fig-
ure 2(a)). Since the MRL grammar is unambiguous,
every logical form has a unique linearized parse. We
assume the alignment to be n-to-1, where each word
is linked to at most one MRL production.
Rules are extracted in a bottom-up manner, start-
ing with MRL productions at the leaves of the
MR parse tree, e.g. FORM —* state(x1) in Fig-
ure 2(a). Given an MRL production, A —* Q, a
rule A —* (α, Axi1 ... Axik.Q) is extracted such that:
(1) α is the NL phrase linked to the MRL produc-
tion; (2) xi1, ... , xik are the logical variables that
appear in Q and outside the current leaf node in the
MR parse tree. If xi1, ... , xik were not bound by
A, they would become free variables in Q, subject to
renaming during function application (and therefore,
invisible to the rest of the logical form). For exam-
ple, since x1 is an argument of the state predicate
as well as answer and area, x1 must be bound
(cf. the corresponding tree node in Figure 2(b)). The
rule extracted for the state predicate is shown in
Figure 3.
The case for the internal nodes of the MR parse
tree is similar. Given an MRL production, A —* Q,
where Q contains non-terminals A1, ... , An, a rule
A —* (α, Axi1 ... Axik.Q′) is extracted such that: (1)
α is the NL phrase linked to the MRL production,
with non-terminals A1, ... , An showing the posi-
tions of the argument strings; (2) Q′ is Q with each
non-terminal Aj replaced with Aj(xj1, ... , xjkj ),
where xj1, ... , xjkj are the bound variables in the
A-function used to rewrite Aj; (3) xi1, ... , xik are
the logical variables that appear in Q′ and outside
the current MR sub-parse. For example, see the rule
</bodyText>
<page confidence="0.982283">
962
</page>
<figure confidence="0.4998875">
FORM (state, λx1.state(x1))
FORM (by area, λx1.λx2.area(x1,x2))
FORM (smallest FORM 1 FORM 2 , λx1.smallest(x2,(FORM 1 (x1),FORM 2 (x1, x2))))
QUERY (whatis (1) FORM 1 , answer(x1,FORM 1 (x1)))
</figure>
<figureCaption confidence="0.944492">
Figure 3: A-SCFG rules for parsing the English sentence in Figure 1(a).
</figureCaption>
<figure confidence="0.993100888888889">
what QUERY answer(x1,FORM)
is FORM smallest(x2,(FORM,FORM))
FORM state(x1)
FORM area(x1,x2)
the
smallest
state
by
area
</figure>
<figureCaption confidence="0.99994">
Figure 4: Word alignment for the sentence pair in Figure 1(a).
</figureCaption>
<bodyText confidence="0.9994126">
extracted for the smallest predicate in Figure 3,
where x2 is an argument of smallest, but it does
not appear outside the formula smallest(...),
so x2 need not be bound by A. On the other
hand, x1 appears in Q′, and it appears outside
smallest(...) (as an argument of answer),
so x1 must be bound.
Rule extraction continues in this manner until the
root of the MR parse tree is reached. Figure 3 shows
all the rules extracted from Figure 4.1
</bodyText>
<subsectionHeader confidence="0.999716">
3.2 Probabilistic Semantic Parsing Model
</subsectionHeader>
<bodyText confidence="0.999920384615385">
Since the learned A-SCFG can be ambiguous, a
probabilistic model is needed for parse disambigua-
tion. We use the maximum-entropy model proposed
in Wong and Mooney (2006), which defines a condi-
tional probability distribution over derivations given
an observed NL sentence. The output MR is the
yield of the most probable derivation according to
this model.
Parameter estimation involves maximizing the
conditional log-likelihood of the training set. For
each rule, r, there is a feature that returns the num-
ber of times r is used in a derivation. More features
will be introduced in Section 5.
</bodyText>
<sectionHeader confidence="0.987698" genericHeader="method">
4 Promoting NL/MRL Isomorphism
</sectionHeader>
<bodyText confidence="0.9993314">
We have described the A-WASP algorithm which
generates logical forms based on A-calculus. While
reasonably effective, it can be improved in several
ways. In this section, we focus on improving lexical
acquisition.
</bodyText>
<footnote confidence="0.93936175">
1For details regarding non-isomorphic NL/MR parse trees,
removal of bad links from alignments, and extraction of word
gaps (e.g. the token (1) in the last rule of Figure 3), see Wong
and Mooney (2006).
</footnote>
<bodyText confidence="0.957959555555555">
To see why the current lexical acquisition algo-
rithm can be problematic, consider the word align-
ment in Figure 5 (for the sentence pair in Fig-
ure 1(b)). No rules can be extracted for the state
predicate, because the shortest NL substring that
covers the word states and the argument string
Texas, i.e. states bordering Texas, contains the word
bordering, which is linked to an MRL production
outside the MR sub-parse rooted at state. Rule
extraction is forbidden in this case because it would
destroy the link between bordering and next to.
In other words, the NL and MR parse trees are not
isomorphic.
This problem can be ameliorated by transforming
the logical form of each training sentence so that
the NL and MR parse trees are maximally isomor-
phic. This is possible because some of the opera-
tors used in the logical forms, notably the conjunc-
tion operator (,), are both associative (a,(b,c)
= (a,b),c = a,b,c) and commutative (a,b =
b,a). Hence, conjuncts can be reordered and re-
grouped without changing the meaning of a conjunc-
tion. For example, rule extraction would be pos-
sible if the positions of the next to and state
conjuncts were switched. We present a method for
regrouping conjuncts to promote isomorphism be-
tween NL and MR parse trees.2 Given a conjunc-
tion, it does the following: (See Figure 6 for the
pseudocode, and Figure 5 for an illustration.)
Step 1. Identify the MRL productions that corre-
spond to the conjuncts and the meta-predicate that
takes the conjunction as an argument (count in
Figure 5), and figure them as vertices in an undi-
2This method also applies to any operators that are associa-
tive and commutative, e.g. disjunction. For concreteness, how-
ever, we use conjunction as an example.
</bodyText>
<page confidence="0.994586">
963
</page>
<figure confidence="0.80134936">
Step 4. Assign edge weights
how
many
major
cities
are
in
states
bordering
texas
QUERY → answer(x1,FORM)
FORM → count(x2,(CONJ),x1)
CONJ → city(x2),CONJ
CONJ → major(x2),CONJ
CONJ → loc(x2,x3),CONJ
CONJ → next to(x3,x4),CONJ
CONJ → state(x3),FORM
FORM → equal(x4,stateid(texas))
QUERY
answer(x1,FORM)
count(x2,(CONJ),x1)
major(x2),CONJ
city(x2),CONJ
loc(x2,x3),CONJ
state(x3),CONJ
next to(x3,x4),FORM
equal(x4,stateid(texas))
Step 5. Find MST (shown above as thick edges)
how many
major
cities Step 6.
in Construct MR parse
states
bordering
texas
Original MR parse
x4
x2
x3
Steps 1–3.
Form graph
QUERY
answer(x1,FORM)
count(x2,(CONJ),x1)
major(x2),CONJ
city(x2),CONJ
loc(x2,x3),CONJ
next to(x3,x4),CONJ
state(x3),FORM
equal(x4,stateid(texas))
</figure>
<figureCaption confidence="0.996642">
Figure 5: Transforming the logical form in Figure 1(b). The step numbers correspond to those in Figure 6.
</figureCaption>
<figure confidence="0.851916">
Input: A conjunction, c, of n conjuncts; MRL productions, p1, ... , pn, that correspond to each conjunct; an MRL production,
p0, that corresponds to the meta-predicate taking c as an argument; an NL sentence, e; a word alignment, a.
1 Let v(p) be the set of logical variables that appear in p. Create an undirected graph, P, with vertices V = {pi|i = 0, ... ,n}
and edges E = {(pi, pj)|i &lt; j, v(pi) ∩ v(pj) =6 ∅}.
2 Let e(p) be the set of words in e to which p is linked according to a. Let span(pi, pj) be the shortest substring of e that
includes e(pi) ∪ e(pj). Subtract {(pi, pj)|i =6 0, span(pi, pj) ∩ e(p0) =6 ∅} from E.
3 Add edges (p0, pi) to E if pi is not already connected to p0.
4 For each edge (pi, pj) in E, set edge weight to the minimum word distance between e(pi) and e(pj).
5 Find a minimum spanning tree, T, for P using Kruskal’s algorithm.
6 Using p0 as the root, construct a conjunction c′ based on T, and then replace c with c′.
</figure>
<figureCaption confidence="0.999954">
Figure 6: Algorithm for regrouping conjuncts to promote isomorphism between NL and MR parse trees.
</figureCaption>
<bodyText confidence="0.99981759375">
rected graph, F. An edge (pi7 pj) is in F if and only
if pi and pj contain occurrences of the same logical
variables. Each edge in F indicates a possible edge
in the transformed MR parse tree. Intuitively, two
concepts are closely related if they involve the same
logical variables, and therefore, should be placed
close together in the MR parse tree. By keeping oc-
currences of a logical variable in close proximity in
the MR parse tree, we also avoid unnecessary vari-
able bindings in the extracted rules.
Step 2. Remove edges from F whose inclusion in
the MR parse tree would prevent the NL and MR
parse trees from being isomorphic.
Step 3. Add edges to F to make sure that a spanning
tree for F exists.
Steps 4–6. Assign edge weights based on word dis-
tance, find a minimum spanning tree, T, for F, then
regroup the conjuncts based on T. The choice of T
reflects the intuition that words that occur close to-
gether in a sentence tend to be semantically related.
This procedure is repeated for all conjunctions
that appear in a logical form. Rules are then ex-
tracted from the same input alignment used to re-
group conjuncts. Of course, the regrouping of con-
juncts requires a good alignment to begin with, and
that requires a reasonable ordering of conjuncts in
the training data, since the alignment model is sen-
sitive to word order. This suggests an iterative algo-
rithm in which a better grouping of conjuncts leads
to a better alignment model, which guides further re-
grouping until convergence. We did not pursue this,
as it is not needed in our experiments so far.
</bodyText>
<page confidence="0.994312">
964
</page>
<figure confidence="0.998949">
(a) answer(x1,largest(x2,(state(x1),major(x1),river(x1),traverse(x1,x2))))
What is the entity that is a state and also a major river, that traverses something that is the largest?
(b) answer(x1,smallest(x2,(highest(x1,(point(x1),loc(x1,x3),state(x3))),density(x1,x2))))
Among the highest points of all states, which one has the lowest population density?
(c) answer(x1,equal(x1,stateid(alaska)))
Alaska?
(d) answer(x1,largest(x2,(largest(x1,(state(x1),next to(x1,x3),state(x3))),population(x1,x2))))
Among the largest state that borders some other state, which is the one with the largest population?
</figure>
<figureCaption confidence="0.8795145">
Figure 7: Typical errors made by the A-WASP parser, along with their English interpretations, before any
language modeling for the target MRL was done.
</figureCaption>
<sectionHeader confidence="0.887266" genericHeader="method">
5 Modeling the Target MRL
</sectionHeader>
<bodyText confidence="0.918934166666667">
In this section, we propose two methods for model-
ing the target MRL. This is motivated by the fact that
many of the errors made by the A-WASP parser can
be detected by inspecting the MR translations alone.
Figure 7 shows some typical errors, which can be
classified into two broad categories:
1. Type mismatch errors. For example, a state can-
not possibly be a river (Figure 7(a)). Also it is
awkward to talk about the population density of a
state’s highest point (Figure 7(b)).
2. Errors that do not involve type mismatch. For ex-
ample, a query can be overly trivial (Figure 7(c)),
or involve aggregate functions on a known single-
ton (Figure 7(d)).
The first type of errors can be fixed by type check-
ing. Each m-place predicate is associated with a list
of m-tuples showing all valid combinations of entity
types that the m arguments can refer to:
</bodyText>
<equation confidence="0.899603">
point( ): {(POINT)}
density( , ):
{(COUNTRY, NUM), (STATE, NUM), (CITY, NUM)}
</equation>
<bodyText confidence="0.999855176470588">
These m-tuples of entity types are given as do-
main knowledge. The parser maintains a set of
possible entity types for each logical variable in-
troduced in a partial derivation (except those that
are no longer visible). If there is a logical vari-
able that cannot refer to any types of entities
(i.e. the set of entity types is empty), then the par-
tial derivation is considered invalid. For exam-
ple, based on the tuples shown above, point(x1)
and density(x1, ) cannot be both true, because
{POINT} n {COUNTRY, STATE, CITY} = ∅. The
use of type checking is to exploit the fact that peo-
ple tend not to ask questions that obviously have no
valid answers (Grice, 1975). It is also similar to
Schuler’s (2003) use of model-theoretic interpreta-
tions to guide syntactic parsing.
Errors that do not involve type mismatch are
handled by adding new features to the maximum-
entropy model (Section 3.2). We only consider fea-
tures that are based on the MR translations, and
therefore, these features can be seen as an implicit
language model of the target MRL (Papineni et al.,
1997). Of the many features that we have tried,
one feature set stands out as being the most effec-
tive, the two-level rules in Collins and Koo (2005),
which give the number of times a given rule is used
to expand a non-terminal in a given parent rule.
We use only the MRL part of the rules. For ex-
ample, a negative weight for the combination of
QUERY → answer(x1,FORM(x1)) and FORM
→ Ax1.equal(x1, ) would discourage any parse
that yields Figure 7(c). The two-level rules features,
along with the features described in Section 3.2, are
used in the final version of A-WASP.
</bodyText>
<sectionHeader confidence="0.998373" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.997517">
We evaluated the A-WASP algorithm in the GEO-
QUERY domain. The larger GEOQUERY corpus con-
sists of 880 English questions gathered from various
sources (Wong and Mooney, 2006). The questions
were manually translated into Prolog logical forms.
The average length of a sentence is 7.57 words.
We performed a single run of 10-fold cross
validation, and measured the performance of the
learned parsers using precision (percentage of trans-
lations that were correct), recall (percentage of test
sentences that were correctly translated), and F-
measure (harmonic mean of precision and recall).
A translation is considered correct if it retrieves the
same answer as the correct logical form.
Figure 8 shows the learning curves for the A-
</bodyText>
<page confidence="0.993099">
965
</page>
<figure confidence="0.998830657894737">
Precision (%)
Recall (%)
0 100 200 300 400 500 600 700 800 900
Number of training examples
(a) Precision
0 100 200 300 400 500 600 700 800 900
Number of training examples
(b) Recall
100
90
80
60
50
40
30
20
70
10
0
lambda-WASP
WASP
SCISSOR
Z&amp;C
100
90
80
60
50
40
30
20
70
10
0
lambda-WASP
WASP
SCISSOR
Z&amp;C
</figure>
<figureCaption confidence="0.999547">
Figure 8: Learning curves for various parsing algorithms on the larger GEOQUERY corpus.
</figureCaption>
<table confidence="0.99990425">
(%) A-WASP WASP SCISSOR Z&amp;C
Precision 91.95 87.19 92.08 96.25
Recall 86.59 74.77 72.27 79.29
F-measure 89.19 80.50 80.98 86.95
</table>
<tableCaption confidence="0.999954">
Table 1: Performance of various parsing algorithms on the larger GEOQUERY corpus.
</tableCaption>
<bodyText confidence="0.999731979166667">
WASP algorithm compared to: (1) the original
WASP algorithm which uses a functional query lan-
guage (FunQL); (2) SCISSOR (Ge and Mooney,
2005), a fully-supervised, combined syntactic-
semantic parsing algorithm which also uses FunQL;
and (3) Zettlemoyer and Collins (2005) (Z&amp;C), a
CCG-based algorithm which uses Prolog logical
forms. Table 1 summarizes the results at the end
of the learning curves (792 training examples for A-
WASP, WASP and SCISSOR, 600 for Z&amp;C).
A few observations can be made. First, algorithms
that use Prolog logical forms as the target MRL gen-
erally show better recall than those using FunQL. In
particular, A-WASP has the best recall by far. One
reason is that it allows lexical items to be combined
in ways not allowed by FunQL or the hand-written
templates in Z&amp;C, e.g. [smallest [state] [by area]]
in Figure 3. Second, Z&amp;C has the best precision, al-
though their results are based on 280 test examples
only, whereas our results are based on 10-fold cross
validation. Third, A-WASP has the best F-measure.
To see the relative importance of each component
of the A-WASP algorithm, we performed two abla-
tion studies. First, we compared the performance
of A-WASP with and without conjunct regrouping
(Section 4). Second, we compared the performance
of A-WASP with and without language modeling for
the MRL (Section 5). Table 2 shows the results.
It is found that conjunct regrouping improves recall
(p &lt; 0.01 based on the paired t-test), and the use of
two-level rules in the maximum-entropy model im-
proves precision and recall (p &lt; 0.05). Type check-
ing also significantly improves precision and recall.
A major advantage of A-WASP over SCISSOR and
Z&amp;C is that it does not require any prior knowl-
edge of the NL syntax. Figure 9 shows the perfor-
mance of A-WASP on the multilingual GEOQUERY
data set. The 250-example data set is a subset of the
larger GEOQUERY corpus. All English questions in
this data set were manually translated into Spanish,
Japanese and Turkish, while the corresponding Pro-
log queries remain unchanged. Figure 9 shows that
A-WASP performed comparably for all NLs. In con-
trast, SCISSOR cannot be used directly on the non-
English data, because syntactic annotations are only
available in English. Z&amp;C cannot be used directly
either, because it requires NL-specific templates for
building CCG grammars.
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9995955">
We have presented A-WASP, a semantic parsing al-
gorithm based on a A-SCFG that generates logical
forms using A-calculus. A semantic parser is learned
given a set of training sentences and their correct
logical forms using standard SMT techniques. The
result is a robust semantic parser for predicate logic,
and it is the best-performing system so far in the
GEOQUERY domain.
This work shows that it is possible to use standard
SMT methods in tasks where logical forms are in-
volved. For example, it should be straightforward
to adapt A-WASP to the NL generation task—all
one needs is a decoder that can handle input logical
forms. Other tasks that can potentially benefit from
</bodyText>
<page confidence="0.992925">
966
</page>
<figure confidence="0.867757352941176">
(%)
A-WASP
w/o conj. regrouping
Precision
91.95
90.73
Recall
86.59
83.07
(%)
A-WASP
w/o two-level rules
and w/o type checking
Precision Recall
91.95 86.59
88.46 84.32
65.45 63.18
</figure>
<tableCaption confidence="0.779686">
Table 2: Performance of A-WASP with certain components of the algorithm removed.
</tableCaption>
<figureCaption confidence="0.974577">
Figure 9: Learning curves for A-WASP on the multilingual GEOQUERY data set.
</figureCaption>
<figure confidence="0.999232178571429">
0 50 100 150 200 250
Number of training examples
(a) Precision
0 50 100 150 200 250
Number of training examples
(b) Recall
Recall (%)
100
80
60
40
20
0
English
Spanish
Japanese
Turkish
Precision (%)
100
80
60
40
20
0
English
Spanish
Japanese
Turkish
</figure>
<bodyText confidence="0.983994769230769">
this include question answering and interlingual MT.
In future work, we plan to further generalize the
synchronous parsing framework to allow different
combinations of grammar formalisms. For exam-
ple, to handle long-distance dependencies that occur
in open-domain text, CCG and TAG would be more
appropriate than CFG. Certain applications may re-
quire different meaning representations, e.g. frame
semantics.
Acknowledgments: We thank Rohit Kate, Raz-
van Bunescu and the anonymous reviewers for their
valuable comments. This work was supported by a
gift from Google Inc.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880161290323">
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice Hall, Englewood
Cliffs, NJ.
S. Bayer, J. Burger, W. Greiff, and B. Wellner. 2004.
The MITRE logical form generation system. In Proc. of
Senseval-3, Barcelona, Spain, July.
P. Blackburn and J. Bos. 2005. Representation and Inference
for Natural Language: A First Course in Computational Se-
mantics. CSLI Publications, Stanford, CA.
J. Bos. 2005. Towards wide-coverage semantic interpretation.
In Proc. of IWCS-05, Tilburg, The Netherlands, January.
D. Chiang. 2005. A hierarchical phrase-based model for sta-
tistical machine translation. In Proc. of ACL-05, pages 263–
270, Ann Arbor, MI, June.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguistics,
31(1):25–69.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models. In Proc. of
COLING/ACL-06, pages 961–968, Sydney, Australia, July.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Proc. of CoNLL-05,
pages 9–16, Ann Arbor, MI, July.
H. P. Grice. 1975. Logic and conversation. In P. Cole and
J. Morgan, eds., Syntax and Semantics 3: Speech Acts, pages
41–58. Academic Press, New York.
A. K. Joshi and K. Vijay-Shanker. 2001. Compositional se-
mantics with lexicalized tree-adjoining grammar (LTAG):
How much underspecification is necessary? In H. Bunt et
al., eds., Computing Meaning, volume 2, pages 147–163.
Kluwer Academic Publishers, Dordrecht, The Netherlands.
R. Montague. 1970. Universal grammar. Theoria, 36:373–398.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19–51.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997. Feature-
based language understanding. In Proc. of EuroSpeech-97,
pages 1435–1438, Rhodes, Greece.
W. Schuler. 2003. Using model-theoretic semantic interpre-
tation to guide statistical parsing and word recognition in a
spoken language interface. In Proc. of ACL-03, pages 529–
536.
Y. W. Wong and R. J. Mooney. 2006. Learning for seman-
tic parsing with statistical machine translation. In Proc. of
HLT/NAACL-06, pages 439–446, New York City, NY.
Y. W. Wong and R. J. Mooney. 2007. Generation by inverting
a semantic parser that uses statistical machine translation. In
Proc. of NAACL/HLT-07, Rochester, NY, to appear.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377–403.
K. Yamada and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proc. of ACL-01, pages 523–530,
Toulouse, France.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of
AAAI-96, pages 1050–1055, Portland, OR, August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map sen-
tences to logical form: Structured classification with proba-
bilistic categorial grammars. In Proc. of UAI-05, Edinburgh,
Scotland, July.
</reference>
<page confidence="0.997636">
967
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.856946">
<title confidence="0.9919995">Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus</title>
<author confidence="0.999992">Yuk Wah Wong</author>
<author confidence="0.999992">Raymond J Mooney</author>
<affiliation confidence="0.999079">Department of Computer Sciences The University of Texas at Austin</affiliation>
<abstract confidence="0.98908275">This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="757" citStr="Aho and Ullman, 1972" startWordPosition="107" endWordPosition="110">es The University of Texas at Austin {ywwong,mooney}@cs.utexas.edu Abstract This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with Aoperators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain. 1 Introduction Originally developed as a theory of compiling programming languages (Aho and Ullman, 1972), synchronous grammars have seen a surge of interest recently in the statistical machine translation (SMT) community as a way of formalizing syntax-based translation models between natural languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (S</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bayer</author>
<author>J Burger</author>
<author>W Greiff</author>
<author>B Wellner</author>
</authors>
<title>The MITRE logical form generation system.</title>
<date>2004</date>
<booktitle>In Proc. of Senseval-3,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3348" citStr="Bayer et al., 2004" startWordPosition="511" endWordPosition="514">cal variables play an important role (Blackburn and Bos, 2005). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). However, development of such a functional language is non-trivial, and as we will see, logical languages can be more appropriate for certain domains. On the other hand, most existing methods for mapping NL sentences to logical forms involve substantial hand-written components that are difficult to maintain (Joshi and Vijay-Shanker, 2001; Bayer et al., 2004; Bos, 2005). Zettlemoyer and Collins (2005) present a statistical method that is consider960 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960–967, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ably more robust, but it still relies on hand-written rules for lexical acquisition, which can create a performance bottleneck. In this work, we show that methods developed for SMT can be brought to bear on tasks where logical forms are involved, such as semantic parsing. In particular, we extend the WASP semantic par</context>
</contexts>
<marker>Bayer, Burger, Greiff, Wellner, 2004</marker>
<rawString>S. Bayer, J. Burger, W. Greiff, and B. Wellner. 2004. The MITRE logical form generation system. In Proc. of Senseval-3, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
<author>J Bos</author>
</authors>
<title>Representation and Inference for Natural Language: A First Course in Computational Semantics.</title>
<date>2005</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="2792" citStr="Blackburn and Bos, 2005" startWordPosition="426" endWordPosition="429">ns. More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007). Currently, the use of learned synchronous grammars in semantic parsing and NL generation is limited to simple MRLs that are free of logical variables. This is because grammar formalisms such as SCFG do not have a principled mechanism for handling logical variables. This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). However, development of such a functional language is non-trivial, and as we will see, logical languages can be more appropriate for certain domains. On the other hand, most existing methods for mapping NL sentences to logical forms involve substantial hand-written components that are difficult to maintain (Joshi and Vijay-Shanker, 2001; Bayer et al., 2004; Bos, 2005). Zettlemoyer and Collins (2005)</context>
</contexts>
<marker>Blackburn, Bos, 2005</marker>
<rawString>P. Blackburn and J. Bos. 2005. Representation and Inference for Natural Language: A First Course in Computational Semantics. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
</authors>
<title>Towards wide-coverage semantic interpretation.</title>
<date>2005</date>
<booktitle>In Proc. of IWCS-05,</booktitle>
<location>Tilburg, The Netherlands,</location>
<contexts>
<context position="2792" citStr="Bos, 2005" startWordPosition="428" endWordPosition="429">tly, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007). Currently, the use of learned synchronous grammars in semantic parsing and NL generation is limited to simple MRLs that are free of logical variables. This is because grammar formalisms such as SCFG do not have a principled mechanism for handling logical variables. This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). However, development of such a functional language is non-trivial, and as we will see, logical languages can be more appropriate for certain domains. On the other hand, most existing methods for mapping NL sentences to logical forms involve substantial hand-written components that are difficult to maintain (Joshi and Vijay-Shanker, 2001; Bayer et al., 2004; Bos, 2005). Zettlemoyer and Collins (2005)</context>
</contexts>
<marker>Bos, 2005</marker>
<rawString>J. Bos. 2005. Towards wide-coverage semantic interpretation. In Proc. of IWCS-05, Tilburg, The Netherlands, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL-05,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="1540" citStr="Chiang, 2005" startWordPosition="226" endWordPosition="227">s between natural languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good </context>
<context position="9446" citStr="Chiang, 2005" startWordPosition="1529" endWordPosition="1530">ves the A-function Ax1.smallest(x2,(state(x1),area(x1,x2))). Applying this A-function to (x1) gives the MR string smallest(x2,(state(x1),area(x1,x2))). Substituting this MR string for the FORM nonterminal in the grandparent node in turn gives the logical form in Figure 1(a). This is the yield of the MR parse tree, since the root node of the parse tree is reached. 3.1 Lexical Acquisition Given a set of training sentences paired with their correct logical forms, {(ei, fi)}, the main learning task is to find a A-SCFG, G, that covers the training data. Like most existing work on syntax-based SMT (Chiang, 2005; Galley et al., 2006), we construct G using rules extracted from word alignments. We use the K = 5 most probable word alignments for the training set given by GIZA++ (Och and Ney, 2003), with variable names ignored to reduce sparsity. Rules are then extracted from each word alignment as follows. To ground our discussion, we use the word alignment in Figure 4 as an example. To represent the logical form in Figure 4, we use its linearized parse—a list of MRL productions that generate the logical form, in top-down, left-most order (cf. Figure 2(a)). Since the MRL grammar is unambiguous, every lo</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL-05, pages 263– 270, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="21696" citStr="Collins and Koo (2005)" startWordPosition="3618" endWordPosition="3621">d not to ask questions that obviously have no valid answers (Grice, 1975). It is also similar to Schuler’s (2003) use of model-theoretic interpretations to guide syntactic parsing. Errors that do not involve type mismatch are handled by adding new features to the maximumentropy model (Section 3.2). We only consider features that are based on the MR translations, and therefore, these features can be seen as an implicit language model of the target MRL (Papineni et al., 1997). Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. We use only the MRL part of the rules. For example, a negative weight for the combination of QUERY → answer(x1,FORM(x1)) and FORM → Ax1.equal(x1, ) would discourage any parse that yields Figure 7(c). The two-level rules features, along with the features described in Section 3.2, are used in the final version of A-WASP. 6 Experiments We evaluated the A-WASP algorithm in the GEOQUERY domain. The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Moo</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL-06,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1562" citStr="Galley et al., 2006" startWordPosition="228" endWordPosition="231">ral languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good performance in various</context>
<context position="9468" citStr="Galley et al., 2006" startWordPosition="1531" endWordPosition="1534">tion Ax1.smallest(x2,(state(x1),area(x1,x2))). Applying this A-function to (x1) gives the MR string smallest(x2,(state(x1),area(x1,x2))). Substituting this MR string for the FORM nonterminal in the grandparent node in turn gives the logical form in Figure 1(a). This is the yield of the MR parse tree, since the root node of the parse tree is reached. 3.1 Lexical Acquisition Given a set of training sentences paired with their correct logical forms, {(ei, fi)}, the main learning task is to find a A-SCFG, G, that covers the training data. Like most existing work on syntax-based SMT (Chiang, 2005; Galley et al., 2006), we construct G using rules extracted from word alignments. We use the K = 5 most probable word alignments for the training set given by GIZA++ (Och and Ney, 2003), with variable names ignored to reduce sparsity. Rules are then extracted from each word alignment as follows. To ground our discussion, we use the word alignment in Figure 4 as an example. To represent the logical form in Figure 4, we use its linearized parse—a list of MRL productions that generate the logical form, in top-down, left-most order (cf. Figure 2(a)). Since the MRL grammar is unambiguous, every logical form has a uniqu</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of COLING/ACL-06, pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL-05,</booktitle>
<pages>9--16</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="23606" citStr="Ge and Mooney, 2005" startWordPosition="3945" endWordPosition="3948">00 600 700 800 900 Number of training examples (b) Recall 100 90 80 60 50 40 30 20 70 10 0 lambda-WASP WASP SCISSOR Z&amp;C 100 90 80 60 50 40 30 20 70 10 0 lambda-WASP WASP SCISSOR Z&amp;C Figure 8: Learning curves for various parsing algorithms on the larger GEOQUERY corpus. (%) A-WASP WASP SCISSOR Z&amp;C Precision 91.95 87.19 92.08 96.25 Recall 86.59 74.77 72.27 79.29 F-measure 89.19 80.50 80.98 86.95 Table 1: Performance of various parsing algorithms on the larger GEOQUERY corpus. WASP algorithm compared to: (1) the original WASP algorithm which uses a functional query language (FunQL); (2) SCISSOR (Ge and Mooney, 2005), a fully-supervised, combined syntacticsemantic parsing algorithm which also uses FunQL; and (3) Zettlemoyer and Collins (2005) (Z&amp;C), a CCG-based algorithm which uses Prolog logical forms. Table 1 summarizes the results at the end of the learning curves (792 training examples for AWASP, WASP and SCISSOR, 600 for Z&amp;C). A few observations can be made. First, algorithms that use Prolog logical forms as the target MRL generally show better recall than those using FunQL. In particular, A-WASP has the best recall by far. One reason is that it allows lexical items to be combined in ways not allowed</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proc. of CoNLL-05, pages 9–16, Ann Arbor, MI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics 3: Speech Acts,</booktitle>
<pages>41--58</pages>
<editor>In P. Cole and J. Morgan, eds.,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="21147" citStr="Grice, 1975" startWordPosition="3525" endWordPosition="3526">n knowledge. The parser maintains a set of possible entity types for each logical variable introduced in a partial derivation (except those that are no longer visible). If there is a logical variable that cannot refer to any types of entities (i.e. the set of entity types is empty), then the partial derivation is considered invalid. For example, based on the tuples shown above, point(x1) and density(x1, ) cannot be both true, because {POINT} n {COUNTRY, STATE, CITY} = ∅. The use of type checking is to exploit the fact that people tend not to ask questions that obviously have no valid answers (Grice, 1975). It is also similar to Schuler’s (2003) use of model-theoretic interpretations to guide syntactic parsing. Errors that do not involve type mismatch are handled by adding new features to the maximumentropy model (Section 3.2). We only consider features that are based on the MR translations, and therefore, these features can be seen as an implicit language model of the target MRL (Papineni et al., 1997). Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is us</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. P. Grice. 1975. Logic and conversation. In P. Cole and J. Morgan, eds., Syntax and Semantics 3: Speech Acts, pages 41–58. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Compositional semantics with lexicalized tree-adjoining grammar (LTAG): How much underspecification is necessary? In</title>
<date>2001</date>
<journal>Computing Meaning,</journal>
<booktitle>Universal grammar. Theoria,</booktitle>
<volume>2</volume>
<pages>147--163</pages>
<editor>H. Bunt et al., eds.,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, The</location>
<contexts>
<context position="3328" citStr="Joshi and Vijay-Shanker, 2001" startWordPosition="507" endWordPosition="510"> on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). However, development of such a functional language is non-trivial, and as we will see, logical languages can be more appropriate for certain domains. On the other hand, most existing methods for mapping NL sentences to logical forms involve substantial hand-written components that are difficult to maintain (Joshi and Vijay-Shanker, 2001; Bayer et al., 2004; Bos, 2005). Zettlemoyer and Collins (2005) present a statistical method that is consider960 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960–967, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ably more robust, but it still relies on hand-written rules for lexical acquisition, which can create a performance bottleneck. In this work, we show that methods developed for SMT can be brought to bear on tasks where logical forms are involved, such as semantic parsing. In particular, we extend t</context>
</contexts>
<marker>Joshi, Vijay-Shanker, 2001</marker>
<rawString>A. K. Joshi and K. Vijay-Shanker. 2001. Compositional semantics with lexicalized tree-adjoining grammar (LTAG): How much underspecification is necessary? In H. Bunt et al., eds., Computing Meaning, volume 2, pages 147–163. Kluwer Academic Publishers, Dordrecht, The Netherlands. R. Montague. 1970. Universal grammar. Theoria, 36:373–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9632" citStr="Och and Ney, 2003" startWordPosition="1562" endWordPosition="1565">for the FORM nonterminal in the grandparent node in turn gives the logical form in Figure 1(a). This is the yield of the MR parse tree, since the root node of the parse tree is reached. 3.1 Lexical Acquisition Given a set of training sentences paired with their correct logical forms, {(ei, fi)}, the main learning task is to find a A-SCFG, G, that covers the training data. Like most existing work on syntax-based SMT (Chiang, 2005; Galley et al., 2006), we construct G using rules extracted from word alignments. We use the K = 5 most probable word alignments for the training set given by GIZA++ (Och and Ney, 2003), with variable names ignored to reduce sparsity. Rules are then extracted from each word alignment as follows. To ground our discussion, we use the word alignment in Figure 4 as an example. To represent the logical form in Figure 4, we use its linearized parse—a list of MRL productions that generate the logical form, in top-down, left-most order (cf. Figure 2(a)). Since the MRL grammar is unambiguous, every logical form has a unique linearized parse. We assume the alignment to be n-to-1, where each word is linked to at most one MRL production. Rules are extracted in a bottom-up manner, starti</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>Featurebased language understanding.</title>
<date>1997</date>
<booktitle>In Proc. of EuroSpeech-97,</booktitle>
<pages>1435--1438</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="21552" citStr="Papineni et al., 1997" startWordPosition="3591" endWordPosition="3594">density(x1, ) cannot be both true, because {POINT} n {COUNTRY, STATE, CITY} = ∅. The use of type checking is to exploit the fact that people tend not to ask questions that obviously have no valid answers (Grice, 1975). It is also similar to Schuler’s (2003) use of model-theoretic interpretations to guide syntactic parsing. Errors that do not involve type mismatch are handled by adding new features to the maximumentropy model (Section 3.2). We only consider features that are based on the MR translations, and therefore, these features can be seen as an implicit language model of the target MRL (Papineni et al., 1997). Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. We use only the MRL part of the rules. For example, a negative weight for the combination of QUERY → answer(x1,FORM(x1)) and FORM → Ax1.equal(x1, ) would discourage any parse that yields Figure 7(c). The two-level rules features, along with the features described in Section 3.2, are used in the final version of A-WASP. 6 Experiments We evaluated the A</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>K. A. Papineni, S. Roukos, and R. T. Ward. 1997. Featurebased language understanding. In Proc. of EuroSpeech-97, pages 1435–1438, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Schuler</author>
</authors>
<title>Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface.</title>
<date>2003</date>
<booktitle>In Proc. of ACL-03,</booktitle>
<pages>529--536</pages>
<marker>Schuler, 2003</marker>
<rawString>W. Schuler. 2003. Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface. In Proc. of ACL-03, pages 529– 536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL-06,</booktitle>
<pages>439--446</pages>
<location>New York City, NY.</location>
<contexts>
<context position="1795" citStr="Wong and Mooney, 2006" startWordPosition="265" endWordPosition="268">anslation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good performance in various domains. More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007). Currently, the use of learned synchronous gr</context>
<context position="5300" citStr="Wong and Mooney, 2006" startWordPosition="828" endWordPosition="831">their English glosses. Throughout this paper, we use the notation x1, x2,... for logical variables. Although Prolog logical forms are the main focus of this paper, our algorithm makes minimal assumptions about the target MRL. The only restriction on the MRL is that it be defined by an unambiguous context-free grammar (CFG) that divides a logical form into subformulas (and terms into subterms). Figure 2(a) shows a sample parse tree of a logical form, where each CFG production corresponds to a subformula. 3 The Semantic Parsing Algorithm Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG. In WASP, each SCFG production has the following form: A —* (α, β) (1) where α is an NL phrase and β is the MR translation of α. Both α and β are strings of terminal and nonterminal symbols. Each non-terminal in α appears in β exactly once. We use indices to show the correspondence between non-terminals in α and β. All derivations start with a pair of co-indexed start symbols, (S1 , S1 ). Each step of a derivation involves the rewriting of a pair of co-indexed non-terminals by the same SCFG production. The yield of a derivation is a pair o</context>
<context position="12719" citStr="Wong and Mooney (2006)" startWordPosition="2118" endWordPosition="2121">edicate in Figure 3, where x2 is an argument of smallest, but it does not appear outside the formula smallest(...), so x2 need not be bound by A. On the other hand, x1 appears in Q′, and it appears outside smallest(...) (as an argument of answer), so x1 must be bound. Rule extraction continues in this manner until the root of the MR parse tree is reached. Figure 3 shows all the rules extracted from Figure 4.1 3.2 Probabilistic Semantic Parsing Model Since the learned A-SCFG can be ambiguous, a probabilistic model is needed for parse disambiguation. We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence. The output MR is the yield of the most probable derivation according to this model. Parameter estimation involves maximizing the conditional log-likelihood of the training set. For each rule, r, there is a feature that returns the number of times r is used in a derivation. More features will be introduced in Section 5. 4 Promoting NL/MRL Isomorphism We have described the A-WASP algorithm which generates logical forms based on A-calculus. While reasonably effective, it can be improved in sever</context>
<context position="22306" citStr="Wong and Mooney, 2006" startWordPosition="3723" endWordPosition="3726">d Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. We use only the MRL part of the rules. For example, a negative weight for the combination of QUERY → answer(x1,FORM(x1)) and FORM → Ax1.equal(x1, ) would discourage any parse that yields Figure 7(c). The two-level rules features, along with the features described in Section 3.2, are used in the final version of A-WASP. 6 Experiments We evaluated the A-WASP algorithm in the GEOQUERY domain. The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006). The questions were manually translated into Prolog logical forms. The average length of a sentence is 7.57 words. We performed a single run of 10-fold cross validation, and measured the performance of the learned parsers using precision (percentage of translations that were correct), recall (percentage of test sentences that were correctly translated), and Fmeasure (harmonic mean of precision and recall). A translation is considered correct if it retrieves the same answer as the correct logical form. Figure 8 shows the learning curves for the A965 Precision (%) Recall (%) 0 100 200 300 400 5</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proc. of HLT/NAACL-06, pages 439–446, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL/HLT-07,</booktitle>
<location>Rochester, NY,</location>
<note>to appear.</note>
<contexts>
<context position="2349" citStr="Wong and Mooney, 2007" startWordPosition="354" endWordPosition="357">(MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good performance in various domains. More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007). Currently, the use of learned synchronous grammars in semantic parsing and NL generation is limited to simple MRLs that are free of logical variables. This is because grammar formalisms such as SCFG do not have a principled mechanism for handling logical variables. This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional qu</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In Proc. of NAACL/HLT-07, Rochester, NY, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1372" citStr="Wu, 1997" startWordPosition="202" endWordPosition="203">hronous grammars have seen a surge of interest recently in the statistical machine translation (SMT) community as a way of formalizing syntax-based translation models between natural languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algori</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of ACL-01,</booktitle>
<pages>523--530</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1447" citStr="Yamada and Knight, 2001" startWordPosition="210" endWordPosition="213">he statistical machine translation (SMT) community as a way of formalizing syntax-based translation models between natural languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexic</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proc. of ACL-01, pages 523–530, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proc. of AAAI-96,</booktitle>
<pages>1050--1055</pages>
<location>Portland, OR,</location>
<contexts>
<context position="4515" citStr="Zelle and Mooney, 1996" startWordPosition="700" endWordPosition="703"> parsing. In particular, we extend the WASP semantic parsing algorithm by adding variable-binding λ-operators to the underlying SCFG. The resulting synchronous grammar generates logical forms using λ-calculus (Montague, 1970). A semantic parser is learned given a set of sentences and their correct logical forms using SMT methods. The new algorithm is called λ- WASP, and is shown to be the best-performing system so far in the GEOQUERY domain. 2 Test Domain In this work, we mainly consider the GEOQUERY domain, where a query language based on Prolog is used to query a database on U.S. geography (Zelle and Mooney, 1996). The query language consists of logical forms augmented with meta-predicates for concepts such as smallest and count. Figure 1 shows two sample logical forms and their English glosses. Throughout this paper, we use the notation x1, x2,... for logical variables. Although Prolog logical forms are the main focus of this paper, our algorithm makes minimal assumptions about the target MRL. The only restriction on the MRL is that it be defined by an unambiguous context-free grammar (CFG) that divides a logical form into subformulas (and terms into subterms). Figure 2(a) shows a sample parse tree of</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>J. M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proc. of AAAI-96, pages 1050–1055, Portland, OR, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proc. of UAI-05,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="3392" citStr="Zettlemoyer and Collins (2005)" startWordPosition="517" endWordPosition="520">role (Blackburn and Bos, 2005). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). However, development of such a functional language is non-trivial, and as we will see, logical languages can be more appropriate for certain domains. On the other hand, most existing methods for mapping NL sentences to logical forms involve substantial hand-written components that are difficult to maintain (Joshi and Vijay-Shanker, 2001; Bayer et al., 2004; Bos, 2005). Zettlemoyer and Collins (2005) present a statistical method that is consider960 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960–967, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics ably more robust, but it still relies on hand-written rules for lexical acquisition, which can create a performance bottleneck. In this work, we show that methods developed for SMT can be brought to bear on tasks where logical forms are involved, such as semantic parsing. In particular, we extend the WASP semantic parsing algorithm by adding variable-binding λ-</context>
<context position="23734" citStr="Zettlemoyer and Collins (2005)" startWordPosition="3962" endWordPosition="3965">C 100 90 80 60 50 40 30 20 70 10 0 lambda-WASP WASP SCISSOR Z&amp;C Figure 8: Learning curves for various parsing algorithms on the larger GEOQUERY corpus. (%) A-WASP WASP SCISSOR Z&amp;C Precision 91.95 87.19 92.08 96.25 Recall 86.59 74.77 72.27 79.29 F-measure 89.19 80.50 80.98 86.95 Table 1: Performance of various parsing algorithms on the larger GEOQUERY corpus. WASP algorithm compared to: (1) the original WASP algorithm which uses a functional query language (FunQL); (2) SCISSOR (Ge and Mooney, 2005), a fully-supervised, combined syntacticsemantic parsing algorithm which also uses FunQL; and (3) Zettlemoyer and Collins (2005) (Z&amp;C), a CCG-based algorithm which uses Prolog logical forms. Table 1 summarizes the results at the end of the learning curves (792 training examples for AWASP, WASP and SCISSOR, 600 for Z&amp;C). A few observations can be made. First, algorithms that use Prolog logical forms as the target MRL generally show better recall than those using FunQL. In particular, A-WASP has the best recall by far. One reason is that it allows lexical items to be combined in ways not allowed by FunQL or the hand-written templates in Z&amp;C, e.g. [smallest [state] [by area]] in Figure 3. Second, Z&amp;C has the best precisio</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proc. of UAI-05, Edinburgh, Scotland, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>