<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002042">
<title confidence="0.977348">
Automatic and human scoring of word definition responses
</title>
<author confidence="0.99067">
Kevyn Collins-Thompson and Jamie Callan
</author>
<affiliation confidence="0.985475">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.864678">
Pittsburgh, PA, U.S.A. 15213-8213
</address>
<email confidence="0.999789">
{kct|callan}@cs.cmu.edu
</email>
<sectionHeader confidence="0.996675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961392857143">
Assessing learning progress is a critical
step in language learning applications and
experiments. In word learning, for exam-
ple, one important type of assessment is
a definition production test, in which sub-
jects are asked to produce a short defini-
tion of the word being learned. In current
practice, each free response is manually
scored according to how well its mean-
ing matches the target definition. Manual
scoring is not only time-consuming, but
also limited in its flexibility and ability to
detect partial learning effects.
This study describes an effective auto-
matic method for scoring free responses
to definition production tests. The algo-
rithm compares the text of the free re-
sponse to the text of a reference definition
using a statistical model of text semantic
similarity that uses Markov chains on a
graph of individual word relations. The
model can take advantage of both corpus-
and knowledge-based resources. Evalu-
ated on a new corpus of human-judged
free responses, our method achieved sig-
nificant improvements over random and
cosine baselines in both rank correlation
and label error.
</bodyText>
<sectionHeader confidence="0.998551" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961527777778">
Human language technologies are playing an in-
creasingly important role in the science and prac-
tice of language learning. For example, intelligent
Computer Assisted Language Learning (CALL) sys-
tems are being developed that can automatically tai-
lor lessons and questions to the needs of individual
students (Heilman et al., 2006). One critical task
that language tutors, word learning experiments, and
related applications have in common is assessing the
learning progress of the student or experiment sub-
ject during the course of the session.
When the task is learning new vocabulary, a vari-
ety of tests have been developed to measure word
learning progress. Some tests, such as multiple-
choice selection of a correct synonym or cloze com-
pletion, are relatively passive. In production tests,
on the other hand, students are asked to write or say
a short phrase or sentence that uses the word being
learned, called the target word, in a specified way.
In one important type of production test, called a
definition production test, the subject is asked to de-
scribe the meaning of the target word, as they under-
stand it at that point in the session. The use of such
tests has typically required a teacher or researcher
to manually score each response by judging its sim-
ilarity in meaning to the reference definition of the
target word. The resulting scores can then be used
to analyze how a person’s learning of the word re-
sponded to different stimuli, such as seeing the word
used in context. A sample target word and its ref-
erence definition, along with examples of human-
judged responses, are given in Sections 3.3 and 4.1.
However, manual scoring of the definition re-
sponses has several drawbacks. First, it is time-
consuming and must be done by trained experts.
Moreover, if the researcher wanted to test a new hy-
</bodyText>
<page confidence="0.98788">
476
</page>
<note confidence="0.7978325">
Proceedings of NAACL HLT 2007, pages 476–483,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999829138888889">
pothesis by examining the responses with respect to
a different but related definition, the entire set of re-
sponses would have to be manually re-scored against
the new target. Second, manual scoring can often
be limited in its ability to detect when partial learn-
ing has taken place. This is due to the basic trade-
off between the sophistication of the graded scoring
scale, and the ease and consistency with which hu-
man judges can use the scale. For example, it may
be that the subject did not learn the complete mean-
ing of a particular target word, but did learn that this
target word had negative connotations. The usual
binary or ternary score would provide no or little
indication of such effects. Finally, because manual
scoring almost always must be done off-line after the
end of the session, it presents an obstacle to our goal
of creating learning systems that can adapt quickly,
within a single learning session.
This study describes an effective automated
method for assessing word learning by scoring free
responses to definition production tests. The method
is flexible: it can be used to analyze a response with
respect to whatever reference target(s) the teacher or
researcher chooses. Such a test represents a pow-
erful new tool for language learning research. It is
also a compelling application of human language
technologies research on semantic similarity, and
we review related work for that area in Section 2.
Our probabilistic model for computing text seman-
tic similarity, described in Section 3, can use both
corpus-based and knowledge-based resources. In
Section 4 we describe a new dataset of human def-
inition judgments and use it to measure the effec-
tiveness of the model against other measures of text
similarity. Finally, in Section 5 we discuss further
directions and applications of our work.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999978263157895">
The problem of judging a subject response against a
target definition is a type of text similarity problem.
Moreover, it is a text semantic similarity task, since
we require more than measuring direct word overlap
between the two text fragments. For example, if the
definition of the target word ameliorate is to improve
something and the subject response is make it better,
the response clearly indicates that the subject knows
the meaning of the word, and thus should receive a
high score, even though the response and the target
definition have no words in common.
Because most responses are short (1 – 10 words)
our task falls somewhere between word-word simi-
larity and passage similarity. There is a broad field
of existing work in estimating the semantic similar-
ity of individual words. This field may be roughly
divided into two groups. First, there are corpus-
based measures, which use statistics or models de-
rived from a large training collection. These require
little or no human effort to construct, but are limited
in the richness of the features they can reliably repre-
sent. Second, there are knowledge-based measures,
which rely on specialized resources such as dictio-
naries, thesauri, experimental data, WordNet, and so
on. Knowledge-based measures tend to be comple-
mentary to a corpus-based approach and emphasize
precision in favor of recall. This is discussed further,
along with a good general summary of text semantic
similarity work, by (Mihalcea et al., 2006).
Because of the fundamental nature of the se-
mantic similarity problem, there are close connec-
tions with other areas of human language tech-
nologies such as information retrieval (Salton and
Lesk, 1971), text alignment in machine transla-
tion (Jayaraman and Lavie, 2005), text summariza-
tion (Mani and Maybury, 1999), and textual co-
herence (Foltz et al., 1998). Educational applica-
tions include automated scoring of essays, surveyed
in (Valenti et al., 2003), and assessment of short-
answer free-response items (Burstein et al., 1999).
As we describe in Section 3, we use a graph to
model relations between words to perform a kind
of semantic smoothing on the language models of
the subject response and target definition before
comparing them. Several types of relation, such
as synonymy and co-occurrence, may be combined
to model the interactions between terms. (Cao
et al., 2005) also formulated a term dependency
model combining multiple term relations in a lan-
guage modeling framework, applied to information
retrieval. Our graph-based approach may be viewed
as a probabilistic variation on the spreading activa-
tion concept, originally proposed for word-word se-
mantic similarity by (Quillian, 1967).
Finally, (Mihalcea et al., 2006) describe a text se-
mantic similarity measure that combines word-word
similarities between the passages being compared.
</bodyText>
<page confidence="0.997501">
477
</page>
<bodyText confidence="0.999862142857143">
Due to limitations in the knowledge-based similarity
measures used, semantic similarity is only estimated
between words with the same part-of-speech. Our
graph-based approach can relate words of different
types and does not have this limitation. (Mihalcea
et al., 2006) also evaluate their method in terms of
paraphrase recognition using binary judgments. We
view our task as somewhat different than paraphrase
recognition. First, our task is not symmetric: we do
not expect the target definition to be a paraphrase
of the subject’s free response. Second, because we
seek sensitive measures of learning, we want to dis-
tinguish a range of semantic differences beyond a
binary yes/no decision.
</bodyText>
<sectionHeader confidence="0.982117" genericHeader="method">
3 Statistical Text Similarity Model
</sectionHeader>
<bodyText confidence="0.99998025">
We start by describing relations between pairs of
terms using a general probability distribution. These
pairs can then combine into a graph, which we can
apply to define a semantic distance between terms.
</bodyText>
<subsectionHeader confidence="0.999051">
3.1 Relations between individual words
</subsectionHeader>
<bodyText confidence="0.999863090909091">
One way to model word-to-word relationships is us-
ing a mixture of links, where each link defines a par-
ticular type of relationship. In a graph, this may be
represented by a pair of nodes being joined by mul-
tiple weighted edges, with each edge correspond-
ing to a different link type. Our link-based model
is partially based on one defined by (Toutanova et
al., 2004) for prepositional attachment. We allow
directed edges because some relationships such as
hypernyms may be asymmetric. The following are
examples of different types of links.
</bodyText>
<listItem confidence="0.995972588235294">
1. Stemming: Two words are based on common
morphology. Example: stem and stemming.
We used Porter stemming (Porter, 1980).
2. Synonyms and near-synonyms: Two words
share practically all aspects of meaning.
Example: quaff and drink. Our synonyms
came from WordNet (Miller, 1995).
3. Co-occurrence. Both words tend to appear to-
gether in the same contexts.
Example: politics and election.
4. Hyper- and hyponyms: Relations such as “X
is a kind of Y ”, as obtained from Wordnet or
other thesaurus-like resources.
Example: airplane and transportation.
5. Free association: A relation defined by the fact
that a person is likely to give one word as a free-
association response to the other.
</listItem>
<bodyText confidence="0.997398736842105">
Example: disaster and fear. Our data was ob-
tained from the Univ. of South Florida associa-
tion database (Nelson et al., 1998).
We denote link functions using A1, ... , Am to
summarize different types of interactions between
words. Each Am(wi, wj) represents a specific type
of lexical or semantic relation or constraint between
wi and wj. For each link Am, we also define a
weight -ym that gives the strength of the relationship
between wi and wj for that link.
Our goal is to predict the likelihood of a target
definition D given a test response R consisting of
terms {w0 ... wk} drawn from a common vocabu-
lary V. We are thus interested in the conditional dis-
tribution p(D  |R). We start by defining a simple
model that can combine the link functions in a gen-
eral purpose way to produce the conditional distribu-
tion p(wi|wj) given arbitrary terms wi and wj. We
use a log-linear model of the general form
</bodyText>
<equation confidence="0.99639525">
�L &apos;1&apos;m(i)Am(wi, wj) (1)
1
p(wi|wj) = Z exp
m=0
</equation>
<bodyText confidence="0.99997175">
In the next sections we show how to combine the
estimate of individual pairs p(wi|wj) into a larger
graph of term relations, which will enable us to cal-
culate the desired p(D  |R).
</bodyText>
<subsectionHeader confidence="0.99984">
3.2 Combining term relations using graphs
</subsectionHeader>
<bodyText confidence="0.999823538461538">
Graphs provide one rich model for representing mul-
tiple word relationships. They can be directed or
undirected, and typically use nodes of words, with
word labels at the vertices, and edges denoting word
relationships. In this model, the dependency be-
tween two words represents a single inference step
in which the label of the destination word is inferred
from the source word. Multiple inference steps may
then be chained together to perform longer-range in-
ference about word relations. In this way, we can in-
fer the similarity of two terms without requiring di-
rect evidence for the relations between that specific
pair. Using the link functions defined in Section 3.1,
</bodyText>
<page confidence="0.994453">
478
</page>
<bodyText confidence="0.998789">
we imagine a generative process where an author A
creates a short text of N words as follows.
Step 0: Choose an initial word w0 with probabil-
ity P(w0|A). (If we have already generated N
words, stop.)
Step i: Given we have chosen wi−1, then with prob-
ability 1−α output the word wi−1 and reset the
process to step 0. Otherwise, with probability
α, sample a new word wi according to the dis-
tribution:
</bodyText>
<equation confidence="0.98975525">
XL
1
P(wi|wi−1) = Z exp
m=0
</equation>
<bodyText confidence="0.992067157894737">
where Z is the normalization quantity.
This conditional probability may be interpreted
as a mixture model in which a particular link type
Am(.) is chosen with probability -ym(i) at timestep
i. Note that the mixture is allowed to change at each
timestep. For simplicity, we limit the number of
such changes by grouping the timesteps of the walk
into three stages: early, middle and final. The func-
tion F(i) defines how timestep i maps to stage s,
where s ∈ {0, 1, 2}, and we now refer to -ym(s) in-
stead of -ym(i).
Suppose we have a definition D consisting of
terms {di}. For each link type am(.) we define a
transition matrix C(D, m) based on the definition
D. The reason D influences the transition matrix
is that some link types, such as proximity and co-
occurrence, are context-specific. Each stage s has
an overall transition matrix C(D, s) as the mixture
of the individual C(D, m), as follows.
</bodyText>
<equation confidence="0.9930105">
C(D, s) = XM -ym(s)C(D, m) (3)
m=1
</equation>
<bodyText confidence="0.895075">
Combining the stages over k steps into a single
transition matrix, which we denote Ck, we have
</bodyText>
<equation confidence="0.992691333333333">
k
Ck = Y C(D, F(i)) (4)
i=0
</equation>
<bodyText confidence="0.999994333333333">
We denote the (i, j) entry of a matrix Ak by Aki,j.
Then for a particular term di, the probability that a
chain reaches di after k steps, starting at word w is
</bodyText>
<equation confidence="0.9757905">
Pk(di|w) = (1 − α)αkCk (5)
w,di
</equation>
<bodyText confidence="0.9999285">
where we identify w and di with their corresponding
indices into the vocabulary V. The overall probabil-
ity p(di|w) of generating a definition term di given
a word w is therefore
</bodyText>
<equation confidence="0.992298333333333">
P(di|w) = X00 Pk(di|w) = (1 − α)(
k=0
(6)
</equation>
<bodyText confidence="0.99993625">
The walk continuation probability α can be
viewed as a penalty for long chains of inference. In
practice, to perform the random walk steps we re-
place the infinite sum of Eq. 6 with a small number
of steps (up to 5) on a sparse representation of the
adjacency graph. We obtained effective link weights
-ym(i) empirically using held-out data. For simplic-
ity we assume that the same α is used across all link
types, but further improvement may be possible by
extending the model to use link-specific decays αm.
Fine-tuning these parameter estimation methods is a
subject of future work.
</bodyText>
<subsectionHeader confidence="0.998158">
3.3 Using the model for definition scoring
</subsectionHeader>
<bodyText confidence="0.998674375">
In our study the reference definition for the target
word consisted of the target word, a rare synonym,
a more frequent synonym, and a short glossary-like
definition phrase. For example, the reference defini-
tion for abscond was
abscond; absquatulate; escape; to leave quickly
and secretly and hide oneself, often to avoid arrest
or prosecution.
In general, we define the score of a response R
with respect to a definition D as the probability
that the definition is generated by the response, or
p(D|R). Equivalently, we can score by log p(D|R)
since the log function is monotonic. So making the
simplifying assumption that the terms di ∈ D are
exchangable (the bag-of-words assumption), and
taking logarithms, we have:
</bodyText>
<equation confidence="0.9934048">
log p(D|R) = log Y p(di|R)
diED
k k
α C )R,di I
(7)
</equation>
<bodyText confidence="0.999066">
Suppose that the response to be scored is run from
the cops. In practical terms, Eq. 7 means that for our
</bodyText>
<equation confidence="0.684208555555556">
-ym(i)Am(wi, wi−1)
(2)
k
α Ck )w,di
00
X
k=0
X= log[(1 − α)( Xm
diED k=0
</equation>
<page confidence="0.99135">
479
</page>
<bodyText confidence="0.949834666666667">
example, we “light up” the nodes in the graph cor-
responding to run, from, the and cops by assigning
some initial probability, and the graph is then “run”
using the transition matrix C according to Eq. 7. In
this study, the initial node probabilities are set to val-
ues proportional to the idf values of the correspond-
ing term, so that P(di) = idf(di) After m steps,
idf (di)
the probabilities at the nocies for each term in the
reference definition R are read off, and their log-
arithms summed. Similar to an AND calculation,
we calculate a product of sums over the graph, so
that responses reflecting multiple aspects of the tar-
get definition are rewarded more highly than a very
strong prediction for only a single definition term.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999616">
We first describe our corpus of gold standard human
judgments. We then explain the different text sim-
ilarity methods and baselines we computed on the
corpus responses. Finally, we give an analysis and
discussion of the results.
</bodyText>
<subsectionHeader confidence="0.98567">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999955291666667">
We obtained a set of 734 responses to definition pro-
duction tests from a word learning experiment at the
University of Pittsburgh (Bolger et al., 2006). In
total, 72 target words, selected by the same group,
were used in the experiment. In this experiment,
subjects were asked to learn the meaning of target
words after seeing them used in a series of context
sentences. We set aside 70 responses for training,
leaving 664 responses in the final test dataset.
Each response instance was coded using the scale
shown in Table 1, and a sample set of subject re-
sponses and scores is shown in Table 2. The target
word was treated as having several key aspects of
meaning. The coders were instructed to judge a re-
sponse according to how well it covered the various
aspects of the target definition. If the response cov-
ered all aspects of the target definition, but also in-
cluded extra irrelevant information, this was treated
as a partial match at the discretion of the coders.
We obtained three codings of the final dataset.
The first two codings were obtained using an in-
dependent group, the QDAP Center at the Univer-
sity of Pittsburgh. Initially, five human coders, with
varying degrees of general coding experience, were
</bodyText>
<table confidence="0.980821">
Score Meaning
0 Completely wrong
1 Some partial aspect is correct
2 One major aspect, or more than one
minor aspect, is correct
3 Covers all aspects correctly
</table>
<tableCaption confidence="0.926814">
Table 1: Scale for human definition judgements.
</tableCaption>
<table confidence="0.997209125">
Response Human
Score
depart secretly 3
quietly make away, escape 3
to flee, run away 2
flee 2
to get away with 1
to steal or take 0
</table>
<tableCaption confidence="0.7700305">
Table 2: Examples of human scores of responses for
the target word abscond.
</tableCaption>
<bodyText confidence="0.9999054">
trained by the authors using one set of 10 example
instances and two training sessions of 30 instances
each. Between the two training sessions, one of the
authors met with the coders to discuss the ratings
and refine the rating guidelines. After training, the
authors selected the two coders who had the best
inter-coder agreement on the 60 training instances.
These two coders then labeled the final test set of
664 instances. Our third coding was obtained from
an initial coding created by an expert in the Univer-
sity of Pittsburgh Psychology department and then
adjusted by one of the authors to resolve a small
number of internal inconsistencies, such as when the
same response to the same target had been given a
different score.
Inter-coder agreement was measured using lin-
ear weighted kappa, a standard technique for or-
dinal scales. Weighted kappa scores for all three
coder pairs are shown in Table 3. Overall, agree-
ment ranged from moderate (0.64) to good (0.72).
</bodyText>
<sectionHeader confidence="0.607408" genericHeader="method">
4.2 Baseline Methods
</sectionHeader>
<bodyText confidence="0.98933175">
We computed three baselines as reference points for
lower and upper performance bounds.
Random. The response items were assigned la-
bels randomly.
</bodyText>
<page confidence="0.995363">
480
</page>
<table confidence="0.9950374">
Coder pair Weighted
Kappa
1, 2 0.68
2, 3 0.64
1, 3 0.72
</table>
<tableCaption confidence="0.9536235">
Table 3: Weighted kappa inter-rater reliability for
three human coders on our definition response
</tableCaption>
<table confidence="0.947495">
dataset (664 items).
Method Spearman Rank
Correlation
Random 0.3661
Cosine 0.4731
LSA 0.4868
Markov 0.6111
LSA + Markov 0.6365
Human 0.8744
</table>
<tableCaption confidence="0.705692333333333">
Table 4: Ability of methods to match human ranking
of responses, as measured by Spearman rank corre-
lation (corrected for ties).
</tableCaption>
<bodyText confidence="0.995283238095238">
Human choice of label. We include a method
that, given an item and a human label from one of the
coders, simply returns a label of the same item from
a different coder, with results repeated and averaged
over all coders. This gives an indication of an upper
bound based on human performance.
Cosine similarity using tf.idf weighting. Cosine
similarity is a widely-used text similarity method
for tasks where the passages being compared of-
ten have significant direct word overlap. We repre-
sent response items and reference definitions as vec-
tors of terms using tf.idf weighting, a standard tech-
nique from information retrieval (Salton and Buck-
ley, 1997) that combines term frequency (tf) with
term specificity (idf). A good summary of arguments
for using idf can be found in (Robertson, 2004). To
compute idf, we used frequencies from a standard
100-million-word corpus of written and spoken En-
glish 1. We included a minimal semantic similar-
ity component by applying Porter stemming (Porter,
1980) on terms.
</bodyText>
<footnote confidence="0.975548">
1The British National Corpus (Burnage and Dunlop, 1992),
using American spelling conversion.
</footnote>
<subsectionHeader confidence="0.993241">
4.3 Methods
</subsectionHeader>
<bodyText confidence="0.999883121212121">
In addition to the baseline methods, we also ran the
following three algorithms over the responses.
Markov chains (“Markov”). This is the method
described in Section 3. A maximum of 5 random
walk steps were used, with a walk continuation
probability of 0.8. Each walk step used a mixture of
synonym, stem, co-occurrence, and free-association
links. The link weights were trained on a small set
of held-out data.
Latent Semantic Analysis (LSA). LSA (Lan-
dauer et al., 1998) is a corpus-based unsupervised
technique that uses dimensionality reduction to clus-
ter terms according to multi-order co-occurrence re-
lations. In these experiments, we obtained LSA-
based similarity scores between responses and target
definitions using the software running on the Univer-
sity of Colorado LSA Web site (LSA site, 2006). We
used the pairwise text passage comparison facility,
using the maximum 300 latent factors and a general
English corpus (Grade 1 – first-year college).
Although LSA and the Markov chain approach
are based on different principles, we chose to ap-
ply LSA to this new response-scoring task and cor-
pus because LSA has been widely used as a text se-
mantic similarity measure for other tasks and shown
good performance (Foltz et al., 1998).
LSA+Markov. To test the effectiveness of com-
bining two different – and possibly complemen-
tary – approaches to response scoring, we created
a normalized, weighted linear combination of the
LSA and Markov scores, with the model combina-
tion weight being derived from cross-validation on a
held-out dataset.
</bodyText>
<subsectionHeader confidence="0.872592">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.994853272727273">
We measured the effectiveness of each scoring
method from two perspectives: ranking quality, and
label accuracy.
First, we measured how well each scoring method
was able to rank response items by similarity to the
target definition. To do this, we calculated the Spear-
man Rank Correlation (corrected for ties) between
the ranking based on the scoring method and the
ranking based on the human-assigned scores, aver-
aged over all sets of target word responses.
Table 4 summarizes the ranking results. For
</bodyText>
<page confidence="0.994267">
481
</page>
<table confidence="0.999723625">
Method Label error (RMS)
Top 1 Top 3
Random 1.4954 1.6643
Cosine 0.8194 1.0540
LSA 0.8009 0.9965
Markov 0.7222 0.7968
LSA + Markov 1.1111 1.0650
Human 0.1944 0.4167
</table>
<tableCaption confidence="0.969819666666667">
Table 5: Root mean squared error (RMSE) of la-
bel(s) for top-ranked item, and top-three items for
all 77 words in the dataset.
</tableCaption>
<bodyText confidence="0.993995103448276">
overall quality of ranking, the Markov method had
significantly better performance than the other au-
tomated methods (p &lt; 2.38e−5). LSA gave a
small, but not significant, improvement in overall
rank quality over the cosine baseline. 2 The sim-
ple combination of LSA and Markov resulted in a
slightly higher but statistically insignificant differ-
ence (p &lt; 0.253).
Second, we examined the ability of each method
to find the most accurate responses – that is, the re-
sponses with the highest human label on average –
for a given target word. To do this, we calculated the
Root Mean Squared Error (RMSE) of the label as-
signed to the top item, and the top three items. The
results are shown in Table 5. For top-item detec-
tion, our Markov model had the lowest RMS error
(0.7222) of the automated methods, but the differ-
ences from Cosine and LSA were not statistically
significant, while differences for all three from Ran-
dom and Human baselines were significant. For
the top three items, the difference between Markov
(0.7968) and LSA (0.9965) was significant at the
p &lt; 0.03 level.
Comparing the overall rank accuracy with top-
item accuracy, the combined LSA + Markov method
was significantly worse at finding the three best-
quality responses (RMSE of 1.0650) than Markov
(0.7968) or LSA (0.9965) alone. The reasons for
this require further study.
</bodyText>
<footnote confidence="0.9160725">
2All statistical significance results reported here used the
Wilcoxon Signed-Ranks test.
</footnote>
<sectionHeader confidence="0.996533" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999993025">
Even though definition scoring may seem more
straightforward than other automated learning as-
sessment problems, human performance was still
significantly above the best automated methods in
our study, for both ranking and label accuracy. There
are certain additions to our model which seem likely
to result in further improvement.
One of the most important is the ability to identify
phrases or colloquial expressions. Given the short
length of a response, these seem critical to handle
properly. For example, to get away with something
is commonly understood to mean secretly guilty, not
a physical action. Yet the near-identical phrase to
get away from something means something very dif-
ferent when phrases and idioms are considered.
Despite the gap between human and automated
performance, the current level of accuracy of the
Markov chain approach has already led to some
promising early results in word learning research.
For example, in a separate study of incremental
word learning (Frishkoff et al., 2006), we used our
measure to track increments in word knowledge
across multiple trials. Each trial consisted of a sin-
gle passage that was either supportive – containing
clues to the meaning of unfamiliar words – or not
supportive. In this separate study, broad learning ef-
fects identified by our measure were consistent with
effects found using manually-scored pre- and post-
tests. Our automated method also revealed a pre-
viously unknown interaction between trial spacing,
the proportion of supportive contexts per word, and
reader skill.
In future applications, we envision using our auto-
mated measure to allow a form of feedback for intel-
ligent language tutors, so that the system can auto-
matically adapt its behavior based on the student’s
test responses. With some adjustments, the same
scoring model described in this study may also be
applied to the problem of finding supportive contexts
for students.
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999855">
We presented results for both automated and hu-
man performance of an important task for language
learning applications: scoring definition responses.
We described a probabilistic model of text seman-
</bodyText>
<page confidence="0.995374">
482
</page>
<bodyText confidence="0.99997225">
tic similarity that uses Markov chains on a graph of
term relations to perform a kind of semantic smooth-
ing. This model incorporated both corpus-based and
knowledge-based resources to compute text seman-
tic similarity. We measured the effectiveness of both
our method and LSA compared to cosine and ran-
dom baselines, using a new corpus of human judg-
ments on definition responses from a language learn-
ing experiment. Our method outperformed the tf.idf
cosine similarity baseline in ranking quality and in
ability to find high-scoring definitions. Because
LSA and our Markov chain method are based on
different approaches and resources, it is difficult to
draw definitive conclusions about performance dif-
ferences between the two methods.
Looking beyond definition scoring, we believe au-
tomated methods for assessing word learning have
great potential as a new scientific tool for language
learning researchers, and as a key component of in-
telligent tutoring systems that can adapt to students.
</bodyText>
<sectionHeader confidence="0.996536" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999985222222222">
We thank D.J. Bolger and C. Perfetti for the use
of their definition response data, Stuart Shulman
for his guidance of the human coding effort, and
the anonymous reviewers for their comments. This
work supported by U.S. Dept. of Education grant
R305G03123. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are the authors’ and do not necessarily reflect those
of the sponsors.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998748125">
D.J. Bolger, M. Balass, E. Landen and C.A. Perfetti.
2006. Contextual Variation and Definitions in Learn-
ing the Meanings of Words. (In press.)
G. Burnage and D. Dunlop. 1992. Encoding the British
National Corpus. English Language Corpora: De-
sign, Analysis and Exploitation. The 13th Intl. Conf.
on Engl. Lang. Res. in Computerized Corpora. Ni-
jmegen. J. Aarts, P. de Haan, N. Oostdijk, Eds.
J. Burstein, S. Wolff, and L. Chi. 1999. Using Lexi-
cal Semantic Techniques to Classify Free-Responses.
Breadth and Depth of Semantic Lexicons. Kluwer
Acad. Press, p. 1–18.
G. Cao, J-Y. Nie, and J. Bai. Integrating Word Relation-
ships into Language Models. SIGIR 2005, 298–305.
P.W. Foltz, W. Kintsch, and T. Landauer. 1998. An Intro-
duction to Latent Semantic Analysis. Discourse Pro-
</reference>
<page confidence="0.77434">
483
</page>
<reference confidence="0.998850448979592">
cesses, 25(2):285–307.
G. Frishkoff, K. Collins-Thompson, J. Callan, and C.
Perfetti. 2007. The Nature of Incremental Word
Learning: Context Quality, Spacing Effects, and Skill
Differences in Meaning Acquisition Across Multiple
Contexts. (In preparation.)
M. Heilman, K. Collins-Thompson, J. Callan and M. Es-
kanazi. 2006. Classroom Success of an Intelligent Tu-
toring System for Lexical Practice and Reading Com-
prehension. ICSLP 2006.
S. Jayaraman and A. Lavie. Multi-Engine Ma-
chine Translation Guided by Explicit Word Matching.
EAMT 2005.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
Introduction to Latent Semantic Analysis. Discourse
Processes, 25:259–284.
LSA Web Site. http://lsa.colorado.edu
I. Mani and M.T. Maybury (Eds.) 1999. Advances in
Automatic Text Summarization. MIT Press.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and Knowledge-based Measures of Text
Semantic Similarity. AAAI2006
G. Miller. 1998. WordNet: A Lexical Database for En-
glish. Communications of the ACM, 38(11) 39–41.
D.L. Nelson, C.L. McEvoy, and T.A. Schreiber.
1998. The University of South Florida word
association, rhyme, and word fragment norms.
http://www.usf.edu/FreeAssociation/
M. Porter. 1980. An Algorithm for Suffix-
stripping. Program, 14(3) 130–137.
http://www.tartarus.org/martin/PorterStemmer
M. Quillian. 1967. Word Concepts: A Theory and Sim-
ulation of some Basic Semantic Capabilities. Behav.
Sci., 12: 410–430.
S. Robertson. 2004. Understanding Inverse Document
Frequency: on Theoretical Arguments for IDF. J. of
Documentation, 60:503–520.
G. Salton and C. Buckley. 1997. Term Weighting Ap-
proaches in Automatic Text Retrieval. Reading in In-
formation Retrieval. Morgan Kaufmann.
G. Salton and M. Lesk. 1971. Computer Evaluation
of Indexing and Text Processing. Prentice-Hall. 143
– 180.
K. Toutanova, C.D. Manning, and A.Y. Ng. 2004. Learn-
ing Random Walk Models for Inducing Word Depen-
dency Distributions. ICML 2004.
S. Valenti, F. Neri, and A. Cucchiarelli. 2003. An
Overview of Current Research on Automated Essay
Grading. J. ofInfo. Tech. Ed., Vol. 2.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.544742">
<title confidence="0.999674">Automatic and human scoring of word definition responses</title>
<author confidence="0.862659">Kevyn Collins-Thompson</author>
<author confidence="0.862659">Jamie</author>
<affiliation confidence="0.852406333333333">Language Technologies School of Computer Carnegie Mellon</affiliation>
<address confidence="0.996831">Pittsburgh, PA, U.S.A.</address>
<abstract confidence="0.999326517241379">Assessing learning progress is a critical step in language learning applications and experiments. In word learning, for example, one important type of assessment is a definition production test, in which subjects are asked to produce a short definition of the word being learned. In current practice, each free response is manually scored according to how well its meaning matches the target definition. Manual scoring is not only time-consuming, but also limited in its flexibility and ability to detect partial learning effects. This study describes an effective automatic method for scoring free responses to definition production tests. The algorithm compares the text of the free response to the text of a reference definition using a statistical model of text semantic similarity that uses Markov chains on a graph of individual word relations. The model can take advantage of both corpusand knowledge-based resources. Evaluated on a new corpus of human-judged free responses, our method achieved significant improvements over random and cosine baselines in both rank correlation and label error.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D J Bolger</author>
<author>M Balass</author>
<author>E Landen</author>
<author>C A Perfetti</author>
</authors>
<date>2006</date>
<booktitle>Contextual Variation and Definitions in Learning the Meanings of Words.</booktitle>
<publisher>(In press.)</publisher>
<contexts>
<context position="16635" citStr="Bolger et al., 2006" startWordPosition="2786" endWordPosition="2789">ulation, we calculate a product of sums over the graph, so that responses reflecting multiple aspects of the target definition are rewarded more highly than a very strong prediction for only a single definition term. 4 Evaluation We first describe our corpus of gold standard human judgments. We then explain the different text similarity methods and baselines we computed on the corpus responses. Finally, we give an analysis and discussion of the results. 4.1 Corpus We obtained a set of 734 responses to definition production tests from a word learning experiment at the University of Pittsburgh (Bolger et al., 2006). In total, 72 target words, selected by the same group, were used in the experiment. In this experiment, subjects were asked to learn the meaning of target words after seeing them used in a series of context sentences. We set aside 70 responses for training, leaving 664 responses in the final test dataset. Each response instance was coded using the scale shown in Table 1, and a sample set of subject responses and scores is shown in Table 2. The target word was treated as having several key aspects of meaning. The coders were instructed to judge a response according to how well it covered the </context>
</contexts>
<marker>Bolger, Balass, Landen, Perfetti, 2006</marker>
<rawString>D.J. Bolger, M. Balass, E. Landen and C.A. Perfetti. 2006. Contextual Variation and Definitions in Learning the Meanings of Words. (In press.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Burnage</author>
<author>D Dunlop</author>
</authors>
<title>Encoding the British National Corpus. English Language Corpora: Design, Analysis and Exploitation.</title>
<date>1992</date>
<booktitle>The 13th Intl. Conf. on Engl. Lang. Res. in Computerized</booktitle>
<location>Eds.</location>
<contexts>
<context position="20746" citStr="Burnage and Dunlop, 1992" startWordPosition="3479" endWordPosition="3482">ften have significant direct word overlap. We represent response items and reference definitions as vectors of terms using tf.idf weighting, a standard technique from information retrieval (Salton and Buckley, 1997) that combines term frequency (tf) with term specificity (idf). A good summary of arguments for using idf can be found in (Robertson, 2004). To compute idf, we used frequencies from a standard 100-million-word corpus of written and spoken English 1. We included a minimal semantic similarity component by applying Porter stemming (Porter, 1980) on terms. 1The British National Corpus (Burnage and Dunlop, 1992), using American spelling conversion. 4.3 Methods In addition to the baseline methods, we also ran the following three algorithms over the responses. Markov chains (“Markov”). This is the method described in Section 3. A maximum of 5 random walk steps were used, with a walk continuation probability of 0.8. Each walk step used a mixture of synonym, stem, co-occurrence, and free-association links. The link weights were trained on a small set of held-out data. Latent Semantic Analysis (LSA). LSA (Landauer et al., 1998) is a corpus-based unsupervised technique that uses dimensionality reduction to</context>
</contexts>
<marker>Burnage, Dunlop, 1992</marker>
<rawString>G. Burnage and D. Dunlop. 1992. Encoding the British National Corpus. English Language Corpora: Design, Analysis and Exploitation. The 13th Intl. Conf. on Engl. Lang. Res. in Computerized Corpora. Nijmegen. J. Aarts, P. de Haan, N. Oostdijk, Eds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burstein</author>
<author>S Wolff</author>
<author>L Chi</author>
</authors>
<title>Using Lexical Semantic Techniques to Classify Free-Responses. Breadth and Depth of Semantic Lexicons.</title>
<date>1999</date>
<pages>1--18</pages>
<publisher>Kluwer Acad. Press,</publisher>
<contexts>
<context position="7153" citStr="Burstein et al., 1999" startWordPosition="1156" endWordPosition="1159">ong with a good general summary of text semantic similarity work, by (Mihalcea et al., 2006). Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al., 1998). Educational applications include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of shortanswer free-response items (Burstein et al., 1999). As we describe in Section 3, we use a graph to model relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et al., 2005) also formulated a term dependency model combining multiple term relations in a language modeling framework, applied to information retrieval. Our graph-based approach may be viewed as a probabilistic variation on the spreading activation concept, ori</context>
</contexts>
<marker>Burstein, Wolff, Chi, 1999</marker>
<rawString>J. Burstein, S. Wolff, and L. Chi. 1999. Using Lexical Semantic Techniques to Classify Free-Responses. Breadth and Depth of Semantic Lexicons. Kluwer Acad. Press, p. 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cao</author>
<author>J-Y Nie</author>
<author>J Bai</author>
</authors>
<title>Integrating Word Relationships into Language Models. SIGIR</title>
<date>2005</date>
<pages>298--305</pages>
<contexts>
<context position="7502" citStr="Cao et al., 2005" startWordPosition="1214" endWordPosition="1217"> 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al., 1998). Educational applications include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of shortanswer free-response items (Burstein et al., 1999). As we describe in Section 3, we use a graph to model relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et al., 2005) also formulated a term dependency model combining multiple term relations in a language modeling framework, applied to information retrieval. Our graph-based approach may be viewed as a probabilistic variation on the spreading activation concept, originally proposed for word-word semantic similarity by (Quillian, 1967). Finally, (Mihalcea et al., 2006) describe a text semantic similarity measure that combines word-word similarities between the passages being compared. 477 Due to limitations in the knowledge-based similarity measures used, semantic similarity is only estimated between words wi</context>
</contexts>
<marker>Cao, Nie, Bai, 2005</marker>
<rawString>G. Cao, J-Y. Nie, and J. Bai. Integrating Word Relationships into Language Models. SIGIR 2005, 298–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
<author>W Kintsch</author>
<author>T Landauer</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="6980" citStr="Foltz et al., 1998" startWordPosition="1131" endWordPosition="1134">WordNet, and so on. Knowledge-based measures tend to be complementary to a corpus-based approach and emphasize precision in favor of recall. This is discussed further, along with a good general summary of text semantic similarity work, by (Mihalcea et al., 2006). Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al., 1998). Educational applications include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of shortanswer free-response items (Burstein et al., 1999). As we describe in Section 3, we use a graph to model relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et al., 2005) also formulated a term dependency model combining multiple term relations in </context>
<context position="22033" citStr="Foltz et al., 1998" startWordPosition="3686" endWordPosition="3689">these experiments, we obtained LSAbased similarity scores between responses and target definitions using the software running on the University of Colorado LSA Web site (LSA site, 2006). We used the pairwise text passage comparison facility, using the maximum 300 latent factors and a general English corpus (Grade 1 – first-year college). Although LSA and the Markov chain approach are based on different principles, we chose to apply LSA to this new response-scoring task and corpus because LSA has been widely used as a text semantic similarity measure for other tasks and shown good performance (Foltz et al., 1998). LSA+Markov. To test the effectiveness of combining two different – and possibly complementary – approaches to response scoring, we created a normalized, weighted linear combination of the LSA and Markov scores, with the model combination weight being derived from cross-validation on a held-out dataset. 4.4 Results We measured the effectiveness of each scoring method from two perspectives: ranking quality, and label accuracy. First, we measured how well each scoring method was able to rank response items by similarity to the target definition. To do this, we calculated the Spearman Rank Corre</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>P.W. Foltz, W. Kintsch, and T. Landauer. 1998. An Introduction to Latent Semantic Analysis. Discourse Processes, 25(2):285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Frishkoff</author>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>C Perfetti</author>
</authors>
<title>The Nature of Incremental Word Learning: Context Quality, Spacing Effects, and Skill Differences in Meaning Acquisition Across Multiple Contexts.</title>
<date>2007</date>
<note>(In preparation.)</note>
<marker>Frishkoff, Collins-Thompson, Callan, Perfetti, 2007</marker>
<rawString>G. Frishkoff, K. Collins-Thompson, J. Callan, and C. Perfetti. 2007. The Nature of Incremental Word Learning: Context Quality, Spacing Effects, and Skill Differences in Meaning Acquisition Across Multiple Contexts. (In preparation.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>M Eskanazi</author>
</authors>
<title>Classroom Success of an Intelligent Tutoring System for Lexical Practice and Reading Comprehension. ICSLP</title>
<date>2006</date>
<contexts>
<context position="1695" citStr="Heilman et al., 2006" startWordPosition="254" endWordPosition="257">individual word relations. The model can take advantage of both corpusand knowledge-based resources. Evaluated on a new corpus of human-judged free responses, our method achieved significant improvements over random and cosine baselines in both rank correlation and label error. 1 Introduction Human language technologies are playing an increasingly important role in the science and practice of language learning. For example, intelligent Computer Assisted Language Learning (CALL) systems are being developed that can automatically tailor lessons and questions to the needs of individual students (Heilman et al., 2006). One critical task that language tutors, word learning experiments, and related applications have in common is assessing the learning progress of the student or experiment subject during the course of the session. When the task is learning new vocabulary, a variety of tests have been developed to measure word learning progress. Some tests, such as multiplechoice selection of a correct synonym or cloze completion, are relatively passive. In production tests, on the other hand, students are asked to write or say a short phrase or sentence that uses the word being learned, called the target word</context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskanazi, 2006</marker>
<rawString>M. Heilman, K. Collins-Thompson, J. Callan and M. Eskanazi. 2006. Classroom Success of an Intelligent Tutoring System for Lexical Practice and Reading Comprehension. ICSLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jayaraman</author>
<author>A Lavie</author>
</authors>
<title>Multi-Engine Machine Translation Guided by Explicit Word Matching. EAMT</title>
<date>2005</date>
<contexts>
<context position="6891" citStr="Jayaraman and Lavie, 2005" startWordPosition="1116" endWordPosition="1119">easures, which rely on specialized resources such as dictionaries, thesauri, experimental data, WordNet, and so on. Knowledge-based measures tend to be complementary to a corpus-based approach and emphasize precision in favor of recall. This is discussed further, along with a good general summary of text semantic similarity work, by (Mihalcea et al., 2006). Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al., 1998). Educational applications include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of shortanswer free-response items (Burstein et al., 1999). As we describe in Section 3, we use a graph to model relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>S. Jayaraman and A. Lavie. Multi-Engine Machine Translation Guided by Explicit Word Matching. EAMT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="21267" citStr="Landauer et al., 1998" startWordPosition="3562" endWordPosition="3566">ying Porter stemming (Porter, 1980) on terms. 1The British National Corpus (Burnage and Dunlop, 1992), using American spelling conversion. 4.3 Methods In addition to the baseline methods, we also ran the following three algorithms over the responses. Markov chains (“Markov”). This is the method described in Section 3. A maximum of 5 random walk steps were used, with a walk continuation probability of 0.8. Each walk step used a mixture of synonym, stem, co-occurrence, and free-association links. The link weights were trained on a small set of held-out data. Latent Semantic Analysis (LSA). LSA (Landauer et al., 1998) is a corpus-based unsupervised technique that uses dimensionality reduction to cluster terms according to multi-order co-occurrence relations. In these experiments, we obtained LSAbased similarity scores between responses and target definitions using the software running on the University of Colorado LSA Web site (LSA site, 2006). We used the pairwise text passage comparison facility, using the maximum 300 latent factors and a general English corpus (Grade 1 – first-year college). Although LSA and the Markov chain approach are based on different principles, we chose to apply LSA to this new r</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An Introduction to Latent Semantic Analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>http lsa colorado edu I Mani</author>
<author>M T Maybury</author>
</authors>
<title>Advances in Automatic Text Summarization.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6936" citStr="Mani and Maybury, 1999" startWordPosition="1123" endWordPosition="1126">h as dictionaries, thesauri, experimental data, WordNet, and so on. Knowledge-based measures tend to be complementary to a corpus-based approach and emphasize precision in favor of recall. This is discussed further, along with a good general summary of text semantic similarity work, by (Mihalcea et al., 2006). Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al., 1998). Educational applications include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of shortanswer free-response items (Burstein et al., 1999). As we describe in Section 3, we use a graph to model relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et al., 2005) also formulated a term dependency</context>
</contexts>
<marker>Mani, Maybury, 1999</marker>
<rawString>LSA Web Site. http://lsa.colorado.edu I. Mani and M.T. Maybury (Eds.) 1999. Advances in Automatic Text Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<date>2006</date>
<booktitle>Corpus-based and Knowledge-based Measures of Text Semantic Similarity. AAAI2006</booktitle>
<contexts>
<context position="6623" citStr="Mihalcea et al., 2006" startWordPosition="1074" endWordPosition="1077">re corpusbased measures, which use statistics or models derived from a large training collection. These require little or no human effort to construct, but are limited in the richness of the features they can reliably represent. Second, there are knowledge-based measures, which rely on specialized resources such as dictionaries, thesauri, experimental data, WordNet, and so on. Knowledge-based measures tend to be complementary to a corpus-based approach and emphasize precision in favor of recall. This is discussed further, along with a good general summary of text semantic similarity work, by (Mihalcea et al., 2006). Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al., 1998). Educational applications include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of shortanswer free-response items (Burstein et al., 1999). As we describe in Section 3, we use a graph to model relations betwe</context>
<context position="7857" citStr="Mihalcea et al., 2006" startWordPosition="1265" endWordPosition="1268">perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et al., 2005) also formulated a term dependency model combining multiple term relations in a language modeling framework, applied to information retrieval. Our graph-based approach may be viewed as a probabilistic variation on the spreading activation concept, originally proposed for word-word semantic similarity by (Quillian, 1967). Finally, (Mihalcea et al., 2006) describe a text semantic similarity measure that combines word-word similarities between the passages being compared. 477 Due to limitations in the knowledge-based similarity measures used, semantic similarity is only estimated between words with the same part-of-speech. Our graph-based approach can relate words of different types and does not have this limitation. (Mihalcea et al., 2006) also evaluate their method in terms of paraphrase recognition using binary judgments. We view our task as somewhat different than paraphrase recognition. First, our task is not symmetric: we do not expect th</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. AAAI2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1998</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<marker>Miller, 1998</marker>
<rawString>G. Miller. 1998. WordNet: A Lexical Database for English. Communications of the ACM, 38(11) 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Nelson</author>
<author>C L McEvoy</author>
<author>T A Schreiber</author>
</authors>
<date>1998</date>
<institution>The University of South Florida</institution>
<note>word association, rhyme, and word fragment norms. http://www.usf.edu/FreeAssociation/</note>
<contexts>
<context position="10300" citStr="Nelson et al., 1998" startWordPosition="1655" endWordPosition="1658">pects of meaning. Example: quaff and drink. Our synonyms came from WordNet (Miller, 1995). 3. Co-occurrence. Both words tend to appear together in the same contexts. Example: politics and election. 4. Hyper- and hyponyms: Relations such as “X is a kind of Y ”, as obtained from Wordnet or other thesaurus-like resources. Example: airplane and transportation. 5. Free association: A relation defined by the fact that a person is likely to give one word as a freeassociation response to the other. Example: disaster and fear. Our data was obtained from the Univ. of South Florida association database (Nelson et al., 1998). We denote link functions using A1, ... , Am to summarize different types of interactions between words. Each Am(wi, wj) represents a specific type of lexical or semantic relation or constraint between wi and wj. For each link Am, we also define a weight -ym that gives the strength of the relationship between wi and wj for that link. Our goal is to predict the likelihood of a target definition D given a test response R consisting of terms {w0 ... wk} drawn from a common vocabulary V. We are thus interested in the conditional distribution p(D |R). We start by defining a simple model that can c</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>D.L. Nelson, C.L. McEvoy, and T.A. Schreiber. 1998. The University of South Florida word association, rhyme, and word fragment norms. http://www.usf.edu/FreeAssociation/</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An Algorithm for Suffixstripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>130--137</pages>
<contexts>
<context position="9613" citStr="Porter, 1980" startWordPosition="1543" endWordPosition="1544">sing a mixture of links, where each link defines a particular type of relationship. In a graph, this may be represented by a pair of nodes being joined by multiple weighted edges, with each edge corresponding to a different link type. Our link-based model is partially based on one defined by (Toutanova et al., 2004) for prepositional attachment. We allow directed edges because some relationships such as hypernyms may be asymmetric. The following are examples of different types of links. 1. Stemming: Two words are based on common morphology. Example: stem and stemming. We used Porter stemming (Porter, 1980). 2. Synonyms and near-synonyms: Two words share practically all aspects of meaning. Example: quaff and drink. Our synonyms came from WordNet (Miller, 1995). 3. Co-occurrence. Both words tend to appear together in the same contexts. Example: politics and election. 4. Hyper- and hyponyms: Relations such as “X is a kind of Y ”, as obtained from Wordnet or other thesaurus-like resources. Example: airplane and transportation. 5. Free association: A relation defined by the fact that a person is likely to give one word as a freeassociation response to the other. Example: disaster and fear. Our data </context>
<context position="20680" citStr="Porter, 1980" startWordPosition="3471" endWordPosition="3472">y method for tasks where the passages being compared often have significant direct word overlap. We represent response items and reference definitions as vectors of terms using tf.idf weighting, a standard technique from information retrieval (Salton and Buckley, 1997) that combines term frequency (tf) with term specificity (idf). A good summary of arguments for using idf can be found in (Robertson, 2004). To compute idf, we used frequencies from a standard 100-million-word corpus of written and spoken English 1. We included a minimal semantic similarity component by applying Porter stemming (Porter, 1980) on terms. 1The British National Corpus (Burnage and Dunlop, 1992), using American spelling conversion. 4.3 Methods In addition to the baseline methods, we also ran the following three algorithms over the responses. Markov chains (“Markov”). This is the method described in Section 3. A maximum of 5 random walk steps were used, with a walk continuation probability of 0.8. Each walk step used a mixture of synonym, stem, co-occurrence, and free-association links. The link weights were trained on a small set of held-out data. Latent Semantic Analysis (LSA). LSA (Landauer et al., 1998) is a corpus-</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. Porter. 1980. An Algorithm for Suffixstripping. Program, 14(3) 130–137. http://www.tartarus.org/martin/PorterStemmer</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Quillian</author>
</authors>
<title>Word Concepts: A Theory and Simulation of some Basic Semantic Capabilities.</title>
<date>1967</date>
<journal>Behav. Sci.,</journal>
<volume>12</volume>
<pages>410--430</pages>
<contexts>
<context position="7823" citStr="Quillian, 1967" startWordPosition="1262" endWordPosition="1263">relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et al., 2005) also formulated a term dependency model combining multiple term relations in a language modeling framework, applied to information retrieval. Our graph-based approach may be viewed as a probabilistic variation on the spreading activation concept, originally proposed for word-word semantic similarity by (Quillian, 1967). Finally, (Mihalcea et al., 2006) describe a text semantic similarity measure that combines word-word similarities between the passages being compared. 477 Due to limitations in the knowledge-based similarity measures used, semantic similarity is only estimated between words with the same part-of-speech. Our graph-based approach can relate words of different types and does not have this limitation. (Mihalcea et al., 2006) also evaluate their method in terms of paraphrase recognition using binary judgments. We view our task as somewhat different than paraphrase recognition. First, our task is </context>
</contexts>
<marker>Quillian, 1967</marker>
<rawString>M. Quillian. 1967. Word Concepts: A Theory and Simulation of some Basic Semantic Capabilities. Behav. Sci., 12: 410–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
</authors>
<title>Understanding Inverse Document Frequency: on Theoretical Arguments for IDF.</title>
<date>2004</date>
<journal>J. of Documentation,</journal>
<pages>60--503</pages>
<contexts>
<context position="20475" citStr="Robertson, 2004" startWordPosition="3439" endWordPosition="3440">esults repeated and averaged over all coders. This gives an indication of an upper bound based on human performance. Cosine similarity using tf.idf weighting. Cosine similarity is a widely-used text similarity method for tasks where the passages being compared often have significant direct word overlap. We represent response items and reference definitions as vectors of terms using tf.idf weighting, a standard technique from information retrieval (Salton and Buckley, 1997) that combines term frequency (tf) with term specificity (idf). A good summary of arguments for using idf can be found in (Robertson, 2004). To compute idf, we used frequencies from a standard 100-million-word corpus of written and spoken English 1. We included a minimal semantic similarity component by applying Porter stemming (Porter, 1980) on terms. 1The British National Corpus (Burnage and Dunlop, 1992), using American spelling conversion. 4.3 Methods In addition to the baseline methods, we also ran the following three algorithms over the responses. Markov chains (“Markov”). This is the method described in Section 3. A maximum of 5 random walk steps were used, with a walk continuation probability of 0.8. Each walk step used a</context>
</contexts>
<marker>Robertson, 2004</marker>
<rawString>S. Robertson. 2004. Understanding Inverse Document Frequency: on Theoretical Arguments for IDF. J. of Documentation, 60:503–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term Weighting Approaches in Automatic Text Retrieval. Reading in Information Retrieval.</title>
<date>1997</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="20336" citStr="Salton and Buckley, 1997" startWordPosition="3413" endWordPosition="3417">nclude a method that, given an item and a human label from one of the coders, simply returns a label of the same item from a different coder, with results repeated and averaged over all coders. This gives an indication of an upper bound based on human performance. Cosine similarity using tf.idf weighting. Cosine similarity is a widely-used text similarity method for tasks where the passages being compared often have significant direct word overlap. We represent response items and reference definitions as vectors of terms using tf.idf weighting, a standard technique from information retrieval (Salton and Buckley, 1997) that combines term frequency (tf) with term specificity (idf). A good summary of arguments for using idf can be found in (Robertson, 2004). To compute idf, we used frequencies from a standard 100-million-word corpus of written and spoken English 1. We included a minimal semantic similarity component by applying Porter stemming (Porter, 1980) on terms. 1The British National Corpus (Burnage and Dunlop, 1992), using American spelling conversion. 4.3 Methods In addition to the baseline methods, we also ran the following three algorithms over the responses. Markov chains (“Markov”). This is the me</context>
</contexts>
<marker>Salton, Buckley, 1997</marker>
<rawString>G. Salton and C. Buckley. 1997. Term Weighting Approaches in Automatic Text Retrieval. Reading in Information Retrieval. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M Lesk</author>
</authors>
<date>1971</date>
<booktitle>Computer Evaluation of Indexing and Text Processing. Prentice-Hall. 143 – 180.</booktitle>
<contexts>
<context position="6824" citStr="Salton and Lesk, 1971" startWordPosition="1106" endWordPosition="1109">hey can reliably represent. Second, there are knowledge-based measures, which rely on specialized resources such as dictionaries, thesauri, experimental data, WordNet, and so on. Knowledge-based measures tend to be complementary to a corpus-based approach and emphasize precision in favor of recall. This is discussed further, along with a good general summary of text semantic similarity work, by (Mihalcea et al., 2006). Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al., 1998). Educational applications include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of shortanswer free-response items (Burstein et al., 1999). As we describe in Section 3, we use a graph to model relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrenc</context>
</contexts>
<marker>Salton, Lesk, 1971</marker>
<rawString>G. Salton and M. Lesk. 1971. Computer Evaluation of Indexing and Text Processing. Prentice-Hall. 143 – 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Learning Random Walk Models for Inducing Word Dependency Distributions. ICML</title>
<date>2004</date>
<contexts>
<context position="9317" citStr="Toutanova et al., 2004" startWordPosition="1497" endWordPosition="1500">ilarity Model We start by describing relations between pairs of terms using a general probability distribution. These pairs can then combine into a graph, which we can apply to define a semantic distance between terms. 3.1 Relations between individual words One way to model word-to-word relationships is using a mixture of links, where each link defines a particular type of relationship. In a graph, this may be represented by a pair of nodes being joined by multiple weighted edges, with each edge corresponding to a different link type. Our link-based model is partially based on one defined by (Toutanova et al., 2004) for prepositional attachment. We allow directed edges because some relationships such as hypernyms may be asymmetric. The following are examples of different types of links. 1. Stemming: Two words are based on common morphology. Example: stem and stemming. We used Porter stemming (Porter, 1980). 2. Synonyms and near-synonyms: Two words share practically all aspects of meaning. Example: quaff and drink. Our synonyms came from WordNet (Miller, 1995). 3. Co-occurrence. Both words tend to appear together in the same contexts. Example: politics and election. 4. Hyper- and hyponyms: Relations such </context>
</contexts>
<marker>Toutanova, Manning, Ng, 2004</marker>
<rawString>K. Toutanova, C.D. Manning, and A.Y. Ng. 2004. Learning Random Walk Models for Inducing Word Dependency Distributions. ICML 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Valenti</author>
<author>F Neri</author>
<author>A Cucchiarelli</author>
</authors>
<date>2003</date>
<journal>An Overview of Current Research on Automated Essay Grading. J. ofInfo. Tech. Ed.,</journal>
<volume>2</volume>
<contexts>
<context position="7078" citStr="Valenti et al., 2003" startWordPosition="1145" endWordPosition="1148"> and emphasize precision in favor of recall. This is discussed further, along with a good general summary of text semantic similarity work, by (Mihalcea et al., 2006). Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al., 1998). Educational applications include automated scoring of essays, surveyed in (Valenti et al., 2003), and assessment of shortanswer free-response items (Burstein et al., 1999). As we describe in Section 3, we use a graph to model relations between words to perform a kind of semantic smoothing on the language models of the subject response and target definition before comparing them. Several types of relation, such as synonymy and co-occurrence, may be combined to model the interactions between terms. (Cao et al., 2005) also formulated a term dependency model combining multiple term relations in a language modeling framework, applied to information retrieval. Our graph-based approach may be v</context>
</contexts>
<marker>Valenti, Neri, Cucchiarelli, 2003</marker>
<rawString>S. Valenti, F. Neri, and A. Cucchiarelli. 2003. An Overview of Current Research on Automated Essay Grading. J. ofInfo. Tech. Ed., Vol. 2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>