<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022441">
<title confidence="0.996921">
Filtering Syntactic Constraints for Statistical Machine Translation
</title>
<author confidence="0.980552">
Hailong Cao and Eiichiro Sumita
</author>
<affiliation confidence="0.985269">
Language Translation Group, MASTAR Project
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.975027">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
</address>
<email confidence="0.999359">
{hlcao, eiichiro.sumita }@nict.go.jp
</email>
<sectionHeader confidence="0.995658" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998334090909091">
Source language parse trees offer very useful
but imperfect reordering constraints for statis-
tical machine translation. A lot of effort has
been made for soft applications of syntactic
constraints. We alternatively propose the se-
lective use of syntactic constraints. A classifier
is built automatically to decide whether a node
in the parse trees should be used as a reorder-
ing constraint or not. Using this information
yields a 0.8 BLEU point improvement over a
full constraint-based system.
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920378378378">
In statistical machine translation (SMT), the
search problem is NP-hard if arbitrary reordering
is allowed (Knight, 1999). Therefore, we need to
restrict the possible reordering in an appropriate
way for both efficiency and translation quality.
The most widely used reordering constraints are
IBM constraints (Berger et al., 1996), ITG con-
straints (Wu, 1995) and syntactic constraints
(Yamada et al., 2000; Galley et al., 2004; Liu et
al., 2006; Marcu et al., 2006; Zollmann and
Venugopal 2006; and numerous others). Syntac-
tic constraints can be imposed from the source
side or target side. This work will focus on syn-
tactic constraints from source parse trees.
Linguistic parse trees can provide very useful
reordering constraints for SMT. However, they
are far from perfect because of both parsing er-
rors and the crossing of the constituents and for-
mal phrases extracted from parallel training data.
The key challenge is how to take advantage of
the prior knowledge in the linguistic parse trees
without affecting the strengths of formal phrases.
Recent efforts attack this problem by using the
constraints softly (Cherry, 2008; Marton and
Resnik, 2008). In their methods, a candidate
translation gets an extra credit if it respects the
parse tree but may incur a cost if it violates a
constituent boundary.
In this paper, we address this challenge from a
less explored direction. Rather than use all con-
straints offered by the parse trees, we propose
using them selectively. Based on parallel training
data, a classifier is built automatically to decide
whether a node in the parse trees should be used
as a reordering constraint or not. As a result, we
obtain a 0.8 BLEU point improvement over a full
constraint-based system.
</bodyText>
<sectionHeader confidence="0.799532" genericHeader="method">
2 Reordering Constraints from Source
</sectionHeader>
<subsectionHeader confidence="0.751344">
Parse Trees
</subsectionHeader>
<bodyText confidence="0.998750086956522">
In this section we briefly review a constraint-
based system named IST-ITG (Imposing Source
Tree on Inversion Transduction Grammar, Ya-
mamoto et al., 2008) upon which this work
builds.
When using ITG constraints during decoding,
the source-side parse tree structure is not consid-
ered. The reordering process can be more tightly
constrained if constraints from the source parse
tree are integrated with the ITG constraints. IST-
ITG constraints directly apply source sentence
tree structure to generate the target with the
following constraint: the target sentence is ob-
tained by rotating any node of the source sen-
tence tree structure.
After parsing the source sentence, a bracketed
sentence is obtained by removing the node
syntactic labels; this bracketed sentence can then
be directly expressed as a tree structure. For
example1, the parse tree “(S1 (S (NP (DT This))
(VP (AUX is) (NP (DT a) (NN pen)))))” is
obtained from the source sentence “This is a
pen”, which consists of four words. By removing
</bodyText>
<footnote confidence="0.943609">
1 We use English examples for the sake of readability.
</footnote>
<page confidence="0.987198">
17
</page>
<note confidence="0.509654">
Proceedings of the ACL 2010 Conference Short Papers, pages 17–21,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.933663173913043">
the node syntactic labels, the bracketed sentence
“((This) ((is) ((a) (pen))))” is obtained. Such a
bracketed sentence can be used to produce
constraints.
For example, for the source-side bracketed
tree “((f1 f2) (f3 f4)) ”, eight target sequences [e1,
e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3], [e2,
e1, e4, e3], [e3, e4, e1, e2], [e3, e4, e2, e1], [e4,
e3, e1, e2], and [e4, e3, e2, e1] are possible. For
the source-side bracketed tree “(((f1f2) f3) f4),”
eight sequences [e1, e2, e3, e4], [e2, e1, e3, e4],
[e3, e1, e2, e4], [e3, e2, e1, e4], [e4, e1, e2, e3],
[e4, e2, e1, e3], [e4, e3, e1, e2], and [e4, e3, e2,
e1] are possible. When the source sentence tree
structure is a binary tree, the number of word
orderings is reduced to 2N-1 where N is the length
of the source sentence.
The parsing results sometimes do not produce
binary trees. In this case, some subtrees have
more than two child nodes. For a non-binary sub-
tree, any reordering of child nodes is allowed.
For example, if a subtree has three child nodes,
six reorderings of the nodes are possible.
</bodyText>
<sectionHeader confidence="0.748243" genericHeader="method">
3 Learning to Classify Parse Tree
Nodes
</sectionHeader>
<bodyText confidence="0.999843045454545">
In IST-ITG and many other methods which use
syntactic constraints, all of the nodes in the parse
trees are utilized. Though many nodes in the
parse trees are useful, we would argue that some
nodes are not trustworthy. For example, if we
constrain the translation of “f1 f2 f3 f4” with
node N2 illustrated in Figure 1, then word “e1”
will never be put in the middle the other three
words. If we want to obtain the translation “e2 e1
e4 e3”, node N3 can offer a good constraint
while node N2 should be filtered out. In real cor-
pora, cases such as node N2 are frequent enough
to be noticeable (see Fox (2002) or section 4.1 in
this paper).
Therefore, we use the definitions in Galley et
al. (2004) to classify the nodes in parse trees into
two types: frontier nodes and interior nodes.
Though the definitions were originally made for
target language parse trees, they can be straight-
forwardly applied to the source side. A node
which satisfies both of the following two condi-
tions is referred as a frontier node:
</bodyText>
<listItem confidence="0.996056833333333">
• All the words covered by the node can be
translated separately. That is to say, these
words do not share a translation with any
word outside the coverage of the node.
• All the words covered by the node remain
contiguous after translation.
</listItem>
<bodyText confidence="0.998558571428572">
Otherwise the node is an interior node.
For example, in Figure 1, both node N1 and
node N3 are frontier nodes. Node N2 is an inte-
rior node because the source words f2, f3 and f4
are translated into e2, e3 and e4, which are not
contiguous in the target side.
Clearly, only frontier nodes should be used as
reordering constraints while interior nodes are
not suitable for this. However, little work has
been done on how to explicitly distinguish these
two kinds of nodes in the source parse trees. In
this section, we will explore building a classifier
which can label the nodes in the parse trees as
frontier nodes or interior nodes.
</bodyText>
<equation confidence="0.9966245">
f1 f2 f3 f4
e2 e1 e4 e3
</equation>
<figureCaption confidence="0.997457">
Figure 1: An example parse tree and align-
</figureCaption>
<bodyText confidence="0.778074">
ments
</bodyText>
<subsectionHeader confidence="0.992178">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.999985291666667">
Ideally, we would have a human-annotated cor-
pus in which each sentence is parsed and each
node in the parse trees is labeled as a frontier
node or an interior node. But such a target lan-
guage specific corpus is hard to come by, and
never in the quantity we would like.
Instead, we generate such a corpus automati-
cally. We begin with a parallel corpus which will
be used to train our SMT model. In our case, it is
the FBIS Chinese-English corpus.
Firstly, the Chinese sentences are segmented,
POS tagged and parsed by the tools described in
Kruengkrai et al. (2009) and Cao et al. (2007),
both of which are trained on the Penn Chinese
Treebank 6.0.
Secondly, we use GIZA++ to align the sen-
tences in both the Chinese-English and English-
Chinese directions. We combine the alignments
using the “grow-diag-final-and” procedure pro-
vided with MOSES (Koehn, 2007). Because
there are many errors in the alignment, we re-
move the links if the alignment count is less than
three for the source or the target word. Addition-
ally, we also remove notoriously bad links in
</bodyText>
<equation confidence="0.841976333333333">
N2
N3
N1
</equation>
<page confidence="0.976068">
18
</page>
<bodyText confidence="0.99943125">
{de, le} × {the, a, an} following Fossum and
Knight (2008).
Thirdly, given the parse trees and the align-
ment information, we label each node as a fron-
tier node or an interior node according to the
definition introduced in this section. Using the
labeled nodes as training data, we can build a
classifier. In theory, a broad class of machine
learning tools can be used; however, due to the
scale of the task (see section 4), we utilize the
Pegasos 2 which is a very fast SVM solver
(Shalev-Shwartz et al, 2007).
</bodyText>
<subsectionHeader confidence="0.960675">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.9966045">
For each node in the parse trees, we use the fol-
lowing feature templates:
</bodyText>
<listItem confidence="0.999702892857143">
• A context-free grammar rule which rewrites
the current node (In this and all the following
grammar based features, a mark is used to
indicate which non terminal is the current
node.)
• A context-free grammar rule which rewrites
the current node’s father
• The combination of the above two rules
• A lexicalized context-free grammar rule
which rewrites the current node
• A lexicalized context-free grammar rule
which rewrites the current node’s father
• Syntactic label, head word, and head POS
tag of the current node
• Syntactic label, head word, and head POS
tag of the current node’s left child
• Syntactic label, head word, and head POS
tag of the current node’s right child
• Syntactic label, head word, and head POS
tag of the current node’s left brother
• Syntactic label, head word, and head POS
tag of the current node’s right brother
• Syntactic label, head word, and head POS
tag of the current node’s father
• The leftmost word covered by the current
node and the word before it
• The rightmost word covered by the current
node and the word after it
</listItem>
<sectionHeader confidence="0.998826" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999793">
Our SMT system is based on a fairly typical
phrase-based model (Finch and Sumita, 2008).
For the training of our SMT model, we use a
modified training toolkit adapted from the
</bodyText>
<footnote confidence="0.636923">
2 http://www.cs.huji.ac.il/~shais/code/index.html
</footnote>
<bodyText confidence="0.998059">
MOSES decoder. Our decoder can operate on the
same principles as the MOSES decoder. Mini-
mum error rate training (MERT) with respect to
BLEU score is used to tune the decoder’s pa-
rameters, and it is performed using the standard
technique of Och (2003). A lexical reordering
model was used in our experiments.
The translation model was created from the
FBIS corpus. We used a 5-gram language model
trained with modified Knesser-Ney smoothing.
The language model was trained on the target
side of FBIS corpus and the Xinhua news in GI-
GAWORD corpus. The development and test
sets are from NIST MT08 evaluation campaign.
Table 1 shows the statistics of the corpora used
in our experiments.
</bodyText>
<table confidence="0.9992685">
Data Sentences Chinese English
words words
Training set 243,698 7,933,133 10,343,140
Development set 1664 38,779 46,387
Test set 1357 32377 42,444
GIGAWORD 19,049,757 - 306,221,306
</table>
<tableCaption confidence="0.999663">
Table 1: Corpora statistics
</tableCaption>
<subsectionHeader confidence="0.997084">
4.1 Experiments on Nodes Classification
</subsectionHeader>
<bodyText confidence="0.999988470588235">
We extracted about 3.9 million example nodes
from the training data, i.e. the FBIS corpus.
There were 2.37 million frontier nodes and 1.59
million interior nodes in these examples, give
rise to about 4.4 million features. To test the per-
formance of our classifier, we simply use the last
ten thousand examples as a test set, and the rest
being used as Pegasos training data. All the pa-
rameters in Pegasos were set as default values. In
this way, the accuracy of the classifier was
71.59%.
Then we retrained our classifier by using all of
the examples. The nodes in the automatically
parsed NIST MT08 test set were labeled by the
classifier. As a result, 17,240 nodes were labeled
as frontier nodes and 5,736 nodes were labeled
as interior nodes.
</bodyText>
<subsectionHeader confidence="0.991387">
4.2 Experiments on Chinese-English SMT
</subsectionHeader>
<bodyText confidence="0.998049">
In order to confirm that it is advantageous to dis-
tinguish between frontier nodes and interior
nodes, we performed four translation experi-
ments.
The first one was a typical beam search decod-
ing without any syntactic constraints.
All the other three experiments were based on
the IST-ITG method which makes use of syntac-
</bodyText>
<page confidence="0.998179">
19
</page>
<bodyText confidence="0.999451727272727">
tic constraints. The difference between these
three experiments lies in what constraints are
used. In detail, the second one used all nodes
recognized by the parser; the third one only used
frontier nodes labeled by the classifier; the fourth
one only used interior nodes labeled by the clas-
sifier.
With the exception of the above differences,
all the other settings were the same in the four
experiments. Table 2 summarizes the SMT per-
formance.
</bodyText>
<table confidence="0.9890788">
Syntactic Constraints BLEU
none 17.26
all nodes 16.83
frontier nodes 17.63
interior nodes 16.59
</table>
<tableCaption confidence="0.9848645">
Table 2: Comparison of different constraints by
SMT quality
</tableCaption>
<bodyText confidence="0.999715705882353">
Clearly, we obtain the best performance if we
constrain the search with only frontier nodes.
Using just frontier yields a 0.8 BLEU point im-
provement over the baseline constraint-based
system which uses all the constraints.
On the other hand, constraints from interior
nodes result in the worst performance. This com-
parison shows it is necessary to explicitly distin-
guish nodes in the source parse trees when they
are used as reordering constraints.
The improvement over the system without
constraints is only modest. It may be too coarse
to use pare trees as hard constraints. We believe
a greater improvement can be expected if we ap-
ply our idea to finer-grained approaches that use
constraints softly (Marton and Resnik (2008) and
Cherry (2008)).
</bodyText>
<sectionHeader confidence="0.993505" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999982416666667">
We propose a selectively approach to syntactic
constraints during decoding. A classifier is built
automatically to decide whether a node in the
parse trees should be used as a reordering con-
straint or not. Preliminary results show that it is
not only advantageous but necessary to explicitly
distinguish between frontier nodes and interior
nodes.
The idea of selecting syntactic constraints is
compatible with the idea of using constraints
softly; we plan to combine the two ideas and ob-
tain further improvements in future work.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999566">
We would like to thank Taro Watanabe and
Andrew Finch for insightful discussions. We also
would like to thank the anonymous reviewers for
their constructive comments.
</bodyText>
<sectionHeader confidence="0.889249" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.998477954545455">
A.L. Berger, P.F. Brown, S.A.D. Pietra, V.J.D. Pietra,
J.R. Gillett, A.S. Kehler, and R.L. Mercer. 1996.
Language translation apparatus and method of us-
ing context-based translation models. United States
patent, patent number 5510981, April.
Hailong Cao, Yujie Zhang and Hitoshi Isahara. Em-
pirical study on parsing Chinese based on Collins&apos;
model. 2007. In PACLING.
Colin Cherry. 2008. Cohesive phrase-Based decoding
for statistical machine translation. In ACL- HLT.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In SMT Workshop.
Victoria Fossum and Kevin Knight. 2008. Using bi-
lingual Chinese-English word alignments to re-
solve PP attachment ambiguity in English. In
AMTA Student Workshop.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What&apos;s in a translation rule?
In HLT-NAACL.
Kevin Knight. 1999. Decoding complexity in word
replacement translation models. Computational
Linguistics, 25(4):607–615.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In ACL demo and poster sessions.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun&apos;ichi
Kazama, Yiou Wang, Kentaro Torisawa and Hito-
shi Isahara. 2009. An error-driven word-character
hybrid model for joint Chinese word segmentation
and POS tagging. In ACL-IJCNLP.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine
translation. In ACL-COLING.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language
phrases. In EMNLP.
</reference>
<page confidence="0.902979">
20
</page>
<reference confidence="0.999354952380953">
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based transla-
tion. In ACL-HLT.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL.
Shai Shalev-Shwartz, Yoram Singer and Nathan Sre-
bro. 2007. Pegasos: Primal estimated sub-gradient
solver for SVM. In ICML.
Dekai Wu. 1995. Stochastic inversion transduction
grammars with application to segmentation, brack-
eting, and alignment of parallel corpora. In IJCAI.
Kenji Yamada and Kevin Knight. 2000. A syntax-
based statistical translation model. In ACL.
Hirofumi Yamamoto, Hideo Okuma and Eiichiro
Sumita. 2008. Imposing constraints from the
source tree on ITG constraints for SMT. In Work-
shop on syntax and structure in statistical transla-
tion.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In SMT Workshop, HLT-NAACL.
</reference>
<page confidence="0.999438">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.853862">
<title confidence="0.999939">Filtering Syntactic Constraints for Statistical Machine Translation</title>
<author confidence="0.96637">Cao Sumita</author>
<affiliation confidence="0.9711975">Language Translation Group, MASTAR Project National Institute of Information and Communications Technology</affiliation>
<address confidence="0.988931">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289</address>
<email confidence="0.930768">hlcao@nict.go.jp</email>
<email confidence="0.930768">eiichiro.sumita@nict.go.jp</email>
<abstract confidence="0.999089166666666">Source language parse trees offer very useful but imperfect reordering constraints for statistical machine translation. A lot of effort has been made for soft applications of syntactic constraints. We alternatively propose the selective use of syntactic constraints. A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>P F Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>J R Gillett</author>
<author>A S Kehler</author>
<author>R L Mercer</author>
</authors>
<title>Language translation apparatus and method of using context-based translation models. United States patent, patent number 5510981,</title>
<date>1996</date>
<contexts>
<context position="1153" citStr="Berger et al., 1996" startWordPosition="158" endWordPosition="161">pose the selective use of syntactic constraints. A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system. 1 Introduction In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to </context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Gillett, Kehler, Mercer, 1996</marker>
<rawString>A.L. Berger, P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, J.R. Gillett, A.S. Kehler, and R.L. Mercer. 1996. Language translation apparatus and method of using context-based translation models. United States patent, patent number 5510981, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hailong Cao</author>
</authors>
<title>Yujie Zhang and Hitoshi Isahara. Empirical study on parsing Chinese based on Collins&apos; model.</title>
<date>2007</date>
<booktitle>In PACLING.</booktitle>
<marker>Cao, 2007</marker>
<rawString>Hailong Cao, Yujie Zhang and Hitoshi Isahara. Empirical study on parsing Chinese based on Collins&apos; model. 2007. In PACLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Cohesive phrase-Based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL- HLT.</booktitle>
<contexts>
<context position="1952" citStr="Cherry, 2008" startWordPosition="290" endWordPosition="291">Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases. Recent efforts attack this problem by using the constraints softly (Cherry, 2008; Marton and Resnik, 2008). In their methods, a candidate translation gets an extra credit if it respects the parse tree but may incur a cost if it violates a constituent boundary. In this paper, we address this challenge from a less explored direction. Rather than use all constraints offered by the parse trees, we propose using them selectively. Based on parallel training data, a classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. As a result, we obtain a 0.8 BLEU point improvement over a full constraint-based system.</context>
<context position="13250" citStr="Cherry (2008)" startWordPosition="2235" endWordPosition="2236">int improvement over the baseline constraint-based system which uses all the constraints. On the other hand, constraints from interior nodes result in the worst performance. This comparison shows it is necessary to explicitly distinguish nodes in the source parse trees when they are used as reordering constraints. The improvement over the system without constraints is only modest. It may be too coarse to use pare trees as hard constraints. We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). 5 Conclusion and Future Work We propose a selectively approach to syntactic constraints during decoding. A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. Preliminary results show that it is not only advantageous but necessary to explicitly distinguish between frontier nodes and interior nodes. The idea of selecting syntactic constraints is compatible with the idea of using constraints softly; we plan to combine the two ideas and obtain further improvements in future work. Acknowledgments We would like to thank </context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>Colin Cherry. 2008. Cohesive phrase-Based decoding for statistical machine translation. In ACL- HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Dynamic model interpolation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In SMT Workshop.</booktitle>
<contexts>
<context position="9722" citStr="Finch and Sumita, 2008" startWordPosition="1657" endWordPosition="1660">d word, and head POS tag of the current node’s left child • Syntactic label, head word, and head POS tag of the current node’s right child • Syntactic label, head word, and head POS tag of the current node’s left brother • Syntactic label, head word, and head POS tag of the current node’s right brother • Syntactic label, head word, and head POS tag of the current node’s father • The leftmost word covered by the current node and the word before it • The rightmost word covered by the current node and the word after it 4 Experiments Our SMT system is based on a fairly typical phrase-based model (Finch and Sumita, 2008). For the training of our SMT model, we use a modified training toolkit adapted from the 2 http://www.cs.huji.ac.il/~shais/code/index.html MOSES decoder. Our decoder can operate on the same principles as the MOSES decoder. Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder’s parameters, and it is performed using the standard technique of Och (2003). A lexical reordering model was used in our experiments. The translation model was created from the FBIS corpus. We used a 5-gram language model trained with modified Knesser-Ney smoothing. The language model w</context>
</contexts>
<marker>Finch, Sumita, 2008</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2008. Dynamic model interpolation for statistical machine translation. In SMT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
</authors>
<title>Using bilingual Chinese-English word alignments to resolve PP attachment ambiguity in English.</title>
<date>2008</date>
<booktitle>In AMTA Student Workshop.</booktitle>
<contexts>
<context position="8016" citStr="Fossum and Knight (2008)" startWordPosition="1353" endWordPosition="1356"> and parsed by the tools described in Kruengkrai et al. (2009) and Cao et al. (2007), both of which are trained on the Penn Chinese Treebank 6.0. Secondly, we use GIZA++ to align the sentences in both the Chinese-English and EnglishChinese directions. We combine the alignments using the “grow-diag-final-and” procedure provided with MOSES (Koehn, 2007). Because there are many errors in the alignment, we remove the links if the alignment count is less than three for the source or the target word. Additionally, we also remove notoriously bad links in N2 N3 N1 18 {de, le} × {the, a, an} following Fossum and Knight (2008). Thirdly, given the parse trees and the alignment information, we label each node as a frontier node or an interior node according to the definition introduced in this section. Using the labeled nodes as training data, we can build a classifier. In theory, a broad class of machine learning tools can be used; however, due to the scale of the task (see section 4), we utilize the Pegasos 2 which is a very fast SVM solver (Shalev-Shwartz et al, 2007). 3.2 Features For each node in the parse trees, we use the following feature templates: • A context-free grammar rule which rewrites the current nod</context>
</contexts>
<marker>Fossum, Knight, 2008</marker>
<rawString>Victoria Fossum and Kevin Knight. 2008. Using bilingual Chinese-English word alignments to resolve PP attachment ambiguity in English. In AMTA Student Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5526" citStr="Fox (2002)" startWordPosition="907" endWordPosition="908">odes In IST-ITG and many other methods which use syntactic constraints, all of the nodes in the parse trees are utilized. Though many nodes in the parse trees are useful, we would argue that some nodes are not trustworthy. For example, if we constrain the translation of “f1 f2 f3 f4” with node N2 illustrated in Figure 1, then word “e1” will never be put in the middle the other three words. If we want to obtain the translation “e2 e1 e4 e3”, node N3 can offer a good constraint while node N2 should be filtered out. In real corpora, cases such as node N2 are frequent enough to be noticeable (see Fox (2002) or section 4.1 in this paper). Therefore, we use the definitions in Galley et al. (2004) to classify the nodes in parse trees into two types: frontier nodes and interior nodes. Though the definitions were originally made for target language parse trees, they can be straightforwardly applied to the source side. A node which satisfies both of the following two conditions is referred as a frontier node: • All the words covered by the node can be translated separately. That is to say, these words do not share a translation with any word outside the coverage of the node. • All the words covered by</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What&apos;s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="1249" citStr="Galley et al., 2004" startWordPosition="174" endWordPosition="177">ether a node in the parse trees should be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system. 1 Introduction In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the streng</context>
<context position="5615" citStr="Galley et al. (2004)" startWordPosition="921" endWordPosition="924">the nodes in the parse trees are utilized. Though many nodes in the parse trees are useful, we would argue that some nodes are not trustworthy. For example, if we constrain the translation of “f1 f2 f3 f4” with node N2 illustrated in Figure 1, then word “e1” will never be put in the middle the other three words. If we want to obtain the translation “e2 e1 e4 e3”, node N3 can offer a good constraint while node N2 should be filtered out. In real corpora, cases such as node N2 are frequent enough to be noticeable (see Fox (2002) or section 4.1 in this paper). Therefore, we use the definitions in Galley et al. (2004) to classify the nodes in parse trees into two types: frontier nodes and interior nodes. Though the definitions were originally made for target language parse trees, they can be straightforwardly applied to the source side. A node which satisfies both of the following two conditions is referred as a frontier node: • All the words covered by the node can be translated separately. That is to say, these words do not share a translation with any word outside the coverage of the node. • All the words covered by the node remain contiguous after translation. Otherwise the node is an interior node. Fo</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What&apos;s in a translation rule? In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in word replacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="944" citStr="Knight, 1999" startWordPosition="129" endWordPosition="130">parse trees offer very useful but imperfect reordering constraints for statistical machine translation. A lot of effort has been made for soft applications of syntactic constraints. We alternatively propose the selective use of syntactic constraints. A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system. 1 Introduction In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering c</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in word replacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. In ACL demo and poster sessions.</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst.</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL demo and poster sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
</authors>
<title>Kiyotaka Uchimoto, Jun&apos;ichi Kazama, Yiou Wang, Kentaro Torisawa and Hitoshi Isahara.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<marker>Kruengkrai, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun&apos;ichi Kazama, Yiou Wang, Kentaro Torisawa and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In ACL-COLING.</booktitle>
<contexts>
<context position="1267" citStr="Liu et al., 2006" startWordPosition="178" endWordPosition="181">arse trees should be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system. 1 Introduction In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phra</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1287" citStr="Marcu et al., 2006" startWordPosition="182" endWordPosition="185">be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system. 1 Introduction In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases. Recent efforts </context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="1978" citStr="Marton and Resnik, 2008" startWordPosition="292" endWordPosition="295">traints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases. Recent efforts attack this problem by using the constraints softly (Cherry, 2008; Marton and Resnik, 2008). In their methods, a candidate translation gets an extra credit if it respects the parse tree but may incur a cost if it violates a constituent boundary. In this paper, we address this challenge from a less explored direction. Rather than use all constraints offered by the parse trees, we propose using them selectively. Based on parallel training data, a classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. As a result, we obtain a 0.8 BLEU point improvement over a full constraint-based system. 2 Reordering Constraints </context>
<context position="13232" citStr="Marton and Resnik (2008)" startWordPosition="2230" endWordPosition="2233">frontier yields a 0.8 BLEU point improvement over the baseline constraint-based system which uses all the constraints. On the other hand, constraints from interior nodes result in the worst performance. This comparison shows it is necessary to explicitly distinguish nodes in the source parse trees when they are used as reordering constraints. The improvement over the system without constraints is only modest. It may be too coarse to use pare trees as hard constraints. We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). 5 Conclusion and Future Work We propose a selectively approach to syntactic constraints during decoding. A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. Preliminary results show that it is not only advantageous but necessary to explicitly distinguish between frontier nodes and interior nodes. The idea of selecting syntactic constraints is compatible with the idea of using constraints softly; we plan to combine the two ideas and obtain further improvements in future work. Acknowledgments We wo</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10111" citStr="Och (2003)" startWordPosition="1721" endWordPosition="1722">vered by the current node and the word before it • The rightmost word covered by the current node and the word after it 4 Experiments Our SMT system is based on a fairly typical phrase-based model (Finch and Sumita, 2008). For the training of our SMT model, we use a modified training toolkit adapted from the 2 http://www.cs.huji.ac.il/~shais/code/index.html MOSES decoder. Our decoder can operate on the same principles as the MOSES decoder. Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder’s parameters, and it is performed using the standard technique of Och (2003). A lexical reordering model was used in our experiments. The translation model was created from the FBIS corpus. We used a 5-gram language model trained with modified Knesser-Ney smoothing. The language model was trained on the target side of FBIS corpus and the Xinhua news in GIGAWORD corpus. The development and test sets are from NIST MT08 evaluation campaign. Table 1 shows the statistics of the corpora used in our experiments. Data Sentences Chinese English words words Training set 243,698 7,933,133 10,343,140 Development set 1664 38,779 46,387 Test set 1357 32377 42,444 GIGAWORD 19,049,75</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for SVM.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8467" citStr="Shalev-Shwartz et al, 2007" startWordPosition="1435" endWordPosition="1438"> less than three for the source or the target word. Additionally, we also remove notoriously bad links in N2 N3 N1 18 {de, le} × {the, a, an} following Fossum and Knight (2008). Thirdly, given the parse trees and the alignment information, we label each node as a frontier node or an interior node according to the definition introduced in this section. Using the labeled nodes as training data, we can build a classifier. In theory, a broad class of machine learning tools can be used; however, due to the scale of the task (see section 4), we utilize the Pegasos 2 which is a very fast SVM solver (Shalev-Shwartz et al, 2007). 3.2 Features For each node in the parse trees, we use the following feature templates: • A context-free grammar rule which rewrites the current node (In this and all the following grammar based features, a mark is used to indicate which non terminal is the current node.) • A context-free grammar rule which rewrites the current node’s father • The combination of the above two rules • A lexicalized context-free grammar rule which rewrites the current node • A lexicalized context-free grammar rule which rewrites the current node’s father • Syntactic label, head word, and head POS tag of the cur</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer and Nathan Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for SVM. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="1181" citStr="Wu, 1995" startWordPosition="165" endWordPosition="166">straints. A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system. 1 Introduction In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior </context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995. Stochastic inversion transduction grammars with application to segmentation, bracketing, and alignment of parallel corpora. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2000</date>
<booktitle>In ACL.</booktitle>
<marker>Yamada, Knight, 2000</marker>
<rawString>Kenji Yamada and Kevin Knight. 2000. A syntaxbased statistical translation model. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirofumi Yamamoto</author>
</authors>
<title>Hideo Okuma and Eiichiro Sumita.</title>
<date>2008</date>
<marker>Yamamoto, 2008</marker>
<rawString>Hirofumi Yamamoto, Hideo Okuma and Eiichiro Sumita. 2008. Imposing constraints from the source tree on ITG constraints for SMT. In Workshop on syntax and structure in statistical translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In SMT Workshop, HLT-NAACL.</booktitle>
<contexts>
<context position="1316" citStr="Zollmann and Venugopal 2006" startWordPosition="186" endWordPosition="189">ing constraint or not. Using this information yields a 0.8 BLEU point improvement over a full constraint-based system. 1 Introduction In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases. Recent efforts attack this problem by using </context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In SMT Workshop, HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>