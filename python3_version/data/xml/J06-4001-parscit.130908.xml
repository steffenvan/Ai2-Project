<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.696798333333333">
ACL Lifetime Achievement Award
Old Linguists Never Die, They Only Get
Obligatorily Deleted∗
</title>
<author confidence="0.986375">
Eva Hajiˇcov´a**
</author>
<affiliation confidence="0.958174">
Charles University
</affiliation>
<sectionHeader confidence="0.989813" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.998638375">
Martin Kay, in his speech delivered in 2005 on receipt of his ACL Lifetime Achievement
Award, specified computational linguistics as follows: “Computational linguistics is
trying to do what linguists do in a computational manner” (Kay 2005, page 429). I
believe it is a legitimate question for a computational linguist to ask what linguists do.
Coming from Prague, it is then quite a natural question for me to look back and to
recollect what the “old” linguists (who never die but get obligatorily deleted on the
visible surface) with the background of the world-famous Prague Linguistic School
(PLS) contributed to linguistic studies and perhaps to suggest what aspects of their
heritage are even today fruitful for computational linguistics.
First, to place the PLS in the course of the development of linguistic studies, it
should be recalled that the Prague Linguistic Circle belongs to the first bodies that took
part in the transition of the older diachronic paradigm of linguistics to a synchronic
theory of language. Soon after its first session (taking place in 1926 in the office of
the chairman of the Circle till his death in 1945), the Circle entered the international
scene first of all with its systematically elaborated phonological theory. Starting with
the Hague Linguistic Congress (see Actes 1928), Praguian phonology became the pilot
discipline of structural linguistics. This approach was far from unified, but the strength
of the Circle was in its spirit of dialogue, which kept the Circle receptive to new ideas,
rather than in any set of postulates commonly professed. In my talk I will concentrate
on three fundamental Prague School tenets, which I believe to have their validity also
in the modern context of linguistic theory and computational linguistics. What I have in
mind here is the Circle’s structural and functional orientation, as well as the attention it
has paid to the opposition of the center and the periphery of language structure, based
on the concept of markedness.
</bodyText>
<sectionHeader confidence="0.934223" genericHeader="method">
2. The Structural Point of View of PLS
</sectionHeader>
<bodyText confidence="0.9997468">
The PLS is generally (and truly) characterized by two attributes: structural and func-
tional. Let us first turn to the structural point of view, namely, the School’s endeavor
to view language as a system of systems rather than to study individual phenomena as
ad hoc, non-systematic issues. The Circle shared de Saussure’s understanding of lan-
guage as a system of (bilateral) signs, in which only oppositions rather than fixed
</bodyText>
<footnote confidence="0.92227075">
* Logo on the Indiana University Linguistic Club tee-shirt, 1984.
** Matematicko-fyzik´alnifakulta, Univerzita Karlova, Malostransk´e n´amesti25, CZ-11800 Praha,
Czech Republic. This paper is the text of the talk given on receipt of the ACL’s Lifetime
Achievement Award in 2006.
</footnote>
<note confidence="0.579831">
© 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
</note>
<bodyText confidence="0.993025">
entities play a role. As mentioned above, this attitude was most apparently reflected
in the study of phonology as a system displaying distinctive features and employing
the notion of binary oppositions. Jakobson (1929) presented the phonological repertory
(both in synchrony and in diachrony) as a system of oppositions (mainly binary, priva-
tive), based on acoustic distinctive features and understood as the clue to the sound and
meaning relationship.
Along with phonemes and morphemes, the sentence was also recognized as one of
the fundamental fields of systematic oppositions, that is, as an ingredient of la langue.
Mathesius (1928, 1936) formulated a concept of functional syntax; a structural view of
syntax, based on the dependency relation, was elaborated by Tesni`ere (1934), a French
member of the Circle, who was a professor of the Ljubljana University; his monograph
was published only posthumously (Tesni`ere 1959), but his papers were known in
Prague, and his approach to syntax was applied to Czech by ˇSmilauer (1947), who
combined dependency syntax with a constituent-based view of the relation between
predicate and subject.
Dependency-based approaches, which understand the verb as the center of the
sentence structure and describe this structure on the basis of binary relations between
heads and their modifiers, have been for a long time a matter of Continental syntactic
theories rather than of the mainstream syntactic approaches on the other side of the
Atlantic. However, the notion of head can be found also in Bloomfield (1933) when
referring to the names of the main constituents of the sentence, that is, NP (noun phrase,
with N as its head) and VP (verb phrase, with V as its head). In the framework of
the Chomskyan approach, originally based exclusively on the concept of immediate
constituents, the notion of head becomes the basic notion of X-bar theory. Originally,
four categories were singled out as possible heads of their respective maximal pro-
jections, namely, N, V, Adj, and P(rep); as remarked by James McCawley (personal
communication, around 1990), such a theory may be interesting unless the specification
of the set of basic categories grows beyond some reasonable limit. McCawley’s critical
remark reflected the gradual development of X-bar theory, which allowed practically
any constituent (or, more generally speaking, any arbitrary symbol for a grammati-
cal value) to act as the head, dependent on the needs of the analysis of this or that
construction.1
The very name head-driven phrase structure grammar, an influential theory combining
an immediate constituent approach with elements of a dependency-based approach,
as proposed by Pollard and Sag (1994), explicitly points out that the theory takes ac-
count of the main element within a constituent. Although their approach is constituent
based (working with a lexically based X-bar syntactic theory; [Pollard and Sag 1994,
page 362]), the authors are aware that the notion of constituent structure is widespread
but that it is not based on sufficiently convincing direct evidence. The authors refer to
Hudson’s (1984) approach and claim that it belongs to exceptions that do not overesti-
mate the constituent structuring of sentence elements.
It is sometimes doubted if the direction of the dependency relation, namely, the
determination of the governor and the dependent in each pair (syntagm) can be reliably
stated. We believe that in the prototypical case, the main criterion for this distinction
1 To be fair, I should add that one of the rare attempts at a more explicit characterization of the notion
of head can be found in Adger’s monograph on minimalism (Adger 2003, page 75), which mentions
two criteria: one based on the distribution of the whole constituent and the other taking into account
which constituent determines the reference of the whole constituent.
</bodyText>
<page confidence="0.996495">
458
</page>
<note confidence="0.58228">
Hajiˇcov´a Old Linguists Never Die
</note>
<bodyText confidence="0.998994244444445">
can be based on the possibility that, in the endocentric constructions, the dependent
can be absent (not just deleted on the surface). Thus, for example, in Yesterday, my father
worked for the whole day in the garden it is possible to leave out the dependents yesterday,
my, for the whole day, and in the garden without the sentence losing its grammaticality.
However, there are exocentric pairs such as (to) find something, where neither of the two
members of the pair can be deleted and for which, therefore, the mentioned method
by itself cannot help to find out which element is the governor and which is the
dependent. What helps here is the principle of analogy on the level of parts of speech:
On the basis of the existence of verbs without objects it can be concluded that in such
pairs the verb is the governor (in our particular example, (to) find). In the same vein,
subject (Actor) can be understood as dependent on the verb, because there are verbs
also without a subject: In It is raining (Latin Pluit), It is just a surface “filler,” absent
in the sentence structure proper. This view is also supported by the observation based
on the annotators’ agreement when assigning the dependency structures (trees) in the
Prague Dependency Treebank: The annotators did not have too many troubles with the
determination of the direction of dependency; if there was a disagreement, it concerned
their assignment of the type (value) of the dependency relation (see Hajiˇcov´a, Pajas, and
Vesel´a 2002).
There is one linguistic phenomenon—present more or less in every language—all
syntactic theories have to bother about, namely, the relation between syntactic struc-
ture and word order (the discontinuity of constituents, for which Gazdar [1981] intro-
duced the term unbounded dependencies, used, for example, also by Pollard and Sag [1994,
pages 157ff.]; see also the term long-distance dependencies used by some other authors) or,
in terms of formal dependency descriptions, the non-projectivity of syntactic construc-
tions. Informally speaking, the strongly restrictive condition of projectivity says that if a
node a depends on b and there is a node c between a and b in the linear ordering, c is
subordinated to b (where subordinated means an irreflexive transitive closure of depends).
See Figure 1 for an example of non-projective parts of a tree; the vertical line leading
from a intersects the dependency edge leading from a to c.2
The more restricted the formal syntactic description is, the more valuable are the
observations based on it; in this sense, the condition of projectivity might well serve
its purpose. However, there are seemingly many non-projective constructions in the
surface shapes of the sentences. The task then is to attempt to classify the constructions
in which the condition of projectivity is not met in the surface shape of the sentence
and to attempt at a description not only meeting the condition as far as the core of the
language system (see Section 4) is concerned, but also accounting by some simple, well-
defined means for the cases of superficial non-projectivity (a preliminary formulation
of movement rules specified as a transition from projective underlying trees to strings
of morphemes in which the condition of projectivity cannot be applied can be found in
Sgall [1997] and Hajiˇcov´a and Sgall [2003]). This is, of course, a rather strong hypothesis
that has to be verified and made more precise on the basis of systematic empirical
research. It should be mentioned in this connection that it is in line with the Praguian
approach that function words are distinguished from autosemantic words and that only
the latter constitute nodes of their own in the underlying trees; from this it follows that
in the numerous cases in which the “non-projectivity” of surface word order concerns
</bodyText>
<footnote confidence="0.9983785">
2 Projectivity as a property of word order important for a formal description of language was already
stated by Hays (1960, 1964) and Lecerf (1960) in formal grammar; the condition of projectivity (in
different forms that have been proved as equivalent) was defined by Marcus (1965) and used in many
writings working with dependency descriptions.
</footnote>
<page confidence="0.996385">
459
</page>
<figure confidence="0.86335">
Computational Linguistics Volume 32, Number 4
</figure>
<figureCaption confidence="0.981904">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.878955">
Examples of non-projective parts of a dependency tree.
</subsectionHeader>
<bodyText confidence="0.999774794871795">
auxiliary verbs or conjunctions, and so forth, the projectivity of underlying syntactic
structure is not at stake.
The introduction of the notion of a head brings into the foreground the connection of
grammar and lexicon; the necessity of such a relationship was already quite apparent in
the earlier works of Fillmore (1966, 1968) introducing the so-called case grammar, which
explicitly follows up Tesni`ere’s notion of valency. The term “case” does not directly refer
to case as a morphological category but to the meaning (function) of a (morphological)
case: for example, Addressee is a prototypical meaning (function) of Dative, Agentive
is a prototypical meaning of Nominative, and so on. The concept of valency is crucial
in that it reflects the fundamental aspect of the presence of grammatical information
in the lexicon: The valency frame is a part of the lexical entry, in which the obligatory
and optional syntactic kinds of dependents of the given (head) word are registered.
It is also well known that Fillmore’s theory (as well as the thematic roles of Gruber
[1967]) played a substantial role in the introduction of theta roles (and theta grids) in
the Chomskyan model of government and binding (later called, more appropriately, the
Principles and Parameters model).
Fillmore himself explicitly mentioned that, when proposing his case grammar,
he did not primarily consider which formal description his approach would fit into.
However, he presents an example of how his approach can be formulated in terms of a
phrase structure model: The sentence S can be decomposed into two parts, Modality and
Proposition; the Proposition in turn can be articulated into the verb and a set of noun
phrases, which are characterized by one of the case markers, that is, K1NP, K2NP,...,
KnNP. Each of these noun phrases can then be decomposed into the noun phrase proper
and the given case marker ki (Agentive, Addressee, Objective, etc.). The analysis of
Robinson (1969, 1970) devoted to the relation between Fillmore’s approach and that
of transformational grammar (of that time) throws an interesting light on the possiblity
of a smooth transition from a phrase-based approach to a dependency-based one, which
is more transparent and economical. In Fillmore’s proposal, the case relations, that is,
the relations of the noun phrase to the verb, are actually captured twice, once as the
marker ki and once as the characteristics of the given phrase (KiNP). It is then possible
to work with a pure dependency tree structure, where the root of the tree is the verb,
and the nouns (or, as the case may be, other word categories) depend on the root as
dependents with a certain type of relation.
It goes without saying that Fillmore’s case grammar and its follow-up frame nets
conception has influenced in a substantial way many of the contemporary approaches
not only to treebank annotation and computational lexicology (cf., e.g., Fillmore et al.
2003) but also the work on underlying sentence structure in general.
Two “historically” motivated and seemingly contradictory observations are in place
here: It can be documented by references to the development of linguistic theory in the
</bodyText>
<page confidence="0.997766">
460
</page>
<subsectionHeader confidence="0.340248">
Hajiˇcov´a Old Linguists Never Die
</subsectionHeader>
<bodyText confidence="0.997046666666667">
past 50 years that the deeper the analysis goes, the more the need of an introduction of
the distinction between the notions of “head” and “modifier” (predicate, argument) is
felt. Let us only recall here such approaches as:
</bodyText>
<listItem confidence="0.9825435">
(i) the lexicosemantic analysis by Katz and Postal (1964), who work with the
notions of head and modifier when specifying selection restrictions;
(ii) the distinction between surface-oriented constituent structure and the
(underlying) functional structure in lexical functional grammar by Bresnan
(1978) and Kaplan and Bresnan (1982);
(iii) the above-mentioned case grammar by Fillmore, motivated by the
conviction that Chomskyan deep structure (with its specification of “deep”
subject as a constituent of S regardless of the (different) semantic relations
of the given NP to the verb) is not deep enough to capture the real
underlying structure of the sentence; and
(iv) the consecutive introduction of theta roles into the government and
binding theory.3
</listItem>
<bodyText confidence="0.977833">
On the other hand, dependency-based considerations have gradually and evasively
penetrated to the ”data”-oriented statistical methods and treebank annotations. As the
freshest example, let us only refer to the recent EACL 2006 conference in Trento and the
HLT-NAACL 2006 conference in New York with its CoNLL-X Shared Task on Multilin-
gual Dependency Parsing (working with treebanks of 12 languages, of different sizes).
In other words, the seemingly surface oriented analysis is prevailingly dependency
based.4
A possible explanation for this apparent contradiction may be looked for in the
economy and transparency of the dependency-based trees: In their applications, the
data-oriented systems also aim at a representation of the meaning of the surface shapes
of sentences (whatever one can understand by “meaning”), so that their attention is
focused on a most transparent and economic way (avoiding ”extra” nodes for phrases
such as NPs, VPs,..., etc.) from the surface to the depth. Dependency analysis offers
such a way.5
3 We have restricted our attention here only to systems staying in principle within the development
of the Chomskyan paradigm or originating as a reaction to it. However, when discussing the relation
between or combination of constituent-based and dependency-based grammars, special attention
should be paid to the lexicalized tree-adjoining grammars (LTAG) continuing the original conception
of tree-adjoining grammar (TAG) as proposed by A. K. Joshi (see, e.g., Joshi 1985), which has served
as a basis for many studies on formal grammar as well as from the NLP domain. The similarity
between LTAG and a dependency-based description in relation to the model using the so-called
supertags (which encode syntactic information in terms of dependency) is analyzed by Joshi and
Srinivas (1994).
4 It should be recalled in this connection that within machine translation dependency-based systems
(sometimes in combination with phrase structure) were already at play in the early times of MT;
see, for example, the works of B. Vauquois, one of the founders of computational linguistics (Vauquois
1975; Vauquois and Chappuy 1985) and the systems developed in Japan under the influence and
guidance (direct or indirect) of M. Nagao (see the survey in Nagao [1989]).
5 In a similar vein, Steedman (2005) argues that the use of statistical language models is the only
way to create a computer program that automatically analyzes sentences on the basis of broadly
conceived grammars (with due regard to ambiguities) such as dependency-based grammars or
grammars specifying heads (governors); according to Steedman, these grammars work well because
they reflect a mixture of semantic information and information based on knowledge of the world.
</bodyText>
<page confidence="0.993583">
461
</page>
<note confidence="0.291863">
Computational Linguistics Volume 32, Number 4
</note>
<bodyText confidence="0.999742">
Among urgent questions to be asked with regard to the approaches to sentence
structure, there are then the following issues:
</bodyText>
<listItem confidence="0.92600925">
(i) Is it more appropriate to analyze a sentence such as In this garden, she was
reading a book on the history of Spain yesterday as having the complex verb
form was reading as its head, with she, (a) book, and garden as its dependents,
or to see the basic characteristics of its structure in distinguishing whether
in this garden or yesterday is more immediately connected with its verb?
(ii) Do we have clearer criteria for answering the former or the latter of these
two questions?
3. Prague School Functionalism
</listItem>
<bodyText confidence="0.999983771428571">
The other attribute of Prague structuralism is functional. Mathesius (1928, 1936), in-
spired especially by the philosophy of language of Marty (1908), presented his theory of
functional grammar, based on the concept of function as related to universal intentional
acts and treated as a dichotomy of functional onomatology and functional syntax.
Mathesius combined this universal dichotomy with the language-specific opposition
of function and form. As Sgall (1987) pointed out, the core of the system of language
was conceived of as consisting of levels, the units of which have their functions in that
they represent or express units of the adjacent higher levels, up to the non-linguistic
layer of cognitive content. The units of the system were understood as constituting
hierarchies in which some of them function as certain parts of the others. Thus, for
example, phonemes were defined and delimited one against the other on a functional
basis (two different phonemes can distinguish two morphemes), and the established
repertory of distinctive features gave a firm foundation to the description of the system
of phonemes as a structured whole. Strings of phonemes (morphs, in more modern
terminology) are understood as expressing morphemes, and sequences of morphemes
as expressing sentence structure.
Another important aspect of the functional approach is to view language as a
functioning system, adapted to its communicative role, diversified in more or less
different social and local varieties, and to describe the sentence structure as adapted
to its functioning in discourse.
This leads me to pay attention to the information structure (in our terms, topic–
focus articulation) of the sentence. Let me first look again at the history of the issue. It
was the Prague scholar Mathesius (1929, 1938) who introduced the study of information
structure into structural linguistics, preferring the terms Thema and Rhema (used before
in German linguistics by H. Ammann) to the older psychologisches Subjekt and Pr¨adikat
(used by G. von der Gabelentz, H. Paul, and others), and understanding the former
(the topic) as one of the functions of the subject in English. He distinguished topic
proper, comment (focus) proper, and the accompanying elements of either of these
two parts. Later on, one of Mathesius’ followers, Jan Firbas, extended the hierarchical
understanding of the information structure of the sentence by postulating a scale of
communicative dynamism. The Praguian concepts met a favorable response within
continental linguistics (one should mention in this connection especially the works by
British linguists M. A. K. Halliday and H. W. Kirkwood, several German linguists such
as J. Esser, R. Bartsch, and J. Jacobs, the French linguist J.-M. Zemb, and others). How-
ever, only the syntactic or word order consequences (or, as the case may be, conditions)
</bodyText>
<page confidence="0.995186">
462
</page>
<note confidence="0.406883">
Hajiˇcov´a Old Linguists Never Die
</note>
<bodyText confidence="0.997626419354838">
of different sentence articulations into topic and focus were mostly taken into account,
and its relevance for and effects on the coherence of discourse.
A new impetus into the study of information structure was given by Petr Sgall,
who was the first to come up with examples testifying to the semantic effects
of this issue and claiming that two utterance tokens differing in their topic–focus
articulation are tokens of two different sentences, that is, that topic and focus belong
to the language system rather than only to the use of language in communication (Sgall
1967, page 205ff.). As a matter of fact, the split of transformational grammar into the
generative and the interpretative semantics wings coming out at the same time operated
with arguments based on sentences that in Praguian terms differ only in their topic–
focus structure (this fact, of course, not being recognized by the authors): See Lakoff’s
(1969) examples: Many men read few books against Few books are read by many men, John
talked about many problems to few girls versus John talked to few girls about many problems.
To be fair to the other side of the dispute, Chomsky (1965, page 224) noticed the semantic
difference between the sentences Everybody in the room knows at least two languages and
At least two languages are known by everybody in the room and was in doubt as to whether
this difference should be ascribed to the difference between active and passive; he
remarks that such a distinction might be described in terms of topic (as Lakoff [1969]
notes, in this consideration, the influence of Halliday [1967–1968] played its role). In
his reaction to the generative semanticists’ criticism of the ”shallowness” of his deep
structure, Chomsky (1968) was even more inclined to use notions related to what in
present-day terms would be called the information structure of the sentence; he claims
that in the semantic interpretation of the sentence, one should take into account the
distinction between what he calls presupposition and focus, and a related notion of
the range of permissible focus. There are two interesting points in his approach: First,
Chomsky connects these syntactic issues with the placement of the intonation center
in the spoken form of the sentence, and second, he connects the possible operational
criterion for the determination of the choice of focus from the range of permissible focus
with the scope of negation. In particular, to decide what is the focus of the answer to Was
it an ex-convict with a red SHIRT that he was warned to look out for?, one should consider
possible different negative continuations such as No, he was warned to look out for an
</bodyText>
<construct confidence="0.5227745">
AUTOMOBILE SALESMAN, or ... for an ex-convict wearing DUNGARIES, ... for an ex-
convict with a CARNATION, ... for an ex-convict with a red TIE.
</construct>
<bodyText confidence="0.999592764705883">
One could argue that it is the presence of structures with quantification rather than
the topic–focus articulation of the quoted examples that is responsible for the indicated
semantic distinction. However, the Praguian writings from the sixties convincingly
demonstrate that it is not difficult to find sentences without quantification that exhibit
the same phenomenon (for reasons I will mention in a minute, in the examples, the
capitals indicate the intonation center): Russian is spoken in SIBERIA versus In Siberia,
RUSSIAN is spoken, or John works on his dissertation on WEEKENDS versus On weekends,
John works on his DISSERTATION. In Russian linguistics, such examples have been
discussed as Kurit’ ZDES’ versus Zdes’ KURIT’. The sentences quoted also document
that the difference cannot be ascribed to the active/passive distinction; neither can it
be claimed that the word order always plays a decisive role: Consider Halliday’s (1970)
famous example from a London underground station: Dogs must be CARRIED. With the
same word order, but with a change in the placement of the intonation center one gets
a certainly unwanted interpretation: DOGS must be carried would imply that everybody
stepping on the escalator has to carry a dog (in a similar vein as Carry DOGS.). A
plausible explanation of the semantic difference covering all these cases is to describe
them in terms of difference in their information structure.
</bodyText>
<page confidence="0.997661">
463
</page>
<note confidence="0.494675">
Computational Linguistics Volume 32, Number 4
</note>
<bodyText confidence="0.99872102631579">
This had not been recognized or at least commonly accepted for some time on an
international scale until the appearance of Mats Rooth’s Ph.D. dissertation in 1984.6 Al-
though restricted to prosodic focus (pointing out that the difference in truth conditions
between such sentences as Mary only introduced BILL to Sue and Mary only introduced
Bill to SUE is only in the location of focus, denoted here by capitals), Rooth’s work
was an impetus for an increasing interest in the related issues, first in the domain of
formal semantics (here the influential role of Barbara H. Partee should be emphasized),
but soon literally everywhere. Let us mention in this context that semantic consid-
erations apparently stood behind the conception of combinatory categorial grammar
first proposed by Steedman (1996, 2000); his introduction of floating constituents, the
division line between which is given by the articulation of the sentence with regard to its
information structure rather than fixed, determined once for all. Steedman, in contrast to
many other researchers presently working in the domain of information structure, pays
a due respect to the close relation between information structure, syntactic sentence
structure, and prosody; in this respect, also the work on corpus annotation led by him
is a pioneering enterprise (Calhoun et al. 2005).7
Due respect paid to the description of the information structure of the sentence is
also crucial for the study of discourse structure and coherence. It might be interesting
to note in this connection that the first systematic study indicating such a relation—
although in terms influenced by the then prevailing psychological view of language—
is Weil’s (1844) study on the order of words. The author introduces the notions of
progression of “ideas,” distinguishing “progression” and marche parall`ele: In the former
sequence (segment), the given sentence B is connected to the preceding sentence A by
starting with a reference to the idea that was at the end of A, whereas in the latter, the
sentences “march in parallel,” that is, they begin with a reference to the same idea. It
is not difficult to see an analogy between this view and a more modern and explicit
treatment of the so-called centering theory (Grosz, Joshi, and Weinstein 1995) and its
shifts of centers.
I am dwelling at such length on the issues of information structure not just because
it is my favorite child (and, indeed, it is), but because I am fully convinced about the
importance of this issue for an adequate description of the sentence structure, both
in formal description of language as well as in natural language processing. Let me
illustrate by a personal recollection that I am not beating a straw man. Some time
ago (if I am not mistaken, it was in 1989) I was invited for an IBM-organized MT
conference in Garmisch-Partenkirchen to deliver a talk on the Praguian approach to
MT. Naturally enough, I devoted most of my time to illustrate examples of translations
from and to several European languages that topic–focus articulation as an important
aspect the translation (be it human or automatic) has to take into account. The group of
</bodyText>
<footnote confidence="0.537008384615385">
6 From a different perspective, the term focus was used by Grosz (1977). The author adopted a
psychological point of view of focus of attention and considered the sentence focus to be that item that is
in such a focus, that is, in our terms the topic of the sentence (what the sentence is about). Grosz’ approach
has found its continuation in the centering theory mentioned below.
7 Let us note in this connection that the difficulties of a syntactic description based on phrase structure for
an adequate capturing of the topic–focus articulation were pointed out already in Sgall, Hajiˇcov´a, and
Beneˇsov´a (1973, page 163ff.) and in Hajiˇcov´a and Sgall (1975) and illustrated on examples such as This
year we will spend two weeks on Mallorca used in the context of How will you spend your holidays this year?, i.e.
with the focus part of the sentence being two weeks on Mallorca. Working with phrase structure, it would
be very difficult to characterize the two groups as a single phrase; in a similar vein, to determine the
topic of the sentence as a single phrase is also difficult: if the question were Where do you spend two holiday
weeks this year?, the focus of the answer would be on Mallorca, with the topic being this year we will spend
two weeks.
</footnote>
<page confidence="0.998456">
464
</page>
<note confidence="0.406067">
Hajiˇcov´a Old Linguists Never Die
</note>
<bodyText confidence="0.999945789473684">
people attending the meeting was extremely nice and friendly, and it was no wonder
that the program chairs, Margaret King and Jonathan Slocum, could dare to make the
concluding session a sort of fun ascribing to each of the papers some characteristic
evaluative attribute: It was quite symptomatic of those times that the issues discussed
in my paper were characterized as “least important for MT” (it may be of interest to
recall that Mercer’s paper on the IBM statistical approach to MT delivered there was
evaluated as “crazy science fiction”).
The question should then be discussed in which way is it possible to describe the
interplay of the dependency (or constituency)-based patterning of the sentence and the
topic–focus articulation (or information strcuture) of the sentence as two basic aspects
of syntax (now cf. Hajiˇcov´a and Sgall, in press). Is it true that a dependency-based view
of the underlying structure as the core of the language system (in which there are no
nodes corresponding to function words and the left-to-right order of lexical items meets
the condition of projectivity; cf. Section 2 above) might be useful in this respect?
I am happy to see that much has changed in this particular domain of studies since
those times; I cannot say I welcome all the changes, but it is encouraging to see that the
two Praguian tenets I have discussed so far—namely, the dependency approach and the
due regard to the information structure—have found an undisputable appraisal within
our field.
</bodyText>
<sectionHeader confidence="0.877386" genericHeader="method">
4. The Core of Language and the Periphery
</sectionHeader>
<bodyText confidence="0.999976464285714">
The third Praguian notion I would like to mention is the distinction made between the
center (core) of language and the periphery. This distinction is closely connected with
the notion of markedness; markedness, characterizing the intrinsic asymmetry of binary
(and other) oppositions (not only in phonology, but also in morphology, in semiotics,
and in many other domains), was first systematically presented by R. Jakobson. It
was properly understood and used as an organizing principle of sign systems, also
in connection with language universals and language acquisition. As Battistella (1995)
notes, this notion belongs to those aspects of the Prague linguistic theory that in some
form have been taken over by Chomsky, who applied it, albeit in a different shape, in
his Principles and Parameters theory, as proposed in the early 1980s.
Although the relationships between the two oppositions of marked versus un-
marked phenomena and the core versus the periphery of the system of language are
far from straightforward (see Sgall 2002, 2004), it can be claimed that because language
is more stable in its core, regularities in language should be searched for first in this core;
only then is it possible to penetrate into the subtleties and irregularities of the periphery.
The relatively simple pattern of the core of language (in Sgall’s view, not far from the
transparent pattern of propositional calculus) makes it possible for children to learn
the regularities of their mother tongue on the basis of shared human mental capacities,
instantiated also by systems such as those of elementary arithmetic or Aristotelian logic.
The freedom of linguistic behavior, limited only by the speakers’ desire to be understood
by their audience, offers space for the flexibility of the large and complex periphery (i.e.,
not only of individual exceptions, but also by most different sets of marked phenomena
determined by contextual conditions and lists).
The question to be asked then is which of the two possible approaches to how
to project this view to a formal description of language is to be preferred: to attempt
to describe all phenomena “at once,” that is, to consider language as a whole and to
describe all phenomena “at a single layer,” or to proceed from the core of the system to
its periphery.
</bodyText>
<page confidence="0.998721">
465
</page>
<note confidence="0.333375">
Computational Linguistics Volume 32, Number 4
</note>
<listItem confidence="0.852664">
5. Corpus Annotation as a Test of Linguistic Theories
</listItem>
<bodyText confidence="0.999940263157895">
At the beginning of my talk, I promised to suggest which aspects of Praguian heritage
(and in a more general view, of linguistics as such) I believe to have been fruitful
for computational linguistics. When talking about the three particular aspects I have
chosen, I have tried to make some suggestions as to urgent questions to be asked. Let me
finish my talk by an illustration taken from the presently flourishing field of language
resources, corpus annotation, and evaluation.
It has been already commonly accepted in computational and corpus linguistics that
grammatical (or lexical semantic, etc.) annotation does not “spoil” a corpus, because the
annotation is done “in addition” to the raw corpus. Thus, on the contrary, annotation
may and should bring an additional value to the corpus. However, there are some
necessary conditions for an annotation to fulfil this aim: Its scenario should be carefully
(i.e., systematically and consistently) designed, and it should be based on a sound
linguistic theory. This view is corroborated by the existence of annotated corpora of
various languages (even if their creation is mostly done manually but supported by
annotator-friendly software tools or semiautomatic procedures): the Penn Treebank, its
successors as PropBank or the Penn Discourse Treebank, Tiger, the Prague Dependency
Treebank, and several others. These conditions being met, corpus annotation serves,
among other things, as an invaluable test for the linguistic theories standing behind
the annotation schemes, and as such represents an irreplaceable resource of linguis-
tic information for the construction and enrichment of grammars, both formal and
theoretical.
This claim can be documented by the case of the multilayered annotation of the
Prague Dependency Treebank (PDT; see, e.g., Hajiˇc 1998), which is based on the frame-
work of the Functional Generative Description (see, e.g., Sgall, Hajiˇcov´a, and Panevov´a
1986). It is important to note that the PDT annotation concerns not only the surface
and morphemic shape of sentences, but also (and first of all) the underlying sentence
structure (tectogrammatical layer), which elucidates phenomena hidden on the sur-
face although unavoidable for the representation of the meaning and functioning of
the sentence, for modeling its comprehension and studying its semantico-pragmatic
interpretation, for the work on lexical semantics, and for dictionary buildup and many
other aims.
We have tried to demonstrate on certain selected grammatical and discourse phe-
nomena (in Hajiˇcov´a, Sgall 2006) that the process of the annotation during the last
decade and its results have allowed for an enrichment of this framework in several
regards. In particular, our examples were taken from the domain of the condition of
projectivity, classification of dependency relations, topic–focus articulation (the biparti-
tion of the sentence into topic and focus and the cannonical underlying word order in
the focus of the sentence), and some aspects of discourse structure.
</bodyText>
<sectionHeader confidence="0.984452" genericHeader="method">
6. Final Remarks
</sectionHeader>
<bodyText confidence="0.998009833333333">
I have always been an optimist, and therefore let me go back to the Indiana Univer-
sity Linguistic Club logo from 1984 quoted in the title of my talk: I strongly believe
that old linguists never die, they only get obligatorily deleted. Deletions concern the
surface rather than the underlying structure so that we may hope that while the old
linguists’ bodies may lie a-moldering in their graves, the best of their ideas will be
marching on.
</bodyText>
<page confidence="0.998972">
466
</page>
<note confidence="0.652552">
Hajiˇcov´a Old Linguists Never Die
</note>
<sectionHeader confidence="0.983158" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999977647058824">
I would like to express my most sincere
thanks to the ACL for the Award and thus
for having given me the opportunity to pay
tribute in this talk to my Praguian teachers. I
would also like to express my deep gratitude
to my mentor and colleague, Petr Sgall, the
founder of Czech computational linguistics,
whose original ideas as well as broad scope
of knowledge and vision have made it
possible for the Prague School linguistic
ideas to cross over the boundaries of time, of
geographic zones, and of linguistic trends
and orientation. Last but not least, I would
like to pay credit to my younger colleagues
and students, the energy, commitment, and
friendliness of whom makes me not to think
of age and to enjoy my professional life.
</bodyText>
<sectionHeader confidence="0.989087" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983526197916667">
Actes du Premier Congr`es international des
linguistes a` la Haye.1928. A. W. Sijthoff,
Leiden.
Adger, David. 2003. Core Syntax. A Minimalist
Approach. Oxford University Press, Oxford.
Battistella, Edwin. 1995. Jakobson and
Chomsky on markedness. In E. Hajiˇcov´a,
M. ˇCervenka, O. Leˇska, and P. Sgall,
editors, Prague Linguistic Circle Papers 1.
John Benjamins, Amsterdam, pages 55–72.
Bloomfield, Leonard. 1933. Language. Holt,
Rinehart and Winston, New York.
Bresnan, Joan. 1978. A realistic
transformational grammar. In M. Halle
et al., editor, Linguistic Theory and
Psychological Reality. MIT Press,
Cambridge, MA, pages 1–59.
Calhoun, Sasha, Malvina Nissim, Mark
Steedman, and Jason Brenier. 2005. A
framework for annotating information
structure in discourse. In A. Meyers,
editor, Pie in the Sky. Proceedings of the
ACL Workshop 2005. Ann Arbor, MI,
pages 45–52.
Chomsky, Noam. 1965. Aspects of the Theory of
Syntax. MIT, Cambridge, MA.
Chomsky, Noam. 1968. Deep structure,
surface structure and semantic
interpretation. In D. D. Steinberg and
L. A. Jakobovits, editors, Semantics: An
Interdisciplinary Reader in Philosophy,
Linguistics and Psychology. Cambridge
University Press, Cambridge,
pages 183–216.
Fillmore, Charles J. 1966. Toward a modern
theory of case. In D. A. Reibel and S. A.
Schane, editors, Modern Studies in English.
Prentice-Hall, Englewood Cliffs, NJ,
pages 361–375.
Fillmore, Charles J. 1968. The case for case. In
E. Bach and R. Harms, editors, Universals
in Linguistic Theory. Holt, Rinehart, and
Winston, New York, pages 1–90.
Gazdar, Gerald. 1981. Unbounded
dependencies and coordinate structure.
Linguistic Inquiry, 12:155–184.
Grosz, Barbara J. 1977. The Representation and
Use of Focus in Dialogue Understanding.
Ph.D. thesis, University of California,
Berkeley, CA.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203–225.
Gruber, Jeffrey S. 1967. Functions of the
lexicon in formal descriptive grammar.
Technical Report (TM)-3770/00, Systems
Development Corporation, Santa Monica.
Hajiˇc, Jan. 1998. Building a syntactically
annotated corpus: The Prague
Dependency Treebank. In E. Hajiˇcov´a,
editor, Issues of Valency and Meaning.
Studies in Honour of Jarmila Panevov´a.
Karolinum, Prague, pages 106–132.
Hajiˇcov´a, Eva, Petr Pajas, and Kateˇrina
Vesel´a. 2002. Corpus annotation on the
tectogrammatical layer: Summarizing of
the first stages of evaluation. The Prague
Bulletin of Mathematical Linguistics,
77:5–18.
Hajiˇcov´a, Eva and Petr Sgall. 1975. Topic and
focus in transformational grammar. Papers
in Linguistics, 8(1–2):13–58.
Hajiˇcov´a, Eva and Petr Sgall. 2003.
Dependency syntax in functional
generative description. In Vilmos Agel
et al., editors, Dependenz und Valenz, Vol. 1.
Walter de Gruyter, Berlin, pages 570–592.
Hajiˇcov´a, Eva and Petr Sgall. 2006. Corpus
annotation as a test of a linguistic theory.
In Proceedings of LREC 2006, Genoa.
Hajiˇcov´a, Eva and Petr Sgall. Forthcoming.
The fundamental significance of
information structure. In C. Caffi and
H. Haberland et al., editors, Future
Prospects of Pragmatics.
Halliday, Michael A. K. 1967–1968. Notes
on transitivity and theme in English.
Journal of Linguistics, 3:37–81, 199–244;
4:179–215.
Halliday, Michael A. K. 1970. A Course in
Spoken English: Intonation. Oxford
University Press, Oxford.
Hays, David G. 1960. Grouping and
dependency theories. In Proceedings of the
</reference>
<page confidence="0.996728">
467
</page>
<note confidence="0.540487">
Computational Linguistics Volume 32, Number 4
</note>
<reference confidence="0.996848966386554">
National Symposium on Machine Translation,
pages 258–266, Englewood Cliffs, NJ.
Hays, David G. 1964. Dependency theory:
A formalism and some observations.
Language, 40:511–525.
Hudson, Richard. 1984. Word Grammar.
Blackwell, Oxford.
Jakobson, Roman. 1929. Remarques sur
l’´evolution phonologique du russe
compar´ee a´ celle des autres langues slaves.
Travaux du Cercle Linguistique de Prague,
volume 2.
Joshi, Aravind. 1985. Tree-adjoining
grammars: How much context-sensitivity
is required to provide reasonable
structural descriptions? In D. Dowty,
editor, Natural Language Processing.
Cambridge University Press, Cambridge,
pages 206–250.
Joshi, Aravind and Bangalore Srinivas.
1994. Disambiguation of super parts of
speech (or supertags): Almost parsing.
In Proceedings of the 15th International
Conference on Computational Linguistics
(COLING 1994), pages 154–160,
Kyoto, Japan.
Kaplan, Ronald and Joan Bresnan. 1982.
Lexical-Functional Grammar: A formal
system for grammatical representation.
In Joan Bresnan, editor, The Mental
Representation of Grammatical
Relations. MIT Press, Cambridge, MA,
pages 173–281.
Katz, Jerrold J. and Paul M. Postal. 1964. An
Integrated Theory of Linguistic Descriptions.
MIT Press, Cambridge, MA.
Kay, Martin. 2005. A life of language.
Computational Linguistics, 31(4):425–438.
Lakoff, George. 1969. On generative
semantics. In D. D. Steinberg and L. A.
Jakobovits, editors, Semantics: An
Interdisciplinary Reader in Philosophy,
Linguistics and Pyschology. Cambridge
University Press, Cambridge,
pages 232–296.
Lecerf, Yves. 1960. Programme des conflits,
mod`ele des conflits. Traduction
Automatique, 1(4):11–18; 1(5):17–36.
Marcus, Solomon. 1965. Sur la notion de
projectivit´e. Zeitschrift f¨ur mathematische
Logik und Grundlagen der Mathematik,
11:181–192.
Marty, Anton. 1908. Untersuchungen zur
Grundlegung der allgemeinen Grammatik und
Sprachphilosophie 1. Halle/S.
Mathesius, Vil´em. 1928. On linguistic
characterology. Actes, pages 56–63.
Mathesius, Vil´em. 1929. Zur satzperspektive
im modernen Englisch. Archiv f¨ur das
Studium der neueren Sprachen und
Literaturen, 155:202–210.
Mathesius, Vil´em. 1936. On some problems
of the systematic analysis of grammar.
Travaux du Cercle Linguistique de Prague,
volume 6, pages 95–107.
Nagao, Makoto. 1989. Machine Translation:
How Far Can It Go? Oxford University
Press, Oxford.
Pollard, Carl and Ivan A. Sag. 1994.
Head-driven Phrase Structure Grammar.
University of Chicago Press, Chicago and
London.
Robinson, Jane J. 1969. Case, category and
configuration. Journal of Linguistics,
6:57–80.
Robinson, Jane J. 1970. Dependency
structures and transformational rules.
Language, 46:259–285.
Sgall, Petr. 1967. Functional sentence
perspective in a generative description.
Prague Studies in Mathematical Linguistics,
2:203–225.
Sgall, Petr. 1987. Prague functionalism
and topic vs. focus. In Ren´e Dirven and
Vil´em Fried, editors, Functionalism in
Linguistics. John Benjamins Publishing
Company, Amsterdam/Philadelphia,
pages 169–189.
Sgall, Petr. 1997. On the usefulness of
movement rules. In B. Caron, editor, Actes
du 16e Congr`es International des Linguistes
(Paris 20-25 juillet 1997). Elsevier Science,
Oxford.
Sgall, Petr. 2002. Freedom of language: Its
nature, its sources and its consequences.
Prague Linguistic Circle Papers, 4:309–329.
Sgall, Petr. 2004. Types of languages and the
simple pattern of the core of language. In
P. Sterkenburg, editor, Linguistics
Today—Facing a Greater Challenge (Plenary
lectures from CIL 17). John Benjamins,
Amsterdam/Philadelphia, pages 243–265.
Sgall, Petr, Eva Hajiˇcov´a, and Eva Beneˇsov´a.
1973. Topic, Focus and Generative Semantics.
Scriptor, Kronberg/Taunus.
Sgall, Petr, Eva Hajiˇcov´a, and Jarmila
Panevov´a. 1986. The Meaning of the Sentence
in Its Semantic and Pragmatic Aspects.
Reidel, Dordrecht; Academia, Prague.
ˇ
Smilauer, Vladim´ır. 1947. Novoˇcesk´a skladba
[The syntax of Modern Czech]. Mikuta,
Prague.
Steedman, Mark. 1996. Surface Structure and
Interpetation. The MIT Press, Cambridge,
MA.
Steedman, Mark. 2000. Information structure
and the syntax–phonology interface.
Linguistic Inquiry, 31:649–689.
</reference>
<page confidence="0.988175">
468
</page>
<reference confidence="0.97062144">
Hajiˇcov´a Old Linguists Never Die
Steedman, Mark. 2005. Grammar acquisition
by child and machine. Invited Talk at the
17th European Summer School of Language,
Logic and Information, Edinburgh.
Tesni`ere, Lucien. 1934. Comment construire
une syntaxe. Bulletin de la Facult´e des lettres
de Strasbourg, 12(7):219–229.
Tesni`ere, Lucien. 1959. El´ements de Syntaxe
Structurale. Klinksieck, Paris.
Vauquois, Bernard. 1975. La traduction
automatique a` grenoble. Documents de
linguistique quantitative, 24.
Vauquois, Bernard and Sylviane Chappuy.
1985. Static grammars: A formalism for the
description of linguistic models. In
Proceedings of the Conference on Theoretical
and Methodological Issues in Machine
Translation, pages 298–322, Colgate
University, Hamilton, New York.
Weil, Henri. 1844. De l’ordre des mots dans les
langues anciennes compar´ees aux langues
modernes (The Order of Words in the Ancient
Languages Compared with That of Modern
Languages), Paris; Amsterdam [1978].
</reference>
<page confidence="0.999625">
469
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.565411">
<title confidence="0.987828">ACL Lifetime Achievement Award</title>
<author confidence="0.57206">Old Linguists Never Die</author>
<author confidence="0.57206">They Only Get</author>
<affiliation confidence="0.916373">Charles University</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Actes du Premier Congr`es international des linguistes a` la Haye.1928. A. W. Sijthoff,</booktitle>
<location>Leiden.</location>
<marker></marker>
<rawString>Actes du Premier Congr`es international des linguistes a` la Haye.1928. A. W. Sijthoff, Leiden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Adger</author>
</authors>
<title>Core Syntax. A Minimalist Approach.</title>
<date>2003</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="6674" citStr="Adger 2003" startWordPosition="1048" endWordPosition="1049">direct evidence. The authors refer to Hudson’s (1984) approach and claim that it belongs to exceptions that do not overestimate the constituent structuring of sentence elements. It is sometimes doubted if the direction of the dependency relation, namely, the determination of the governor and the dependent in each pair (syntagm) can be reliably stated. We believe that in the prototypical case, the main criterion for this distinction 1 To be fair, I should add that one of the rare attempts at a more explicit characterization of the notion of head can be found in Adger’s monograph on minimalism (Adger 2003, page 75), which mentions two criteria: one based on the distribution of the whole constituent and the other taking into account which constituent determines the reference of the whole constituent. 458 Hajiˇcov´a Old Linguists Never Die can be based on the possibility that, in the endocentric constructions, the dependent can be absent (not just deleted on the surface). Thus, for example, in Yesterday, my father worked for the whole day in the garden it is possible to leave out the dependents yesterday, my, for the whole day, and in the garden without the sentence losing its grammaticality. Ho</context>
</contexts>
<marker>Adger, 2003</marker>
<rawString>Adger, David. 2003. Core Syntax. A Minimalist Approach. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin Battistella</author>
</authors>
<title>Jakobson and Chomsky on markedness.</title>
<date>1995</date>
<booktitle>Prague Linguistic Circle Papers 1. John Benjamins,</booktitle>
<pages>55--72</pages>
<editor>In E. Hajiˇcov´a, M. ˇCervenka, O. Leˇska, and P. Sgall, editors,</editor>
<location>Amsterdam,</location>
<contexts>
<context position="32708" citStr="Battistella (1995)" startWordPosition="5232" endWordPosition="5233">ge and the Periphery The third Praguian notion I would like to mention is the distinction made between the center (core) of language and the periphery. This distinction is closely connected with the notion of markedness; markedness, characterizing the intrinsic asymmetry of binary (and other) oppositions (not only in phonology, but also in morphology, in semiotics, and in many other domains), was first systematically presented by R. Jakobson. It was properly understood and used as an organizing principle of sign systems, also in connection with language universals and language acquisition. As Battistella (1995) notes, this notion belongs to those aspects of the Prague linguistic theory that in some form have been taken over by Chomsky, who applied it, albeit in a different shape, in his Principles and Parameters theory, as proposed in the early 1980s. Although the relationships between the two oppositions of marked versus unmarked phenomena and the core versus the periphery of the system of language are far from straightforward (see Sgall 2002, 2004), it can be claimed that because language is more stable in its core, regularities in language should be searched for first in this core; only then is i</context>
</contexts>
<marker>Battistella, 1995</marker>
<rawString>Battistella, Edwin. 1995. Jakobson and Chomsky on markedness. In E. Hajiˇcov´a, M. ˇCervenka, O. Leˇska, and P. Sgall, editors, Prague Linguistic Circle Papers 1. John Benjamins, Amsterdam, pages 55–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Bloomfield</author>
</authors>
<date>1933</date>
<location>Language. Holt, Rinehart and Winston, New York.</location>
<contexts>
<context position="4552" citStr="Bloomfield (1933)" startWordPosition="710" endWordPosition="711">apers were known in Prague, and his approach to syntax was applied to Czech by ˇSmilauer (1947), who combined dependency syntax with a constituent-based view of the relation between predicate and subject. Dependency-based approaches, which understand the verb as the center of the sentence structure and describe this structure on the basis of binary relations between heads and their modifiers, have been for a long time a matter of Continental syntactic theories rather than of the mainstream syntactic approaches on the other side of the Atlantic. However, the notion of head can be found also in Bloomfield (1933) when referring to the names of the main constituents of the sentence, that is, NP (noun phrase, with N as its head) and VP (verb phrase, with V as its head). In the framework of the Chomskyan approach, originally based exclusively on the concept of immediate constituents, the notion of head becomes the basic notion of X-bar theory. Originally, four categories were singled out as possible heads of their respective maximal projections, namely, N, V, Adj, and P(rep); as remarked by James McCawley (personal communication, around 1990), such a theory may be interesting unless the specification of </context>
</contexts>
<marker>Bloomfield, 1933</marker>
<rawString>Bloomfield, Leonard. 1933. Language. Holt, Rinehart and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>A realistic transformational grammar.</title>
<date>1978</date>
<booktitle>Linguistic Theory and Psychological Reality.</booktitle>
<pages>1--59</pages>
<editor>In M. Halle et al., editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="14968" citStr="Bresnan (1978)" startWordPosition="2383" endWordPosition="2384">to the development of linguistic theory in the 460 Hajiˇcov´a Old Linguists Never Die past 50 years that the deeper the analysis goes, the more the need of an introduction of the distinction between the notions of “head” and “modifier” (predicate, argument) is felt. Let us only recall here such approaches as: (i) the lexicosemantic analysis by Katz and Postal (1964), who work with the notions of head and modifier when specifying selection restrictions; (ii) the distinction between surface-oriented constituent structure and the (underlying) functional structure in lexical functional grammar by Bresnan (1978) and Kaplan and Bresnan (1982); (iii) the above-mentioned case grammar by Fillmore, motivated by the conviction that Chomskyan deep structure (with its specification of “deep” subject as a constituent of S regardless of the (different) semantic relations of the given NP to the verb) is not deep enough to capture the real underlying structure of the sentence; and (iv) the consecutive introduction of theta roles into the government and binding theory.3 On the other hand, dependency-based considerations have gradually and evasively penetrated to the ”data”-oriented statistical methods and treeban</context>
</contexts>
<marker>Bresnan, 1978</marker>
<rawString>Bresnan, Joan. 1978. A realistic transformational grammar. In M. Halle et al., editor, Linguistic Theory and Psychological Reality. MIT Press, Cambridge, MA, pages 1–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Calhoun</author>
<author>Malvina Nissim</author>
<author>Mark Steedman</author>
<author>Jason Brenier</author>
</authors>
<title>A framework for annotating information structure in discourse.</title>
<date>2005</date>
<booktitle>Pie in the Sky. Proceedings of the ACL Workshop 2005. Ann Arbor, MI,</booktitle>
<pages>45--52</pages>
<editor>In A. Meyers, editor,</editor>
<contexts>
<context position="27432" citStr="Calhoun et al. 2005" startWordPosition="4352" endWordPosition="4355">ception of combinatory categorial grammar first proposed by Steedman (1996, 2000); his introduction of floating constituents, the division line between which is given by the articulation of the sentence with regard to its information structure rather than fixed, determined once for all. Steedman, in contrast to many other researchers presently working in the domain of information structure, pays a due respect to the close relation between information structure, syntactic sentence structure, and prosody; in this respect, also the work on corpus annotation led by him is a pioneering enterprise (Calhoun et al. 2005).7 Due respect paid to the description of the information structure of the sentence is also crucial for the study of discourse structure and coherence. It might be interesting to note in this connection that the first systematic study indicating such a relation— although in terms influenced by the then prevailing psychological view of language— is Weil’s (1844) study on the order of words. The author introduces the notions of progression of “ideas,” distinguishing “progression” and marche parall`ele: In the former sequence (segment), the given sentence B is connected to the preceding sentence </context>
</contexts>
<marker>Calhoun, Nissim, Steedman, Brenier, 2005</marker>
<rawString>Calhoun, Sasha, Malvina Nissim, Mark Steedman, and Jason Brenier. 2005. A framework for annotating information structure in discourse. In A. Meyers, editor, Pie in the Sky. Proceedings of the ACL Workshop 2005. Ann Arbor, MI, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="22948" citStr="Chomsky (1965" startWordPosition="3637" endWordPosition="3638">e in communication (Sgall 1967, page 205ff.). As a matter of fact, the split of transformational grammar into the generative and the interpretative semantics wings coming out at the same time operated with arguments based on sentences that in Praguian terms differ only in their topic– focus structure (this fact, of course, not being recognized by the authors): See Lakoff’s (1969) examples: Many men read few books against Few books are read by many men, John talked about many problems to few girls versus John talked to few girls about many problems. To be fair to the other side of the dispute, Chomsky (1965, page 224) noticed the semantic difference between the sentences Everybody in the room knows at least two languages and At least two languages are known by everybody in the room and was in doubt as to whether this difference should be ascribed to the difference between active and passive; he remarks that such a distinction might be described in terms of topic (as Lakoff [1969] notes, in this consideration, the influence of Halliday [1967–1968] played its role). In his reaction to the generative semanticists’ criticism of the ”shallowness” of his deep structure, Chomsky (1968) was even more in</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, Noam. 1965. Aspects of the Theory of Syntax. MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Deep structure, surface structure and semantic interpretation.</title>
<date>1968</date>
<booktitle>Semantics: An Interdisciplinary Reader in Philosophy, Linguistics and Psychology.</booktitle>
<pages>183--216</pages>
<editor>In D. D. Steinberg and L. A. Jakobovits, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="23531" citStr="Chomsky (1968)" startWordPosition="3731" endWordPosition="3732">e of the dispute, Chomsky (1965, page 224) noticed the semantic difference between the sentences Everybody in the room knows at least two languages and At least two languages are known by everybody in the room and was in doubt as to whether this difference should be ascribed to the difference between active and passive; he remarks that such a distinction might be described in terms of topic (as Lakoff [1969] notes, in this consideration, the influence of Halliday [1967–1968] played its role). In his reaction to the generative semanticists’ criticism of the ”shallowness” of his deep structure, Chomsky (1968) was even more inclined to use notions related to what in present-day terms would be called the information structure of the sentence; he claims that in the semantic interpretation of the sentence, one should take into account the distinction between what he calls presupposition and focus, and a related notion of the range of permissible focus. There are two interesting points in his approach: First, Chomsky connects these syntactic issues with the placement of the intonation center in the spoken form of the sentence, and second, he connects the possible operational criterion for the determina</context>
</contexts>
<marker>Chomsky, 1968</marker>
<rawString>Chomsky, Noam. 1968. Deep structure, surface structure and semantic interpretation. In D. D. Steinberg and L. A. Jakobovits, editors, Semantics: An Interdisciplinary Reader in Philosophy, Linguistics and Psychology. Cambridge University Press, Cambridge, pages 183–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Toward a modern theory of case. In</title>
<date>1966</date>
<booktitle>Modern Studies in English. Prentice-Hall, Englewood Cliffs, NJ,</booktitle>
<pages>361--375</pages>
<editor>D. A. Reibel and S. A. Schane, editors,</editor>
<contexts>
<context position="11535" citStr="Fillmore (1966" startWordPosition="1837" endWordPosition="1838">condition of projectivity (in different forms that have been proved as equivalent) was defined by Marcus (1965) and used in many writings working with dependency descriptions. 459 Computational Linguistics Volume 32, Number 4 Figure 1 Examples of non-projective parts of a dependency tree. auxiliary verbs or conjunctions, and so forth, the projectivity of underlying syntactic structure is not at stake. The introduction of the notion of a head brings into the foreground the connection of grammar and lexicon; the necessity of such a relationship was already quite apparent in the earlier works of Fillmore (1966, 1968) introducing the so-called case grammar, which explicitly follows up Tesni`ere’s notion of valency. The term “case” does not directly refer to case as a morphological category but to the meaning (function) of a (morphological) case: for example, Addressee is a prototypical meaning (function) of Dative, Agentive is a prototypical meaning of Nominative, and so on. The concept of valency is crucial in that it reflects the fundamental aspect of the presence of grammatical information in the lexicon: The valency frame is a part of the lexical entry, in which the obligatory and optional synta</context>
</contexts>
<marker>Fillmore, 1966</marker>
<rawString>Fillmore, Charles J. 1966. Toward a modern theory of case. In D. A. Reibel and S. A. Schane, editors, Modern Studies in English. Prentice-Hall, Englewood Cliffs, NJ, pages 361–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>The case for case.</title>
<date>1968</date>
<booktitle>Universals in Linguistic Theory.</booktitle>
<pages>1--90</pages>
<editor>In E. Bach and R. Harms, editors,</editor>
<location>Holt, Rinehart, and Winston, New York,</location>
<marker>Fillmore, 1968</marker>
<rawString>Fillmore, Charles J. 1968. The case for case. In E. Bach and R. Harms, editors, Universals in Linguistic Theory. Holt, Rinehart, and Winston, New York, pages 1–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Unbounded dependencies and coordinate structure. Linguistic Inquiry,</title>
<date>1981</date>
<pages>12--155</pages>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, Gerald. 1981. Unbounded dependencies and coordinate structure. Linguistic Inquiry, 12:155–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
</authors>
<title>The Representation and Use of Focus in Dialogue Understanding.</title>
<date>1977</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="29329" citStr="Grosz (1977)" startWordPosition="4673" endWordPosition="4674">ural language processing. Let me illustrate by a personal recollection that I am not beating a straw man. Some time ago (if I am not mistaken, it was in 1989) I was invited for an IBM-organized MT conference in Garmisch-Partenkirchen to deliver a talk on the Praguian approach to MT. Naturally enough, I devoted most of my time to illustrate examples of translations from and to several European languages that topic–focus articulation as an important aspect the translation (be it human or automatic) has to take into account. The group of 6 From a different perspective, the term focus was used by Grosz (1977). The author adopted a psychological point of view of focus of attention and considered the sentence focus to be that item that is in such a focus, that is, in our terms the topic of the sentence (what the sentence is about). Grosz’ approach has found its continuation in the centering theory mentioned below. 7 Let us note in this connection that the difficulties of a syntactic description based on phrase structure for an adequate capturing of the topic–focus articulation were pointed out already in Sgall, Hajiˇcov´a, and Beneˇsov´a (1973, page 163ff.) and in Hajiˇcov´a and Sgall (1975) and ill</context>
</contexts>
<marker>Grosz, 1977</marker>
<rawString>Grosz, Barbara J. 1977. The Representation and Use of Focus in Dialogue Understanding. Ph.D. thesis, University of California, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Grosz, Barbara J., Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey S Gruber</author>
</authors>
<title>Functions of the lexicon in formal descriptive grammar.</title>
<date>1967</date>
<tech>Technical Report (TM)-3770/00,</tech>
<institution>Systems Development Corporation, Santa Monica.</institution>
<marker>Gruber, 1967</marker>
<rawString>Gruber, Jeffrey S. 1967. Functions of the lexicon in formal descriptive grammar. Technical Report (TM)-3770/00, Systems Development Corporation, Santa Monica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Building a syntactically annotated corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>Issues of Valency and Meaning. Studies in Honour of Jarmila Panevov´a. Karolinum,</booktitle>
<pages>106--132</pages>
<editor>In E. Hajiˇcov´a, editor,</editor>
<location>Prague,</location>
<marker>Hajiˇc, 1998</marker>
<rawString>Hajiˇc, Jan. 1998. Building a syntactically annotated corpus: The Prague Dependency Treebank. In E. Hajiˇcov´a, editor, Issues of Valency and Meaning. Studies in Honour of Jarmila Panevov´a. Karolinum, Prague, pages 106–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˇcov´a</author>
<author>Petr Pajas</author>
<author>Kateˇrina Vesel´a</author>
</authors>
<title>Corpus annotation on the tectogrammatical layer: Summarizing of the first stages of evaluation.</title>
<date>2002</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>77--5</pages>
<marker>Hajiˇcov´a, Pajas, Vesel´a, 2002</marker>
<rawString>Hajiˇcov´a, Eva, Petr Pajas, and Kateˇrina Vesel´a. 2002. Corpus annotation on the tectogrammatical layer: Summarizing of the first stages of evaluation. The Prague Bulletin of Mathematical Linguistics, 77:5–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
</authors>
<title>Topic and focus in transformational grammar.</title>
<date>1975</date>
<booktitle>Papers in Linguistics,</booktitle>
<pages>8--1</pages>
<marker>Hajiˇcov´a, Sgall, 1975</marker>
<rawString>Hajiˇcov´a, Eva and Petr Sgall. 1975. Topic and focus in transformational grammar. Papers in Linguistics, 8(1–2):13–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
</authors>
<title>Dependency syntax in functional generative description.</title>
<date>2003</date>
<booktitle>In Vilmos Agel</booktitle>
<pages>570--592</pages>
<editor>et al., editors,</editor>
<location>Berlin,</location>
<marker>Hajiˇcov´a, Sgall, 2003</marker>
<rawString>Hajiˇcov´a, Eva and Petr Sgall. 2003. Dependency syntax in functional generative description. In Vilmos Agel et al., editors, Dependenz und Valenz, Vol. 1. Walter de Gruyter, Berlin, pages 570–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
</authors>
<title>Corpus annotation as a test of a linguistic theory.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC 2006,</booktitle>
<location>Genoa.</location>
<marker>Hajiˇcov´a, Sgall, 2006</marker>
<rawString>Hajiˇcov´a, Eva and Petr Sgall. 2006. Corpus annotation as a test of a linguistic theory. In Proceedings of LREC 2006, Genoa.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Forthcoming</author>
</authors>
<title>The fundamental significance of information structure.</title>
<booktitle>Future Prospects of Pragmatics.</booktitle>
<editor>In C. Caffi and H. Haberland et al., editors,</editor>
<marker>Forthcoming, </marker>
<rawString>Hajiˇcov´a, Eva and Petr Sgall. Forthcoming. The fundamental significance of information structure. In C. Caffi and H. Haberland et al., editors, Future Prospects of Pragmatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
</authors>
<title>Notes on transitivity and theme in English.</title>
<date>1967</date>
<journal>Journal of Linguistics,</journal>
<volume>3</volume>
<pages>4--179</pages>
<marker>Halliday, 1967</marker>
<rawString>Halliday, Michael A. K. 1967–1968. Notes on transitivity and theme in English. Journal of Linguistics, 3:37–81, 199–244; 4:179–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
</authors>
<title>A Course in Spoken English: Intonation.</title>
<date>1970</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<marker>Halliday, 1970</marker>
<rawString>Halliday, Michael A. K. 1970. A Course in Spoken English: Intonation. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Hays</author>
</authors>
<title>Grouping and dependency theories.</title>
<date>1960</date>
<booktitle>In Proceedings of the National Symposium on Machine Translation,</booktitle>
<pages>258--266</pages>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="10872" citStr="Hays (1960" startWordPosition="1734" endWordPosition="1735">003]). This is, of course, a rather strong hypothesis that has to be verified and made more precise on the basis of systematic empirical research. It should be mentioned in this connection that it is in line with the Praguian approach that function words are distinguished from autosemantic words and that only the latter constitute nodes of their own in the underlying trees; from this it follows that in the numerous cases in which the “non-projectivity” of surface word order concerns 2 Projectivity as a property of word order important for a formal description of language was already stated by Hays (1960, 1964) and Lecerf (1960) in formal grammar; the condition of projectivity (in different forms that have been proved as equivalent) was defined by Marcus (1965) and used in many writings working with dependency descriptions. 459 Computational Linguistics Volume 32, Number 4 Figure 1 Examples of non-projective parts of a dependency tree. auxiliary verbs or conjunctions, and so forth, the projectivity of underlying syntactic structure is not at stake. The introduction of the notion of a head brings into the foreground the connection of grammar and lexicon; the necessity of such a relationship wa</context>
</contexts>
<marker>Hays, 1960</marker>
<rawString>Hays, David G. 1960. Grouping and dependency theories. In Proceedings of the National Symposium on Machine Translation, pages 258–266, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Hays</author>
</authors>
<title>Dependency theory: A formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--511</pages>
<marker>Hays, 1964</marker>
<rawString>Hays, David G. 1964. Dependency theory: A formalism and some observations. Language, 40:511–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>Word Grammar.</title>
<date>1984</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<marker>Hudson, 1984</marker>
<rawString>Hudson, Richard. 1984. Word Grammar. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Jakobson</author>
</authors>
<title>Remarques sur l’´evolution phonologique du russe compar´ee a´ celle des autres langues slaves. Travaux du Cercle Linguistique de Prague,</title>
<date>1929</date>
<volume>2</volume>
<contexts>
<context position="3214" citStr="Jakobson (1929)" startWordPosition="502" endWordPosition="503">ons rather than fixed * Logo on the Indiana University Linguistic Club tee-shirt, 1984. ** Matematicko-fyzik´alnifakulta, Univerzita Karlova, Malostransk´e n´amesti25, CZ-11800 Praha, Czech Republic. This paper is the text of the talk given on receipt of the ACL’s Lifetime Achievement Award in 2006. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 entities play a role. As mentioned above, this attitude was most apparently reflected in the study of phonology as a system displaying distinctive features and employing the notion of binary oppositions. Jakobson (1929) presented the phonological repertory (both in synchrony and in diachrony) as a system of oppositions (mainly binary, privative), based on acoustic distinctive features and understood as the clue to the sound and meaning relationship. Along with phonemes and morphemes, the sentence was also recognized as one of the fundamental fields of systematic oppositions, that is, as an ingredient of la langue. Mathesius (1928, 1936) formulated a concept of functional syntax; a structural view of syntax, based on the dependency relation, was elaborated by Tesni`ere (1934), a French member of the Circle, w</context>
</contexts>
<marker>Jakobson, 1929</marker>
<rawString>Jakobson, Roman. 1929. Remarques sur l’´evolution phonologique du russe compar´ee a´ celle des autres langues slaves. Travaux du Cercle Linguistique de Prague, volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>Tree-adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions? In</title>
<date>1985</date>
<booktitle>Natural Language Processing.</booktitle>
<pages>206--250</pages>
<editor>D. Dowty, editor,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="16926" citStr="Joshi 1985" startWordPosition="2678" endWordPosition="2679"> (avoiding ”extra” nodes for phrases such as NPs, VPs,..., etc.) from the surface to the depth. Dependency analysis offers such a way.5 3 We have restricted our attention here only to systems staying in principle within the development of the Chomskyan paradigm or originating as a reaction to it. However, when discussing the relation between or combination of constituent-based and dependency-based grammars, special attention should be paid to the lexicalized tree-adjoining grammars (LTAG) continuing the original conception of tree-adjoining grammar (TAG) as proposed by A. K. Joshi (see, e.g., Joshi 1985), which has served as a basis for many studies on formal grammar as well as from the NLP domain. The similarity between LTAG and a dependency-based description in relation to the model using the so-called supertags (which encode syntactic information in terms of dependency) is analyzed by Joshi and Srinivas (1994). 4 It should be recalled in this connection that within machine translation dependency-based systems (sometimes in combination with phrase structure) were already at play in the early times of MT; see, for example, the works of B. Vauquois, one of the founders of computational lingui</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Joshi, Aravind. 1985. Tree-adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions? In D. Dowty, editor, Natural Language Processing. Cambridge University Press, Cambridge, pages 206–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>Bangalore Srinivas</author>
</authors>
<title>Disambiguation of super parts of speech (or supertags): Almost parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING</booktitle>
<pages>154--160</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="17241" citStr="Joshi and Srinivas (1994)" startWordPosition="2727" endWordPosition="2730">ever, when discussing the relation between or combination of constituent-based and dependency-based grammars, special attention should be paid to the lexicalized tree-adjoining grammars (LTAG) continuing the original conception of tree-adjoining grammar (TAG) as proposed by A. K. Joshi (see, e.g., Joshi 1985), which has served as a basis for many studies on formal grammar as well as from the NLP domain. The similarity between LTAG and a dependency-based description in relation to the model using the so-called supertags (which encode syntactic information in terms of dependency) is analyzed by Joshi and Srinivas (1994). 4 It should be recalled in this connection that within machine translation dependency-based systems (sometimes in combination with phrase structure) were already at play in the early times of MT; see, for example, the works of B. Vauquois, one of the founders of computational linguistics (Vauquois 1975; Vauquois and Chappuy 1985) and the systems developed in Japan under the influence and guidance (direct or indirect) of M. Nagao (see the survey in Nagao [1989]). 5 In a similar vein, Steedman (2005) argues that the use of statistical language models is the only way to create a computer progra</context>
</contexts>
<marker>Joshi, Srinivas, 1994</marker>
<rawString>Joshi, Aravind and Bangalore Srinivas. 1994. Disambiguation of super parts of speech (or supertags): Almost parsing. In Proceedings of the 15th International Conference on Computational Linguistics (COLING 1994), pages 154–160, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations.</booktitle>
<pages>173--281</pages>
<editor>In Joan Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="14998" citStr="Kaplan and Bresnan (1982)" startWordPosition="2386" endWordPosition="2389">of linguistic theory in the 460 Hajiˇcov´a Old Linguists Never Die past 50 years that the deeper the analysis goes, the more the need of an introduction of the distinction between the notions of “head” and “modifier” (predicate, argument) is felt. Let us only recall here such approaches as: (i) the lexicosemantic analysis by Katz and Postal (1964), who work with the notions of head and modifier when specifying selection restrictions; (ii) the distinction between surface-oriented constituent structure and the (underlying) functional structure in lexical functional grammar by Bresnan (1978) and Kaplan and Bresnan (1982); (iii) the above-mentioned case grammar by Fillmore, motivated by the conviction that Chomskyan deep structure (with its specification of “deep” subject as a constituent of S regardless of the (different) semantic relations of the given NP to the verb) is not deep enough to capture the real underlying structure of the sentence; and (iv) the consecutive introduction of theta roles into the government and binding theory.3 On the other hand, dependency-based considerations have gradually and evasively penetrated to the ”data”-oriented statistical methods and treebank annotations. As the freshest</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Kaplan, Ronald and Joan Bresnan. 1982. Lexical-Functional Grammar: A formal system for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations. MIT Press, Cambridge, MA, pages 173–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
<author>Paul M Postal</author>
</authors>
<title>An Integrated Theory of Linguistic Descriptions.</title>
<date>1964</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="14722" citStr="Katz and Postal (1964)" startWordPosition="2349" endWordPosition="2352">ion and computational lexicology (cf., e.g., Fillmore et al. 2003) but also the work on underlying sentence structure in general. Two “historically” motivated and seemingly contradictory observations are in place here: It can be documented by references to the development of linguistic theory in the 460 Hajiˇcov´a Old Linguists Never Die past 50 years that the deeper the analysis goes, the more the need of an introduction of the distinction between the notions of “head” and “modifier” (predicate, argument) is felt. Let us only recall here such approaches as: (i) the lexicosemantic analysis by Katz and Postal (1964), who work with the notions of head and modifier when specifying selection restrictions; (ii) the distinction between surface-oriented constituent structure and the (underlying) functional structure in lexical functional grammar by Bresnan (1978) and Kaplan and Bresnan (1982); (iii) the above-mentioned case grammar by Fillmore, motivated by the conviction that Chomskyan deep structure (with its specification of “deep” subject as a constituent of S regardless of the (different) semantic relations of the given NP to the verb) is not deep enough to capture the real underlying structure of the sen</context>
</contexts>
<marker>Katz, Postal, 1964</marker>
<rawString>Katz, Jerrold J. and Paul M. Postal. 1964. An Integrated Theory of Linguistic Descriptions. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>A life of language.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Kay, 2005</marker>
<rawString>Kay, Martin. 2005. A life of language. Computational Linguistics, 31(4):425–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<title>On generative semantics. In</title>
<date>1969</date>
<booktitle>Semantics: An Interdisciplinary Reader in Philosophy, Linguistics and Pyschology.</booktitle>
<pages>232--296</pages>
<editor>D. D. Steinberg and L. A. Jakobovits, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<marker>Lakoff, 1969</marker>
<rawString>Lakoff, George. 1969. On generative semantics. In D. D. Steinberg and L. A. Jakobovits, editors, Semantics: An Interdisciplinary Reader in Philosophy, Linguistics and Pyschology. Cambridge University Press, Cambridge, pages 232–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lecerf</author>
</authors>
<title>Programme des conflits, mod`ele des conflits.</title>
<date>1960</date>
<journal>Traduction Automatique,</journal>
<volume>1</volume>
<issue>4</issue>
<pages>1--5</pages>
<contexts>
<context position="10897" citStr="Lecerf (1960)" startWordPosition="1738" endWordPosition="1739">rse, a rather strong hypothesis that has to be verified and made more precise on the basis of systematic empirical research. It should be mentioned in this connection that it is in line with the Praguian approach that function words are distinguished from autosemantic words and that only the latter constitute nodes of their own in the underlying trees; from this it follows that in the numerous cases in which the “non-projectivity” of surface word order concerns 2 Projectivity as a property of word order important for a formal description of language was already stated by Hays (1960, 1964) and Lecerf (1960) in formal grammar; the condition of projectivity (in different forms that have been proved as equivalent) was defined by Marcus (1965) and used in many writings working with dependency descriptions. 459 Computational Linguistics Volume 32, Number 4 Figure 1 Examples of non-projective parts of a dependency tree. auxiliary verbs or conjunctions, and so forth, the projectivity of underlying syntactic structure is not at stake. The introduction of the notion of a head brings into the foreground the connection of grammar and lexicon; the necessity of such a relationship was already quite apparent </context>
</contexts>
<marker>Lecerf, 1960</marker>
<rawString>Lecerf, Yves. 1960. Programme des conflits, mod`ele des conflits. Traduction Automatique, 1(4):11–18; 1(5):17–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Marcus</author>
</authors>
<title>Sur la notion de projectivit´e.</title>
<date>1965</date>
<booktitle>Zeitschrift f¨ur mathematische Logik und Grundlagen der Mathematik,</booktitle>
<pages>11--181</pages>
<contexts>
<context position="11032" citStr="Marcus (1965)" startWordPosition="1759" endWordPosition="1760">be mentioned in this connection that it is in line with the Praguian approach that function words are distinguished from autosemantic words and that only the latter constitute nodes of their own in the underlying trees; from this it follows that in the numerous cases in which the “non-projectivity” of surface word order concerns 2 Projectivity as a property of word order important for a formal description of language was already stated by Hays (1960, 1964) and Lecerf (1960) in formal grammar; the condition of projectivity (in different forms that have been proved as equivalent) was defined by Marcus (1965) and used in many writings working with dependency descriptions. 459 Computational Linguistics Volume 32, Number 4 Figure 1 Examples of non-projective parts of a dependency tree. auxiliary verbs or conjunctions, and so forth, the projectivity of underlying syntactic structure is not at stake. The introduction of the notion of a head brings into the foreground the connection of grammar and lexicon; the necessity of such a relationship was already quite apparent in the earlier works of Fillmore (1966, 1968) introducing the so-called case grammar, which explicitly follows up Tesni`ere’s notion of</context>
</contexts>
<marker>Marcus, 1965</marker>
<rawString>Marcus, Solomon. 1965. Sur la notion de projectivit´e. Zeitschrift f¨ur mathematische Logik und Grundlagen der Mathematik, 11:181–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Marty</author>
</authors>
<date>1908</date>
<booktitle>Untersuchungen zur Grundlegung der allgemeinen Grammatik und Sprachphilosophie 1. Halle/S.</booktitle>
<contexts>
<context position="19024" citStr="Marty (1908)" startWordPosition="3014" endWordPosition="3015">h as In this garden, she was reading a book on the history of Spain yesterday as having the complex verb form was reading as its head, with she, (a) book, and garden as its dependents, or to see the basic characteristics of its structure in distinguishing whether in this garden or yesterday is more immediately connected with its verb? (ii) Do we have clearer criteria for answering the former or the latter of these two questions? 3. Prague School Functionalism The other attribute of Prague structuralism is functional. Mathesius (1928, 1936), inspired especially by the philosophy of language of Marty (1908), presented his theory of functional grammar, based on the concept of function as related to universal intentional acts and treated as a dichotomy of functional onomatology and functional syntax. Mathesius combined this universal dichotomy with the language-specific opposition of function and form. As Sgall (1987) pointed out, the core of the system of language was conceived of as consisting of levels, the units of which have their functions in that they represent or express units of the adjacent higher levels, up to the non-linguistic layer of cognitive content. The units of the system were u</context>
</contexts>
<marker>Marty, 1908</marker>
<rawString>Marty, Anton. 1908. Untersuchungen zur Grundlegung der allgemeinen Grammatik und Sprachphilosophie 1. Halle/S.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vil´em Mathesius</author>
</authors>
<title>On linguistic characterology. Actes,</title>
<date>1928</date>
<pages>56--63</pages>
<contexts>
<context position="3632" citStr="Mathesius (1928" startWordPosition="566" endWordPosition="567">As mentioned above, this attitude was most apparently reflected in the study of phonology as a system displaying distinctive features and employing the notion of binary oppositions. Jakobson (1929) presented the phonological repertory (both in synchrony and in diachrony) as a system of oppositions (mainly binary, privative), based on acoustic distinctive features and understood as the clue to the sound and meaning relationship. Along with phonemes and morphemes, the sentence was also recognized as one of the fundamental fields of systematic oppositions, that is, as an ingredient of la langue. Mathesius (1928, 1936) formulated a concept of functional syntax; a structural view of syntax, based on the dependency relation, was elaborated by Tesni`ere (1934), a French member of the Circle, who was a professor of the Ljubljana University; his monograph was published only posthumously (Tesni`ere 1959), but his papers were known in Prague, and his approach to syntax was applied to Czech by ˇSmilauer (1947), who combined dependency syntax with a constituent-based view of the relation between predicate and subject. Dependency-based approaches, which understand the verb as the center of the sentence structu</context>
<context position="18950" citStr="Mathesius (1928" startWordPosition="3002" endWordPosition="3003">en the following issues: (i) Is it more appropriate to analyze a sentence such as In this garden, she was reading a book on the history of Spain yesterday as having the complex verb form was reading as its head, with she, (a) book, and garden as its dependents, or to see the basic characteristics of its structure in distinguishing whether in this garden or yesterday is more immediately connected with its verb? (ii) Do we have clearer criteria for answering the former or the latter of these two questions? 3. Prague School Functionalism The other attribute of Prague structuralism is functional. Mathesius (1928, 1936), inspired especially by the philosophy of language of Marty (1908), presented his theory of functional grammar, based on the concept of function as related to universal intentional acts and treated as a dichotomy of functional onomatology and functional syntax. Mathesius combined this universal dichotomy with the language-specific opposition of function and form. As Sgall (1987) pointed out, the core of the system of language was conceived of as consisting of levels, the units of which have their functions in that they represent or express units of the adjacent higher levels, up to the</context>
</contexts>
<marker>Mathesius, 1928</marker>
<rawString>Mathesius, Vil´em. 1928. On linguistic characterology. Actes, pages 56–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vil´em Mathesius</author>
</authors>
<title>Zur satzperspektive im modernen Englisch.</title>
<date>1929</date>
<booktitle>Archiv f¨ur das Studium der neueren Sprachen und Literaturen,</booktitle>
<pages>155--202</pages>
<contexts>
<context position="20677" citStr="Mathesius (1929" startWordPosition="3271" endWordPosition="3272">modern terminology) are understood as expressing morphemes, and sequences of morphemes as expressing sentence structure. Another important aspect of the functional approach is to view language as a functioning system, adapted to its communicative role, diversified in more or less different social and local varieties, and to describe the sentence structure as adapted to its functioning in discourse. This leads me to pay attention to the information structure (in our terms, topic– focus articulation) of the sentence. Let me first look again at the history of the issue. It was the Prague scholar Mathesius (1929, 1938) who introduced the study of information structure into structural linguistics, preferring the terms Thema and Rhema (used before in German linguistics by H. Ammann) to the older psychologisches Subjekt and Pr¨adikat (used by G. von der Gabelentz, H. Paul, and others), and understanding the former (the topic) as one of the functions of the subject in English. He distinguished topic proper, comment (focus) proper, and the accompanying elements of either of these two parts. Later on, one of Mathesius’ followers, Jan Firbas, extended the hierarchical understanding of the information struct</context>
</contexts>
<marker>Mathesius, 1929</marker>
<rawString>Mathesius, Vil´em. 1929. Zur satzperspektive im modernen Englisch. Archiv f¨ur das Studium der neueren Sprachen und Literaturen, 155:202–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vil´em Mathesius</author>
</authors>
<title>On some problems of the systematic analysis of grammar.</title>
<date>1936</date>
<booktitle>Travaux du Cercle Linguistique de Prague,</booktitle>
<volume>6</volume>
<pages>95--107</pages>
<marker>Mathesius, 1936</marker>
<rawString>Mathesius, Vil´em. 1936. On some problems of the systematic analysis of grammar. Travaux du Cercle Linguistique de Prague, volume 6, pages 95–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
</authors>
<title>Machine Translation: How Far Can It Go?</title>
<date>1989</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<marker>Nagao, 1989</marker>
<rawString>Nagao, Makoto. 1989. Machine Translation: How Far Can It Go? Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago and London.</location>
<contexts>
<context position="5703" citStr="Pollard and Sag (1994)" startWordPosition="888" endWordPosition="891">nd 1990), such a theory may be interesting unless the specification of the set of basic categories grows beyond some reasonable limit. McCawley’s critical remark reflected the gradual development of X-bar theory, which allowed practically any constituent (or, more generally speaking, any arbitrary symbol for a grammatical value) to act as the head, dependent on the needs of the analysis of this or that construction.1 The very name head-driven phrase structure grammar, an influential theory combining an immediate constituent approach with elements of a dependency-based approach, as proposed by Pollard and Sag (1994), explicitly points out that the theory takes account of the main element within a constituent. Although their approach is constituent based (working with a lexically based X-bar syntactic theory; [Pollard and Sag 1994, page 362]), the authors are aware that the notion of constituent structure is widespread but that it is not based on sufficiently convincing direct evidence. The authors refer to Hudson’s (1984) approach and claim that it belongs to exceptions that do not overestimate the constituent structuring of sentence elements. It is sometimes doubted if the direction of the dependency re</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>Case, category and configuration.</title>
<date>1969</date>
<journal>Journal of Linguistics,</journal>
<pages>6--57</pages>
<contexts>
<context position="13184" citStr="Robinson (1969" startWordPosition="2102" endWordPosition="2103">r, he did not primarily consider which formal description his approach would fit into. However, he presents an example of how his approach can be formulated in terms of a phrase structure model: The sentence S can be decomposed into two parts, Modality and Proposition; the Proposition in turn can be articulated into the verb and a set of noun phrases, which are characterized by one of the case markers, that is, K1NP, K2NP,..., KnNP. Each of these noun phrases can then be decomposed into the noun phrase proper and the given case marker ki (Agentive, Addressee, Objective, etc.). The analysis of Robinson (1969, 1970) devoted to the relation between Fillmore’s approach and that of transformational grammar (of that time) throws an interesting light on the possiblity of a smooth transition from a phrase-based approach to a dependency-based one, which is more transparent and economical. In Fillmore’s proposal, the case relations, that is, the relations of the noun phrase to the verb, are actually captured twice, once as the marker ki and once as the characteristics of the given phrase (KiNP). It is then possible to work with a pure dependency tree structure, where the root of the tree is the verb, and </context>
</contexts>
<marker>Robinson, 1969</marker>
<rawString>Robinson, Jane J. 1969. Case, category and configuration. Journal of Linguistics, 6:57–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>Dependency structures and transformational rules.</title>
<date>1970</date>
<journal>Language,</journal>
<pages>46--259</pages>
<marker>Robinson, 1970</marker>
<rawString>Robinson, Jane J. 1970. Dependency structures and transformational rules. Language, 46:259–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>Functional sentence perspective in a generative description.</title>
<date>1967</date>
<booktitle>Prague Studies in Mathematical Linguistics,</booktitle>
<pages>2--203</pages>
<contexts>
<context position="22365" citStr="Sgall 1967" startWordPosition="3538" endWordPosition="3539">Hajiˇcov´a Old Linguists Never Die of different sentence articulations into topic and focus were mostly taken into account, and its relevance for and effects on the coherence of discourse. A new impetus into the study of information structure was given by Petr Sgall, who was the first to come up with examples testifying to the semantic effects of this issue and claiming that two utterance tokens differing in their topic–focus articulation are tokens of two different sentences, that is, that topic and focus belong to the language system rather than only to the use of language in communication (Sgall 1967, page 205ff.). As a matter of fact, the split of transformational grammar into the generative and the interpretative semantics wings coming out at the same time operated with arguments based on sentences that in Praguian terms differ only in their topic– focus structure (this fact, of course, not being recognized by the authors): See Lakoff’s (1969) examples: Many men read few books against Few books are read by many men, John talked about many problems to few girls versus John talked to few girls about many problems. To be fair to the other side of the dispute, Chomsky (1965, page 224) notic</context>
</contexts>
<marker>Sgall, 1967</marker>
<rawString>Sgall, Petr. 1967. Functional sentence perspective in a generative description. Prague Studies in Mathematical Linguistics, 2:203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>Prague functionalism and topic vs. focus.</title>
<date>1987</date>
<booktitle>In Ren´e Dirven and Vil´em Fried, editors, Functionalism in Linguistics. John Benjamins Publishing Company, Amsterdam/Philadelphia,</booktitle>
<pages>169--189</pages>
<contexts>
<context position="19339" citStr="Sgall (1987)" startWordPosition="3059" endWordPosition="3060">nected with its verb? (ii) Do we have clearer criteria for answering the former or the latter of these two questions? 3. Prague School Functionalism The other attribute of Prague structuralism is functional. Mathesius (1928, 1936), inspired especially by the philosophy of language of Marty (1908), presented his theory of functional grammar, based on the concept of function as related to universal intentional acts and treated as a dichotomy of functional onomatology and functional syntax. Mathesius combined this universal dichotomy with the language-specific opposition of function and form. As Sgall (1987) pointed out, the core of the system of language was conceived of as consisting of levels, the units of which have their functions in that they represent or express units of the adjacent higher levels, up to the non-linguistic layer of cognitive content. The units of the system were understood as constituting hierarchies in which some of them function as certain parts of the others. Thus, for example, phonemes were defined and delimited one against the other on a functional basis (two different phonemes can distinguish two morphemes), and the established repertory of distinctive features gave </context>
</contexts>
<marker>Sgall, 1987</marker>
<rawString>Sgall, Petr. 1987. Prague functionalism and topic vs. focus. In Ren´e Dirven and Vil´em Fried, editors, Functionalism in Linguistics. John Benjamins Publishing Company, Amsterdam/Philadelphia, pages 169–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>On the usefulness of movement rules.</title>
<date>1997</date>
<booktitle>Actes du 16e Congr`es International des Linguistes</booktitle>
<editor>In B. Caron, editor,</editor>
<publisher>Elsevier Science,</publisher>
<location>Paris</location>
<marker>Sgall, 1997</marker>
<rawString>Sgall, Petr. 1997. On the usefulness of movement rules. In B. Caron, editor, Actes du 16e Congr`es International des Linguistes (Paris 20-25 juillet 1997). Elsevier Science, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>Freedom of language: Its nature, its sources and its consequences. Prague Linguistic Circle Papers,</title>
<date>2002</date>
<pages>4--309</pages>
<contexts>
<context position="33149" citStr="Sgall 2002" startWordPosition="5305" endWordPosition="5306"> was properly understood and used as an organizing principle of sign systems, also in connection with language universals and language acquisition. As Battistella (1995) notes, this notion belongs to those aspects of the Prague linguistic theory that in some form have been taken over by Chomsky, who applied it, albeit in a different shape, in his Principles and Parameters theory, as proposed in the early 1980s. Although the relationships between the two oppositions of marked versus unmarked phenomena and the core versus the periphery of the system of language are far from straightforward (see Sgall 2002, 2004), it can be claimed that because language is more stable in its core, regularities in language should be searched for first in this core; only then is it possible to penetrate into the subtleties and irregularities of the periphery. The relatively simple pattern of the core of language (in Sgall’s view, not far from the transparent pattern of propositional calculus) makes it possible for children to learn the regularities of their mother tongue on the basis of shared human mental capacities, instantiated also by systems such as those of elementary arithmetic or Aristotelian logic. The f</context>
</contexts>
<marker>Sgall, 2002</marker>
<rawString>Sgall, Petr. 2002. Freedom of language: Its nature, its sources and its consequences. Prague Linguistic Circle Papers, 4:309–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>Types of languages and the simple pattern of the core of language. In</title>
<date>2004</date>
<booktitle>Linguistics Today—Facing a Greater Challenge (Plenary lectures from CIL 17). John Benjamins, Amsterdam/Philadelphia,</booktitle>
<pages>243--265</pages>
<editor>P. Sterkenburg, editor,</editor>
<marker>Sgall, 2004</marker>
<rawString>Sgall, Petr. 2004. Types of languages and the simple pattern of the core of language. In P. Sterkenburg, editor, Linguistics Today—Facing a Greater Challenge (Plenary lectures from CIL 17). John Benjamins, Amsterdam/Philadelphia, pages 243–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Eva Beneˇsov´a</author>
</authors>
<title>Topic, Focus and Generative Semantics.</title>
<date>1973</date>
<location>Scriptor, Kronberg/Taunus.</location>
<marker>Sgall, Hajiˇcov´a, Beneˇsov´a, 1973</marker>
<rawString>Sgall, Petr, Eva Hajiˇcov´a, and Eva Beneˇsov´a. 1973. Topic, Focus and Generative Semantics. Scriptor, Kronberg/Taunus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<location>Reidel, Dordrecht; Academia, Prague. ˇ</location>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Sgall, Petr, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. Reidel, Dordrecht; Academia, Prague. ˇ</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladim´ır Smilauer</author>
</authors>
<title>Novoˇcesk´a skladba [The syntax of Modern Czech].</title>
<date>1947</date>
<location>Mikuta, Prague.</location>
<contexts>
<context position="4030" citStr="Smilauer (1947)" startWordPosition="629" endWordPosition="630">e sound and meaning relationship. Along with phonemes and morphemes, the sentence was also recognized as one of the fundamental fields of systematic oppositions, that is, as an ingredient of la langue. Mathesius (1928, 1936) formulated a concept of functional syntax; a structural view of syntax, based on the dependency relation, was elaborated by Tesni`ere (1934), a French member of the Circle, who was a professor of the Ljubljana University; his monograph was published only posthumously (Tesni`ere 1959), but his papers were known in Prague, and his approach to syntax was applied to Czech by ˇSmilauer (1947), who combined dependency syntax with a constituent-based view of the relation between predicate and subject. Dependency-based approaches, which understand the verb as the center of the sentence structure and describe this structure on the basis of binary relations between heads and their modifiers, have been for a long time a matter of Continental syntactic theories rather than of the mainstream syntactic approaches on the other side of the Atlantic. However, the notion of head can be found also in Bloomfield (1933) when referring to the names of the main constituents of the sentence, that is</context>
</contexts>
<marker>Smilauer, 1947</marker>
<rawString>Smilauer, Vladim´ır. 1947. Novoˇcesk´a skladba [The syntax of Modern Czech]. Mikuta, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Surface Structure and Interpetation.</title>
<date>1996</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="26886" citStr="Steedman (1996" startWordPosition="4271" endWordPosition="4272">to prosodic focus (pointing out that the difference in truth conditions between such sentences as Mary only introduced BILL to Sue and Mary only introduced Bill to SUE is only in the location of focus, denoted here by capitals), Rooth’s work was an impetus for an increasing interest in the related issues, first in the domain of formal semantics (here the influential role of Barbara H. Partee should be emphasized), but soon literally everywhere. Let us mention in this context that semantic considerations apparently stood behind the conception of combinatory categorial grammar first proposed by Steedman (1996, 2000); his introduction of floating constituents, the division line between which is given by the articulation of the sentence with regard to its information structure rather than fixed, determined once for all. Steedman, in contrast to many other researchers presently working in the domain of information structure, pays a due respect to the close relation between information structure, syntactic sentence structure, and prosody; in this respect, also the work on corpus annotation led by him is a pioneering enterprise (Calhoun et al. 2005).7 Due respect paid to the description of the informat</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Steedman, Mark. 1996. Surface Structure and Interpetation. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Information structure and the syntax–phonology interface. Linguistic Inquiry,</title>
<date>2000</date>
<pages>31--649</pages>
<marker>Steedman, 2000</marker>
<rawString>Steedman, Mark. 2000. Information structure and the syntax–phonology interface. Linguistic Inquiry, 31:649–689.</rawString>
</citation>
<citation valid="false">
<institution>Hajiˇcov´a Old Linguists Never Die</institution>
<marker></marker>
<rawString>Hajiˇcov´a Old Linguists Never Die</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Grammar acquisition by child and machine.</title>
<date>2005</date>
<booktitle>Invited Talk at the 17th European Summer School of Language, Logic and Information,</booktitle>
<location>Edinburgh.</location>
<contexts>
<context position="17746" citStr="Steedman (2005)" startWordPosition="2810" endWordPosition="2811">supertags (which encode syntactic information in terms of dependency) is analyzed by Joshi and Srinivas (1994). 4 It should be recalled in this connection that within machine translation dependency-based systems (sometimes in combination with phrase structure) were already at play in the early times of MT; see, for example, the works of B. Vauquois, one of the founders of computational linguistics (Vauquois 1975; Vauquois and Chappuy 1985) and the systems developed in Japan under the influence and guidance (direct or indirect) of M. Nagao (see the survey in Nagao [1989]). 5 In a similar vein, Steedman (2005) argues that the use of statistical language models is the only way to create a computer program that automatically analyzes sentences on the basis of broadly conceived grammars (with due regard to ambiguities) such as dependency-based grammars or grammars specifying heads (governors); according to Steedman, these grammars work well because they reflect a mixture of semantic information and information based on knowledge of the world. 461 Computational Linguistics Volume 32, Number 4 Among urgent questions to be asked with regard to the approaches to sentence structure, there are then the foll</context>
</contexts>
<marker>Steedman, 2005</marker>
<rawString>Steedman, Mark. 2005. Grammar acquisition by child and machine. Invited Talk at the 17th European Summer School of Language, Logic and Information, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>Comment construire une syntaxe.</title>
<date>1934</date>
<booktitle>Bulletin de la Facult´e des lettres de Strasbourg,</booktitle>
<volume>12</volume>
<issue>7</issue>
<marker>Tesni`ere, 1934</marker>
<rawString>Tesni`ere, Lucien. 1934. Comment construire une syntaxe. Bulletin de la Facult´e des lettres de Strasbourg, 12(7):219–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>El´ements de Syntaxe Structurale.</title>
<date>1959</date>
<location>Klinksieck, Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>Tesni`ere, Lucien. 1959. El´ements de Syntaxe Structurale. Klinksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Vauquois</author>
</authors>
<title>La traduction automatique a` grenoble. Documents de linguistique quantitative,</title>
<date>1975</date>
<pages>24</pages>
<contexts>
<context position="17546" citStr="Vauquois 1975" startWordPosition="2776" endWordPosition="2777"> has served as a basis for many studies on formal grammar as well as from the NLP domain. The similarity between LTAG and a dependency-based description in relation to the model using the so-called supertags (which encode syntactic information in terms of dependency) is analyzed by Joshi and Srinivas (1994). 4 It should be recalled in this connection that within machine translation dependency-based systems (sometimes in combination with phrase structure) were already at play in the early times of MT; see, for example, the works of B. Vauquois, one of the founders of computational linguistics (Vauquois 1975; Vauquois and Chappuy 1985) and the systems developed in Japan under the influence and guidance (direct or indirect) of M. Nagao (see the survey in Nagao [1989]). 5 In a similar vein, Steedman (2005) argues that the use of statistical language models is the only way to create a computer program that automatically analyzes sentences on the basis of broadly conceived grammars (with due regard to ambiguities) such as dependency-based grammars or grammars specifying heads (governors); according to Steedman, these grammars work well because they reflect a mixture of semantic information and inform</context>
</contexts>
<marker>Vauquois, 1975</marker>
<rawString>Vauquois, Bernard. 1975. La traduction automatique a` grenoble. Documents de linguistique quantitative, 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Vauquois</author>
<author>Sylviane Chappuy</author>
</authors>
<title>Static grammars: A formalism for the description of linguistic models.</title>
<date>1985</date>
<booktitle>In Proceedings of the Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>298--322</pages>
<institution>Colgate University,</institution>
<location>Hamilton, New York.</location>
<contexts>
<context position="17574" citStr="Vauquois and Chappuy 1985" startWordPosition="2778" endWordPosition="2781">a basis for many studies on formal grammar as well as from the NLP domain. The similarity between LTAG and a dependency-based description in relation to the model using the so-called supertags (which encode syntactic information in terms of dependency) is analyzed by Joshi and Srinivas (1994). 4 It should be recalled in this connection that within machine translation dependency-based systems (sometimes in combination with phrase structure) were already at play in the early times of MT; see, for example, the works of B. Vauquois, one of the founders of computational linguistics (Vauquois 1975; Vauquois and Chappuy 1985) and the systems developed in Japan under the influence and guidance (direct or indirect) of M. Nagao (see the survey in Nagao [1989]). 5 In a similar vein, Steedman (2005) argues that the use of statistical language models is the only way to create a computer program that automatically analyzes sentences on the basis of broadly conceived grammars (with due regard to ambiguities) such as dependency-based grammars or grammars specifying heads (governors); according to Steedman, these grammars work well because they reflect a mixture of semantic information and information based on knowledge of </context>
</contexts>
<marker>Vauquois, Chappuy, 1985</marker>
<rawString>Vauquois, Bernard and Sylviane Chappuy. 1985. Static grammars: A formalism for the description of linguistic models. In Proceedings of the Conference on Theoretical and Methodological Issues in Machine Translation, pages 298–322, Colgate University, Hamilton, New York.</rawString>
</citation>
<citation valid="true">
<title>De l’ordre des mots dans les langues anciennes compar´ees aux langues modernes (The Order</title>
<date>1978</date>
<booktitle>of Words in the Ancient Languages Compared with That of Modern Languages),</booktitle>
<location>Paris; Amsterdam</location>
<contexts>
<context position="14968" citStr="(1978)" startWordPosition="2384" endWordPosition="2384">evelopment of linguistic theory in the 460 Hajiˇcov´a Old Linguists Never Die past 50 years that the deeper the analysis goes, the more the need of an introduction of the distinction between the notions of “head” and “modifier” (predicate, argument) is felt. Let us only recall here such approaches as: (i) the lexicosemantic analysis by Katz and Postal (1964), who work with the notions of head and modifier when specifying selection restrictions; (ii) the distinction between surface-oriented constituent structure and the (underlying) functional structure in lexical functional grammar by Bresnan (1978) and Kaplan and Bresnan (1982); (iii) the above-mentioned case grammar by Fillmore, motivated by the conviction that Chomskyan deep structure (with its specification of “deep” subject as a constituent of S regardless of the (different) semantic relations of the given NP to the verb) is not deep enough to capture the real underlying structure of the sentence; and (iv) the consecutive introduction of theta roles into the government and binding theory.3 On the other hand, dependency-based considerations have gradually and evasively penetrated to the ”data”-oriented statistical methods and treeban</context>
</contexts>
<marker>1978</marker>
<rawString>Weil, Henri. 1844. De l’ordre des mots dans les langues anciennes compar´ees aux langues modernes (The Order of Words in the Ancient Languages Compared with That of Modern Languages), Paris; Amsterdam [1978].</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>