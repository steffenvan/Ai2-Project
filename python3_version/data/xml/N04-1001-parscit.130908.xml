<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000071">
<title confidence="0.991825">
A Statistical Model for Multilingual Entity Detection and Tracking
</title>
<author confidence="0.834251">
R. Florian, H. Hassan , A. Ittycheriah, H. Jing
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos
</author>
<affiliation confidence="0.632781">
I.B.M. T.J. Watson Research Center
</affiliation>
<address confidence="0.69997">
Yorktown Heights, NY 10598
</address>
<email confidence="0.9764025">
{raduf,abei,hjing,nanda,xiaoluo, nicolas,roukos}@us.ibm.com
hanyh@eg.ibm.com
</email>
<sectionHeader confidence="0.995271" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900818181818">
Entity detection and tracking is a relatively new
addition to the repertoire of natural language
tasks. In this paper, we present a statistical
language-independent framework for identify-
ing and tracking named, nominal and pronom-
inal references to entities within unrestricted
text documents, and chaining them into clusters
corresponding to each logical entity present in
the text. Both the mention detection model
and the novel entity tracking model can use
arbitrary feature types, being able to integrate
a wide array of lexical, syntactic and seman-
tic features. In addition, the mention detec-
tion model crucially uses feature streams de-
rived from different named entity classifiers.
The proposed framework is evaluated with sev-
eral experiments run in Arabic, Chinese and
English texts; a system based on the approach
described here and submitted to the latest Au-
tomatic Content Extraction (ACE) evaluation
achieved top-tier results in all three evaluation
languages.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999748933333333">
Detecting entities, whether named, nominal or pronom-
inal, in unrestricted text is a crucial step toward under-
standing the text, as it identifies the important concep-
tual objects in a discourse. It is also a necessary step for
identifying the relations present in the text and populating
a knowledge database. This task has applications in in-
formation extraction and summarization, information re-
trieval (one can get all hits for Washington/person and not
the ones for Washington/state or Washington/city), data
mining and question answering.
The Entity Detection and Tracking task (EDT hence-
forth) has close ties to the named entity recognition
(NER) and coreference resolution tasks, which have been
the focus of attention of much investigation in the recent
past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev
et al., 1999; Miller et al., 1998; Aberdeen et al., 1995;
Ng and Cardie, 2002; Soon et al., 2001), and have been
at the center of several evaluations: MUC-6, MUC-7,
CoNLL’02 and CoNLL’03 shared tasks. Usually, in com-
putational linguistic literature, a named entity represents
an instance of a name, either a location, a person, an or-
ganization, and the NER task consists of identifying each
individual occurrence of such an entity. We will instead
adopt the nomenclature of the Automatic Content Extrac-
tion program&apos; (NIST, 2003a): we will call the instances
of textual references to objects or abstractions mentions,
which can be either named (e.g. John Mayor), nominal
(e.g. the president) or pronominal (e.g. she, it). An entity
consists of all the mentions (of any level) which refer to
one conceptual entity. For instance, in the sentence
</bodyText>
<subsectionHeader confidence="0.628907">
President John Smith said he has no comments.
</subsectionHeader>
<bodyText confidence="0.993738172413794">
there are two mentions: John Smith and he (in the order
of appearance, their levels are named and pronominal),
but one entity, formed by the set {John Smith, he}.
In this paper, we present a general statistical frame-
work for entity detection and tracking in unrestricted text.
The framework is not language specific, as proved by ap-
plying it to three radically different languages: Arabic,
Chinese and English. We separate the EDT task into a
mention detection part – the task of finding all mentions
in the text – and an entity tracking part – the task of com-
bining the detected mentions into groups of references to
the same object.
The work presented here is motivated by the ACE eval-
uation framework, which has the more general goal of
building multilingual systems which detect not only enti-
ties, but also relations among them and, more recently,
events in which they participate. The EDT task is ar-
guably harder than traditional named entity recognition,
because of the additional complexity involved in extract-
ing non-named mentions (nominals and pronouns) and
the requirement of grouping mentions into entities.
We present and evaluate empirically statistical mod-
els for both mention detection and entity tracking prob-
lems. For mention detection we use approaches based on
Maximum Entropy (MaxEnt henceforth) (Berger et al.,
1996) and Robust Risk Minimization (RRM henceforth)
&apos;For a description of the ACE program see
http://www.nist.gov/speech/tests/ace/.
(Zhang et al., 2002). The task is transformed into a se-
quence classification problem. We investigate a wide ar-
ray of lexical, syntactic and semantic features to perform
the mention detection and classification task including,
for all three languages, features based on pre-existing sta-
tistical semantic taggers, even though these taggers have
been trained on different corpora and use different seman-
tic categories. Moreover, the presented approach implic-
itly learns the correlation between these different seman-
tic types and the desired output types.
We propose a novel MaxEnt-based model for predict-
ing whether a mention should or should not be linked to
an existing entity, and show how this model can be used
to build entity chains. The effectiveness of the approach
is tested by applying it on data from the above mentioned
languages — Arabic, Chinese, English.
The framework presented in this paper is language-
universal – the classification method does not make any
assumption about the type of input. Most of the fea-
ture types are shared across the languages, but there are a
small number of useful feature types which are language-
specific, especially for the mention detection task.
The paper is organized as follows: Section 2 describes
the algorithms and feature types used for mention detec-
tion. Section 3 presents our approach to entity tracking.
Section 4 describes the experimental framework and the
systems’ results for Arabic, Chinese and English on the
data from the latest ACE evaluation (September 2003), an
investigation of the effect of using different feature types,
as well as a discussion of the results.
</bodyText>
<sectionHeader confidence="0.94227" genericHeader="method">
2 Mention Detection
</sectionHeader>
<bodyText confidence="0.9999748">
The mention detection system identifies the named, nom-
inal and pronominal mentions introduced in the previous
section. Similarly to classical NLP tasks such as base
noun phrase chunking (Ramshaw and Marcus, 1994), text
chunking (Ramshaw and Marcus, 1995) or named entity
recognition (Tjong Kim Sang, 2002), we formulate the
mention detection problem as a classification problem,
by assigning to each token in the text a label, indicating
whether it starts a specific mention, is inside a specific
mention, or is outside any mentions.
</bodyText>
<subsectionHeader confidence="0.999149">
2.1 The Statistical Classifiers
</subsectionHeader>
<bodyText confidence="0.988488076923077">
Good performance in many natural language process-
ing tasks, such as part-of-speech tagging, shallow pars-
ing and named entity recognition, has been shown to de-
pend heavily on integrating many sources of information
(Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al.,
2003). Given the stated focus of integrating many feature
types, we are interested in algorithms that can easily in-
tegrate and make effective use of diverse input types. We
selected two methods which satisfy these criteria: a linear
classifier – the Robust Risk Minimization classifier – and
a log-linear classifier – the Maximum Entropy classifier.
Both methods can integrate arbitrary types of informa-
tion and make a classification decision by aggregating all
information available for a given classification.
Before formally describing the methods2, we introduce
some notations: let be the set of pre-
dicted classes, be the example space and
be the feature space. Each example has associated
a vector of binary features .
We also assume the existence of a training data set
and a test set .
The RRM algorithm (Zhang et al., 2002) constructs
linear classifiers (one for each predicted class),
each predicting whether the current example belongs to
the class or not. Every such classifier has an associ-
ated feature weight vector, , which is learned
during the training phase so as to minimize the classifica-
tion error rate3. At test time, for each example , the
model computes a score
and labels the example with either the class correspond-
ing to the classifier with the highest score, if above
0, or outside, otherwise. The full decoding algorithm
is presented in Algorithm 1. This algorithm can also
be used for sequence classification (Williams and Peng,
1990), by converting the activation scores into probabili-
ties (through the soft-max function, for instance) and us-
ing the standard dynamic programing search algorithm
(also known as Viterbi search).
Algorithm 1 The RRM Decoding Algorithm
Somewhat similarly, the MaxEnt algorithm has an as-
sociated set of weights , which are estimated
during the training phase so as to maximize the likelihood
of the data (Berger et al., 1996). Given these weights, the
model computes the probability distribution of a particu-
lar example as follows:
where is a normalization factor.
After computing the class probability distribution, the
assigned class is the most probable one a posteriori. The
sketch of applying MaxEnt to the test data is presented
in Algorithm 2. Similarly to the RRM model, we use
the model to perform sequence classification, through dy-
namic programing.
</bodyText>
<footnote confidence="0.9918026">
2This is not meant to be an in-depth introduction to the meth-
ods, but a brief overview to familiarize the reader with them.
3Actually, the optimizing function contains a regularization
factor which considerably improves the robustness of the sys-
tem — for full details, see Zhang et al. (2002).
</footnote>
<equation confidence="0.6149874">
foreach
foreach
Algorithm 2 The MaxEnt Decoding Algorithm
foreach
Normalize (p)
</equation>
<bodyText confidence="0.999870333333333">
Within this framework, any type of feature can be used,
enabling the system designer to experiment with interest-
ing feature types, rather than worry about specific feature
interactions. In contrast, in a rule based system, the sys-
tem designer would have to consider how, for instance,
a WordNet (Miller, 1995) derived information for a par-
ticular example interacts with a part-of-speech-based in-
formation and chunking information. That is not to say,
ultimately, that rule-based systems are in some way infe-
rior to statistical models – they are built using valuable
insight which is hard to obtain from a statistical-model-
only approach. Instead, we are just suggesting that the
output of such a system can be easily integrated into the
previously described framework, as one of the input fea-
tures, most likely leading to improved performance.
</bodyText>
<subsectionHeader confidence="0.998004">
2.2 The Combination Hypothesis
</subsectionHeader>
<bodyText confidence="0.999980263157895">
In addition to using rich lexical, syntactic, and semantic
features, we leveraged several pre-existing mention tag-
gers. These pre-existing taggers were trained on datasets
outside of ACE training data and they identify types of
mentions different from the ACE types of mentions. For
instance, a pre-existing tagger may identify dates or oc-
cupation mentions (not used in ACE), among other types.
It could also have a class called PERSON, but the anno-
tation guideline of what represents a PERSON may not
match exactly to the notion of the PERSON type in ACE.
Our hypothesis – the combination hypothesis – is that
combining pre-existing classifiers from diverse sources
will boost performance by injecting complementary in-
formation into the mention detection models. Hence, we
used the output of these pre-existing taggers and used
them as additional feature streams for the mention de-
tection models. This approach allows the system to au-
tomatically correlate the (different) mention types to the
desired output.
</bodyText>
<subsectionHeader confidence="0.997109">
2.3 Language-Independent Features
</subsectionHeader>
<bodyText confidence="0.998228458333334">
Even if the three languages (Arabic, Chinese and English)
are radically different syntacticly, semantically, and even
graphically, all models use a few universal types of fea-
tures, while others are language-specific. Let us note
again that, while some types of features only apply to
one language, the models have the same basic structure,
treating the problem as an abstract classification task.
The following is a list of the features that are shared
across languages ( is considered by default the current
token):
tokens4 in a window of : ;
the part-of-speech associated with token
dictionary information (whether the current token
is part of a large collection of dictionaries - one
boolean value for each dictionary)
the output of named mention detectors trained on
different style of entities.
the previously assigned classification tags5.
The following sections describe in detail the language-
specific features, and Table 1 summarizes the feature
types used in building the models in the three languages.
Finally, the experiments in Section 4 detail the perfor-
mance obtained by using selected combinations of fea-
ture subsets.
</bodyText>
<subsectionHeader confidence="0.974108">
2.4 Arabic Mention Detection
</subsectionHeader>
<bodyText confidence="0.990052458333333">
Arabic, a highly inflected language, has linguistic pecu-
liarities that affect any mention detection system. An im-
portant aspect that needs to be addressed is segmentation:
which style should be used, how to deal with the inher-
ent segmentation ambiguity of mention names, especially
persons and locations, and, finally, how to handle the at-
tachment of pronouns to stems. Arabic blank-delimited
words are composed of zero or more prefixes, followed
by a stem and zero or more suffixes. Each prefix, stem or
suffix will be called a token in this discussion; any con-
tiguous sequence of tokens can represent a mention.
For example, the word “trwmAn” (translation: “Tru-
man”) could be segmented in 3 tokens (for instance, if
the word was not seen in the training data):
trwmAn t rwm An
which introduces ambiguity, as the three tokens form re-
ally just one mention, and, in the case of the word “tm-
nEh”, which has the segmentation
tmnEh t mnE h
the first and third tokens should both be labeled as
pronominal mentions – but, to do this, they need to be
separated from the stem mnE.
Pragmatically, we found segmenting Arabic text to be a
necessary and beneficial process due mainly to two facts:
</bodyText>
<listItem confidence="0.816537166666667">
1. some prefixes/suffixes can receive a different men-
tion type than the stem they are glued to (for in-
stance, in the case of pronouns);
2. keeping words together results in significant data
sparseness, because of the inflected nature of the
language.
</listItem>
<footnote confidence="0.99244825">
4Each language may have a different notion of what repre-
sents a token.
5In the current implementation, the models use a history of
2 tags.
</footnote>
<table confidence="0.998426">
foreach
Feature Type Ar Zh En
Token in window of 5
Morph in window of 5 N/A
POS info
Text chunking info — —
Capitalization/word-type N/A N/A
Prefixes/suffixes N/A
Gazetteer info
Gap — —
Wordnet info — —
Segmentation N/A
Additional systems’ output
</table>
<tableCaption confidence="0.999903">
Table 1: Summary of features used by the 3 systems
</tableCaption>
<bodyText confidence="0.999787032258065">
Given these observations, we decided to “condition” the
output of the system on the segmented data: the text is
first segmented into tokens, and the classification is then
performed on tokens. The segmentation model is similar
to the one presented by Lee et al. (2003), and obtains an
accuracy of about 98%.
In addition, special attention is paid to prefixes and suf-
fixes: in order to reduce the number of spurious tokens
we re-merge the prefixes or suffixes to their correspond-
ing stem if they are not essential to the classification pro-
cess. For this purpose, we collect the following statistics
for each prefix/suffix from the ACE training data: the
frequency of occurring as a mention by itself ( ) and
the frequency of occurring as a part of mention ( ).
If the ratio is below a threshold (estimated on the de-
velopment data), is re-merged with its corresponding
stem. Only few prefixes and suffixes were merged using
these criteria. This is appropriate for the ACE task, since
a large percentage of prefixes and suffixes are annotated
as pronoun mentions6.
In addition to the language-general features described
in Section 2.3, the Arabic system implements a feature
specifying for each token its original stem.
For this system, the gazetteer features are computed on
words, not on tokens; the gazetteers consist of 12000 per-
son names and 3000 location and country names, all of
which have been collected by few man-hours web brows-
ing. The system also uses features based on the output
of three additional mention detection classifiers: a RRM
model predicting 48 mention categories, a RRM model
and a HMM model predicting 32 mention categories.
</bodyText>
<subsectionHeader confidence="0.892733">
2.5 Chinese Mention Detection
</subsectionHeader>
<bodyText confidence="0.914784740740741">
In Chinese text, unlike in Indo-European languages,
words neither are white-space delimited nor do they have
capitalization markers. Instead of a word-based model,
we build a character-based one, since word segmentation
6For some additional data, annotated with 32 named cate-
gories, mentioned later on, we use the same approach of col-
lecting the and statistics, but, since named mentions are
predominant and there are no pronominal mentions in that case,
most suffixes and some prefixes are merged back to their origi-
nal stem.
errors can lead to irrecoverable mention detection errors;
Jing et al. (2003) also observe that character-based mod-
els are better performing than word-based ones for Chi-
nese named entity recognition. Although the model is
character-based, segmentation information is still useful
and is integrated as an additional feature stream.
Some more information about additional resources
used in building the system:
Gazetteers include dictionaries of 10k person
names, 8k location and country names, and 3k orga-
nization names, compiled from annotated corpora.
There are four additional classifiers whose output is
used as features: a RRM model which outputs 32
named categories, a RRM model identifying 49 cat-
egories, a RRM model identifying 45 mention cat-
egories, and a RRM model that classifies whether a
character is an English character, a numeral or other.
</bodyText>
<subsectionHeader confidence="0.790544">
2.6 English Mention Detection
</subsectionHeader>
<bodyText confidence="0.9981032">
The English mention detection model is similar to the
system described in (Ittycheriah et al., 2003)7.The fol-
lowing is a list of additional features (again, is the
current token):
Shallow parsing information associated with the to-
kens in window of 3;
Prefixes/suffixes of length up to 4;
A capitalization/word-type flag (similar to the ones
described by Bikel et al. (1997));
Gazetteer information: a handful of location (55k
entries) person names (30k) and organizations (5k)
dictionaries;
A combination of gazetteer, POS and capitalization
information, obtained as follows: if the word is a
closed-class word — select its class, else if it’s in
a dictionary — select that class, otherwise back-off
to its capitalization information; we call this feature
gap;
WordNet information (the synsets and hypernyms of
the two most frequent senses of the word);
The outputs of three systems (HMM, RRM and
MaxEnt) trained on a 32-category named entity data,
the output of an RRM system trained on the MUC-6
data, and the output of RRM model identifying 49
categories.
</bodyText>
<sectionHeader confidence="0.996248" genericHeader="method">
3 Entity Tracking
</sectionHeader>
<bodyText confidence="0.999749">
This section introduces a novel statistical approach to en-
tity tracking. We choose to model the process of forming
entities from mentions, one step at a time. The process
works from left to right: it starts with an initial entity
consisting of the first mention of a document, and the next
mention is processed by either linking it with one of the
</bodyText>
<footnote confidence="0.9578625">
7The main difference between their system and ours is that
they build a MaxEnt model capable of building hierarchical
structures – therefore treating the problem as a parsing task –
while our system treats the problem as a classification task.
</footnote>
<bodyText confidence="0.999282">
existing entities, or starting a new entity. The process
could have as output any one of the possible partitions of
the mention set.8 Two separate models are used to score
the linking and starting actions, respectively.
</bodyText>
<subsectionHeader confidence="0.998808">
3.1 Tracking Algorithm
</subsectionHeader>
<bodyText confidence="0.97966425">
Formally, let be mentions in a
document. Let be the map from mention index
to entity index. For a mention index ,
let us define
the set of indices of the partially-established entities to
the left of (note that ), and
the set of the partially-established entities.
Given that has been formed to the left of the ac-
tive mention ,can take two possible actions: if
, then the active mention is said to link with
the entity ; Otherwise it starts a new entity . At
training time, the action is known to us, and at testing
time, both hypotheses will be kept during search. Notice
that a sequence of such actions corresponds uniquely to
an entity outcome (or a partition of mentions). There-
fore, the problem of coreference resolution is equivalent
to ranking the action sequences.
In this work, a binary model
is used to compute the link probability, where ,
is iff links with ; the random variable is the
index of the partial entity to which is linking. Since
starting a new entity means that does not link with
any entities in , the probability of starting a new entity,
, can be computed as
</bodyText>
<equation confidence="0.968932">
(1)
</equation>
<bodyText confidence="0.93916725">
Therefore, the probability of starting an entity can
be computed using the linking probabilities
, provided that the marginal
is known. While other models are possible, in
the results reported in this paper, is
approximated as:
(2)
&apos;The number of all possible partitions of a set is given by
the Bell number (Bell, 1934). This number is very large even
for a document with a moderate number of mentions: about
trillion for a 20-mention document. For practical reasons,
the search space has to be reduced to a reasonably small set of
hypotheses.
That is, the starting probability is just one minus the max-
imum linking probability.
Training directly the model
is difficult since it depends on all partial entities . As
a first attempt of modeling the process from mentions to
entities, we make the following modeling assumptions:
Algorithm 3 Coreference Decoding Algorithm
</bodyText>
<subsectionHeader confidence="0.998204">
3.2 Entity Tracking Features
</subsectionHeader>
<bodyText confidence="0.986838851851852">
A maximum entropy model is used to implement (4).
Atomic features used by the model include:
string match – whether or not the mention strings of
and are exactly match, or partially match;
context – surrounding words or part-of-speech tags
(if available) of mentions ;
mention count – how many times a mention string
appears in the document. The count is quantized;
distance – distance between the two mentions in
words and sentences. This number is also quantized;
editing distance – quantized editing distance be-
tween the two mentions;
mention information – spellings of the two mentions
and other information (such as POS tags) if avail-
able; If a mention is a pronoun, the feature also com-
putes gender, plurality, possessiveness and reflexive-
ness;
acronym – whether or not one mention is the
acronym of the other mention;
syntactic features – whether or not the two mentions
appear in apposition. This information is extracted
from a parse tree, and can be computed only when a
parser is available;
Once the linking probability
is available, the starting probability
can be computed using (1) and (2). The strategy used to
find the best set of entities is shown in Algorithm 3.
</bodyText>
<table confidence="0.890743416666667">
Input: mentions in text
Output: a partition of the set
foreach
foreach
foreach
return
if
otherwise
Data Set Arabic Chinese English
Train 65.6k 86.5k 340.7k
Development Test 7.7k 7.2k 71k
Sep’03 Eval Test 93.5k 108.2k 60.7k
</table>
<tableCaption confidence="0.9906355">
Table 2: Data statistics (number of tokens) for Arabic,
Chinese and English
</tableCaption>
<bodyText confidence="0.999803333333334">
Another category of features is created by taking con-
junction of the atomic features. For example, the model
can capture how far a pronoun mention is from a named
mention when the distance feature is used in conjunction
with mention information feature.
As it is the case with with mention detection ap-
proach presented in Section 2, most features used here are
language-independent and are instantiated from the train-
ing data, while some are language-specific, but mostly
because the resources were not available for the specific
language. For example, syntactic features are not used in
the Arabic system due to the lack of an Arabic parser.
Simple as it seems, the mention-pair model has been
shown to work well (Soon et al., 2001; Ng and Cardie,
2002). As will be shown in Section 4, the relatively
knowledge-lean feature sets work fairly well in our tasks.
Although we also use a mention-pair model, our
tracking algorithm differs from Soon et al. (2001),
Ng and Cardie (2002) in several aspects. First, the
mention-pair model is used as an approximation to the
entity-mention model (3), which itself is an approxima-
tion of . Second, instead of
doing a pick-first (Soon et al., 2001) or best-first (Ng and
Cardie, 2002) selection, the mention-pair linking model
is used to compute a starting probability. The starting
probability enables us to score the action of creating a
new entity without thresholding the link probabilities.
Third, this probabilistic framework allows us to search
the space of all possible entities, while Soon et al. (2001),
Ng and Cardie (2002) take the “best” local hypothesis.
</bodyText>
<sectionHeader confidence="0.990626" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999860636363636">
The data used in all experiments presented in this sec-
tion is provided by the Linguistic Data Consortium and is
distributed by NIST to all participants in the ACE evalua-
tion. In the comparative experiments for the mention de-
tection and entity tracking tasks, the training data for the
English system consists of the training data from both the
2002 evaluation and the 2003 evaluation, while for Ara-
bic and Chinese, new additions to the ACE task in 2003,
consists of 80% of the provided training data. Table 2
shows the sizes of the training, development and eval-
uation test data for the 3 languages. The data is anno-
tated with five types of entities: person, organization,
geo-political entity, location, facility; each mention can
be either named, nominal or pronominal, and can be ei-
ther generic (not referring to a clearly described entity)
or specific.
The models for all three languages are built as joint
models, simultaneously predicting the type, level and
genericity of a mention – basically each mention is la-
beled with a 3-pronged tag. To transform the problem
into a classification task, we use the IOB2 classification
scheme (Tjong Kim Sang and Veenstra, 1999).
</bodyText>
<subsectionHeader confidence="0.992406">
4.1 The ACE Value
</subsectionHeader>
<bodyText confidence="0.99969204">
A gauge of the performance of an EDT system is the ACE
value, a measure developed especially for this purpose. It
estimates the normalized weighted cost of detection of
specific-only entities in terms of misses, false alarms and
substitution errors (entities marked generic are excluded
from computation): any undetected entity is considered
a miss, system-output entities with no corresponding ref-
erence entities are considered false alarms, and entities
whose type was mis-assigned are substitution errors. The
ACE value computes a weighted cost by applying differ-
ent weights to each error, depending on the error type and
target entity type (e.g. PERSON-NAMEs are weighted
a lot more heavily than FACILITY-PRONOUNs) (NIST,
2003a). The cumulative cost is normalized by the cost
of a (hypothetical) system that outputs no entities at all
– which would receive an ACE value of . Finally, the
normalized cost is subtracted from 100.0 to obtain the
ACE value; a value of 100% corresponds to perfect en-
tity detection. A system can obtain a negative score if it
proposed too many incorrect entities.
In addition, for the mention detection task, we will also
present results by using the more established F-measure,
computed as the harmonic mean of precision and recall
– this measure gives equal importance to all entities, re-
gardless of their type, level or genericity.
</bodyText>
<subsectionHeader confidence="0.99101">
4.2 EDT Results
</subsectionHeader>
<bodyText confidence="0.99855025">
As described in Section 2.6, the mention detection sys-
tems make use of a large set of features. To better assert
the contribution of the different types of features to the fi-
nal performance, we have grouped them into 4 categories:
</bodyText>
<listItem confidence="0.916798272727273">
1. Surface features: lexical features that can be derived
from investigating the words: words, morphs, pre-
fix/suffix, capitalization/word-form flags
2. Features derived from processing the data with NLP
techniques: POS tags, text chunks, word segmenta-
tion, etc.
3. Gazetteer/dictionary features
4. Features obtained by running other named-entity
classifiers (with different tag sets): FrMM, MaxEnt
and RRM output on the 32-category, 49-category
and MUC data sets.9
</listItem>
<bodyText confidence="0.991691">
Table 3 presents the mention detection comparative re-
sults, F-measure and ACE value, on Arabic and Chinese
data. The Arabic and Chinese models were built using
</bodyText>
<footnote confidence="0.993643666666667">
9In the English MaxEnt system, which uses 295k features,
the distribution among the four classes of features is: 1:72%,
2:24%, 3:1%, 4:3%.
</footnote>
<table confidence="0.849035666666667">
Feature Arabic Chinese
Sets
F-measure ACE F-measure ACE
1 59.7 43.1 62.6 51.1
1+2 60.8 46.0 67.1 57.7
1+2+3 63.4 51.8 68.4 67.7
1+2+3+4 68.5 53.2 68.6 74.1
Table 3: Mention detection results for the Arabic and
Chinese
Arabic Chinese English
Feb02 Sept02
ACE value 83.2 89.4 90.9 88.0
</table>
<figure confidence="0.988057523809524">
72.1
1+2+3
72.1
72.6 72.5
73.2
1+2+3+4
1+2+4 1+3+4
73.4
72.0 73.2
70.8 70.7
1+2
1
RRM MaxEnt
71.4
69.1 70.4
1+3
71.8
71.3
1+4
72.5
English
</figure>
<tableCaption confidence="0.987219">
Table 4: Entity tracking results on true mentions
</tableCaption>
<bodyText confidence="0.999900533333334">
the RRM model. There are some interesting observa-
tions: first, the F-measure performance does not correlate
well with an improvement in ACE value – small improve-
ments in F-measure sometimes are paired with large rela-
tive improvements in ACE value, fact due to the different
weighting of entity types. Second, the largest single im-
provement in ACE value is obtained by adding dictionary
features, at least in this order of adding features.
For English, we investigated in more detail the way
features interact. Figure 1 presents a hierarchical direct
comparison between the performance of the RRM model
and the MaxEnt model. We can observe that the RRM
model makes better use of gazetteers, and manages to
close the initial performance gap to the MaxEnt model.
Table 4 presents the results obtained by running the en-
tity tracking algorithm on true mentions. It is interesting
to compare the entity tracking results with inter-annotator
agreements. LDC reported (NIST, 2003b) that the inter-
annotator agreement (computed as ACE-values) between
annotators are %, % and % for Arabic, Chi-
nese and English, respectively. The system performance
is very close to human performance on this task; this
small difference in performance highlights the difficulty
of the entity tracking task.
Finally, Table 5 presents the results obtained by run-
ning both mention detection followed by entity tracking
on the ACE’03 evaluation data. Our submission in the
evaluation performed well relative to the other partici-
pating systems (contractual obligations prevent us from
elaborating further).
</bodyText>
<subsectionHeader confidence="0.944358">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9996245">
The same basic model was used to perform EDT in three
languages. Our approach is language-independent, in that
</bodyText>
<table confidence="0.893567333333333">
Arabic Chinese English
RRM MaxEnt
ACE value 54.5 58.8 69.7 73.4
</table>
<tableCaption confidence="0.9636285">
Table 5: ACE value results for the three languages on
ACE’03 evaluation data.
</tableCaption>
<figureCaption confidence="0.985132">
Figure 1: Performance of the English mention detection
</figureCaption>
<bodyText confidence="0.982177976744186">
system on different sets of features (uniformly penalized
F-measure), September’02 data. The lower part of each
box describes the particular combination of feature types;
the arrows show a inclusion relationship between the fea-
ture sets.
the fundamental classification algorithm can be applied to
every language and the only changes involve finding ap-
propriate and available feature streams for each language.
The entity tracking system uses even fewer language-
specific features than the mention detection systems.
One limitation apparent in our mention detection sys-
tem is that it does not model explicitly the genericity of
a mention. Deciding whether a mention refers to a spe-
cific entity or a generic entity requires knowledge of sub-
stantially wider context than the window of 5 tokens we
currently use in our mention detection systems. One way
we plan to improve performance for such cases is to sep-
arate the task into two parts: one in which the mention
type and level are predicted, followed by a genericity-
predicting model which uses long-range features, such as
sentence or document level features.
Our entity tracking system currently cannot resolve the
coreference of pronouns very accurately. Although this is
weighted lightly in ACE evaluation, good anaphora res-
olution can be very useful in many applications and we
will continue exploring this task in the future.
The Arabic and Chinese EDT tasks were included in
the ACE evaluation for the first time in 2003. Unlike
the English case, the systems had access to only a small
amount of training data (60k words for Arabic and 90k
characters for Chinese, in contrast with 340k words for
English), which made it difficult to train statistical mod-
els with large number of feature types. Future ACE evalu-
ations will shed light on whether this lower performance,
shown in Table 3, is due to lack of training data or to
specific language-specific ambiguity.
The final observation we want to make is that the sys-
tems were not directly optimized for the ACE value, and
there is no obvious way to do so. As Table 3 shows, the
F-measure and ACE value do not correlate well: systems
trained to optimize the former might not end up optimiz-
ing the latter. It is an open research question whether a
system can be directly optimized for the ACE value.
</bodyText>
<sectionHeader confidence="0.998185" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999883789473684">
This paper presents a language-independent framework
for the entity detection and tracking task, which is shown
to obtain top-tier performance on three radically differ-
ent languages: Arabic, Chinese and English. The task is
separated into two sub-tasks: a mention detection part,
which is modeled through a named entity-like approach,
and an entity tracking part, for a which a novel modeling
approach is proposed.
This statistical framework is general and can incor-
porate heterogeneous feature types — the models were
built using a wide array of lexical, syntactic and seman-
tic features extracted from texts, and further enhanced
by adding the output of pre-existing semantic classifiers
as feature streams; additional feature types help improve
the performance significantly, especially in terms of ACE
value. The experimental results show that the systems
perform remarkably well, for both well investigated lan-
guages, such as English, and for the relatively new addi-
tions Arabic and Chinese.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999964666666667">
We would like to thank Dr. Tong Zhang for providing us
with the RRM toolkit.
This work was partially supported by the Defense
Advanced Research Projects Agency and monitored by
SPAWAR under contract No. N66001-99-2-8916. The
views and findings contained in this material are those
of the authors and do not necessarily reflect the position
of policy of the U.S. government and no official endorse-
ment should be inferred.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996247666666667">
J. Aberdeen, D. Day, L. Hirschman, P. Robinson, and
M. Vilain. 1995. Mitre: Description of the Alembic
system used for MUC-6. In Proceedings of MUC-6,
pages 141–155.
E. T. Bell. 1934. Exponential numbers. American Math.
Monthly, 41:411–419.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings ofANLP-97, pages 194–201.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grish-
man. 1998. Exploiting diverse knowledge sources via
maximum entropy in named entity recognition.
A. Ittycheriah, L. Lita, N. Kambhatla, N. Nicolov,
S. Roukos, and M. Stys. 2003. Identifying and track-
ing entity mentions in a maximum entropy framework.
In HLT-NAACL 2003: Short Papers, May 27 - June 1.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. Itty-
cheriah. 2003. HowtogetaChineseName(Entity): Seg-
mentation and combination issues. In Proceedings of
EMNLP’03, pages 200–207.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and
H. Hassan. 2003. Language model based Arabic word
segmentation. In Proceedings of the ACL’03, pages
399–406.
A. Mikheev, M. Moens, and C. Grover. 1999. Named
entity recognition without gazetteers. In Proceedings
of EACL’99.
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz,
R. Stone, and R. Weischedel. 1998. Bbn: Description
of the SIFT system as used for MUC-7. In MUC-7.
G. A. Miller. 1995. WordNet: A lexical database. Com-
munications of the ACM, 38(11).
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of the ACL’02, pages 104–111.
NIST. 2003a. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
NIST. 2003b. Proceedings of ACE’03. Booklet, Alexan-
dria, VA, September.
L. Ramshaw and M. Marcus. 1994. Exploring the sta-
tistical derivation of transformational rule sequences
for part-of-speech tagging. In Proceedings of the ACL
Workshop on Combining Symbolic and Statistical Ap-
proaches to Language, pages 128–135.
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings of
WVLC’95, pages 82–94.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521–544.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Represent-
ing text chunks. In Proceedings of EACL’99.
E. F. Tjong Kim Sang. 2002. Introduction to the CoNLL-
2002 shared task: Language-independent named en-
tity recognition. In Proceedings of CoNLL-2002,
pages 155–158.
R. J. Williams and J. Peng. 1990. An efficient
gradient–based algorithm for on–line training of re-
current neural networks trajectories. Neural Compu-
tation, 2(4):490–501.
T. Zhang, F. Damerau, and D. E. Johnson. 2002. Text
chunking based on a generalization of Winnow. Jour-
nal ofMachine Learning Research, 2:615–637.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.508270">
<title confidence="0.999472">A Statistical Model for Multilingual Entity Detection and Tracking</title>
<author confidence="0.6999695">A Ittycheriah</author>
<author confidence="0.6999695">H Kambhatla</author>
<author confidence="0.6999695">X Luo</author>
<author confidence="0.6999695">N Nicolov</author>
<affiliation confidence="0.871198">I.B.M. T.J. Watson Research</affiliation>
<address confidence="0.963681">Yorktown Heights, NY 10598</address>
<email confidence="0.9960535">raduf@us.ibm.comhanyh@eg.ibm.com</email>
<email confidence="0.9960535">abei@us.ibm.comhanyh@eg.ibm.com</email>
<email confidence="0.9960535">hjing@us.ibm.comhanyh@eg.ibm.com</email>
<email confidence="0.9960535">nanda@us.ibm.comhanyh@eg.ibm.com</email>
<email confidence="0.9960535">xiaoluo@us.ibm.comhanyh@eg.ibm.com</email>
<email confidence="0.9960535">nicolas@us.ibm.comhanyh@eg.ibm.com</email>
<email confidence="0.9960535">roukos@us.ibm.comhanyh@eg.ibm.com</email>
<abstract confidence="0.997128086956522">Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aberdeen</author>
<author>D Day</author>
<author>L Hirschman</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>Mitre: Description of the Alembic system used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<pages>141--155</pages>
<contexts>
<context position="2172" citStr="Aberdeen et al., 1995" startWordPosition="324" endWordPosition="327">resent in the text and populating a knowledge database. This task has applications in information extraction and summarization, information retrieval (one can get all hits for Washington/person and not the ones for Washington/state or Washington/city), data mining and question answering. The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev et al., 1999; Miller et al., 1998; Aberdeen et al., 1995; Ng and Cardie, 2002; Soon et al., 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computational linguistic literature, a named entity represents an instance of a name, either a location, a person, an organization, and the NER task consists of identifying each individual occurrence of such an entity. We will instead adopt the nomenclature of the Automatic Content Extraction program&apos; (NIST, 2003a): we will call the instances of textual references to objects or abstractions mentions, which can be either named (e.g. John Ma</context>
</contexts>
<marker>Aberdeen, Day, Hirschman, Robinson, Vilain, 1995</marker>
<rawString>J. Aberdeen, D. Day, L. Hirschman, P. Robinson, and M. Vilain. 1995. Mitre: Description of the Alembic system used for MUC-6. In Proceedings of MUC-6, pages 141–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Bell</author>
</authors>
<title>Exponential numbers.</title>
<date>1934</date>
<journal>American Math. Monthly,</journal>
<pages>41--411</pages>
<contexts>
<context position="21021" citStr="Bell, 1934" startWordPosition="3398" endWordPosition="3399">el is used to compute the link probability, where , is iff links with ; the random variable is the index of the partial entity to which is linking. Since starting a new entity means that does not link with any entities in , the probability of starting a new entity, , can be computed as (1) Therefore, the probability of starting an entity can be computed using the linking probabilities , provided that the marginal is known. While other models are possible, in the results reported in this paper, is approximated as: (2) &apos;The number of all possible partitions of a set is given by the Bell number (Bell, 1934). This number is very large even for a document with a moderate number of mentions: about trillion for a 20-mention document. For practical reasons, the search space has to be reduced to a reasonably small set of hypotheses. That is, the starting probability is just one minus the maximum linking probability. Training directly the model is difficult since it depends on all partial entities . As a first attempt of modeling the process from mentions to entities, we make the following modeling assumptions: Algorithm 3 Coreference Decoding Algorithm 3.2 Entity Tracking Features A maximum entropy mo</context>
</contexts>
<marker>Bell, 1934</marker>
<rawString>E. T. Bell. 1934. Exponential numbers. American Math. Monthly, 41:411–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4343" citStr="Berger et al., 1996" startWordPosition="678" endWordPosition="681"> more general goal of building multilingual systems which detect not only entities, but also relations among them and, more recently, events in which they participate. The EDT task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominals and pronouns) and the requirement of grouping mentions into entities. We present and evaluate empirically statistical models for both mention detection and entity tracking problems. For mention detection we use approaches based on Maximum Entropy (MaxEnt henceforth) (Berger et al., 1996) and Robust Risk Minimization (RRM henceforth) &apos;For a description of the ACE program see http://www.nist.gov/speech/tests/ace/. (Zhang et al., 2002). The task is transformed into a sequence classification problem. We investigate a wide array of lexical, syntactic and semantic features to perform the mention detection and classification task including, for all three languages, features based on pre-existing statistical semantic taggers, even though these taggers have been trained on different corpora and use different semantic categories. Moreover, the presented approach implicitly learns the c</context>
<context position="8844" citStr="Berger et al., 1996" startWordPosition="1396" endWordPosition="1399">th the highest score, if above 0, or outside, otherwise. The full decoding algorithm is presented in Algorithm 1. This algorithm can also be used for sequence classification (Williams and Peng, 1990), by converting the activation scores into probabilities (through the soft-max function, for instance) and using the standard dynamic programing search algorithm (also known as Viterbi search). Algorithm 1 The RRM Decoding Algorithm Somewhat similarly, the MaxEnt algorithm has an associated set of weights , which are estimated during the training phase so as to maximize the likelihood of the data (Berger et al., 1996). Given these weights, the model computes the probability distribution of a particular example as follows: where is a normalization factor. After computing the class probability distribution, the assigned class is the most probable one a posteriori. The sketch of applying MaxEnt to the test data is presented in Algorithm 2. Similarly to the RRM model, we use the model to perform sequence classification, through dynamic programing. 2This is not meant to be an in-depth introduction to the methods, but a brief overview to familiarize the reader with them. 3Actually, the optimizing function contai</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a high-performance learning namefinder.</title>
<date>1997</date>
<booktitle>In Proceedings ofANLP-97,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="2082" citStr="Bikel et al., 1997" startWordPosition="308" endWordPosition="311">ual objects in a discourse. It is also a necessary step for identifying the relations present in the text and populating a knowledge database. This task has applications in information extraction and summarization, information retrieval (one can get all hits for Washington/person and not the ones for Washington/state or Washington/city), data mining and question answering. The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev et al., 1999; Miller et al., 1998; Aberdeen et al., 1995; Ng and Cardie, 2002; Soon et al., 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computational linguistic literature, a named entity represents an instance of a name, either a location, a person, an organization, and the NER task consists of identifying each individual occurrence of such an entity. We will instead adopt the nomenclature of the Automatic Content Extraction program&apos; (NIST, 2003a): we will call the instances of textu</context>
<context position="18072" citStr="Bikel et al. (1997)" startWordPosition="2889" endWordPosition="2892">ich outputs 32 named categories, a RRM model identifying 49 categories, a RRM model identifying 45 mention categories, and a RRM model that classifies whether a character is an English character, a numeral or other. 2.6 English Mention Detection The English mention detection model is similar to the system described in (Ittycheriah et al., 2003)7.The following is a list of additional features (again, is the current token): Shallow parsing information associated with the tokens in window of 3; Prefixes/suffixes of length up to 4; A capitalization/word-type flag (similar to the ones described by Bikel et al. (1997)); Gazetteer information: a handful of location (55k entries) person names (30k) and organizations (5k) dictionaries; A combination of gazetteer, POS and capitalization information, obtained as follows: if the word is a closed-class word — select its class, else if it’s in a dictionary — select that class, otherwise back-off to its capitalization information; we call this feature gap; WordNet information (the synsets and hypernyms of the two most frequent senses of the word); The outputs of three systems (HMM, RRM and MaxEnt) trained on a 32-category named entity data, the output of an RRM sys</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: a high-performance learning namefinder. In Proceedings ofANLP-97, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<title>Exploiting diverse knowledge sources via maximum entropy in named entity recognition.</title>
<date>1998</date>
<contexts>
<context position="2106" citStr="Borthwick et al., 1998" startWordPosition="312" endWordPosition="315">course. It is also a necessary step for identifying the relations present in the text and populating a knowledge database. This task has applications in information extraction and summarization, information retrieval (one can get all hits for Washington/person and not the ones for Washington/state or Washington/city), data mining and question answering. The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev et al., 1999; Miller et al., 1998; Aberdeen et al., 1995; Ng and Cardie, 2002; Soon et al., 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computational linguistic literature, a named entity represents an instance of a name, either a location, a person, an organization, and the NER task consists of identifying each individual occurrence of such an entity. We will instead adopt the nomenclature of the Automatic Content Extraction program&apos; (NIST, 2003a): we will call the instances of textual references to objects</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998. Exploiting diverse knowledge sources via maximum entropy in named entity recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>L Lita</author>
<author>N Kambhatla</author>
<author>N Nicolov</author>
<author>S Roukos</author>
<author>M Stys</author>
</authors>
<title>Identifying and tracking entity mentions in a maximum entropy framework.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003: Short Papers,</booktitle>
<contexts>
<context position="6965" citStr="Ittycheriah et al., 2003" startWordPosition="1090" endWordPosition="1093">hunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. 2.1 The Statistical Classifiers Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al., 2003). Given the stated focus of integrating many feature types, we are interested in algorithms that can easily integrate and make effective use of diverse input types. We selected two methods which satisfy these criteria: a linear classifier – the Robust Risk Minimization classifier – and a log-linear classifier – the Maximum Entropy classifier. Both methods can integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classification. Before formally describing the methods2, we introduce some notations: let be the set of predi</context>
<context position="17799" citStr="Ittycheriah et al., 2003" startWordPosition="2845" endWordPosition="2848">dditional resources used in building the system: Gazetteers include dictionaries of 10k person names, 8k location and country names, and 3k organization names, compiled from annotated corpora. There are four additional classifiers whose output is used as features: a RRM model which outputs 32 named categories, a RRM model identifying 49 categories, a RRM model identifying 45 mention categories, and a RRM model that classifies whether a character is an English character, a numeral or other. 2.6 English Mention Detection The English mention detection model is similar to the system described in (Ittycheriah et al., 2003)7.The following is a list of additional features (again, is the current token): Shallow parsing information associated with the tokens in window of 3; Prefixes/suffixes of length up to 4; A capitalization/word-type flag (similar to the ones described by Bikel et al. (1997)); Gazetteer information: a handful of location (55k entries) person names (30k) and organizations (5k) dictionaries; A combination of gazetteer, POS and capitalization information, obtained as follows: if the word is a closed-class word — select its class, else if it’s in a dictionary — select that class, otherwise back-off </context>
</contexts>
<marker>Ittycheriah, Lita, Kambhatla, Nicolov, Roukos, Stys, 2003</marker>
<rawString>A. Ittycheriah, L. Lita, N. Kambhatla, N. Nicolov, S. Roukos, and M. Stys. 2003. Identifying and tracking entity mentions in a maximum entropy framework. In HLT-NAACL 2003: Short Papers, May 27 - June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Florian</author>
<author>X Luo</author>
<author>T Zhang</author>
<author>A Ittycheriah</author>
</authors>
<title>HowtogetaChineseName(Entity): Segmentation and combination issues.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP’03,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="6938" citStr="Jing et al., 2003" startWordPosition="1086" endWordPosition="1089">rcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. 2.1 The Statistical Classifiers Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al., 2003). Given the stated focus of integrating many feature types, we are interested in algorithms that can easily integrate and make effective use of diverse input types. We selected two methods which satisfy these criteria: a linear classifier – the Robust Risk Minimization classifier – and a log-linear classifier – the Maximum Entropy classifier. Both methods can integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classification. Before formally describing the methods2, we introduce some notatio</context>
<context position="16891" citStr="Jing et al. (2003)" startWordPosition="2706" endWordPosition="2709">ion Detection In Chinese text, unlike in Indo-European languages, words neither are white-space delimited nor do they have capitalization markers. Instead of a word-based model, we build a character-based one, since word segmentation 6For some additional data, annotated with 32 named categories, mentioned later on, we use the same approach of collecting the and statistics, but, since named mentions are predominant and there are no pronominal mentions in that case, most suffixes and some prefixes are merged back to their original stem. errors can lead to irrecoverable mention detection errors; Jing et al. (2003) also observe that character-based models are better performing than word-based ones for Chinese named entity recognition. Although the model is character-based, segmentation information is still useful and is integrated as an additional feature stream. Some more information about additional resources used in building the system: Gazetteers include dictionaries of 10k person names, 8k location and country names, and 3k organization names, compiled from annotated corpora. There are four additional classifiers whose output is used as features: a RRM model which outputs 32 named categories, a RRM</context>
</contexts>
<marker>Jing, Florian, Luo, Zhang, Ittycheriah, 2003</marker>
<rawString>H. Jing, R. Florian, X. Luo, T. Zhang, and A. Ittycheriah. 2003. HowtogetaChineseName(Entity): Segmentation and combination issues. In Proceedings of EMNLP’03, pages 200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-S Lee</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>O Emam</author>
<author>H Hassan</author>
</authors>
<title>Language model based Arabic word segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL’03,</booktitle>
<pages>399--406</pages>
<contexts>
<context position="14875" citStr="Lee et al. (2003)" startWordPosition="2376" endWordPosition="2379">mplementation, the models use a history of 2 tags. foreach Feature Type Ar Zh En Token in window of 5 Morph in window of 5 N/A POS info Text chunking info — — Capitalization/word-type N/A N/A Prefixes/suffixes N/A Gazetteer info Gap — — Wordnet info — — Segmentation N/A Additional systems’ output Table 1: Summary of features used by the 3 systems Given these observations, we decided to “condition” the output of the system on the segmented data: the text is first segmented into tokens, and the classification is then performed on tokens. The segmentation model is similar to the one presented by Lee et al. (2003), and obtains an accuracy of about 98%. In addition, special attention is paid to prefixes and suffixes: in order to reduce the number of spurious tokens we re-merge the prefixes or suffixes to their corresponding stem if they are not essential to the classification process. For this purpose, we collect the following statistics for each prefix/suffix from the ACE training data: the frequency of occurring as a mention by itself ( ) and the frequency of occurring as a part of mention ( ). If the ratio is below a threshold (estimated on the development data), is re-merged with its corresponding s</context>
</contexts>
<marker>Lee, Papineni, Roukos, Emam, Hassan, 2003</marker>
<rawString>Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan. 2003. Language model based Arabic word segmentation. In Proceedings of the ACL’03, pages 399–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>M Moens</author>
<author>C Grover</author>
</authors>
<title>Named entity recognition without gazetteers.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL’99.</booktitle>
<contexts>
<context position="2128" citStr="Mikheev et al., 1999" startWordPosition="316" endWordPosition="319">essary step for identifying the relations present in the text and populating a knowledge database. This task has applications in information extraction and summarization, information retrieval (one can get all hits for Washington/person and not the ones for Washington/state or Washington/city), data mining and question answering. The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev et al., 1999; Miller et al., 1998; Aberdeen et al., 1995; Ng and Cardie, 2002; Soon et al., 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computational linguistic literature, a named entity represents an instance of a name, either a location, a person, an organization, and the NER task consists of identifying each individual occurrence of such an entity. We will instead adopt the nomenclature of the Automatic Content Extraction program&apos; (NIST, 2003a): we will call the instances of textual references to objects or abstractions menti</context>
</contexts>
<marker>Mikheev, Moens, Grover, 1999</marker>
<rawString>A. Mikheev, M. Moens, and C. Grover. 1999. Named entity recognition without gazetteers. In Proceedings of EACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>M Crystal</author>
<author>H Fox</author>
<author>L Ramshaw</author>
<author>R Schwarz</author>
<author>R Stone</author>
<author>R Weischedel</author>
</authors>
<title>Bbn: Description of the SIFT system as used for MUC-7.</title>
<date>1998</date>
<booktitle>In MUC-7.</booktitle>
<contexts>
<context position="2149" citStr="Miller et al., 1998" startWordPosition="320" endWordPosition="323">fying the relations present in the text and populating a knowledge database. This task has applications in information extraction and summarization, information retrieval (one can get all hits for Washington/person and not the ones for Washington/state or Washington/city), data mining and question answering. The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev et al., 1999; Miller et al., 1998; Aberdeen et al., 1995; Ng and Cardie, 2002; Soon et al., 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computational linguistic literature, a named entity represents an instance of a name, either a location, a person, an organization, and the NER task consists of identifying each individual occurrence of such an entity. We will instead adopt the nomenclature of the Automatic Content Extraction program&apos; (NIST, 2003a): we will call the instances of textual references to objects or abstractions mentions, which can be eit</context>
</contexts>
<marker>Miller, Crystal, Fox, Ramshaw, Schwarz, Stone, Weischedel, 1998</marker>
<rawString>S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz, R. Stone, and R. Weischedel. 1998. Bbn: Description of the SIFT system as used for MUC-7. In MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: A lexical database.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="9962" citStr="Miller, 1995" startWordPosition="1576" endWordPosition="1577">a brief overview to familiarize the reader with them. 3Actually, the optimizing function contains a regularization factor which considerably improves the robustness of the system — for full details, see Zhang et al. (2002). foreach foreach Algorithm 2 The MaxEnt Decoding Algorithm foreach Normalize (p) Within this framework, any type of feature can be used, enabling the system designer to experiment with interesting feature types, rather than worry about specific feature interactions. In contrast, in a rule based system, the system designer would have to consider how, for instance, a WordNet (Miller, 1995) derived information for a particular example interacts with a part-of-speech-based information and chunking information. That is not to say, ultimately, that rule-based systems are in some way inferior to statistical models – they are built using valuable insight which is hard to obtain from a statistical-modelonly approach. Instead, we are just suggesting that the output of such a system can be easily integrated into the previously described framework, as one of the input features, most likely leading to improved performance. 2.2 The Combination Hypothesis In addition to using rich lexical, </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. WordNet: A lexical database. Communications of the ACM, 38(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL’02,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2193" citStr="Ng and Cardie, 2002" startWordPosition="328" endWordPosition="331">populating a knowledge database. This task has applications in information extraction and summarization, information retrieval (one can get all hits for Washington/person and not the ones for Washington/state or Washington/city), data mining and question answering. The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev et al., 1999; Miller et al., 1998; Aberdeen et al., 1995; Ng and Cardie, 2002; Soon et al., 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computational linguistic literature, a named entity represents an instance of a name, either a location, a person, an organization, and the NER task consists of identifying each individual occurrence of such an entity. We will instead adopt the nomenclature of the Automatic Content Extraction program&apos; (NIST, 2003a): we will call the instances of textual references to objects or abstractions mentions, which can be either named (e.g. John Mayor), nominal (e.g. t</context>
<context position="23835" citStr="Ng and Cardie, 2002" startWordPosition="3863" endWordPosition="3866">noun mention is from a named mention when the distance feature is used in conjunction with mention information feature. As it is the case with with mention detection approach presented in Section 2, most features used here are language-independent and are instantiated from the training data, while some are language-specific, but mostly because the resources were not available for the specific language. For example, syntactic features are not used in the Arabic system due to the lack of an Arabic parser. Simple as it seems, the mention-pair model has been shown to work well (Soon et al., 2001; Ng and Cardie, 2002). As will be shown in Section 4, the relatively knowledge-lean feature sets work fairly well in our tasks. Although we also use a mention-pair model, our tracking algorithm differs from Soon et al. (2001), Ng and Cardie (2002) in several aspects. First, the mention-pair model is used as an approximation to the entity-mention model (3), which itself is an approximation of . Second, instead of doing a pick-first (Soon et al., 2001) or best-first (Ng and Cardie, 2002) selection, the mention-pair linking model is used to compute a starting probability. The starting probability enables us to score </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the ACL’02, pages 104–111.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2003a</author>
</authors>
<title>The ACE evaluation plan.</title>
<note>www.nist.gov/speech/tests/ace/index.htm.</note>
<contexts>
<context position="14875" citStr="(2003)" startWordPosition="2379" endWordPosition="2379">on, the models use a history of 2 tags. foreach Feature Type Ar Zh En Token in window of 5 Morph in window of 5 N/A POS info Text chunking info — — Capitalization/word-type N/A N/A Prefixes/suffixes N/A Gazetteer info Gap — — Wordnet info — — Segmentation N/A Additional systems’ output Table 1: Summary of features used by the 3 systems Given these observations, we decided to “condition” the output of the system on the segmented data: the text is first segmented into tokens, and the classification is then performed on tokens. The segmentation model is similar to the one presented by Lee et al. (2003), and obtains an accuracy of about 98%. In addition, special attention is paid to prefixes and suffixes: in order to reduce the number of spurious tokens we re-merge the prefixes or suffixes to their corresponding stem if they are not essential to the classification process. For this purpose, we collect the following statistics for each prefix/suffix from the ACE training data: the frequency of occurring as a mention by itself ( ) and the frequency of occurring as a part of mention ( ). If the ratio is below a threshold (estimated on the development data), is re-merged with its corresponding s</context>
<context position="16891" citStr="(2003)" startWordPosition="2709" endWordPosition="2709">n In Chinese text, unlike in Indo-European languages, words neither are white-space delimited nor do they have capitalization markers. Instead of a word-based model, we build a character-based one, since word segmentation 6For some additional data, annotated with 32 named categories, mentioned later on, we use the same approach of collecting the and statistics, but, since named mentions are predominant and there are no pronominal mentions in that case, most suffixes and some prefixes are merged back to their original stem. errors can lead to irrecoverable mention detection errors; Jing et al. (2003) also observe that character-based models are better performing than word-based ones for Chinese named entity recognition. Although the model is character-based, segmentation information is still useful and is integrated as an additional feature stream. Some more information about additional resources used in building the system: Gazetteers include dictionaries of 10k person names, 8k location and country names, and 3k organization names, compiled from annotated corpora. There are four additional classifiers whose output is used as features: a RRM model which outputs 32 named categories, a RRM</context>
</contexts>
<marker>2003a, </marker>
<rawString>NIST. 2003a. The ACE evaluation plan. www.nist.gov/speech/tests/ace/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>2003b</author>
</authors>
<date></date>
<booktitle>Proceedings of ACE’03. Booklet,</booktitle>
<location>Alexandria, VA,</location>
<contexts>
<context position="14875" citStr="(2003)" startWordPosition="2379" endWordPosition="2379">on, the models use a history of 2 tags. foreach Feature Type Ar Zh En Token in window of 5 Morph in window of 5 N/A POS info Text chunking info — — Capitalization/word-type N/A N/A Prefixes/suffixes N/A Gazetteer info Gap — — Wordnet info — — Segmentation N/A Additional systems’ output Table 1: Summary of features used by the 3 systems Given these observations, we decided to “condition” the output of the system on the segmented data: the text is first segmented into tokens, and the classification is then performed on tokens. The segmentation model is similar to the one presented by Lee et al. (2003), and obtains an accuracy of about 98%. In addition, special attention is paid to prefixes and suffixes: in order to reduce the number of spurious tokens we re-merge the prefixes or suffixes to their corresponding stem if they are not essential to the classification process. For this purpose, we collect the following statistics for each prefix/suffix from the ACE training data: the frequency of occurring as a mention by itself ( ) and the frequency of occurring as a part of mention ( ). If the ratio is below a threshold (estimated on the development data), is re-merged with its corresponding s</context>
<context position="16891" citStr="(2003)" startWordPosition="2709" endWordPosition="2709">n In Chinese text, unlike in Indo-European languages, words neither are white-space delimited nor do they have capitalization markers. Instead of a word-based model, we build a character-based one, since word segmentation 6For some additional data, annotated with 32 named categories, mentioned later on, we use the same approach of collecting the and statistics, but, since named mentions are predominant and there are no pronominal mentions in that case, most suffixes and some prefixes are merged back to their original stem. errors can lead to irrecoverable mention detection errors; Jing et al. (2003) also observe that character-based models are better performing than word-based ones for Chinese named entity recognition. Although the model is character-based, segmentation information is still useful and is integrated as an additional feature stream. Some more information about additional resources used in building the system: Gazetteers include dictionaries of 10k person names, 8k location and country names, and 3k organization names, compiled from annotated corpora. There are four additional classifiers whose output is used as features: a RRM model which outputs 32 named categories, a RRM</context>
</contexts>
<marker>2003b, </marker>
<rawString>NIST. 2003b. Proceedings of ACE’03. Booklet, Alexandria, VA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="6332" citStr="Ramshaw and Marcus, 1994" startWordPosition="990" endWordPosition="993">describes the algorithms and feature types used for mention detection. Section 3 presents our approach to entity tracking. Section 4 describes the experimental framework and the systems’ results for Arabic, Chinese and English on the data from the latest ACE evaluation (September 2003), an investigation of the effect of using different feature types, as well as a discussion of the results. 2 Mention Detection The mention detection system identifies the named, nominal and pronominal mentions introduced in the previous section. Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. 2.1 The Statistical Classifiers Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al.</context>
</contexts>
<marker>Ramshaw, Marcus, 1994</marker>
<rawString>L. Ramshaw and M. Marcus. 1994. Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging. In Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of WVLC’95,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="6374" citStr="Ramshaw and Marcus, 1995" startWordPosition="996" endWordPosition="999"> used for mention detection. Section 3 presents our approach to entity tracking. Section 4 describes the experimental framework and the systems’ results for Arabic, Chinese and English on the data from the latest ACE evaluation (September 2003), an investigation of the effect of using different feature types, as well as a discussion of the results. 2 Mention Detection The mention detection system identifies the named, nominal and pronominal mentions introduced in the previous section. Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. 2.1 The Statistical Classifiers Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al., 2003). Given t</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of WVLC’95, pages 82–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="2213" citStr="Soon et al., 2001" startWordPosition="332" endWordPosition="335">e database. This task has applications in information extraction and summarization, information retrieval (one can get all hits for Washington/person and not the ones for Washington/state or Washington/city), data mining and question answering. The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al., 1997; Borthwick et al., 1998; Mikheev et al., 1999; Miller et al., 1998; Aberdeen et al., 1995; Ng and Cardie, 2002; Soon et al., 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computational linguistic literature, a named entity represents an instance of a name, either a location, a person, an organization, and the NER task consists of identifying each individual occurrence of such an entity. We will instead adopt the nomenclature of the Automatic Content Extraction program&apos; (NIST, 2003a): we will call the instances of textual references to objects or abstractions mentions, which can be either named (e.g. John Mayor), nominal (e.g. the president) or pro</context>
<context position="23813" citStr="Soon et al., 2001" startWordPosition="3859" endWordPosition="3862">pture how far a pronoun mention is from a named mention when the distance feature is used in conjunction with mention information feature. As it is the case with with mention detection approach presented in Section 2, most features used here are language-independent and are instantiated from the training data, while some are language-specific, but mostly because the resources were not available for the specific language. For example, syntactic features are not used in the Arabic system due to the lack of an Arabic parser. Simple as it seems, the mention-pair model has been shown to work well (Soon et al., 2001; Ng and Cardie, 2002). As will be shown in Section 4, the relatively knowledge-lean feature sets work fairly well in our tasks. Although we also use a mention-pair model, our tracking algorithm differs from Soon et al. (2001), Ng and Cardie (2002) in several aspects. First, the mention-pair model is used as an approximation to the entity-mention model (3), which itself is an approximation of . Second, instead of doing a pick-first (Soon et al., 2001) or best-first (Ng and Cardie, 2002) selection, the mention-pair linking model is used to compute a starting probability. The starting probabilit</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL’99.</booktitle>
<contexts>
<context position="25885" citStr="Sang and Veenstra, 1999" startWordPosition="4201" endWordPosition="4204">ment and evaluation test data for the 3 languages. The data is annotated with five types of entities: person, organization, geo-political entity, location, facility; each mention can be either named, nominal or pronominal, and can be either generic (not referring to a clearly described entity) or specific. The models for all three languages are built as joint models, simultaneously predicting the type, level and genericity of a mention – basically each mention is labeled with a 3-pronged tag. To transform the problem into a classification task, we use the IOB2 classification scheme (Tjong Kim Sang and Veenstra, 1999). 4.1 The ACE Value A gauge of the performance of an EDT system is the ACE value, a measure developed especially for this purpose. It estimates the normalized weighted cost of detection of specific-only entities in terms of misses, false alarms and substitution errors (entities marked generic are excluded from computation): any undetected entity is considered a miss, system-output entities with no corresponding reference entities are considered false alarms, and entities whose type was mis-assigned are substitution errors. The ACE value computes a weighted cost by applying different weights to</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E. F. Tjong Kim Sang and J. Veenstra. 1999. Representing text chunks. In Proceedings of EACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL2002 shared task: Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>155--158</pages>
<contexts>
<context position="6425" citStr="Sang, 2002" startWordPosition="1006" endWordPosition="1007">ntity tracking. Section 4 describes the experimental framework and the systems’ results for Arabic, Chinese and English on the data from the latest ACE evaluation (September 2003), an investigation of the effect of using different feature types, as well as a discussion of the results. 2 Mention Detection The mention detection system identifies the named, nominal and pronominal mentions introduced in the previous section. Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. 2.1 The Statistical Classifiers Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al., 2003). Given the stated focus of integrating many feature types, </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E. F. Tjong Kim Sang. 2002. Introduction to the CoNLL2002 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2002, pages 155–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Williams</author>
<author>J Peng</author>
</authors>
<title>An efficient gradient–based algorithm for on–line training of recurrent neural networks trajectories.</title>
<date>1990</date>
<journal>Neural Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="8423" citStr="Williams and Peng, 1990" startWordPosition="1329" endWordPosition="1332">cts linear classifiers (one for each predicted class), each predicting whether the current example belongs to the class or not. Every such classifier has an associated feature weight vector, , which is learned during the training phase so as to minimize the classification error rate3. At test time, for each example , the model computes a score and labels the example with either the class corresponding to the classifier with the highest score, if above 0, or outside, otherwise. The full decoding algorithm is presented in Algorithm 1. This algorithm can also be used for sequence classification (Williams and Peng, 1990), by converting the activation scores into probabilities (through the soft-max function, for instance) and using the standard dynamic programing search algorithm (also known as Viterbi search). Algorithm 1 The RRM Decoding Algorithm Somewhat similarly, the MaxEnt algorithm has an associated set of weights , which are estimated during the training phase so as to maximize the likelihood of the data (Berger et al., 1996). Given these weights, the model computes the probability distribution of a particular example as follows: where is a normalization factor. After computing the class probability d</context>
</contexts>
<marker>Williams, Peng, 1990</marker>
<rawString>R. J. Williams and J. Peng. 1990. An efficient gradient–based algorithm for on–line training of recurrent neural networks trajectories. Neural Computation, 2(4):490–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F Damerau</author>
<author>D E Johnson</author>
</authors>
<title>Text chunking based on a generalization of Winnow.</title>
<date>2002</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>2--615</pages>
<contexts>
<context position="4491" citStr="Zhang et al., 2002" startWordPosition="697" endWordPosition="700"> they participate. The EDT task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominals and pronouns) and the requirement of grouping mentions into entities. We present and evaluate empirically statistical models for both mention detection and entity tracking problems. For mention detection we use approaches based on Maximum Entropy (MaxEnt henceforth) (Berger et al., 1996) and Robust Risk Minimization (RRM henceforth) &apos;For a description of the ACE program see http://www.nist.gov/speech/tests/ace/. (Zhang et al., 2002). The task is transformed into a sequence classification problem. We investigate a wide array of lexical, syntactic and semantic features to perform the mention detection and classification task including, for all three languages, features based on pre-existing statistical semantic taggers, even though these taggers have been trained on different corpora and use different semantic categories. Moreover, the presented approach implicitly learns the correlation between these different semantic types and the desired output types. We propose a novel MaxEnt-based model for predicting whether a menti</context>
<context position="6919" citStr="Zhang et al., 2002" startWordPosition="1082" endWordPosition="1085">king (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. 2.1 The Statistical Classifiers Good performance in many natural language processing tasks, such as part-of-speech tagging, shallow parsing and named entity recognition, has been shown to depend heavily on integrating many sources of information (Zhang et al., 2002; Jing et al., 2003; Ittycheriah et al., 2003). Given the stated focus of integrating many feature types, we are interested in algorithms that can easily integrate and make effective use of diverse input types. We selected two methods which satisfy these criteria: a linear classifier – the Robust Risk Minimization classifier – and a log-linear classifier – the Maximum Entropy classifier. Both methods can integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classification. Before formally describing the methods2, we int</context>
<context position="9571" citStr="Zhang et al. (2002)" startWordPosition="1513" endWordPosition="1516">here is a normalization factor. After computing the class probability distribution, the assigned class is the most probable one a posteriori. The sketch of applying MaxEnt to the test data is presented in Algorithm 2. Similarly to the RRM model, we use the model to perform sequence classification, through dynamic programing. 2This is not meant to be an in-depth introduction to the methods, but a brief overview to familiarize the reader with them. 3Actually, the optimizing function contains a regularization factor which considerably improves the robustness of the system — for full details, see Zhang et al. (2002). foreach foreach Algorithm 2 The MaxEnt Decoding Algorithm foreach Normalize (p) Within this framework, any type of feature can be used, enabling the system designer to experiment with interesting feature types, rather than worry about specific feature interactions. In contrast, in a rule based system, the system designer would have to consider how, for instance, a WordNet (Miller, 1995) derived information for a particular example interacts with a part-of-speech-based information and chunking information. That is not to say, ultimately, that rule-based systems are in some way inferior to sta</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>T. Zhang, F. Damerau, and D. E. Johnson. 2002. Text chunking based on a generalization of Winnow. Journal ofMachine Learning Research, 2:615–637.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>