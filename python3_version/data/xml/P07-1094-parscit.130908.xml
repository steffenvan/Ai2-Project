<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003468">
<title confidence="0.998265">
A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging*
</title>
<author confidence="0.997281">
Sharon Goldwater Thomas L. Griffiths
</author>
<affiliation confidence="0.989875">
Department of Linguistics Department of Psychology
Stanford University UC Berkeley
</affiliation>
<email confidence="0.997442">
sgwater@stanford.edu tom griffiths@berkeley.edu
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846291666667">
Unsupervised learning of linguistic structure
is a difficult problem. A common approach
is to define a generative model and max-
imize the probability of the hidden struc-
ture given the observed data. Typically,
this is done using maximum-likelihood es-
timation (MLE) of the model parameters.
We show using part-of-speech tagging that
a fully Bayesian approach can greatly im-
prove performance. Rather than estimating
a single set of parameters, the Bayesian ap-
proach integrates over all possible parame-
ter values. This difference ensures that the
learned structure will have high probability
over a range of possible parameters, and per-
mits the use of priors favoring the sparse
distributions that are typical of natural lan-
guage. Our model has the structure of a
standard trigram HMM, yet its accuracy is
closer to that of a state-of-the-art discrimi-
native model (Smith and Eisner, 2005), up
to 14 percentage points better than MLE. We
find improvements both when training from
data alone, and using a tagging dictionary.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.977736">
Unsupervised learning of linguistic structure is a dif-
ficult problem. Recently, several new model-based
approaches have improved performance on a vari-
ety of tasks (Klein and Manning, 2002; Smith and
</bodyText>
<footnote confidence="0.757045333333333">
∗This work was supported by grants NSF 0631518 and
ONR MURI N000140510388. We would also like to thank
Noah Smith for providing us with his data sets.
</footnote>
<page confidence="0.987585">
744
</page>
<bodyText confidence="0.999912676470588">
Eisner, 2005). Nearly all of these approaches have
one aspect in common: the goal of learning is to
identify the set of model parameters that maximizes
some objective function. Values for the hidden vari-
ables in the model are then chosen based on the
learned parameterization. Here, we propose a dif-
ferent approach based on Bayesian statistical prin-
ciples: rather than searching for an optimal set of
parameter values, we seek to directly maximize the
probability of the hidden variables given the ob-
served data, integrating over all possible parame-
ter values. Using part-of-speech (POS) tagging as
an example application, we show that the Bayesian
approach provides large performance improvements
over maximum-likelihood estimation (MLE) for the
same model structure. Two factors can explain the
improvement. First, integrating over parameter val-
ues leads to greater robustness in the choice of tag
sequence, since it must have high probability over
a range of parameters. Second, integration permits
the use of priors favoring sparse distributions, which
are typical of natural language. These kinds of pri-
ors can lead to degenerate solutions if the parameters
are estimated directly.
Before describing our approach in more detail,
we briefly review previous work on unsupervised
POS tagging. Perhaps the most well-known is that
of Merialdo (1994), who used MLE to train a tri-
gram hidden Markov model (HMM). More recent
work has shown that improvements can be made
by modifying the basic HMM structure (Banko and
Moore, 2004), using better smoothing techniques or
added constraints (Wang and Schuurmans, 2005), or
using a discriminative model rather than an HMM
</bodyText>
<note confidence="0.9052535">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998154">
(Smith and Eisner, 2005). Non-model-based ap-
proaches have also been proposed (Brill (1995); see
also discussion in Banko and Moore (2004)). All of
this work is really POS disambiguation: learning is
strongly constrained by a dictionary listing the al-
lowable tags for each word in the text. Smith and
Eisner (2005) also present results using a diluted
dictionary, where infrequent words may have any
tag. Haghighi and Klein (2006) use a small list of
labeled prototypes and no dictionary.
A different tradition treats the identification of
syntactic classes as a knowledge-free clustering
problem. Distributional clustering and dimen-
sionality reduction techniques are typically applied
when linguistically meaningful classes are desired
(Sch¨utze, 1995; Clark, 2000; Finch et al., 1995);
probabilistic models have been used to find classes
that can improve smoothing and reduce perplexity
(Brown et al., 1992; Saul and Pereira, 1997). Unfor-
tunately, due to a lack of standard and informative
evaluation techniques, it is difficult to compare the
effectiveness of different clustering methods.
In this paper, we hope to unify the problems of
POS disambiguation and syntactic clustering by pre-
senting results for conditions ranging from a full tag
dictionary to no dictionary at all. We introduce the
use of a new information-theoretic criterion, varia-
tion of information (Meilˇa, 2002), which can be used
to compare a gold standard clustering to the clus-
tering induced from a tagger’s output, regardless of
the cluster labels. We also evaluate using tag ac-
curacy when possible. Our system outperforms an
HMM trained with MLE on both metrics in all cir-
cumstances tested, often by a wide margin. Its ac-
curacy in some cases is close to that of Smith and
Eisner’s (2005) discriminative model. Our results
show that the Bayesian approach is particularly use-
ful when learning is less constrained, either because
less evidence is available (corpus size is small) or
because the dictionary contains less information.
In the following section, we discuss the motiva-
tion for a Bayesian approach and present our model
and search procedure. Section 3 gives results illus-
trating how the parameters of the prior affect re-
sults, and Section 4 describes how to infer a good
choice of parameters from unlabeled data. Section 5
presents results for a range of corpus sizes and dic-
tionary information, and Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.924442" genericHeader="method">
2 A Bayesian HMM
</sectionHeader>
<subsectionHeader confidence="0.978477">
2.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999661">
In model-based approaches to unsupervised lan-
guage learning, the problem is formulated in terms
of identifying latent structure from data. We de-
fine a model with parameters 0, some observed vari-
ables w (the linguistic input), and some latent vari-
ables t (the hidden structure). The goal is to as-
sign appropriate values to the latent variables. Stan-
dard approaches do so by selecting values for the
model parameters, and then choosing the most prob-
able variable assignment based on those parame-
ters. For example, maximum-likelihood estimation
(MLE) seeks parameters 0� such that
</bodyText>
<equation confidence="0.9919835">
0 = argmax P(w|0), (1)
0
</equation>
<bodyText confidence="0.995339">
where P(w|0) = &amp; P(w, t|0). Sometimes, a
non-uniform prior distribution over 0 is introduced,
in which case 0� is the maximum a posteriori (MAP)
solution for 0:
</bodyText>
<equation confidence="0.98379">
P(w|0)P(0). (2)
</equation>
<bodyText confidence="0.998789375">
The values of the latent variables are then taken to
be those that maximize P(t|w, �0).
In contrast, the Bayesian approach we advocate in
this paper seeks to identify a distribution over latent
variables directly, without ever fixing particular val-
ues for the model parameters. The distribution over
latent variables given the observed data is obtained
by integrating over all possible values of 0:
</bodyText>
<equation confidence="0.999316">
JP(t|w) = P(t|w,0)P(0|w)d0. (3)
</equation>
<bodyText confidence="0.99995075">
This distribution can be used in various ways, in-
cluding choosing the MAP assignment to the latent
variables, or estimating expected values for them.
To see why integrating over possible parameter
values can be useful when inducing latent structure,
consider the following example. We are given a
coin, which may be biased (t = 1) or fair (t = 0),
each with probability .5. Let 0 be the probability of
heads. If the coin is biased, we assume a uniform
distribution over 0, otherwise 0 = .5. We observe
w, the outcomes of 10 coin flips, and we wish to de-
termine whether the coin is biased (i.e. the value of
</bodyText>
<equation confidence="0.9128985">
0 = argmax
0
</equation>
<page confidence="0.982574">
745
</page>
<bodyText confidence="0.999287071428571">
t). Assume that we have a uniform prior on B, with
p(B) = 1 for all B ∈ [0, 1]. First, we apply the stan-
dard methodology of finding the MAP estimate for
B and then selecting the value of t that maximizes
P(t|w, B). In this case, an elementary calculation
shows that the MAP estimate is B = nH/10, where
nH is the number of heads in w (likewise, nT is
the number of tails). Consequently, P(t|w, �B) favors
t = 1 for any sequence that does not contain exactly
five heads, and assigns equal probability tot = 1
and t = 0 for any sequence that does contain exactly
five heads — a counterintuitive result. In contrast,
using some standard results in Bayesian analysis we
can show that applying Equation 3 yields
</bodyText>
<listItem confidence="0.952518142857143">
(4)
which is significantly less than .5 when nH = 5, and
only favors t = 1 for sequences where nH
8 or
nH
2. This intuitively sensible prediction results
from the fact that the Bayesian
</listItem>
<equation confidence="0.8179622">
� 1 + 11!
P(t = 1|w) = 1/
nHInT!210
≥
≤
</equation>
<bodyText confidence="0.998109652173913">
approach is sensitive
to the robustness of a choice of t to the value of B,
as illustrated in Figure 1. Even though a sequence
(Figure 1 (a)), P(t =
B) is only greater than
0.5 for a small range of B around B (Figure 1 (b)),
meaning that the choice oft = 1 is not very robust to
variation in B. In contrast, a sequence with nH = 8
favors t = 1 for a wide range of B around B. By
integrating over B, Equation 3 takes into account the
consequences of possible variation in B.
Another advantage of integrating over B is that
it permits the use of linguistically appropriate pri-
ors. In many linguistic models, including HMMs,
the distributions over variables are multinomial. For
a multinomial with parameters B =
... , BK), a
natural choice of prior is the K-dimensional Dirich-
let distribution, which is conjugate to the
For simplicity, we initially assume that all
K parameters (also known as hyperparameters) of
the Dirichlet distribution are equal to Q, i.e. the
Diri
</bodyText>
<equation confidence="0.95629975">
1|w,
(B1,
multino-
mial.1
</equation>
<bodyText confidence="0.932613333333333">
chlet is symmetric. The value of Q determines
which parameters B will have high probability: when
Q = 1, all parameter values are equally likely; when
Q &gt; 1, multinomials that are closer to uniform are
prior is conjugate to a distribution if the posterior has the
same form as the pri
</bodyText>
<equation confidence="0.899745">
1A
or.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
θ
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
θ
that t = 1 given w an
</equation>
<bodyText confidence="0.981637818181818">
d B as a function of B.
mation. For a sequence of draws x =
... , xn)
from a multinomial distribution B with observed
counts
... , nK, a symmetric
prior
over B yields the MAP estimate Bk =
When Q
1, standard MLE techniques such as
EM can be used to find the MAP estimate simply
by adding
of size Q
1 to each of
the expected counts nk at each iteration. However,
when Q &lt; 1, the values of B that set one or more
of the Bk equal to 0 can have infinitely high poste-
rior probability, meaning that MAP estimation can
yield degenerate solutions. If, instead of estimating
B, we integrate over all possible values, we no longer
encounter such difficulties. Instead, the probability
that outcome xi
</bodyText>
<equation confidence="0.959375">
(x1,
n1,
Dirichlet(Q)
n��β−1
n�K(β−1�.
≥
“pseudocounts”
−
takes value k given previous out-
comes x−i = (x1, ... , xi−1) is
P(k|x−i, Q) P(k|B)P(B|x−i, Q) dB
with nH = 6 yields a MAP estimate of B = 0.6
</equation>
<figureCaption confidence="0.993611">
Figure 1: The Bayesian approach to estimating the
</figureCaption>
<bodyText confidence="0.940149923076923">
value of a latent variable, t, from observed data, w,
chooses a value of t robust to uncertainty in B. (a)
Posterior distribution on B given w. (b) Probability
preferred; and when Q &lt; 1, high probability is as-
signed to sparse multinomials, where one or more
parameters are at or near 0.
Typically, linguistic structures are characterized
by sparse distributions (e.g., POS tags are followed
with high probability by only a few other tags, and
have highly skewed output distributions). Conse-
quently, it makes sense to use a Dirichlet prior with
Q &lt; 1. However, as noted by Johnson et al. (2007),
this choice of Q leads to difficulties with MAP esti-
</bodyText>
<equation confidence="0.900744266666667">
f
nk +
Q (5)
i − 1 + KQ
w = HHTHTTHHTH
w = HHTHHHTHHH
(b)
P( t = 1  |W, θ )
0.5
0
1
w = HHTHTTHHTH
w = HHTHHHTHHH
(e)
P( θ  |W )
</equation>
<page confidence="0.970131">
746
</page>
<bodyText confidence="0.996769413043478">
where nk is the number of times k occurred in x−i. 2.3 Inference
See MacKay and Peto (1995) for a derivation. To perform inference in our model, we use Gibbs
2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic
Our model has the structure of a standard trigram procedure that produces samples from the posterior
HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)P(t|α). We
ors over the transition and output distributions: initialize the tags at random, then iteratively resam-
ti|ti−1 = t,ti−2 = t′, τ(t,t′) — Mult(τ(t,t′)) ple each tag according to its conditional distribution
wi|ti = t, ω(t) — Mult(ω(t)) given the current values of all other tags. Exchange-
τ(t,t′)|α — Dirichlet(α) ability allows us to treat the current counts of the
ω(t)|β — Dirichlet(β) other tag trigrams and outputs as “previous” obser-
where ti and wi are the ith tag and word. We assume vations. The only complication is that resampling
that sentence boundaries are marked with a distin- a tag changes the identity of three trigrams at once,
guished tag. For a model with T possible tags, each and we must account for this in computing its condi-
of the transition distributions τ(t,t′) has T compo- tional distribution. The sampling distribution for ti
nents, and each of the output distributions ω(t) has is given in Figure 2.
Wt components, where Wt is the number of word In Bayesian statistical inference, multiple samples
types that are permissible outputs for tag t. We will from the posterior are often used in order to obtain
use τ and ω to refer to the entire transition and out- statistics such as the expected values of model vari-
put parameter sets. This model assumes that the ables. For POS tagging, estimates based on multi-
prior over state transitions is the same for all his- ple samples might be useful if we were interested in,
tories, and the prior over output distributions is the for example, the probability that two words have the
same for all states. We relax the latter assumption in same tag. However, computing such probabilities
Section 4. across all pairs of words does not necessarily lead to
Under this model, Equation 5 gives us a consistent clustering, and the result would be diffi-
n(ti−2,ti−1,ti) + α cult to evaluate. Using a single sample makes stan-
P(ti|t−i, α) = (6) dard evaluation methods possible, but yields sub-
n(ti−2,ti−1) + Tα optimal results because the value for each tag is sam-
n(ti,wi) + β pled from a distribution, and some tags will be as-
P(wi|ti, t−i,w−i,β) = (7) signed low-probability values. Our solution is to
n(ti) + Wtiβ treat the Gibbs sampler as a stochastic search pro-
where n(ti−2,ti−1,ti) and n(ti,wi) are the number of cedure with the goal of identifying the MAP tag se-
occurrences of the trigram (ti−2,ti−1,ti) and the quence. This can be done using tempering (anneal-
tag-word pair (ti, wi) in the i — 1 previously gener- ing), where a temperature of φ is equivalent to rais-
ated tags and words. Note that, by integrating out ing the probabilities in the sampling distribution to
the parameters τ and ω, we induce dependencies the power of 1 φ. As φ approaches 0, even a single
between the variables in the model. The probabil- sample will provide a good MAP estimate.
ity of generating a particular trigram tag sequence 3 Fixed Hyperparameter Experiments
(likewise, output) depends on the number of times 3.1 Method
that sequence (output) has been generated previ- Our initial experiments follow in the tradition begun
ously. Importantly, trigrams (and outputs) remain by Merialdo (1994), using a tag dictionary to con-
exchangeable: the probability of a set of trigrams strain the possible parts of speech allowed for each
(outputs) is the same regardless of the order in which word. (This also fixes Wt, the number of possible
it was generated. The property of exchangeability is words for tag t.) The dictionary was constructed by
crucial to the inference algorithm we describe next. listing, for each word, all tags found for that word in
747 the entire WSJ treebank. For the experiments in this
section, we used a 24,000-word subset of the tree-
</bodyText>
<equation confidence="0.9998785">
n(ti−1,ti,ti+1) + I(ti−2 = ti−1 = ti = ti+1) + α
n(ti−1,ti) + I(ti−2 = ti−1 = ti) + Tα
n(ti,ti+1,ti+2) + I(ti−2 = ti = ti+2, ti−1 = ti+1) + I(ti−1 = ti = ti+1 = ti+2) + α
·n(ti,ti+1) + I(ti−2 = ti, ti−1 = ti+1) + I(ti−1 = ti = ti+1) + Tα
</equation>
<figureCaption confidence="0.967789">
Figure 2: Conditional distribution for ti. Here, t−i refers to the current values of all tags except for ti, I(.)
is a function that takes on the value 1 when its argument is true and 0 otherwise, and all counts n,, are with
respect to the tag trigrams and tag-word pairs in (t−i, w−i).
</figureCaption>
<equation confidence="0.991239">
P(tijt−i,w,α,Q) a Q
n(ti,wi) + n(ti−2,ti−1,ti) + α ·
nti + WtiQn(ti−2,ti−1) + Tα
</equation>
<bodyText confidence="0.997625973684211">
bank as our unlabeled training corpus. 54.5% of the
tokens in this corpus have at least two possible tags,
with the average number of tags per token being 2.3.
We varied the values of the hyperparameters α and
Q and evaluated overall tagging accuracy. For com-
parison with our Bayesian HMM (BHMM) in this
and following sections, we also present results from
the Viterbi decoding of an HMM trained using MLE
by running EM to convergence (MLHMM). Where
direct comparison is possible, we list the scores re-
ported by Smith and Eisner (2005) for their condi-
tional random field model trained using contrastive
estimation (CRF/CE).2
For all experiments, we ran our Gibbs sampling
algorithm for 20,000 iterations over the entire data
set. The algorithm was initialized with a random tag
assignment and a temperature of 2, and the temper-
ature was gradually decreased to .08. Since our in-
ference procedure is stochastic, our reported results
are an average over 5 independent runs.
Results from our model for a range of hyperpa-
rameters are presented in Table 1. With the best
choice of hyperparameters (α = .003, Q = 1), we
achieve average tagging accuracy of 86.8%. This
far surpasses the MLHMM performance of 74.5%,
and is closer to the 90.1% accuracy of CRF/CE on
the same data set using oracle parameter selection.
The effects of α, which determines the probabil-
2Results of CRF/CE depend on the set of features used and
the contrast neighborhood. In all cases, we list the best score
reported for any contrast neighborhood using trigram (but no
spelling) features. To ensure proper comparison, all corpora
used in our experiments consist of the same randomized sets of
sentences used by Smith and Eisner. Note that training on sets
of contiguous sentences from the beginning of the treebank con-
sistently improves our results, often by 1-2 percentage points or
more. MLHMM scores show less difference between random-
ized and contiguous corpora.
</bodyText>
<table confidence="0.999495777777778">
Value Value of 3
of α .001 .003 .01 .03 .1 .3 1.0
.001 85.0 85.7 86.1 86.0 86.2 86.5 86.6
.003 85.5 85.5 85.8 86.6 86.7 86.7 86.8
.01 85.3 85.5 85.6 85.9 86.4 86.4 86.2
.03 85.9 85.8 86.1 86.2 86.6 86.8 86.4
.1 85.2 85.0 85.2 85.1 84.9 85.5 84.9
.3 84.4 84.4 84.6 84.4 84.5 85.7 85.3
1.0 83.1 83.0 83.2 83.3 83.5 83.7 83.9
</table>
<tableCaption confidence="0.997749">
Table 1: Percentage of words tagged correctly by
</tableCaption>
<bodyText confidence="0.998621653846154">
BHMM as a function of the hyperparameters α and
Q. Results are averaged over 5 runs on the 24k cor-
pus with full tag dictionary. Standard deviations in
most cases are less than .5.
ity of the transition distributions, are stronger than
the effects of Q, which determines the probability
of the output distributions. The optimal value of
.003 for α reflects the fact that the true transition
probability matrix for this corpus is indeed sparse.
As α grows larger, the model prefers more uniform
transition probabilities, which causes it to perform
worse. Although the true output distributions tend to
be sparse as well, the level of sparseness depends on
the tag (consider function words vs. content words
in particular). Therefore, a value of Q that accu-
rately reflects the most probable output distributions
for some tags may be a poor choice for other tags.
This leads to the smaller effect of Q, and suggests
that performance might be improved by selecting a
different Q for each tag, as we do in the next section.
A final point worth noting is that even when
α = Q = 1 (i.e., the Dirichlet priors exert no influ-
ence) the BHMM still performs much better than the
MLHMM. This result underscores the importance
of integrating over model parameters: the BHMM
identifies a sequence of tags that have high proba-
</bodyText>
<page confidence="0.994845">
748
</page>
<bodyText confidence="0.9999544">
bility over a range of parameter values, rather than
choosing tags based on the single best set of para-
meters. The improved results of the BHMM demon-
strate that selecting a sequence that is robust to vari-
ations in the parameters leads to better performance.
</bodyText>
<sectionHeader confidence="0.986994" genericHeader="method">
4 Hyperparameter Inference
</sectionHeader>
<bodyText confidence="0.999984892857143">
In our initial experiments, we experimented with dif-
ferent fixed values of the hyperparameters and re-
ported results based on their optimal values. How-
ever, choosing hyperparameters in this way is time-
consuming at best and impossible at worst, if there
is no gold standard available. Luckily, the Bayesian
approach allows us to automatically select values
for the hyperparameters by treating them as addi-
tional variables in the model. We augment the model
with priors over the hyperparameters (here, we as-
sume an improper uniform prior), and use a sin-
gle Metropolis-Hastings update (Gilks et al., 1996)
to resample the value of each hyperparameter after
each iteration of the Gibbs sampler. Informally, to
update the value of hyperparameter α, we sample a
proposed new value α′ from a normal distribution
with p = α and a = .1α. The probability of ac-
cepting the new value depends on the ratio between
P(t|w, α) and P(t|w, α′) and a term correcting for
the asymmetric proposal distribution.
Performing inference on the hyperparameters al-
lows us to relax the assumption that every tag has
the same prior on its output distribution. In the ex-
periments reported in the following section, we used
two different versions of our model. The first ver-
sion (BHMM1) uses a single value of Q for all word
classes (as above); the second version (BHMM2)
uses a separate Qj for each tag class j.
</bodyText>
<sectionHeader confidence="0.989289" genericHeader="method">
5 Inferred Hyperparameter Experiments
</sectionHeader>
<subsectionHeader confidence="0.998061">
5.1 Varying corpus size
</subsectionHeader>
<bodyText confidence="0.999723777777778">
In this set of experiments, we used the full tag dictio-
nary (as above), but performed inference on the hy-
perparameters. Following Smith and Eisner (2005),
we trained on four different corpora, consisting of
the first 12k, 24k, 48k, and 96k words of the WSJ
corpus. For all corpora, the percentage of ambigu-
ous tokens is 54%-55% and the average number of
tags per token is 2.3. Table 2 shows results for
the various models and a random baseline (averaged
</bodyText>
<table confidence="0.99968175">
Accuracy 12k Corpus size 96k
24k 48k
random 64.8 64.6 64.6 64.6
MLHMM 71.3 74.5 76.7 78.3
CRF/CE 86.2 88.6 88.4 89.4
BHMM1 85.8 85.2 83.6 85.0
BHMM2 85.8 84.4 85.7 85.8
a&lt; .7 .2 .6 .2
</table>
<tableCaption confidence="0.993422">
Table 2: Percentage of words tagged correctly
</tableCaption>
<bodyText confidence="0.972257529411765">
by the various models on different sized corpora.
BHMM1 and BHMM2 use hyperparameter infer-
ence; CRF/CE uses parameter selection based on an
unlabeled development set. Standard deviations (a)
for the BHMM results fell below those shown for
each corpus size.
over 5 random tag assignments). Hyperparameter
inference leads to slightly lower scores than are ob-
tained by oracle hyperparameter selection, but both
versions of BHMM are still far superior to MLHMM
for all corpus sizes. Not surprisingly, the advantages
of BHMM are most pronounced on the smallest cor-
pus: the effects of parameter integration and sensible
priors are stronger when less evidence is available
from the input. In the limit as corpus size goes to in-
finity, the BHMM and MLHMM will make identical
predictions.
</bodyText>
<subsectionHeader confidence="0.999815">
5.2 Varying dictionary knowledge
</subsectionHeader>
<bodyText confidence="0.999980647058824">
In unsupervised learning, it is not always reasonable
to assume that a large tag dictionary is available. To
determine the effects of reduced or absent dictionary
information, we ran a set of experiments inspired
by those of Smith and Eisner (2005). First, we col-
lapsed the set of 45 treebank tags onto a smaller set
of 17 (the same set used by Smith and Eisner). We
created a full tag dictionary for this set of tags from
the entire treebank, and also created several reduced
dictionaries. Each reduced dictionary contains the
tag information only for words that appear at least
d times in the training corpus (the 24k corpus, for
these experiments). All other words are fully am-
biguous between all 17 classes. We ran tests with
d = 1, 2, 3, 5, 10, and oc (i.e., knowledge-free syn-
tactic clustering).
With standard accuracy measures, it is difficult to
</bodyText>
<page confidence="0.995386">
749
</page>
<table confidence="0.999946529411765">
Value of d
Accuracy 1 2 3 5 10 cc
random 69.6 56.7 51.0 45.2 38.6
MLHMM 83.2 70.6 65.5 59.0 50.9
CRF/CE 90.4 77.0 71.7
BHMM1 86.0 76.4 71.0 64.3 58.0
BHMM2 87.3 79.6 65.0 59.2 49.7
v &lt; .2 .8 .6 .3 1.4
VI
random 2.65 3.96 4.38 4.75 5.13 7.29
MLHMM 1.13 2.51 3.00 3.41 3.89 6.50
BHMM1 1.09 2.44 2.82 3.19 3.47 4.30
BHMM2 1.04 1.78 2.31 2.49 2.97 4.04
v &lt; .02 .03 .04 .03 .07 .17
Corpus stats
% ambig. 49.0 61.3 66.3 70.9 75.8 100
tags/token 1.9 4.4 5.5 6.8 8.3 17
</table>
<tableCaption confidence="0.997499">
Table 3: Percentage of words tagged correctly and
</tableCaption>
<bodyText confidence="0.9953449125">
variation of information between clusterings in-
duced by the assigned and gold standard tags as the
amount of information in the dictionary is varied.
Standard deviations (Q) for the BHMM results fell
below those shown in each column. The percentage
of ambiguous tokens and average number of tags per
token for each value of d is also shown.
evaluate the quality of a syntactic clustering when
no dictionary is used, since cluster names are inter-
changeable. We therefore introduce another evalua-
tion measure for these experiments, a distance met-
ric on clusterings known as variation of information
(Meilˇa, 2002). The variation of information (VI) be-
tween two clusterings C (the gold standard) and C′
(the found clustering) of a set of data points is a sum
of the amount of information lost in moving from C
to C′, and the amount that must be gained. It is de-
fined in terms of entropy H and mutual information
I: V I(C, C′) = H(C) + H(C′) − 2I(C, C′). Even
when accuracy can be measured, VI may be more in-
formative: two different tag assignments may have
the same accuracy but different VI with respect to
the gold standard if the errors in one assignment are
less consistent than those in the other.
Table 3 gives the results for this set of experi-
ments. One or both versions of BHMM outperform
MLHMM in terms of tag accuracy for all values of
d, although the differences are not as great as in ear-
lier experiments. The differences in VI are more
striking, particularly as the amount of dictionary in-
formation is reduced. When ambiguity is greater,
both versions of BHMM show less confusion with
respect to the true tags than does MLHMM, and
BHMM2 performs the best in all circumstances. The
confusion matrices in Figure 3 provide a more intu-
itive picture of the very different sorts of clusterings
produced by MLHMM and BHMM2 when no tag
dictionary is available. Similar differences hold to a
lesser degree when a partial dictionary is provided.
With MLHMM, different tokens of the same word
type are usually assigned to the same cluster, but
types are assigned to clusters more or less at ran-
dom, and all clusters have approximately the same
number of types (542 on average, with a standard
deviation of 174). The clusters found by BHMM2
tend to be more coherent and more variable in size:
in the 5 runs of BHMM2, the average number of
types per cluster ranged from 436 to 465 (i.e., to-
kens of the same word are spread over fewer clus-
ters than in MLHMM), with a standard deviation
between 460 and 674. Determiners, prepositions,
the possessive marker, and various kinds of punc-
tuation are mostly clustered coherently. Nouns are
spread over a few clusters, partly due to a distinction
found between common and proper nouns. Like-
wise, modal verbs and the copula are mostly sep-
arated from other verbs. Errors are often sensible:
adjectives and nouns are frequently confused, as are
verbs and adverbs.
The kinds of results produced by BHMM1 and
BHMM2 are more similar to each other than to
the results of MLHMM, but the differences are still
informative. Recall that BHMM1 learns a single
value for Q that is used for all output distribu-
tions, while BHMM2 learns separate hyperparame-
ters for each cluster. This leads to different treat-
ments of difficult-to-classify low-frequency items.
In BHMM1, these items tend to be spread evenly
among all clusters, so that all clusters have simi-
larly sparse output distributions. In BHMM2, the
system creates one or two clusters consisting en-
tirely of very infrequent items, where the priors on
these clusters strongly prefer uniform outputs, and
all other clusters prefer extremely sparse outputs
(and are more coherent than in BHMM1). This
explains the difference in VI between the two sys-
tems, as well as the higher accuracy of BHMM1
for d &gt; 3: the single Q discourages placing low-
frequency items in their own cluster, so they are
more likely to be clustered with items that have sim-
</bodyText>
<page confidence="0.987876">
750
</page>
<table confidence="0.96329605">
(a) BHMM2 (b) MLHMM
True Tags N True Tags N
INPUNC INPUNC
ADJ ADJ
V V
DET DET
PREP PREP
ENDPUNC ENDPUNC
VBG VBG
CONJ CONJ
VBN VBN
ADV ADV
TO TO
WH WH
PRT PRT
POS POS
LPUNC LPUNC
RPUNC RPUNC
1 2 3 4 5 6 7 8 9 1011121314151617 1 2 3 4 5 6 7 8 9 1011121314151617
Found Tags Found Tags
</table>
<figureCaption confidence="0.997346">
Figure 3: Confusion matrices for the dictionary-free clusterings found by (a) BHMM2 and (b) MLHMM.
</figureCaption>
<bodyText confidence="0.99883225">
ilar transition probabilities. The problem of junk
clusters in BHMM2 might be alleviated by using a
non-uniform prior over the hyperparameters to en-
courage some degree of sparsity in all clusters.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999978875">
In this paper, we have demonstrated that, for a stan-
dard trigram HMM, taking a Bayesian approach
to POS tagging dramatically improves performance
over maximum-likelihood estimation. Integrating
over possible parameter values leads to more robust
solutions and allows the use of priors favoring sparse
distributions. The Bayesian approach is particularly
helpful when learning is less constrained, either be-
cause less data is available or because dictionary
information is limited or absent. For knowledge-
free clustering, our approach can also be extended
through the use of infinite models so that the num-
ber of clusters need not be specified in advance. We
hope that our success with POS tagging will inspire
further research into Bayesian methods for other nat-
ural language learning tasks.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9989913">
M. Banko and R. Moore. 2004. A study of unsupervised part-
of-speech tagging. In Proceedings of COLING ’04.
E. Brill. 1995. Unsupervised learning of disambiguation rules
for part of speech tagging. In Proceedings of the 3rd Work-
shop on Very Large Corpora, pages 1–13.
P. Brown, V. Della Pietra, V. de Souza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural language.
Computational Linguistics, 18:467–479.
A. Clark. 2000. Inducing syntactic categories by context dis-
tribution clustering. In Proceedings of the Conference on
Natural Language Learning (CONLL).
S. Finch, N. Chater, and M. Redington. 1995. Acquiring syn-
tactic information from distributional statistics. In J. In Levy,
D. Bairaktaris, J. Bullinaria, and P. Cairns, editors, Connec-
tionist Models of Memory and Language. UCL Press, Lon-
don.
S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs
distributions and the Bayesian restoration of images. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
6:721–741.
W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors.
1996. Markov Chain Monte Carlo in Practice. Chapman
and Hall, Suffolk.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In Proceedings ofHLT-NAACL.
M. Johnson, T. Griffiths, and S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo.
D. Klein and C. Manning. 2002. A generative constituent-
context model for improved grammar induction. In Proceed-
ings of the ACL.
D. MacKay and L. Bauman Peto. 1995. A hierarchical Dirich-
let language model. Natural Language Engineering, 1:289–
307.
M. Meilˇa. 2002. Comparing clusterings. Technical Report 418,
University of Washington Statistics Department.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155–172.
L. Saul and F. Pereira. 1997. Aggregate and mixed-order
markov models for statistical language processing. In Pro-
ceedings of the Second Conference on Empirical Methods in
Natural Language Processing (EMNLP).
H. Sch¨utze. 1995. Distributional part-of-speech tagging. In
Proceedings of the European Chapter of the Association for
Computational Linguistics (EACL).
N. Smith and J. Eisner. 2005. Contrastive estimation: Training
log-linear models on unlabeled data. In Proceedings ofACL.
I. Wang and D. Schuurmans. 2005. Improved estimation
for unsupervised part-of-speech tagging. In Proceedings
of the IEEE International Conference on Natural Language
Processing and Knowledge Engineering (IEEE NLP-KE).
</reference>
<page confidence="0.998243">
751
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945892">
<title confidence="0.995095">Fully Bayesian Approach to Unsupervised Part-of-Speech</title>
<author confidence="0.99987">Sharon Goldwater Thomas L Griffiths</author>
<affiliation confidence="0.999653">Department of Linguistics Department of Psychology Stanford University UC Berkeley</affiliation>
<email confidence="0.984424">sgwater@stanford.edutomgriffiths@berkeley.edu</email>
<abstract confidence="0.99863048">Unsupervised learning of linguistic structure is a difficult problem. A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data. Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters. We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance. Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values. This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We find improvements both when training from data alone, and using a tagging dictionary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>R Moore</author>
</authors>
<title>A study of unsupervised partof-speech tagging.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING ’04.</booktitle>
<contexts>
<context position="3158" citStr="Banko and Moore, 2004" startWordPosition="487" endWordPosition="490">nce, since it must have high probability over a range of parameters. Second, integration permits the use of priors favoring sparse distributions, which are typical of natural language. These kinds of priors can lead to degenerate solutions if the parameters are estimated directly. Before describing our approach in more detail, we briefly review previous work on unsupervised POS tagging. Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics (Smith and Eisner, 2005). Non-model-based approaches have also been proposed (Brill (1995); see also discussion in Banko and Moore (2004)). All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word</context>
</contexts>
<marker>Banko, Moore, 2004</marker>
<rawString>M. Banko and R. Moore. 2004. A study of unsupervised partof-speech tagging. In Proceedings of COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Unsupervised learning of disambiguation rules for part of speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="3572" citStr="Brill (1995)" startWordPosition="546" endWordPosition="547">t of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics (Smith and Eisner, 2005). Non-model-based approaches have also been proposed (Brill (1995); see also discussion in Banko and Moore (2004)). All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text. Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering and dimensionality reduction techniques are typically applied when </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Unsupervised learning of disambiguation rules for part of speech tagging. In Proceedings of the 3rd Workshop on Very Large Corpora, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V Della Pietra</author>
<author>V de Souza</author>
<author>J Lai</author>
<author>R Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>P. Brown, V. Della Pietra, V. de Souza, J. Lai, and R. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CONLL).</booktitle>
<contexts>
<context position="4246" citStr="Clark, 2000" startWordPosition="646" endWordPosition="647">ork is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text. Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997). Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult to compare the effectiveness of different clustering methods. In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all. We introduce the use of a new information-theoretic criterion, variation of informa</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>A. Clark. 2000. Inducing syntactic categories by context distribution clustering. In Proceedings of the Conference on Natural Language Learning (CONLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Finch</author>
<author>N Chater</author>
<author>M Redington</author>
</authors>
<title>Acquiring syntactic information from distributional statistics. In</title>
<date>1995</date>
<booktitle>Connectionist Models of Memory and Language.</booktitle>
<editor>J. In Levy, D. Bairaktaris, J. Bullinaria, and P. Cairns, editors,</editor>
<publisher>UCL Press,</publisher>
<location>London.</location>
<contexts>
<context position="4267" citStr="Finch et al., 1995" startWordPosition="648" endWordPosition="651"> POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text. Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997). Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult to compare the effectiveness of different clustering methods. In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all. We introduce the use of a new information-theoretic criterion, variation of information (Meilˇa, 2002), </context>
</contexts>
<marker>Finch, Chater, Redington, 1995</marker>
<rawString>S. Finch, N. Chater, and M. Redington. 1995. Acquiring syntactic information from distributional statistics. In J. In Levy, D. Bairaktaris, J. Bullinaria, and P. Cairns, editors, Connectionist Models of Memory and Language. UCL Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="12008" citStr="Geman and Geman, 1984" startWordPosition="2049" endWordPosition="2052">, POS tags are followed with high probability by only a few other tags, and have highly skewed output distributions). Consequently, it makes sense to use a Dirichlet prior with Q &lt; 1. However, as noted by Johnson et al. (2007), this choice of Q leads to difficulties with MAP estif nk + Q (5) i − 1 + KQ w = HHTHTTHHTH w = HHTHHHTHHH (b) P( t = 1 |W, θ ) 0.5 0 1 w = HHTHTTHHTH w = HHTHHHTHHH (e) P( θ |W ) 746 where nk is the number of times k occurred in x−i. 2.3 Inference See MacKay and Peto (1995) for a derivation. To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)P(t|α). We ors over the transition and output distributions: initialize the tags at random, then iteratively resamti|ti−1 = t,ti−2 = t′, τ(t,t′) — Mult(τ(t,t′)) ple each tag according to its conditional distribution wi|ti = t, ω(t) — Mult(ω(t)) given the current values of all other tags. Exchangeτ(t,t′)|α — Dirichlet(α) ability allows us to treat the current counts of the ω(t)|β — Dirichlet(β)</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<date>1996</date>
<booktitle>Markov Chain Monte Carlo in Practice.</booktitle>
<editor>W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors.</editor>
<publisher>Chapman and Hall,</publisher>
<location>Suffolk.</location>
<marker>1996</marker>
<rawString>W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. 1996. Markov Chain Monte Carlo in Practice. Chapman and Hall, Suffolk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="3911" citStr="Haghighi and Klein (2006)" startWordPosition="600" endWordPosition="603">n an HMM Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics (Smith and Eisner, 2005). Non-model-based approaches have also been proposed (Brill (1995); see also discussion in Banko and Moore (2004)). All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text. Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997). Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<contexts>
<context position="11612" citStr="Johnson et al. (2007)" startWordPosition="1962" endWordPosition="1965"> approach to estimating the value of a latent variable, t, from observed data, w, chooses a value of t robust to uncertainty in B. (a) Posterior distribution on B given w. (b) Probability preferred; and when Q &lt; 1, high probability is assigned to sparse multinomials, where one or more parameters are at or near 0. Typically, linguistic structures are characterized by sparse distributions (e.g., POS tags are followed with high probability by only a few other tags, and have highly skewed output distributions). Consequently, it makes sense to use a Dirichlet prior with Q &lt; 1. However, as noted by Johnson et al. (2007), this choice of Q leads to difficulties with MAP estif nk + Q (5) i − 1 + KQ w = HHTHTTHHTH w = HHTHHHTHHH (b) P( t = 1 |W, θ ) 0.5 0 1 w = HHTHTTHHTH w = HHTHHHTHHH (e) P( θ |W ) 746 where nk is the number of times k occurred in x−i. 2.3 Inference See MacKay and Peto (1995) for a derivation. To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. Griffiths, and S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>A generative constituentcontext model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1464" citStr="Klein and Manning, 2002" startWordPosition="216" endWordPosition="219">ge of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We find improvements both when training from data alone, and using a tagging dictionary. 1 Introduction Unsupervised learning of linguistic structure is a difficult problem. Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and ∗This work was supported by grants NSF 0631518 and ONR MURI N000140510388. We would also like to thank Noah Smith for providing us with his data sets. 744 Eisner, 2005). Nearly all of these approaches have one aspect in common: the goal of learning is to identify the set of model parameters that maximizes some objective function. Values for the hidden variables in the model are then chosen based on the learned parameterization. Here, we propose a different approach based on Bayesian statistical principles: rather than searching for an optimal set of parameter values, we seek to dir</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. Manning. 2002. A generative constituentcontext model for improved grammar induction. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D MacKay</author>
<author>L Bauman Peto</author>
</authors>
<title>A hierarchical Dirichlet language model.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<pages>307</pages>
<contexts>
<context position="11888" citStr="MacKay and Peto (1995)" startWordPosition="2029" endWordPosition="2032">ne or more parameters are at or near 0. Typically, linguistic structures are characterized by sparse distributions (e.g., POS tags are followed with high probability by only a few other tags, and have highly skewed output distributions). Consequently, it makes sense to use a Dirichlet prior with Q &lt; 1. However, as noted by Johnson et al. (2007), this choice of Q leads to difficulties with MAP estif nk + Q (5) i − 1 + KQ w = HHTHTTHHTH w = HHTHHHTHHH (b) P( t = 1 |W, θ ) 0.5 0 1 w = HHTHTTHHTH w = HHTHHHTHHH (e) P( θ |W ) 746 where nk is the number of times k occurred in x−i. 2.3 Inference See MacKay and Peto (1995) for a derivation. To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)P(t|α). We ors over the transition and output distributions: initialize the tags at random, then iteratively resamti|ti−1 = t,ti−2 = t′, τ(t,t′) — Mult(τ(t,t′)) ple each tag according to its conditional distribution wi|ti = t, ω(t) — Mult(ω(t)) given the current values of all</context>
</contexts>
<marker>MacKay, Peto, 1995</marker>
<rawString>D. MacKay and L. Bauman Peto. 1995. A hierarchical Dirichlet language model. Natural Language Engineering, 1:289– 307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meilˇa</author>
</authors>
<title>Comparing clusterings.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<tech>Technical Report 418,</tech>
<volume>20</volume>
<issue>2</issue>
<institution>University of Washington Statistics</institution>
<marker>Meilˇa, 2002</marker>
<rawString>M. Meilˇa. 2002. Comparing clusterings. Technical Report 418, University of Washington Statistics Department. B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Saul</author>
<author>F Pereira</author>
</authors>
<title>Aggregate and mixed-order markov models for statistical language processing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4414" citStr="Saul and Pereira, 1997" startWordPosition="671" endWordPosition="674">005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997). Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult to compare the effectiveness of different clustering methods. In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all. We introduce the use of a new information-theoretic criterion, variation of information (Meilˇa, 2002), which can be used to compare a gold standard clustering to the clustering induced from a tagger’s output, regardless of the cluster labels. We also</context>
</contexts>
<marker>Saul, Pereira, 1997</marker>
<rawString>L. Saul and F. Pereira. 1997. Aggregate and mixed-order markov models for statistical language processing. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<marker>Sch¨utze, 1995</marker>
<rawString>H. Sch¨utze. 1995. Distributional part-of-speech tagging. In Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1127" citStr="Smith and Eisner, 2005" startWordPosition="164" endWordPosition="167">E) of the model parameters. We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance. Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values. This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We find improvements both when training from data alone, and using a tagging dictionary. 1 Introduction Unsupervised learning of linguistic structure is a difficult problem. Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and ∗This work was supported by grants NSF 0631518 and ONR MURI N000140510388. We would also like to thank Noah Smith for providing us with his data sets. 744 Eisner, 2005). Nearly all of these approaches have one aspect in common: the goal of learning is</context>
<context position="3506" citStr="Smith and Eisner, 2005" startWordPosition="535" endWordPosition="538">previous work on unsupervised POS tagging. Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics (Smith and Eisner, 2005). Non-model-based approaches have also been proposed (Brill (1995); see also discussion in Banko and Moore (2004)). All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text. Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem. Distributional clustering a</context>
<context position="17067" citStr="Smith and Eisner (2005)" startWordPosition="2931" endWordPosition="2934"> w−i). P(tijt−i,w,α,Q) a Q n(ti,wi) + n(ti−2,ti−1,ti) + α · nti + WtiQn(ti−2,ti−1) + Tα bank as our unlabeled training corpus. 54.5% of the tokens in this corpus have at least two possible tags, with the average number of tags per token being 2.3. We varied the values of the hyperparameters α and Q and evaluated overall tagging accuracy. For comparison with our Bayesian HMM (BHMM) in this and following sections, we also present results from the Viterbi decoding of an HMM trained using MLE by running EM to convergence (MLHMM). Where direct comparison is possible, we list the scores reported by Smith and Eisner (2005) for their conditional random field model trained using contrastive estimation (CRF/CE).2 For all experiments, we ran our Gibbs sampling algorithm for 20,000 iterations over the entire data set. The algorithm was initialized with a random tag assignment and a temperature of 2, and the temperature was gradually decreased to .08. Since our inference procedure is stochastic, our reported results are an average over 5 independent runs. Results from our model for a range of hyperparameters are presented in Table 1. With the best choice of hyperparameters (α = .003, Q = 1), we achieve average taggin</context>
<context position="22033" citStr="Smith and Eisner (2005)" startWordPosition="3783" endWordPosition="3786">al distribution. Performing inference on the hyperparameters allows us to relax the assumption that every tag has the same prior on its output distribution. In the experiments reported in the following section, we used two different versions of our model. The first version (BHMM1) uses a single value of Q for all word classes (as above); the second version (BHMM2) uses a separate Qj for each tag class j. 5 Inferred Hyperparameter Experiments 5.1 Varying corpus size In this set of experiments, we used the full tag dictionary (as above), but performed inference on the hyperparameters. Following Smith and Eisner (2005), we trained on four different corpora, consisting of the first 12k, 24k, 48k, and 96k words of the WSJ corpus. For all corpora, the percentage of ambiguous tokens is 54%-55% and the average number of tags per token is 2.3. Table 2 shows results for the various models and a random baseline (averaged Accuracy 12k Corpus size 96k 24k 48k random 64.8 64.6 64.6 64.6 MLHMM 71.3 74.5 76.7 78.3 CRF/CE 86.2 88.6 88.4 89.4 BHMM1 85.8 85.2 83.6 85.0 BHMM2 85.8 84.4 85.7 85.8 a&lt; .7 .2 .6 .2 Table 2: Percentage of words tagged correctly by the various models on different sized corpora. BHMM1 and BHMM2 use</context>
<context position="23625" citStr="Smith and Eisner (2005)" startWordPosition="4051" endWordPosition="4054"> superior to MLHMM for all corpus sizes. Not surprisingly, the advantages of BHMM are most pronounced on the smallest corpus: the effects of parameter integration and sensible priors are stronger when less evidence is available from the input. In the limit as corpus size goes to infinity, the BHMM and MLHMM will make identical predictions. 5.2 Varying dictionary knowledge In unsupervised learning, it is not always reasonable to assume that a large tag dictionary is available. To determine the effects of reduced or absent dictionary information, we ran a set of experiments inspired by those of Smith and Eisner (2005). First, we collapsed the set of 45 treebank tags onto a smaller set of 17 (the same set used by Smith and Eisner). We created a full tag dictionary for this set of tags from the entire treebank, and also created several reduced dictionaries. Each reduced dictionary contains the tag information only for words that appear at least d times in the training corpus (the 24k corpus, for these experiments). All other words are fully ambiguous between all 17 classes. We ran tests with d = 1, 2, 3, 5, 10, and oc (i.e., knowledge-free syntactic clustering). With standard accuracy measures, it is difficu</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Wang</author>
<author>D Schuurmans</author>
</authors>
<title>Improved estimation for unsupervised part-of-speech tagging.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering (IEEE NLP-KE).</booktitle>
<contexts>
<context position="3242" citStr="Wang and Schuurmans, 2005" startWordPosition="498" endWordPosition="501">egration permits the use of priors favoring sparse distributions, which are typical of natural language. These kinds of priors can lead to degenerate solutions if the parameters are estimated directly. Before describing our approach in more detail, we briefly review previous work on unsupervised POS tagging. Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics (Smith and Eisner, 2005). Non-model-based approaches have also been proposed (Brill (1995); see also discussion in Banko and Moore (2004)). All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text. Smith and Eisner (2005) also present results using a diluted dictionar</context>
</contexts>
<marker>Wang, Schuurmans, 2005</marker>
<rawString>I. Wang and D. Schuurmans. 2005. Improved estimation for unsupervised part-of-speech tagging. In Proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering (IEEE NLP-KE).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>