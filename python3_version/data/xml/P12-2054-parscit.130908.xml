<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002169">
<title confidence="0.905703">
Efficient Tree-Based Topic Modeling
</title>
<author confidence="0.998899">
Yuening Hu
</author>
<affiliation confidence="0.999443">
Department of Computer Science
University of Maryland, College Park
</affiliation>
<email confidence="0.993578">
ynhu@cs.umd.edu
</email>
<sectionHeader confidence="0.997345" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999105941176471">
Topic modeling with a tree-based prior has
been used for a variety of applications be-
cause it can encode correlations between words
that traditional topic modeling cannot. How-
ever, its expressive power comes at the cost
of more complicated inference. We extend
the SPARSELDA (Yao et al., 2009) inference
scheme for latent Dirichlet allocation (LDA)
to tree-based topic models. This sampling
scheme computes the exact conditional distri-
bution for Gibbs sampling much more quickly
than enumerating all possible latent variable
assignments. We further improve performance
by iteratively refining the sampling distribution
only when needed. Experiments show that the
proposed techniques dramatically improve the
computation time.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99979975">
Topic models, exemplified by latent Dirichlet alloca-
tion (LDA) (Blei et al., 2003), discover latent themes
present in text collections. “Topics” discovered by
topic models are multinomial probability distribu-
tions over words that evince thematic coherence.
Topic models are used in computational biology, com-
puter vision, music, and, of course, text analysis.
One of LDA’s virtues is that it is a simple model
that assumes a symmetric Dirichlet prior over its
word distributions. Recent work argues for structured
distributions that constrain clusters (Andrzejewski et
al., 2009), span languages (Jagarlamudi and Daum´e
III, 2010), or incorporate human feedback (Hu et al.,
2011) to improve the quality and flexibility of topic
modeling. These models all use different tree-based
prior distributions (Section 2).
These approaches are appealing because they
preserve conjugacy, making inference using Gibbs
sampling (Heinrich, 2004) straightforward. While
straightforward, inference isn’t cheap. Particularly
</bodyText>
<author confidence="0.451605">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.8534815">
iSchool and UMIACS
University of Maryland, College Park
</affiliation>
<email confidence="0.956011">
jbg@umiacs.umd.edu
</email>
<bodyText confidence="0.9999204">
for interactive settings (Hu et al., 2011), efficient
inference would improve perceived latency.
SPARSELDA (Yao et al., 2009) is an efficient
Gibbs sampling algorithm for LDA based on a refac-
torization of the conditional topic distribution (re-
viewed in Section 3). However, it is not directly
applicable to tree-based priors. In Section 4, we pro-
vide a factorization for tree-based models within a
broadly applicable inference framework that empiri-
cally improves the efficiency of inference (Section 5).
</bodyText>
<sectionHeader confidence="0.988544" genericHeader="method">
2 Topic Modeling with Tree-Based Priors
</sectionHeader>
<bodyText confidence="0.999945615384615">
Trees are intuitive methods for encoding human
knowledge. Abney and Light (1999) used tree-
structured multinomials to model selectional restric-
tions, which was later put into a Bayesian context
for topic modeling (Boyd-Graber et al., 2007). In
both cases, the tree came from WordNet (Miller,
1990), but the tree could also come from domain
experts (Andrzejewski et al., 2009).
Organizing words in this way induces correlations
that are mathematically impossible to represent with
a symmetric Dirichlet prior. To see how correlations
can occur, consider the generative process. Start with
a rooted tree structure that contains internal nodes
and leaf nodes. This skeleton is a prior that generates
K topics. Like vanilla LDA, these topics are distribu-
tions over words. Unlike vanilla LDA, their structure
correlates words. Internal nodes have a distribution
irk,i over children, where irk,i comes from per-node
Dirichlet parameterized by 0i.1 Each leaf node is
associated with a word, and each word must appear
in at least (possibly more than) one leaf node.
To generate a word from topic k, start at the root.
Select a child x0 — Mult(irk,ROOT), and traverse
the tree until reaching a leaf node. Then emit the
leaf’s associated word. This walk replaces the draw
from a topic’s multinomial distribution over words.
</bodyText>
<footnote confidence="0.778583">
1Choosing these Dirichlet priors specifies the direction (i.e.,
positive or negative) and strength of correlations that appear.
</footnote>
<page confidence="0.947817">
275
</page>
<note confidence="0.6927745">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 275–279,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936833333333">
The rest of the generative process for LDA remains
the same, with 0, the per-document topic multinomial,
and z, the topic assignment.
This tree structure encodes correlations. The closer
types are in the tree, the more correlated they are.
Because types can appear in multiple leaf nodes, this
encodes polysemy. The path that generates a token is
an additional latent variable we must sample.
Gibbs sampling is straightforward because the tree-
based prior maintains conjugacy (Andrzejewski et
al., 2009). We integrate the per-document topic dis-
tributions 0 and the transition distributions 7r. The
remaining latent variables are the topic assignment z
and path l, which we sample jointly:2
where nk|d is topic k’s count in the document d;
αk is topic k’s prior; Z_ and L_ are topic and path
assignments excluding wd,n; ,3i,j is the prior for
edge i → j, ni,j|t is the count of edge i → j in
topic k; and j&apos; denotes other children of node i.
The complexity of computing the sampling distri-
bution is O(KLS) for models with K topics, paths
at most L nodes long, and at most S paths per word
type. In contrast, for vanilla LDA the analogous
conditional sampling distribution requires O(K).
</bodyText>
<sectionHeader confidence="0.994459" genericHeader="method">
3 Efficient LDA
</sectionHeader>
<bodyText confidence="0.999962875">
The SPARSELDA (Yao et al., 2009) scheme for
speeding inference begins by rearranging LDA’s sam-
pling equation into three terms:3
Following their lead, we call these three terms
“buckets”. A bucket is the total probability mass
marginalizing over latent variable assignments (i.e.,
sLDA ≡ Ek pv+a.�k, similarly for the other buck-
ets). The three buckets are a smoothing only bucket
</bodyText>
<footnote confidence="0.973683666666667">
2For clarity, we omit indicators that ensure A ends at wd,n.
3To ease notation we drop the d,n subscript for z and w in
this and future equations.
</footnote>
<bodyText confidence="0.999931875">
sLDA, document topic bucket rLDA, and topic word
bucket qLDA (we use the “LDA” subscript to contrast
with our method, for which we use the same bucket
names without subscripts).
Caching the buckets’ total mass speeds the compu-
tation of the sampling distribution. Bucket sLDA is
shared by all tokens, and bucket rLDA is shared by a
document’s tokens. Both have simple constant time
updates. Bucket qLDA has to be computed specifi-
cally for each token, but only for the (typically) few
types with non-zero counts in a topic.
To sample from the conditional distribution, first
sample which bucket you need and then (and only
then) select a topic within that bucket. Because the
topic-term bucket qLDA often has the largest mass
and has few non-zero terms, this speeds inference.
</bodyText>
<sectionHeader confidence="0.97869" genericHeader="method">
4 Efficient Inference in Tree-Based Models
</sectionHeader>
<bodyText confidence="0.999888666666667">
In this section, we extend the sampling techniques
for SPARSELDA to tree-based topic modeling. We
first factor Equation 1:
</bodyText>
<equation confidence="0.997062">
p(z = k,l = A|Z−, L−, w) (3)
∝ (αk + nk|d)N−�
k,λ[Sλ + Ok,λ].
</equation>
<bodyText confidence="0.999900333333333">
Henceforth we call Nk,a the normalizer for path A
in topic k, Sa the smoothing factor for path A, and
Ok,a the observation for path A in topic k, which are
</bodyText>
<equation confidence="0.970811833333333">
11 Nk,λ = E (,3i→j, + ni→j,|k)
(i→j)∈λ j0
11 Sλ = ,3i→j (4)
(i→j)∈λ
11 Ok,λ = (,3i→j + ni→j|k) − 11 ,3i→j.
(i→j)∈λ (i→j)∈λ
</equation>
<bodyText confidence="0.999490333333333">
Equation 3 can be rearranged in the same way
as Equation 5, yielding buckets analogous to
SPARSELDA’s,
</bodyText>
<equation confidence="0.811843222222222">
p(z = k,l = A|Z−, L−, w) (5)
∝ αkSλ + nk|dSλ (αk + nk|d)Ok,λ
Nk,λ +
� �� � Nk,λ
r � �� �
q
Nk,λ .
� �� �
s
</equation>
<bodyText confidence="0.998373">
Buckets sum both topics and paths. The sampling
process is much the same as for SPARSELDA: select
which bucket and then select a topic / path combina-
tion within the bucket (for a slightly more complex
example, see Algorithm 1).
</bodyText>
<equation confidence="0.999207714285714">
p(z = k,l = A|Z−, L−, w) (1)
11 ∝ (αk + nk|d) ,3i→j + ni→j|k
(i→j)∈λ
E
j0 (,3i→j, + ni→j,|k)
p(z = k|Z−,w) ∝ (αk + nk|d) ,3 + nw|k (2)
,3V + n·|k
αk,3
,3V + n·|k
� �� �
sLDA
nk|d,3
+
,3V + n·|k
� �� �
rLDA
∝
� �� �
qLDA
+ ,3V + n·|k
(αk + nk|d)nw|k
</equation>
<page confidence="0.988483">
276
</page>
<bodyText confidence="0.998131051282051">
Recall that one of the benefits of SPARSELDA was
that s was shared across tokens. This is no longer
possible, as Nk�a is distinct for each path in tree-
based LDA. Moreover, Nk�a is coupled; changing
ni,i|k in one path changes the normalizers of all
cousin paths (paths that share some node i).
This negates the benefit of caching s, but we re-
cover some of the benefits by splitting the normalizer
to two parts: the “root” normalizer from the root node
(shared by all paths) and the “downstream” normal-
izer. We precompute which paths share downstream
normalizers; all paths are partitioned into cousin sets,
defined as sets for which changing the count of one
member of the set changes the downstream normal-
izer of other paths in the set. Thus, when updating
the counts for path l, we only recompute Nk�l&apos; for all
l&apos; in the cousin set.
SPARSELDA’s computation of q, the topic-word
bucket, benefits from topics with unobserved (i.e.,
zero count) types. In our case, any non-zero path, a
path with any non-zero edge, contributes.4 To quickly
determine whether a path contributes, we introduce
an edge-masked count (EMC) for each path. Higher
order bits encode whether edges have been observed
and lower order bits encode the number of times the
path has been observed. For example, if a path of
length three only has its first two edges observed, its
EMC is 11000000. If the same path were observed
seven times, its EMC is 11100111. With this formu-
lation we can ignore any paths with a zero EMC.
Efficient sampling with refined bucket While
caching the sampling equation as described in the
previous section improved the efficiency, the smooth-
ing only bucket s is small, but computing the asso-
ciated mass is costly because it requires us to con-
sider all topics and paths. This is not a problem
for SparseLDA because s is shared across all tokens.
However, we can achieve computational gains with
an upper bound on s,
</bodyText>
<equation confidence="0.968499">
Q
αk (i→j)∈λ 13i→j
Q(i→j)∈λ Pj0 (13i→j&apos; + ni→j&apos; |k)
77ak Q(i→j)∈λ 13i→j
11(i→j)∈λ Pj0 /3i→j&apos;
</equation>
<bodyText confidence="0.908733857142857">
A sampling algorithm can take advantage of this
by not explicitly calculating s. Instead, we use s&apos;
4C.f. observed paths, where all edges are non-zero.
as proxy, and only compute the exact s if we hit the
bucket s&apos; (Algorithm 1). Removing s&apos; and always
computing s yields the first algorithm in Section 4.
Algorithm 1 SAMPLING WITH REFINED BUCKET
</bodyText>
<listItem confidence="0.983745785714286">
1: for word w in this document do
2: sample = rand() *(s&apos; + r + q)
3: if sample &lt; s&apos; then
4: compute s
5: sample = sample *(s + r + q)/(s&apos; + r + q)
6: if sample &lt; s then
7: return topic k and path A sampled from s
8: sample − = s
9: else
10: sample − = s&apos;
11: if sample &lt; r then
12: return topic k and path A sampled from r
13: sample − = r
14: return topic k and path A sampled from q
</listItem>
<bodyText confidence="0.999581380952381">
Sorting Thus far, we described techniques for ef-
ficiently computing buckets, but quickly sampling
assignments within a bucket is also important. Here
we propose two techniques to consider latent vari-
able assignments in decreasing order of probability
mass. By considering fewer possible assignments,
we can speed sampling at the cost of the overhead
of maintaining sorted data structures. We sort top-
ics’ prominence within a document (SD) and sort the
topics and paths of a word (SW).
Sorting topics’ prominence within a document
(SD) can improve sampling from r and q; when we
need to sample within a bucket, we consider paths in
decreasing order of nk|d.
Sorting path prominence for a word (SW) can im-
prove our ability to sample from q. The edge-masked
count (EMC), as described above, serves as a proxy
for the probability of a path and topic. If, when sam-
pling a topic and path from q, we sample based on
the decreasing EMC, which roughly correlates with
path probability.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.898017333333333">
In this section, we compare the running time5 of our
sampling algorithm (FAST) and our algorithm with
the refined bucket (RB) against the unfactored Gibbs
sampler (NA¨IVE) and examine the effect of sorting.
Our corpus has editorials from New York Times
5Mean of five chains on a 6-Core 2.8-GHz CPU,16GB RAM
</bodyText>
<equation confidence="0.9924672">
Xs =
k,λ
� X
k,λ
= s0. (6)
</equation>
<page confidence="0.994495">
277
</page>
<table confidence="0.99989175">
Number of Topics
T50 T100 T200 T500
NAIVE 5.700 12.655 29.200 71.223
FAST 4.935 9.222 17.559 40.691
FAST-RB 2.937 4.037 5.880 8.551
FAST-RB-SD 2.675 3.795 5.400 8.363
FAST-RB-SW 2.449 3.363 4.894 7.404
FAST-RB-SDW 2.225 3.241 4.672 7.424
Vocabulary Size
V5000 V10000 V20000 V30000
NA¨IVE 4.815 12.351 28.783 51.088
FAST 2.897 9.063 20.460 38.119
FAST-RB 1.012 3.900 9.777 20.040
FAST-RB-SD 0.972 3.684 9.287 18.685
FAST-RB-SW 0.889 3.376 8.406 16.640
FAST-RB-SDW 0.828 3.113 7.777 15.397
Number of Correlations
C50 C100 C200 C500
NA¨IVE 11.166 12.586 13.000 15.377
FAST 8.889 9.165 9.177 8.079
FAST-RB 3.995 4.078 3.858 3.156
FAST-RB-SD 3.660 3.795 3.593 3.065
FAST-RB-SW 3.272 3.363 3.308 2.787
FAST-RB-SDW 3.026 3.241 3.091 2.627
</table>
<tableCaption confidence="0.8945996">
Table 1: The average running time per iteration (S) over
100 iterations, averaged over 5 seeds. Experiments begin
with 100 topics, 100 correlations, vocab size 10000 and
then vary one dimension: number of topics (top), vocabu-
lary size (middle), and number of correlations (bottom).
</tableCaption>
<bodyText confidence="0.9999246">
from 1987 to 1996.6 Since we are interested in vary-
ing vocabulary size, we rank types by average tf-idf
and choose the top V . WordNet 3.0 generates the cor-
relations between types. For each synset in WordNet,
we generate a subtree with all types in the synset—
that are also in our vocabulary—as leaves connected
to a common parent. This subtree’s common parent
is then attached to the root node.
We compared the FAST and FAST-RB against
NA¨IVE (Table 1) on different numbers of topics, var-
ious vocabulary sizes and different numbers of cor-
relations. FAST is consistently faster than NA¨IVE
and FAST-RB is consistently faster than FAST. Their
benefits are clearer as distributions become sparse
(e.g., the first iteration for FAST is slower than later
iterations). Gains accumulate as the topic number
increases, but decrease a little with the vocabulary
size. While both sorting strategies reduce time, sort-
ing topics and paths for a word (SW) helps more than
sorting topics in a document (SD), and combining the
</bodyText>
<page confidence="0.639665">
613284 documents, 41554 types, and 2714634 tokens.
</page>
<figure confidence="0.626144">
Average number of senses per constraint word
</figure>
<figureCaption confidence="0.887299666666667">
Figure 1: The average running time per iteration against
the average number of senses per correlated words.
two is (with one exception) better than either alone.
</figureCaption>
<bodyText confidence="0.997930941176471">
As more correlations are added, NA¨IVE’s time in-
creases while that of FAST-RB decreases. This is be-
cause the number of non-zero paths for uncorrelated
words decreases as more correlations are added to the
model. Since our techniques save computation for
every zero path, the overall computation decreases
as correlations push uncorrelated words to a limited
number of topics (Figure 1). Qualitatively, when the
synset with “king” and “baron” is added to a model,
it is associated with “drug, inmate, colombia, water-
front, baron” in a topic; when “king” is correlated
with “queen”, the associated topic has “king, parade,
museum, queen, jackson” as its most probable words.
These represent reasonable disambiguations. In con-
trast to previous approaches, inference speeds up as
topics become more semantically coherent (Boyd-
Graber et al., 2007).
</bodyText>
<sectionHeader confidence="0.991114" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998531">
We demonstrated efficient inference techniques for
topic models with tree-based priors. These methods
scale well, allowing for faster exploration of models
that use semantics to encode correlations without sac-
rificing accuracy. Improved scalability for such algo-
rithms, especially in distributed environments (Smola
and Narayanamurthy, 2010), could improve applica-
tions such as cross-language information retrieval,
unsupervised word sense disambiguation, and knowl-
edge discovery via interactive topic modeling.
</bodyText>
<figure confidence="0.994047733333333">
Average running time per iteration (S)
16
14
12
10
4
21 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4
8
6
Naive
Fast
Fast−RB
Fast−RB−sD
Fast−RB−sW
Fast−RB−sDW
</figure>
<page confidence="0.989662">
278
</page>
<sectionHeader confidence="0.999495" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999965142857143">
We would like to thank David Mimno and the anony-
mous reviewers for their helpful comments. This
work was supported by the Army Research Labora-
tory through ARL Cooperative Agreement W911NF-
09-2-0072. Any opinions or conclusions expressed
are the authors’ and do not necessarily reflect those
of the sponsors.
</bodyText>
<sectionHeader confidence="0.999372" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999444171428572">
Steven Abney and Marc Light. 1999. Hiding a seman-
tic hierarchy in a Markov model. In Proceedings of
the Workshop on Unsupervised Learning in Natural
Language Processing.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In Proceedings of
International Conference of Machine Learning.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine Learn-
ing Research, 3:993–1022.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Gregor Heinrich. 2004. Parameter estima-
tion for text analysis. Technical report.
http://www.arbylon.net/publications/text-est.pdf.
Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.
2011. Interactive topic modeling. In Association for
Computational Linguistics.
Jagadeesh Jagarlamudi and Hal Daum´e III. 2010. Ex-
tracting multilingual topics from unaligned corpora. In
Proceedings of the European Conference on Informa-
tion Retrieval (ECIR).
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245–264.
Alexander J. Smola and Shravan Narayanamurthy. 2010.
An architecture for parallel topic models. International
Conference on Very Large Databases, 3.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In Knowledge Discovery and
Data Mining.
</reference>
<page confidence="0.998555">
279
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.710269">
<title confidence="0.999873">Efficient Tree-Based Topic Modeling</title>
<author confidence="0.714423">Yuening</author>
<affiliation confidence="0.9996445">Department of Computer University of Maryland, College</affiliation>
<email confidence="0.999804">ynhu@cs.umd.edu</email>
<abstract confidence="0.999698888888889">Topic modeling with a tree-based prior has been used for a variety of applications because it can encode correlations between words that traditional topic modeling cannot. However, its expressive power comes at the cost of more complicated inference. We extend (Yao et al., 2009) inference scheme for latent Dirichlet allocation (LDA) to tree-based topic models. This sampling scheme computes the exact conditional distribution for Gibbs sampling much more quickly than enumerating all possible latent variable assignments. We further improve performance by iteratively refining the sampling distribution only when needed. Experiments show that the proposed techniques dramatically improve the computation time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Marc Light</author>
</authors>
<title>Hiding a semantic hierarchy in a Markov model.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Unsupervised Learning in Natural Language Processing.</booktitle>
<contexts>
<context position="2607" citStr="Abney and Light (1999)" startWordPosition="368" endWordPosition="371">r interactive settings (Hu et al., 2011), efficient inference would improve perceived latency. SPARSELDA (Yao et al., 2009) is an efficient Gibbs sampling algorithm for LDA based on a refactorization of the conditional topic distribution (reviewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we provide a factorization for tree-based models within a broadly applicable inference framework that empirically improves the efficiency of inference (Section 5). 2 Topic Modeling with Tree-Based Priors Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used treestructured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al., 2007). In both cases, the tree came from WordNet (Miller, 1990), but the tree could also come from domain experts (Andrzejewski et al., 2009). Organizing words in this way induces correlations that are mathematically impossible to represent with a symmetric Dirichlet prior. To see how correlations can occur, consider the generative process. Start with a rooted tree structure that contains internal nodes and leaf nodes. This skeleton is a prior</context>
</contexts>
<marker>Abney, Light, 1999</marker>
<rawString>Steven Abney and Marc Light. 1999. Hiding a semantic hierarchy in a Markov model. In Proceedings of the Workshop on Unsupervised Learning in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via Dirichlet forest priors.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="1460" citStr="Andrzejewski et al., 2009" startWordPosition="206" endWordPosition="209">y improve the computation time. 1 Introduction Topic models, exemplified by latent Dirichlet allocation (LDA) (Blei et al., 2003), discover latent themes present in text collections. “Topics” discovered by topic models are multinomial probability distributions over words that evince thematic coherence. Topic models are used in computational biology, computer vision, music, and, of course, text analysis. One of LDA’s virtues is that it is a simple model that assumes a symmetric Dirichlet prior over its word distributions. Recent work argues for structured distributions that constrain clusters (Andrzejewski et al., 2009), span languages (Jagarlamudi and Daum´e III, 2010), or incorporate human feedback (Hu et al., 2011) to improve the quality and flexibility of topic modeling. These models all use different tree-based prior distributions (Section 2). These approaches are appealing because they preserve conjugacy, making inference using Gibbs sampling (Heinrich, 2004) straightforward. While straightforward, inference isn’t cheap. Particularly Jordan Boyd-Graber iSchool and UMIACS University of Maryland, College Park jbg@umiacs.umd.edu for interactive settings (Hu et al., 2011), efficient inference would improve</context>
<context position="2901" citStr="Andrzejewski et al., 2009" startWordPosition="415" endWordPosition="418"> applicable to tree-based priors. In Section 4, we provide a factorization for tree-based models within a broadly applicable inference framework that empirically improves the efficiency of inference (Section 5). 2 Topic Modeling with Tree-Based Priors Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used treestructured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al., 2007). In both cases, the tree came from WordNet (Miller, 1990), but the tree could also come from domain experts (Andrzejewski et al., 2009). Organizing words in this way induces correlations that are mathematically impossible to represent with a symmetric Dirichlet prior. To see how correlations can occur, consider the generative process. Start with a rooted tree structure that contains internal nodes and leaf nodes. This skeleton is a prior that generates K topics. Like vanilla LDA, these topics are distributions over words. Unlike vanilla LDA, their structure correlates words. Internal nodes have a distribution irk,i over children, where irk,i comes from per-node Dirichlet parameterized by 0i.1 Each leaf node is associated with</context>
<context position="4668" citStr="Andrzejewski et al., 2009" startWordPosition="688" endWordPosition="691">ional Linguistics, pages 275–279, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics The rest of the generative process for LDA remains the same, with 0, the per-document topic multinomial, and z, the topic assignment. This tree structure encodes correlations. The closer types are in the tree, the more correlated they are. Because types can appear in multiple leaf nodes, this encodes polysemy. The path that generates a token is an additional latent variable we must sample. Gibbs sampling is straightforward because the treebased prior maintains conjugacy (Andrzejewski et al., 2009). We integrate the per-document topic distributions 0 and the transition distributions 7r. The remaining latent variables are the topic assignment z and path l, which we sample jointly:2 where nk|d is topic k’s count in the document d; αk is topic k’s prior; Z_ and L_ are topic and path assignments excluding wd,n; ,3i,j is the prior for edge i → j, ni,j|t is the count of edge i → j in topic k; and j&apos; denotes other children of node i. The complexity of computing the sampling distribution is O(KLS) for models with K topics, paths at most L nodes long, and at most S paths per word type. In contra</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. Incorporating domain knowledge into topic modeling via Dirichlet forest priors. In Proceedings of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="963" citStr="Blei et al., 2003" startWordPosition="133" endWordPosition="136">es at the cost of more complicated inference. We extend the SPARSELDA (Yao et al., 2009) inference scheme for latent Dirichlet allocation (LDA) to tree-based topic models. This sampling scheme computes the exact conditional distribution for Gibbs sampling much more quickly than enumerating all possible latent variable assignments. We further improve performance by iteratively refining the sampling distribution only when needed. Experiments show that the proposed techniques dramatically improve the computation time. 1 Introduction Topic models, exemplified by latent Dirichlet allocation (LDA) (Blei et al., 2003), discover latent themes present in text collections. “Topics” discovered by topic models are multinomial probability distributions over words that evince thematic coherence. Topic models are used in computational biology, computer vision, music, and, of course, text analysis. One of LDA’s virtues is that it is a simple model that assumes a symmetric Dirichlet prior over its word distributions. Recent work argues for structured distributions that constrain clusters (Andrzejewski et al., 2009), span languages (Jagarlamudi and Daum´e III, 2010), or incorporate human feedback (Hu et al., 2011) to</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2765" citStr="Boyd-Graber et al., 2007" startWordPosition="392" endWordPosition="395">algorithm for LDA based on a refactorization of the conditional topic distribution (reviewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we provide a factorization for tree-based models within a broadly applicable inference framework that empirically improves the efficiency of inference (Section 5). 2 Topic Modeling with Tree-Based Priors Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used treestructured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al., 2007). In both cases, the tree came from WordNet (Miller, 1990), but the tree could also come from domain experts (Andrzejewski et al., 2009). Organizing words in this way induces correlations that are mathematically impossible to represent with a symmetric Dirichlet prior. To see how correlations can occur, consider the generative process. Start with a rooted tree structure that contains internal nodes and leaf nodes. This skeleton is a prior that generates K topics. Like vanilla LDA, these topics are distributions over words. Unlike vanilla LDA, their structure correlates words. Internal nodes ha</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of Emperical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Heinrich</author>
</authors>
<title>Parameter estimation for text analysis.</title>
<date>2004</date>
<tech>Technical report. http://www.arbylon.net/publications/text-est.pdf.</tech>
<contexts>
<context position="1812" citStr="Heinrich, 2004" startWordPosition="257" endWordPosition="258">sion, music, and, of course, text analysis. One of LDA’s virtues is that it is a simple model that assumes a symmetric Dirichlet prior over its word distributions. Recent work argues for structured distributions that constrain clusters (Andrzejewski et al., 2009), span languages (Jagarlamudi and Daum´e III, 2010), or incorporate human feedback (Hu et al., 2011) to improve the quality and flexibility of topic modeling. These models all use different tree-based prior distributions (Section 2). These approaches are appealing because they preserve conjugacy, making inference using Gibbs sampling (Heinrich, 2004) straightforward. While straightforward, inference isn’t cheap. Particularly Jordan Boyd-Graber iSchool and UMIACS University of Maryland, College Park jbg@umiacs.umd.edu for interactive settings (Hu et al., 2011), efficient inference would improve perceived latency. SPARSELDA (Yao et al., 2009) is an efficient Gibbs sampling algorithm for LDA based on a refactorization of the conditional topic distribution (reviewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we provide a factorization for tree-based models within a broadly applicable inference fr</context>
</contexts>
<marker>Heinrich, 2004</marker>
<rawString>Gregor Heinrich. 2004. Parameter estimation for text analysis. Technical report. http://www.arbylon.net/publications/text-est.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Jordan Boyd-Graber</author>
<author>Brianna Satinoff</author>
</authors>
<title>Interactive topic modeling.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1560" citStr="Hu et al., 2011" startWordPosition="221" endWordPosition="224">(Blei et al., 2003), discover latent themes present in text collections. “Topics” discovered by topic models are multinomial probability distributions over words that evince thematic coherence. Topic models are used in computational biology, computer vision, music, and, of course, text analysis. One of LDA’s virtues is that it is a simple model that assumes a symmetric Dirichlet prior over its word distributions. Recent work argues for structured distributions that constrain clusters (Andrzejewski et al., 2009), span languages (Jagarlamudi and Daum´e III, 2010), or incorporate human feedback (Hu et al., 2011) to improve the quality and flexibility of topic modeling. These models all use different tree-based prior distributions (Section 2). These approaches are appealing because they preserve conjugacy, making inference using Gibbs sampling (Heinrich, 2004) straightforward. While straightforward, inference isn’t cheap. Particularly Jordan Boyd-Graber iSchool and UMIACS University of Maryland, College Park jbg@umiacs.umd.edu for interactive settings (Hu et al., 2011), efficient inference would improve perceived latency. SPARSELDA (Yao et al., 2009) is an efficient Gibbs sampling algorithm for LDA ba</context>
</contexts>
<marker>Hu, Boyd-Graber, Satinoff, 2011</marker>
<rawString>Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff. 2011. Interactive topic modeling. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
</authors>
<title>Extracting multilingual topics from unaligned corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Information Retrieval (ECIR).</booktitle>
<marker>Jagarlamudi, Daum´e, 2010</marker>
<rawString>Jagadeesh Jagarlamudi and Hal Daum´e III. 2010. Extracting multilingual topics from unaligned corpora. In Proceedings of the European Conference on Information Retrieval (ECIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Nouns in WordNet: A lexical inheritance system.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="2823" citStr="Miller, 1990" startWordPosition="404" endWordPosition="405">distribution (reviewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we provide a factorization for tree-based models within a broadly applicable inference framework that empirically improves the efficiency of inference (Section 5). 2 Topic Modeling with Tree-Based Priors Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used treestructured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al., 2007). In both cases, the tree came from WordNet (Miller, 1990), but the tree could also come from domain experts (Andrzejewski et al., 2009). Organizing words in this way induces correlations that are mathematically impossible to represent with a symmetric Dirichlet prior. To see how correlations can occur, consider the generative process. Start with a rooted tree structure that contains internal nodes and leaf nodes. This skeleton is a prior that generates K topics. Like vanilla LDA, these topics are distributions over words. Unlike vanilla LDA, their structure correlates words. Internal nodes have a distribution irk,i over children, where irk,i comes f</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. Nouns in WordNet: A lexical inheritance system. International Journal of Lexicography, 3(4):245–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander J Smola</author>
<author>Shravan Narayanamurthy</author>
</authors>
<title>An architecture for parallel topic models.</title>
<date>2010</date>
<booktitle>International Conference on Very Large Databases,</booktitle>
<volume>3</volume>
<contexts>
<context position="15418" citStr="Smola and Narayanamurthy, 2010" startWordPosition="2562" endWordPosition="2565">with “queen”, the associated topic has “king, parade, museum, queen, jackson” as its most probable words. These represent reasonable disambiguations. In contrast to previous approaches, inference speeds up as topics become more semantically coherent (BoydGraber et al., 2007). 6 Conclusion We demonstrated efficient inference techniques for topic models with tree-based priors. These methods scale well, allowing for faster exploration of models that use semantics to encode correlations without sacrificing accuracy. Improved scalability for such algorithms, especially in distributed environments (Smola and Narayanamurthy, 2010), could improve applications such as cross-language information retrieval, unsupervised word sense disambiguation, and knowledge discovery via interactive topic modeling. Average running time per iteration (S) 16 14 12 10 4 21 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 8 6 Naive Fast Fast−RB Fast−RB−sD Fast−RB−sW Fast−RB−sDW 278 Acknowledgments We would like to thank David Mimno and the anonymous reviewers for their helpful comments. This work was supported by the Army Research Laboratory through ARL Cooperative Agreement W911NF09-2-0072. Any opinions or conclusions expressed are the authors’ and do </context>
</contexts>
<marker>Smola, Narayanamurthy, 2010</marker>
<rawString>Alexander J. Smola and Shravan Narayanamurthy. 2010. An architecture for parallel topic models. International Conference on Very Large Databases, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<booktitle>In Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="2108" citStr="Yao et al., 2009" startWordPosition="291" endWordPosition="294">nd Daum´e III, 2010), or incorporate human feedback (Hu et al., 2011) to improve the quality and flexibility of topic modeling. These models all use different tree-based prior distributions (Section 2). These approaches are appealing because they preserve conjugacy, making inference using Gibbs sampling (Heinrich, 2004) straightforward. While straightforward, inference isn’t cheap. Particularly Jordan Boyd-Graber iSchool and UMIACS University of Maryland, College Park jbg@umiacs.umd.edu for interactive settings (Hu et al., 2011), efficient inference would improve perceived latency. SPARSELDA (Yao et al., 2009) is an efficient Gibbs sampling algorithm for LDA based on a refactorization of the conditional topic distribution (reviewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we provide a factorization for tree-based models within a broadly applicable inference framework that empirically improves the efficiency of inference (Section 5). 2 Topic Modeling with Tree-Based Priors Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used treestructured multinomials to model selectional restrictions, which was later put into a Bayes</context>
<context position="5399" citStr="Yao et al., 2009" startWordPosition="821" endWordPosition="824">riables are the topic assignment z and path l, which we sample jointly:2 where nk|d is topic k’s count in the document d; αk is topic k’s prior; Z_ and L_ are topic and path assignments excluding wd,n; ,3i,j is the prior for edge i → j, ni,j|t is the count of edge i → j in topic k; and j&apos; denotes other children of node i. The complexity of computing the sampling distribution is O(KLS) for models with K topics, paths at most L nodes long, and at most S paths per word type. In contrast, for vanilla LDA the analogous conditional sampling distribution requires O(K). 3 Efficient LDA The SPARSELDA (Yao et al., 2009) scheme for speeding inference begins by rearranging LDA’s sampling equation into three terms:3 Following their lead, we call these three terms “buckets”. A bucket is the total probability mass marginalizing over latent variable assignments (i.e., sLDA ≡ Ek pv+a.�k, similarly for the other buckets). The three buckets are a smoothing only bucket 2For clarity, we omit indicators that ensure A ends at wd,n. 3To ease notation we drop the d,n subscript for z and w in this and future equations. sLDA, document topic bucket rLDA, and topic word bucket qLDA (we use the “LDA” subscript to contrast with </context>
</contexts>
<marker>Yao, Mimno, McCallum, 2009</marker>
<rawString>Limin Yao, David Mimno, and Andrew McCallum. 2009. Efficient methods for topic model inference on streaming document collections. In Knowledge Discovery and Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>