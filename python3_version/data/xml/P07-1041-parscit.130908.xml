<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000260">
<title confidence="0.780042666666667">
Generating Constituent Order in German Clauses
Katja Filippova and Michael Strube
EML Research gGmbH
</title>
<address confidence="0.431983">
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
</address>
<email confidence="0.885249">
http://www.eml-research.de/nlp
</email>
<sectionHeader confidence="0.994031" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999165">
We investigate the factors which determine
constituent order in German clauses and pro-
pose an algorithm which performs the task
in two steps: First, the best candidate for
the initial sentence position is chosen. Then,
the order for the remaining constituents is
determined. The first task is more difficult
than the second one because of properties
of the German sentence-initial position. Ex-
periments show a significant improvement
over competing approaches. Our algorithm
is also more efficient than these.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999512119047619">
Many natural languages allow variation in the word
order. This is a challenge for natural language gen-
eration and machine translation systems, or for text
summarizers. E.g., in text-to-text generation (Barzi-
lay &amp; McKeown, 2005; Marsi &amp; Krahmer, 2005;
Wan et al., 2005), new sentences are fused from de-
pendency structures of input sentences. The last step
of sentence fusion is linearization of the resulting
parse. Even for English, which is a language with
fixed word order, this is not a trivial task.
German has a relatively free word order. This
concerns the order of constituents&apos; within sentences
while the order of words within constituents is rela-
tively rigid. The grammar only partially prescribes
how constituents dependent on the verb should be
ordered, and for many clauses each of the n! possi-
ble permutations of n constituents is grammatical.
&apos;Henceforth, we will use this term to refer to constituents
dependent on the clausal top node, i.e. a verb, only.
In spite of the permanent interest in German word
order in the linguistics community, most studies
have limited their scope to the order of verb argu-
ments and few researchers have implemented – and
even less evaluated – a generation algorithm. In this
paper, we present an algorithm, which orders not
only verb arguments but all kinds of constituents,
and evaluate it on a corpus of biographies. For
each parsed sentence in the test set, our maximum-
entropy-based algorithm aims at reproducing the or-
der found in the original text. We investigate the
importance of different linguistic factors and sug-
gest an algorithm to constituent ordering which first
determines the sentence initial constituent and then
orders the remaining ones. We provide evidence
that the task requires language-specific knowledge
to achieve better results and point to the most diffi-
cult part of it. Similar to Langkilde &amp; Knight (1998)
we utilize statistical methods. Unlike overgenera-
tion approaches (Varges &amp; Mellish, 2001, inter alia)
which select the best of all possible outputs ours is
more efficient, because we do not need to generate
every permutation.
</bodyText>
<sectionHeader confidence="0.99581" genericHeader="introduction">
2 Theoretical Premises
</sectionHeader>
<subsectionHeader confidence="0.763172">
2.1 Background
</subsectionHeader>
<bodyText confidence="0.999918142857143">
It has been suggested that several factors have an in-
fluence on German constituent order. Apart from
the constraints posed by the grammar, information
structure, surface form, and discourse status have
also been shown to play a role. It has also been
observed that there are preferences for a particular
order. The preferences summarized below have mo-
</bodyText>
<page confidence="0.955224">
320
</page>
<note confidence="0.8570395">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320–327,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<listItem confidence="0.96315872">
tivated our choice of features: (1) [Außerdem] entwickelte [Lummer eine
• constituents in the nominative case precede Apart from that developed Lummer a
those in other cases, and dative constituents Quecksilberdampflampe, um
often precede those in the accusative case Mercury-vapor lamp to
(Uszkoreit, 1987; Keller, 2000); monochromatisches Licht herzustellen].
• the verb arguments’ order depends on the monochrome light produce.
verb’s subcategorization properties (Kurz, ’Apart from that, Lummer developed a
2000); Mercury-vapor lamp to produce monochrome
• constituents with a definite article precede light’.
those with an indefinite one (Weber &amp; M¨uller, 2.2 Our Hypothesis
2004); The essential contribution of our study is that we
• pronominalized constituents precede non- treat preverbal and postverbal parts of the sentence
pronominalized ones (Kempen &amp; Harbusch, differently. The sentence-initial position, which in
2004); German is the VF, has been shown to be cognitively
• animate referents precede inanimate ones (Pap- more prominent than other positions (Gernsbacher
pert et al., 2007); &amp; Hargreaves, 1988). Motivated by the theoretical
• short constituents precede longer ones (Kim- work by Chafe (1976) and Jacobs (2001), we view
ball, 1973); the VF as the place for elements which modify the
• the preferred topic position is right after the situation described in the sentence, i.e. for so called
verb (Frey, 2004); frame-setting topics (Jacobs, 2001). For example,
• the initial position is usually occupied by temporal or locational constituents, or anaphoric ad-
scene-setting elements and topics (Speyer, verbs are good candidates for the VF. We hypoth-
2005). esize that the reasons which bring a constituent to
• there is a default order based on semantic prop- the VF are different from those which place it, say,
erties of constituents (Sgall et al., 1986): to the beginning of the MF, for the order in the MF
</listItem>
<bodyText confidence="0.97765675">
Actor &lt; Temporal &lt; SpaceLocative &lt; Means &lt; Ad- has been shown to be relatively rigid (Keller, 2000;
dressee &lt; Patient &lt; Source &lt; Destination &lt; Purpose Kempen &amp; Harbusch, 2004). Speakers have the
Note that most of these preferences were identified freedom of selecting the outgoing point for a sen-
in corpus studies and experiments with native speak- tence. Once they have selected it, the remaining con-
ers and concern the order of verb arguments only. stituents are arranged in the MF, mainly according to
Little has been said so far about how non-arguments their grammatical properties.
should be ordered. This last observation motivates another hypothe-
German is a verb second language, i.e., the po- sis we make: The cumulation of the properties of
sition of the verb in the main clause is determined a constituent determines its salience. This salience
exclusively by the grammar and is insensitive to can be calculated and used for ordering with a sim-
other factors. Thus, the German main clause is di- ple rule stating that more salient constituents should
vided into two parts by the finite verb: Vorfeld (VF), precede less salient ones. In this case there is no
which contains exactly one constituent, and Mit- need to generate all possible orders and rank them.
telfeld (MF), where the remaining constituents are The best order can be obtained from a random one
located. The subordinate clause normally has only by sorting. Our experiments support this view. A
MF. The VF and MF are marked with brackets in two-step approach, which first selects the best can-
Example 1: didate for the VF and then arranges the remaining
321 constituents in the MF with respect to their salience
performs better than algorithms which generate the
order for a sentence as a whole.
</bodyText>
<sectionHeader confidence="0.999881" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999131484848485">
Uszkoreit (1987) addresses the problem from a
mostly grammar-based perspective and suggests
weighted constraints, such as [+NOM] � [+DAT],
[+PRO] � [–PRO], [–FOCUS] � [+FOCUS], etc.
Kruijff et al. (2001) describe an architecture
which supports generating the appropriate word or-
der for different languages. Inspired by the findings
of the Prague School (Sgall et al., 1986) and Sys-
temic Functional Linguistics (Halliday, 1985), they
focus on the role that information structure plays
in constituent ordering. Kruijff-Korbayov´a et al.
(2002) address the task of word order generation in
the same vein. Similar to ours, their algorithm rec-
ognizes the special role of the sentence-initial po-
sition which they reserve for the theme – the point
of departure of the message. Unfortunately, they did
not implement their algorithm, and it is hard to judge
how well the system would perform on real data.
Harbusch et al. (2006) present a generation work-
bench, which has the goal of producing not the most
appropriate order, but all grammatical ones. They
also do not provide experimental results.
The work of Uchimoto et al. (2000) is done on
the free word order language Japanese. They de-
termine the order of phrasal units dependent on the
same modifiee. Their approach is similar to ours in
that they aim at regenerating the original order from
a dependency parse, but differs in the scope of the
problem as they regenerate the order of modifers for
all and not only for the top clausal node. Using a
maximum entropy framework, they choose the most
probable order from the set of all permutations of n
words by the following formula:
</bodyText>
<equation confidence="0.99931125">
P(1|h) = P({Wi,i+j = 1|1 &lt; i &lt; n − 1, 1 &lt; j &lt; n − il|h)
P(Wi,i+j = 1|hi,i+j)
PME(1|hi,i+j)
(1)
</equation>
<bodyText confidence="0.994652666666667">
For each permutation, for every pair of words , they
multiply the probability of their being in the correct2
order given the history h. Random variable Wi,i+j
</bodyText>
<subsectionHeader confidence="0.857725">
2Only reference orders are assumed to be correct.
</subsectionHeader>
<bodyText confidence="0.999734133333333">
is 1 if word wi precedes wi+j in the reference sen-
tence, 0 otherwise. The features they use are akin
to those which play a role in determining German
word order. We use their approach as a non-trivial
baseline in our study.
Ringger et al. (2004) aim at regenerating the or-
der of constituents as well as the order within them
for German and French technical manuals. Utilizing
syntactic, semantic, sub-categorization and length
features, they test several statistical models to find
the order which maximizes the probability of an or-
dered tree. Using “Markov grammars” as the start-
ing point and conditioning on the syntactic category
only, they expand a non-terminal node C by predict-
ing its daughters from left to right:
</bodyText>
<equation confidence="0.9996185">
P(C|h) = Yn P(di|di−1, ..., di−j, c, h) (2)
i=1
</equation>
<bodyText confidence="0.97601">
Here, c is the syntactic category of C, d and h
are the syntactic categories of C’s daughters and the
daughter which is the head of C respectively.
In their simplest system, whose performance is
only 2.5% worse than the performance of the best
one, they condition on both syntactic categories and
semantic relations (0) according to the formula:
</bodyText>
<equation confidence="0.972223">
–
n
Y» P(ψi  |di,,/-&apos; 1 , ψi−1,&amp;quot;�.�..di−j , ψi−j, c, h)
xP(di  |Wi, di−1, 0i-1.... i-1...f di−j, ψi−j, c, h)
i=1
(3)
</equation>
<bodyText confidence="0.999969636363636">
Although they test their system on German data,
it is hard to compare their results to ours directly.
First, the metric they use does not describe the per-
formance appropriately (see Section 6.1). Second,
while the word order within NPs and PPs as well as
the verb position are prescribed by the grammar to a
large extent, the constituents can theoretically be or-
dered in any way. Thus, by generating the order for
every non-terminal node, they combine two tasks of
different complexity and mix the results of the more
difficult task with those of the easier one.
</bodyText>
<sectionHeader confidence="0.997391" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999540666666667">
The data we work with is a collection of biogra-
phies from the German version of Wikipedia3. Fully
automatic preprocessing in our system comprises
the following steps: First, a list of people of a
certain Wikipedia category is taken and an article
is extracted for every person. Second, sentence
</bodyText>
<footnote confidence="0.815799">
3http://de.wikipedia.org
</footnote>
<equation confidence="0.9897643">
n−iY
j=1
=
n−1Y�
i=1
n−1Y
i=1
n−iY
j=1
P(C|h) =
</equation>
<page confidence="0.996182">
322
</page>
<figure confidence="0.962037666666667">
entwickelte
Lummer SUBJ (pers) außerdem ADV (conn) eine Quecksilberdampflampe OBJA um herzustellen SUB
monochromatisches Licht
</figure>
<figureCaption confidence="0.999962">
Figure 1: The representation of the sentence in Example 1
</figureCaption>
<bodyText confidence="0.947794615384616">
boundaries are identified with a Perl CPAN mod-
ule4 whose performance we improved by extend-
ing the list of abbreviations. Next, the sentences
are split into tokens. The TnT tagger (Brants, 2000)
and the TreeTagger (Schmid, 1997) are used for tag-
ging and lemmatization. Finally, the articles are
parsed with the CDG dependency parser (Foth &amp;
Menzel, 2006). Named entities are classified accord-
ing to their semantic type using lists and category
information from Wikipedia: person (pers), location
(loc), organization (org), or undefined named entity
(undef ne). Temporal expressions (Oktober 1915,
danach (after that) etc.) are identified automatically
by a set of patterns. Inevitable during automatic an-
notation, errors at one of the preprocessing stages
cause errors at the ordering stage.
Distinguishing between main and subordinate
clauses, we split the total of about 19 000 sentences
into training, development and test sets (Table 1).
Clauses with one constituent are sorted out as trivial.
The distribution of both types of clauses according
to their length in constituents is given in Table 2.
train dev test
main 14324 3344 1683
sub 3304 777 408
total 17628 4121 2091
</bodyText>
<tableCaption confidence="0.995221">
Table 1: Size of the data sets in clauses
</tableCaption>
<bodyText confidence="0.878367333333333">
2 3 4 5 6+
main 20% 35% 27% 12% 6%
sub 49% 35% 11% 2% 3%
</bodyText>
<tableCaption confidence="0.994217">
Table 2: Proportion of clauses with certain lengths
</tableCaption>
<footnote confidence="0.874368">
4http://search.cpan.org/˜holsten/Lingua-DE-Sentence-
0.07/Sentence.pm
</footnote>
<bodyText confidence="0.9999634">
Given the sentence in Example 1, we first trans-
form its dependency parse into a more general
representation (Figure 15) and then, based on the
predictions of our learner, arrange the four con-
stituents. For evaluation, we compare the arranged
order against the original one.
Note that we predict neither the position of the
verb, nor the order within constituents as the former
is explicitly determined by the grammar, and the lat-
ter is much more rigid than the order of constituents.
</bodyText>
<sectionHeader confidence="0.968865" genericHeader="method">
5 Baselines and Algorithms
</sectionHeader>
<bodyText confidence="0.998933">
We compare the performance of two our algorithms
with four baselines.
</bodyText>
<subsectionHeader confidence="0.921141">
5.1 Random
</subsectionHeader>
<bodyText confidence="0.9999892">
We improve a trivial random baseline (RAND) by
two syntax-oriented rules: the first position is re-
served for the subject and the second for the direct
object if there is any; the order of the remaining con-
stituents is generated randomly (RAND IMP).
</bodyText>
<subsectionHeader confidence="0.993971">
5.2 Statistical Bigram Model
</subsectionHeader>
<bodyText confidence="0.999793">
Similar to Ringger et al. (2004), we find the order
with the highest probability conditioned on syntac-
tic and semantic categories. Unlike them we use de-
pendency parses and compute the probability of the
top node only, which is modified by all constituents.
With these adjustments the probability of an order
O given the history h, if conditioned on syntactic
functions of constituents (s1...sn), is simply:
</bodyText>
<equation confidence="0.989342">
n
P(Olh) = P(si|si−1, h) (4)
i=1
</equation>
<bodyText confidence="0.7507635">
Ringger et al. (2004) do not make explicit, what
their set of semantic relations consists of. From the
</bodyText>
<footnote confidence="0.952492">
5OBJA stands for the accusative object.
</footnote>
<page confidence="0.998259">
323
</page>
<bodyText confidence="0.9999848">
example in the paper, it seems that these are a mix-
ture of lexical and syntactic information6. Our anno-
tation does not specify semantic relations. Instead,
some of the constituents are categorized as pers, loc,
temp, org or undef ne if their heads bear one of these
labels. By joining these with possible syntactic func-
tions, we obtain a larger set of syntactic-semantic
tags as, e.g., subj-pers, pp-loc, adv-temp. We trans-
form each clause in the training set into a sequence
of such tags, plus three tags for the verb position (v),
the beginning (b) and the end (e) of the clause. Then
we compute the bigram probabilities7.
For our third baseline (BIGRAM), we select from
all possible orders the one with the highest probabil-
ity as calculated by the following formula:
</bodyText>
<equation confidence="0.990269333333333">
n
P(O|h) = P (ti|ti−1, h) (5)
i=1
</equation>
<bodyText confidence="0.99958075">
where ti is from the set of joined tags. For Example
1, possible tag sequences (i.e. orders) are ’b subj-
pers v adv obja sub e’, ’b adv v subj-pers obja sub
e’, ’b obja v adv sub subj-pers e’, etc.
</bodyText>
<subsectionHeader confidence="0.974088">
5.3 Uchimoto
</subsectionHeader>
<bodyText confidence="0.994552266666667">
For the fourth baseline (UCHIMOTO), we utilized a
maximum entropy learner (OpenNLP8) and reim-
plemented the algorithm of Uchimoto et al. (2000).
For every possible permutation, its probability is es-
timated according to Formula (1). The binary clas-
sifier, whose task was to predict the probability that
the order of a pair of constituents is correct, was
trained on the following features describing the verb
or hc – the head of a constituent c9:
vlex, vpass, vmod the lemma of the root of the
clause (non-auxiliary verb), the voice of the
verb and the number of constituents to order;
lex the lemma of hc or, if hc is a functional word,
the lemma of the word which depends on it;
pos part-of-speech tag of hc;
</bodyText>
<footnote confidence="0.936058833333333">
6E.g. DejDet, Coords, Possr, werden
7We use the CMU Toolkit (Clarkson &amp; Rosenfeld, 1997).
8http://opennlp.sourceforge.net
9We disregarded features which use information specific to
Japanese and non-applicable to German (e.g. on postpositional
particles).
</footnote>
<bodyText confidence="0.98879325">
sem if defined, the semantic class of c; e.g. im April
1900 and mit Albert Einstein (with Albert Ein-
stein) are classified temp and pers respectively;
syn, same the syntactic function of hc and whether
it is the same for the two constituents;
mod number of modifiers of hc;
rep whether hc appears in the preceding sentence;
pro whether c contains a (anaphoric) pronoun.
</bodyText>
<subsectionHeader confidence="0.96237">
5.4 Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.999682088235294">
The first configuration of our system is an extended
version of the UCHIMOTO baseline (MAXENT). To
the features describing c we added the following
ones:
det the kind of determiner modifying hc (def, indef,
non-appl);
rel whether hc is modified by a relative clause (yes,
no, non-appl);
dep the depth of c;
len the length of c in words.
The first two features describe the discourse status
of a constituent; the other two provide information
on its “weight”. Since our learner treats all values
as nominal, we discretized the values of dep and len
with a C4.5 classifier (Kohavi &amp; Sahami, 1996).
Another modification concerns the efficiency of
the algorithm. Instead of calculating probabilities
for all pairs, we obtain the right order from a random
one by sorting. We compare adjacent elements by
consulting the learner as if we would sort an array of
numbers. Given two adjacent constituents, ci &lt; cj,
we check the probability of their being in the right
order, i.e. that ci precedes cj: Ppre(ci, cj). If it is
less than 0.5, we transpose the two and compare ci
with the next one.
Since the sorting method presupposes that the pre-
dicted relation is transitive, we checked whether this
is really so on the development and test data sets. We
looked for three constituents ci, cj, ck from a sen-
tence 5, such that Ppre(ci, cj) &gt; 0.5, Ppre(cj, ck) &gt;
0.5, Ppre(ci, ck) &lt; 0.5 and found none. Therefore,
unlike UCHIMOTO, where one needs to make exactly
N! * N(N − 1)/2 comparisons, we have to make
N(N − 1)/2 comparisons at most.
</bodyText>
<page confidence="0.997266">
324
</page>
<subsectionHeader confidence="0.978134">
5.5 The Two-Step Approach
</subsectionHeader>
<bodyText confidence="0.952774230769231">
The main difference between our first algorithm
(MAXENT) and the second one (TWO-STEP) is that
we generate the order in two steps10 (both classifiers
are trained on the same features):
1. For the VF, using the OpenNLP maximum en-
tropy learner for a binary classification (VF vs.
MF), we select the constituent c with the high-
est probability of being in the VF.
2. For the MF, the remaining constituents are put
into a random order and then sorted the way it
is done for MAXENT. The training data for the
second task was generated only from the MF of
clauses.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.997187">
6.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.995749107142857">
We use several metrics to evaluate our systems and
the baselines. The first is per-sentence accuracy
(acc) which is the proportion of correctly regener-
ated sentences. Kendall’s T, which has been used for
evaluating sentence ordering tasks (Lapata, 2006),
is the second metric we use. T is calculated as
1−4 t
N(N�1),where t is the number of interchanges
of consecutive elements to arrange N elements in
the right order. T is sensitive to near misses and
assigns abdc (almost correct order) a score of 0.66
while dcba (inverse order) gets −1. Note that it is
questionable whether this metric is as appropriate
for word ordering tasks as for sentence ordering ones
because a near miss might turn out to be ungrammat-
ical whereas a more different order stays acceptable.
Apart from acc and T, we also adopt the metrics
used by Uchimoto et al. (2000) and Ringger et al.
(2004). The former use agreement rate (agr) cal-
culated as 2p
N(N�1): the number of correctly ordered
pairs of constituents over the total number of all pos-
sible pairs, as well as complete agreement which is
basically per-sentence accuracy. Unlike T, which
has −1 as the lowest score, agr ranges from 0 to 1.
Ringger et al. (2004) evaluate the performance only
in terms of per-constituent edit distance calculated
as mN , where m is the minimum number of moves11
</bodyText>
<footnote confidence="0.518288333333333">
10Since subordinate clauses do not have a VF, the first step is
not needed.
11A move is a deletion combined with an insertion.
</footnote>
<bodyText confidence="0.999818466666667">
needed to arrange N constituents in the right order.
This measure seems less appropriate than T or agr
because it does not take the distance of the move into
account and scores abced and eabcd equally (0.2).
Since T and agr, unlike edit distance, give higher
scores to better orders, we compute inverse distance:
inv = 1 – edit distance instead. Thus, all three met-
rics (T, agr, inv) give the maximum of 1 if con-
stituents are ordered correctly. However, like T, agr
and inv can give a positive score to an ungrammat-
ical order. Hence, none of the evaluation metrics
describes the performance perfectly. Human eval-
uation which reliably distinguishes between appro-
priate, acceptable, grammatical and ingrammatical
orders was out of choice because of its high cost.
</bodyText>
<subsectionHeader confidence="0.87517">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.990235">
The results on the test data are presented in Table
</bodyText>
<listItem confidence="0.956762">
3. The performance of TWO-STEP is significantly
better than any other method (x2, p &lt; 0.01). The
performance of MAXENT does not significantly dif-
fer from UCHIMOTO. BIGRAM performed about as
good as UCHIMOTO and MAXENT. We also checked
how well TWO-STEP performs on each of the two
sub-tasks (Table 4) and found that the VF selection
is considerably more difficult than the sorting part.
</listItem>
<table confidence="0.997799857142857">
acc T agr inv
RAND 15% 0.02 0.51 0.64
RAND IMP 23% 0.24 0.62 0.71
BIGRAM 51% 0.60 0.80 0.83
UCHIMOTO 50% 0.65 0.82 0.83
MAXENT 52% 0.67 0.84 0.84
TWO-STEP 61% 0.72 0.86 0.87
</table>
<tableCaption confidence="0.999935">
Table 3: Per-clause mean of the results
</tableCaption>
<bodyText confidence="0.999957545454545">
The most important conclusion we draw from the
results is that the gain of 9% accuracy is due to the
VF selection only, because the feature sets are iden-
tical for MAXENT and TWO-STEP. From this fol-
lows that doing feature selection without splitting
the task in two is ineffective, because the importance
of a feature depends on whether the VF or the MF is
considered. For the MF, feature selection has shown
syn and pos to be the most relevant features. They
alone bring the performance in the MF up to 75%. In
contrast, these two features explain only 56% of the
</bodyText>
<page confidence="0.997876">
325
</page>
<bodyText confidence="0.99357825">
cases in the VF. This implies that the order in the MF
mainly depends on grammatical features, while for
the VF all features are important because removal of
any feature caused a loss in accuracy.
</bodyText>
<tableCaption confidence="0.984854">
Table 4: Mean of the results for the VF and the MF
</tableCaption>
<bodyText confidence="0.999842">
Another important finding is that there is no need
to overgenerate to find the right order. Insignificant
for clauses with two or three constituents, for clauses
with 10 constituents, the number of comparisons is
reduced drastically from 163,296,000 to 45.
According to the inv metric, our results are con-
siderably worse than those reported by Ringger et al.
(2004). As mentioned in Section 3, the fact that they
generate the order for every non-terminal node se-
riously inflates their numbers. Apart from that, they
do not report accuracy, and it is unknown, how many
sentences they actually reproduced correctly.
</bodyText>
<subsectionHeader confidence="0.992314">
6.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999961275">
To reveal the main error sources, we analyzed incor-
rect predictions concerning the VF and the MF, one
hundred for each. Most errors in the VF did not lead
to unacceptability or ungrammaticality. From lexi-
cal and semantic features, the classifier learned that
some expressions are often used in the beginning of
a sentence. These are temporal or locational PPs,
anaphoric adverbials, some connectives or phrases
starting with unlike X, together with X, as X, etc.
Such elements were placed in the VF instead of the
subject and caused an error although both variants
were equally acceptable. In other cases the classi-
fier could not find a better candidate but the subject
because it could not conclude from the provided fea-
tures that another constituent would nicely introduce
the sentence into the discourse. Mainly this con-
cerns recognizing information familiar to the reader
not by an already mentioned entity, but one which is
inferrable from what has been read.
In the MF, many orders had a PP transposed with
the direct object. In some cases the predicted order
seemed as good as the correct one. Often the algo-
rithm failed at identifying verb-specific preferences:
E.g., some verbs take PPs with the locational mean-
ing as an argument and normally have them right
next to them, whereas others do not. Another fre-
quent error was the wrong placement of superficially
identical constituents, e.g. two PPs of the same size.
To handle this error, the system needs more spe-
cific semantic information. Some errors were caused
by the parser, which created extra constituents (e.g.
false PP or adverb attachment) or confused the sub-
ject with the direct verb.
We retrained our system on a corpus of newspaper
articles (Telljohann et al., 2003, T¨uBa-D/Z) which is
manually annotated but encodes no semantic knowl-
edge. The results for the MF were the same as on the
data from Wikipedia. The results for the VF were
much worse (45%) because of the lack of semantic
information.
</bodyText>
<sectionHeader confidence="0.999045" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999239214285714">
We presented a novel approach to ordering con-
stituents in German. The results indicate that a
linguistically-motivated two-step system, which first
selects a constituent for the initial position and then
orders the remaining ones, works significantly better
than approaches which do not make this separation.
Our results also confirm the hypothesis – which has
been attested in several corpus studies – that the or-
der in the MF is rather rigid and dependent on gram-
matical properties.
We have also demonstrated that there is no need
to overgenerate to find the best order. On a prac-
tical side, this finding reduces the amount of work
considerably. Theoretically, it lets us conclude that
the relatively fixed order in the MF depends on the
salience which can be predicted mainly from gram-
matical features. It is much harder to predict which
element should be placed in the VF. We suppose that
this difficulty comes from the double function of the
initial position which can either introduce the ad-
dressation topic, or be the scene- or frame-setting
position (Jacobs, 2001).
Acknowledgements: This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a KTF
grant (09.009.2004). We would also like to thank
Elke Teich and the three anonymous reviewers for
their useful comments.
</bodyText>
<figure confidence="0.7789946">
acc Ir agr inv
TWO-STEP VF 68%
- - -
TWO-STEP MF
80% 0.92 0.96 0.95
</figure>
<page confidence="0.995931">
326
</page>
<sectionHeader confidence="0.989701" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999054504000001">
Barzilay, R. &amp; K. R. McKeown (2005). Sentence fusion for
multidocument news summarization. Computational Lin-
guistics, 31(3):297–327.
Brants, T. (2000). TnT – A statistical Part-of-Speech tagger. In
Proceedings of the 6th Conference on Applied Natural Lan-
guage Processing, Seattle, Wash., 29 April – 4 May 2000,
pp. 224–231.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, sub-
jects, topics, and point of view. In C. Li (Ed.), Subject and
Topic, pp. 25–55. New York, N.Y.: Academic Press.
Clarkson, P. &amp; R. Rosenfeld (1997). Statistical language mod-
eling using the CMU-Cambridge toolkit. In Proceedings
of the 5th European Conference on Speech Communication
and Technology, Rhodes, Greece, 22-25 September 1997, pp.
2707–2710.
Foth, K. &amp; W. Menzel (2006). Hybrid parsing: Using proba-
bilistic models as predictors for a symbolic parser. In Pro-
ceedings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, Sydney, Australia, 17–
21 July 2006, pp. 321–327.
Frey, W. (2004). A medial topic position for German. Linguis-
tische Berichte, 198:153–190.
Gernsbacher, M. A. &amp; D. J. Hargreaves (1988). Accessing sen-
tence participants: The advantage of first mention. Journal
of Memory and Language, 27:699–717.
Halliday, M. A. K. (1985). Introduction to Functional Gram-
mar. London, UK: Arnold.
Harbusch, K., G. Kempen, C. van Breugel &amp; U. Koch (2006).
A generation-oriented workbench for performance grammar:
Capturing linear order variability in German and Dutch. In
Proceedings of the International Workshop on Natural Lan-
guage Generation, Sydney, Australia, 15-16 July 2006, pp.
9–11.
Jacobs, J. (2001). The dimensions of topic-comment. Linguis-
tics, 39(4):641–681.
Keller, F. (2000). Gradience in Grammar: Experimental
and Computational Aspects of Degrees of Grammaticality,
(Ph.D. thesis). University of Edinburgh.
Kempen, G. &amp; K. Harbusch (2004). How flexible is con-
stituent order in the midfield of German subordinate clauses?
A corpus study revealing unexpected rigidity. In Proceed-
ings of the International Conference on Linguistic Evidence,
T¨ubingen, Germany, 29–31 January 2004, pp. 81–85.
Kimball, J. (1973). Seven principles of surface structure parsing
in natural language. Cognition, 2:15–47.
Kohavi, R. &amp; M. Sahami (1996). Error-based and entropy-based
discretization of continuous features. In Proceedings of the
2nd International Conference on Data Mining and Knowl-
edge Discovery, Portland, Oreg., 2–4 August, 1996, pp. 114–
119.
Kruijff, G.-J., I. Kruijff-Korbayov´a, J. Bateman &amp; E. Teich
(2001). Linear order as higher-level decision: Information
structure in strategic and tactical generation. In Proceedings
of the 8th European Workshop on Natural Language Gener-
ation, Toulouse, France, 6-7 July 2001, pp. 74–83.
Kruijff-Korbayov´a, I., G.-J. Kruijff &amp; J. Bateman (2002). Gen-
eration of appropriate word order. In K. van Deemter &amp;
R. Kibble (Eds.), Information Sharing: Reference and Pre-
supposition in Language Generation and Interpretation, pp.
193–222. Stanford, Cal.: CSLI.
Kurz, D. (2000). A statistical account on word order variation
in German. In A. Abeill´e, T. Brants &amp; H. Uszkoreit (Eds.),
Proceedings of the COLING Workshop on Linguistically In-
terpreted Corpora, Luxembourg, 6 August 2000.
Langkilde, I. &amp; K. Knight (1998). Generation that exploits
corpus-based statistical knowledge. In Proceedings of the
17th International Conference on Computational Linguistics
and 36th Annual Meeting of the Association for Computa-
tional Linguistics, Montr´eal, Qu´ebec, Canada, 10–14 August
1998, pp. 704–710.
Lapata, M. (2006). Automatic evaluation of information order-
ing: Kendall’s tau. Computational Linguistics, 32(4):471–
484.
Marsi, E. &amp; E. Krahmer (2005). Explorations in sentence fu-
sion. In Proceedings of the European Workshop on Nat-
ural Language Generation, Aberdeen, Scotland, 8–10 Au-
gust, 2005, pp. 109–117.
Pappert, S., J. Schliesser, D. P. Janssen &amp; T. Pechmann (2007).
Corpus- and psycholinguistic investigations of linguistic
constraints on German word order. In A. Steube (Ed.),
The discourse potential of underspecified structures: Event
structures and information structures. Berlin, New York:
Mouton de Gruyter. In press.
Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets &amp;
S. Corston-Oliver (2004). Linguistically informed statistical
models of constituent structure for ordering in sentence real-
ization. In Proceedings of the 20th International Conference
on Computational Linguistics, Geneva, Switzerland, 23–27
August 2004, pp. 673–679.
Schmid, H. (1997). Probabilistic Part-of-Speech tagging using
decision trees. In D. Jones &amp; H. Somers (Eds.), New Methods
in Language Processing, pp. 154–164. London, UK: UCL
Press.
Sgall, P., E. Haji�cov´a &amp; J. Panevov´a (1986). The Meaning of the
Sentence in Its Semantic and Pragmatic Aspects. Dordrecht,
The Netherlands: D. Reidel.
Speyer, A. (2005). Competing constraints on Vorfeldbesetzung
in German. In Proceedings of the Constraints in Discourse
Workshop, Dortmund, 3–5 July 2005, pp. 79–87.
Telljohann, H., E. W. Hinrichs &amp; S. K¨ubler (2003). Stylebook
for the T¨ubingen treebank of written German (T¨uBa-D/Z.
Technical Report: Seminar f¨ur Sprachwissenschaft, Univer-
sit¨at T¨ubingen, T¨ubingen, Germany.
Uchimoto, K., M. Murata, Q. Ma, S. Sekine &amp; H. Isahara
(2000). Word order acquisition from corpora. In Proceedings
of the 18th International Conference on Computational Lin-
guistics, Saarbr¨ucken, Germany, 31 July – 4 August 2000,
pp. 871–877.
Uszkoreit, H. (1987). Word Order and Constituent Structure in
German. CSLI Lecture Notes. Stanford: CSLI.
Varges, S. &amp; C. Mellish (2001). Instance-based natural lan-
guage generation. In Proceedings of the 2nd Conference of
the North American Chapter of the Association for Compu-
tational Linguistics, Pittsburgh, Penn., 2–7 June, 2001, pp.
1–8.
Wan, S., R. Dale, M. Dras &amp; C. Paris (2005). Searching for
grammaticality and consistency: Propagating dependencies
in the Viterbi algorithm. In Proceedings of the 10th Euro-
pean Workshop on Natural Language Generation, Aberdeen,
Scotland, 8–10 August, 2005, pp. 211–216.
Weber, A. &amp; K. M¨uller (2004). Word order variation in Ger-
man main clauses: A corpus analysis. In Proceedings of
the 5th International Workshop on Linguistically Interpreted
Corpora, 29 August, 2004, Geneva, Switzerland, pp. 71–77.
</reference>
<page confidence="0.998328">
327
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829681">
<title confidence="0.999933">Generating Constituent Order in German Clauses</title>
<author confidence="0.999516">Katja Filippova</author>
<author confidence="0.999516">Michael Strube</author>
<affiliation confidence="0.988575">EML Research gGmbH</affiliation>
<address confidence="0.97207">Schloss-Wolfsbrunnenweg 33 69118 Heidelberg, Germany</address>
<web confidence="0.905914">http://www.eml-research.de/nlp</web>
<abstract confidence="0.998150076923077">We investigate the factors which determine constituent order in German clauses and propose an algorithm which performs the task in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="945" citStr="Barzilay &amp; McKeown, 2005" startWordPosition="131" endWordPosition="135"> in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these. 1 Introduction Many natural languages allow variation in the word order. This is a challenge for natural language generation and machine translation systems, or for text summarizers. E.g., in text-to-text generation (Barzilay &amp; McKeown, 2005; Marsi &amp; Krahmer, 2005; Wan et al., 2005), new sentences are fused from dependency structures of input sentences. The last step of sentence fusion is linearization of the resulting parse. Even for English, which is a language with fixed word order, this is not a trivial task. German has a relatively free word order. This concerns the order of constituents&apos; within sentences while the order of words within constituents is relatively rigid. The grammar only partially prescribes how constituents dependent on the verb should be ordered, and for many clauses each of the n! possible permutations of </context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Barzilay, R. &amp; K. R. McKeown (2005). Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT – A statistical Part-of-Speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing,</booktitle>
<volume>4</volume>
<pages>224--231</pages>
<location>Seattle, Wash.,</location>
<contexts>
<context position="11606" citStr="Brants, 2000" startWordPosition="1886" endWordPosition="1887">omprises the following steps: First, a list of people of a certain Wikipedia category is taken and an article is extracted for every person. Second, sentence 3http://de.wikipedia.org n−iY j=1 = n−1Y� i=1 n−1Y i=1 n−iY j=1 P(C|h) = 322 entwickelte Lummer SUBJ (pers) außerdem ADV (conn) eine Quecksilberdampflampe OBJA um herzustellen SUB monochromatisches Licht Figure 1: The representation of the sentence in Example 1 boundaries are identified with a Perl CPAN module4 whose performance we improved by extending the list of abbreviations. Next, the sentences are split into tokens. The TnT tagger (Brants, 2000) and the TreeTagger (Schmid, 1997) are used for tagging and lemmatization. Finally, the articles are parsed with the CDG dependency parser (Foth &amp; Menzel, 2006). Named entities are classified according to their semantic type using lists and category information from Wikipedia: person (pers), location (loc), organization (org), or undefined named entity (undef ne). Temporal expressions (Oktober 1915, danach (after that) etc.) are identified automatically by a set of patterns. Inevitable during automatic annotation, errors at one of the preprocessing stages cause errors at the ordering stage. Di</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Brants, T. (2000). TnT – A statistical Part-of-Speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, Wash., 29 April – 4 May 2000, pp. 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chafe</author>
</authors>
<title>Givenness, contrastiveness, definiteness, subjects, topics, and point of view.</title>
<date>1976</date>
<booktitle>In C. Li (Ed.), Subject and Topic,</booktitle>
<pages>25--55</pages>
<publisher>Academic Press.</publisher>
<location>New York, N.Y.:</location>
<contexts>
<context position="4628" citStr="Chafe (1976)" startWordPosition="703" endWordPosition="704">ght’. those with an indefinite one (Weber &amp; M¨uller, 2.2 Our Hypothesis 2004); The essential contribution of our study is that we • pronominalized constituents precede non- treat preverbal and postverbal parts of the sentence pronominalized ones (Kempen &amp; Harbusch, differently. The sentence-initial position, which in 2004); German is the VF, has been shown to be cognitively • animate referents precede inanimate ones (Pap- more prominent than other positions (Gernsbacher pert et al., 2007); &amp; Hargreaves, 1988). Motivated by the theoretical • short constituents precede longer ones (Kim- work by Chafe (1976) and Jacobs (2001), we view ball, 1973); the VF as the place for elements which modify the • the preferred topic position is right after the situation described in the sentence, i.e. for so called verb (Frey, 2004); frame-setting topics (Jacobs, 2001). For example, • the initial position is usually occupied by temporal or locational constituents, or anaphoric adscene-setting elements and topics (Speyer, verbs are good candidates for the VF. We hypoth2005). esize that the reasons which bring a constituent to • there is a default order based on semantic prop- the VF are different from those whic</context>
</contexts>
<marker>Chafe, 1976</marker>
<rawString>Chafe, W. (1976). Givenness, contrastiveness, definiteness, subjects, topics, and point of view. In C. Li (Ed.), Subject and Topic, pp. 25–55. New York, N.Y.: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology,</booktitle>
<pages>2707--2710</pages>
<location>Rhodes,</location>
<contexts>
<context position="16077" citStr="Clarkson &amp; Rosenfeld, 1997" startWordPosition="2640" endWordPosition="2643">tion, its probability is estimated according to Formula (1). The binary classifier, whose task was to predict the probability that the order of a pair of constituents is correct, was trained on the following features describing the verb or hc – the head of a constituent c9: vlex, vpass, vmod the lemma of the root of the clause (non-auxiliary verb), the voice of the verb and the number of constituents to order; lex the lemma of hc or, if hc is a functional word, the lemma of the word which depends on it; pos part-of-speech tag of hc; 6E.g. DejDet, Coords, Possr, werden 7We use the CMU Toolkit (Clarkson &amp; Rosenfeld, 1997). 8http://opennlp.sourceforge.net 9We disregarded features which use information specific to Japanese and non-applicable to German (e.g. on postpositional particles). sem if defined, the semantic class of c; e.g. im April 1900 and mit Albert Einstein (with Albert Einstein) are classified temp and pers respectively; syn, same the syntactic function of hc and whether it is the same for the two constituents; mod number of modifiers of hc; rep whether hc appears in the preceding sentence; pro whether c contains a (anaphoric) pronoun. 5.4 Maximum Entropy The first configuration of our system is an </context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Clarkson, P. &amp; R. Rosenfeld (1997). Statistical language modeling using the CMU-Cambridge toolkit. In Proceedings of the 5th European Conference on Speech Communication and Technology, Rhodes, Greece, 22-25 September 1997, pp. 2707–2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Foth</author>
<author>W Menzel</author>
</authors>
<title>Hybrid parsing: Using probabilistic models as predictors for a symbolic parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>17</volume>
<pages>321--327</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="11766" citStr="Foth &amp; Menzel, 2006" startWordPosition="1910" endWordPosition="1913">ence 3http://de.wikipedia.org n−iY j=1 = n−1Y� i=1 n−1Y i=1 n−iY j=1 P(C|h) = 322 entwickelte Lummer SUBJ (pers) außerdem ADV (conn) eine Quecksilberdampflampe OBJA um herzustellen SUB monochromatisches Licht Figure 1: The representation of the sentence in Example 1 boundaries are identified with a Perl CPAN module4 whose performance we improved by extending the list of abbreviations. Next, the sentences are split into tokens. The TnT tagger (Brants, 2000) and the TreeTagger (Schmid, 1997) are used for tagging and lemmatization. Finally, the articles are parsed with the CDG dependency parser (Foth &amp; Menzel, 2006). Named entities are classified according to their semantic type using lists and category information from Wikipedia: person (pers), location (loc), organization (org), or undefined named entity (undef ne). Temporal expressions (Oktober 1915, danach (after that) etc.) are identified automatically by a set of patterns. Inevitable during automatic annotation, errors at one of the preprocessing stages cause errors at the ordering stage. Distinguishing between main and subordinate clauses, we split the total of about 19 000 sentences into training, development and test sets (Table 1). Clauses with</context>
</contexts>
<marker>Foth, Menzel, 2006</marker>
<rawString>Foth, K. &amp; W. Menzel (2006). Hybrid parsing: Using probabilistic models as predictors for a symbolic parser. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia, 17– 21 July 2006, pp. 321–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Frey</author>
</authors>
<title>A medial topic position for German. Linguistische Berichte,</title>
<date>2004</date>
<contexts>
<context position="4842" citStr="Frey, 2004" startWordPosition="741" endWordPosition="742">e sentence pronominalized ones (Kempen &amp; Harbusch, differently. The sentence-initial position, which in 2004); German is the VF, has been shown to be cognitively • animate referents precede inanimate ones (Pap- more prominent than other positions (Gernsbacher pert et al., 2007); &amp; Hargreaves, 1988). Motivated by the theoretical • short constituents precede longer ones (Kim- work by Chafe (1976) and Jacobs (2001), we view ball, 1973); the VF as the place for elements which modify the • the preferred topic position is right after the situation described in the sentence, i.e. for so called verb (Frey, 2004); frame-setting topics (Jacobs, 2001). For example, • the initial position is usually occupied by temporal or locational constituents, or anaphoric adscene-setting elements and topics (Speyer, verbs are good candidates for the VF. We hypoth2005). esize that the reasons which bring a constituent to • there is a default order based on semantic prop- the VF are different from those which place it, say, erties of constituents (Sgall et al., 1986): to the beginning of the MF, for the order in the MF Actor &lt; Temporal &lt; SpaceLocative &lt; Means &lt; Ad- has been shown to be relatively rigid (Keller, 2000; </context>
</contexts>
<marker>Frey, 2004</marker>
<rawString>Frey, W. (2004). A medial topic position for German. Linguistische Berichte, 198:153–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Gernsbacher</author>
<author>D J Hargreaves</author>
</authors>
<title>Accessing sentence participants: The advantage of first mention.</title>
<date>1988</date>
<journal>Journal of Memory and Language,</journal>
<pages>27--699</pages>
<marker>Gernsbacher, Hargreaves, 1988</marker>
<rawString>Gernsbacher, M. A. &amp; D. J. Hargreaves (1988). Accessing sentence participants: The advantage of first mention. Journal of Memory and Language, 27:699–717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>Introduction to Functional Grammar.</title>
<date>1985</date>
<publisher>Arnold.</publisher>
<location>London, UK:</location>
<contexts>
<context position="7550" citStr="Halliday, 1985" startWordPosition="1189" endWordPosition="1190">and then arranges the remaining 321 constituents in the MF with respect to their salience performs better than algorithms which generate the order for a sentence as a whole. 3 Related Work Uszkoreit (1987) addresses the problem from a mostly grammar-based perspective and suggests weighted constraints, such as [+NOM] � [+DAT], [+PRO] � [–PRO], [–FOCUS] � [+FOCUS], etc. Kruijff et al. (2001) describe an architecture which supports generating the appropriate word order for different languages. Inspired by the findings of the Prague School (Sgall et al., 1986) and Systemic Functional Linguistics (Halliday, 1985), they focus on the role that information structure plays in constituent ordering. Kruijff-Korbayov´a et al. (2002) address the task of word order generation in the same vein. Similar to ours, their algorithm recognizes the special role of the sentence-initial position which they reserve for the theme – the point of departure of the message. Unfortunately, they did not implement their algorithm, and it is hard to judge how well the system would perform on real data. Harbusch et al. (2006) present a generation workbench, which has the goal of producing not the most appropriate order, but all gr</context>
</contexts>
<marker>Halliday, 1985</marker>
<rawString>Halliday, M. A. K. (1985). Introduction to Functional Grammar. London, UK: Arnold.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Harbusch</author>
<author>G Kempen</author>
<author>C van Breugel</author>
<author>U Koch</author>
</authors>
<title>A generation-oriented workbench for performance grammar: Capturing linear order variability in German and Dutch.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop on Natural Language Generation,</booktitle>
<pages>9--11</pages>
<location>Sydney, Australia,</location>
<marker>Harbusch, Kempen, van Breugel, Koch, 2006</marker>
<rawString>Harbusch, K., G. Kempen, C. van Breugel &amp; U. Koch (2006). A generation-oriented workbench for performance grammar: Capturing linear order variability in German and Dutch. In Proceedings of the International Workshop on Natural Language Generation, Sydney, Australia, 15-16 July 2006, pp. 9–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jacobs</author>
</authors>
<title>The dimensions of topic-comment.</title>
<date>2001</date>
<journal>Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="4646" citStr="Jacobs (2001)" startWordPosition="706" endWordPosition="707">an indefinite one (Weber &amp; M¨uller, 2.2 Our Hypothesis 2004); The essential contribution of our study is that we • pronominalized constituents precede non- treat preverbal and postverbal parts of the sentence pronominalized ones (Kempen &amp; Harbusch, differently. The sentence-initial position, which in 2004); German is the VF, has been shown to be cognitively • animate referents precede inanimate ones (Pap- more prominent than other positions (Gernsbacher pert et al., 2007); &amp; Hargreaves, 1988). Motivated by the theoretical • short constituents precede longer ones (Kim- work by Chafe (1976) and Jacobs (2001), we view ball, 1973); the VF as the place for elements which modify the • the preferred topic position is right after the situation described in the sentence, i.e. for so called verb (Frey, 2004); frame-setting topics (Jacobs, 2001). For example, • the initial position is usually occupied by temporal or locational constituents, or anaphoric adscene-setting elements and topics (Speyer, verbs are good candidates for the VF. We hypoth2005). esize that the reasons which bring a constituent to • there is a default order based on semantic prop- the VF are different from those which place it, say, e</context>
</contexts>
<marker>Jacobs, 2001</marker>
<rawString>Jacobs, J. (2001). The dimensions of topic-comment. Linguistics, 39(4):641–681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
</authors>
<date>2000</date>
<booktitle>Gradience in Grammar: Experimental and Computational Aspects of Degrees of Grammaticality, (Ph.D. thesis).</booktitle>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="3729" citStr="Keller, 2000" startWordPosition="574" endWordPosition="575"> observed that there are preferences for a particular order. The preferences summarized below have mo320 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320–327, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics tivated our choice of features: (1) [Außerdem] entwickelte [Lummer eine • constituents in the nominative case precede Apart from that developed Lummer a those in other cases, and dative constituents Quecksilberdampflampe, um often precede those in the accusative case Mercury-vapor lamp to (Uszkoreit, 1987; Keller, 2000); monochromatisches Licht herzustellen]. • the verb arguments’ order depends on the monochrome light produce. verb’s subcategorization properties (Kurz, ’Apart from that, Lummer developed a 2000); Mercury-vapor lamp to produce monochrome • constituents with a definite article precede light’. those with an indefinite one (Weber &amp; M¨uller, 2.2 Our Hypothesis 2004); The essential contribution of our study is that we • pronominalized constituents precede non- treat preverbal and postverbal parts of the sentence pronominalized ones (Kempen &amp; Harbusch, differently. The sentence-initial position, whi</context>
<context position="5440" citStr="Keller, 2000" startWordPosition="843" endWordPosition="844">b (Frey, 2004); frame-setting topics (Jacobs, 2001). For example, • the initial position is usually occupied by temporal or locational constituents, or anaphoric adscene-setting elements and topics (Speyer, verbs are good candidates for the VF. We hypoth2005). esize that the reasons which bring a constituent to • there is a default order based on semantic prop- the VF are different from those which place it, say, erties of constituents (Sgall et al., 1986): to the beginning of the MF, for the order in the MF Actor &lt; Temporal &lt; SpaceLocative &lt; Means &lt; Ad- has been shown to be relatively rigid (Keller, 2000; dressee &lt; Patient &lt; Source &lt; Destination &lt; Purpose Kempen &amp; Harbusch, 2004). Speakers have the Note that most of these preferences were identified freedom of selecting the outgoing point for a senin corpus studies and experiments with native speak- tence. Once they have selected it, the remaining coners and concern the order of verb arguments only. stituents are arranged in the MF, mainly according to Little has been said so far about how non-arguments their grammatical properties. should be ordered. This last observation motivates another hypotheGerman is a verb second language, i.e., the p</context>
</contexts>
<marker>Keller, 2000</marker>
<rawString>Keller, F. (2000). Gradience in Grammar: Experimental and Computational Aspects of Degrees of Grammaticality, (Ph.D. thesis). University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kempen</author>
<author>K Harbusch</author>
</authors>
<title>How flexible is constituent order in the midfield of German subordinate clauses? A corpus study revealing unexpected rigidity.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Linguistic Evidence,</booktitle>
<pages>81--85</pages>
<location>T¨ubingen,</location>
<contexts>
<context position="5517" citStr="Kempen &amp; Harbusch, 2004" startWordPosition="854" endWordPosition="857"> • the initial position is usually occupied by temporal or locational constituents, or anaphoric adscene-setting elements and topics (Speyer, verbs are good candidates for the VF. We hypoth2005). esize that the reasons which bring a constituent to • there is a default order based on semantic prop- the VF are different from those which place it, say, erties of constituents (Sgall et al., 1986): to the beginning of the MF, for the order in the MF Actor &lt; Temporal &lt; SpaceLocative &lt; Means &lt; Ad- has been shown to be relatively rigid (Keller, 2000; dressee &lt; Patient &lt; Source &lt; Destination &lt; Purpose Kempen &amp; Harbusch, 2004). Speakers have the Note that most of these preferences were identified freedom of selecting the outgoing point for a senin corpus studies and experiments with native speak- tence. Once they have selected it, the remaining coners and concern the order of verb arguments only. stituents are arranged in the MF, mainly according to Little has been said so far about how non-arguments their grammatical properties. should be ordered. This last observation motivates another hypotheGerman is a verb second language, i.e., the po- sis we make: The cumulation of the properties of sition of the verb in the</context>
</contexts>
<marker>Kempen, Harbusch, 2004</marker>
<rawString>Kempen, G. &amp; K. Harbusch (2004). How flexible is constituent order in the midfield of German subordinate clauses? A corpus study revealing unexpected rigidity. In Proceedings of the International Conference on Linguistic Evidence, T¨ubingen, Germany, 29–31 January 2004, pp. 81–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kimball</author>
</authors>
<title>Seven principles of surface structure parsing in natural language.</title>
<date>1973</date>
<journal>Cognition,</journal>
<pages>2--15</pages>
<marker>Kimball, 1973</marker>
<rawString>Kimball, J. (1973). Seven principles of surface structure parsing in natural language. Cognition, 2:15–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kohavi</author>
<author>M Sahami</author>
</authors>
<title>Error-based and entropy-based discretization of continuous features.</title>
<date>1996</date>
<booktitle>In Proceedings of the 2nd International Conference on Data Mining and Knowledge Discovery,</booktitle>
<pages>114--119</pages>
<location>Portland, Oreg.,</location>
<contexts>
<context position="17227" citStr="Kohavi &amp; Sahami, 1996" startWordPosition="2827" endWordPosition="2830">ronoun. 5.4 Maximum Entropy The first configuration of our system is an extended version of the UCHIMOTO baseline (MAXENT). To the features describing c we added the following ones: det the kind of determiner modifying hc (def, indef, non-appl); rel whether hc is modified by a relative clause (yes, no, non-appl); dep the depth of c; len the length of c in words. The first two features describe the discourse status of a constituent; the other two provide information on its “weight”. Since our learner treats all values as nominal, we discretized the values of dep and len with a C4.5 classifier (Kohavi &amp; Sahami, 1996). Another modification concerns the efficiency of the algorithm. Instead of calculating probabilities for all pairs, we obtain the right order from a random one by sorting. We compare adjacent elements by consulting the learner as if we would sort an array of numbers. Given two adjacent constituents, ci &lt; cj, we check the probability of their being in the right order, i.e. that ci precedes cj: Ppre(ci, cj). If it is less than 0.5, we transpose the two and compare ci with the next one. Since the sorting method presupposes that the predicted relation is transitive, we checked whether this is rea</context>
</contexts>
<marker>Kohavi, Sahami, 1996</marker>
<rawString>Kohavi, R. &amp; M. Sahami (1996). Error-based and entropy-based discretization of continuous features. In Proceedings of the 2nd International Conference on Data Mining and Knowledge Discovery, Portland, Oreg., 2–4 August, 1996, pp. 114– 119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-J Kruijff</author>
<author>I Kruijff-Korbayov´a</author>
<author>J Bateman</author>
<author>E Teich</author>
</authors>
<title>Linear order as higher-level decision: Information structure in strategic and tactical generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th European Workshop on Natural Language Generation,</booktitle>
<pages>74--83</pages>
<location>Toulouse,</location>
<marker>Kruijff, Kruijff-Korbayov´a, Bateman, Teich, 2001</marker>
<rawString>Kruijff, G.-J., I. Kruijff-Korbayov´a, J. Bateman &amp; E. Teich (2001). Linear order as higher-level decision: Information structure in strategic and tactical generation. In Proceedings of the 8th European Workshop on Natural Language Generation, Toulouse, France, 6-7 July 2001, pp. 74–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kruijff-Korbayov´a</author>
<author>G-J Kruijff</author>
<author>J Bateman</author>
</authors>
<title>Generation of appropriate word order.</title>
<date>2002</date>
<booktitle>In K. van Deemter &amp; R. Kibble (Eds.), Information Sharing: Reference and Presupposition in Language Generation and Interpretation,</booktitle>
<pages>193--222</pages>
<publisher>CSLI.</publisher>
<location>Stanford, Cal.:</location>
<marker>Kruijff-Korbayov´a, Kruijff, Bateman, 2002</marker>
<rawString>Kruijff-Korbayov´a, I., G.-J. Kruijff &amp; J. Bateman (2002). Generation of appropriate word order. In K. van Deemter &amp; R. Kibble (Eds.), Information Sharing: Reference and Presupposition in Language Generation and Interpretation, pp. 193–222. Stanford, Cal.: CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kurz</author>
</authors>
<title>A statistical account on word order variation in German. In</title>
<date>2000</date>
<booktitle>Proceedings of the COLING Workshop on Linguistically Interpreted Corpora,</booktitle>
<location>Luxembourg, 6</location>
<marker>Kurz, 2000</marker>
<rawString>Kurz, D. (2000). A statistical account on word order variation in German. In A. Abeill´e, T. Brants &amp; H. Uszkoreit (Eds.), Proceedings of the COLING Workshop on Linguistically Interpreted Corpora, Luxembourg, 6 August 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>704--710</pages>
<location>Montr´eal, Qu´ebec, Canada, 10–14</location>
<contexts>
<context position="2601" citStr="Langkilde &amp; Knight (1998)" startWordPosition="404" endWordPosition="407">ich orders not only verb arguments but all kinds of constituents, and evaluate it on a corpus of biographies. For each parsed sentence in the test set, our maximumentropy-based algorithm aims at reproducing the order found in the original text. We investigate the importance of different linguistic factors and suggest an algorithm to constituent ordering which first determines the sentence initial constituent and then orders the remaining ones. We provide evidence that the task requires language-specific knowledge to achieve better results and point to the most difficult part of it. Similar to Langkilde &amp; Knight (1998) we utilize statistical methods. Unlike overgeneration approaches (Varges &amp; Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation. 2 Theoretical Premises 2.1 Background It has been suggested that several factors have an influence on German constituent order. Apart from the constraints posed by the grammar, information structure, surface form, and discourse status have also been shown to play a role. It has also been observed that there are preferences for a particular order. The preferences summariz</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, I. &amp; K. Knight (1998). Generation that exploits corpus-based statistical knowledge. In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics, Montr´eal, Qu´ebec, Canada, 10–14 August 1998, pp. 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendall’s tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>484</pages>
<contexts>
<context position="19032" citStr="Lapata, 2006" startWordPosition="3145" endWordPosition="3146">py learner for a binary classification (VF vs. MF), we select the constituent c with the highest probability of being in the VF. 2. For the MF, the remaining constituents are put into a random order and then sorted the way it is done for MAXENT. The training data for the second task was generated only from the MF of clauses. 6 Results 6.1 Evaluation Metrics We use several metrics to evaluate our systems and the baselines. The first is per-sentence accuracy (acc) which is the proportion of correctly regenerated sentences. Kendall’s T, which has been used for evaluating sentence ordering tasks (Lapata, 2006), is the second metric we use. T is calculated as 1−4 t N(N�1),where t is the number of interchanges of consecutive elements to arrange N elements in the right order. T is sensitive to near misses and assigns abdc (almost correct order) a score of 0.66 while dcba (inverse order) gets −1. Note that it is questionable whether this metric is as appropriate for word ordering tasks as for sentence ordering ones because a near miss might turn out to be ungrammatical whereas a more different order stays acceptable. Apart from acc and T, we also adopt the metrics used by Uchimoto et al. (2000) and Rin</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Lapata, M. (2006). Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4):471– 484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Marsi</author>
<author>E Krahmer</author>
</authors>
<title>Explorations in sentence fusion.</title>
<date>2005</date>
<booktitle>In Proceedings of the European Workshop on Natural Language Generation,</booktitle>
<pages>109--117</pages>
<location>Aberdeen, Scotland, 8–10</location>
<contexts>
<context position="968" citStr="Marsi &amp; Krahmer, 2005" startWordPosition="136" endWordPosition="139">best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these. 1 Introduction Many natural languages allow variation in the word order. This is a challenge for natural language generation and machine translation systems, or for text summarizers. E.g., in text-to-text generation (Barzilay &amp; McKeown, 2005; Marsi &amp; Krahmer, 2005; Wan et al., 2005), new sentences are fused from dependency structures of input sentences. The last step of sentence fusion is linearization of the resulting parse. Even for English, which is a language with fixed word order, this is not a trivial task. German has a relatively free word order. This concerns the order of constituents&apos; within sentences while the order of words within constituents is relatively rigid. The grammar only partially prescribes how constituents dependent on the verb should be ordered, and for many clauses each of the n! possible permutations of n constituents is gramm</context>
</contexts>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>Marsi, E. &amp; E. Krahmer (2005). Explorations in sentence fusion. In Proceedings of the European Workshop on Natural Language Generation, Aberdeen, Scotland, 8–10 August, 2005, pp. 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pappert</author>
<author>J Schliesser</author>
<author>D P Janssen</author>
<author>T Pechmann</author>
</authors>
<title>Corpus- and psycholinguistic investigations of linguistic constraints on German word order. In</title>
<date>2007</date>
<location>Berlin, New York: Mouton</location>
<note>In press.</note>
<marker>Pappert, Schliesser, Janssen, Pechmann, 2007</marker>
<rawString>Pappert, S., J. Schliesser, D. P. Janssen &amp; T. Pechmann (2007). Corpus- and psycholinguistic investigations of linguistic constraints on German word order. In A. Steube (Ed.), The discourse potential of underspecified structures: Event structures and information structures. Berlin, New York: Mouton de Gruyter. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>M Gamon</author>
<author>R C Moore</author>
<author>D Rojas</author>
<author>M Smets</author>
<author>S Corston-Oliver</author>
</authors>
<title>Linguistically informed statistical models of constituent structure for ordering in sentence realization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>673--679</pages>
<location>Geneva,</location>
<contexts>
<context position="9301" citStr="Ringger et al. (2004)" startWordPosition="1497" endWordPosition="1500">r from the set of all permutations of n words by the following formula: P(1|h) = P({Wi,i+j = 1|1 &lt; i &lt; n − 1, 1 &lt; j &lt; n − il|h) P(Wi,i+j = 1|hi,i+j) PME(1|hi,i+j) (1) For each permutation, for every pair of words , they multiply the probability of their being in the correct2 order given the history h. Random variable Wi,i+j 2Only reference orders are assumed to be correct. is 1 if word wi precedes wi+j in the reference sentence, 0 otherwise. The features they use are akin to those which play a role in determining German word order. We use their approach as a non-trivial baseline in our study. Ringger et al. (2004) aim at regenerating the order of constituents as well as the order within them for German and French technical manuals. Utilizing syntactic, semantic, sub-categorization and length features, they test several statistical models to find the order which maximizes the probability of an ordered tree. Using “Markov grammars” as the starting point and conditioning on the syntactic category only, they expand a non-terminal node C by predicting its daughters from left to right: P(C|h) = Yn P(di|di−1, ..., di−j, c, h) (2) i=1 Here, c is the syntactic category of C, d and h are the syntactic categories</context>
<context position="13711" citStr="Ringger et al. (2004)" startWordPosition="2226" endWordPosition="2229">at we predict neither the position of the verb, nor the order within constituents as the former is explicitly determined by the grammar, and the latter is much more rigid than the order of constituents. 5 Baselines and Algorithms We compare the performance of two our algorithms with four baselines. 5.1 Random We improve a trivial random baseline (RAND) by two syntax-oriented rules: the first position is reserved for the subject and the second for the direct object if there is any; the order of the remaining constituents is generated randomly (RAND IMP). 5.2 Statistical Bigram Model Similar to Ringger et al. (2004), we find the order with the highest probability conditioned on syntactic and semantic categories. Unlike them we use dependency parses and compute the probability of the top node only, which is modified by all constituents. With these adjustments the probability of an order O given the history h, if conditioned on syntactic functions of constituents (s1...sn), is simply: n P(Olh) = P(si|si−1, h) (4) i=1 Ringger et al. (2004) do not make explicit, what their set of semantic relations consists of. From the 5OBJA stands for the accusative object. 323 example in the paper, it seems that these are</context>
<context position="19650" citStr="Ringger et al. (2004)" startWordPosition="3254" endWordPosition="3257">06), is the second metric we use. T is calculated as 1−4 t N(N�1),where t is the number of interchanges of consecutive elements to arrange N elements in the right order. T is sensitive to near misses and assigns abdc (almost correct order) a score of 0.66 while dcba (inverse order) gets −1. Note that it is questionable whether this metric is as appropriate for word ordering tasks as for sentence ordering ones because a near miss might turn out to be ungrammatical whereas a more different order stays acceptable. Apart from acc and T, we also adopt the metrics used by Uchimoto et al. (2000) and Ringger et al. (2004). The former use agreement rate (agr) calculated as 2p N(N�1): the number of correctly ordered pairs of constituents over the total number of all possible pairs, as well as complete agreement which is basically per-sentence accuracy. Unlike T, which has −1 as the lowest score, agr ranges from 0 to 1. Ringger et al. (2004) evaluate the performance only in terms of per-constituent edit distance calculated as mN , where m is the minimum number of moves11 10Since subordinate clauses do not have a VF, the first step is not needed. 11A move is a deletion combined with an insertion. needed to arrange</context>
<context position="22841" citStr="Ringger et al. (2004)" startWordPosition="3814" endWordPosition="3817">the 325 cases in the VF. This implies that the order in the MF mainly depends on grammatical features, while for the VF all features are important because removal of any feature caused a loss in accuracy. Table 4: Mean of the results for the VF and the MF Another important finding is that there is no need to overgenerate to find the right order. Insignificant for clauses with two or three constituents, for clauses with 10 constituents, the number of comparisons is reduced drastically from 163,296,000 to 45. According to the inv metric, our results are considerably worse than those reported by Ringger et al. (2004). As mentioned in Section 3, the fact that they generate the order for every non-terminal node seriously inflates their numbers. Apart from that, they do not report accuracy, and it is unknown, how many sentences they actually reproduced correctly. 6.3 Error Analysis To reveal the main error sources, we analyzed incorrect predictions concerning the VF and the MF, one hundred for each. Most errors in the VF did not lead to unacceptability or ungrammaticality. From lexical and semantic features, the classifier learned that some expressions are often used in the beginning of a sentence. These are</context>
</contexts>
<marker>Ringger, Gamon, Moore, Rojas, Smets, Corston-Oliver, 2004</marker>
<rawString>Ringger, E., M. Gamon, R. C. Moore, D. Rojas, M. Smets &amp; S. Corston-Oliver (2004). Linguistically informed statistical models of constituent structure for ordering in sentence realization. In Proceedings of the 20th International Conference on Computational Linguistics, Geneva, Switzerland, 23–27 August 2004, pp. 673–679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech tagging using decision trees.</title>
<date>1997</date>
<booktitle>In D. Jones &amp; H. Somers (Eds.), New Methods in Language Processing,</booktitle>
<pages>154--164</pages>
<publisher>UCL Press.</publisher>
<location>London, UK:</location>
<contexts>
<context position="11640" citStr="Schmid, 1997" startWordPosition="1891" endWordPosition="1892">t, a list of people of a certain Wikipedia category is taken and an article is extracted for every person. Second, sentence 3http://de.wikipedia.org n−iY j=1 = n−1Y� i=1 n−1Y i=1 n−iY j=1 P(C|h) = 322 entwickelte Lummer SUBJ (pers) außerdem ADV (conn) eine Quecksilberdampflampe OBJA um herzustellen SUB monochromatisches Licht Figure 1: The representation of the sentence in Example 1 boundaries are identified with a Perl CPAN module4 whose performance we improved by extending the list of abbreviations. Next, the sentences are split into tokens. The TnT tagger (Brants, 2000) and the TreeTagger (Schmid, 1997) are used for tagging and lemmatization. Finally, the articles are parsed with the CDG dependency parser (Foth &amp; Menzel, 2006). Named entities are classified according to their semantic type using lists and category information from Wikipedia: person (pers), location (loc), organization (org), or undefined named entity (undef ne). Temporal expressions (Oktober 1915, danach (after that) etc.) are identified automatically by a set of patterns. Inevitable during automatic annotation, errors at one of the preprocessing stages cause errors at the ordering stage. Distinguishing between main and subo</context>
</contexts>
<marker>Schmid, 1997</marker>
<rawString>Schmid, H. (1997). Probabilistic Part-of-Speech tagging using decision trees. In D. Jones &amp; H. Somers (Eds.), New Methods in Language Processing, pp. 154–164. London, UK: UCL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sgall</author>
<author>E Haji�cov´a</author>
<author>J Panevov´a</author>
</authors>
<title>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<location>Dordrecht, The Netherlands: D. Reidel.</location>
<marker>Sgall, Haji�cov´a, Panevov´a, 1986</marker>
<rawString>Sgall, P., E. Haji�cov´a &amp; J. Panevov´a (1986). The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. Dordrecht, The Netherlands: D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Speyer</author>
</authors>
<title>Competing constraints on Vorfeldbesetzung in German.</title>
<date>2005</date>
<booktitle>In Proceedings of the Constraints in Discourse Workshop, Dortmund, 3–5</booktitle>
<pages>79--87</pages>
<marker>Speyer, 2005</marker>
<rawString>Speyer, A. (2005). Competing constraints on Vorfeldbesetzung in German. In Proceedings of the Constraints in Discourse Workshop, Dortmund, 3–5 July 2005, pp. 79–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Telljohann</author>
<author>E W Hinrichs</author>
<author>S K¨ubler</author>
</authors>
<title>Stylebook for the T¨ubingen treebank of written German (T¨uBa-D/Z. Technical Report: Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen,</title>
<date>2003</date>
<location>T¨ubingen, Germany.</location>
<marker>Telljohann, Hinrichs, K¨ubler, 2003</marker>
<rawString>Telljohann, H., E. W. Hinrichs &amp; S. K¨ubler (2003). Stylebook for the T¨ubingen treebank of written German (T¨uBa-D/Z. Technical Report: Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen, T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
<author>M Murata</author>
<author>Q Ma</author>
<author>S Sekine</author>
<author>H Isahara</author>
</authors>
<title>Word order acquisition from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics, Saarbr¨ucken, Germany, 31 July – 4</booktitle>
<pages>871--877</pages>
<contexts>
<context position="8247" citStr="Uchimoto et al. (2000)" startWordPosition="1303" endWordPosition="1306">dering. Kruijff-Korbayov´a et al. (2002) address the task of word order generation in the same vein. Similar to ours, their algorithm recognizes the special role of the sentence-initial position which they reserve for the theme – the point of departure of the message. Unfortunately, they did not implement their algorithm, and it is hard to judge how well the system would perform on real data. Harbusch et al. (2006) present a generation workbench, which has the goal of producing not the most appropriate order, but all grammatical ones. They also do not provide experimental results. The work of Uchimoto et al. (2000) is done on the free word order language Japanese. They determine the order of phrasal units dependent on the same modifiee. Their approach is similar to ours in that they aim at regenerating the original order from a dependency parse, but differs in the scope of the problem as they regenerate the order of modifers for all and not only for the top clausal node. Using a maximum entropy framework, they choose the most probable order from the set of all permutations of n words by the following formula: P(1|h) = P({Wi,i+j = 1|1 &lt; i &lt; n − 1, 1 &lt; j &lt; n − il|h) P(Wi,i+j = 1|hi,i+j) PME(1|hi,i+j) (1) </context>
<context position="15422" citStr="Uchimoto et al. (2000)" startWordPosition="2523" endWordPosition="2526">he beginning (b) and the end (e) of the clause. Then we compute the bigram probabilities7. For our third baseline (BIGRAM), we select from all possible orders the one with the highest probability as calculated by the following formula: n P(O|h) = P (ti|ti−1, h) (5) i=1 where ti is from the set of joined tags. For Example 1, possible tag sequences (i.e. orders) are ’b subjpers v adv obja sub e’, ’b adv v subj-pers obja sub e’, ’b obja v adv sub subj-pers e’, etc. 5.3 Uchimoto For the fourth baseline (UCHIMOTO), we utilized a maximum entropy learner (OpenNLP8) and reimplemented the algorithm of Uchimoto et al. (2000). For every possible permutation, its probability is estimated according to Formula (1). The binary classifier, whose task was to predict the probability that the order of a pair of constituents is correct, was trained on the following features describing the verb or hc – the head of a constituent c9: vlex, vpass, vmod the lemma of the root of the clause (non-auxiliary verb), the voice of the verb and the number of constituents to order; lex the lemma of hc or, if hc is a functional word, the lemma of the word which depends on it; pos part-of-speech tag of hc; 6E.g. DejDet, Coords, Possr, werd</context>
<context position="19624" citStr="Uchimoto et al. (2000)" startWordPosition="3249" endWordPosition="3252"> ordering tasks (Lapata, 2006), is the second metric we use. T is calculated as 1−4 t N(N�1),where t is the number of interchanges of consecutive elements to arrange N elements in the right order. T is sensitive to near misses and assigns abdc (almost correct order) a score of 0.66 while dcba (inverse order) gets −1. Note that it is questionable whether this metric is as appropriate for word ordering tasks as for sentence ordering ones because a near miss might turn out to be ungrammatical whereas a more different order stays acceptable. Apart from acc and T, we also adopt the metrics used by Uchimoto et al. (2000) and Ringger et al. (2004). The former use agreement rate (agr) calculated as 2p N(N�1): the number of correctly ordered pairs of constituents over the total number of all possible pairs, as well as complete agreement which is basically per-sentence accuracy. Unlike T, which has −1 as the lowest score, agr ranges from 0 to 1. Ringger et al. (2004) evaluate the performance only in terms of per-constituent edit distance calculated as mN , where m is the minimum number of moves11 10Since subordinate clauses do not have a VF, the first step is not needed. 11A move is a deletion combined with an in</context>
</contexts>
<marker>Uchimoto, Murata, Ma, Sekine, Isahara, 2000</marker>
<rawString>Uchimoto, K., M. Murata, Q. Ma, S. Sekine &amp; H. Isahara (2000). Word order acquisition from corpora. In Proceedings of the 18th International Conference on Computational Linguistics, Saarbr¨ucken, Germany, 31 July – 4 August 2000, pp. 871–877.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Uszkoreit</author>
</authors>
<date>1987</date>
<booktitle>Word Order and Constituent Structure in German. CSLI Lecture Notes.</booktitle>
<publisher>CSLI.</publisher>
<location>Stanford:</location>
<contexts>
<context position="3714" citStr="Uszkoreit, 1987" startWordPosition="572" endWordPosition="573"> It has also been observed that there are preferences for a particular order. The preferences summarized below have mo320 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320–327, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics tivated our choice of features: (1) [Außerdem] entwickelte [Lummer eine • constituents in the nominative case precede Apart from that developed Lummer a those in other cases, and dative constituents Quecksilberdampflampe, um often precede those in the accusative case Mercury-vapor lamp to (Uszkoreit, 1987; Keller, 2000); monochromatisches Licht herzustellen]. • the verb arguments’ order depends on the monochrome light produce. verb’s subcategorization properties (Kurz, ’Apart from that, Lummer developed a 2000); Mercury-vapor lamp to produce monochrome • constituents with a definite article precede light’. those with an indefinite one (Weber &amp; M¨uller, 2.2 Our Hypothesis 2004); The essential contribution of our study is that we • pronominalized constituents precede non- treat preverbal and postverbal parts of the sentence pronominalized ones (Kempen &amp; Harbusch, differently. The sentence-initia</context>
<context position="7140" citStr="Uszkoreit (1987)" startWordPosition="1128" endWordPosition="1129">exactly one constituent, and Mit- need to generate all possible orders and rank them. telfeld (MF), where the remaining constituents are The best order can be obtained from a random one located. The subordinate clause normally has only by sorting. Our experiments support this view. A MF. The VF and MF are marked with brackets in two-step approach, which first selects the best canExample 1: didate for the VF and then arranges the remaining 321 constituents in the MF with respect to their salience performs better than algorithms which generate the order for a sentence as a whole. 3 Related Work Uszkoreit (1987) addresses the problem from a mostly grammar-based perspective and suggests weighted constraints, such as [+NOM] � [+DAT], [+PRO] � [–PRO], [–FOCUS] � [+FOCUS], etc. Kruijff et al. (2001) describe an architecture which supports generating the appropriate word order for different languages. Inspired by the findings of the Prague School (Sgall et al., 1986) and Systemic Functional Linguistics (Halliday, 1985), they focus on the role that information structure plays in constituent ordering. Kruijff-Korbayov´a et al. (2002) address the task of word order generation in the same vein. Similar to our</context>
</contexts>
<marker>Uszkoreit, 1987</marker>
<rawString>Uszkoreit, H. (1987). Word Order and Constituent Structure in German. CSLI Lecture Notes. Stanford: CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Varges</author>
<author>C Mellish</author>
</authors>
<title>Instance-based natural language generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Pittsburgh, Penn.,</location>
<contexts>
<context position="2690" citStr="Varges &amp; Mellish, 2001" startWordPosition="416" endWordPosition="419">s of biographies. For each parsed sentence in the test set, our maximumentropy-based algorithm aims at reproducing the order found in the original text. We investigate the importance of different linguistic factors and suggest an algorithm to constituent ordering which first determines the sentence initial constituent and then orders the remaining ones. We provide evidence that the task requires language-specific knowledge to achieve better results and point to the most difficult part of it. Similar to Langkilde &amp; Knight (1998) we utilize statistical methods. Unlike overgeneration approaches (Varges &amp; Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation. 2 Theoretical Premises 2.1 Background It has been suggested that several factors have an influence on German constituent order. Apart from the constraints posed by the grammar, information structure, surface form, and discourse status have also been shown to play a role. It has also been observed that there are preferences for a particular order. The preferences summarized below have mo320 Proceedings of the 45th Annual Meeting of the Association of Computat</context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>Varges, S. &amp; C. Mellish (2001). Instance-based natural language generation. In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics, Pittsburgh, Penn., 2–7 June, 2001, pp. 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>R Dale</author>
<author>M Dras</author>
<author>C Paris</author>
</authors>
<title>Searching for grammaticality and consistency: Propagating dependencies in the Viterbi algorithm.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th European Workshop on Natural Language Generation,</booktitle>
<pages>211--216</pages>
<location>Aberdeen, Scotland, 8–10</location>
<contexts>
<context position="987" citStr="Wan et al., 2005" startWordPosition="140" endWordPosition="143">initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these. 1 Introduction Many natural languages allow variation in the word order. This is a challenge for natural language generation and machine translation systems, or for text summarizers. E.g., in text-to-text generation (Barzilay &amp; McKeown, 2005; Marsi &amp; Krahmer, 2005; Wan et al., 2005), new sentences are fused from dependency structures of input sentences. The last step of sentence fusion is linearization of the resulting parse. Even for English, which is a language with fixed word order, this is not a trivial task. German has a relatively free word order. This concerns the order of constituents&apos; within sentences while the order of words within constituents is relatively rigid. The grammar only partially prescribes how constituents dependent on the verb should be ordered, and for many clauses each of the n! possible permutations of n constituents is grammatical. &apos;Henceforth</context>
</contexts>
<marker>Wan, Dale, Dras, Paris, 2005</marker>
<rawString>Wan, S., R. Dale, M. Dras &amp; C. Paris (2005). Searching for grammaticality and consistency: Propagating dependencies in the Viterbi algorithm. In Proceedings of the 10th European Workshop on Natural Language Generation, Aberdeen, Scotland, 8–10 August, 2005, pp. 211–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Weber</author>
<author>K M¨uller</author>
</authors>
<title>Word order variation in German main clauses: A corpus analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th International Workshop on Linguistically Interpreted Corpora,</booktitle>
<volume>29</volume>
<pages>71--77</pages>
<location>Geneva,</location>
<marker>Weber, M¨uller, 2004</marker>
<rawString>Weber, A. &amp; K. M¨uller (2004). Word order variation in German main clauses: A corpus analysis. In Proceedings of the 5th International Workshop on Linguistically Interpreted Corpora, 29 August, 2004, Geneva, Switzerland, pp. 71–77.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>