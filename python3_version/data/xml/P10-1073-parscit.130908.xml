<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9825615">
Kernel Based Discourse Relation Recognition with Temporal
Ordering Information
</title>
<author confidence="0.999034">
WenTing Wang1 Jian Su1 Chew Lim Tan2
</author>
<affiliation confidence="0.988704">
1Institute for Infocomm Research
</affiliation>
<address confidence="0.832815">
1 Fusionopolis Way, #21-01 Connexis
Singapore 138632
</address>
<email confidence="0.999349">
{wwang,sujian}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.9968885" genericHeader="abstract">
1 Introduction
Abstract
</sectionHeader>
<affiliation confidence="0.99578">
2Department of Computer Science
University of Singapore
</affiliation>
<address confidence="0.979315">
Singapore 117417
</address>
<email confidence="0.967938">
tacl@comp.nus.edu.sg
</email>
<bodyText confidence="0.999911579710145">
Syntactic knowledge is important for dis-
course relation recognition. Yet only heu-
ristically selected flat paths and 2-level
production rules have been used to incor-
porate such information so far. In this
paper we propose using tree kernel based
approach to automatically mine the syn-
tactic information from the parse trees for
discourse analysis, applying kernel func-
tion to the tree structures directly. These
structural syntactic features, together
with other normal flat features are incor-
porated into our composite kernel to cap-
ture diverse knowledge for simultaneous
discourse identification and classification
for both explicit and implicit relations.
The experiment shows tree kernel ap-
proach is able to give statistical signifi-
cant improvements over flat syntactic
path feature. We also illustrate that tree
kernel approach covers more structure in-
formation than the production rules,
which allows tree kernel to further incor-
porate information from a higher dimen-
sion space for possible better discrimina-
tion. Besides, we further propose to leve-
rage on temporal ordering information to
constrain the interpretation of discourse
relation, which also demonstrate statistic-
al significant improvements for discourse
relation recognition on PDTB 2.0 for
both explicit and implicit as well.
Discourse relations capture the internal structure
and logical relationship of coherent text, includ-
ing Temporal, Causal and Contrastive relations
etc. The ability of recognizing such relations be-
tween text units including identifying and classi-
fying provides important information to other
natural language processing systems, such as
language generation, document summarization,
and question answering. For example, Causal
relation can be used to answer more sophisti-
cated, non-factoid ‘Why’ questions.
Lee et al. (2006) demonstrates that modeling
discourse structure requires prior linguistic anal-
ysis on syntax. This shows the importance of
syntactic knowledge to discourse analysis. How-
ever, most of previous work only deploys lexical
and semantic features (Marcu and Echihabi,
2002; Pettibone and PonBarry, 2003; Saito et al.,
2006; Ben and James, 2007; Lin et al., 2009; Pit-
ler et al., 2009) with only two exceptions (Ben
and James, 2007; Lin et al., 2009). Nevertheless,
Ben and James (2007) only uses flat syntactic
path connecting connective and arguments in the
parse tree. The hierarchical structured informa-
tion in the trees is not well preserved in their flat
syntactic path features. Besides, such a syntactic
feature selected and defined according to linguis-
tic intuition has its limitation, as it remains un-
clear what kinds of syntactic heuristics are effec-
tive for discourse analysis.
The more recent work from Lin et al. (2009)
uses 2-level production rules to represent parse
tree information. Yet it doesn’t cover all the oth-
er sub-trees structural information which can be
also useful for the recognition.
In this paper we propose using tree kernel
based method to automatically mine the syntactic
</bodyText>
<page confidence="0.950003">
710
</page>
<note confidence="0.94267">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 710–719,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940666666666">
information from the parse trees for discourse
analysis, applying kernel function to the parse
tree structures directly. These structural syntactic
features, together with other flat features are then
incorporated into our composite kernel to capture
diverse knowledge for simultaneous discourse
identification and classification. The experiment
shows that tree kernel is able to effectively in-
corporate syntactic structural information and
produce statistical significant improvements over
flat syntactic path feature for the recognition of
both explicit and implicit relation in Penn Dis-
course Treebank (PDTB; Prasad et al., 2008).
We also illustrate that tree kernel approach cov-
ers more structure information than the produc-
tion rules, which allows tree kernel to further
work on a higher dimensional space for possible
better discrimination.
Besides, inspired by the linguistic study on
tense and discourse anaphor (Webber, 1988), we
further propose to incorporate temporal ordering
information to constrain the interpretation of dis-
course relation, which also demonstrates statis-
tical significant improvements for discourse rela-
tion recognition on PDTB v2.0 for both explicit
and implicit relations.
The organization of the rest of the paper is as
follows. We briefly introduce PDTB in Section
2. Section 3 gives the related work on tree kernel
approach in NLP and its difference with produc-
tion rules, and also linguistic study on tense and
discourse anaphor. Section 4 introduces the
frame work for discourse recognition, as well as
the baseline feature space and the SVM classifi-
er. We present our kernel-based method in Sec-
tion 5, and the usage of temporal ordering feature
in Section 6. Section 7 shows the experiments
and discussions. We conclude our works in Sec-
tion 8.
</bodyText>
<sectionHeader confidence="0.950335" genericHeader="introduction">
2 Penn Discourse Tree Bank
</sectionHeader>
<bodyText confidence="0.956793847826087">
The Penn Discourse Treebank (PDTB) is the
largest available annotated corpora of discourse
relations (Prasad et al., 2008) over 2,312 Wall
Street Journal articles. The PDTB models dis-
course relation in the predicate-argument view,
where a discourse connective (e.g., but) is treated
as a predicate taking two text spans as its argu-
ments. The argument that the discourse connec-
tive syntactically bounds to is called Arg2, and
the other argument is called Arg1.
The PDTB provides annotations for both ex-
plicit and implicit discourse relations. An explicit
relation is triggered by an explicit connective.
Example (1) shows an explicit Contrast relation
signaled by the discourse connective ‘but’.
(1). Arg1. Yesterday, the retailing and finan-
cial services giant reported a 16% drop in
third-quarter earnings to $257.5 million,
or 75 cents a share, from a restated $305
million, or 80 cents a share, a year earlier.
Arg2. But the news was even worse for
Sears&apos;s core U.S. retailing operation, the
largest in the nation.
In the PDTB, local implicit relations are also
annotated. The annotators insert a connective
expression that best conveys the inferred implicit
relation between adjacent sentences within the
same paragraph. In Example (2), the annotators
select ‘because’ as the most appropriate connec-
tive to express the inferred Causal relation be-
tween the sentences. There is one special label
AltLex pre-defined for cases where the insertion
of an Implicit connective to express an inferred
relation led to a redundancy in the expression of
the relation. In Example (3), the Causal relation
derived between sentences is alternatively lexi-
calized by some non-connective expression
shown in square brackets, so no implicit connec-
tive is inserted. In our experiments, we treat Alt-
Lex Relations the same way as normal Implicit
relations.
(2). Arg1. Some have raised their cash posi-
tions to record levels.
Arg2. Implicit = Because High cash po-
sitions help buffer a fund when the market
falls.
</bodyText>
<listItem confidence="0.815624285714286">
(3). Arg1. Ms. Bartlett’s previous work,
which earned her an international reputa-
tion in the non-horticultural art world, of-
ten took gardens as its nominal subject.
Arg2. [Mayhap this metaphorical con-
nection made] the BPC Fine Arts Com-
mittee think she had a literal green thumb.
</listItem>
<bodyText confidence="0.935414285714286">
The PDTB also captures two non-implicit cas-
es: (a) Entity relation where the relation between
adjacent sentences is based on entity coherence
(Knott et al., 2001) as in Example (4); and (b) No
relation where no discourse or entity-based cohe-
rence relation can be inferred between adjacent
sentences.
</bodyText>
<page confidence="0.995691">
711
</page>
<bodyText confidence="0.994196411764706">
(4). But for South Garden, the grid was to be
a 3-D network of masonry or hedge walls
with real plants inside them.
In a Letter to the BPCA, kelly/varnell
called this “arbitrary and amateurish.”
Each Explicit, Implicit and AltLex relation is
annotated with a sense. The senses in PDTB are
arranged in a three-level hierarchy. The top level
has four tags representing four major semantic
classes: Temporal, Contingency, Comparison
and Expansion. For each class, a second level of
types is defined to further refine the semantic of
the class levels. For example, Contingency has
two types Cause and Condition. A third level of
subtype specifies the semantic contribution of
each argument. In our experiments, we use only
the top level of the sense annotations.
</bodyText>
<sectionHeader confidence="0.995591" genericHeader="related work">
3 Related Work
</sectionHeader>
<figure confidence="0.987952">
G H D
(Th) C (TO A (Ti) C
</figure>
<figureCaption confidence="0.9259202">
Figure 1. Different sub-tree sets for T1 used by
2-level production rules and convolution tree
kernel approaches. Ta -Ti and T1 itself are cov-
ered by tree kernel, while only Ta-Te are covered
by production rules.
</figureCaption>
<figure confidence="0.999456108108108">
F
B C E
D
G
F
B
C
Decomposition
(T1)
A
G
H
B
E
C
D
F
(Ta) (Tb)
A D
(T�) (Td)
C E
(Tf) (Tg) A
D
E
(Te)
F
H
D
E F
D
E
F
G
H
B C
D
E F
</figure>
<bodyText confidence="0.9332775">
Tree Kernel based Approach in NLP. While
the feature based approach may not be able to
fully utilize the syntactic information in a parse
tree, an alternative to the feature-based methods,
tree kernel methods (Haussler, 1999) have been
proposed to implicitly explore features in a high
dimensional space by employing a kernel func-
tion to calculate the similarity between two ob-
jects directly. In particular, the kernel methods
could be very effective at reducing the burden of
feature engineering for structured objects in NLP
research (Culotta and Sorensen, 2004). This is
because a kernel can measure the similarity be-
tween two discrete structured objects by directly
using the original representation of the objects
instead of explicitly enumerating their features.
Indeed, using kernel methods to mine structur-
al knowledge has shown success in some NLP
applications like parsing (Collins and Duffy,
2001; Moschitti, 2004) and relation extraction
(Zelenko et al., 2003; Zhang et al., 2006). How-
ever, to our knowledge, the application of such a
technique to discourse relation recognition still
remains unexplored.
Lin et al. (2009) has explored the 2-level pro-
duction rules for discourse analysis. However,
Figure 1 shows that only 2-level sub-tree struc-
tures (e.g. Ta - Te ) are covered in production
rules. Other sub-trees beyond 2-level (e.g. Tf- Ti)
are only captured in the tree kernel, which allows
tree kernel to further leverage on information
from higher dimension space for possible better
discrimination. Especially, when there are
enough training data, this is similar to the study
on language modeling that N-gram beyond uni-
gram and bigram further improves the perfor-
mance in large corpus.
Tense and Temporal Ordering Information.
Linguistic studies (Webber, 1988) show that a
tensed clause Cb provides two pieces of semantic
information: (a) a description of an event (or sit-
uation) Eb; and (b) a particular configuration of
the point of event (ET), the point of reference
(RT) and the point of speech (ST). Both the cha-
racteristics of Eb and the configuration of ET, RT
and ST are critical to interpret the relationship of
event Eb with other events in the discourse mod-
el. Our observation on temporal ordering infor-
mation is in line with the above, which is also
incorporated in our discourse analyzer.
</bodyText>
<sectionHeader confidence="0.990237" genericHeader="method">
4 The Recognition Framework
</sectionHeader>
<bodyText confidence="0.99993075">
In the learning framework, a training or testing
instance is formed by a non-overlapping
clause(s)/sentence(s) pair. Specifically, since im-
plicit relations in PDTB are defined to be local,
only clauses from adjacent sentences are paired
for implicit cases. During training, for each dis-
course relation encountered, a positive instance
is created by pairing the two arguments. Also a
</bodyText>
<page confidence="0.984694">
712
</page>
<bodyText confidence="0.9999825">
set of negative instances is formed by paring
each argument with neighboring non-argument
clauses or sentences. Based on the training in-
stances, a binary classifier is generated for each
type using a particular learning algorithm. Dur-
ing resolution, (a) clauses within same sentence
and sentences within three-sentence spans are
paired to form an explicit testing instance; and
(b) neighboring sentences within three-sentence
spans are paired to form an implicit testing in-
stance. The instance is presented to each explicit
or implicit relation classifier which then returns a
class label with a confidence value indicating the
likelihood that the candidate pair holds a particu-
lar discourse relation. The relation with the high-
est confidence value will be assigned to the pair.
</bodyText>
<subsectionHeader confidence="0.999189">
4.1 Base Features
</subsectionHeader>
<bodyText confidence="0.956507571428571">
In our system, the base features adopted include
lexical pair, distance and attribution etc. as listed
in Table 1. All these base features have been
proved effective for discourse analysis in pre-
vious work.
Feature Description
Names
</bodyText>
<listItem confidence="0.9943363">
(F1) cue phrase
(F2) neighboring punctuation
(F3) position of connective if
presents
(F4) extents of arguments
(F5) relative order of arguments
(F6) distance between arguments
(F7) grammatical role of arguments
(F8) lexical pairs
(F9) attribution
</listItem>
<tableCaption confidence="0.991723">
Table 1. Base Feature Set
</tableCaption>
<subsectionHeader confidence="0.997512">
4.2 Support Vector Machine
</subsectionHeader>
<bodyText confidence="0.999917444444445">
In theory, any discriminative learning algorithm
is applicable to learn the classifier for discourse
analysis. In our study, we use Support Vector
Machine (Vapnik, 1995) to allow the use of ker-
nels to incorporate the structure feature.
Suppose the training set S consists of labeled
vectors { (xi, yi)}, where xi is the feature vector
of a training instance and yi is its class label. The
classifier learned by SVM is:
</bodyText>
<equation confidence="0.9668385">
f (x) = sgn — yiaix * xi + b)
i-1
</equation>
<bodyText confidence="0.9999607">
where ai is the learned parameter for a feature
vector xi, and b is another parameter which can
be derived from ai . A testing instance x is clas-
sified as positive if f (x) &gt; 01.
One advantage of SVM is that we can use tree
kernel approach to capture syntactic parse tree
information in a particular high-dimension space.
In the next section, we will discuss how to use
kernel to incorporate the more complex structure
feature.
</bodyText>
<sectionHeader confidence="0.9973255" genericHeader="method">
5 Incorporating Structural Syntactic
Information
</sectionHeader>
<bodyText confidence="0.999962772727273">
A parse tree that covers both discourse argu-
ments could provide us much syntactic informa-
tion related to the pair. Both the syntactic flat
path connecting connective and arguments and
the 2-level production rules in the parse tree used
in previous study can be directly described by the
tree structure. Other syntactic knowledge that
may be helpful for discourse resolution could
also be implicitly represented in the tree. There-
fore, by comparing the common sub-structures
between two trees we can find out to which level
two trees contain similar syntactic information,
which can be done using a convolution tree ker-
nel.
The value returned from the tree kernel re-
flects the similarity between two instances in
syntax. Such syntactic similarity can be further
combined with other flat linguistic features to
compute the overall similarity between two in-
stances through a composite kernel. And thus an
SVM classifier can be learned and then used for
recognition.
</bodyText>
<subsectionHeader confidence="0.988794">
5.1 Structural Syntactic Feature
</subsectionHeader>
<bodyText confidence="0.999930777777778">
Parsing is a sentence level processing. However,
in many cases two discourse arguments do not
occur in the same sentence. To present their syn-
tactic properties and relations in a single tree
structure, we construct a syntax tree for each pa-
ragraph by attaching the parsing trees of all its
sentences to an upper paragraph node. In this
paper, we only consider discourse relations with-
in 3 sentences, which only occur within each pa-
</bodyText>
<footnote confidence="0.9236">
1 In our task, the result of f (x) is used as the confidence
value of the candidate argument pair x to hold a particular
discourse relation.
</footnote>
<page confidence="0.996928">
713
</page>
<bodyText confidence="0.992091244897959">
ragraph, thus paragraph parse trees are sufficient.
Our 3-sentence spans cover 95% discourse rela-
tion cases in PDTB v2.0.
Having obtained the parse tree of a paragraph,
we shall consider how to select the appropriate
portion of the tree as the structured feature for a
given instance. As each instance is related to two
arguments, the structured feature at least should
be able to cover both of these two arguments.
Generally, the more substructure of the tree is
included, the more syntactic information would
be provided, but at the same time the more noisy
information would likely be introduced. In our
study, we examine three structured features that
contain different substructures of the paragraph
parse tree:
Min-Expansion This feature records the mi-
nimal structure covering both arguments
and connective word in the parse tree. It
only includes the nodes occurring in the
shortest path connecting Arg1, Arg2 and
connective, via the nearest commonly
commanding node. For example, consi-
dering Example (5), Figure 2 illustrates
the representation of the structured feature
for this relation instance. Note that the
two clauses underlined with dashed lines
are attributions which are not part of the
relation.
(5). Arg1. Suppression of the book, Judge
Oakes observed, would operate as a prior
restraint and thus involve the First
Amendment.
Arg2. Moreover, and here Judge Oakes
went to the heart of the question, “Respon-
sible biographers and historians constantly
use primary sources, letters, diaries and
memoranda.”
Simple-Expansion Min-Expansion could, to
some degree, describe the syntactic rela-
tionships between the connective and ar-
guments. However, the syntactic proper-
ties of the argument pair might not be
captured, because the tree structure sur-
rounding the argument is not taken into
consideration. To incorporate such infor-
mation, Simple-Expansion not only con-
tains all the nodes in Min-Expansion, but
also includes the first-level children of
</bodyText>
<figureCaption confidence="0.9372647">
Figure 2. Min-Expansion tree built from gol-
den standard parse tree for the explicit dis-
course relation in Example (5). Note that to
distinguish from other words, we explicitly
mark up in the structured feature the arguments
and connective, by appending a string tag
“Arg1”, “Arg2” and “Connective” respective-
ly.
Figure 3. Simple-Expansion tree for the expli-
cit discourse relation in Example (5).
</figureCaption>
<bodyText confidence="0.999875583333333">
these nodes2. Figure 3 illustrates such a
feature for Example (5). We can see that
the nodes “PRN” in both sentences are in-
cluded in the feature.
Full-Expansion This feature focuses on the
tree structure between two arguments. It
not only includes all the nodes in Simple-
Expansion, but also the nodes (beneath
the nearest commanding parent) that cov-
er the words between the two arguments.
Such a feature keeps the most information
related to the argument pair. Figure 4
</bodyText>
<footnote confidence="0.8616715">
2 We will not expand the nodes denoting the sentences other
than where the arguments occur.
</footnote>
<page confidence="0.99737">
714
</page>
<bodyText confidence="0.999349666666667">
shows the structure for feature Full-
Expansion of Example (5). As illustrated,
different from in Simple-Expansion, each
sub-tree of “PRN” in each sentence is ful-
ly expanded and all its children nodes are
included in Full-Expansion.
</bodyText>
<figureCaption confidence="0.988582">
Figure 4. Full-Expansion tree for the explicit
discourse relation in Example (5).
</figureCaption>
<subsectionHeader confidence="0.998127">
5.2 Convolution Parse Tree Kernel
</subsectionHeader>
<bodyText confidence="0.9763396">
Given the parse tree defined above, we use the
same convolution tree kernel as described in
(Collins and Duffy, 2002) and (Moschitti, 2004).
In general, we can represent a parse tree 𝑇 by a
vector of integer counts of each sub-tree type
(regardless of its ancestors):
∅ 𝑇 = (#𝑜𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜𝑓 𝑡𝑦𝑝𝑒 1, ..., # 𝑜𝑓
𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜 𝑓 𝑡𝑦𝑝𝑒 𝐼, ...,# 𝑜𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜𝑓
𝑡𝑦𝑝𝑒 𝑛).
This results in a very high dimensionality
since the number of different sub-trees is expo-
nential in its size. Thus, it is computational in-
feasible to directly use the feature vector ∅(𝑇).
To solve the computational issue, a tree kernel
function is introduced to calculate the dot prod-
uct between the above high dimensional vectors
efficiently.
Given two tree segments 𝑇1 and 𝑇2, the tree
kernel function is defined:
𝐾 𝑇1,𝑇2 = &lt; ∅ 𝑇1 ,∅ 𝑇2 &gt; 𝑖 ∗ 𝐼𝑖 (𝑛2)
= 𝑖 ∅ 𝑇1 𝑖 , ∅ 𝑇2 [𝑖]
= 𝑛1∈𝑁1 𝑛2∈𝑁2 𝐼𝑖 𝑛1
where 𝑁1and 𝑁2 are the sets of all nodes in trees
𝑇1and 𝑇2, respectively; and 𝐼𝑖 (𝑛) is the indicator
function that is 1 iff a subtree of type 𝑖 occurs
with root at node 𝑛 or zero otherwise. (Collins
and Duffy, 2002) shows that 𝐾(𝑇1, 𝑇2) is an in-
stance of convolution kernels over tree struc-
tures, and can be computed in 𝑂 ( 𝑁1 , 𝑁2 ) by
the following recursive definitions:
</bodyText>
<equation confidence="0.762615">
∆ 𝑛1, 𝑛2 = 𝑖 𝐼𝑖 𝑛1 ∗ 𝐼𝑖 (𝑛2)
</equation>
<listItem confidence="0.8691142">
(1) ∆ 𝑛1,𝑛2 = 0 if 𝑛1 and 𝑛2 do not have the
same syntactic tag or their children are different;
(2) else if both 𝑛1 and 𝑛2 are pre-terminals (i.e.
POS tags), ∆ 𝑛1, 𝑛2 = 1 × 𝜆;
(3) else, ∆ 𝑛1, 𝑛2 =
</listItem>
<equation confidence="0.996290333333333">
𝜆 (1 + ∆(𝑐𝑕(
𝑛𝑐 (𝑛1) 𝑛1,𝑗),𝑐𝑕(𝑛2,𝑗))),
𝑗 =1
</equation>
<bodyText confidence="0.999632785714286">
where 𝑛𝑐(𝑛1) is the number of the children of
𝑛1 , 𝑐𝑕(𝑛, 𝑗) is the 𝑗𝑡𝑕 child of node 𝑛 and 𝜆
(0 &lt; 𝜆 &lt; 1) is the decay factor in order to make
the kernel value less variable with respect to the
sub-tree sizes. In addition, the recursive rule (3)
holds because given two nodes with the same
children, one can construct common sub-trees
using these children and common sub-trees of
further offspring.
The parse tree kernel counts the number of
common sub-trees as the syntactic similarity
measure between two instances. The time com-
plexity for computing this kernel is 𝑂 ( 𝑁1 ∙
𝑁2 ).
</bodyText>
<subsectionHeader confidence="0.996315">
5.3 Composite Tree Kernel
</subsectionHeader>
<bodyText confidence="0.999120666666667">
Besides the above convolution parse tree kernel
𝐾 𝑡𝑟𝑒𝑒 𝑥1, 𝑥2 = 𝐾(𝑇1,𝑇2) defined to capture the
syntactic information between two instances 𝑥1
and 𝑥2, we also use another kernel 𝐾 𝑓𝑙𝑎𝑡 to cap-
ture other flat features, such as base features (de-
scribed in Table 1) and temporal ordering infor-
mation (described in Section 6). In our study, the
composite kernel is defined in the following
way:
</bodyText>
<equation confidence="0.9722486">
𝐾 1 𝑥1, 𝑥2 = 𝛼 ∙ 𝐾 𝑓𝑙𝑎𝑡 𝑥1, 𝑥2 +
1 − 𝛼 ∙ 𝐾 𝑡𝑟𝑒𝑒 𝑥1, 𝑥2 .
Here, 𝐾 (∙,∙) can be normalized by 𝐾 𝑦,𝑧 =
𝐾 𝑦, 𝑧 𝐾 𝑦, 𝑦 ∙ 𝐾 𝑧, 𝑧 and 𝛼 is the coeffi-
cient.
</equation>
<sectionHeader confidence="0.991793" genericHeader="method">
6 Using Temporal Ordering Informa-
tion
</sectionHeader>
<bodyText confidence="0.999983375">
In our discourse analyzer, we also add in tem-
poral information to be used as features to pre-
dict discourse relations. This is because both our
observations and some linguistic studies (Web-
ber, 1988) show that temporal ordering informa-
tion including tense, aspectual and event orders
between two arguments may constrain the dis-
course relation type. For example, the connective
</bodyText>
<page confidence="0.996469">
715
</page>
<bodyText confidence="0.99877275">
word is the same in both Example (6) and (7),
but the tense shift from progressive form in
clause 6.a to simple past form in clause 6.b, indi-
cating that the twisting occurred during the state
of running the marathon, usually signals a tem-
poral discourse relation; while in Example (7),
both clauses are in past tense and it is marked as
a Causal relation.
</bodyText>
<listItem confidence="0.878178666666667">
(6). a. Yesterday Holly was running a mara-
thon
b. when she twisted her ankle.
(7). a. Use of dispersants was approved
b. when a test on the third day showed
some positive results.
</listItem>
<bodyText confidence="0.993311666666667">
Inspired by the linguistic model from Webber
(1988) as described in Section 3, we explore the
temporal order of events in two adjacent sen-
tences for discourse relation interpretation. Here
event is represented by the head of verb, and the
temporal order refers to the logical occurrence
(i.e. before/at/after) between events. For in-
stance, the event ordering in Example (8) can be
interpreted as:
</bodyText>
<equation confidence="0.819652">
Event(broken) &lt;before Event(went) .
</equation>
<bodyText confidence="0.945953142857143">
8. a. John went to the hospital.
b. He had broken his ankle on a patch of
ice.
We notice that the feasible temporal order of
events differs for different discourse relations.
For example, in causal relations, cause event
usually happens before effect event, i.e.
</bodyText>
<equation confidence="0.532929">
Event(cause) &lt;before Event(effect).
</equation>
<bodyText confidence="0.998864272727273">
So it is possible to infer a causal relation in
Example (8) if and only if 8.b is taken to be the
cause event and 8.a is taken to be the effect
event. That is, 8.b is taken as happening prior to
his going into hospital.
In our experiments, we use the TARSQI3 sys-
tem to identify event, analyze tense and aspectual
information, and label the temporal order of
events. Then the tense and temporal ordering
information is extracted as features for discourse
relation recognition.
</bodyText>
<footnote confidence="0.810515">
3 http://www.isi.edu/tarsqi/
</footnote>
<sectionHeader confidence="0.814409" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999886">
In this section we provide the results of a set of
experiments focused on the task of simultaneous
discourse identification and classification.
</bodyText>
<subsectionHeader confidence="0.99632">
7.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999712">
We experiment on PDTB v2.0 corpus. Besides
four top-level discourse relations, we also con-
sider Entity and No relations described in Section
2. We directly use the golden standard parse
trees in Penn TreeBank. We employ an SVM
coreference resolver trained and tested on ACE
2005 with 79.5% Precision, 66.7% Recall and
72.5% F1 to label coreference mentions of the
same named entity in an article. For learning, we
use the binary SVMLight developed by (Joa-
chims, 1998) and Tree Kernel Toolkits devel-
oped by (Moschitti, 2004). All classifiers are
trained with default learning parameters.
The performance is evaluated using Accuracy
which is calculated as follow:
</bodyText>
<equation confidence="0.2584205">
TruePositive + TrueNegative
All
</equation>
<bodyText confidence="0.998423375">
Sections 2-22 are used for training and Sec-
tions 23-24 for testing. In this paper, we only
consider any non-overlapping clauses/sentences
pair in 3-sentence spans. For training, there were
14812, 12843 and 4410 instances for Explicit,
Implicit and Entity+No relations respectively;
while for testing, the number was 1489, 1167 and
380.
</bodyText>
<subsectionHeader confidence="0.999283">
7.2 System with Structural Kernel
</subsectionHeader>
<bodyText confidence="0.999971714285715">
Table 2 lists the performance of simultaneous
identification and classification on level-1 dis-
course senses. In the first row, only base features
described in Section 4 are used. In the second
row, we test Ben and James (2007)’s algorithm
which uses heuristically defined syntactic paths
and acts as a good baseline to compare with our
learned-based approach using the structured in-
formation. The last three rows of Table 2 reports
the results combining base features with three
syntactic structured features (i.e. Min-Expansion,
Simple-Expansion and Full-Expansion) de-
scribed in Section 5.
We can see that all our tree kernels outperform
the manually constructed flat path feature in all
three groups including Explicit only, Implicit
only and All relations, with the accuracy increas-
ing by 1.8%, 6.7% and 3.1% respectively. Espe-
cially, it shows that structural syntactic informa-
tion is more helpful for Implicit cases which is
generally much harder than Explicit cases. We
</bodyText>
<equation confidence="0.631618">
Accuracy =
</equation>
<page confidence="0.991449">
716
</page>
<table confidence="0.999662416666667">
Features Accuracy
Explicit Implicit All
Base Features 67.1 29 48.6
Base + Manually 70.3 32 52.6
selected flat path
features
Base + Tree kernel 71.9 38.6 55.6
(Min-Expansion)
Base + Tree kernel 72.1 38.7 55.7
(Simple-Expansion)
Base + Tree kernel 71.8 38.4 55.4
(Full-Expansion)
</table>
<tableCaption confidence="0.9550145">
Table 2. Results of the syntactic structured ker-
nels on level-1 discourse relation recognition.
</tableCaption>
<bodyText confidence="0.997875271186441">
conduct chi square statistical significance test on
All relations between flat path approach and
Simple-Expansion approach, which shows the
performance improvements are statistical signifi-
cant (𝜌 &lt; 0.05) through incorporating tree ker-
nel. This proves that structural syntactic informa-
tion has good predication power for discourse
analysis in both explicit and implicit relations.
We also observe that among the three syntactic
structured features, Min-Expansion and Simple-
Expansion achieve similar performances which
are better than the result for Full-Expansion. This
may be due to that most significant information
is with the arguments and the shortest path con-
necting connectives and arguments. However,
Full-Expansion that includes more information
in other branches may introduce too many details
which are rather tangential to discourse recogni-
tion. Our subsequent reports will focus on Sim-
ple-Expansion, unless otherwise specified.
As described in Section 5, to compute the
structural information, parse trees for different
sentences are connected to form a large tree for a
paragraph. It would be interesting to find how
the structured information works for discourse
relations whose arguments reside in different
sentences. For this purpose, we test the accuracy
for discourse relations with the two arguments
occurring in the same sentence, one-sentence
apart, and two-sentence apart. Table 3 compares
the learning systems with/without the structured
feature present. From the table, for all three cas-
es, the accuracies drop with the increase of the
distances between the two arguments. However,
adding the structured information would bring
consistent improvement against the baselines
regardless of the number of sentence distance.
This observation suggests that the structured syn-
tactic information is more helpful for inter-
sentential discourse analysis.
We also concern about how the structured in-
formation works for identification and classifica-
tion respectively. Table 4 lists the results for the
two sub-tasks. As shown, with the structured in-
formation incorporated, the system (Base + Tree
Kernel) can boost the performance of the two
baselines (Base Features in the first row andBase
+ Manually selected paths in the second row), for
both identification and classification respective-
ly. We also observe that the structural syntactic
information is more helpful for classification task
which is generally harder than identification.
This is in line with the intuition that classifica-
tion is generally a much harder task. We find that
due to the weak modeling of Entity relations,
many Entity relations which are non-discourse
relation instances are mis-identified as implicit
Expansion relations. Nevertheless, it clearly di-
rects our future work.
</bodyText>
<table confidence="0.999328125">
Sentence Dis- 0 1 2
tance (959) (1746) (331)
Base Features 52 49.2 35.5
Base + Manually 56.7 52 43.8
selected flat path
features
Base + Tree 58.3 55.6 49.7
Kernel
</table>
<tableCaption confidence="0.545291666666667">
Table 3. Results of the syntactic structured kernel
for discourse relations recognition with argu-
ments in different sentences apart.
</tableCaption>
<table confidence="0.402275875">
Tasks Identifica- Classifica-
tion tion
Base Features 58.6 50.5
Base + Manually 59.7 52.6
selected flat path
features
Base + Tree 63.3 59.3
Kernel
</table>
<tableCaption confidence="0.676297666666667">
Table 4. Results of the syntactic structured ker-
nel for simultaneous discourse identification and
classification subtasks.
</tableCaption>
<subsectionHeader confidence="0.530234">
7.3 System with Temporal Ordering Infor-
mation
</subsectionHeader>
<bodyText confidence="0.999256">
To examine the effectiveness of our temporal
ordering information, we perform experiments
</bodyText>
<page confidence="0.986838">
717
</page>
<bodyText confidence="0.999783071428572">
on simultaneous identification and classification
of level-1 discourse relations to compare with
using only base feature set as baseline. The re-
sults are shown in Table 5. We observe that the
use of temporal ordering information increases
the accuracy by 3%, 3.6% and 3.2% for Explicit,
Implicit and All groups respectively. We conduct
chi square statistical significant test on All rela-
tions, which shows the performance improve-
ment is statistical significant (𝜌 &lt; 0.05). It indi-
cates that temporal ordering information can
constrain the discourse relation types inferred
within a clause(s)/sentence(s) pair for both expli-
cit and implicit relations.
</bodyText>
<table confidence="0.982482">
Features Accuracy
Explicit Implicit All
Base Features 67.1 29 48.6
Base + Tem- 70.1 32.6 51.8
poral Ordering
</table>
<tableCaption confidence="0.87543">
Information
Table 5. Results of tense and temporal order
information on level-1 discourse relations.
</tableCaption>
<bodyText confidence="0.99999055">
We observe that although temporal ordering
information is useful in both explicit and implicit
relation recognition, the contributions of the spe-
cific information are quite different for the two
cases. In our experiments, we use tense and as-
pectual information for explicit relations, while
event ordering information is used for implicit
relations. The reason is explicit connective itself
provides a strong hint for explicit relation, so
tense and aspectual analysis which yields a relia-
ble result can provide additional constraints, thus
can help explicit relation recognition. However,
event ordering which would inevitably involve
more noises will adversely affect the explicit re-
lation recognition performance. On the other
hand, for implicit relations with no explicit con-
nective words, tense and aspectual information
alone is not enough for discourse analysis. Event
ordering can provide more necessary information
to further constrain the inferred relations.
</bodyText>
<subsectionHeader confidence="0.985166">
7.4 Overall Results
</subsectionHeader>
<bodyText confidence="0.99986">
We also evaluate our model which combines
base features, tree kernel and tense/temporal or-
dering information together on Explicit, Implicit
and All Relations respectively. The overall re-
sults are shown in Table 6.
</bodyText>
<figure confidence="0.42785025">
Relations Accuracy
Explicit 74.2
Implicit 40.0
All 57.3
</figure>
<bodyText confidence="0.7684775">
Table 6. Overall results for combined model
(Base + Tree Kernel + Tense/Temporal).
</bodyText>
<sectionHeader confidence="0.993612" genericHeader="conclusions">
8 Conclusions and Future Works
</sectionHeader>
<bodyText confidence="0.999989357142857">
The purpose of this paper is to explore how to
make use of the structural syntactic knowledge to
do discourse relation recognition. In previous
work, syntactic information from parse trees is
represented as a set of heuristically selected flat
paths or 2-level production rules. However, the
features defined this way may not necessarily
capture all useful syntactic information provided
by the parse trees for discourse analysis. In the
paper, we propose a kernel-based method to in-
corporate the structural information embedded in
parse trees. Specifically, we directly utilize the
syntactic parse tree as a structure feature, and
then apply kernels to such a feature, together
with other normal features. The experimental
results on PDTB v2.0 show that our kernel-based
approach is able to give statistical significant
improvement over flat syntactic path method. In
addition, we also propose to incorporate tempor-
al ordering information to constrain the interpre-
tation of discourse relations, which also demon-
strate statistical significant improvements for
discourse relation recognition, both explicit and
implicit.
In future, we plan to model Entity relations
which constitute 24% of Implicit+Entity+No re-
lation cases, thus to improve the accuracy of re-
lation detection.
</bodyText>
<sectionHeader confidence="0.994153" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.991969076923077">
Ben W. and James P. 2007. Automatically Identifying
the Arguments of Discourse Connectives. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
92-101.
Culotta A. and Sorensen J. 2004. Dependency Tree
Kernel for Relation Extraction. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2004), pages 423-
429.
Collins M. and Duffy N. 2001. New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Dis-
</reference>
<page confidence="0.976317">
718
</page>
<reference confidence="0.999457486842105">
crete Structures and the Voted Perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2002),
pages 263-270.
Collins M. and Duffy N. 2002. Convolution Kernels
for Natural Language. NIPS-2001.
Haussler D. 1999. Convolution Kernels on Discrete
Structures. Technical Report UCS-CRL-99-10,
University of California, Santa Cruz.
Joachims T. 1999. Making Large-scale SVM Learn-
ing Practical. In Advances in Kernel Methods –
Support Vector Learning. MIT Press.
Knott, A., Oberlander, J., O’Donnel, M., and Mellish,
C. 2001. Beyond elaboration: the interaction of re-
lations and focus in coherent text. In T. Sanders, J.
Schilperoord, and W. Spooren, editors, Text Re-
presentation: Linguistic and Psycholinguistics As-
pects, pages 181-196. Benjamins, Amsterdam.
Lee A., Prasad R., Joshi A., Dinesh N. and Webber
B. 2006. Complexity of dependencies in discourse:
are dependencies in discourse more complex than
in syntax? In Proceedings of the 5th International
Workshop on Treebanks and Linguistic Theories.
Prague, Czech Republic, December.
Lin Z., Kan M. and Ng H. 2009. Recognizing Implicit
Discourse Relations in the Penn Discourse Tree-
bank. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing (EMNLP 2009), Singapore, August.
Marcu D. and Echihabi A. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. In
Proceedings of the 40th Annual Meeting of ACL,
pages 368-375.
Moschitti A. 2004. A Study on Convolution Kernels
for Shallow Semantic Parsing. In Proceedings of
the 42th Annual Meeting of the Association for
Computational Linguistics (ACL 2004), pages 335-
342.
Pettibone J. and Pon-Barry H. 2003. A Maximum En-
tropy Approach to Recognizing Discourse Rela-
tions in Spoken Language. Working Paper. The
Stanford Natural Language Processing Group, June
6.
Pitler E., Louis A. and Nenkova A. 2009. Automatic
Sense Predication for Implicit Discourse Relations
in Text. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language
Processing (ACL-IJCNLP 2009).
Prasad R., Dinesh N., Lee A., Miltsakaki E., Robaldo
L., Joshi A. and Webber B. 2008. The Penn Dis-
course TreeBank 2.0. In Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC 2008).
Saito M., Yamamoto K. and Sekine S. 2006. Using
phrasal patterns to identify discourse relations. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL 2006), pages 133–136, New York, USA.
Vapnik V. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
Webber Bonnie. 1988. Tense as Discourse Anaphor.
Computational Linguistics, 14:61–73.
Zelenko D., Aone C. and Richardella A. 2003. Ker-
nel Methods for Relation Extraction. Journal of
Machine Learning Research, 3(6):1083-1106.
Zhang M., Zhang J. and Su J. Exploring Syntactic
Features for Relation Extraction using a Convolu-
tion Tree Kernel. In Proceedings of the Human
Language Technology conference - North Ameri-
can chapter of the Association for Computational
Linguistics annual meeting (HLT-NAACL 2006),
New York, USA.
</reference>
<page confidence="0.998656">
719
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.213684">
<title confidence="0.999167">Kernel Based Discourse Relation Recognition with Temporal Ordering Information</title>
<author confidence="0.981145">Jian Chew Lim</author>
<affiliation confidence="0.654824">for Infocomm Research</affiliation>
<note confidence="0.809496">1 Fusionopolis Way, #21-01 Connexis Singapore 138632</note>
<intro confidence="0.550321">{wwang,sujian}@i2r.a-star.edu.sg</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Ben</author>
<author>P James</author>
</authors>
<title>Automatically Identifying the Arguments of Discourse Connectives.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>92--101</pages>
<contexts>
<context position="2505" citStr="Ben and James, 2007" startWordPosition="351" endWordPosition="354">classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et al. (2009) uses 2-leve</context>
<context position="25252" citStr="Ben and James (2007)" startWordPosition="4141" endWordPosition="4144">tive + TrueNegative All Sections 2-22 are used for training and Sections 23-24 for testing. In this paper, we only consider any non-overlapping clauses/sentences pair in 3-sentence spans. For training, there were 14812, 12843 and 4410 instances for Explicit, Implicit and Entity+No relations respectively; while for testing, the number was 1489, 1167 and 380. 7.2 System with Structural Kernel Table 2 lists the performance of simultaneous identification and classification on level-1 discourse senses. In the first row, only base features described in Section 4 are used. In the second row, we test Ben and James (2007)’s algorithm which uses heuristically defined syntactic paths and acts as a good baseline to compare with our learned-based approach using the structured information. The last three rows of Table 2 reports the results combining base features with three syntactic structured features (i.e. Min-Expansion, Simple-Expansion and Full-Expansion) described in Section 5. We can see that all our tree kernels outperform the manually constructed flat path feature in all three groups including Explicit only, Implicit only and All relations, with the accuracy increasing by 1.8%, 6.7% and 3.1% respectively. </context>
</contexts>
<marker>Ben, James, 2007</marker>
<rawString>Ben W. and James P. 2007. Automatically Identifying the Arguments of Discourse Connectives. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 92-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency Tree Kernel for Relation Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>423--429</pages>
<contexts>
<context position="9652" citStr="Culotta and Sorensen, 2004" startWordPosition="1515" endWordPosition="1518">�) (Td) C E (Tf) (Tg) A D E (Te) F H D E F D E F G H B C D E F Tree Kernel based Approach in NLP. While the feature based approach may not be able to fully utilize the syntactic information in a parse tree, an alternative to the feature-based methods, tree kernel methods (Haussler, 1999) have been proposed to implicitly explore features in a high dimensional space by employing a kernel function to calculate the similarity between two objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level prod</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Culotta A. and Sorensen J. 2004. Dependency Tree Kernel for Relation Extraction. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages 423-429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures and the Voted Perceptron.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="9995" citStr="Collins and Duffy, 2001" startWordPosition="1567" endWordPosition="1570">h dimensional space by employing a kernel function to calculate the similarity between two objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. Ta - Te ) are covered in production rules. Other sub-trees beyond 2-level (e.g. Tf- Ti) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better disc</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins M. and Duffy N. 2001. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures and the Voted Perceptron. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2002</date>
<contexts>
<context position="19044" citStr="Collins and Duffy, 2002" startWordPosition="3029" endWordPosition="3032">eps the most information related to the argument pair. Figure 4 2 We will not expand the nodes denoting the sentences other than where the arguments occur. 714 shows the structure for feature FullExpansion of Example (5). As illustrated, different from in Simple-Expansion, each sub-tree of “PRN” in each sentence is fully expanded and all its children nodes are included in Full-Expansion. Figure 4. Full-Expansion tree for the explicit discourse relation in Example (5). 5.2 Convolution Parse Tree Kernel Given the parse tree defined above, we use the same convolution tree kernel as described in (Collins and Duffy, 2002) and (Moschitti, 2004). In general, we can represent a parse tree 𝑇 by a vector of integer counts of each sub-tree type (regardless of its ancestors): ∅ 𝑇 = (#𝑜𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜𝑓 𝑡𝑦𝑝𝑒 1, ..., # 𝑜𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜 𝑓 𝑡𝑦𝑝𝑒 𝐼, ...,# 𝑜𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜𝑓 𝑡𝑦𝑝𝑒 𝑛). This results in a very high dimensionality since the number of different sub-trees is exponential in its size. Thus, it is computational infeasible to directly use the feature vector ∅(𝑇). To solve the computational issue, a tree kernel function is introduced to calculate the dot product between the above high dimensional vectors efficiently. Given two tr</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Collins M. and Duffy N. 2002. Convolution Kernels for Natural Language. NIPS-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution Kernels on Discrete Structures.</title>
<date>1999</date>
<tech>Technical Report UCS-CRL-99-10,</tech>
<institution>University of California,</institution>
<location>Santa Cruz.</location>
<contexts>
<context position="9313" citStr="Haussler, 1999" startWordPosition="1464" endWordPosition="1465">ns. 3 Related Work G H D (Th) C (TO A (Ti) C Figure 1. Different sub-tree sets for T1 used by 2-level production rules and convolution tree kernel approaches. Ta -Ti and T1 itself are covered by tree kernel, while only Ta-Te are covered by production rules. F B C E D G F B C Decomposition (T1) A G H B E C D F (Ta) (Tb) A D (T�) (Td) C E (Tf) (Tg) A D E (Te) F H D E F D E F G H B C D E F Tree Kernel based Approach in NLP. While the feature based approach may not be able to fully utilize the syntactic information in a parse tree, an alternative to the feature-based methods, tree kernel methods (Haussler, 1999) have been proposed to implicitly explore features in a high dimensional space by employing a kernel function to calculate the similarity between two objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledg</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>Haussler D. 1999. Convolution Kernels on Discrete Structures. Technical Report UCS-CRL-99-10, University of California, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making Large-scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods – Support Vector Learning.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Joachims, 1999</marker>
<rawString>Joachims T. 1999. Making Large-scale SVM Learning Practical. In Advances in Kernel Methods – Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Knott</author>
<author>J Oberlander</author>
<author>M O’Donnel</author>
<author>C Mellish</author>
</authors>
<title>Beyond elaboration: the interaction of relations and focus in coherent text. In</title>
<date>2001</date>
<pages>181--196</pages>
<editor>T. Sanders, J. Schilperoord, and W. Spooren, editors, Text</editor>
<location>Benjamins, Amsterdam.</location>
<marker>Knott, Oberlander, O’Donnel, Mellish, 2001</marker>
<rawString>Knott, A., Oberlander, J., O’Donnel, M., and Mellish, C. 2001. Beyond elaboration: the interaction of relations and focus in coherent text. In T. Sanders, J. Schilperoord, and W. Spooren, editors, Text Representation: Linguistic and Psycholinguistics Aspects, pages 181-196. Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lee</author>
<author>R Prasad</author>
<author>A Joshi</author>
<author>N Dinesh</author>
<author>B Webber</author>
</authors>
<title>Complexity of dependencies in discourse: are dependencies in discourse more complex than in syntax?</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Workshop on Treebanks and Linguistic Theories.</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2169" citStr="Lee et al. (2006)" startWordPosition="301" endWordPosition="304">vements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well. Discourse relations capture the internal structure and logical relationship of coherent text, including Temporal, Causal and Contrastive relations etc. The ability of recognizing such relations between text units including identifying and classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured informati</context>
</contexts>
<marker>Lee, Prasad, Joshi, Dinesh, Webber, 2006</marker>
<rawString>Lee A., Prasad R., Joshi A., Dinesh N. and Webber B. 2006. Complexity of dependencies in discourse: are dependencies in discourse more complex than in syntax? In Proceedings of the 5th International Workshop on Treebanks and Linguistic Theories. Prague, Czech Republic, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Lin</author>
<author>M Kan</author>
<author>H Ng</author>
</authors>
<title>Recognizing Implicit Discourse Relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), Singapore,</booktitle>
<contexts>
<context position="2523" citStr="Lin et al., 2009" startWordPosition="355" endWordPosition="358">important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et al. (2009) uses 2-level production rules</context>
<context position="10222" citStr="Lin et al. (2009)" startWordPosition="1602" endWordPosition="1605">ts in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. Ta - Te ) are covered in production rules. Other sub-trees beyond 2-level (e.g. Tf- Ti) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better discrimination. Especially, when there are enough training data, this is similar to the study on language modeling that N-gram beyond unigram and bigram further improves the performance in large corpus. Tense and Temporal Ordering </context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Lin Z., Kan M. and Ng H. 2009. Recognizing Implicit Discourse Relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>A Echihabi</author>
</authors>
<title>An Unsupervised Approach to Recognizing Discourse Relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of ACL,</booktitle>
<pages>368--375</pages>
<contexts>
<context position="2434" citStr="Marcu and Echihabi, 2002" startWordPosition="339" endWordPosition="342"> of recognizing such relations between text units including identifying and classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for disc</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Marcu D. and Echihabi A. 2002. An Unsupervised Approach to Recognizing Discourse Relations. In Proceedings of the 40th Annual Meeting of ACL, pages 368-375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>A Study on Convolution Kernels for Shallow Semantic Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>335--342</pages>
<contexts>
<context position="10013" citStr="Moschitti, 2004" startWordPosition="1571" endWordPosition="1572">ploying a kernel function to calculate the similarity between two objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. Ta - Te ) are covered in production rules. Other sub-trees beyond 2-level (e.g. Tf- Ti) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better discrimination. Especi</context>
<context position="19066" citStr="Moschitti, 2004" startWordPosition="3034" endWordPosition="3035">ed to the argument pair. Figure 4 2 We will not expand the nodes denoting the sentences other than where the arguments occur. 714 shows the structure for feature FullExpansion of Example (5). As illustrated, different from in Simple-Expansion, each sub-tree of “PRN” in each sentence is fully expanded and all its children nodes are included in Full-Expansion. Figure 4. Full-Expansion tree for the explicit discourse relation in Example (5). 5.2 Convolution Parse Tree Kernel Given the parse tree defined above, we use the same convolution tree kernel as described in (Collins and Duffy, 2002) and (Moschitti, 2004). In general, we can represent a parse tree 𝑇 by a vector of integer counts of each sub-tree type (regardless of its ancestors): ∅ 𝑇 = (#𝑜𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜𝑓 𝑡𝑦𝑝𝑒 1, ..., # 𝑜𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜 𝑓 𝑡𝑦𝑝𝑒 𝐼, ...,# 𝑜𝑓 𝑠𝑢𝑏𝑡𝑟𝑒𝑒𝑠 𝑜𝑓 𝑡𝑦𝑝𝑒 𝑛). This results in a very high dimensionality since the number of different sub-trees is exponential in its size. Thus, it is computational infeasible to directly use the feature vector ∅(𝑇). To solve the computational issue, a tree kernel function is introduced to calculate the dot product between the above high dimensional vectors efficiently. Given two tree segments 𝑇1 and 𝑇2,</context>
<context position="24485" citStr="Moschitti, 2004" startWordPosition="4026" endWordPosition="4027"> task of simultaneous discourse identification and classification. 7.1 Experimental Settings We experiment on PDTB v2.0 corpus. Besides four top-level discourse relations, we also consider Entity and No relations described in Section 2. We directly use the golden standard parse trees in Penn TreeBank. We employ an SVM coreference resolver trained and tested on ACE 2005 with 79.5% Precision, 66.7% Recall and 72.5% F1 to label coreference mentions of the same named entity in an article. For learning, we use the binary SVMLight developed by (Joachims, 1998) and Tree Kernel Toolkits developed by (Moschitti, 2004). All classifiers are trained with default learning parameters. The performance is evaluated using Accuracy which is calculated as follow: TruePositive + TrueNegative All Sections 2-22 are used for training and Sections 23-24 for testing. In this paper, we only consider any non-overlapping clauses/sentences pair in 3-sentence spans. For training, there were 14812, 12843 and 4410 instances for Explicit, Implicit and Entity+No relations respectively; while for testing, the number was 1489, 1167 and 380. 7.2 System with Structural Kernel Table 2 lists the performance of simultaneous identificatio</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Moschitti A. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages 335-342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pettibone</author>
<author>H Pon-Barry</author>
</authors>
<title>A Maximum Entropy Approach to Recognizing Discourse Relations in Spoken Language. Working Paper. The Stanford Natural Language Processing Group,</title>
<date>2003</date>
<marker>Pettibone, Pon-Barry, 2003</marker>
<rawString>Pettibone J. and Pon-Barry H. 2003. A Maximum Entropy Approach to Recognizing Discourse Relations in Spoken Language. Working Paper. The Stanford Natural Language Processing Group, June 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic Sense Predication for Implicit Discourse Relations in Text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP</booktitle>
<contexts>
<context position="2545" citStr="Pitler et al., 2009" startWordPosition="359" endWordPosition="363">ion to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et al. (2009) uses 2-level production rules to represent parse tr</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Pitler E., Louis A. and Nenkova A. 2009. Automatic Sense Predication for Implicit Discourse Relations in Text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="4190" citStr="Prasad et al., 2008" startWordPosition="604" endWordPosition="607">from the parse trees for discourse analysis, applying kernel function to the parse tree structures directly. These structural syntactic features, together with other flat features are then incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification. The experiment shows that tree kernel is able to effectively incorporate syntactic structural information and produce statistical significant improvements over flat syntactic path feature for the recognition of both explicit and implicit relation in Penn Discourse Treebank (PDTB; Prasad et al., 2008). We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further work on a higher dimensional space for possible better discrimination. Besides, inspired by the linguistic study on tense and discourse anaphor (Webber, 1988), we further propose to incorporate temporal ordering information to constrain the interpretation of discourse relation, which also demonstrates statistical significant improvements for discourse relation recognition on PDTB v2.0 for both explicit and implicit relations. The organization of the re</context>
<context position="5490" citStr="Prasad et al., 2008" startWordPosition="811" endWordPosition="814"> gives the related work on tree kernel approach in NLP and its difference with production rules, and also linguistic study on tense and discourse anaphor. Section 4 introduces the frame work for discourse recognition, as well as the baseline feature space and the SVM classifier. We present our kernel-based method in Section 5, and the usage of temporal ordering feature in Section 6. Section 7 shows the experiments and discussions. We conclude our works in Section 8. 2 Penn Discourse Tree Bank The Penn Discourse Treebank (PDTB) is the largest available annotated corpora of discourse relations (Prasad et al., 2008) over 2,312 Wall Street Journal articles. The PDTB models discourse relation in the predicate-argument view, where a discourse connective (e.g., but) is treated as a predicate taking two text spans as its arguments. The argument that the discourse connective syntactically bounds to is called Arg2, and the other argument is called Arg1. The PDTB provides annotations for both explicit and implicit discourse relations. An explicit relation is triggered by an explicit connective. Example (1) shows an explicit Contrast relation signaled by the discourse connective ‘but’. (1). Arg1. Yesterday, the r</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Prasad R., Dinesh N., Lee A., Miltsakaki E., Robaldo L., Joshi A. and Webber B. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Saito</author>
<author>K Yamamoto</author>
<author>S Sekine</author>
</authors>
<title>Using phrasal patterns to identify discourse relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL</booktitle>
<pages>133--136</pages>
<location>New York, USA.</location>
<contexts>
<context position="2484" citStr="Saito et al., 2006" startWordPosition="347" endWordPosition="350">ing identifying and classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et a</context>
</contexts>
<marker>Saito, Yamamoto, Sekine, 2006</marker>
<rawString>Saito M., Yamamoto K. and Sekine S. 2006. Using phrasal patterns to identify discourse relations. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL 2006), pages 133–136, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="13324" citStr="Vapnik, 1995" startWordPosition="2093" endWordPosition="2094">tc. as listed in Table 1. All these base features have been proved effective for discourse analysis in previous work. Feature Description Names (F1) cue phrase (F2) neighboring punctuation (F3) position of connective if presents (F4) extents of arguments (F5) relative order of arguments (F6) distance between arguments (F7) grammatical role of arguments (F8) lexical pairs (F9) attribution Table 1. Base Feature Set 4.2 Support Vector Machine In theory, any discriminative learning algorithm is applicable to learn the classifier for discourse analysis. In our study, we use Support Vector Machine (Vapnik, 1995) to allow the use of kernels to incorporate the structure feature. Suppose the training set S consists of labeled vectors { (xi, yi)}, where xi is the feature vector of a training instance and yi is its class label. The classifier learned by SVM is: f (x) = sgn — yiaix * xi + b) i-1 where ai is the learned parameter for a feature vector xi, and b is another parameter which can be derived from ai . A testing instance x is classified as positive if f (x) &gt; 01. One advantage of SVM is that we can use tree kernel approach to capture syntactic parse tree information in a particular high-dimension s</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik V. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Webber Bonnie</author>
</authors>
<date>1988</date>
<booktitle>Tense as Discourse Anaphor. Computational Linguistics,</booktitle>
<pages>14--61</pages>
<marker>Bonnie, 1988</marker>
<rawString>Webber Bonnie. 1988. Tense as Discourse Anaphor. Computational Linguistics, 14:61–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel Methods for Relation Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--6</pages>
<contexts>
<context position="10059" citStr="Zelenko et al., 2003" startWordPosition="1576" endWordPosition="1579"> similarity between two objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. Ta - Te ) are covered in production rules. Other sub-trees beyond 2-level (e.g. Tf- Ti) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better discrimination. Especially, when there are enough training data, thi</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Zelenko D., Aone C. and Richardella A. 2003. Kernel Methods for Relation Extraction. Journal of Machine Learning Research, 3(6):1083-1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Zhang</author>
<author>J Su</author>
</authors>
<title>Exploring Syntactic Features for Relation Extraction using a Convolution Tree Kernel.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics annual meeting (HLT-NAACL</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="10080" citStr="Zhang et al., 2006" startWordPosition="1580" endWordPosition="1583">o objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. Ta - Te ) are covered in production rules. Other sub-trees beyond 2-level (e.g. Tf- Ti) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better discrimination. Especially, when there are enough training data, this is similar to the s</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Zhang M., Zhang J. and Su J. Exploring Syntactic Features for Relation Extraction using a Convolution Tree Kernel. In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics annual meeting (HLT-NAACL 2006), New York, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>