<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001065">
<title confidence="0.991416">
Multi-Word Expression Identification Using Sentence Surface Features
</title>
<author confidence="0.997924">
Ram Boukobza Ari Rappoport
</author>
<affiliation confidence="0.9992835">
School of Computer Science School of Computer Science
Hebrew University of Jerusalem Hebrew University of Jerusalem
</affiliation>
<email confidence="0.997746">
ram.boukobza@mail.huji.ac.il arir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.993858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969807692308">
Much NLP research on Multi-Word Ex-
pressions (MWEs) focuses on the discov-
ery of new expressions, as opposed to the
identification in texts of known expres-
sions. However, MWE identification is
not trivial because many expressions al-
low variation in form and differ in the
range of variations they allow. We show
that simple rule-based baselines do not
perform identification satisfactorily, and
present a supervised learning method for
identification that uses sentence surface
features based on expressions’ canonical
form. To evaluate the method, we have
annotated 3350 sentences from the British
National Corpus, containing potential uses
of 24 verbal MWEs. The method achieves
an F-score of 94.86%, compared with
80.70% for the leading rule-based base-
line. Our method is easily applicable to
any expression type. Experiments in pre-
vious research have been limited to the
compositional/non-compositional distinc-
tion, while we also test on sentences in
which the words comprising the MWE ap-
pear but not as an expression.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790952380952">
Multi-Word Expressions (MWEs) such as ‘pull
strings’, ‘make a face’ and ‘get on one’s nerves’
are very common in language. Such MWEs can
be characterized as being non-compositional: the
meaning of the expression does not transparently
follow from the meaning of the words that com-
prise it. Much of the work on MWEs in NLP has
been in MWE extraction – the discovery of new
MWEs from a corpus, using statistical and other
methods. Identification of known MWEs in text
has received less attention, but is necessary for
many NLP applications, for example in machine
translation. The current work deals with the MWE
identification task: deciding if a sentence contains
a use of a known expression.
MWE identification is not as simple as may ini-
tially appear, as will be shown by the performance
of two rule-based baselines in our experiments.
One source of difficulty is variations in expres-
sions’ usage in text. Although MWEs generally
show less variation than single words, they show
enough that it cannot be ignored. In a study on
V+NP idioms, Riehemann (2001) found that the
idioms’ canonical form accounted for 75% of their
appearances in a corpus. Additionally, expressions
differ considerably in the types of variations they
allow, which include passivization, nominalization
and addition of modifying words (Moon, 1998).
A second source of difficulty is that expressions
consisting of very frequent words will often co-
occur in sentences in a non-MWE usage and in
similar but distinct expressions.
MWE identification can be modeled as a two
step process. Given a sentence and a known ex-
pression, step (1) is to decide if the sentence con-
tains a potential use of the expression. This is a
relatively simple step based on the appearance in
the sentence of the words comprising the MWE.
Step (2) is to decide if the potential use is indeed
non-compositional. Consider the following sen-
tences with regard to the expression hit the road,
meaning ‘to leave on a journey’:
</bodyText>
<listItem confidence="0.964216333333333">
(a) ‘At the time, the road was long and difficult
with few travelers daring to take it.’
(b) ‘The headlights of the taxi-van behind us
</listItem>
<page confidence="0.987778">
468
</page>
<note confidence="0.9965025">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 468–477,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.771998">
flashed as it hit bumps in the road.’
</bodyText>
<listItem confidence="0.9384138">
(c) ‘The bullets were hitting the road and I could
see them coming towards me a lot faster than
I was able to reverse.’
(d) ‘Lorry trailers which would have been hitting
the road tomorrow now stand idle.’
</listItem>
<bodyText confidence="0.994447564516129">
Sentence (a) does not contain a potential use of
the expression due to the missing component ‘hit’.
Each of (b)-(d) does contain a potential use of the
expression. In (b) all of the expression compo-
nents are present, but they do not form an expres-
sion. In (c), the words form an expression, but
with a compositional (literal) meaning. Only (d)
contains a non-compositional use of hit the road.
The task we address in this paper is to identify
whether or not we are in case (d), for a given ex-
pression in a given sentence.
To date, most work in MWE identification has
focused on manually encoding rules that identify
expressions in text. The encodings, usually con-
sisting of regular expressions and syntactic struc-
tures, are intended to contain all the necessary in-
formation for processing the MWE in text. Being
manual, this is time-consuming work and requires
expert knowledge of individual expressions. In
terms of the above model, such encodings handle
both MWE identification steps.
A second approach is to use machine learning
methods to learn an expression’s behavior from a
corpus. Studies taking this approach have focused
on distinguishing between compositional and non-
compositional uses of an expression (cases (c) and
(d) above). As will be detailed in Section 2, exist-
ing methods are tailored to an expression’s type,
and experiment with a single MWE pattern. In ad-
dition, the training and test sets they used did not
contain non-expression uses as in case (b), which
can be quite common in practice.
Our approach is more general. Given a set of
sentences with potential MWE uses, we use sen-
tence surface features to create a Support Vec-
tor Machine (SVM) classifier for each expres-
sion. The classifier is binary and differentiates be-
tween non-compositional uses of the expression
((d) above) on the one hand, and compositional
and non-expression uses ((b) and (c)) on the other.
The experiments and results presented below fo-
cus on verbal MWEs, since verbal MWEs are
quite common in language use and have also been
investigated in related MWE research (e.g., (Cook
et al., 2007)). However, the developed features are
not specific to a particular type of expression.
The supervised method is compared with two
simple rule-based baselines in order to test
whether a simple approach is sufficient. In addi-
tion, the use of surface features is compared with
the use of syntactic features (based on dependency
parse trees of the sentences). Averaged over ex-
pressions in an independent test set, the super-
vised classifiers outperform the rule-based base-
lines, with F-scores of 94.86% (surface features)
and 87.77% (syntactic features), compared with
80.70% for the best baseline.
Section 2 reviews previous work. Section 3 dis-
cusses the features used for the supervised classi-
fier. Section 4 explains the experimental setting.
The results and a discussion are given in sections
5 and 6.
</bodyText>
<sectionHeader confidence="0.998908" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.995722">
2.1 MWE Lexical Encoding
</subsectionHeader>
<bodyText confidence="0.999969066666667">
The approach to handling MWEs in early systems
was to employ a list of expressions, each with
a quasi regular expression that encodes morpho-
syntactic variations. One example is Leech et
al. (1994) who used this method for automatic
part-of-speech tagging for the BNC. Another is a
formalism called IDAREX (IDioms And Regular
EXpressions) (Breidt et al., 1996).
More recent research emphasizes the integra-
tion of MWE lexical entries into existing single
word lexicons and grammar systems (Villavicen-
cio et al., 2004; Alegria et al., 2004). There is
also an attempt to take advantage of regularities in
morpho-syntactic properties across MWE groups,
which allows encoding the behavior of the group
instead of individual expressions (Villavicencio et
al., 2004; Gr´egoire, 2007). Fellbaum (1998) dis-
cusses some difficulties in representing idioms,
which are largely figurative in meaning, in Word-
Net. More recent work (Fellbaum et al., 2006) fo-
cuses on German VP idioms.
As already mentioned, one issue with lexi-
cal encoding is that it is done manually, mak-
ing lexicons difficult to create, maintain and ex-
tend. The use of regularities among different types
of MWEs is one way of reducing the amount
of work required. A second issue is that im-
plementations tend to ignore the likelihood and
even the possibility of compositional and other
interpretations of expressions in text, which can
</bodyText>
<page confidence="0.999409">
469
</page>
<bodyText confidence="0.999638375">
be common for some expressions. For exam-
ple, in an MWE identification study, Hashimoto
et al. (2006) built an identification system us-
ing hand crafted rules for some 100 Japanese id-
ioms. The results showed near perfect perfor-
mance on expressions without compositional/non-
compositional ambiguity but significantly poorer
performance on expressions with ambiguity.
</bodyText>
<subsectionHeader confidence="0.993339">
2.2 MWE Identification by ML
</subsectionHeader>
<bodyText confidence="0.999908810810811">
Katz and Giesbrecht (2006) used a supervised
learning method to distinguish between composi-
tional and non-compositional uses of an expres-
sion (in German text) by using contextual infor-
mation in the form of Latent Semantic Analy-
sis (LSA) vectors. LSA vectors of compositional
and non-compositional meaning were built from a
training set of example sentences and then a near-
est neighbor algorithm was applied on the LSA
vector of one tested MWE. The technique was
tested more thoroughly in Cook et al. (2007).
Cook et al. (2007) devised two unsupervised
methods to distinguish between compositional (lit-
eral) and non-compositional (idiomatic) tokens of
verb-object expressions. The first method is based
on an expression’s canonical form. In a previ-
ous study (Fazly and Stevenson, 2006), the authors
came up with a dozen possible syntactic forms for
verb-object pairs (based on passivization, deter-
miner, and object pluralization) and used a corpus-
based statistical measure to determine the canoni-
cal form(s). The method classifies new tokens as
idiomatic if they use a canonical form, and literal
otherwise.
The second method uses context as well as
form. Co-occurrence vectors representing the id-
iomatic and literal meaning of each expression
were computed based on corpus data. Idiomatic-
meaning vectors were based on examples match-
ing the expressions’ canonical form. Literal mean-
ing vectors were based on examples that did not
match the canonical form. New tokens were
classified as literal/idiomatic based on their (co-
occurrence) vector’s cosine similarity to the id-
iomatic and literal vectors.
(Sporleder and Li, 2009) also attempted to dis-
tinguish compositional from non-compositional
uses of expressions in text. Their assumption was
that if an expression is used literally, but not id-
iomatically, its component words will be related
semantically to several words in the surrounding
discourse. For example, when the expression ‘play
with fire’ is used literally, words such as ‘smoke,
‘burn’, ‘fire department’, and ‘alarm’ tend to also
be used nearby; when it is used idiomatically, they
aren’t (indeed, other words, e.g., ‘danger’ or ‘risk’
appear nearby but they are not close semantically
to ‘play’ or to ‘fire’). This property was used
to distinguish literal and non-literal instances by
measuring the semantic relatedness of an expres-
sion’s component words to nearby words in the
text. If one or more of the expression’s compo-
nents were sufficiently related to enough nearby
words, forming a ‘lexical chain’, the usage was
classified as literal. Otherwise it was idiomatic.
Two classifiers based on lexical chains were de-
vised. These were compared with a supervised
method that trains a classifier for each expression
based on surrounding context. The results showed
that the supervised classifier method did much bet-
ter (90% F-score on literal uses) than the lexical
chain classifier methods (60% F-score).
In the above studies the focus is on the
compositional/non-compositional expression dis-
tinction. The sentence data used contains exam-
ples of either one or the other. In (Sporleder and
Li, 2009) the experimental data included only sen-
tences in which the expressions were in canoni-
cal form (allowing for verb inflection). In (Cook
et al., 2007) a syntactic parser was used to col-
lect sentences containing the MWEs in the active
and passive voice using heuristics. Thus, exam-
ples such as the following (from the BNC) would
not be included in their sample:
</bodyText>
<listItem confidence="0.984793583333333">
1. take a chance: ‘While he still had a chance
of being near Maisie, he would take it’.
2. face the consequences: ‘... she did not have
to face, it appears, the possible serious or
even fatal consequences of her decision’.
3. make a distinction: ‘Logically, the distinc-
tion between the two aspects of the theory
can and should be made’.
4. break the ice: ‘The ice, if not broken, was
beginning to soften a little’.
5. settle a score: ‘Morrissey had another score
to settle’.
</listItem>
<bodyText confidence="0.799104">
This means that their experiments have not in-
cluded all types of sentences that might be encoun-
tered in practice when attempting MWE identifi-
</bodyText>
<page confidence="0.987651">
470
</page>
<bodyText confidence="0.999877636363636">
cation. Specifically, they would miss many ex-
amples in which the MWE words are present but
are not used as an expression (case (b) in Sec-
tion 1). Moreover, their heuristics are tailored
to the Verb-Direct Object MWE type. Different
heuristics would need to be employed for different
MWE types.
In our approach there is no pre-processing stage
requiring type-specific knowledge. Specifically,
the above examples are used as training sentences
in our experiments.
</bodyText>
<subsectionHeader confidence="0.998903">
2.3 MWE Extraction
</subsectionHeader>
<bodyText confidence="0.999948692307692">
There exists an extensive body of research on
MWE extraction (see Wermter and Hahn (2004)
for a review), where the only input is a corpus,
and the output is a list of MWEs found in it. Most
methods collect MWE candidates from the corpus,
score them according to some association measure
between their components, and accept candidates
with scores passing some threshold. The focus of
research has been on developing association mea-
sures, including statistical, information-theoretic
and linguistically motivated measures (e.g., Juste-
son and Katz (1995), Wermter and Hahn (2006),
and Deane (2005)).
</bodyText>
<sectionHeader confidence="0.996325" genericHeader="method">
3 MWE Identification Method
</sectionHeader>
<bodyText confidence="0.9999742">
Our method decides if a potential use of a
known expression in a given sentence is non-
compositional. The input to the method, for each
MWE, is a labeled training set of sentences con-
taining one or more potentially non-compositional
uses of the MWE. The output, for each MWE, is a
binary classifier, trained on those sentences. Thus,
we target step (2) of MWE identification, which is
the difficult one.
The learning algorithm used is Support Vector
Machine (SVM), which outputs a binary classifier,
using Sequential Minimal Optimization (Platt,
1998)1 in the Weka toolkit2 (Witten and Frank,
2000).
For training, sentences are converted into fea-
ture vectors. Features depend on the assignment
of the lexical components of the expression to spe-
cific tokens in the sentence. In some cases, there
are several tokens in the sentence that match a sin-
gle component in the expression, and this leads to
</bodyText>
<footnote confidence="0.9802915">
1Using the PUK kernel (The Pearson VII function-
based Universal Kernel), with parameters omega=1.0 and
sigma=1.0.
2Weka version 3.5.6; www.cs.waikato.ac.nz/ ml/ weka/
</footnote>
<bodyText confidence="0.999383652173913">
multiple (potential) assignments. So in the gen-
eral case a sentence is converted to a set of feature
vectors, each corresponding to a single assignment
of the MWE’s lexical components to sentence to-
kens.
Training sentences are labeled positive if they
contain a non-compositional use of the expression
and negative if they do not (i.e., literal and other
uses). If the sentence is positive, at least one of
the assignments is the true assignment (there may
be more than one, e.g., when an expression is used
twice in the same sentence). The vector matching
the true assignment is labeled positive. The others
are labeled negative. If the sentence is negative,
all of the vectors are labeled negative.
As mentioned, the output of the method is a
distinct binary classifier for each MWE. Although
having a single classifier for all expressions would
seem advantageous, the wide variation exhibited
by MWEs (e.g., for some the passive is common,
for other not at all) precludes this option and re-
quires having a separate classifier for each expres-
sion.
</bodyText>
<subsectionHeader confidence="0.93704">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999903315789474">
Surface features include order and distance, part-
of-speech and inflection of an expression’s words
in a sentence.
Use of surface features is intuitive and relatively
cheap. In addition, many studies have shown the
importance of order and distance in MWE extrac-
tion in English (two recent examples are (Dias,
2003; Deane, 2005)). Thus, we develop a super-
vised classifier based on surface features.
Many of the surface features make use of an
expression’s Canonical Form (CF), thus the learn-
ing algorithm assumes that it is given such a form.
Formally defining the CF is difficult. Indeed, some
researchers have concluded that some expressions
do not have a CF (Moon, 1998). For our purposes,
CF can be informally defined as the most frequent
form in which the expression appears. In practice,
an approximation of this definition, explained in
Section 4, is used.
</bodyText>
<subsectionHeader confidence="0.516855">
3.1.1 Surface Features
</subsectionHeader>
<listItem confidence="0.9928622">
1. Word Distance: The number of words be-
tween the leftmost and rightmost MWE to-
kens in the sentence.
2. Ordered Gap List: A list of gaps, measured
in number of words, between each pair of the
</listItem>
<page confidence="0.919003">
471
</page>
<listItem confidence="0.922756769230769">
expression’s tokens in their canonical form
order. For example, if the token locations (in
canonical form order) are 10, 7 and 3, the or-
dered gap list would be (10 ↔ 7 = 2,10 ↔
3 = 6,7 ↔ 3 = 3).
3. Word Order: A boolean value indicating
whether the expression’s word order in the
sentence matches the canonical form word
order.
4. Word Order Permutation: The permutation
of word order relative to the canonical form.
For example, the permutation (1,0,2) indi-
cates that component words 1 and 0 have
switched order in the sentence.
5. Inflection Ratio: The fraction of words in the
expression that have undergone inflection rel-
ative to the canonical form.
6. Lexical Values: A list of the tokens in the
sentence matching the expression’s compo-
nent words, ordered according to canonical
form. For example, if the expression is ‘make
a distinction’, a possible lexical values list
is (made,no,distinction) in the sentence ‘No
possible distinction can be made between the
two’.
7. POS Pattern: A boolean value indicating
</listItem>
<bodyText confidence="0.9531821">
whether the expression’s use in the sentence
has the same part-of-speech pattern as the
canonical form.
Two combinations of surface features are used
in the experiments below. The first, named R1,
uses all of the above features. The second, R2,
uses only Word Distance, Ordered Gap List and
Word Order Permutation. Using R2 the learner
has only word order and distance information from
which to create a classifier.
</bodyText>
<subsectionHeader confidence="0.722943">
3.1.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.977926476190476">
An expression’s words may appear unrelated in
a sentence, because of distance, order, part-of-
speech and other surface variations. However, the
words will still be closely related syntactically.
Syntactic analysis of the sentence in the form of
a dependency parse tree directly gives the syntac-
tic relationships between the expression’s compo-
nents. Thus, we also develop a classifier based on
syntactic features.
Dependency Parsing. A dependency parse tree
is a directed acyclic graph in which the nodes rep-
resent tokens in the sentence and the edges rep-
resent syntactic dependencies between the words
(e.g., direct-object, prepositional-object, noun-
subject etc.). The Stanford Parser3 (Marneffe et
al., 2006) was used.
Minimal Sub-Tree. To compute a syntactic fea-
ture, the dependency tree is computed and then the
minimal sub-tree containing the expression’s to-
kens is extracted.
The features are:
</bodyText>
<listItem confidence="0.931408333333333">
1. Sub-Tree Distance Sum: The number of
edges in the minimal sub-tree. A large num-
ber of edges suggests a weaker dependency.
2. Sub-Tree Distance List: A list of the dis-
tances of the MWE component nodes from
the root of their sub-tree.
3. Descendant Relations List: A list of descen-
dant relations between each pair of MWE
component nodes.
</listItem>
<bodyText confidence="0.999728625">
A descendant relation between two nodes ex-
ists if there is a directed path from one node
(the ancestor) to the other (the descendant).
Descendant relations are either direct (parent-
child) or indirect. The list consists of the lev-
els of descendant relations between the MWE
component nodes, which can be none, indi-
rect or direct.
</bodyText>
<listItem confidence="0.873360666666667">
4. Descendant Direction List: A list of the di-
rections of the descendant relations between
each pair of MWE component nodes.
</listItem>
<bodyText confidence="0.951551071428571">
If there are descendant relations between a
pair of nodes, the direction of the depen-
dency, indicating which is the modifying and
which the modified node, is important.
5. Sibling Relations List: A list of sibling rela-
tions between each pair of MWE component
nodes.
Two nodes are first degree siblings if they
share the same parent (which usually means
they modify the same word). Two nodes are
second degree siblings if they share a com-
mon ancestor no more than two edges away,
and so on. The list consists of the level of
sibling relations for each pair of component
</bodyText>
<footnote confidence="0.925115">
3http://www-nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.998015">
472
</page>
<bodyText confidence="0.9906965">
nodes, which can be first, second and third
degree.
</bodyText>
<listItem confidence="0.6977764">
6. Descendant Type List: A list of the depen-
dency types (e.g., subject, direct object etc.)
between each pair of component nodes. If the
component nodes are not direct descendants
their dependency type is null.
7. Sibling Type List: A list of pairs of depen-
dency types corresponding to the dependen-
cies between a pair of component nodes and
their common parent. If the component nodes
are not first degree siblings, the type is null.
</listItem>
<bodyText confidence="0.9999405">
In the experiments reported below, the classifier
using only the syntactic features is denoted by S,
and the one using all surface and all syntactic fea-
tures is denoted by C. We have experimented with
additional feature combinations, with no improve-
ment in results.
</bodyText>
<sectionHeader confidence="0.998857" genericHeader="method">
4 Experimental Method
</sectionHeader>
<bodyText confidence="0.989021210526316">
Canonical form. As described, an expression’s
canonical form (CF) is used in many of the learn-
ing algorithm’s features. The CF is taken from
Collins COBUILD Advanced Learner’s English
Dictionary (2003) which is also used as our source
for MWEs. COBUILD is an English-English dic-
tionary based on the Bank of English (BOE) cor-
pus (over 520 million words) with approximately
34,000 entries.
Traditional single-word dictionaries are a good
source for expressions because they usually list, as
part of single-word entries, expressions in which
the word is a component. The CF is not explic-
itly given in COBUILD, so an approximation is
the form which appears in the expression’s defini-
tion. This is a reasonable approximation since the
COBUILD authors claim to have selected typical
uses of the expressions in their definitions.
Each CF also has a matching part-of-speech
(POS) pattern, which is a list of the parts-
of-speech of the components in the CF. For
example, ‘walking on air’ has the pattern
(Verb, Preposition, Noun). COBUILD does
not include part-of-speech information for expres-
sions so this information was determined using the
British National Corpus (BNC) (BNC, 2001), a
(mostly) automatically POS tagged corpus (using
the CLAWS tagger). For each MWE, the POS pat-
terns of all instances of the CF in the corpus were
counted. The most frequent pattern is the expres-
sion’s POS pattern.
The expressions. A set of 17 verbal MWEs, the
development set, was used for development of the
surface and syntactic features described above. All
of the development set MWEs had the POS pattern
(Verb, Determiner, Noun). Another set of 24
verbal MWEs, the training/test set4, was then used
to test the method. Because the method is not spe-
cific to the (Verb, Determiner, Noun) pattern,
new POS patterns are included in the training/test
set. The training/test set consists of 8 MWEs
of the POS pattern (Verb, Determiner, Noun),
7 (Verb, Preposition, Noun) MWEs and and 9
(Verb, Noun, Preposition) MWEs. The list of
MWEs was selected randomly from the corre-
sponding POS pattern types. MWEs with a pos-
itive or negative percentage of under 5% in their
data set were discarded5. The MWEs, in their
canonical form, are:
Development set:
(Verb, Determiner, Noun) [17]: break the ice,
calls the shots, catch a cold, clear the air, face
the consequences, fits the bill, hit the road, make
a face, make a distinction, makes an impression,
raise the alarm, set an example, sound the alarm,
stay the course, take a chance, take the initiative,
tie the knot.
Training/test set:
(Verb, Determiner, Noun) [8]: changes the
subject, get a grip, get the picture, lead the way,
makes the grade, sets the scene, take a seat, take
the plunge;
(Verb, Preposition, Noun) [7]: fall into place,
goes to extremes, brought to justice, take to heart,
gets on nerves, keep up appearances, comes to
light;
(Verb, Noun, Preposition) [9]: take aim at,
make allowances for, takes advantage of, keep
hands off, lay claim to, take care of, make contact
with, gives rise to, wash hands of.
The sentences. As mentioned, the first step of
MWE identification is to identify if the sentence
contains a potential non-compositional use of the
expression. In order to test our method, which tar-
gets step (2), a set of such sentences (for each ex-
pression) was collected from the BNC corpus and
</bodyText>
<footnote confidence="0.994268333333333">
4Using 10-fold cross validation.
5Initially there were 20 MWEs in the development set and
30 (10 per group) in the training/test set.
</footnote>
<page confidence="0.998853">
473
</page>
<bodyText confidence="0.999761479166667">
then labeled for use as training/test sentences6.
The collection method was intended to allow
a wide range of variations in expression use. In
practice, for each expression sentences contain-
ing all of the expression’s CF components, in any
of their inflections, were collected, but excluding
common auxiliary words. So for example, when
targeting the MWE ‘make an impression’ we al-
lowed inflections of ‘make’ and ‘impression’ and
did not require ‘an’, to allow for variations such
as ‘make no impression’ and ‘make some impres-
sion’. For some expressions, sentences were lim-
ited to those with a distance of up to 8 words be-
tween each expression component. Very long sen-
tences (above 80 words) were discarded. The final
set of sentences was then randomly selected.
Given this method, training/test sentences al-
low non-lexical variations: inflection, word or-
der, part-of-speech, syntactic structure and other
non-syntactic transformations. Lexical variations
which involve a change in one of the expression’s
components are not allowed, except for common
auxiliary words.
For the development set an average of 97 (40-
137) sentences were collected per MWE, giving a
total of 1663 sentences, with a micro average of
49% positive labels. For the training/test set there
were 139 (73-150) sentences per MWE on aver-
age, totaling 3350, with a 40% average positive
ratio.
The sentences were manually labeled as posi-
tive if they contained a non-compositional use of
the MWE and negative if they contained a compo-
sitional or non-expression usage. Judgment was
based on a single sentence, without wider context.
Baseline methods. Two baseline methods are
used to test the intuitive notion that simple rule-
based methods are sufficient for MWE identifica-
tion as well as for comparison with the supervised
learning methods.
The first method, CanonicalForm (CF), accepts
a sentence use as a non-compositional MWE use
if and only if the MWE is in canonical form (there
are no intervening words between the MWE com-
ponents, their order matches canonical-form order,
and there is an inflection in at most one component
word).
The second method, DistanceOrder (DO), ac-
</bodyText>
<footnote confidence="0.996234666666667">
6The PyLucene software package, http://pylucene. os-
afoundation. org/, was used for building an index to the BNC
and for searching.
</footnote>
<table confidence="0.999758818181818">
CF DO R1 R2 S C
Verb-Det-Noun: All (17)
A 73.53 82.27 89.48 90.83 88.58 87.02
P 97.09 89.29 82.71 87.18 83.89 78.54
R 58.81 76.83 92.29 90.35 92.97 97.19
F 67.39 79.68 86.92 88.56 87.78 86.00
Verb-Det-Noun: Best (8)
A 84.51 91.56 95.33 95.48 92.52 93.27
P 95.90 85.70 92.50 95.63 91.12 87.63
R 73.50 89.80 97.25 95.25 95.83 98.50
F 78.63 86.29 94.70 95.36 93.44 92.25
</table>
<tableCaption confidence="0.9939825">
Table 1: Development set: Average performance over all
MWEs and best 8. Supervised classifiers outperform base-
lines. A: Accuracy; P: Positive Precision; R: Positive Recall;
F: F-Score.
</tableCaption>
<bodyText confidence="0.9983418">
cepts a sentence use if and only if the number of
words between the leftmost and rightmost MWE
components is less than or equal to 2 (not count-
ing the middle MWE component), and if the order
matches the canonical form order.
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99777970967742">
The baseline methods (CF and DO) and the super-
vised methods (R1,R2,S,C) were run on the devel-
opment and training/test sets. For the supervised
methods, for each MWE we used 10-fold cross-
validation7.
Tables 1 and 2 summarize the results for the de-
velopment and test sets, respectively. For the de-
velopment set, average results over all 17 MWEs
and over the best 8 MWEs (on R1), a group size
comparable to the test set, are shown. For the test
set, results over all 24 MWEs and the three MWE
types tested are shown.
The tables show average overall accuracy and
average precision, recall and F-score on posi-
tive instances, where the averages are taken over
the results of the individual MWEs (i.e., micro-
averaged).
Baselines. Baseline accuracy, (for DO) 82.27%
on the development set and 87.2% on the test set
(over all groups), is probably insufficient for many
NLP applications.
The baselines perform similarly in terms of av-
erage accuracy. CF does this with very high preci-
sion and low recall, while for DO recall improves
at the expense of precision. Looking at individ-
ual MWEs reveals that for expressions which al-
low more variation in terms of intervening words
7I.e., we ran 10 experiments where in each experiment we
divided the corresponding annotated sentence sets into 90%
training sentences and 10% test sentences, and the results re-
ported are the average of the 10 experiments.
</bodyText>
<page confidence="0.996951">
474
</page>
<table confidence="0.999963476190476">
CF DO R1 R2 S C
All (24)
A 86.16 87.15 93.50 91.61 89.73 91.50
P 94.16 80.38 93.08 93.16 89.86 89.26
R 68.86 86.88 93.00 89.74 88.94 93.33
F 75.53 80.70 94.86 93.09 87.77 92.80
Verb-Det-Noun (8)
A 89.08 89.08 93.83 93.65 90.07 91.33
P 95.44 84.13 92.88 94.00 91.04 89.25
R 73.30 88.53 97.50 95.50 91.57 97.63
F 80.97 84.91 95.09 94.71 91.21 93.08
Verb-Prep-Noun (7)
A 85.53 91.15 93.64 92.62 88.75 92.10
P 97.13 81.40 96.81 97.20 92.48 94.33
R 64.36 92.67 84.73 82.79 82.71 85.00
F 74.08 86.03 97.81 96.87 83.13 96.65
Verb-Noun-Prep (9)
A 84.06 82.32 93.11 88.99 90.18 91.18
P 90.72 76.26 90.78 89.73 86.78 85.89
R 68.41 80.90 95.44 90.03 91.44 96.00
F 71.82 72.82 92.69 89.14 88.33 89.99
</table>
<tableCaption confidence="0.86515425">
Table 2: Test set: Average performance over all MWEs and
by group. The best supervised classifier outperforms base-
lines in all groups. A: Accuracy; P: Precision; R: Recall; F:
F-Score.
</tableCaption>
<bodyText confidence="0.999068115384615">
and lexical change, DO outperforms CF. To name
a few, make an impression, raise the alarm, take
a chance and make allowances for. For example,
for take a chance intervening words are quite com-
mon, as in: ‘I’m taking a real chance on you.’, or
a change in determiner as in: ‘I preferred to take
my chances’. Indeed, CF showed poor precision
only for MWEs with a common literal usage. Two
such MWEs were present in the development set
(break the ice and tie the knot ) and two in the test
set (wash hands of and keep hands off).
Baselines versus supervised classifiers. As
shown in the tables, R1 outperforms the best base-
line in terms of accuracy in both test and devel-
opment. Moreover, the supervised classifiers are
more stable in their accuracy. For the develop-
ment set the standard deviation of accuracy scores
averages 22.58 for CF and DO, and 6.68 for R1,
R2, S, and C. For the test set the baselines av-
erage 9.07 (Verb-Det-Noun), 11.11 (Verb-Prep-
Noun) and 14.26 (Verb-Noun-Prep), and the su-
pervised methods average 4.97 (Verb-Det-Noun),
7.66 (Verb-Prep-Noun) and 7.97. This stability
means that the supervised classifiers are able to
perform well on MWEs with different behavior.
For example, R1 is able to perform well on ex-
pressions where order is strict, as DO does (e.g.,
make a face), while also performing well on those
where order varies (e.g., make a distinction).
Supervised classifiers. R1 and R2, based on
surface features, show similar accuracy values,
with R1 doing somewhat better in the Verb-Prep-
Noun and Verb-Noun-Prep groups. This is due
to the Lexical Values feature, which accounts for
a change in preposition. A change in preposi-
tion (as in ‘wash hands of some matter’ versus
‘wash hands in the sink’) is more significant than
a change in determiner in the Verb-Determiner-
Noun group. This improves precision on negative
instances, which are rejected more precisely based
on the preposition value. Nevertheless, the rela-
tively simple features in R2, essentially order and
distance, perform quite well.
The F-score result for R1, 94.86, is an improve-
ment over the F-score result of the supervised clas-
sifier used in (Sporleder and Li, 2009), 90.15.
Although the sentence data is different (our data
includes sentences with non-expression uses) the
number of sentences used is similar.
S, based on syntactic features, performs worse
than R1/2. It shows better accuracy than the base-
lines in all but the (Verb-Prep-Noun) group and
is also more stable. C, a combination of surface
and syntactic features, performs better than S and
slightly worse than R1/2.
Why do the syntactic features perform worse
than surface features? An analysis of the S clas-
sifier errors reveals two important causes. First,
there is substantial variation in the dependency
tree structures of the non-compositional uses of
the expressions as output by the parser. Thus,
the syntactic feature classifier was more difficult
to learn than the surface feature one, requiring a
larger training set. This is not surprising, given
that many MWEs exhibit an irregular syntactic be-
havior that might even seem strange at times. For
example, in the sentence fragment “and then he
came to.”, ‘came to’ is an MWE. A parser might
find it difficult to parse the sentence correctly, ex-
pecting a noun phrase to follow the ‘to’.
Second, as described above, the syntactic fea-
tures consist of general syntactic relations ex-
tracted from the parse tree and not type-specific
knowledge. As a result, literal or non-expression
uses of the MWE’s components, which have a
close syntactic relation in a given sentence, appear
as non-compositional uses of the expression to the
classifier.
</bodyText>
<page confidence="0.998889">
475
</page>
<sectionHeader confidence="0.999576" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99998125">
This study has addressed MWE identification: de-
ciding if a potential use of an expression is a non-
compositional one. Despite its importance in ba-
sic NLP tasks, the problem has been largely over-
looked in NLP research, probably due to it pre-
sumed simplicity. However, as we have shown,
simple methods for MWE identification, such as
our baselines, do not perform consistently well
across MWEs. This study serves to highlight this
point and the need for more sophisticated methods
for MWE identification.
We have shown that using a supervised learning
method employing surface sentence features based
on canonical form, it is possible to improve perfor-
mance significantly. Unlike previous research, our
method is not tailored to specific MWE types, and
we did not ignore non-expression uses in our ex-
periments.
Future research should experiment with non-
verbal MWEs, since our features are not spe-
cific to verbal MWE types. Another direction is
a more sophisticated corpus sampling algorithm.
The current work ignored MWEs which had an un-
balanced training set (usually too few positives).
Methods for gathering enough positive instances
of such MWEs will be useful for testing the meth-
ods proposed here, as well as for general MWE
research.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999842171428571">
I˜nki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza,
Koldo Gojenola and Ruben Urizar. 2004. Repre-
sentation and treatment of multiword expressions in
Basque. ACL ’04 Workshop on Multiword Expres-
sions.
The British National Corpus. 2001. The British
National Corpus, version 2 (BNC World). Dis-
tributed by Oxford University Computing Ser-
vices on behalf of the BNC Consortium. URL:
http://www.natcorp.ox.ac.uk/
Elisabeth Breidt, Frederique Segond, and Giuseppen
Valetto. 1996. Local grammars for the description
of multi-word lexemes and their automatic recogni-
tion in texts. COMPLEX ’96. Budapest.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. ACL ’07 Workshop on A
Broader Perspective on Multiword Expressions.
Collins COBUILD. 2003. Collins COBUILD Ad-
vanced Learner’s English Dictionary. Harper-
Collins Publishers, 4th edition.
Paul Deane. 2005. A nonparametric method for ex-
traction of candidate phrasal terms. ACL ’05.
Gael Dias. 2003. Multiword unit hybrid extrac-
tion. ACL ’03 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. EACL ’06.
Christiane Fellbaum, Alexander Geyken, Axel Herold,
Fabian Koerner, and Gerald Neumann. 2006.
Corpus-based studies of German idioms and light
verbs. International Journal of Lexicography,
19(4):349–361.
Christiane Fellbaum. 1998. Towards a representation
of idioms in WordNet. COLING-ACL ’98 Workshop
on the Use of WordNet in Natural Language Pro-
cessing Systems.
Nicole Gr´egoire. 2007. Design and implementation of
a lexicon of Dutch multiword expressions. ACL ’07
Workshop on A Broader Perspective on Multiword
Expressions.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006. Japanese idiom recognition: Drawing a line
between literal and idiomatic meanings. COLING-
ACL ’06, Poster Sessions.
John S. Justeson and Slava M. Katz. 1995. Technical
terminology: some linguistic properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1:9–27.
Graham Katz and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis.
COLING-ACL ’06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties..
Geoffrey Leech, Roger Garside and Michael Bryant.
1994. CLAWS4: The tagging of the British Na-
tional Corpus. COLING ’94.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
LREC ’06.
Rosamund Moon. 1998. Fixed Expressions and Id-
ioms in English. Oxford: Clarendon Press.
John Platt. 1998. Machines using sequential minimal
optimization. In In B. Schoelkopf and C. Burges and
A. Smola, editors, Advances in Kernel Methods –
Support Vector Learning.
</reference>
<page confidence="0.988724">
476
</page>
<reference confidence="0.9992989">
Susanne Z. Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. Thesis.
Stanford.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. EACL ’09.
Aline Villavicencio, Ann Copestake, Benjamin Wal-
dron, and Fabre Lambeau. 2004. Lexical encoding
of MWE. ACL ’04 Workshop on Multiword Expres-
sions.
Joachim Wermter and Udo Hahn. 2004. Collocation
extraction based on modifiability statistics. COL-
ING ’04.
Joachim Wermter and Udo Hahn. 2006. You can’t beat
frequency (unless you use linguistic knowledge) –
a qualitative evaluation of association measures for
collocation and term extraction. COLING-ACL ’06.
Ian H. Witten amd Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann.
</reference>
<page confidence="0.998423">
477
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961128">
<title confidence="0.999956">Multi-Word Expression Identification Using Sentence Surface Features</title>
<author confidence="0.999915">Ram Boukobza Ari Rappoport</author>
<affiliation confidence="0.9998255">School of Computer Science School of Computer Science Hebrew University of Jerusalem Hebrew University of Jerusalem</affiliation>
<email confidence="0.976885">ram.boukobza@mail.huji.ac.ilarir@cs.huji.ac.il</email>
<abstract confidence="0.999407592592592">Much NLP research on Multi-Word Expressions (MWEs) focuses on the discovery of new expressions, as opposed to the identification in texts of known expressions. However, MWE identification is not trivial because many expressions allow variation in form and differ in the range of variations they allow. We show that simple rule-based baselines do not perform identification satisfactorily, and present a supervised learning method for identification that uses sentence surface features based on expressions’ canonical form. To evaluate the method, we have annotated 3350 sentences from the British National Corpus, containing potential uses of 24 verbal MWEs. The method achieves an F-score of 94.86%, compared with 80.70% for the leading rule-based baseline. Our method is easily applicable to any expression type. Experiments in previous research have been limited to the compositional/non-compositional distinction, while we also test on sentences in which the words comprising the MWE appear but not as an expression.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I˜nki Alegria</author>
</authors>
<title>Olatz Ansa, Xabier Artola, Nerea Ezeiza, Koldo Gojenola and</title>
<date>2004</date>
<booktitle>in Basque. ACL ’04 Workshop on Multiword Expressions.</booktitle>
<marker>Alegria, 2004</marker>
<rawString>I˜nki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza, Koldo Gojenola and Ruben Urizar. 2004. Representation and treatment of multiword expressions in Basque. ACL ’04 Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<title>The British National Corpus.</title>
<date>2001</date>
<booktitle>The British National Corpus, version 2 (BNC World). Distributed by Oxford University Computing Services on behalf of the BNC Consortium. URL: http://www.natcorp.ox.ac.uk/</booktitle>
<contexts>
<context position="2363" citStr="(2001)" startWordPosition="367" endWordPosition="367"> MWEs in text has received less attention, but is necessary for many NLP applications, for example in machine translation. The current work deals with the MWE identification task: deciding if a sentence contains a use of a known expression. MWE identification is not as simple as may initially appear, as will be shown by the performance of two rule-based baselines in our experiments. One source of difficulty is variations in expressions’ usage in text. Although MWEs generally show less variation than single words, they show enough that it cannot be ignored. In a study on V+NP idioms, Riehemann (2001) found that the idioms’ canonical form accounted for 75% of their appearances in a corpus. Additionally, expressions differ considerably in the types of variations they allow, which include passivization, nominalization and addition of modifying words (Moon, 1998). A second source of difficulty is that expressions consisting of very frequent words will often cooccur in sentences in a non-MWE usage and in similar but distinct expressions. MWE identification can be modeled as a two step process. Given a sentence and a known expression, step (1) is to decide if the sentence contains a potential u</context>
</contexts>
<marker>2001</marker>
<rawString>The British National Corpus. 2001. The British National Corpus, version 2 (BNC World). Distributed by Oxford University Computing Services on behalf of the BNC Consortium. URL: http://www.natcorp.ox.ac.uk/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elisabeth Breidt</author>
<author>Frederique Segond</author>
<author>Giuseppen Valetto</author>
</authors>
<title>Local grammars for the description of multi-word lexemes and their automatic recognition in texts.</title>
<date>1996</date>
<journal>COMPLEX</journal>
<volume>96</volume>
<location>Budapest.</location>
<contexts>
<context position="7099" citStr="Breidt et al., 1996" startWordPosition="1152" endWordPosition="1155">st baseline. Section 2 reviews previous work. Section 3 discusses the features used for the supervised classifier. Section 4 explains the experimental setting. The results and a discussion are given in sections 5 and 6. 2 Previous Work 2.1 MWE Lexical Encoding The approach to handling MWEs in early systems was to employ a list of expressions, each with a quasi regular expression that encodes morphosyntactic variations. One example is Leech et al. (1994) who used this method for automatic part-of-speech tagging for the BNC. Another is a formalism called IDAREX (IDioms And Regular EXpressions) (Breidt et al., 1996). More recent research emphasizes the integration of MWE lexical entries into existing single word lexicons and grammar systems (Villavicencio et al., 2004; Alegria et al., 2004). There is also an attempt to take advantage of regularities in morpho-syntactic properties across MWE groups, which allows encoding the behavior of the group instead of individual expressions (Villavicencio et al., 2004; Gr´egoire, 2007). Fellbaum (1998) discusses some difficulties in representing idioms, which are largely figurative in meaning, in WordNet. More recent work (Fellbaum et al., 2006) focuses on German VP</context>
</contexts>
<marker>Breidt, Segond, Valetto, 1996</marker>
<rawString>Elisabeth Breidt, Frederique Segond, and Giuseppen Valetto. 1996. Local grammars for the description of multi-word lexemes and their automatic recognition in texts. COMPLEX ’96. Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context.</title>
<date>2007</date>
<booktitle>ACL ’07 Workshop on A Broader Perspective on Multiword Expressions.</booktitle>
<contexts>
<context position="5897" citStr="Cook et al., 2007" startWordPosition="960" endWordPosition="963">, which can be quite common in practice. Our approach is more general. Given a set of sentences with potential MWE uses, we use sentence surface features to create a Support Vector Machine (SVM) classifier for each expression. The classifier is binary and differentiates between non-compositional uses of the expression ((d) above) on the one hand, and compositional and non-expression uses ((b) and (c)) on the other. The experiments and results presented below focus on verbal MWEs, since verbal MWEs are quite common in language use and have also been investigated in related MWE research (e.g., (Cook et al., 2007)). However, the developed features are not specific to a particular type of expression. The supervised method is compared with two simple rule-based baselines in order to test whether a simple approach is sufficient. In addition, the use of surface features is compared with the use of syntactic features (based on dependency parse trees of the sentences). Averaged over expressions in an independent test set, the supervised classifiers outperform the rule-based baselines, with F-scores of 94.86% (surface features) and 87.77% (syntactic features), compared with 80.70% for the best baseline. Secti</context>
<context position="9028" citStr="Cook et al. (2007)" startWordPosition="1458" endWordPosition="1461">onal ambiguity but significantly poorer performance on expressions with ambiguity. 2.2 MWE Identification by ML Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analysis (LSA) vectors. LSA vectors of compositional and non-compositional meaning were built from a training set of example sentences and then a nearest neighbor algorithm was applied on the LSA vector of one tested MWE. The technique was tested more thoroughly in Cook et al. (2007). Cook et al. (2007) devised two unsupervised methods to distinguish between compositional (literal) and non-compositional (idiomatic) tokens of verb-object expressions. The first method is based on an expression’s canonical form. In a previous study (Fazly and Stevenson, 2006), the authors came up with a dozen possible syntactic forms for verb-object pairs (based on passivization, determiner, and object pluralization) and used a corpusbased statistical measure to determine the canonical form(s). The method classifies new tokens as idiomatic if they use a canonical form, and literal otherwise.</context>
<context position="11784" citStr="Cook et al., 2007" startWordPosition="1887" endWordPosition="1890">e were compared with a supervised method that trains a classifier for each expression based on surrounding context. The results showed that the supervised classifier method did much better (90% F-score on literal uses) than the lexical chain classifier methods (60% F-score). In the above studies the focus is on the compositional/non-compositional expression distinction. The sentence data used contains examples of either one or the other. In (Sporleder and Li, 2009) the experimental data included only sentences in which the expressions were in canonical form (allowing for verb inflection). In (Cook et al., 2007) a syntactic parser was used to collect sentences containing the MWEs in the active and passive voice using heuristics. Thus, examples such as the following (from the BNC) would not be included in their sample: 1. take a chance: ‘While he still had a chance of being near Maisie, he would take it’. 2. face the consequences: ‘... she did not have to face, it appears, the possible serious or even fatal consequences of her decision’. 3. make a distinction: ‘Logically, the distinction between the two aspects of the theory can and should be made’. 4. break the ice: ‘The ice, if not broken, was begin</context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2007</marker>
<rawString>Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2007. Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context. ACL ’07 Workshop on A Broader Perspective on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collins COBUILD</author>
</authors>
<date>2003</date>
<booktitle>Collins COBUILD Advanced Learner’s English Dictionary. HarperCollins Publishers, 4th edition.</booktitle>
<marker>COBUILD, 2003</marker>
<rawString>Collins COBUILD. 2003. Collins COBUILD Advanced Learner’s English Dictionary. HarperCollins Publishers, 4th edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Deane</author>
</authors>
<title>A nonparametric method for extraction of candidate phrasal terms.</title>
<date>2005</date>
<journal>ACL</journal>
<volume>05</volume>
<contexts>
<context position="13691" citStr="Deane (2005)" startWordPosition="2206" endWordPosition="2207">3 MWE Extraction There exists an extensive body of research on MWE extraction (see Wermter and Hahn (2004) for a review), where the only input is a corpus, and the output is a list of MWEs found in it. Most methods collect MWE candidates from the corpus, score them according to some association measure between their components, and accept candidates with scores passing some threshold. The focus of research has been on developing association measures, including statistical, information-theoretic and linguistically motivated measures (e.g., Justeson and Katz (1995), Wermter and Hahn (2006), and Deane (2005)). 3 MWE Identification Method Our method decides if a potential use of a known expression in a given sentence is noncompositional. The input to the method, for each MWE, is a labeled training set of sentences containing one or more potentially non-compositional uses of the MWE. The output, for each MWE, is a binary classifier, trained on those sentences. Thus, we target step (2) of MWE identification, which is the difficult one. The learning algorithm used is Support Vector Machine (SVM), which outputs a binary classifier, using Sequential Minimal Optimization (Platt, 1998)1 in the Weka toolk</context>
<context position="16173" citStr="Deane, 2005" startWordPosition="2610" endWordPosition="2611">ach MWE. Although having a single classifier for all expressions would seem advantageous, the wide variation exhibited by MWEs (e.g., for some the passive is common, for other not at all) precludes this option and requires having a separate classifier for each expression. 3.1 Features Surface features include order and distance, partof-speech and inflection of an expression’s words in a sentence. Use of surface features is intuitive and relatively cheap. In addition, many studies have shown the importance of order and distance in MWE extraction in English (two recent examples are (Dias, 2003; Deane, 2005)). Thus, we develop a supervised classifier based on surface features. Many of the surface features make use of an expression’s Canonical Form (CF), thus the learning algorithm assumes that it is given such a form. Formally defining the CF is difficult. Indeed, some researchers have concluded that some expressions do not have a CF (Moon, 1998). For our purposes, CF can be informally defined as the most frequent form in which the expression appears. In practice, an approximation of this definition, explained in Section 4, is used. 3.1.1 Surface Features 1. Word Distance: The number of words bet</context>
</contexts>
<marker>Deane, 2005</marker>
<rawString>Paul Deane. 2005. A nonparametric method for extraction of candidate phrasal terms. ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gael Dias</author>
</authors>
<title>Multiword unit hybrid extraction.</title>
<date>2003</date>
<booktitle>ACL ’03 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment.</booktitle>
<contexts>
<context position="16159" citStr="Dias, 2003" startWordPosition="2608" endWordPosition="2609">sifier for each MWE. Although having a single classifier for all expressions would seem advantageous, the wide variation exhibited by MWEs (e.g., for some the passive is common, for other not at all) precludes this option and requires having a separate classifier for each expression. 3.1 Features Surface features include order and distance, partof-speech and inflection of an expression’s words in a sentence. Use of surface features is intuitive and relatively cheap. In addition, many studies have shown the importance of order and distance in MWE extraction in English (two recent examples are (Dias, 2003; Deane, 2005)). Thus, we develop a supervised classifier based on surface features. Many of the surface features make use of an expression’s Canonical Form (CF), thus the learning algorithm assumes that it is given such a form. Formally defining the CF is difficult. Indeed, some researchers have concluded that some expressions do not have a CF (Moon, 1998). For our purposes, CF can be informally defined as the most frequent form in which the expression appears. In practice, an approximation of this definition, explained in Section 4, is used. 3.1.1 Surface Features 1. Word Distance: The numbe</context>
</contexts>
<marker>Dias, 2003</marker>
<rawString>Gael Dias. 2003. Multiword unit hybrid extraction. ACL ’03 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically constructing a lexicon of verb phrase idiomatic combinations.</title>
<date>2006</date>
<journal>EACL</journal>
<volume>06</volume>
<contexts>
<context position="9306" citStr="Fazly and Stevenson, 2006" startWordPosition="1498" endWordPosition="1501"> by using contextual information in the form of Latent Semantic Analysis (LSA) vectors. LSA vectors of compositional and non-compositional meaning were built from a training set of example sentences and then a nearest neighbor algorithm was applied on the LSA vector of one tested MWE. The technique was tested more thoroughly in Cook et al. (2007). Cook et al. (2007) devised two unsupervised methods to distinguish between compositional (literal) and non-compositional (idiomatic) tokens of verb-object expressions. The first method is based on an expression’s canonical form. In a previous study (Fazly and Stevenson, 2006), the authors came up with a dozen possible syntactic forms for verb-object pairs (based on passivization, determiner, and object pluralization) and used a corpusbased statistical measure to determine the canonical form(s). The method classifies new tokens as idiomatic if they use a canonical form, and literal otherwise. The second method uses context as well as form. Co-occurrence vectors representing the idiomatic and literal meaning of each expression were computed based on corpus data. Idiomaticmeaning vectors were based on examples matching the expressions’ canonical form. Literal meaning</context>
</contexts>
<marker>Fazly, Stevenson, 2006</marker>
<rawString>Afsaneh Fazly and Suzanne Stevenson. 2006. Automatically constructing a lexicon of verb phrase idiomatic combinations. EACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
<author>Alexander Geyken</author>
<author>Axel Herold</author>
<author>Fabian Koerner</author>
<author>Gerald Neumann</author>
</authors>
<title>Corpus-based studies of German idioms and light verbs.</title>
<date>2006</date>
<journal>International Journal of Lexicography,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="7678" citStr="Fellbaum et al., 2006" startWordPosition="1239" endWordPosition="1242"> Regular EXpressions) (Breidt et al., 1996). More recent research emphasizes the integration of MWE lexical entries into existing single word lexicons and grammar systems (Villavicencio et al., 2004; Alegria et al., 2004). There is also an attempt to take advantage of regularities in morpho-syntactic properties across MWE groups, which allows encoding the behavior of the group instead of individual expressions (Villavicencio et al., 2004; Gr´egoire, 2007). Fellbaum (1998) discusses some difficulties in representing idioms, which are largely figurative in meaning, in WordNet. More recent work (Fellbaum et al., 2006) focuses on German VP idioms. As already mentioned, one issue with lexical encoding is that it is done manually, making lexicons difficult to create, maintain and extend. The use of regularities among different types of MWEs is one way of reducing the amount of work required. A second issue is that implementations tend to ignore the likelihood and even the possibility of compositional and other interpretations of expressions in text, which can 469 be common for some expressions. For example, in an MWE identification study, Hashimoto et al. (2006) built an identification system using hand craft</context>
</contexts>
<marker>Fellbaum, Geyken, Herold, Koerner, Neumann, 2006</marker>
<rawString>Christiane Fellbaum, Alexander Geyken, Axel Herold, Fabian Koerner, and Gerald Neumann. 2006. Corpus-based studies of German idioms and light verbs. International Journal of Lexicography, 19(4):349–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>Towards a representation of idioms in WordNet.</title>
<date>1998</date>
<booktitle>COLING-ACL ’98 Workshop on the Use of WordNet in Natural Language Processing Systems.</booktitle>
<contexts>
<context position="7532" citStr="Fellbaum (1998)" startWordPosition="1218" endWordPosition="1219">Leech et al. (1994) who used this method for automatic part-of-speech tagging for the BNC. Another is a formalism called IDAREX (IDioms And Regular EXpressions) (Breidt et al., 1996). More recent research emphasizes the integration of MWE lexical entries into existing single word lexicons and grammar systems (Villavicencio et al., 2004; Alegria et al., 2004). There is also an attempt to take advantage of regularities in morpho-syntactic properties across MWE groups, which allows encoding the behavior of the group instead of individual expressions (Villavicencio et al., 2004; Gr´egoire, 2007). Fellbaum (1998) discusses some difficulties in representing idioms, which are largely figurative in meaning, in WordNet. More recent work (Fellbaum et al., 2006) focuses on German VP idioms. As already mentioned, one issue with lexical encoding is that it is done manually, making lexicons difficult to create, maintain and extend. The use of regularities among different types of MWEs is one way of reducing the amount of work required. A second issue is that implementations tend to ignore the likelihood and even the possibility of compositional and other interpretations of expressions in text, which can 469 be</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. Towards a representation of idioms in WordNet. COLING-ACL ’98 Workshop on the Use of WordNet in Natural Language Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicole Gr´egoire</author>
</authors>
<title>Design and implementation of a lexicon of Dutch multiword expressions.</title>
<date>2007</date>
<booktitle>ACL ’07 Workshop on A Broader Perspective on Multiword Expressions.</booktitle>
<marker>Gr´egoire, 2007</marker>
<rawString>Nicole Gr´egoire. 2007. Design and implementation of a lexicon of Dutch multiword expressions. ACL ’07 Workshop on A Broader Perspective on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Satoshi Sato</author>
<author>Takehito Utsuro</author>
</authors>
<title>Japanese idiom recognition: Drawing a line between literal and idiomatic meanings. COLINGACL ’06, Poster Sessions.</title>
<date>2006</date>
<contexts>
<context position="8230" citStr="Hashimoto et al. (2006)" startWordPosition="1334" endWordPosition="1337">rative in meaning, in WordNet. More recent work (Fellbaum et al., 2006) focuses on German VP idioms. As already mentioned, one issue with lexical encoding is that it is done manually, making lexicons difficult to create, maintain and extend. The use of regularities among different types of MWEs is one way of reducing the amount of work required. A second issue is that implementations tend to ignore the likelihood and even the possibility of compositional and other interpretations of expressions in text, which can 469 be common for some expressions. For example, in an MWE identification study, Hashimoto et al. (2006) built an identification system using hand crafted rules for some 100 Japanese idioms. The results showed near perfect performance on expressions without compositional/noncompositional ambiguity but significantly poorer performance on expressions with ambiguity. 2.2 MWE Identification by ML Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analysis (LSA) vectors. LSA vectors of compositional and non-compositional meaning wer</context>
</contexts>
<marker>Hashimoto, Sato, Utsuro, 2006</marker>
<rawString>Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro. 2006. Japanese idiom recognition: Drawing a line between literal and idiomatic meanings. COLINGACL ’06, Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Justeson</author>
<author>Slava M Katz</author>
</authors>
<title>Technical terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering,</title>
<date>1995</date>
<contexts>
<context position="13648" citStr="Justeson and Katz (1995)" startWordPosition="2196" endWordPosition="2200">s are used as training sentences in our experiments. 2.3 MWE Extraction There exists an extensive body of research on MWE extraction (see Wermter and Hahn (2004) for a review), where the only input is a corpus, and the output is a list of MWEs found in it. Most methods collect MWE candidates from the corpus, score them according to some association measure between their components, and accept candidates with scores passing some threshold. The focus of research has been on developing association measures, including statistical, information-theoretic and linguistically motivated measures (e.g., Justeson and Katz (1995), Wermter and Hahn (2006), and Deane (2005)). 3 MWE Identification Method Our method decides if a potential use of a known expression in a given sentence is noncompositional. The input to the method, for each MWE, is a labeled training set of sentences containing one or more potentially non-compositional uses of the MWE. The output, for each MWE, is a binary classifier, trained on those sentences. Thus, we target step (2) of MWE identification, which is the difficult one. The learning algorithm used is Support Vector Machine (SVM), which outputs a binary classifier, using Sequential Minimal Op</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>John S. Justeson and Slava M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering, 1:9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multiword expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>COLING-ACL ’06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties..</booktitle>
<contexts>
<context position="8548" citStr="Katz and Giesbrecht (2006)" startWordPosition="1379" endWordPosition="1382">ing the amount of work required. A second issue is that implementations tend to ignore the likelihood and even the possibility of compositional and other interpretations of expressions in text, which can 469 be common for some expressions. For example, in an MWE identification study, Hashimoto et al. (2006) built an identification system using hand crafted rules for some 100 Japanese idioms. The results showed near perfect performance on expressions without compositional/noncompositional ambiguity but significantly poorer performance on expressions with ambiguity. 2.2 MWE Identification by ML Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analysis (LSA) vectors. LSA vectors of compositional and non-compositional meaning were built from a training set of example sentences and then a nearest neighbor algorithm was applied on the LSA vector of one tested MWE. The technique was tested more thoroughly in Cook et al. (2007). Cook et al. (2007) devised two unsupervised methods to distinguish between compositional (literal) and non-composition</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Graham Katz and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multiword expressions using latent semantic analysis. COLING-ACL ’06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>CLAWS4: The tagging of the British National Corpus.</title>
<date>1994</date>
<journal>COLING</journal>
<volume>94</volume>
<contexts>
<context position="6936" citStr="Leech et al. (1994)" startWordPosition="1127" endWordPosition="1130">vised classifiers outperform the rule-based baselines, with F-scores of 94.86% (surface features) and 87.77% (syntactic features), compared with 80.70% for the best baseline. Section 2 reviews previous work. Section 3 discusses the features used for the supervised classifier. Section 4 explains the experimental setting. The results and a discussion are given in sections 5 and 6. 2 Previous Work 2.1 MWE Lexical Encoding The approach to handling MWEs in early systems was to employ a list of expressions, each with a quasi regular expression that encodes morphosyntactic variations. One example is Leech et al. (1994) who used this method for automatic part-of-speech tagging for the BNC. Another is a formalism called IDAREX (IDioms And Regular EXpressions) (Breidt et al., 1996). More recent research emphasizes the integration of MWE lexical entries into existing single word lexicons and grammar systems (Villavicencio et al., 2004; Alegria et al., 2004). There is also an attempt to take advantage of regularities in morpho-syntactic properties across MWE groups, which allows encoding the behavior of the group instead of individual expressions (Villavicencio et al., 2004; Gr´egoire, 2007). Fellbaum (1998) dis</context>
</contexts>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Geoffrey Leech, Roger Garside and Michael Bryant. 1994. CLAWS4: The tagging of the British National Corpus. COLING ’94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<journal>LREC</journal>
<volume>06</volume>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. LREC ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosamund Moon</author>
</authors>
<title>Fixed Expressions and Idioms in English.</title>
<date>1998</date>
<publisher>Clarendon Press.</publisher>
<location>Oxford:</location>
<contexts>
<context position="2627" citStr="Moon, 1998" startWordPosition="403" endWordPosition="404"> is not as simple as may initially appear, as will be shown by the performance of two rule-based baselines in our experiments. One source of difficulty is variations in expressions’ usage in text. Although MWEs generally show less variation than single words, they show enough that it cannot be ignored. In a study on V+NP idioms, Riehemann (2001) found that the idioms’ canonical form accounted for 75% of their appearances in a corpus. Additionally, expressions differ considerably in the types of variations they allow, which include passivization, nominalization and addition of modifying words (Moon, 1998). A second source of difficulty is that expressions consisting of very frequent words will often cooccur in sentences in a non-MWE usage and in similar but distinct expressions. MWE identification can be modeled as a two step process. Given a sentence and a known expression, step (1) is to decide if the sentence contains a potential use of the expression. This is a relatively simple step based on the appearance in the sentence of the words comprising the MWE. Step (2) is to decide if the potential use is indeed non-compositional. Consider the following sentences with regard to the expression h</context>
<context position="16518" citStr="Moon, 1998" startWordPosition="2668" endWordPosition="2669">nd inflection of an expression’s words in a sentence. Use of surface features is intuitive and relatively cheap. In addition, many studies have shown the importance of order and distance in MWE extraction in English (two recent examples are (Dias, 2003; Deane, 2005)). Thus, we develop a supervised classifier based on surface features. Many of the surface features make use of an expression’s Canonical Form (CF), thus the learning algorithm assumes that it is given such a form. Formally defining the CF is difficult. Indeed, some researchers have concluded that some expressions do not have a CF (Moon, 1998). For our purposes, CF can be informally defined as the most frequent form in which the expression appears. In practice, an approximation of this definition, explained in Section 4, is used. 3.1.1 Surface Features 1. Word Distance: The number of words between the leftmost and rightmost MWE tokens in the sentence. 2. Ordered Gap List: A list of gaps, measured in number of words, between each pair of the 471 expression’s tokens in their canonical form order. For example, if the token locations (in canonical form order) are 10, 7 and 3, the ordered gap list would be (10 ↔ 7 = 2,10 ↔ 3 = 6,7 ↔ 3 =</context>
</contexts>
<marker>Moon, 1998</marker>
<rawString>Rosamund Moon. 1998. Fixed Expressions and Idioms in English. Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Machines using sequential minimal optimization. In</title>
<date>1998</date>
<booktitle>Advances in Kernel Methods – Support Vector Learning.</booktitle>
<editor>In B. Schoelkopf and C. Burges and A. Smola, editors,</editor>
<contexts>
<context position="14272" citStr="Platt, 1998" startWordPosition="2300" endWordPosition="2301">d Hahn (2006), and Deane (2005)). 3 MWE Identification Method Our method decides if a potential use of a known expression in a given sentence is noncompositional. The input to the method, for each MWE, is a labeled training set of sentences containing one or more potentially non-compositional uses of the MWE. The output, for each MWE, is a binary classifier, trained on those sentences. Thus, we target step (2) of MWE identification, which is the difficult one. The learning algorithm used is Support Vector Machine (SVM), which outputs a binary classifier, using Sequential Minimal Optimization (Platt, 1998)1 in the Weka toolkit2 (Witten and Frank, 2000). For training, sentences are converted into feature vectors. Features depend on the assignment of the lexical components of the expression to specific tokens in the sentence. In some cases, there are several tokens in the sentence that match a single component in the expression, and this leads to 1Using the PUK kernel (The Pearson VII functionbased Universal Kernel), with parameters omega=1.0 and sigma=1.0. 2Weka version 3.5.6; www.cs.waikato.ac.nz/ ml/ weka/ multiple (potential) assignments. So in the general case a sentence is converted to a se</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>John Platt. 1998. Machines using sequential minimal optimization. In In B. Schoelkopf and C. Burges and A. Smola, editors, Advances in Kernel Methods – Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susanne Z Riehemann</author>
</authors>
<title>A Constructional Approach to Idioms and Word Formation.</title>
<date>2001</date>
<tech>Ph.D. Thesis. Stanford.</tech>
<contexts>
<context position="2363" citStr="Riehemann (2001)" startWordPosition="366" endWordPosition="367">n of known MWEs in text has received less attention, but is necessary for many NLP applications, for example in machine translation. The current work deals with the MWE identification task: deciding if a sentence contains a use of a known expression. MWE identification is not as simple as may initially appear, as will be shown by the performance of two rule-based baselines in our experiments. One source of difficulty is variations in expressions’ usage in text. Although MWEs generally show less variation than single words, they show enough that it cannot be ignored. In a study on V+NP idioms, Riehemann (2001) found that the idioms’ canonical form accounted for 75% of their appearances in a corpus. Additionally, expressions differ considerably in the types of variations they allow, which include passivization, nominalization and addition of modifying words (Moon, 1998). A second source of difficulty is that expressions consisting of very frequent words will often cooccur in sentences in a non-MWE usage and in similar but distinct expressions. MWE identification can be modeled as a two step process. Given a sentence and a known expression, step (1) is to decide if the sentence contains a potential u</context>
</contexts>
<marker>Riehemann, 2001</marker>
<rawString>Susanne Z. Riehemann. 2001. A Constructional Approach to Idioms and Word Formation. Ph.D. Thesis. Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Linlin Li</author>
</authors>
<title>Unsupervised recognition of literal and non-literal use of idiomatic expressions.</title>
<date>2009</date>
<journal>EACL</journal>
<volume>09</volume>
<contexts>
<context position="10144" citStr="Sporleder and Li, 2009" startWordPosition="1628" endWordPosition="1631">form(s). The method classifies new tokens as idiomatic if they use a canonical form, and literal otherwise. The second method uses context as well as form. Co-occurrence vectors representing the idiomatic and literal meaning of each expression were computed based on corpus data. Idiomaticmeaning vectors were based on examples matching the expressions’ canonical form. Literal meaning vectors were based on examples that did not match the canonical form. New tokens were classified as literal/idiomatic based on their (cooccurrence) vector’s cosine similarity to the idiomatic and literal vectors. (Sporleder and Li, 2009) also attempted to distinguish compositional from non-compositional uses of expressions in text. Their assumption was that if an expression is used literally, but not idiomatically, its component words will be related semantically to several words in the surrounding discourse. For example, when the expression ‘play with fire’ is used literally, words such as ‘smoke, ‘burn’, ‘fire department’, and ‘alarm’ tend to also be used nearby; when it is used idiomatically, they aren’t (indeed, other words, e.g., ‘danger’ or ‘risk’ appear nearby but they are not close semantically to ‘play’ or to ‘fire’)</context>
<context position="11635" citStr="Sporleder and Li, 2009" startWordPosition="1862" endWordPosition="1865">ds, forming a ‘lexical chain’, the usage was classified as literal. Otherwise it was idiomatic. Two classifiers based on lexical chains were devised. These were compared with a supervised method that trains a classifier for each expression based on surrounding context. The results showed that the supervised classifier method did much better (90% F-score on literal uses) than the lexical chain classifier methods (60% F-score). In the above studies the focus is on the compositional/non-compositional expression distinction. The sentence data used contains examples of either one or the other. In (Sporleder and Li, 2009) the experimental data included only sentences in which the expressions were in canonical form (allowing for verb inflection). In (Cook et al., 2007) a syntactic parser was used to collect sentences containing the MWEs in the active and passive voice using heuristics. Thus, examples such as the following (from the BNC) would not be included in their sample: 1. take a chance: ‘While he still had a chance of being near Maisie, he would take it’. 2. face the consequences: ‘... she did not have to face, it appears, the possible serious or even fatal consequences of her decision’. 3. make a distinc</context>
<context position="32454" citStr="Sporleder and Li, 2009" startWordPosition="5333" endWordPosition="5336">groups. This is due to the Lexical Values feature, which accounts for a change in preposition. A change in preposition (as in ‘wash hands of some matter’ versus ‘wash hands in the sink’) is more significant than a change in determiner in the Verb-DeterminerNoun group. This improves precision on negative instances, which are rejected more precisely based on the preposition value. Nevertheless, the relatively simple features in R2, essentially order and distance, perform quite well. The F-score result for R1, 94.86, is an improvement over the F-score result of the supervised classifier used in (Sporleder and Li, 2009), 90.15. Although the sentence data is different (our data includes sentences with non-expression uses) the number of sentences used is similar. S, based on syntactic features, performs worse than R1/2. It shows better accuracy than the baselines in all but the (Verb-Prep-Noun) group and is also more stable. C, a combination of surface and syntactic features, performs better than S and slightly worse than R1/2. Why do the syntactic features perform worse than surface features? An analysis of the S classifier errors reveals two important causes. First, there is substantial variation in the depe</context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>Caroline Sporleder and Linlin Li. 2009. Unsupervised recognition of literal and non-literal use of idiomatic expressions. EACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
<author>Ann Copestake</author>
<author>Benjamin Waldron</author>
<author>Fabre Lambeau</author>
</authors>
<date>2004</date>
<booktitle>Lexical encoding of MWE. ACL ’04 Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="7254" citStr="Villavicencio et al., 2004" startWordPosition="1175" endWordPosition="1179">l setting. The results and a discussion are given in sections 5 and 6. 2 Previous Work 2.1 MWE Lexical Encoding The approach to handling MWEs in early systems was to employ a list of expressions, each with a quasi regular expression that encodes morphosyntactic variations. One example is Leech et al. (1994) who used this method for automatic part-of-speech tagging for the BNC. Another is a formalism called IDAREX (IDioms And Regular EXpressions) (Breidt et al., 1996). More recent research emphasizes the integration of MWE lexical entries into existing single word lexicons and grammar systems (Villavicencio et al., 2004; Alegria et al., 2004). There is also an attempt to take advantage of regularities in morpho-syntactic properties across MWE groups, which allows encoding the behavior of the group instead of individual expressions (Villavicencio et al., 2004; Gr´egoire, 2007). Fellbaum (1998) discusses some difficulties in representing idioms, which are largely figurative in meaning, in WordNet. More recent work (Fellbaum et al., 2006) focuses on German VP idioms. As already mentioned, one issue with lexical encoding is that it is done manually, making lexicons difficult to create, maintain and extend. The u</context>
</contexts>
<marker>Villavicencio, Copestake, Waldron, Lambeau, 2004</marker>
<rawString>Aline Villavicencio, Ann Copestake, Benjamin Waldron, and Fabre Lambeau. 2004. Lexical encoding of MWE. ACL ’04 Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>Collocation extraction based on modifiability statistics.</title>
<date>2004</date>
<journal>COLING</journal>
<volume>04</volume>
<contexts>
<context position="13185" citStr="Wermter and Hahn (2004)" startWordPosition="2126" endWordPosition="2129">ountered in practice when attempting MWE identifi470 cation. Specifically, they would miss many examples in which the MWE words are present but are not used as an expression (case (b) in Section 1). Moreover, their heuristics are tailored to the Verb-Direct Object MWE type. Different heuristics would need to be employed for different MWE types. In our approach there is no pre-processing stage requiring type-specific knowledge. Specifically, the above examples are used as training sentences in our experiments. 2.3 MWE Extraction There exists an extensive body of research on MWE extraction (see Wermter and Hahn (2004) for a review), where the only input is a corpus, and the output is a list of MWEs found in it. Most methods collect MWE candidates from the corpus, score them according to some association measure between their components, and accept candidates with scores passing some threshold. The focus of research has been on developing association measures, including statistical, information-theoretic and linguistically motivated measures (e.g., Justeson and Katz (1995), Wermter and Hahn (2006), and Deane (2005)). 3 MWE Identification Method Our method decides if a potential use of a known expression in </context>
</contexts>
<marker>Wermter, Hahn, 2004</marker>
<rawString>Joachim Wermter and Udo Hahn. 2004. Collocation extraction based on modifiability statistics. COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>You can’t beat frequency (unless you use linguistic knowledge) – a qualitative evaluation of association measures for collocation and term extraction.</title>
<date>2006</date>
<journal>COLING-ACL</journal>
<volume>06</volume>
<contexts>
<context position="13673" citStr="Wermter and Hahn (2006)" startWordPosition="2201" endWordPosition="2204">tences in our experiments. 2.3 MWE Extraction There exists an extensive body of research on MWE extraction (see Wermter and Hahn (2004) for a review), where the only input is a corpus, and the output is a list of MWEs found in it. Most methods collect MWE candidates from the corpus, score them according to some association measure between their components, and accept candidates with scores passing some threshold. The focus of research has been on developing association measures, including statistical, information-theoretic and linguistically motivated measures (e.g., Justeson and Katz (1995), Wermter and Hahn (2006), and Deane (2005)). 3 MWE Identification Method Our method decides if a potential use of a known expression in a given sentence is noncompositional. The input to the method, for each MWE, is a labeled training set of sentences containing one or more potentially non-compositional uses of the MWE. The output, for each MWE, is a binary classifier, trained on those sentences. Thus, we target step (2) of MWE identification, which is the difficult one. The learning algorithm used is Support Vector Machine (SVM), which outputs a binary classifier, using Sequential Minimal Optimization (Platt, 1998)1</context>
</contexts>
<marker>Wermter, Hahn, 2006</marker>
<rawString>Joachim Wermter and Udo Hahn. 2006. You can’t beat frequency (unless you use linguistic knowledge) – a qualitative evaluation of association measures for collocation and term extraction. COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten amd Eibe Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14319" citStr="Frank, 2000" startWordPosition="2308" endWordPosition="2309">ication Method Our method decides if a potential use of a known expression in a given sentence is noncompositional. The input to the method, for each MWE, is a labeled training set of sentences containing one or more potentially non-compositional uses of the MWE. The output, for each MWE, is a binary classifier, trained on those sentences. Thus, we target step (2) of MWE identification, which is the difficult one. The learning algorithm used is Support Vector Machine (SVM), which outputs a binary classifier, using Sequential Minimal Optimization (Platt, 1998)1 in the Weka toolkit2 (Witten and Frank, 2000). For training, sentences are converted into feature vectors. Features depend on the assignment of the lexical components of the expression to specific tokens in the sentence. In some cases, there are several tokens in the sentence that match a single component in the expression, and this leads to 1Using the PUK kernel (The Pearson VII functionbased Universal Kernel), with parameters omega=1.0 and sigma=1.0. 2Weka version 3.5.6; www.cs.waikato.ac.nz/ ml/ weka/ multiple (potential) assignments. So in the general case a sentence is converted to a set of feature vectors, each corresponding to a s</context>
</contexts>
<marker>Frank, 2000</marker>
<rawString>Ian H. Witten amd Eibe Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>