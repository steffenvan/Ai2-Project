<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005709">
<title confidence="0.997697">
Topic Models for Image Annotation and Text Illustration
</title>
<author confidence="0.997686">
Yansong Feng and Mirella Lapata
</author>
<affiliation confidence="0.999909">
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.987985">
10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.998773">
Y.Feng-4@sms.ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.975159578947368">
Image annotation, the task of automatically
generating description words for a picture,
is a key component in various image search
and retrieval applications. Creating image
databases for model development is, however,
costly and time consuming, since the key-
words must be hand-coded and the process
repeated for new collections. In this work
we exploit the vast resource of images and
documents available on the web for develop-
ing image annotation models without any hu-
man involvement. We describe a probabilistic
model based on the assumption that images
and their co-occurring textual data are gener-
ated by mixtures of latent topics. We show that
this model outperforms previously proposed
approaches when applied to image annotation
and the related task of text illustration despite
the noisy nature of our dataset.
</bodyText>
<sectionHeader confidence="0.998549" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952090909091">
Recent years have witnessed the rapid growth of im-
age collections available for searching and browsing
over the Internet. Although image search engines are
still in their infancy, initial research suggests that the
deployed algorithms are not very accurate (Hawking
et al., 1999). Given a query, search engines retrieve
relevant pictures by analyzing the image caption (if
it exists), textual descriptions found adjacent to the
image, and other text-related factors such as the file
name of the image. However, since they do not an-
alyze the actual content of the images, search en-
gines cannot be used to retrieve pictures from unan-
notated collections. The ability to perform the an-
notation task automatically would be of significant
practical import for many image-based applications.
Besides search and retrieval, other examples include
browsing support (e.g., by clustering images into
groups that are visually and semantically coherent)
and story picturing (i.e., automatically suggesting
images to illustrate text).
Automatic image annotation is a popular task in
computer vision; a large number of approaches have
been proposed in the literature under many distinct
learning paradigms. These range from supervised
classification (Smeulders et al., 2000; Vailaya et al.,
2001) to instantiations of the noisy-channel model
(Duygulu et al., 2002), to clustering (Barnard et al.,
2002), and methods inspired by information retrieval
(Feng et al., 2004; Lavrenko et al., 2003). Despite
differences in application and formulation, all these
methods essentially attempt to learn the correlation
between image features and words from examples
of annotated images. The Corel database has been
extensively used as a testbed for the development
and evaluation of image annotation models. It is a
collection of stock photographs, divided into themes
(e.g., tigers, sunsets) each of which are associated
with keywords (e.g., sun, sea) that are considered
appropriate descriptors for all images belonging to
the same theme.
Unfortunately, the Corel database is not represen-
tative of the size or content of real-world image col-
lections. It has a small number of themes with many
closely related images which in turn share keyword
descriptions. It is therefore relatively easy to learn
the associations between images and keywords and
do well on annotation and retrieval tasks (Tang and
Lewis, 2007; Westerveld and de Vries, 2003). An
appealing alternative is the use of resources where
images and their annotations co-occur naturally. Ex-
amples include images found in news documents,
consumer photo collections, Wikipedia articles, il-
lustrated stories and so on. The key idea here is to
treat the words in the surrounding text as annota-
tions for the image. These annotations are undoubt-
</bodyText>
<page confidence="0.974342">
831
</page>
<note confidence="0.752452">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 831–839,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999921454545455">
edly noisy, but plenty and cost-free. Moreover, the
collateral text is often longer and more informative
in comparison to the few keywords reserved for each
image in Corel.
In this paper we propose a probabilistic image
annotation model that learns to automatically label
images under such noisy conditions. We use the
database created in Feng and Lapata (2008) which
consists of news articles, images, and their cap-
tions. Our model exploits the redundancy inherent
in this multimodal dataset by assuming that images
and their surrounding text are generated by a shared
set of latent variables or topics. Specifically, we de-
scribe documents and images by a common multi-
modal vocabulary consisting of textual words and
visual terms (visiterms). Due to polysemy and syn-
onymy many words in this vocabulary will refer to
the same underlying concept. Using Latent Dirichlet
Allocation (LDA, Blei and Jordan 2003), a proba-
bilistic model of text generation, we represent visual
and textual meaning jointly as a probability distribu-
tion over a set of topics. Our annotation model takes
these topic distributions into account while finding
the most likely keywords for an image and its asso-
ciated document. We also show how the model can
be straightforwardly modified to perform automatic
text illustration.1 The task is routinely performed by
news writers who often have to search large image
collections in order to find suitable pictures for their
text. Here, the model takes a document as input and
suggests images that match its content. Experimen-
tal results on both tasks bring improvements over
competitive models.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999693125">
A variety of learning methods have been applied to
the image annotation task. These generally fall un-
der two broad categories. Supervised methods de-
fine annotation as a classification task, e.g., by as-
suming a one-to-one correspondence between vo-
cabulary words and classes or by grouping several
words into a single class (see Chai and Hung 2008
for an overview). Unsupervised approaches attempt
to discover the underlying connections between vi-
sual features and words, typically by introducing
latent variables. Standard latent semantic analysis
(LSA) and its probabilistic variant (PLSA) have
been applied to this task (Hofmann, 2001; Monay
and Gatica-Perez, 2007; Pan et al., 2004). More so-
phisticated models estimate the joint distribution of
words and regional image features, whilst treating
</bodyText>
<footnote confidence="0.6887265">
1We use the terms “text illustration” and “story picturing”
interchangeably throughout the paper.
</footnote>
<bodyText confidence="0.99994520754717">
annotation as a problem of statistical inference in a
graphical model (Barnard et al., 2002; Blei and Jor-
dan, 2003; Wang et al., 2009).
Irrespectively of the underlying model or task at
hand, much work has focused how to best represent
the visual and textual modalities in order to exploit
their synergy. Several approaches attempt to render
images more word-like, by reducing the dimension-
ality of the image feature space (Bosch et al., 2008;
Fei-Fei and Perona, 2005) or by learning a single
representation for both visual and textual features
(Monay and Gatica-Perez, 2007; Zhao and Grosky,
2003).
Our own work approaches the image annotation
(and related story picturing) task from a slightly dif-
ferent angle. We train and test our model on im-
ages that contain implicit (and thus noisy) annota-
tions that have not been specifically created for our
task. On account of this, our model has access to
knowledge sources other than the image and its key-
words (i.e., the news article containing the image
we wish to annotate). In Feng and Lapata (2008)
we addressed this problem with a modified ver-
sion of the continuous relevance annotation model
(Lavrenko et al., 2003). Unlike other unsupervised
approaches where a set of latent variables is in-
troduced, each defining a joint distribution on the
space of keywords and image features, the relevance
model captures the joint probability of images and
annotated words directly, without requiring an inter-
mediate clustering stage (i.e., each annotated image
in the training set is treated as a latent variable). We
modified this model so as to exploit the informa-
tion present in the document in two ways. First, in
estimating the conditional probability of a keyword
given an image, we also considered its likelihood in
the collateral document. Secondly, we used an LDA
model (trained on the document collection) to prune
from the model’s output words that are not represen-
tative of the document’s topics.
The proposed approach differs from Feng
and Lapata (2008) in three important respects:
(a) document-based information is an integral part
of our model as we predict caption words given the
image and its accompanying document (b) LDA is
no longer a post-processing step; our model relies on
LDA to infer meaningful topics that capture the co-
occurrence of visual features and words; (c) beyond
image annotation, we show how the same frame-
work can be applied to story picturing (Joshi et al.,
2006), a task which has received less attention in the
literature.
In terms of model structure, Blei and Jordan
</bodyText>
<page confidence="0.995667">
832
</page>
<bodyText confidence="0.999804772727273">
(2003) and Monay and Gatica-Perez (2007) are clos-
est to our work. The first model, known as corre-
spondence LDA (CorrLDA), has been successfully
employed for modeling annotated images in the
Corel domain. CorrLDA also uses the notion of topic
to model the generation of images and their captions.
In this model, the visual modality drives the defini-
tion of the latent space to which the textual modality
is linked. The second model is based on PLSA and
learns a representation similar to ours consisting of
textual and visual features. It is also trained using
captioned images from the Corel database. We work
with noisier and larger datasets. Our model exploits
the captions accompanying the images as well as
their surrounding documents. As a result, we obtain
a similar number of textual and visual words; these
are often imbalanced in the Corel database, where
visual words are nearly 50 times more than textual
words. The different nature of our data dictates the
use of a model where the visual and textual modal-
ity are given equal importance in defining the latent
space.
</bodyText>
<sectionHeader confidence="0.98789" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999988538461539">
In this section we give a brief description of the im-
age database we employ and also define the image
annotation and story picturing tasks we are attempt-
ing here. As mentioned earlier, we use the dataset
created in Feng and Lapata (2008).2 It contains
3,361 articles that have been downloaded from the
BBC News website3. Each article contains a news
image which in turn is associated with a caption.
The images are usually 203 pixels wide and 152 pix-
els high. The average caption length is 5.35 tokens,
and the average document length 133.85 tokens. The
captions vocabulary is 2,167 words and the docu-
ment vocabulary is 6,253. The vocabulary shared
between captions and documents is 2,056 words.
In contrast to the Corel database, this dataset con-
tains more complex images (with many objects) and
has a larger vocabulary (Corel’s vocabulary is ap-
proximately 300 words). An example of an abridged
database entry is shown in Figure 1.
Due to the non-standard nature of the database
we assume that the caption and news article de-
scribe the content of the image either directly or in-
directly. It also follows that we may not be able to
name all objects depicted in the image. Now, given
these constraints our goal is twofold. Firstly, we will
perform image annotation. Our model is trained on
</bodyText>
<footnote confidence="0.986221333333333">
2Available from http://homepages.inf.ed.ac.uk/
s0677528/data.html.
3http://news.bbc.co.uk/
</footnote>
<tableCaption confidence="0.996628">
Table 1: Each entry in the BBC News database contains a
document, an image, and its caption (shown in boldface).
</tableCaption>
<bodyText confidence="0.99994275">
document-image-caption tuples like the one shown
in Table 1. During testing, we must infer the cap-
tion for an image. Secondly, we use the same dataset
to perform automatic text illustration. During train-
ing, the model has access to the same collection of
image-caption-document tuples. During testing, we
are given a document and must find the images that
best illustrate it.
</bodyText>
<sectionHeader confidence="0.993281" genericHeader="method">
4 Image and Document Representation
</sectionHeader>
<bodyText confidence="0.999822692307692">
Words and images represent distinct modalities, im-
ages live in a continuous feature space, whereas
words are discrete. Yet, both modalities on some
level capture the same underlying concepts as they
are used to describe the same objects. A common
first step to all previous methods is the segmenta-
tion of the image into regions, using either a fixed-
grid layout or an image segmentation algorithm. Re-
gions are usually described by a standard set of fea-
tures including color, texture, and shape which are
treated as continuous vectors (e.g., Barnard et al.
2002; Blei and Jordan 2003) or in quantized form
(e.g., Duygulu et al. 2002). Through this process,
the low-level image features are made to resemble
word-like units.
Here, we go one step further and represent each
image by a bag of visual words, thereby convert-
ing visual features from a continuous onto a discrete
space. In order to do this we use the Scale Invariant
Feature Transform (SIFT) algorithm (Lowe, 1999).
The general idea behind the algorithm is to first
sample an image with the difference-of-Gaussians
point detector at different scales and locations. Im-
portantly, this detector is, to some extent, invariant to
translation, scale, rotation and illumination changes.
Each detected region is represented with a SIFT de-
scriptor which is a histogram of edge directions at
A woman from East
Sussex who bought an
emu egg sold as a nov-
elty food item on a
farm on the Isle of
Wight has managed to
hatch it into a chick. Osborne the emu will grow
Gillian Stone, from to over 6ft tall
Bexhill, who breeds chickens, brought home three large
green emu eggs from a holiday and put them in an incu-
bator in her kitchen. Two turned out to be infertile, but
after 52 days little Osborne hatched
</bodyText>
<page confidence="0.939508">
833
</page>
<bodyText confidence="0.999805666666667">
different locations. SIFT descriptors are quantized
using the K-means clustering algorithm to obtain a
discrete set of visual terms (visiterms) which form
our visual vocabulary Vocv. Each entry in this vo-
cabulary stands for a group of image regions which
are similar in content or appearance and assumed to
originate from similar objects. More formally, each
image I is expressed in a bag-of-words format vec-
tor, [wv1,wv2,...,wvL], where wvi = n only if I has n
regions labeled with vi.
Since visual and textual modalities have now the
same status—they are both represented as bags-of-
words—we can also represent any image-caption-
document tuple jointly as a mixed document dMix.
The underlying assumption is that the two modali-
ties express the same meaning which, as we explain
below, can be operationalized as a probability distri-
bution over a set of topics.
</bodyText>
<sectionHeader confidence="0.991328" genericHeader="method">
5 Modeling
</sectionHeader>
<bodyText confidence="0.9712928">
Latent Dirichlet Allocation For ease of exposi-
tion, we first describe the basics of Latent Dirichlet
Allocation (LDA; Blei et al. 2003), a probabilistic
model of text generation and then move on to dis-
cuss our models which make use of probabilities es-
timated by LDA.
LDA can be represented as a three level hierarchi-
cal Bayesian model. Given a corpus consisting of M
documents, Blei et al. (2003) define the generative
process for a document d as follows:
</bodyText>
<listItem confidence="0.990244">
1. Choose θ|α ∼ Dir(α)
2. For n ∈ 1,2,...,N :
(a) Choose topic zn|θ ∼ Mult(θ)
(b) Choose a word wn|zn,β1:K ∼ Mult(βzn)
</listItem>
<bodyText confidence="0.999011714285714">
The mixing proportion over topics θ is drawn from
a Dirichlet prior with parameters α whose role is to
create a smoothed topic distribution. Once α and β
are sampled, then each document is generated ac-
cording to the topic proportions z1:K and word prob-
abilities over topics β. The probability of a docu-
ment d in a corpus is defined as:
</bodyText>
<equation confidence="0.9431464">
a f N
P PP
(dI , R)=JP (0Ia) � � (zkl0) (wn|zk, d0
R)
0 n=1 zk
</equation>
<bodyText confidence="0.997068785714286">
Computing the posterior distribution P(θ,z|d,α,β)
of the hidden variables given a document is in-
tractable in general. However, a variety of approx-
imate inference algorithms have been proposed in
the literature including variational inference which
our model adopts (Blei et al., 2003). In this case,
training an LDA model on a document collection
will give two sets of parameters, the word proba-
bilities given topics, p(w|z1:K), and the topic pro-
portions given documents, P(z1:K|d). The latter are
document-specific, whereas the former represent the
set of topics learned from the document collection.
Given a trained model, it is possible to do infer-
ence on an unseen document dnew:
</bodyText>
<equation confidence="0.999769">
P(w|zk) γk (1)
∑K j=1 γj
</equation>
<bodyText confidence="0.9986946">
where P(w|z1:K) are word probabilities over top-
ics z1:K estimated during model training, and γ1:K
are variational Dirichlet parameters obtained during
inference on the new document (and can be consid-
ered as the posteriors of topic proportions over doc-
uments).
Image Annotation In the standard image annota-
tion setting, a hypothetical model is given an image I
and a set of keywords W, and must find the subset WI
(WI ⊆ W) which appropriately describes image I:
</bodyText>
<equation confidence="0.992188">
P(W|I) (2)
</equation>
<bodyText confidence="0.944181333333333">
The keywords are usually assumed to be condition-
ally independent on each other, so Equation (2) sim-
plifies to:
</bodyText>
<equation confidence="0.9983665">
W∗I = argmax ∏
W w∈W
</equation>
<bodyText confidence="0.9035802">
Each entry in our database is an image-caption-
document tuple (I,C,D). In this setting, we must
find the subset of keywords WI which appropriately
describe image I with the help of the accompanying
document D:
</bodyText>
<equation confidence="0.983468">
W∗I = argmaxP(Wt|I,D) (4)
Wt
</equation>
<bodyText confidence="0.9999096">
Here, Wt denotes a set of textual words (we use the
subscript t to discriminate from the visual words
which are not part of the model’s output). We also
assume that the keywords are conditionally indepen-
dent of each other:
</bodyText>
<equation confidence="0.527698">
W∗I =argmaxP(Wt|I,D)=argmax ∏P(wt |I, D) (5)
Wt Wt wt∈Wt
</equation>
<bodyText confidence="0.99749">
Since I and D are represented jointly as the con-
catenation of textual and visual terms, we may intu-
itively simplify the problem and use the mixed doc-
ument representation dMix directly in estimating the
conditional probabilities P(wt|I,D):
</bodyText>
<equation confidence="0.999907625">
P(wt|I,D) ≈ P(wt|dMix) (6)
p(w|dnew) ≈
K
∑
k=1
W∗I = argmax
W
P(w|I) (3)
</equation>
<page confidence="0.918161">
834
</page>
<bodyText confidence="0.542658">
Substituting Equation (6) into (5) yields:
</bodyText>
<equation confidence="0.676008">
∏ P(wt|dMix) (7)
wt∈Wt
</equation>
<bodyText confidence="0.999789047619048">
As mentioned earlier, we assume that the image and
its associated text are generated by a mixture of
latent topics which we infer using LDA. Specifi-
cally, we select the number of topics K and apply
the LDA algorithm to a corpus consisting of doc-
uments {dMix} in order to obtain the multimodal
word distributions over topics P(w|z1:K), and the
estimated posterior of the topic proportions over
documents P(z1:K|dMix). We infer the topic pro-
portions P(z1:K|dMixnew) on a new document-image
pair dMixnew approximately using Equations (1)
and (7):4
where P(wt|zk) are obtained during training, and γ1:K
are inferred on the image-document test pair.
However, note that for an unseen image dI and ac-
companying document dD, the estimated topic pro-
portions are solely based on variational inference
which is an approximate algorithm. In order to ren-
der the model more robust, we smooth the topic pro-
portions P(z1:K|dMix) with probabilities based on a
single modality:
</bodyText>
<equation confidence="0.9999765">
P∗(z1:K|dMix) ≈ (9)
q1P(z1:K|dMix) + q2P(z1:K|dD) + q3P(z1:K|dI)
</equation>
<bodyText confidence="0.999972583333333">
where P(z1:K|dD) and P(z1:K|dI) are inferred on dD
and dI, respectively, and q1, q2, q3 are smoothing
parameters (which we tune experimentally on held-
out data); q3 is a shorthand for (1− q1 − q2).
In sum, calculating P(Wt|I,D) boils down to es-
timating the probabilities P(wt|dMix) according to
Equations (8) and (9) which we obtain using the
LDA model. We train LDA on the document col-
lection {dMix} and use inference to obtain the topic
distributions of unseen image-document pairs. In the
end, we obtain a ranked list of textual words wt, the
n-best of which are the annotations for image I.
</bodyText>
<footnote confidence="0.589303">
4During training, the model has access to all three elements
(I,C,D), so the mixed document dMix is the concatenation of
the visual terms and words in the caption and document. Dur-
ing testing, the model is given an image and its accompanying
document, so dMix contains words based on I and D, but not C.
</footnote>
<bodyText confidence="0.999501538461539">
Text Illustration Previous text illustration models
are based on Corel-like databases with manual im-
age descriptions (Barnard and Forsyth, 2001; Blei
and Jordan, 2003) or instance-based learning using
complex learning schemes (Joshi et al., 2006). Here,
we present a relatively simple model, again under
the topic mixture framework.
Given a test document D and a candidate image
database I1...N with captions C, we must find the im-
age or images which best describe the document.
We can simply compute the probability of each vi-
sual term in the vocabulary given D by marginaliz-
ing over the document topics z1:K:
</bodyText>
<equation confidence="0.9958025">
P(wv|D) = ∑ P(wv|zk)P(zk|dD) (10)
z1:K
</equation>
<bodyText confidence="0.999946388888889">
where wv is a visual term and P(wv|zk) the probabil-
ity of wv given topic zk (as estimated on the training
set).
Equation (10) delivers a ranked list of visual terms
according to a given document. We could multiply
these probabilities together mirroring Equation (7),
however this is not reliable. In contrast to textual
words, for which we may infer whether they are
linguistically meaningful (e.g., by resorting to their
part of speech), there is no easy way of knowing
which visual words are important. Relying solely on
frequency is not reliable either, as frequent visiterms
may simply represent features common in all images
(e.g., most images have some white color). To avoid
a bias towards frequent but potentially irrelevant vi-
sual words, we output a fixed number of visual terms
and select the image with the highest overlap as the
correct illustration.
</bodyText>
<sectionHeader confidence="0.999122" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999972529411765">
In this section we discuss our experimental design
for assessing the performance of the models pre-
sented above. We give details on our training pro-
cedure and parameter estimation, describe our fea-
tures, and present the baseline methods used for
comparison with our models.
Data We evaluated the image annotation and text
illustration tasks on the dataset described in Sec-
tion 3. Documents and captions were part-of-speech
tagged and lemmatized with Tree Tagger (Schmid,
1994). We excluded from the vocabulary low fre-
quency words (appearing fewer than five times)
and words other than nouns, verbs, and adjectives.
For the image annotation task we follow the data
split used in Feng and Lapata (2008). The training
set contains 2,881 image-caption-document tuples;
240 tuples are reserved for development and 240 for
</bodyText>
<equation confidence="0.997399555555556">
= argmax ∏
Wt wt∈Wt
≈ argmax ∏
Wt wt∈Wt
P(wt|zk)P(zk|dMix)
P(wt|zk) γk
∑K j=1 γj
W∗I ≈ argmax
Wt wt∈Wt
∏ P(wt|dMix) (8)
K
∑
k=1
K
∑
k=1
W∗I ≈ argmax
Wt
</equation>
<page confidence="0.98497">
835
</page>
<bodyText confidence="0.998417611650486">
testing. Our text illustration experiments, used 2,881
image-caption-document tuples for training. For the
purposes of simulating a real story picturing engine
environment, we created a large image pool of 450
image-caption pairs and tested on 300 of them.
Model Parameters For each image we ex-
tracted 150 (on average) SIFT features. These were
quantized into a discrete set of visual terms us-
ing K-means. We varied K from 100 to 2,000. We
trained the LDA topic model on the multimodal doc-
ument collection {dMix� and varied the number of
topics from 15 to 1,000. The hyperparameter a was
initialized to 0.1; the R probabilities were initial-
ized randomly. The maximum number of iterations
for variational inference was set to 1,000. We tuned
the smoothing parameters q1, q2, and q3 (see Equa-
tion (9)) on the development set. The best values
were q1 = 0.84, q2 = 0.12, and q3 = 0.04 (for both
tasks). As the number of visual terms and topics are
interrelated we exhaustively examined all possible
combinations on the development set. We obtained
best results on image annotation with 1,000 topics
and 750 visual terms. On text illustration the best pa-
rameters were 1,000 topics and 2,000 visual terms.
Baselines For the image annotation experiments,
we compared our model against the following base-
lines. Firstly, we trained a vanilla LDA model on
the document collection without taking the im-
ages into account. This model estimates P(wt|D) =
EKk=1 P(wt|zk)P(zk|D), the probability of textual
word wt given text document D. We assume that the
most probable words are the captions for the accom-
panying image. Our second baseline is the extended
relevance model (Feng and Lapata, 2008) that also
takes the document into account but crucially as-
sumes that the process of generating the images is
independent from the process of generating its key-
words.
We also compared our approach with two
closely related latent variable models (developed for
image-caption pairs), a PLSA-based model (Monay
and Gatica-Perez, 2007) and CorrLDA (Blei and
Jordan, 2003). The former model is an asymmet-
ric version of PLSA; it estimates the topic structure
solely from the textual modality and keeps it fixed
for the visual modality. The technique is similar to
folding-in (Hofmann, 2001), the standard PLSA pro-
cedure for inference in unseen documents and al-
lows modeling an image as a mixture of latent top-
ics that is defined only by one modality (in this
case the caption words). CorrLDA first generates
image regions from a Gaussian multinomial distri-
bution parametrized with Dirichlet priors. Then, for
each annotation word, it uniformly selects a region
from the image and generates a word according to
the topic used to generate that region. We optimized
the parameters for both models on the development
set. For CorrLDA, we followed the mean-field vari-
ational inference strategy proposed in Blei (2004).
The optimal number of topics for PLSA, was 200
(with 2000 visual terms) and for CorrLDA 50.
For the text illustration experiments, the pro-
posed model was compared with three baselines.
The first one is essentially word overlap. We se-
lect the image whose caption has the largest num-
ber of words in common with the test document.
The second one is a straightforward implementa-
tion of the vector space model (Salton and McGill,
1983) where documents and captions are repre-
sented by vectors whose components correspond to
term-document co-occurrences. We followed com-
mon practice in weighting terms by their tf-idf val-
ues, and used the cosine similarity measure to find
the image whose caption is most similar to the
test document. Our third baseline uses a text-based
LDA model to estimate document-caption similar-
ity probabilistically, through topic sharing. The im-
ages most relevant to a document are found by max-
imizing the conditional probability of the candi-
date captions C given the document dD: P(C|dD) =
EKk=1 P(wc|zk)P(zk|dD) (where wc are the cap-
tion words, P(wc|zk) the conditional distribution of
each wc given a topic zk, and P(zk|dD) the condi-
tional distribution of zk given dD, the document we
wish to illustrate.
Evaluation In the image annotation task we
follow the evaluation methodology proposed in
Duygulu et al. (2002). We are given an un-annotated
image I and asked to automatically produce the
n-best keywords. For all models discussed here, we
report results with the top 10 annotation words us-
ing precision, recall and F1. In the text illustration
task, we are given a test document d and a pool
of candidate images I1...N with captions C1...N. The
model is expected to find an image from the can-
didate pool that matches the test document. We use
equation (10) to output a ranked list of MI visual
terms. The image having the highest overlap with
the top 30 visual terms is selected as the illustration
for the test document. All illustration models were
evaluated using top 1 accuracy, which is the percent-
age of successfully matched image-document pairs
in the test set.
</bodyText>
<equation confidence="0.798138">
n
wcEC
</equation>
<page confidence="0.986164">
836
</page>
<table confidence="0.999732">
Model Top 10
Precision Recall F1
CorrLDA 5.33 11.80 7.36
TxtLDA 7.30 16.90 10.20
PLSA 10.26 22.60 14.12
ExtRel 14.70 27.90 19.80
MixLDA 16.30 33.10 21.60
</table>
<tableCaption confidence="0.99877">
Table 2: Automatic image annotation results.
</tableCaption>
<sectionHeader confidence="0.998568" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999949902439025">
Our results on the image annotation task are summa-
rized in Table 2. Here, we compare our own model
(MixLDA) which is trained on both visual and tex-
tual information against an LDA model based solely
on textual information (TxtLDA), an extended ver-
sion of the Continuous Relevance model that also
exploits collateral document information (ExtRel;
Feng and Lapata 2008), a PLSA model that prior-
itizes the textual over visual modality (Monay and
Gatica-Perez, 2007), and CorrLDA (Blei and Jor-
dan, 2003) which does the opposite. We performed
significance testing on F1 using stratified shuffling
(Noreen, 1989), an instance of assumption-free ap-
proximative randomization testing.
Let us first discuss the performance of TxtLDA
and MixLDA. These two models are closely related
— they both rely on the probabilities P(wt|d) to
generate the image keywords — save one important
difference. MixLDA uses a concatenated represen-
tation of words and visual features assuming that
the two modalities have equal importance in defin-
ing the latent space, whereas TxtLDA considers only
the textual modality. Our results show that MixLDA
outperforms TxtLDA in terms of precision (by 9%),
recall (by 16.2%). MixLDA improves F1 by 11.4%,
and the difference is significant (p &lt; 0.01).
PLSA significantly (p &lt; 0.01) improves upon
TxtLDA. The key difference is the visual informa-
tion which makes up (to a certain extent) for the
lack of richer textual data. Interestingly, CorrLDA
performs significantly (p &lt; 0.01) worse than both
PLSA and TxtLDA. Recall that in CorrLDA word
topic assignments are drawn from the image regions
which are in turn drawn from a Gaussian distribu-
tion. Although this modeling choice delivers bet-
ter results on the simpler Corel dataset, it does not
seem able to capture the characteristics of our im-
ages which are noisier and more complex. More-
over, CorrLDA assumes that annotation keywords
must correspond to image regions. This assumption
is too restrictive in our setting where a single key-
</bodyText>
<table confidence="0.9899678">
TxtLDA
Afghanistan, Taleban, police, Burgess, time,
soldier, British, zone, letter, crash, case,
kill, force, Microsoft, death, operation,
troop, NATO investigation, jail
MixLDA
Afghanistan, troop, Diana, police, case,
Blair, British, NATO, crash, Princess, re-
helicopter, soldier, port, death, inquest,
support, operation, Paris, Burgess
commander
Caption
Troops need more Princess Diana died in
Chinook helicopters to a car crash in Paris in
carry out operations 1997
</table>
<figureCaption confidence="0.994437666666667">
Figure 1: Annotations generated by the TxtLDA and
MixLDA models. Words in bold face indicate exact
matches. The original captions are in the last row.
</figureCaption>
<bodyText confidence="0.994318714285714">
word may refer to many objects or persons in an
image (e.g.,the word badminton is used to collec-
tively describe an image depicting players, shuttle-
cocks, and rackets). As an aside, it is interesting to
note, that neither PLSA nor CorrLDA achieve better
results, when modified to take the captions and asso-
ciated documents into account. PLSA scores are in
the same ballpark (see Table 2), whereas CorrLDA
performs worse, F1 decreases by 2%.
The extended relevance model improves consid-
erably upon TxtLDA, CorrLDA, and PLSA but is
significantly worse (p &lt; 0.01) than MixLDA. On
the surface, MixLDA seems similar to ExtRel, both
models take advantage of visual and textual informa-
tion. ExtRel smooths the conditional probability of a
word given an image with the conditional probabil-
ity of the same word given the document and uses an
LDA model (trained on the document collection) to
remove non-topical keywords from the model’s out-
put. MixLDA is conceptually simpler, LDA is the
actual model rather than a post-processing step, and
exploits the synergy between visual and textual in-
formation more directly. Topics are created based on
both modalities which are treated on an equal foot-
ing. Compared to ExtRel, MixLDA improves pre-
cision by 1.6%, recall by 5.2% and the overall F1
by 1.8%.
Figure 1 illustrates examples of annotations gen-
</bodyText>
<page confidence="0.943447">
837
</page>
<figure confidence="0.737578875">
Model Accuracy
TxtLDA 31.0
Overlap 31.3
VectorSpace 38.7
MixLDA 57.3
Table 3: Text Illustration results.
VectorSpace
MixLDA
</figure>
<bodyText confidence="0.998162921052632">
erated by TxtLDA and MixLDA for two images. For
comparison, we also show the goldstandard image
captions. Note that TxtLDA fails to generate any
words relating to the objects shown in the image.
It finds primarily words relating to the topics of the
associated articles such as troops and crash. On the
contrary, MixLDA is more successful at identifying
the depicted objects, since it takes visual informa-
tion into account.
Table 3 presents our results on the automatic
text illustration task. Here, we compare our mul-
timodal topic model (MixLDA) against three text-
based baselines, namely word overlap (Overlap)
a standard vector space model (VectorSpace), and
TxtLDA. We examined whether differences in per-
formance are statistically significant using a x2 test.
As can be seen, MixLDA significantly (p &lt; 0.01)
outperforms these models by a wide margin (accu-
racy is 57.3% for MixLDA vs. 31.0% for TxtLDA,
38.7% for the vector space model, and 31.3% for
word overlap). These results are encouraging given
the simplicity of our model. They also indicate that
substantial mileage can be gained by taking into ac-
count the visual modality.
Figure 2 shows the three best illustrations found
by MixLDA and VectorSpace (incidentally, Overlap
delivered the same ranking as VectorSpace). The im-
ages are presented in ranked order, i.e., the first im-
age was given a higher score than the second one,
etc. The document discusses Smart 1 Probe, a lunar
satellite about to end its mission by crashing onto
the moon’s surface. MixLDA identifies an image de-
picting this satellite. The second best picture is also
relevant, it resembles the moon’s surface. The Vec-
torSpace model does not find any related images, the
first one is a DNA image, the second one depicts
policemen at a crime scene and the third one Ben
Nevis, the highest mountain in the British Isles.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.982394818181818">
In this paper we have presented a probabilistic ap-
proach for automatic image annotation and text il-
lustration. Our model postulates that visual terms
and words are generated by common (hidden) top-
Europe’s lunar satellite, the Smart 1 probe, is
about to end its mission by crashing onto the
Moon’s surface. It will be a spectacular end
for the robot which has spent the past 16 months
testing innovative and miniaturized space tech-
nologies. Smart 1 has also produced detailed
maps of the Moon’s chemical make-up.
</bodyText>
<figureCaption confidence="0.994168">
Figure 2: Top-3 illustrations for document in bottom row.
</figureCaption>
<bodyText confidence="0.99983409375">
ics and is trained on a dataset consisting of images
available on the Internet, their captions, and associ-
ated news articles. The annotations are implicit and
the dataset is representative of the scale, diversity,
and difficulty of real-world image collections. Over-
all, our results show that the model is robust to the
noise inherent in such data. It improves upon com-
petitive approaches that prioritize one modality over
the other or exploit them indirectly. We also show
that with minor modifications the model can be used
to automatically illustrate a document with an appro-
priate image. Our method shows promise for multi-
modal search and image retrieval and other applica-
tions which have been traditionally text-based. An
interesting future direction involves generating ac-
tual sentence descriptions rather than isolated key-
words. Another relevant application is summariza-
tion. Our results suggest that taking visual informa-
tion into account could potentially create more fo-
cused and accurate summaries.
The model presented here could be further im-
proved in two ways. Firstly, we could allow an in-
finite number of topics and develop a nonparamet-
ric version that learns how many topics are optimal.
Secondly, our model is based on word unigrams, and
in this sense takes very little linguistic knowledge
into account. Recent developments in topic model-
ing could potentially rectify this, e.g., by assuming
that each word is generated by a distribution that
combines document-specific topics and parse-tree-
specific syntactic transitions (Boyd-Graber and Blei,
2009).
</bodyText>
<page confidence="0.997335">
838
</page>
<sectionHeader confidence="0.994481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999511295238095">
Barnard, K., P. Duygulu, D. Forsyth, N. de Freitas,
D. Blei, and M. Jordan. 2002. Matching words and pic-
tures. Journal of Machine Learning Research 3:1107–
1135.
Barnard, K. and D. Forsyth. 2001. Learning the semantics
of words and pictures. In Proceedings of the 8th Inter-
national Conference on Computer Vision. Vancouver,
BC, pages 408–415.
Blei, D. 2004. Probabilistic Models of Text and Images.
Ph.D. thesis, U.C. Berkeley, Division of Computer Sci-
ence.
Blei, D. and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference. Toronto, ON, pages 127–134.
Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirich-
let allocation. Journal of Machine Learning Research
3:993–1022.
Bosch, A., A. Zisserman, and X. Munoz. 2008. Scene
classification using a hybrid generative/discriminative
approach. IEEE Transactions on Pattern Analysis and
Machine Intelligence 30(4):712–727.
Boyd-Graber, J. and D. Blei. 2009. Syntactic topic
models. In Proceedings of the 22nd Conference on
Advances in Neural Information Processing Systems.
MIT, Press, Cambridge, MA, pages 185–192.
Chai, C. and C. Hung. 2008. Automatically annotating
images with keywords: A review of image annotation
systems. Recent Patents on Computer Science 1:55–
68.
Duygulu, P., K. Barnard, J. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary. In
Proceedings of the 7th European Conference on Com-
puter Vision. Copenhagen, Danemark, pages 97–112.
Fei-Fei, L. and P. Perona. 2005. A Bayesian hierarchi-
cal model for learning natural scene categories. In
Proceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition.
IEEE Computer Society Washington, DC, volume 2,
pages 524–531.
Feng, S., V. Lavrenko, and R. Manmatha. 2004. Mul-
tiple Bernoulli relevance models for image and video
annotation. In Proceedings of the International Con-
ference on Computer Vision and Pattern Recognition.
Washington, DC, pages 1002–1009.
Feng, Y. and M. Lapata. 2008. Automatic image annota-
tion using auxiliary text information. In Proceedings
of ACL-08: HLT. Columbus, OH, pages 272–280.
Hawking, D., N. Craswell, P. Thistlewaite, and D. Har-
man. 1999. Results and challenges in web search eval-
uation. Computer Networks 31(11):1321–1330.
Hofmann, T. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning
41(2):177–196.
Joshi, D., J.Z. Wang, and J. Li. 2006. The story picturing
engine—a system for automatic text illustration. ACM
Transactions on Multimedia Computing, Communica-
tions, and Applications 2(1):68–89.
Lavrenko, V., R. Manmatha, and J. Jeon. 2003. A model
for learning the semantics of pictures. In Proceedings
of the 17th Conference on Advances in Neural Infor-
mation Processing Systems. MIT, Press, Cambridge,
MA.
Lowe, D. 1999. Object recognition from local scale-
invariant features. In Proceedings of International
Conference on Computer Vision. IEEE Computer So-
ciety, pages 1150–1157.
Monay, F. and D. Gatica-Perez. 2007. Modeling semantic
aspects for cross-media image indexing. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
29(10):1802–1817.
Pan, J., H. Yang, P. Duygulu, and C. Faloutsos. 2004. Au-
tomatic image captioning. In Proceedings of the 2004
International Conference on Multimedia and Expo.
Taipei, pages 1987–1990.
Salton, G. and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill, New York.
Schmid, H. 1994. Probabilistic part-of-speech tagging us-
ing decision trees. In Proceedings of the International
Conference on New Methods in Language Processing.
Manchester, UK, pages 44–49.
Smeulders, A. W., M. Worring, S. Santini, A. Gupta, and
R. Jain. 2000. Content-based image retrieval at the
end of the early years. IEEE Transactions on Pattern
Analysis and Machine Intelligence 22(12):1349–1380.
Tang, J. and P. H. Lewis. 2007. A study of quality is-
sues for image auto-annotation with the Corel data-set.
IEEE Transactions on Circuits and Systems for Video
Technology 17(3):384–389.
Vailaya, A., M. Figueiredo, A. Jain, and H. Zhang. 2001.
Image classification for content-based indexing. IEEE
Transactions on Image Processing 10:117–130.
Wang, C., D. Blei, and L. Fei-Fei. 2009. Simultaneous
image classification and annotation. In Proceedings of
CVPR. Miami, FL, pages 1903–1910.
Westerveld, T. and A. P. de Vries. 2003. Experimental
evaluation of a generative probabilistic image retrieval
model on ‘easy’ data. In Proceedings of the SIGIR
Multimedia Information Retrieval Workshop. Toronto,
ON.
Zhao, R. and W. I. Grosky. 2003. Video shot detection
using color anglogram and latent semantic indexing:
From contents to semantics. In B. Furht and O. Mar-
ques, editors, Handbook of Video Databases: Design
and Applications, CRC Press, pages 371–392.
</reference>
<page confidence="0.998818">
839
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.689862">
<title confidence="0.999152">Topic Models for Image Annotation and Text Illustration</title>
<author confidence="0.934375">Feng</author>
<affiliation confidence="0.974464">School of Informatics, University of</affiliation>
<address confidence="0.7401">10 Crichton Street, Edinburgh EH8 9AB,</address>
<abstract confidence="0.99924205">Image annotation, the task of automatically generating description words for a picture, is a key component in various image search and retrieval applications. Creating image databases for model development is, however, costly and time consuming, since the keywords must be hand-coded and the process repeated for new collections. In this work we exploit the vast resource of images and documents available on the web for developing image annotation models without any human involvement. We describe a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. We show that this model outperforms previously proposed approaches when applied to image annotation and the related task of text illustration despite the noisy nature of our dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>D Forsyth</author>
<author>N de Freitas</author>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>1135</pages>
<marker>Barnard, Duygulu, Forsyth, de Freitas, Blei, Jordan, 2002</marker>
<rawString>Barnard, K., P. Duygulu, D. Forsyth, N. de Freitas, D. Blei, and M. Jordan. 2002. Matching words and pictures. Journal of Machine Learning Research 3:1107– 1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>D Forsyth</author>
</authors>
<title>Learning the semantics of words and pictures.</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th International Conference on Computer Vision.</booktitle>
<pages>408--415</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="20004" citStr="Barnard and Forsyth, 2001" startWordPosition="3265" endWordPosition="3268">e to obtain the topic distributions of unseen image-document pairs. In the end, we obtain a ranked list of textual words wt, the n-best of which are the annotations for image I. 4During training, the model has access to all three elements (I,C,D), so the mixed document dMix is the concatenation of the visual terms and words in the caption and document. During testing, the model is given an image and its accompanying document, so dMix contains words based on I and D, but not C. Text Illustration Previous text illustration models are based on Corel-like databases with manual image descriptions (Barnard and Forsyth, 2001; Blei and Jordan, 2003) or instance-based learning using complex learning schemes (Joshi et al., 2006). Here, we present a relatively simple model, again under the topic mixture framework. Given a test document D and a candidate image database I1...N with captions C, we must find the image or images which best describe the document. We can simply compute the probability of each visual term in the vocabulary given D by marginalizing over the document topics z1:K: P(wv|D) = ∑ P(wv|zk)P(zk|dD) (10) z1:K where wv is a visual term and P(wv|zk) the probability of wv given topic zk (as estimated on </context>
</contexts>
<marker>Barnard, Forsyth, 2001</marker>
<rawString>Barnard, K. and D. Forsyth. 2001. Learning the semantics of words and pictures. In Proceedings of the 8th International Conference on Computer Vision. Vancouver, BC, pages 408–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
</authors>
<title>Probabilistic Models of Text and Images.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>U.C. Berkeley, Division of Computer Science.</institution>
<contexts>
<context position="25254" citStr="Blei (2004)" startWordPosition="4136" endWordPosition="4137">ard PLSA procedure for inference in unseen documents and allows modeling an image as a mixture of latent topics that is defined only by one modality (in this case the caption words). CorrLDA first generates image regions from a Gaussian multinomial distribution parametrized with Dirichlet priors. Then, for each annotation word, it uniformly selects a region from the image and generates a word according to the topic used to generate that region. We optimized the parameters for both models on the development set. For CorrLDA, we followed the mean-field variational inference strategy proposed in Blei (2004). The optimal number of topics for PLSA, was 200 (with 2000 visual terms) and for CorrLDA 50. For the text illustration experiments, the proposed model was compared with three baselines. The first one is essentially word overlap. We select the image whose caption has the largest number of words in common with the test document. The second one is a straightforward implementation of the vector space model (Salton and McGill, 1983) where documents and captions are represented by vectors whose components correspond to term-document co-occurrences. We followed common practice in weighting terms by </context>
</contexts>
<marker>Blei, 2004</marker>
<rawString>Blei, D. 2004. Probabilistic Models of Text and Images. Ph.D. thesis, U.C. Berkeley, Division of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference.</booktitle>
<pages>127--134</pages>
<location>Toronto, ON,</location>
<contexts>
<context position="4946" citStr="Blei and Jordan 2003" startWordPosition="753" endWordPosition="756">uch noisy conditions. We use the database created in Feng and Lapata (2008) which consists of news articles, images, and their captions. Our model exploits the redundancy inherent in this multimodal dataset by assuming that images and their surrounding text are generated by a shared set of latent variables or topics. Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms (visiterms). Due to polysemy and synonymy many words in this vocabulary will refer to the same underlying concept. Using Latent Dirichlet Allocation (LDA, Blei and Jordan 2003), a probabilistic model of text generation, we represent visual and textual meaning jointly as a probability distribution over a set of topics. Our annotation model takes these topic distributions into account while finding the most likely keywords for an image and its associated document. We also show how the model can be straightforwardly modified to perform automatic text illustration.1 The task is routinely performed by news writers who often have to search large image collections in order to find suitable pictures for their text. Here, the model takes a document as input and suggests imag</context>
<context position="6677" citStr="Blei and Jordan, 2003" startWordPosition="1022" endWordPosition="1026">ttempt to discover the underlying connections between visual features and words, typically by introducing latent variables. Standard latent semantic analysis (LSA) and its probabilistic variant (PLSA) have been applied to this task (Hofmann, 2001; Monay and Gatica-Perez, 2007; Pan et al., 2004). More sophisticated models estimate the joint distribution of words and regional image features, whilst treating 1We use the terms “text illustration” and “story picturing” interchangeably throughout the paper. annotation as a problem of statistical inference in a graphical model (Barnard et al., 2002; Blei and Jordan, 2003; Wang et al., 2009). Irrespectively of the underlying model or task at hand, much work has focused how to best represent the visual and textual modalities in order to exploit their synergy. Several approaches attempt to render images more word-like, by reducing the dimensionality of the image feature space (Bosch et al., 2008; Fei-Fei and Perona, 2005) or by learning a single representation for both visual and textual features (Monay and Gatica-Perez, 2007; Zhao and Grosky, 2003). Our own work approaches the image annotation (and related story picturing) task from a slightly different angle. </context>
<context position="12709" citStr="Blei and Jordan 2003" startWordPosition="2024" endWordPosition="2027">t. 4 Image and Document Representation Words and images represent distinct modalities, images live in a continuous feature space, whereas words are discrete. Yet, both modalities on some level capture the same underlying concepts as they are used to describe the same objects. A common first step to all previous methods is the segmentation of the image into regions, using either a fixedgrid layout or an image segmentation algorithm. Regions are usually described by a standard set of features including color, texture, and shape which are treated as continuous vectors (e.g., Barnard et al. 2002; Blei and Jordan 2003) or in quantized form (e.g., Duygulu et al. 2002). Through this process, the low-level image features are made to resemble word-like units. Here, we go one step further and represent each image by a bag of visual words, thereby converting visual features from a continuous onto a discrete space. In order to do this we use the Scale Invariant Feature Transform (SIFT) algorithm (Lowe, 1999). The general idea behind the algorithm is to first sample an image with the difference-of-Gaussians point detector at different scales and locations. Importantly, this detector is, to some extent, invariant to</context>
<context position="20028" citStr="Blei and Jordan, 2003" startWordPosition="3269" endWordPosition="3272">ibutions of unseen image-document pairs. In the end, we obtain a ranked list of textual words wt, the n-best of which are the annotations for image I. 4During training, the model has access to all three elements (I,C,D), so the mixed document dMix is the concatenation of the visual terms and words in the caption and document. During testing, the model is given an image and its accompanying document, so dMix contains words based on I and D, but not C. Text Illustration Previous text illustration models are based on Corel-like databases with manual image descriptions (Barnard and Forsyth, 2001; Blei and Jordan, 2003) or instance-based learning using complex learning schemes (Joshi et al., 2006). Here, we present a relatively simple model, again under the topic mixture framework. Given a test document D and a candidate image database I1...N with captions C, we must find the image or images which best describe the document. We can simply compute the probability of each visual term in the vocabulary given D by marginalizing over the document topics z1:K: P(wv|D) = ∑ P(wv|zk)P(zk|dD) (10) z1:K where wv is a visual term and P(wv|zk) the probability of wv given topic zk (as estimated on the training set). Equat</context>
<context position="24415" citStr="Blei and Jordan, 2003" startWordPosition="3996" endWordPosition="3999">ates P(wt|D) = EKk=1 P(wt|zk)P(zk|D), the probability of textual word wt given text document D. We assume that the most probable words are the captions for the accompanying image. Our second baseline is the extended relevance model (Feng and Lapata, 2008) that also takes the document into account but crucially assumes that the process of generating the images is independent from the process of generating its keywords. We also compared our approach with two closely related latent variable models (developed for image-caption pairs), a PLSA-based model (Monay and Gatica-Perez, 2007) and CorrLDA (Blei and Jordan, 2003). The former model is an asymmetric version of PLSA; it estimates the topic structure solely from the textual modality and keeps it fixed for the visual modality. The technique is similar to folding-in (Hofmann, 2001), the standard PLSA procedure for inference in unseen documents and allows modeling an image as a mixture of latent topics that is defined only by one modality (in this case the caption words). CorrLDA first generates image regions from a Gaussian multinomial distribution parametrized with Dirichlet priors. Then, for each annotation word, it uniformly selects a region from the ima</context>
<context position="28069" citStr="Blei and Jordan, 2003" startWordPosition="4604" endWordPosition="4608"> 10.26 22.60 14.12 ExtRel 14.70 27.90 19.80 MixLDA 16.30 33.10 21.60 Table 2: Automatic image annotation results. 7 Results Our results on the image annotation task are summarized in Table 2. Here, we compare our own model (MixLDA) which is trained on both visual and textual information against an LDA model based solely on textual information (TxtLDA), an extended version of the Continuous Relevance model that also exploits collateral document information (ExtRel; Feng and Lapata 2008), a PLSA model that prioritizes the textual over visual modality (Monay and Gatica-Perez, 2007), and CorrLDA (Blei and Jordan, 2003) which does the opposite. We performed significance testing on F1 using stratified shuffling (Noreen, 1989), an instance of assumption-free approximative randomization testing. Let us first discuss the performance of TxtLDA and MixLDA. These two models are closely related — they both rely on the probabilities P(wt|d) to generate the image keywords — save one important difference. MixLDA uses a concatenated representation of words and visual features assuming that the two modalities have equal importance in defining the latent space, whereas TxtLDA considers only the textual modality. Our resul</context>
</contexts>
<marker>Blei, Jordan, 2003</marker>
<rawString>Blei, D. and M. Jordan. 2003. Modeling annotated data. In Proceedings of the 26th Annual International ACM SIGIR Conference. Toronto, ON, pages 127–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--993</pages>
<contexts>
<context position="14886" citStr="Blei et al. 2003" startWordPosition="2393" endWordPosition="2396">-of-words format vector, [wv1,wv2,...,wvL], where wvi = n only if I has n regions labeled with vi. Since visual and textual modalities have now the same status—they are both represented as bags-ofwords—we can also represent any image-captiondocument tuple jointly as a mixed document dMix. The underlying assumption is that the two modalities express the same meaning which, as we explain below, can be operationalized as a probability distribution over a set of topics. 5 Modeling Latent Dirichlet Allocation For ease of exposition, we first describe the basics of Latent Dirichlet Allocation (LDA; Blei et al. 2003), a probabilistic model of text generation and then move on to discuss our models which make use of probabilities estimated by LDA. LDA can be represented as a three level hierarchical Bayesian model. Given a corpus consisting of M documents, Blei et al. (2003) define the generative process for a document d as follows: 1. Choose θ|α ∼ Dir(α) 2. For n ∈ 1,2,...,N : (a) Choose topic zn|θ ∼ Mult(θ) (b) Choose a word wn|zn,β1:K ∼ Mult(βzn) The mixing proportion over topics θ is drawn from a Dirichlet prior with parameters α whose role is to create a smoothed topic distribution. Once α and β are sa</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bosch</author>
<author>A Zisserman</author>
<author>X Munoz</author>
</authors>
<title>Scene classification using a hybrid generative/discriminative approach.</title>
<date>2008</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="7005" citStr="Bosch et al., 2008" startWordPosition="1077" endWordPosition="1080"> the joint distribution of words and regional image features, whilst treating 1We use the terms “text illustration” and “story picturing” interchangeably throughout the paper. annotation as a problem of statistical inference in a graphical model (Barnard et al., 2002; Blei and Jordan, 2003; Wang et al., 2009). Irrespectively of the underlying model or task at hand, much work has focused how to best represent the visual and textual modalities in order to exploit their synergy. Several approaches attempt to render images more word-like, by reducing the dimensionality of the image feature space (Bosch et al., 2008; Fei-Fei and Perona, 2005) or by learning a single representation for both visual and textual features (Monay and Gatica-Perez, 2007; Zhao and Grosky, 2003). Our own work approaches the image annotation (and related story picturing) task from a slightly different angle. We train and test our model on images that contain implicit (and thus noisy) annotations that have not been specifically created for our task. On account of this, our model has access to knowledge sources other than the image and its keywords (i.e., the news article containing the image we wish to annotate). In Feng and Lapata</context>
</contexts>
<marker>Bosch, Zisserman, Munoz, 2008</marker>
<rawString>Bosch, A., A. Zisserman, and X. Munoz. 2008. Scene classification using a hybrid generative/discriminative approach. IEEE Transactions on Pattern Analysis and Machine Intelligence 30(4):712–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D Blei</author>
</authors>
<title>Syntactic topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 22nd Conference on Advances in Neural Information Processing Systems.</booktitle>
<pages>185--192</pages>
<publisher>MIT, Press,</publisher>
<location>Cambridge, MA,</location>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Boyd-Graber, J. and D. Blei. 2009. Syntactic topic models. In Proceedings of the 22nd Conference on Advances in Neural Information Processing Systems. MIT, Press, Cambridge, MA, pages 185–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chai</author>
<author>C Hung</author>
</authors>
<title>Automatically annotating images with keywords: A review of image annotation systems.</title>
<date>2008</date>
<journal>Recent Patents on Computer Science</journal>
<volume>1</volume>
<pages>68</pages>
<contexts>
<context position="6012" citStr="Chai and Hung 2008" startWordPosition="926" endWordPosition="929">en have to search large image collections in order to find suitable pictures for their text. Here, the model takes a document as input and suggests images that match its content. Experimental results on both tasks bring improvements over competitive models. 2 Related Work A variety of learning methods have been applied to the image annotation task. These generally fall under two broad categories. Supervised methods define annotation as a classification task, e.g., by assuming a one-to-one correspondence between vocabulary words and classes or by grouping several words into a single class (see Chai and Hung 2008 for an overview). Unsupervised approaches attempt to discover the underlying connections between visual features and words, typically by introducing latent variables. Standard latent semantic analysis (LSA) and its probabilistic variant (PLSA) have been applied to this task (Hofmann, 2001; Monay and Gatica-Perez, 2007; Pan et al., 2004). More sophisticated models estimate the joint distribution of words and regional image features, whilst treating 1We use the terms “text illustration” and “story picturing” interchangeably throughout the paper. annotation as a problem of statistical inference </context>
</contexts>
<marker>Chai, Hung, 2008</marker>
<rawString>Chai, C. and C. Hung. 2008. Automatically annotating images with keywords: A review of image annotation systems. Recent Patents on Computer Science 1:55– 68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Duygulu</author>
<author>K Barnard</author>
<author>J de Freitas</author>
<author>D Forsyth</author>
</authors>
<title>Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th European Conference on Computer Vision. Copenhagen, Danemark,</booktitle>
<pages>97--112</pages>
<marker>Duygulu, Barnard, de Freitas, Forsyth, 2002</marker>
<rawString>Duygulu, P., K. Barnard, J. de Freitas, and D. Forsyth. 2002. Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary. In Proceedings of the 7th European Conference on Computer Vision. Copenhagen, Danemark, pages 97–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Fei-Fei</author>
<author>P Perona</author>
</authors>
<title>A Bayesian hierarchical model for learning natural scene categories.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society</booktitle>
<volume>2</volume>
<pages>524--531</pages>
<location>Washington, DC,</location>
<contexts>
<context position="7032" citStr="Fei-Fei and Perona, 2005" startWordPosition="1081" endWordPosition="1084">ion of words and regional image features, whilst treating 1We use the terms “text illustration” and “story picturing” interchangeably throughout the paper. annotation as a problem of statistical inference in a graphical model (Barnard et al., 2002; Blei and Jordan, 2003; Wang et al., 2009). Irrespectively of the underlying model or task at hand, much work has focused how to best represent the visual and textual modalities in order to exploit their synergy. Several approaches attempt to render images more word-like, by reducing the dimensionality of the image feature space (Bosch et al., 2008; Fei-Fei and Perona, 2005) or by learning a single representation for both visual and textual features (Monay and Gatica-Perez, 2007; Zhao and Grosky, 2003). Our own work approaches the image annotation (and related story picturing) task from a slightly different angle. We train and test our model on images that contain implicit (and thus noisy) annotations that have not been specifically created for our task. On account of this, our model has access to knowledge sources other than the image and its keywords (i.e., the news article containing the image we wish to annotate). In Feng and Lapata (2008) we addressed this p</context>
</contexts>
<marker>Fei-Fei, Perona, 2005</marker>
<rawString>Fei-Fei, L. and P. Perona. 2005. A Bayesian hierarchical model for learning natural scene categories. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Washington, DC, volume 2, pages 524–531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feng</author>
<author>V Lavrenko</author>
<author>R Manmatha</author>
</authors>
<title>Multiple Bernoulli relevance models for image and video annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computer Vision and Pattern Recognition.</booktitle>
<pages>1002--1009</pages>
<location>Washington, DC,</location>
<contexts>
<context position="2513" citStr="Feng et al., 2004" startWordPosition="373" endWordPosition="376">de browsing support (e.g., by clustering images into groups that are visually and semantically coherent) and story picturing (i.e., automatically suggesting images to illustrate text). Automatic image annotation is a popular task in computer vision; a large number of approaches have been proposed in the literature under many distinct learning paradigms. These range from supervised classification (Smeulders et al., 2000; Vailaya et al., 2001) to instantiations of the noisy-channel model (Duygulu et al., 2002), to clustering (Barnard et al., 2002), and methods inspired by information retrieval (Feng et al., 2004; Lavrenko et al., 2003). Despite differences in application and formulation, all these methods essentially attempt to learn the correlation between image features and words from examples of annotated images. The Corel database has been extensively used as a testbed for the development and evaluation of image annotation models. It is a collection of stock photographs, divided into themes (e.g., tigers, sunsets) each of which are associated with keywords (e.g., sun, sea) that are considered appropriate descriptors for all images belonging to the same theme. Unfortunately, the Corel database is </context>
</contexts>
<marker>Feng, Lavrenko, Manmatha, 2004</marker>
<rawString>Feng, S., V. Lavrenko, and R. Manmatha. 2004. Multiple Bernoulli relevance models for image and video annotation. In Proceedings of the International Conference on Computer Vision and Pattern Recognition. Washington, DC, pages 1002–1009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Feng</author>
<author>M Lapata</author>
</authors>
<title>Automatic image annotation using auxiliary text information.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<pages>272--280</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="4400" citStr="Feng and Lapata (2008)" startWordPosition="666" endWordPosition="669">as annotations for the image. These annotations are undoubt831 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 831–839, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics edly noisy, but plenty and cost-free. Moreover, the collateral text is often longer and more informative in comparison to the few keywords reserved for each image in Corel. In this paper we propose a probabilistic image annotation model that learns to automatically label images under such noisy conditions. We use the database created in Feng and Lapata (2008) which consists of news articles, images, and their captions. Our model exploits the redundancy inherent in this multimodal dataset by assuming that images and their surrounding text are generated by a shared set of latent variables or topics. Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms (visiterms). Due to polysemy and synonymy many words in this vocabulary will refer to the same underlying concept. Using Latent Dirichlet Allocation (LDA, Blei and Jordan 2003), a probabilistic model of text generation, we represe</context>
<context position="7612" citStr="Feng and Lapata (2008)" startWordPosition="1180" endWordPosition="1183">ch et al., 2008; Fei-Fei and Perona, 2005) or by learning a single representation for both visual and textual features (Monay and Gatica-Perez, 2007; Zhao and Grosky, 2003). Our own work approaches the image annotation (and related story picturing) task from a slightly different angle. We train and test our model on images that contain implicit (and thus noisy) annotations that have not been specifically created for our task. On account of this, our model has access to knowledge sources other than the image and its keywords (i.e., the news article containing the image we wish to annotate). In Feng and Lapata (2008) we addressed this problem with a modified version of the continuous relevance annotation model (Lavrenko et al., 2003). Unlike other unsupervised approaches where a set of latent variables is introduced, each defining a joint distribution on the space of keywords and image features, the relevance model captures the joint probability of images and annotated words directly, without requiring an intermediate clustering stage (i.e., each annotated image in the training set is treated as a latent variable). We modified this model so as to exploit the information present in the document in two ways</context>
<context position="10459" citStr="Feng and Lapata (2008)" startWordPosition="1655" endWordPosition="1658"> surrounding documents. As a result, we obtain a similar number of textual and visual words; these are often imbalanced in the Corel database, where visual words are nearly 50 times more than textual words. The different nature of our data dictates the use of a model where the visual and textual modality are given equal importance in defining the latent space. 3 Problem Formulation In this section we give a brief description of the image database we employ and also define the image annotation and story picturing tasks we are attempting here. As mentioned earlier, we use the dataset created in Feng and Lapata (2008).2 It contains 3,361 articles that have been downloaded from the BBC News website3. Each article contains a news image which in turn is associated with a caption. The images are usually 203 pixels wide and 152 pixels high. The average caption length is 5.35 tokens, and the average document length 133.85 tokens. The captions vocabulary is 2,167 words and the document vocabulary is 6,253. The vocabulary shared between captions and documents is 2,056 words. In contrast to the Corel database, this dataset contains more complex images (with many objects) and has a larger vocabulary (Corel’s vocabul</context>
<context position="22094" citStr="Feng and Lapata (2008)" startWordPosition="3609" endWordPosition="3612">nce of the models presented above. We give details on our training procedure and parameter estimation, describe our features, and present the baseline methods used for comparison with our models. Data We evaluated the image annotation and text illustration tasks on the dataset described in Section 3. Documents and captions were part-of-speech tagged and lemmatized with Tree Tagger (Schmid, 1994). We excluded from the vocabulary low frequency words (appearing fewer than five times) and words other than nouns, verbs, and adjectives. For the image annotation task we follow the data split used in Feng and Lapata (2008). The training set contains 2,881 image-caption-document tuples; 240 tuples are reserved for development and 240 for = argmax ∏ Wt wt∈Wt ≈ argmax ∏ Wt wt∈Wt P(wt|zk)P(zk|dMix) P(wt|zk) γk ∑K j=1 γj W∗I ≈ argmax Wt wt∈Wt ∏ P(wt|dMix) (8) K ∑ k=1 K ∑ k=1 W∗I ≈ argmax Wt 835 testing. Our text illustration experiments, used 2,881 image-caption-document tuples for training. For the purposes of simulating a real story picturing engine environment, we created a large image pool of 450 image-caption pairs and tested on 300 of them. Model Parameters For each image we extracted 150 (on average) SIFT fea</context>
<context position="24048" citStr="Feng and Lapata, 2008" startWordPosition="3939" endWordPosition="3942"> image annotation with 1,000 topics and 750 visual terms. On text illustration the best parameters were 1,000 topics and 2,000 visual terms. Baselines For the image annotation experiments, we compared our model against the following baselines. Firstly, we trained a vanilla LDA model on the document collection without taking the images into account. This model estimates P(wt|D) = EKk=1 P(wt|zk)P(zk|D), the probability of textual word wt given text document D. We assume that the most probable words are the captions for the accompanying image. Our second baseline is the extended relevance model (Feng and Lapata, 2008) that also takes the document into account but crucially assumes that the process of generating the images is independent from the process of generating its keywords. We also compared our approach with two closely related latent variable models (developed for image-caption pairs), a PLSA-based model (Monay and Gatica-Perez, 2007) and CorrLDA (Blei and Jordan, 2003). The former model is an asymmetric version of PLSA; it estimates the topic structure solely from the textual modality and keeps it fixed for the visual modality. The technique is similar to folding-in (Hofmann, 2001), the standard P</context>
<context position="27937" citStr="Feng and Lapata 2008" startWordPosition="4583" endWordPosition="4586">ge-document pairs in the test set. n wcEC 836 Model Top 10 Precision Recall F1 CorrLDA 5.33 11.80 7.36 TxtLDA 7.30 16.90 10.20 PLSA 10.26 22.60 14.12 ExtRel 14.70 27.90 19.80 MixLDA 16.30 33.10 21.60 Table 2: Automatic image annotation results. 7 Results Our results on the image annotation task are summarized in Table 2. Here, we compare our own model (MixLDA) which is trained on both visual and textual information against an LDA model based solely on textual information (TxtLDA), an extended version of the Continuous Relevance model that also exploits collateral document information (ExtRel; Feng and Lapata 2008), a PLSA model that prioritizes the textual over visual modality (Monay and Gatica-Perez, 2007), and CorrLDA (Blei and Jordan, 2003) which does the opposite. We performed significance testing on F1 using stratified shuffling (Noreen, 1989), an instance of assumption-free approximative randomization testing. Let us first discuss the performance of TxtLDA and MixLDA. These two models are closely related — they both rely on the probabilities P(wt|d) to generate the image keywords — save one important difference. MixLDA uses a concatenated representation of words and visual features assuming that </context>
</contexts>
<marker>Feng, Lapata, 2008</marker>
<rawString>Feng, Y. and M. Lapata. 2008. Automatic image annotation using auxiliary text information. In Proceedings of ACL-08: HLT. Columbus, OH, pages 272–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hawking</author>
<author>N Craswell</author>
<author>P Thistlewaite</author>
<author>D Harman</author>
</authors>
<title>Results and challenges in web search evaluation.</title>
<date>1999</date>
<journal>Computer Networks</journal>
<volume>31</volume>
<issue>11</issue>
<contexts>
<context position="1340" citStr="Hawking et al., 1999" startWordPosition="197" endWordPosition="200">cribe a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. We show that this model outperforms previously proposed approaches when applied to image annotation and the related task of text illustration despite the noisy nature of our dataset. 1 Introduction Recent years have witnessed the rapid growth of image collections available for searching and browsing over the Internet. Although image search engines are still in their infancy, initial research suggests that the deployed algorithms are not very accurate (Hawking et al., 1999). Given a query, search engines retrieve relevant pictures by analyzing the image caption (if it exists), textual descriptions found adjacent to the image, and other text-related factors such as the file name of the image. However, since they do not analyze the actual content of the images, search engines cannot be used to retrieve pictures from unannotated collections. The ability to perform the annotation task automatically would be of significant practical import for many image-based applications. Besides search and retrieval, other examples include browsing support (e.g., by clustering ima</context>
</contexts>
<marker>Hawking, Craswell, Thistlewaite, Harman, 1999</marker>
<rawString>Hawking, D., N. Craswell, P. Thistlewaite, and D. Harman. 1999. Results and challenges in web search evaluation. Computer Networks 31(11):1321–1330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<journal>Machine Learning</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="6302" citStr="Hofmann, 2001" startWordPosition="968" endWordPosition="969">methods have been applied to the image annotation task. These generally fall under two broad categories. Supervised methods define annotation as a classification task, e.g., by assuming a one-to-one correspondence between vocabulary words and classes or by grouping several words into a single class (see Chai and Hung 2008 for an overview). Unsupervised approaches attempt to discover the underlying connections between visual features and words, typically by introducing latent variables. Standard latent semantic analysis (LSA) and its probabilistic variant (PLSA) have been applied to this task (Hofmann, 2001; Monay and Gatica-Perez, 2007; Pan et al., 2004). More sophisticated models estimate the joint distribution of words and regional image features, whilst treating 1We use the terms “text illustration” and “story picturing” interchangeably throughout the paper. annotation as a problem of statistical inference in a graphical model (Barnard et al., 2002; Blei and Jordan, 2003; Wang et al., 2009). Irrespectively of the underlying model or task at hand, much work has focused how to best represent the visual and textual modalities in order to exploit their synergy. Several approaches attempt to rend</context>
<context position="24632" citStr="Hofmann, 2001" startWordPosition="4034" endWordPosition="4035"> model (Feng and Lapata, 2008) that also takes the document into account but crucially assumes that the process of generating the images is independent from the process of generating its keywords. We also compared our approach with two closely related latent variable models (developed for image-caption pairs), a PLSA-based model (Monay and Gatica-Perez, 2007) and CorrLDA (Blei and Jordan, 2003). The former model is an asymmetric version of PLSA; it estimates the topic structure solely from the textual modality and keeps it fixed for the visual modality. The technique is similar to folding-in (Hofmann, 2001), the standard PLSA procedure for inference in unseen documents and allows modeling an image as a mixture of latent topics that is defined only by one modality (in this case the caption words). CorrLDA first generates image regions from a Gaussian multinomial distribution parametrized with Dirichlet priors. Then, for each annotation word, it uniformly selects a region from the image and generates a word according to the topic used to generate that region. We optimized the parameters for both models on the development set. For CorrLDA, we followed the mean-field variational inference strategy p</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Hofmann, T. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning 41(2):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Joshi</author>
<author>J Z Wang</author>
<author>J Li</author>
</authors>
<title>The story picturing engine—a system for automatic text illustration.</title>
<date>2006</date>
<journal>ACM Transactions on Multimedia Computing, Communications, and Applications</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="9010" citStr="Joshi et al., 2006" startWordPosition="1408" endWordPosition="1411">ed on the document collection) to prune from the model’s output words that are not representative of the document’s topics. The proposed approach differs from Feng and Lapata (2008) in three important respects: (a) document-based information is an integral part of our model as we predict caption words given the image and its accompanying document (b) LDA is no longer a post-processing step; our model relies on LDA to infer meaningful topics that capture the cooccurrence of visual features and words; (c) beyond image annotation, we show how the same framework can be applied to story picturing (Joshi et al., 2006), a task which has received less attention in the literature. In terms of model structure, Blei and Jordan 832 (2003) and Monay and Gatica-Perez (2007) are closest to our work. The first model, known as correspondence LDA (CorrLDA), has been successfully employed for modeling annotated images in the Corel domain. CorrLDA also uses the notion of topic to model the generation of images and their captions. In this model, the visual modality drives the definition of the latent space to which the textual modality is linked. The second model is based on PLSA and learns a representation similar to ou</context>
<context position="20107" citStr="Joshi et al., 2006" startWordPosition="3280" endWordPosition="3283">xtual words wt, the n-best of which are the annotations for image I. 4During training, the model has access to all three elements (I,C,D), so the mixed document dMix is the concatenation of the visual terms and words in the caption and document. During testing, the model is given an image and its accompanying document, so dMix contains words based on I and D, but not C. Text Illustration Previous text illustration models are based on Corel-like databases with manual image descriptions (Barnard and Forsyth, 2001; Blei and Jordan, 2003) or instance-based learning using complex learning schemes (Joshi et al., 2006). Here, we present a relatively simple model, again under the topic mixture framework. Given a test document D and a candidate image database I1...N with captions C, we must find the image or images which best describe the document. We can simply compute the probability of each visual term in the vocabulary given D by marginalizing over the document topics z1:K: P(wv|D) = ∑ P(wv|zk)P(zk|dD) (10) z1:K where wv is a visual term and P(wv|zk) the probability of wv given topic zk (as estimated on the training set). Equation (10) delivers a ranked list of visual terms according to a given document. </context>
</contexts>
<marker>Joshi, Wang, Li, 2006</marker>
<rawString>Joshi, D., J.Z. Wang, and J. Li. 2006. The story picturing engine—a system for automatic text illustration. ACM Transactions on Multimedia Computing, Communications, and Applications 2(1):68–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>R Manmatha</author>
<author>J Jeon</author>
</authors>
<title>A model for learning the semantics of pictures.</title>
<date>2003</date>
<booktitle>In Proceedings of the 17th Conference on Advances in Neural Information Processing Systems.</booktitle>
<publisher>MIT, Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2537" citStr="Lavrenko et al., 2003" startWordPosition="377" endWordPosition="380"> (e.g., by clustering images into groups that are visually and semantically coherent) and story picturing (i.e., automatically suggesting images to illustrate text). Automatic image annotation is a popular task in computer vision; a large number of approaches have been proposed in the literature under many distinct learning paradigms. These range from supervised classification (Smeulders et al., 2000; Vailaya et al., 2001) to instantiations of the noisy-channel model (Duygulu et al., 2002), to clustering (Barnard et al., 2002), and methods inspired by information retrieval (Feng et al., 2004; Lavrenko et al., 2003). Despite differences in application and formulation, all these methods essentially attempt to learn the correlation between image features and words from examples of annotated images. The Corel database has been extensively used as a testbed for the development and evaluation of image annotation models. It is a collection of stock photographs, divided into themes (e.g., tigers, sunsets) each of which are associated with keywords (e.g., sun, sea) that are considered appropriate descriptors for all images belonging to the same theme. Unfortunately, the Corel database is not representative of th</context>
<context position="7731" citStr="Lavrenko et al., 2003" startWordPosition="1199" endWordPosition="1202">(Monay and Gatica-Perez, 2007; Zhao and Grosky, 2003). Our own work approaches the image annotation (and related story picturing) task from a slightly different angle. We train and test our model on images that contain implicit (and thus noisy) annotations that have not been specifically created for our task. On account of this, our model has access to knowledge sources other than the image and its keywords (i.e., the news article containing the image we wish to annotate). In Feng and Lapata (2008) we addressed this problem with a modified version of the continuous relevance annotation model (Lavrenko et al., 2003). Unlike other unsupervised approaches where a set of latent variables is introduced, each defining a joint distribution on the space of keywords and image features, the relevance model captures the joint probability of images and annotated words directly, without requiring an intermediate clustering stage (i.e., each annotated image in the training set is treated as a latent variable). We modified this model so as to exploit the information present in the document in two ways. First, in estimating the conditional probability of a keyword given an image, we also considered its likelihood in th</context>
</contexts>
<marker>Lavrenko, Manmatha, Jeon, 2003</marker>
<rawString>Lavrenko, V., R. Manmatha, and J. Jeon. 2003. A model for learning the semantics of pictures. In Proceedings of the 17th Conference on Advances in Neural Information Processing Systems. MIT, Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lowe</author>
</authors>
<title>Object recognition from local scaleinvariant features.</title>
<date>1999</date>
<booktitle>In Proceedings of International Conference on Computer Vision. IEEE Computer Society,</booktitle>
<pages>1150--1157</pages>
<contexts>
<context position="13099" citStr="Lowe, 1999" startWordPosition="2092" endWordPosition="2093">or an image segmentation algorithm. Regions are usually described by a standard set of features including color, texture, and shape which are treated as continuous vectors (e.g., Barnard et al. 2002; Blei and Jordan 2003) or in quantized form (e.g., Duygulu et al. 2002). Through this process, the low-level image features are made to resemble word-like units. Here, we go one step further and represent each image by a bag of visual words, thereby converting visual features from a continuous onto a discrete space. In order to do this we use the Scale Invariant Feature Transform (SIFT) algorithm (Lowe, 1999). The general idea behind the algorithm is to first sample an image with the difference-of-Gaussians point detector at different scales and locations. Importantly, this detector is, to some extent, invariant to translation, scale, rotation and illumination changes. Each detected region is represented with a SIFT descriptor which is a histogram of edge directions at A woman from East Sussex who bought an emu egg sold as a novelty food item on a farm on the Isle of Wight has managed to hatch it into a chick. Osborne the emu will grow Gillian Stone, from to over 6ft tall Bexhill, who breeds chick</context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>Lowe, D. 1999. Object recognition from local scaleinvariant features. In Proceedings of International Conference on Computer Vision. IEEE Computer Society, pages 1150–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Monay</author>
<author>D Gatica-Perez</author>
</authors>
<title>Modeling semantic aspects for cross-media image indexing.</title>
<date>2007</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>29</volume>
<issue>10</issue>
<contexts>
<context position="6332" citStr="Monay and Gatica-Perez, 2007" startWordPosition="970" endWordPosition="973">en applied to the image annotation task. These generally fall under two broad categories. Supervised methods define annotation as a classification task, e.g., by assuming a one-to-one correspondence between vocabulary words and classes or by grouping several words into a single class (see Chai and Hung 2008 for an overview). Unsupervised approaches attempt to discover the underlying connections between visual features and words, typically by introducing latent variables. Standard latent semantic analysis (LSA) and its probabilistic variant (PLSA) have been applied to this task (Hofmann, 2001; Monay and Gatica-Perez, 2007; Pan et al., 2004). More sophisticated models estimate the joint distribution of words and regional image features, whilst treating 1We use the terms “text illustration” and “story picturing” interchangeably throughout the paper. annotation as a problem of statistical inference in a graphical model (Barnard et al., 2002; Blei and Jordan, 2003; Wang et al., 2009). Irrespectively of the underlying model or task at hand, much work has focused how to best represent the visual and textual modalities in order to exploit their synergy. Several approaches attempt to render images more word-like, by r</context>
<context position="9161" citStr="Monay and Gatica-Perez (2007)" startWordPosition="1433" endWordPosition="1436">oach differs from Feng and Lapata (2008) in three important respects: (a) document-based information is an integral part of our model as we predict caption words given the image and its accompanying document (b) LDA is no longer a post-processing step; our model relies on LDA to infer meaningful topics that capture the cooccurrence of visual features and words; (c) beyond image annotation, we show how the same framework can be applied to story picturing (Joshi et al., 2006), a task which has received less attention in the literature. In terms of model structure, Blei and Jordan 832 (2003) and Monay and Gatica-Perez (2007) are closest to our work. The first model, known as correspondence LDA (CorrLDA), has been successfully employed for modeling annotated images in the Corel domain. CorrLDA also uses the notion of topic to model the generation of images and their captions. In this model, the visual modality drives the definition of the latent space to which the textual modality is linked. The second model is based on PLSA and learns a representation similar to ours consisting of textual and visual features. It is also trained using captioned images from the Corel database. We work with noisier and larger datase</context>
<context position="24379" citStr="Monay and Gatica-Perez, 2007" startWordPosition="3990" endWordPosition="3993">g the images into account. This model estimates P(wt|D) = EKk=1 P(wt|zk)P(zk|D), the probability of textual word wt given text document D. We assume that the most probable words are the captions for the accompanying image. Our second baseline is the extended relevance model (Feng and Lapata, 2008) that also takes the document into account but crucially assumes that the process of generating the images is independent from the process of generating its keywords. We also compared our approach with two closely related latent variable models (developed for image-caption pairs), a PLSA-based model (Monay and Gatica-Perez, 2007) and CorrLDA (Blei and Jordan, 2003). The former model is an asymmetric version of PLSA; it estimates the topic structure solely from the textual modality and keeps it fixed for the visual modality. The technique is similar to folding-in (Hofmann, 2001), the standard PLSA procedure for inference in unseen documents and allows modeling an image as a mixture of latent topics that is defined only by one modality (in this case the caption words). CorrLDA first generates image regions from a Gaussian multinomial distribution parametrized with Dirichlet priors. Then, for each annotation word, it uni</context>
<context position="28032" citStr="Monay and Gatica-Perez, 2007" startWordPosition="4598" endWordPosition="4601">5.33 11.80 7.36 TxtLDA 7.30 16.90 10.20 PLSA 10.26 22.60 14.12 ExtRel 14.70 27.90 19.80 MixLDA 16.30 33.10 21.60 Table 2: Automatic image annotation results. 7 Results Our results on the image annotation task are summarized in Table 2. Here, we compare our own model (MixLDA) which is trained on both visual and textual information against an LDA model based solely on textual information (TxtLDA), an extended version of the Continuous Relevance model that also exploits collateral document information (ExtRel; Feng and Lapata 2008), a PLSA model that prioritizes the textual over visual modality (Monay and Gatica-Perez, 2007), and CorrLDA (Blei and Jordan, 2003) which does the opposite. We performed significance testing on F1 using stratified shuffling (Noreen, 1989), an instance of assumption-free approximative randomization testing. Let us first discuss the performance of TxtLDA and MixLDA. These two models are closely related — they both rely on the probabilities P(wt|d) to generate the image keywords — save one important difference. MixLDA uses a concatenated representation of words and visual features assuming that the two modalities have equal importance in defining the latent space, whereas TxtLDA considers</context>
</contexts>
<marker>Monay, Gatica-Perez, 2007</marker>
<rawString>Monay, F. and D. Gatica-Perez. 2007. Modeling semantic aspects for cross-media image indexing. IEEE Transactions on Pattern Analysis and Machine Intelligence 29(10):1802–1817.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pan</author>
<author>H Yang</author>
<author>P Duygulu</author>
<author>C Faloutsos</author>
</authors>
<title>Automatic image captioning.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 International Conference on Multimedia and Expo. Taipei,</booktitle>
<pages>1987--1990</pages>
<contexts>
<context position="6351" citStr="Pan et al., 2004" startWordPosition="974" endWordPosition="977">tion task. These generally fall under two broad categories. Supervised methods define annotation as a classification task, e.g., by assuming a one-to-one correspondence between vocabulary words and classes or by grouping several words into a single class (see Chai and Hung 2008 for an overview). Unsupervised approaches attempt to discover the underlying connections between visual features and words, typically by introducing latent variables. Standard latent semantic analysis (LSA) and its probabilistic variant (PLSA) have been applied to this task (Hofmann, 2001; Monay and Gatica-Perez, 2007; Pan et al., 2004). More sophisticated models estimate the joint distribution of words and regional image features, whilst treating 1We use the terms “text illustration” and “story picturing” interchangeably throughout the paper. annotation as a problem of statistical inference in a graphical model (Barnard et al., 2002; Blei and Jordan, 2003; Wang et al., 2009). Irrespectively of the underlying model or task at hand, much work has focused how to best represent the visual and textual modalities in order to exploit their synergy. Several approaches attempt to render images more word-like, by reducing the dimensi</context>
</contexts>
<marker>Pan, Yang, Duygulu, Faloutsos, 2004</marker>
<rawString>Pan, J., H. Yang, P. Duygulu, and C. Faloutsos. 2004. Automatic image captioning. In Proceedings of the 2004 International Conference on Multimedia and Expo. Taipei, pages 1987–1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="25686" citStr="Salton and McGill, 1983" startWordPosition="4209" endWordPosition="4212">used to generate that region. We optimized the parameters for both models on the development set. For CorrLDA, we followed the mean-field variational inference strategy proposed in Blei (2004). The optimal number of topics for PLSA, was 200 (with 2000 visual terms) and for CorrLDA 50. For the text illustration experiments, the proposed model was compared with three baselines. The first one is essentially word overlap. We select the image whose caption has the largest number of words in common with the test document. The second one is a straightforward implementation of the vector space model (Salton and McGill, 1983) where documents and captions are represented by vectors whose components correspond to term-document co-occurrences. We followed common practice in weighting terms by their tf-idf values, and used the cosine similarity measure to find the image whose caption is most similar to the test document. Our third baseline uses a text-based LDA model to estimate document-caption similarity probabilistically, through topic sharing. The images most relevant to a document are found by maximizing the conditional probability of the candidate captions C given the document dD: P(C|dD) = EKk=1 P(wc|zk)P(zk|dD</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and M.J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing.</booktitle>
<pages>44--49</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="21870" citStr="Schmid, 1994" startWordPosition="3573" endWordPosition="3574"> we output a fixed number of visual terms and select the image with the highest overlap as the correct illustration. 6 Experimental Setup In this section we discuss our experimental design for assessing the performance of the models presented above. We give details on our training procedure and parameter estimation, describe our features, and present the baseline methods used for comparison with our models. Data We evaluated the image annotation and text illustration tasks on the dataset described in Section 3. Documents and captions were part-of-speech tagged and lemmatized with Tree Tagger (Schmid, 1994). We excluded from the vocabulary low frequency words (appearing fewer than five times) and words other than nouns, verbs, and adjectives. For the image annotation task we follow the data split used in Feng and Lapata (2008). The training set contains 2,881 image-caption-document tuples; 240 tuples are reserved for development and 240 for = argmax ∏ Wt wt∈Wt ≈ argmax ∏ Wt wt∈Wt P(wt|zk)P(zk|dMix) P(wt|zk) γk ∑K j=1 γj W∗I ≈ argmax Wt wt∈Wt ∏ P(wt|dMix) (8) K ∑ k=1 K ∑ k=1 W∗I ≈ argmax Wt 835 testing. Our text illustration experiments, used 2,881 image-caption-document tuples for training. For </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing. Manchester, UK, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A W Smeulders</author>
<author>M Worring</author>
<author>S Santini</author>
<author>A Gupta</author>
<author>R Jain</author>
</authors>
<title>Content-based image retrieval at the end of the early years.</title>
<date>2000</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>22</volume>
<issue>12</issue>
<contexts>
<context position="2318" citStr="Smeulders et al., 2000" startWordPosition="343" endWordPosition="346">ted collections. The ability to perform the annotation task automatically would be of significant practical import for many image-based applications. Besides search and retrieval, other examples include browsing support (e.g., by clustering images into groups that are visually and semantically coherent) and story picturing (i.e., automatically suggesting images to illustrate text). Automatic image annotation is a popular task in computer vision; a large number of approaches have been proposed in the literature under many distinct learning paradigms. These range from supervised classification (Smeulders et al., 2000; Vailaya et al., 2001) to instantiations of the noisy-channel model (Duygulu et al., 2002), to clustering (Barnard et al., 2002), and methods inspired by information retrieval (Feng et al., 2004; Lavrenko et al., 2003). Despite differences in application and formulation, all these methods essentially attempt to learn the correlation between image features and words from examples of annotated images. The Corel database has been extensively used as a testbed for the development and evaluation of image annotation models. It is a collection of stock photographs, divided into themes (e.g., tigers,</context>
</contexts>
<marker>Smeulders, Worring, Santini, Gupta, Jain, 2000</marker>
<rawString>Smeulders, A. W., M. Worring, S. Santini, A. Gupta, and R. Jain. 2000. Content-based image retrieval at the end of the early years. IEEE Transactions on Pattern Analysis and Machine Intelligence 22(12):1349–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tang</author>
<author>P H Lewis</author>
</authors>
<title>A study of quality issues for image auto-annotation with the Corel data-set.</title>
<date>2007</date>
<booktitle>IEEE Transactions on Circuits and Systems for Video Technology</booktitle>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="3448" citStr="Tang and Lewis, 2007" startWordPosition="518" endWordPosition="521">tation models. It is a collection of stock photographs, divided into themes (e.g., tigers, sunsets) each of which are associated with keywords (e.g., sun, sea) that are considered appropriate descriptors for all images belonging to the same theme. Unfortunately, the Corel database is not representative of the size or content of real-world image collections. It has a small number of themes with many closely related images which in turn share keyword descriptions. It is therefore relatively easy to learn the associations between images and keywords and do well on annotation and retrieval tasks (Tang and Lewis, 2007; Westerveld and de Vries, 2003). An appealing alternative is the use of resources where images and their annotations co-occur naturally. Examples include images found in news documents, consumer photo collections, Wikipedia articles, illustrated stories and so on. The key idea here is to treat the words in the surrounding text as annotations for the image. These annotations are undoubt831 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 831–839, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics edly nois</context>
</contexts>
<marker>Tang, Lewis, 2007</marker>
<rawString>Tang, J. and P. H. Lewis. 2007. A study of quality issues for image auto-annotation with the Corel data-set. IEEE Transactions on Circuits and Systems for Video Technology 17(3):384–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vailaya</author>
<author>M Figueiredo</author>
<author>A Jain</author>
<author>H Zhang</author>
</authors>
<title>Image classification for content-based indexing.</title>
<date>2001</date>
<journal>IEEE Transactions on Image Processing</journal>
<pages>10--117</pages>
<contexts>
<context position="2341" citStr="Vailaya et al., 2001" startWordPosition="347" endWordPosition="350">lity to perform the annotation task automatically would be of significant practical import for many image-based applications. Besides search and retrieval, other examples include browsing support (e.g., by clustering images into groups that are visually and semantically coherent) and story picturing (i.e., automatically suggesting images to illustrate text). Automatic image annotation is a popular task in computer vision; a large number of approaches have been proposed in the literature under many distinct learning paradigms. These range from supervised classification (Smeulders et al., 2000; Vailaya et al., 2001) to instantiations of the noisy-channel model (Duygulu et al., 2002), to clustering (Barnard et al., 2002), and methods inspired by information retrieval (Feng et al., 2004; Lavrenko et al., 2003). Despite differences in application and formulation, all these methods essentially attempt to learn the correlation between image features and words from examples of annotated images. The Corel database has been extensively used as a testbed for the development and evaluation of image annotation models. It is a collection of stock photographs, divided into themes (e.g., tigers, sunsets) each of which</context>
</contexts>
<marker>Vailaya, Figueiredo, Jain, Zhang, 2001</marker>
<rawString>Vailaya, A., M. Figueiredo, A. Jain, and H. Zhang. 2001. Image classification for content-based indexing. IEEE Transactions on Image Processing 10:117–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>D Blei</author>
<author>L Fei-Fei</author>
</authors>
<title>Simultaneous image classification and annotation.</title>
<date>2009</date>
<booktitle>In Proceedings of CVPR.</booktitle>
<pages>1903--1910</pages>
<location>Miami, FL,</location>
<contexts>
<context position="6697" citStr="Wang et al., 2009" startWordPosition="1027" endWordPosition="1030">underlying connections between visual features and words, typically by introducing latent variables. Standard latent semantic analysis (LSA) and its probabilistic variant (PLSA) have been applied to this task (Hofmann, 2001; Monay and Gatica-Perez, 2007; Pan et al., 2004). More sophisticated models estimate the joint distribution of words and regional image features, whilst treating 1We use the terms “text illustration” and “story picturing” interchangeably throughout the paper. annotation as a problem of statistical inference in a graphical model (Barnard et al., 2002; Blei and Jordan, 2003; Wang et al., 2009). Irrespectively of the underlying model or task at hand, much work has focused how to best represent the visual and textual modalities in order to exploit their synergy. Several approaches attempt to render images more word-like, by reducing the dimensionality of the image feature space (Bosch et al., 2008; Fei-Fei and Perona, 2005) or by learning a single representation for both visual and textual features (Monay and Gatica-Perez, 2007; Zhao and Grosky, 2003). Our own work approaches the image annotation (and related story picturing) task from a slightly different angle. We train and test ou</context>
</contexts>
<marker>Wang, Blei, Fei-Fei, 2009</marker>
<rawString>Wang, C., D. Blei, and L. Fei-Fei. 2009. Simultaneous image classification and annotation. In Proceedings of CVPR. Miami, FL, pages 1903–1910.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Westerveld</author>
<author>A P de Vries</author>
</authors>
<title>Experimental evaluation of a generative probabilistic image retrieval model on ‘easy’ data.</title>
<date>2003</date>
<booktitle>In Proceedings of the SIGIR Multimedia Information Retrieval Workshop.</booktitle>
<location>Toronto, ON.</location>
<marker>Westerveld, de Vries, 2003</marker>
<rawString>Westerveld, T. and A. P. de Vries. 2003. Experimental evaluation of a generative probabilistic image retrieval model on ‘easy’ data. In Proceedings of the SIGIR Multimedia Information Retrieval Workshop. Toronto, ON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zhao</author>
<author>W I Grosky</author>
</authors>
<title>Video shot detection using color anglogram and latent semantic indexing: From contents to semantics.</title>
<date>2003</date>
<booktitle>Handbook of Video Databases: Design and Applications,</booktitle>
<pages>371--392</pages>
<editor>In B. Furht and O. Marques, editors,</editor>
<publisher>CRC Press,</publisher>
<contexts>
<context position="7162" citStr="Zhao and Grosky, 2003" startWordPosition="1101" endWordPosition="1104"> throughout the paper. annotation as a problem of statistical inference in a graphical model (Barnard et al., 2002; Blei and Jordan, 2003; Wang et al., 2009). Irrespectively of the underlying model or task at hand, much work has focused how to best represent the visual and textual modalities in order to exploit their synergy. Several approaches attempt to render images more word-like, by reducing the dimensionality of the image feature space (Bosch et al., 2008; Fei-Fei and Perona, 2005) or by learning a single representation for both visual and textual features (Monay and Gatica-Perez, 2007; Zhao and Grosky, 2003). Our own work approaches the image annotation (and related story picturing) task from a slightly different angle. We train and test our model on images that contain implicit (and thus noisy) annotations that have not been specifically created for our task. On account of this, our model has access to knowledge sources other than the image and its keywords (i.e., the news article containing the image we wish to annotate). In Feng and Lapata (2008) we addressed this problem with a modified version of the continuous relevance annotation model (Lavrenko et al., 2003). Unlike other unsupervised app</context>
</contexts>
<marker>Zhao, Grosky, 2003</marker>
<rawString>Zhao, R. and W. I. Grosky. 2003. Video shot detection using color anglogram and latent semantic indexing: From contents to semantics. In B. Furht and O. Marques, editors, Handbook of Video Databases: Design and Applications, CRC Press, pages 371–392.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>