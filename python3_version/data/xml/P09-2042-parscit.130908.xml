<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005444">
<title confidence="0.998317">
Hierarchical Multi-Class Text Categorization
with Global Margin Maximization
</title>
<author confidence="0.998157">
Xipeng Qiu
</author>
<affiliation confidence="0.9971635">
School of Computer Science
Fudan University
</affiliation>
<email confidence="0.979355">
xpqiu@fudan.edu.cn
</email>
<author confidence="0.98304">
Wenjun Gao
</author>
<affiliation confidence="0.996356">
School of Computer Science
Fudan University
</affiliation>
<email confidence="0.973397">
wjgao616@gmail.com
</email>
<author confidence="0.991765">
Xuanjing Huang
</author>
<affiliation confidence="0.960501">
School of Computer Science
Fudan University
</affiliation>
<email confidence="0.990342">
xjhuang@fudan.edu.cn
</email>
<sectionHeader confidence="0.994637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999795166666667">
Text categorization is a crucial and well-
proven method for organizing the collec-
tion of large scale documents. In this pa-
per, we propose a hierarchical multi-class
text categorization method with global
margin maximization. We not only max-
imize the margins among leaf categories,
but also maximize the margins among
their ancestors. Experiments show that the
performance of our algorithm is competi-
tive with the recently proposed hierarchi-
cal multi-class classification algorithms.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947025641026">
In the past serval years, hierarchical text catego-
rization has become an active research topic in
database area (Koller and Sahami, 1997; Weigend
et al., 1999) and machine learning area (Rousu et
al., 2006; Cai and Hofmann, 2007).
Hierarchical categorization methods can be di-
vided in two types: local and global approaches
(Wang et al., 1999; Sun and Lim, 2001). A lo-
cal approach usually proceeds in a top-down fash-
ion, which firstly picks the most relevant cate-
gories of the top level and then recursively making
the choice among the low-level categories. The
global approach builds only one classifier to dis-
criminate all categories in a hierarchy. Due that the
global hierarchical categorization can avoid the
drawbacks about those high-level irrecoverable er-
ror, it is more popular in the machine learning do-
main.
The essential idea behind global approach is
that the close classes(nodes) have some common
underlying factors. Especially, the descendant
classes can share the characteristics of the ances-
tor classes, which is similar with multi-task learn-
ing(Caruana, 1997). A key problem for global hi-
erarchical categorization is how to combine these
underlying factors.
In this paper, we propose an method for hierar-
chical multi-class text categorization with global
margin maximization. We emphasize that it is im-
portant to separate all the nodes of the correct path
in the class hierarchy from their sibling node, then
we incorporate such information into the formula-
tion of hierarchical support vector machine.
The rest of the paper is organized as follows.
Section 2 describes the basic model of multi-class
hierarchical categorization with maximizing mar-
gin. Then we propose our improved versions in
section 3. Section 4 gives the experimental analy-
sis. Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.977849" genericHeader="introduction">
2 Hierarchical Multi-Class Text
Categorization
</sectionHeader>
<bodyText confidence="0.999022285714286">
Multiclass SVM can be generalized to the problem
of hierarchical categorization (Cai and Hofmann,
2007), which has more than two categories in most
of the case. Denote Yi as the multilabels of xi and
¯Yi the multilabels set not in Yi. The separation
margin of w, with respect to xi, can be approxi-
mated as:
</bodyText>
<equation confidence="0.997573">
γi(W) = min ¯i OΦ(Xi, Y) − Φ(Xi,¯Y), W) (1)
Y∈Yi,¯Y∈ Y
</equation>
<bodyText confidence="0.996991875">
The loss function can be accommodated to
multi-class SVM to scale the penalties for margin
violations proportional to the loss. This is moti-
vated by the fact that margin violations involving
an incorrect class with high loss should be penal-
ized more severely. So the cost-sensitive hierar-
chical multiclass formulation takes takes the fol-
lowing form:
</bodyText>
<equation confidence="0.9868663">
2||w||2 + C
1
s.t.(w,δΦi(y,¯y))&gt;1− ξi
l(Y,¯Y), (Vi,yEYi,¯yE¯Yi)
ξi &gt; 0(Vi)
min
w,ξ
n
ξi (2)
i=1
</equation>
<page confidence="0.977018">
165
</page>
<note confidence="0.923757">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 165–168,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.931626666666667">
where δ4&apos;i(y, ¯y) = 4&apos;(xi, y) − 4&apos;(xi, ¯y),
l(y, ¯y) &gt; 0 and 4&apos;(x, y) is the joint feature of in-
put x and output y, which can be represented as:
</bodyText>
<equation confidence="0.998102">
4&apos;(x, y) = A(y) ® φ(x) (3)
</equation>
<bodyText confidence="0.950555666666667">
where ® is the tensor product. A(y) is the feature
representation of y.
Thus, we can classify a document x to label y?:
</bodyText>
<equation confidence="0.9646325">
y? = arg max F(w, 4&apos;(x, y)) (4)
y
</equation>
<bodyText confidence="0.98284825">
where F(·) is a map function.
There are different kinds of loss functions
l(y, ¯y).
One is thezero-one loss, l0/1(y, u) = [y =� u].
Another is specially designed for the hierarchy
is tree loss(Dekel et al., 2004). Tree loss is defined
as the length of the path between two multilabels
with positive microlabels,
</bodyText>
<equation confidence="0.8821504">
ltr = |path(i : yi = 1, j : uj = 1) |(5)
(Rousu et al., 2006) proposed a simplified ver-
sion of lH, namely l ˜H:
�l Hˆ = cj[yj =� uj&amp;ypa(j) = upa(j)], (6)
j
</equation>
<bodyText confidence="0.999956">
that penalizes a mistake in a child only if the label
of the parent was correct. There are some different
choices for setting cj. One naive idea is to use
a uniform weighting (cj = 1). Another possible
choice is to divide the loss among the sibling:
</bodyText>
<equation confidence="0.887593">
croot = 1,cj = cParent(j)/(|Sib(j) |+ 1) (7)
</equation>
<bodyText confidence="0.951337">
Another possible choice is to scale the loss by the
proportion of the hierarchy that is in the subtree
T (j) rooted by j:
</bodyText>
<equation confidence="0.995729">
cj = |T(j)|/|T(root) |(8)
</equation>
<bodyText confidence="0.994819333333333">
Using these scaling weights, the derived losses are
referred as lu; i,l ˆb and l ˆsub respectively.
si
</bodyText>
<sectionHeader confidence="0.914594333333333" genericHeader="method">
3 Hierarchical Multi-Class Text
Categorization with Global Margin
Maximization
</sectionHeader>
<bodyText confidence="0.99996941025641">
In previous literature (Cai and Hofmann, 2004;
Tsochantaridis et al., 2005), they focused on sep-
arating the correct path from those incorrect path.
Inspired by the example in Figure 1, we emphasize
it is also important to separate the ancestor node in
the correct path from their sibling node.
The vector w can be decomposed in to the set
of wi for each node (category) in the hierarchy. In
Figure 1, the example hierarchy has 7 nodes and 4
of them are leaf nodes. The category is encode
as an integer, 1, ... , 7. Suppose that the train-
ing pattern x belongs to category 4. Both w in
the Figure 1a and Figure 1b can successfully clas-
sify x into category 4, since F(w,4&apos;(x, y4)) =
E1,2,4 (wi, x) is the maximal among all the possi-
ble discriminate functions. So both learned param-
eter w is acceptable in current hierarchical support
vector machine.
Here we claim the w in Figure 1b is better than the
w in Figure 1a. Since we notice in Figure 1a, the
discriminate function (w2, x) is smaller than the
discriminate function (w3, x). The discriminate
function (wi, x) measures the similarity of x to
category i. The larger the discriminate function is,
the more similar x is to category i. Since category
2 is in the path from the root to the correct cate-
gory and category 3 is not, intuitively, x should be
closer to category 2 than category 3. But the dis-
criminate function in Figure 1a is contradictive to
this assumption. But such information is reflected
correctly in Figure 1b. So we conclude w in Fig.
1b is superior to w in 1a.
Here we propose a novel formulation to incor-
porate such information. Denote Ai as the mul-
tilabel in Yi that corresponds to the nonleaf cate-
gories and Sib(z) denotes the sibling nodes of z,
that is the set of nodes that have the same parent
with z, except z itself. Implementing the above
idea, we can get the following formulation:
</bodyText>
<equation confidence="0.985281136363636">
2IIwII2 + C1
1 �
i
s.t.(w, δ4&apos;i(y, ¯y)) &gt; 1 − ξi , (bi, y E Yi )
l(y, y) y E Yi
(w, δ4&apos;i(z, ¯z)) &gt; 1 − ζi
l(z, ¯z), (bi, z Ez¯ E Sib()) )
ξi &gt; 0(bi)
ζi &gt; 0(bi)
It arrives at the following Lagrangian:
L(w, ξ1, ..., ξn, S1, ..., Sn)
= 1kwk2 + C1X ξi + C2 X Si
i i
αiY¯Y(hw, δΦi(y,¯y)i − 1 + ξi
l(y, ¯y))
min
W,ξ,ζ
ξi + C2 � ζi (9)
i
X− X
i Y∈Yi
¯Y∈¯Yi
</equation>
<page confidence="0.981733">
166
</page>
<figure confidence="0.975462">
a) b)
</figure>
<figureCaption confidence="0.997033">
Figure 1: Two different discriminant function in a hierarchy
</figureCaption>
<figure confidence="0.98450836">
w2,x =3
w4,x =5 w5,x =1 w6,x =2 w7,x =1
4
2
5
1
6
w1,x
3
10
w3,x =5
7
w2,x =7
w4,x =5 w5,x =1 w6,x =2 w7,x =1
4
2
5
1
6
w1,x
3
10
w3,x
7
3
</figure>
<equation confidence="0.944785230769231">
− X Niz¯z(hw, − 1 + Si
i z∈Ai l(z, ¯z))
¯z∈Sib(z)
− Xci�i − diSi (10)
i i
The dual QP becomes
X
max Θ(α) =
α
i
� 1 0, =
θ2 0,
2 X X X i,j,z,¯z,k,¯k,
</equation>
<table confidence="0.782784470588235">
i,j zEAi kEAj
¯zESib(z) ¯kESib(k)
s.t.αiy¯y &gt;
βjz¯z &gt;
αiy¯y &lt;
X l (Y1 y)
yEYi
¯yE ¯Yi
βiz¯z
&lt;
X l(z, ¯z)
zEAi
¯zESib(z)
where θ1
i,j,y,¯y,r,¯r
αiy¯yαjr¯r(δΦi(y,¯y), δΦj(r, ¯r)) and θ2 i,j,z,¯z,k,¯k =
βiz¯zβjk¯k(δΦi(z, ¯z), δΦj(k, k)).
</table>
<subsectionHeader confidence="0.99632">
3.1 Optimization Algorithm
</subsectionHeader>
<bodyText confidence="0.999633">
The derived QP can be very large, since the num-
ber of α and β variables is up to O(n * 2N), where
n is number of training pattern and N is the num-
ber of nodes in the hierarchy. But two properties
of the dual problem can be exploited to design a
much more efficient optimization.
First, the constraints in the dual problem Eq. 11
- Eq. 15 factorize over the instance index for both
α-variables and β-variables. The constraints in
Eq. 14 do not couple α-variables and β-variables
together. Further, dual variables αiy¯y and αjy, ¯y,
belonging to different training instances i and j do
not join in a same constraints. This inspired an
optimization procedure which iteratively performs
subspace optimization over all dual variables αiy¯y
belonging to the same training instance. This will
in general reduced to a much smaller QP, since
it freezes all αjy¯y with j =� i and β-variables at
their current values. This strategy can be applied
in solving β-variables.
Secondly, the number of active constraints at the
solution is expected to be relatively small, since
only a small fraction of categories y¯ E ¯Yi ( or
y¯ E Sib(y) when y E Ai) will typically fail to
achieve the required margin. The expected sparse-
ness of the variable for the dual problem can be
exploited by employing a variable selection strat-
egy. Equivalently, this corresponds to a cutting
plane algorithm for the primal QP. Intuitively, we
will identify the most violated margin constraint
with index (i, y, ¯y) and then add the correspond-
ing variable to the optimization problem. This
means that we start with extremely sparse prob-
lems and only successively increase the number of
variables in the active set. This general approach
to deal with large linear or quadratic optimization
problems is also known as column selection. In
practice, it is often not necessary to optimize until
final convergence, which adds to the attractiveness
of this approach.
We have used the LOQO optimization package
(Vanderbei, 1999) in our experiments.
</bodyText>
<sectionHeader confidence="0.997236" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999421">
We evaluate our proposed model on the section D
in the WIPO-alpha collection1, which consists of
the 1372 training and 358 testing document. The
</bodyText>
<footnote confidence="0.626265">
1World Intellectual Property Organization (WIPO)
</footnote>
<figure confidence="0.7757443125">
1
�
X
rEYj
¯rE¯Yj
X
2
i,j
X
yEYi
¯yE¯Yi
X Xαiy¯y + X βiz¯z
yEYi i zEAi
¯yE ¯Yi ¯zESib(z)
θ1 (11)
i,j,y,¯y,r,¯r
</figure>
<page confidence="0.986583">
167
</page>
<tableCaption confidence="0.953332">
Table 1: Prediction losses (%) obtained on WIPO.
The values per column is calculated with the dif-
ferent loss function.
</tableCaption>
<table confidence="0.998719666666667">
Test l0/1 lo ltr luni lsib lsub
Tra
l0/1 HSVM 48.6 188.8 94.4 97.2 5.4 7.5
HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4
lo HSVM 49.7 187.7 93.9 99.4 5.0 7.1
HSVM-S 47.8 165.3 89.7 90.5 4.8 6.9
HM3 70.9 167.0 - 89.1 5.0 7.0
ltr HSVM 49.4 186.0 93.0 98.9 5.0 7.5
HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1
l ˆuni HSVM 47.2 181.0 90.5 94.4 5.0 7.0
HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9
HM3 70.1 172.1 - 88.8 5.2 7.4
l ˆsib HSVM 49.4 184.9 92.5 98.9 4.8 7.4
HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4
HM3 64.8 172.9 - 92.7 4.8 7.1
l ˆsub HSVM 50.6 189.9 95.0 101.1 5.2 7.5
HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6
HM3 65.0 170.9 - 91.9 4.8 7.2
</table>
<bodyText confidence="0.9995998">
number of nodes in the hierarchy is 188, with max-
imum depth 3.
We compared the performance of our proposed
method HSVM-S with two algorithms: HSVM(Cai
and Hofmann, 2007) and HM3(Rousu et al., 2006).
</bodyText>
<subsectionHeader confidence="0.997647">
4.1 Effect of Different Loss Function
</subsectionHeader>
<bodyText confidence="0.99996865">
We compare the methods based on different loss
functions, l0/1, lo, ltr, lˆuni, lˆsib and lˆsub. The per-
formances for three algorithms can be seen in Ta-
ble 1. Those empty cells, denoted by “-”, are not
available in (Rousu et al., 2006).
As expected, l0/1 is inferior to other hierarchi-
cal losses by getting poorest performance in all the
testing losses, since it can not take into account the
hierarchical information between categories. The
results suggests that training with a hierarchical
losses function, like lˆsib or lˆuni, would lead to a
better reduced l0/1 on the test set as well as in
terms of the hierarchical loss. In Table 1, we can
also point out that when training with the same
hierarchical loss, the performance of HSVM-S is
better than HSVM under the measure of most hier-
archical losses, since HSVM-S includes more hier-
archical information,the relationship between the
sibling categories, than HSVM which only separate
the leave categories.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998493857142857">
In this paper we present a hierarchical multi-class
document categorization, which focus on maxi-
mize the margin of the classes at the different
levels in the class hierarchy. In future work, we
plan to extend the proposed hierarchical learning
method to the case where the hierarchy is a DAG
instead of tree and scale up the method further.
</bodyText>
<sectionHeader confidence="0.997139" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9989002">
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302.
</bodyText>
<sectionHeader confidence="0.998788" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999449076923077">
L. Cai and T Hofmann. 2004. Hierarchical docu-
ment categorization with support vector machines.
In Proceedings of the ACM Conference on Informa-
tion and Knowledge Management.
L. Cai and T. Hofmann. 2007. Exploiting known tax-
onomies in learning overlapping concepts. In Pro-
ceedings of International Joint Conferences on Arti-
ficial Intelligence.
R. Caruana. 1997. Multi-task learning. Machine
Learning, 28(1):41–75.
Ofer Dekel, Joseph Keshet, and Yoram Singer. 2004.
Large margin hierarchical classification. In Pro-
ceedings of the 21 st International Conference on
Machine Learning.
D. Koller and M Sahami. 1997. Hierarchically classi-
fying documents using very few words. In Proceed-
ings of the International Conference on Machine
Learning (ICML).
Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2006. Kernel-based learning
of hierarchical multilabel classification models. In
Journal of Machine Learning Research.
A. Sun and E.-P Lim. 2001. Hierarchical text classi-
fication and evaluation. In Proceedings of the IEEE
International Conference on Data Mining (ICDM).
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning.
R. J. Vanderbei. 1999. Loqo: An interior point code
for quadratic programming. In Optimization Meth-
ods and Software.
K. Wang, S. Zhou, and S Liew. 1999. Building hier-
archical classifiers using class proximities. In Pro-
ceedings of the International Conference on Very
Large Data Bases (VLDB).
A. Weigend, E. Wiener, and J Pedersen. 1999. Exploit-
ing hierarchy in text categorization. In Information
Retrieval.
</reference>
<page confidence="0.997272">
168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.620816">
<title confidence="0.996816">Hierarchical Multi-Class Text Categorization with Global Margin Maximization</title>
<author confidence="0.995665">Xipeng Qiu</author>
<affiliation confidence="0.99983">School of Computer Science Fudan University</affiliation>
<email confidence="0.778953">xpqiu@fudan.edu.cn</email>
<author confidence="0.933461">Wenjun Gao</author>
<affiliation confidence="0.9999675">School of Computer Science Fudan University</affiliation>
<email confidence="0.992704">wjgao616@gmail.com</email>
<author confidence="0.996552">Xuanjing Huang</author>
<affiliation confidence="0.999828">School of Computer Science Fudan University</affiliation>
<email confidence="0.889927">xjhuang@fudan.edu.cn</email>
<abstract confidence="0.998175230769231">Text categorization is a crucial and wellproven method for organizing the collection of large scale documents. In this paper, we propose a hierarchical multi-class text categorization method with global margin maximization. We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Cai</author>
<author>T Hofmann</author>
</authors>
<title>Hierarchical document categorization with support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="5060" citStr="Cai and Hofmann, 2004" startWordPosition="835" endWordPosition="838">abel of the parent was correct. There are some different choices for setting cj. One naive idea is to use a uniform weighting (cj = 1). Another possible choice is to divide the loss among the sibling: croot = 1,cj = cParent(j)/(|Sib(j) |+ 1) (7) Another possible choice is to scale the loss by the proportion of the hierarchy that is in the subtree T (j) rooted by j: cj = |T(j)|/|T(root) |(8) Using these scaling weights, the derived losses are referred as lu; i,l ˆb and l ˆsub respectively. si 3 Hierarchical Multi-Class Text Categorization with Global Margin Maximization In previous literature (Cai and Hofmann, 2004; Tsochantaridis et al., 2005), they focused on separating the correct path from those incorrect path. Inspired by the example in Figure 1, we emphasize it is also important to separate the ancestor node in the correct path from their sibling node. The vector w can be decomposed in to the set of wi for each node (category) in the hierarchy. In Figure 1, the example hierarchy has 7 nodes and 4 of them are leaf nodes. The category is encode as an integer, 1, ... , 7. Suppose that the training pattern x belongs to category 4. Both w in the Figure 1a and Figure 1b can successfully classify x into </context>
</contexts>
<marker>Cai, Hofmann, 2004</marker>
<rawString>L. Cai and T Hofmann. 2004. Hierarchical document categorization with support vector machines. In Proceedings of the ACM Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cai</author>
<author>T Hofmann</author>
</authors>
<title>Exploiting known taxonomies in learning overlapping concepts.</title>
<date>2007</date>
<booktitle>In Proceedings of International Joint Conferences on Artificial Intelligence.</booktitle>
<contexts>
<context position="1040" citStr="Cai and Hofmann, 2007" startWordPosition="144" endWordPosition="147">ments. In this paper, we propose a hierarchical multi-class text categorization method with global margin maximization. We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms. 1 Introduction In the past serval years, hierarchical text categorization has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Hierarchical categorization methods can be divided in two types: local and global approaches (Wang et al., 1999; Sun and Lim, 2001). A local approach usually proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories. The global approach builds only one classifier to discriminate all categories in a hierarchy. Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. The essent</context>
<context position="2757" citStr="Cai and Hofmann, 2007" startWordPosition="411" endWordPosition="414">ant to separate all the nodes of the correct path in the class hierarchy from their sibling node, then we incorporate such information into the formulation of hierarchical support vector machine. The rest of the paper is organized as follows. Section 2 describes the basic model of multi-class hierarchical categorization with maximizing margin. Then we propose our improved versions in section 3. Section 4 gives the experimental analysis. Section 5 concludes the paper. 2 Hierarchical Multi-Class Text Categorization Multiclass SVM can be generalized to the problem of hierarchical categorization (Cai and Hofmann, 2007), which has more than two categories in most of the case. Denote Yi as the multilabels of xi and ¯Yi the multilabels set not in Yi. The separation margin of w, with respect to xi, can be approximated as: γi(W) = min ¯i OΦ(Xi, Y) − Φ(Xi,¯Y), W) (1) Y∈Yi,¯Y∈ Y The loss function can be accommodated to multi-class SVM to scale the penalties for margin violations proportional to the loss. This is motivated by the fact that margin violations involving an incorrect class with high loss should be penalized more severely. So the cost-sensitive hierarchical multiclass formulation takes takes the followi</context>
<context position="11072" citStr="Cai and Hofmann, 2007" startWordPosition="1971" endWordPosition="1974">.8 165.3 89.7 90.5 4.8 6.9 HM3 70.9 167.0 - 89.1 5.0 7.0 ltr HSVM 49.4 186.0 93.0 98.9 5.0 7.5 HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1 l ˆuni HSVM 47.2 181.0 90.5 94.4 5.0 7.0 HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9 HM3 70.1 172.1 - 88.8 5.2 7.4 l ˆsib HSVM 49.4 184.9 92.5 98.9 4.8 7.4 HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4 HM3 64.8 172.9 - 92.7 4.8 7.1 l ˆsub HSVM 50.6 189.9 95.0 101.1 5.2 7.5 HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6 HM3 65.0 170.9 - 91.9 4.8 7.2 number of nodes in the hierarchy is 188, with maximum depth 3. We compared the performance of our proposed method HSVM-S with two algorithms: HSVM(Cai and Hofmann, 2007) and HM3(Rousu et al., 2006). 4.1 Effect of Different Loss Function We compare the methods based on different loss functions, l0/1, lo, ltr, lˆuni, lˆsib and lˆsub. The performances for three algorithms can be seen in Table 1. Those empty cells, denoted by “-”, are not available in (Rousu et al., 2006). As expected, l0/1 is inferior to other hierarchical losses by getting poorest performance in all the testing losses, since it can not take into account the hierarchical information between categories. The results suggests that training with a hierarchical losses function, like lˆsib or lˆuni, w</context>
</contexts>
<marker>Cai, Hofmann, 2007</marker>
<rawString>L. Cai and T. Hofmann. 2007. Exploiting known taxonomies in learning overlapping concepts. In Proceedings of International Joint Conferences on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multi-task learning. Machine Learning,</booktitle>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="1888" citStr="Caruana, 1997" startWordPosition="278" endWordPosition="280">gories of the top level and then recursively making the choice among the low-level categories. The global approach builds only one classifier to discriminate all categories in a hierarchy. Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. The essential idea behind global approach is that the close classes(nodes) have some common underlying factors. Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learning(Caruana, 1997). A key problem for global hierarchical categorization is how to combine these underlying factors. In this paper, we propose an method for hierarchical multi-class text categorization with global margin maximization. We emphasize that it is important to separate all the nodes of the correct path in the class hierarchy from their sibling node, then we incorporate such information into the formulation of hierarchical support vector machine. The rest of the paper is organized as follows. Section 2 describes the basic model of multi-class hierarchical categorization with maximizing margin. Then we</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>R. Caruana. 1997. Multi-task learning. Machine Learning, 28(1):41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Yoram Singer</author>
</authors>
<title>Large margin hierarchical classification.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21 st International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4134" citStr="Dekel et al., 2004" startWordPosition="665" endWordPosition="668">rt Papers, pages 165–168, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP where δ4&apos;i(y, ¯y) = 4&apos;(xi, y) − 4&apos;(xi, ¯y), l(y, ¯y) &gt; 0 and 4&apos;(x, y) is the joint feature of input x and output y, which can be represented as: 4&apos;(x, y) = A(y) ® φ(x) (3) where ® is the tensor product. A(y) is the feature representation of y. Thus, we can classify a document x to label y?: y? = arg max F(w, 4&apos;(x, y)) (4) y where F(·) is a map function. There are different kinds of loss functions l(y, ¯y). One is thezero-one loss, l0/1(y, u) = [y =� u]. Another is specially designed for the hierarchy is tree loss(Dekel et al., 2004). Tree loss is defined as the length of the path between two multilabels with positive microlabels, ltr = |path(i : yi = 1, j : uj = 1) |(5) (Rousu et al., 2006) proposed a simplified version of lH, namely l ˜H: �l Hˆ = cj[yj =� uj&amp;ypa(j) = upa(j)], (6) j that penalizes a mistake in a child only if the label of the parent was correct. There are some different choices for setting cj. One naive idea is to use a uniform weighting (cj = 1). Another possible choice is to divide the loss among the sibling: croot = 1,cj = cParent(j)/(|Sib(j) |+ 1) (7) Another possible choice is to scale the loss by t</context>
</contexts>
<marker>Dekel, Keshet, Singer, 2004</marker>
<rawString>Ofer Dekel, Joseph Keshet, and Yoram Singer. 2004. Large margin hierarchical classification. In Proceedings of the 21 st International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>M Sahami</author>
</authors>
<title>Hierarchically classifying documents using very few words.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="947" citStr="Koller and Sahami, 1997" startWordPosition="128" endWordPosition="131">orization is a crucial and wellproven method for organizing the collection of large scale documents. In this paper, we propose a hierarchical multi-class text categorization method with global margin maximization. We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms. 1 Introduction In the past serval years, hierarchical text categorization has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Hierarchical categorization methods can be divided in two types: local and global approaches (Wang et al., 1999; Sun and Lim, 2001). A local approach usually proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories. The global approach builds only one classifier to discriminate all categories in a hierarchy. Due that the global hierarchical categorization can avoid the drawbacks about those </context>
</contexts>
<marker>Koller, Sahami, 1997</marker>
<rawString>D. Koller and M Sahami. 1997. Hierarchically classifying documents using very few words. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juho Rousu</author>
<author>Craig Saunders</author>
<author>Sandor Szedmak</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Kernel-based learning of hierarchical multilabel classification models.</title>
<date>2006</date>
<journal>In Journal of Machine Learning Research.</journal>
<contexts>
<context position="1016" citStr="Rousu et al., 2006" startWordPosition="140" endWordPosition="143"> of large scale documents. In this paper, we propose a hierarchical multi-class text categorization method with global margin maximization. We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms. 1 Introduction In the past serval years, hierarchical text categorization has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Hierarchical categorization methods can be divided in two types: local and global approaches (Wang et al., 1999; Sun and Lim, 2001). A local approach usually proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories. The global approach builds only one classifier to discriminate all categories in a hierarchy. Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine lea</context>
<context position="4295" citStr="Rousu et al., 2006" startWordPosition="698" endWordPosition="701">nt feature of input x and output y, which can be represented as: 4&apos;(x, y) = A(y) ® φ(x) (3) where ® is the tensor product. A(y) is the feature representation of y. Thus, we can classify a document x to label y?: y? = arg max F(w, 4&apos;(x, y)) (4) y where F(·) is a map function. There are different kinds of loss functions l(y, ¯y). One is thezero-one loss, l0/1(y, u) = [y =� u]. Another is specially designed for the hierarchy is tree loss(Dekel et al., 2004). Tree loss is defined as the length of the path between two multilabels with positive microlabels, ltr = |path(i : yi = 1, j : uj = 1) |(5) (Rousu et al., 2006) proposed a simplified version of lH, namely l ˜H: �l Hˆ = cj[yj =� uj&amp;ypa(j) = upa(j)], (6) j that penalizes a mistake in a child only if the label of the parent was correct. There are some different choices for setting cj. One naive idea is to use a uniform weighting (cj = 1). Another possible choice is to divide the loss among the sibling: croot = 1,cj = cParent(j)/(|Sib(j) |+ 1) (7) Another possible choice is to scale the loss by the proportion of the hierarchy that is in the subtree T (j) rooted by j: cj = |T(j)|/|T(root) |(8) Using these scaling weights, the derived losses are referred a</context>
<context position="11100" citStr="Rousu et al., 2006" startWordPosition="1976" endWordPosition="1979">70.9 167.0 - 89.1 5.0 7.0 ltr HSVM 49.4 186.0 93.0 98.9 5.0 7.5 HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1 l ˆuni HSVM 47.2 181.0 90.5 94.4 5.0 7.0 HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9 HM3 70.1 172.1 - 88.8 5.2 7.4 l ˆsib HSVM 49.4 184.9 92.5 98.9 4.8 7.4 HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4 HM3 64.8 172.9 - 92.7 4.8 7.1 l ˆsub HSVM 50.6 189.9 95.0 101.1 5.2 7.5 HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6 HM3 65.0 170.9 - 91.9 4.8 7.2 number of nodes in the hierarchy is 188, with maximum depth 3. We compared the performance of our proposed method HSVM-S with two algorithms: HSVM(Cai and Hofmann, 2007) and HM3(Rousu et al., 2006). 4.1 Effect of Different Loss Function We compare the methods based on different loss functions, l0/1, lo, ltr, lˆuni, lˆsib and lˆsub. The performances for three algorithms can be seen in Table 1. Those empty cells, denoted by “-”, are not available in (Rousu et al., 2006). As expected, l0/1 is inferior to other hierarchical losses by getting poorest performance in all the testing losses, since it can not take into account the hierarchical information between categories. The results suggests that training with a hierarchical losses function, like lˆsib or lˆuni, would lead to a better reduce</context>
</contexts>
<marker>Rousu, Saunders, Szedmak, Shawe-Taylor, 2006</marker>
<rawString>Juho Rousu, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. In Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sun</author>
<author>E-P Lim</author>
</authors>
<title>Hierarchical text classification and evaluation.</title>
<date>2001</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM).</booktitle>
<contexts>
<context position="1173" citStr="Sun and Lim, 2001" startWordPosition="166" endWordPosition="169">ize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms. 1 Introduction In the past serval years, hierarchical text categorization has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Hierarchical categorization methods can be divided in two types: local and global approaches (Wang et al., 1999; Sun and Lim, 2001). A local approach usually proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories. The global approach builds only one classifier to discriminate all categories in a hierarchy. Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. The essential idea behind global approach is that the close classes(nodes) have some common underlying factors. Especially, the descendant clas</context>
</contexts>
<marker>Sun, Lim, 2001</marker>
<rawString>A. Sun and E.-P Lim. 2001. Hierarchical text classification and evaluation. In Proceedings of the IEEE International Conference on Data Mining (ICDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>In Journal of Machine Learning.</journal>
<contexts>
<context position="5090" citStr="Tsochantaridis et al., 2005" startWordPosition="839" endWordPosition="842">correct. There are some different choices for setting cj. One naive idea is to use a uniform weighting (cj = 1). Another possible choice is to divide the loss among the sibling: croot = 1,cj = cParent(j)/(|Sib(j) |+ 1) (7) Another possible choice is to scale the loss by the proportion of the hierarchy that is in the subtree T (j) rooted by j: cj = |T(j)|/|T(root) |(8) Using these scaling weights, the derived losses are referred as lu; i,l ˆb and l ˆsub respectively. si 3 Hierarchical Multi-Class Text Categorization with Global Margin Maximization In previous literature (Cai and Hofmann, 2004; Tsochantaridis et al., 2005), they focused on separating the correct path from those incorrect path. Inspired by the example in Figure 1, we emphasize it is also important to separate the ancestor node in the correct path from their sibling node. The vector w can be decomposed in to the set of wi for each node (category) in the hierarchy. In Figure 1, the example hierarchy has 7 nodes and 4 of them are leaf nodes. The category is encode as an integer, 1, ... , 7. Suppose that the training pattern x belongs to category 4. Both w in the Figure 1a and Figure 1b can successfully classify x into category 4, since F(w,4&apos;(x, y4</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. In Journal of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Vanderbei</author>
</authors>
<title>Loqo: An interior point code for quadratic programming.</title>
<date>1999</date>
<booktitle>In Optimization Methods and Software.</booktitle>
<contexts>
<context position="9831" citStr="Vanderbei, 1999" startWordPosition="1735" endWordPosition="1736">ithm for the primal QP. Intuitively, we will identify the most violated margin constraint with index (i, y, ¯y) and then add the corresponding variable to the optimization problem. This means that we start with extremely sparse problems and only successively increase the number of variables in the active set. This general approach to deal with large linear or quadratic optimization problems is also known as column selection. In practice, it is often not necessary to optimize until final convergence, which adds to the attractiveness of this approach. We have used the LOQO optimization package (Vanderbei, 1999) in our experiments. 4 Experiment We evaluate our proposed model on the section D in the WIPO-alpha collection1, which consists of the 1372 training and 358 testing document. The 1World Intellectual Property Organization (WIPO) 1 � X rEYj ¯rE¯Yj X 2 i,j X yEYi ¯yE¯Yi X Xαiy¯y + X βiz¯z yEYi i zEAi ¯yE ¯Yi ¯zESib(z) θ1 (11) i,j,y,¯y,r,¯r 167 Table 1: Prediction losses (%) obtained on WIPO. The values per column is calculated with the different loss function. Test l0/1 lo ltr luni lsib lsub Tra l0/1 HSVM 48.6 188.8 94.4 97.2 5.4 7.5 HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4 lo HSVM 49.7 187.7 93.9 99.</context>
</contexts>
<marker>Vanderbei, 1999</marker>
<rawString>R. J. Vanderbei. 1999. Loqo: An interior point code for quadratic programming. In Optimization Methods and Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wang</author>
<author>S Zhou</author>
<author>S Liew</author>
</authors>
<title>Building hierarchical classifiers using class proximities.</title>
<date>1999</date>
<booktitle>In Proceedings of the International Conference on Very Large Data Bases (VLDB).</booktitle>
<contexts>
<context position="1153" citStr="Wang et al., 1999" startWordPosition="162" endWordPosition="165">. We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms. 1 Introduction In the past serval years, hierarchical text categorization has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Hierarchical categorization methods can be divided in two types: local and global approaches (Wang et al., 1999; Sun and Lim, 2001). A local approach usually proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories. The global approach builds only one classifier to discriminate all categories in a hierarchy. Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain. The essential idea behind global approach is that the close classes(nodes) have some common underlying factors. Especially,</context>
</contexts>
<marker>Wang, Zhou, Liew, 1999</marker>
<rawString>K. Wang, S. Zhou, and S Liew. 1999. Building hierarchical classifiers using class proximities. In Proceedings of the International Conference on Very Large Data Bases (VLDB).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Weigend</author>
<author>E Wiener</author>
<author>J Pedersen</author>
</authors>
<title>Exploiting hierarchy in text categorization.</title>
<date>1999</date>
<booktitle>In Information Retrieval.</booktitle>
<contexts>
<context position="970" citStr="Weigend et al., 1999" startWordPosition="132" endWordPosition="135">d wellproven method for organizing the collection of large scale documents. In this paper, we propose a hierarchical multi-class text categorization method with global margin maximization. We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors. Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms. 1 Introduction In the past serval years, hierarchical text categorization has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007). Hierarchical categorization methods can be divided in two types: local and global approaches (Wang et al., 1999; Sun and Lim, 2001). A local approach usually proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories. The global approach builds only one classifier to discriminate all categories in a hierarchy. Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverabl</context>
</contexts>
<marker>Weigend, Wiener, Pedersen, 1999</marker>
<rawString>A. Weigend, E. Wiener, and J Pedersen. 1999. Exploiting hierarchy in text categorization. In Information Retrieval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>