<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002246">
<title confidence="0.9581045">
Parsing the Wall Street Journal with the
Inside-Outside Algorithm
</title>
<author confidence="0.94881">
Yves Schabes Michal Roth Randy Osborne
</author>
<affiliation confidence="0.713054">
Mitsubishi Electric Research Laboratories
</affiliation>
<address confidence="0.6149845">
Cambridge MA 02139
USA
</address>
<email confidence="0.998257">
(schabesirothiosborne@merl.com)
</email>
<sectionHeader confidence="0.996624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999537555555556">
We report grammar inference experiments on
partially parsed sentences taken from the Wall
Street Journal corpus using the inside-outside
algorithm for stochastic context-free grammars.
The initial grammar for the inference process
makes no assumption of the kinds of structures
and their distributions. The inferred grammar is
evaluated by its predicting power and by com-
paring the bracketing of held out sentences
imposed by the inferred grammar with the par-
tial bracketings of these sentences given in the
corpus. Using part-of-speech tags as the only
source of lexical information, high bracketing
accuracy is achieved even with a small subset
of the available training material (1045 sen-
tences): 94.4% for test sentences shorter than
10 words and 90.2% for sentences shorter than
15 words.
</bodyText>
<sectionHeader confidence="0.998871" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947894736842">
Most broad coverage natural Language parsers have
been designed by incorporating hand-crafted rules.
These rules are also very often further refined by statisti-
cal training. Furthermore, it is widely believed that high
performance can only be achieved by disambiguating
lexically sensitive phenomena such as prepositional
attachment ambiguity, coordination or subcategoriza-
tion.
So far, grammar inference has not been shown to be
effective for designing wide coverage parsers.
Baker (1979) describes a training algorithm for sto-
chastic context-free grammars (SCFG) which can be
used for grammar reestimation (Fujisaki et al. 1989,
Sharman et al. 1990, Black et al. 1992, Briscoe and Wae-
gner 1992) or grammar inference from scratch (Lan i and
Young 1990). However, the application of SCFGs and
the original inside-outside algorithm for grammar infer-
ence has been inconclusive for two reasons. First, each
iteration of the algorithm on a grammar with n nontermi-
nals requires 0( n31w13 ) time per training sentence w. Sec-
ond, the inferred grammar imposes bracketings which do
not agree with linguistic judgments of sentence struc-
ture.
Pereira and Schabes (1992) extended the inside-out-
side algorithm for inferring the parameters of a stochas-
tic context-free grammar to take advantage of
constituent bracketing information in the training text.
Although they report encouraging experiments (90%
bracketing accuracy) on language transcriptions in the
Texas Instrument subset of the Air Travel Information
System (ATIS), the small size of the corpus (770 brack-
eted sentences containing a total of 7812 words), its lin-
guistic simplicity, and the computation time required to
train the grammar were reasons to believe that these
results may not scale up to a larger and more diverse cor-
pus.
We report grammar inference experiments with this
algorithm from the parsed Wall Street Journal corpus.
</bodyText>
<page confidence="0.997063">
341
</page>
<bodyText confidence="0.999834405405405">
The experiments prove the feasibility and effectiveness
of the inside-outside algorithm on a large corpus.
Such experiments are made possible by assuming a
right branching structure whenever the parsed corpus
leaves portions of the parsed tree unspecified. This pre-
processing of the corpus makes it fully bracketed. By
taking advantage of this fact in the implementation of the
inside-outside algorithm, its complexity becomes linear
with respect to the input length (as noted by Pereira and
Schabes, 1992) and therefore tractable for large corpora.
We report experiments using several kinds of initial
grammars and a variety of subsets of the corpus as train-
ing data. When the entire Wall Street Journal corpus was
used as training material, the time required for training
has been further reduced by using a parallel implementa-
tion of the inside-outside algorithm.
The inferred grammar is evaluated by measuring the
percentage of compatible brackets of the bracketing
imposed by the inferred grammar with the partial brack-
eting of held out sentences. Surprisingly high bracketing
accuracy is achieved with only 1042 sentences as train-
ing material: 94.4% for test sentences shorter than 10
words and 90.2% for sentences shorter than 15 words.
Furthermore, the bracketing accuracy does not drop
drastically as longer sentences are considered. These
results are surprising since the training uses part-of-
speech tags as the only source of lexical information.
This raises questions about the statistical distribution of
sentence structures observed in naturally occurring text.
After having described the training material used, we
report experiments using several subsets of the available
training material and evaluate the effect of the training
size on the bracketing performance. Then, we describe a
method for reducing the number of parameters in the
inferred grammars. Finally, we suggest a stochastic
model for inferring labels on the produced binary
branching trees.
</bodyText>
<sectionHeader confidence="0.961782" genericHeader="introduction">
2 Training Corpus
</sectionHeader>
<bodyText confidence="0.996324217391304">
The experiments use texts from the Wall Street Journal
Corpus and its partially bracketed version provided by
the Penn Treekuik (Brill et al., 1990). Out of 38 600
bracketed sentences (914 000 words), we extracted
34500 sentences (817 000 words) as possible source of
training material and 4100 sentences (97 000 words) as
source for testing. We experimented with several subsets
(350, 1095, 8000 and 34500 sentences) of the available
training material.
For practical purposes, the part of the tree bank used
for training is preprocessed before being used. First, flat
portions of parse trees found in the tree bank are turned
into a right linear binary branching structure. This
enables us to take full advantage of the fact that the
extended inside-outside algorithm (as described in
Pereira and Schabes, 1992) behaves in linear time when
the text is fully bracketed. Then, the syntactic labels are
ignored. This allows the reestimation algorithm to dis-
tribute its own set of labels based on their actual distri-
bution. We later suggest a method for recovering these
labels.
The following is an example of a partially parsed sen-
tence found in the Penn Treebank:
</bodyText>
<figure confidence="0.9557565625">
NP
DT NN PP
II
No price IN NP
for DT 11 NNS
de new shares
The above parse corresponds to the fully bracketed
unlabeled parse
•
VBZ
DT
No NN has VBN •
price IN been VBN
for DT set
the JJ NNS
new shares
</figure>
<bodyText confidence="0.984260833333333">
found in the training corpus. The experiments reported
in this paper use only the part-of-speech sequences of
this corpus and the resulting fully bracketed parses. For
the above example, the following bracketing is used in
the training material:
(DT (NN (IN (DT (JJ NNS)))) (VBZ (VBN VBN)))
</bodyText>
<sectionHeader confidence="0.992148" genericHeader="method">
3 Inferring Bracketings
</sectionHeader>
<bodyText confidence="0.9843835">
For the set of experiments described in this section,
the initial grammar consists of all 4095 possible Chom-
</bodyText>
<figure confidence="0.77801375">
VBZ VP
has VBN VP
been VBN
set
</figure>
<page confidence="0.986876">
342
</page>
<bodyText confidence="0.967401736842105">
sky Normal Form rules over 15 nonterminals
(Xi, I &lt;i &lt; 15) and 48 terminal symbols (tin, 1 &lt;m &lt;48)
for part-of-speech tags (the same set as the one used in
the Penn Treebank):
Xi XiXk
The parameters of the initial stochastic context-free
grammar are set randomly while maintaining the proper
conditions for stochastic context-free grammars.1
Using the algorithm described in Pereira and Schabes
(1992), the current rule probabilities and the parsed
training set C are used to estimate the expected frequen-
cies of each rule. Once these frequencies are computed
over each bracketed sentence c in the training set, new
rule probabilities are assigned in a way that increases the
estimated probability of the bracketed training set. This
process is iterated until the increase in the estimated
probability of the bracketed training text becomes negli-
gible, or equivalently, until the decrease in cross entropy
(negative log probability)
</bodyText>
<equation confidence="0.9948555">
logP (c)
(C,G) = `Ec
E d
CE C
</equation>
<bodyText confidence="0.9920565">
becomes negligible. In the above formula, the probabil-
ity P(c) of the partially bracketed sentence c is computed
as the sum of the probabilities of all derivations compat-
ible with the bracketing of the sentence. This notion of
compatible bracketing is defined in details in Pereira and
Schabes (1992). Informally speaking, a derivation is
compatible with the bracketing of the input given in the
tree bank, if no bracket imposed by the derivation
crosses a bracket in the input.
Compatible bracket
Input bracketing
Incompatible bracket
Input bracketing
As training material, we selected randomly out of the
available training material 1042 sentences of length
shorter than 15 words. For evaluation purposes, we also
1. The sum of the probabilities of the rules with same left hand
side must be one.
randomly selected 84 sentences of length shorter than 15
words among the test sentences.
Figure 1 shows the cross entropy of the training after
each iteration. It also shows for each iteration the cross
entropies fi of 84 sentences randomly selected among
the test sentences of length shorter than 15 words. The
cross entropy decreases as more iterations are performed
and no over training is observed..
</bodyText>
<figure confidence="0.988679083333333">
8.5
8 Training set H
Test set. H
7.5
7
6.5
6
5.5
4.5
4
3.5
0 20
</figure>
<figureCaption confidence="0.999248">
Figure 1. Training and Test Set -log prob
</figureCaption>
<figure confidence="0.9769915">
20 40 60 80 100
iteration
</figure>
<figureCaption confidence="0.999857">
Figure 2. Bracketing and sentence accuracy of 84
</figureCaption>
<bodyText confidence="0.997740285714286">
test sentences shorter than 15 words.
To evaluate the quality of the analyses yielded by the
inferred grammars obtained after each iteration, we used
a Viterbi-style parser to find the most likely analyses of
sentences in several test samples, and compared them
with the Treebank partial bracketings of the sentences of
those samples. For each sample, we counted the percent-
</bodyText>
<figure confidence="0.999244578947368">
Bracketing Accuracy ---
Sentence Accuracy ---
A
\r,
40 60 80 100
iteration
cIP
100
90
80
70
60
50
40
30
20
10
0
0
</figure>
<page confidence="0.996043">
343
</page>
<bodyText confidence="0.975087333333333">
age of brackets of the most likely analysis that are not
&amp;quot;crossing&amp;quot; the partial bracketing of the same sentences
found in the Treebank. This percentage is called the
bracketing accuracy (see Pereira and Schabes, 1992 for
the precise definition of this measure). We also com-
puted the percentage of sentences in each sample in
which no crossing bracket was found. This percentage is
called the sentence accuracy.
Figure 2 shows the bracketing and sentence accuracy
for the same 84 test sentences.
Table 1 shows the bracketing and sentence accuracy
for test sentences within various length ranges. High
bracketing accuracy is obtained even on relatively long
sentences. However, as expected, the sentence accuracy
decreases rapidly as the sentences get longer.
</bodyText>
<table confidence="0.9996444">
Length 0-10 0-15 10-19 20-30
Bracketing 94.4% 90.2% 82.5% 71.5%
Accuracy
Sentence 82% 57.1% 30% 6.8%
Accuracy
</table>
<tableCaption confidence="0.808919181818182">
TABLE 1. Bracketing Accuracy on test sentences of
different lengths (using 1042 sentences of
lengths shorter than 15 words as training
material).
Table 2 compares our results with the bracketing accu-
racy of analyses obtained by a systematic right linear
branching structure for all words except for the final
punctuation mark (which we attached high).2 We also
evaluated the stochastic context-free grammar obtained
by collecting each level of the trees found in the training
tree bank (see Table 2).
</tableCaption>
<table confidence="0.99945275">
Length . 0-10 0-15 10-19 20-30
Inferred grammar 94.4% 90.2% 82.5% 71.5%
Right linear trees 76% 70% 63% 50%
Treebank Grammar 46% 31% 25%
</table>
<tableCaption confidence="0.98773">
TABLE 2. Bracketing accuracy of the inferred
</tableCaption>
<bodyText confidence="0.9881125">
grammar, of right linear structures and of
the Treebank grammar.
Right linear structures perform surprisingly well. Our
results improve by 20 percentage points upon this base
line performance. These results suggest that the distribu-
tion of sentence structure in naturally occurring text is
simpler than one may have thought, especially since
only part-of-speech tags were used. This may suggest
</bodyText>
<footnote confidence="0.7968195">
2. We thank Eric Brill and David Yarowsky for suggesting
these experiments.
</footnote>
<bodyText confidence="0.991493666666667">
the existence of clusters of trees in the training material.
However, using the number of crossing brackets as a dis-
tance between trees, we have been unable to reveal the
existence of clusters.
The grammar obtained by collecting rules from the
tree bank performs very poorly. One can conclude that
the labels used in the tree bank do not have any statisti-
cal property. The task of inferring a stochastic grammar
from a tree bank is not trivial and therefore requires sta-
tistical training.
In the appendix we give examples of the most likely
analyses output by the inferred grammar on several test
sentences
In Table 3, different subsets of the available training
sentences of lengths up to 15 words long and the gram-
mars were evaluated on the same set of test sentences of
lengths shorter than 15 words. The size of the training
set does not seem to affect the performance of the parser.
</bodyText>
<table confidence="0.998980833333333">
Training Size 350 1095 8000
(sentences)
Bracketing 89.37% 90.22% 89.86%
Accuracy
Sentence 52.38% 57.14% 55.95%
Accuracy
</table>
<tableCaption confidence="0.953285">
TABLE 3. Effect of the size of the training set on the
bracketing and sentence accuracy.
</tableCaption>
<bodyText confidence="0.993829285714286">
However if one includes all available sentences
(34700 sentences), for the same test set, the bracketing
accuracy drops to 84% and the sentence accuracy to
40%.
We have also experimented with the following initial
grammar which defines a large number of rules
(110640):
</bodyText>
<equation confidence="0.988496">
Xi X X
j k
ti
</equation>
<bodyText confidence="0.9984788">
In this grammar, each non-terminal symbol is uniquely
associated with a terminal symbol. We observed over-
training with this grammar and better statistical conver-
gence was obtained, however the performance of the
parser did not improve.
</bodyText>
<page confidence="0.998724">
344
</page>
<sectionHeader confidence="0.902538" genericHeader="method">
4 Reducing the Grammar Size and
</sectionHeader>
<subsectionHeader confidence="0.782894">
Smoothing Issues
</subsectionHeader>
<bodyText confidence="0.999921514285714">
As grammars are being inferred at each iteration, the
training algorithm was designed to guarantee that no
parameter was set below some small threshold. This
constraint is important for smoothing. It implies that no
rule ever disappears at a reestimation step.
However, once the final grammar is found, for practi-
cal purposes, one can reduce the number of parameters
being used. For example, the size of the grammar can be
reduced by eliminating the rules whose probabilities are
below some threshold or by keeping for each non-termi-
nal only the top rules rewriting it.
However, one runs into the risk of not being able to
parse sentences given as input. We used the following
smoothing heuristics.
Lexical rule smoothing. In the case no rule in the
grammar introduces a terminal symbol found in the input
string, we assigned a lexical rule (Xi tm) with very low
probability for all non-terminal symbols. This case will
not happen if the training is representative of the lexical
items.
Syntactic rule smoothing. When the sentence is not
recognized from the starting symbol, we considered all
possible non-terminal symbols as starting symbols and
considered as starting symbol the one that yields the
most likely analysis. Although this procedure may not
guarantee that all sentences will be recognized, we found
it is very useful in practice.
When none of the above procedures enable parsing of
the sentence, we used the entire set of parameters of the
inferred grammar (this was never the case on the test
sentences we considered).
For example, the grammar whose performance is
depicted in Table 2 defines 4095 parameters. However,
the same performance is achieved on these test sets by
using only 450 rules (the top 20 binary branching rules
</bodyText>
<equation confidence="0.9299765">
X X X for each non-terminal symbol and the top 10
k
</equation>
<bodyText confidence="0.821971">
lexical rules Xi tin for each non-terminal symbol),
</bodyText>
<sectionHeader confidence="0.98766" genericHeader="method">
5. Implementation
</sectionHeader>
<bodyText confidence="0.999738888888889">
Pereira and Schabes (1992) note that the training algo-
rithm behaves in linear time (with respect to the sentence
length) when the training material consists of fully
bracketed sentences. By taking advantage of this fact,
the experiments using a small number of initial rules and
a small subset of the available training materials do not
require a lot of computation time and can be performed
on a single workstation. However, the experiments using
larger initial grammars or using more material require
more computation.
The training algorithm can be parallelized by dividing
the training corpus into fixed size blocks of sentences
and by having multiple workstations processing each
one of them independently. When all blocks have been
computed, the counts are merged and the parameters are
reestimated. For this purpose, we used PVM (Beguelin
et al., 1991) as a mechanism for message passing across
workstations.
</bodyText>
<sectionHeader confidence="0.877026" genericHeader="method">
6. Stochastic Model of Labeling for
Binary Branching Trees
</sectionHeader>
<bodyText confidence="0.999985111111111">
The stochastic grammars inferred by the training pro-
cedures produce unlabeled parse trees. We are currently
evaluating the following stochastic model for labeling a
binary branching tree. In this approach, we make the
simplifying assumption that the label of a node only
depends on the labels of its children. Under this assump-
tion, the probability of labeling a tree is the product of
the probability of labeling each level in the tree. For
example, the probability of the following labeling:
</bodyText>
<equation confidence="0.79069225">
NP VP
DT NN VBZ NNS
is P(S NP VP) P(NP DT NN) P(VP VBZ
NNS)
</equation>
<bodyText confidence="0.9999797">
These probabilities can be estimated in a simple man-
ner given a tree bank. For example, the probability of
labeling a level as NP DT NN is estimated as the num-
ber of occurrences (in the tree bank) of NP = DT NN
divided by the number of occurrences of X =&gt; DT NN
where X ranges over every label.
Then the probability of a labeling can be computed
bottom-up from leaves to root. Using dynamic program-
ming on increasingly large subtrees, the labeling with
the highest probability can be computed.
</bodyText>
<page confidence="0.996899">
345
</page>
<bodyText confidence="0.9994265">
We are currently evaluating the effectiveness of this
method.
</bodyText>
<sectionHeader confidence="0.955016" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999970555555556">
The experiments described in this paper prove the
effectiveness of the inside-outside algorithm on a large
corpus, and also shed some light on the distribution of
sentence structures found in natural languages.
We reported grammar inference experiments using the
inside-outside algorithm on the parsed Wall Street Jour-
nal corpus. The experiments were made possible by
turning the partially parsed training corpus into a fully
bracketed corpus.
Considering the fact that part-of-speech tags were the
only source of lexical information actually used, surpris-
ingly high bracketing accuracy is achieved (90.2% on
sentences of length up to 15). We believe that even
higher results can be achieved by using a richer set of
part-of-speech tags. These results show that the use of
simple distributions of constituency structures can pro-
vide high accuracy performance for broad coverage nat-
ural language parsers.
</bodyText>
<sectionHeader confidence="0.997302" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.969281333333333">
We thank Eric Brill, Aravind Joshi, Mark Liberman,
Mitchel Marcus, Fernando Pereira, Stuart Shieber and
David Yarowsky for valuable discussions.
</bodyText>
<sectionHeader confidence="0.992075" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982144613636364">
Baker, J.K. 1979. Trainable grammars for speech recog-
nition. In Jared J. Wolf and Dennis H. Klatt, editors,
Speech communication papers presented at the 97th
Meeting of the Acoustical Society of America, MIT,
Cambridge, MA, June.
Adam Beguelin, Jack Dongarra, Al Geist, Robert
Manchek, Vaidy Sunderam. July 1991.&amp;quot;A Users&apos;
guide to PVM Parallel Virtual Machine&amp;quot;, Oak Ridge
National Lab, TM-11826.
E. Black, S. Abney, D. Flickenger, R. Grishman, P. Har-
rison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M.
Liberman, M. Marcus, S. Roukos, B. Santorini, and T.
Strzalkowski. 1991. A procedure for quantitatively
comparing the syntactic coverage of English gram-
mars. DARPA Speech and Natural Language Work-
shop, pages 306-311, Pacific Grove, California.
Morgan Kaufmann.
Ezra Black, John Lafferty, and Salim Roukos. 1992.
Development and Evaluation of a Broad-Coverage
Probabilistic Grammar of English-Language Com-
puter Manuals. In 20th Meeting of the Association for
Computational Linguistics (ACL&apos; 92), Newark, Dela-
ware.
Eric Brill, David Magertnan, Mitchell Marcus, and Beat-
rice Santorini. 1990. Deducing linguistic structure
from the statistics of large corpora. In DARPA Speech
and Natural Language Workshop. Morgan Kaufmann,
Hidden Valley, Pennsylvania, June.
Ted Briscoe and Nick Waegner. July 1992. Robust Sto-
chastic Parsing Using the Inside-Outside Algorithm.
In AAA! workshop on Statistically-based Techniques
in Natural Language Processing.
T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and T. Nish-
ino. 1989. A probabilistic parsing method for sentence
disambiguation. Proceedings of the International
Workshop on Parsing Technologies, Pittsburgh,
August.
K. Lan i and S.J. Young. 1990. The estimation of stochas-
tic context-free grammars using the Inside-Outside
algorithm. Computer Speech and Language, 4:35-56.
Pereira, Fernando and Yves Schabes. 1992. Inside-out-
side reestimation from partially bracketed corpora. In
201h Meeting of the Association for Computational
Linguistics (ACL&apos; 92), Newark, Delaware.
</reference>
<page confidence="0.999401">
346
</page>
<sectionHeader confidence="0.721085" genericHeader="references">
Appendix Examples of parses
</sectionHeader>
<bodyText confidence="0.99579725">
The following parsed sentences are the most likely analyses output by the grammar inferred from 1042 training sen-
tences (at iteration 68) for some randomly selected sentences of length not exceeding 10 words. Each parse is pre-
ceded by the bracketing given in the Treebank. Sentences output by the parser are printed in bold face and crossing
brackets are marked with an asterisk (*).
</bodyText>
<equation confidence="0.986194">
(((The/DT Celtona/NP operations/NNS) would/MD (become/VB (part/NN (of/IN (those/DT ventures/NNS))))) .1.)
(((The/DT (Celtona/NP operations/NNS)) (would/MD (become/VB (part/NN (of/IN (those/DT ventures/
NNS)))))))
((But/CC then/RB they/PP (wake/VBP up/IN (to/TO (a/DT nightmare/NN)))) J.)
((But/CC (then/RB (they/PP (wake/VBP (up/IN (to/TO (a/DT nightmare/NN))))))) J.)
(((Mr./NP Strieber/NP) (knows/VBZ (a/DT lot/NN (about/IN aliens/NNS)))) J.)
(((Mr./NP Strieber/NP) (knows/VBZ ((a/DT lot/NN) (about/IN aliens/NNS)))) J.)
(((The/DT companies/NNS) (are/VBP (automotive-emissions-testing/JJ concerns/NNS))) J.)
(((The/DT companies/NNS) (are/VBP (automotive-emissions-testing/JJ concerns/NNS))) ./.)
(((Chief/JJ executives/NNS and/CC presidents/NNS) had/VBD (come/VBN and/CC gone/VBN) ./.))
(((Chief/JJ (executives/NNS (and/CC presidents/NNS))) (had/VBD (come/VBN (and/CC gone/VBN)))) ./.)
(((How/WRB quickly/RB) (things/NNS change/VBP) J.))
((How/WRB (* quickly/RB (things/NNS change/VBP) *)) J.)
((This/DT (means/VBZ ((the/DT returns/NNS) can/MD (vary/VB (a/DT great/JJ deal/NN)))))
((This/DT (means/VBZ ((the/DT returns/NNS) (can/MD (vary/VB (a/DT (great/JJ deal/NN))))))) J.)
(((Flight/NN Attendants/NNS) (Lag/NN (Before/1N (Jets/NNS Even/RB Land/VB?)))))
((* Flight/NN (* Attendants/NNS (* Lag/NN (* Before/IN Jets/NNS *) *) *) *) (Even/RB Land/VBP))
((They/PP (talked/VBD (of/IN (the/DT home/NN run/NN)))) .1.)
((They/PP (talked/VBD (of/IN (the/DT (home/NN run/NN))))) J.)
(((The/DT entire/JJ division/NN) (employs/VBZ (about/IN 850/CD workers/NNS))) .1.)
(((The/DT (entire/JJ division/NN)) (employs/VBZ (about/IN (850/CD workers/NNS)))) J.)
(((At/IN least/JJS) (before/IN (8/CD p.m/RB)) J.))
(((At/IN least/JS) (before/IN (8/CD p.m/RB))) J.)
((Pretend/VB (Nothing/NN Happened/VBD)))
((* Pretend/VB Nothing/NN *) Happened/VBD)
(((The/DT highlight/NN) :/: (a/DT &amp;quot;1&apos; fragrance/NN control/NN system/NN J. &amp;quot;I&amp;quot;)))
((* (The/DT highlight/NN) (* :/: (a/DT (C&apos;/— fragrance/NN) (control/NN system/NN)))*)*) (./. &amp;quot;/&amp;quot;))
(((Stock/NP prices/NNS) (slipped/VBD lower/JJR (in/IN (moderate/JJ trading,/NN))) J.))
(((Stock/NP prices/NNS) (slipped/VBD (lower/JJR (in/IN (moderate/JJ trading/NN))))) J.)
(((Some/DT jewelers/NNS) (have/VBP (Geiger/NP counters/NNS) (to/TO (measure/VB (topaz/NN radiation/NN))))
.1.))
(((Some/DT jewelers/NNS) (have/VBP ((Geiger/NP counters/NNS) (to/TO (measure/VB (topaz/NN radiation/
NN)))))) J.)
((That/DT cs/VBZ ( (the/DT only/JJ question/NN ) (we/PP (need/VBP (to/TO address/VB)))))) J.)
((That/DT Cs/VBZ ((the/DT (only/JJ question/NN)) (we/PP (need/VBP (to/TO address/VB)))))) ./.)
((She/PP (was/VBD (as/RB (cool/JJ (as/IN (a/DT cucumber/NN)))))) ./.)
((She/PP (was/VBD (as/RB (cool/JJ (as/IN (a/DT cucumber/NN)))))) J.)
(((The/DT index/NN) (gained/VBD (99.14/CD points/NNS) Monday/NP)) J.)
(((The/DT index/NN) (gained/VBD ((99.14/CD points/NNS) Monday/NP))) J.)
</equation>
<page confidence="0.994611">
347
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.802386">
<title confidence="0.998603">Parsing the Wall Street Journal with the Inside-Outside Algorithm</title>
<author confidence="0.999583">Yves Schabes Michal Roth Randy Osborne</author>
<affiliation confidence="0.998316">Mitsubishi Electric Research Laboratories</affiliation>
<address confidence="0.999671">Cambridge MA 02139 USA</address>
<email confidence="0.93427">(schabesirothiosborne@merl.com)</email>
<abstract confidence="0.991722789473684">We report grammar inference experiments on partially parsed sentences taken from the Wall Street Journal corpus using the inside-outside algorithm for stochastic context-free grammars. The initial grammar for the inference process makes no assumption of the kinds of structures and their distributions. The inferred grammar is evaluated by its predicting power and by comparing the bracketing of held out sentences imposed by the inferred grammar with the partial bracketings of these sentences given in the corpus. Using part-of-speech tags as the only source of lexical information, high bracketing accuracy is achieved even with a small subset of the available training material (1045 sentences): 94.4% for test sentences shorter than 10 words and 90.2% for sentences shorter than 15 words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition. In</title>
<date>1979</date>
<booktitle>Speech communication papers presented at the 97th Meeting of the Acoustical Society of America, MIT,</booktitle>
<editor>Jared J. Wolf and Dennis H. Klatt, editors,</editor>
<location>Cambridge, MA,</location>
<contexts>
<context position="1509" citStr="Baker (1979)" startWordPosition="216" endWordPosition="217">5 sentences): 94.4% for test sentences shorter than 10 words and 90.2% for sentences shorter than 15 words. 1 Introduction Most broad coverage natural Language parsers have been designed by incorporating hand-crafted rules. These rules are also very often further refined by statistical training. Furthermore, it is widely believed that high performance can only be achieved by disambiguating lexically sensitive phenomena such as prepositional attachment ambiguity, coordination or subcategorization. So far, grammar inference has not been shown to be effective for designing wide coverage parsers. Baker (1979) describes a training algorithm for stochastic context-free grammars (SCFG) which can be used for grammar reestimation (Fujisaki et al. 1989, Sharman et al. 1990, Black et al. 1992, Briscoe and Waegner 1992) or grammar inference from scratch (Lan i and Young 1990). However, the application of SCFGs and the original inside-outside algorithm for grammar inference has been inconclusive for two reasons. First, each iteration of the algorithm on a grammar with n nonterminals requires 0( n31w13 ) time per training sentence w. Second, the inferred grammar imposes bracketings which do not agree with l</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, J.K. 1979. Trainable grammars for speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presented at the 97th Meeting of the Acoustical Society of America, MIT, Cambridge, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Beguelin</author>
<author>Jack Dongarra</author>
<author>Al Geist</author>
<author>Robert Manchek</author>
</authors>
<title>Vaidy Sunderam. July 1991.&amp;quot;A Users&apos; guide to PVM Parallel Virtual Machine&amp;quot;, Oak Ridge National Lab,</title>
<date>1182</date>
<marker>Beguelin, Dongarra, Geist, Manchek, 1182</marker>
<rawString>Adam Beguelin, Jack Dongarra, Al Geist, Robert Manchek, Vaidy Sunderam. July 1991.&amp;quot;A Users&apos; guide to PVM Parallel Virtual Machine&amp;quot;, Oak Ridge National Lab, TM-11826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars. DARPA Speech and Natural Language Workshop,</title>
<date>1991</date>
<pages>306--311</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Pacific Grove, California.</location>
<marker>Black, Abney, Flickenger, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. DARPA Speech and Natural Language Workshop, pages 306-311, Pacific Grove, California. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>John Lafferty</author>
<author>Salim Roukos</author>
</authors>
<title>Development and Evaluation of a Broad-Coverage Probabilistic Grammar of English-Language Computer Manuals.</title>
<date>1992</date>
<booktitle>In 20th Meeting of the Association for Computational Linguistics (ACL&apos; 92),</booktitle>
<location>Newark, Delaware.</location>
<contexts>
<context position="1689" citStr="Black et al. 1992" startWordPosition="243" endWordPosition="246">en designed by incorporating hand-crafted rules. These rules are also very often further refined by statistical training. Furthermore, it is widely believed that high performance can only be achieved by disambiguating lexically sensitive phenomena such as prepositional attachment ambiguity, coordination or subcategorization. So far, grammar inference has not been shown to be effective for designing wide coverage parsers. Baker (1979) describes a training algorithm for stochastic context-free grammars (SCFG) which can be used for grammar reestimation (Fujisaki et al. 1989, Sharman et al. 1990, Black et al. 1992, Briscoe and Waegner 1992) or grammar inference from scratch (Lan i and Young 1990). However, the application of SCFGs and the original inside-outside algorithm for grammar inference has been inconclusive for two reasons. First, each iteration of the algorithm on a grammar with n nonterminals requires 0( n31w13 ) time per training sentence w. Second, the inferred grammar imposes bracketings which do not agree with linguistic judgments of sentence structure. Pereira and Schabes (1992) extended the inside-outside algorithm for inferring the parameters of a stochastic context-free grammar to tak</context>
</contexts>
<marker>Black, Lafferty, Roukos, 1992</marker>
<rawString>Ezra Black, John Lafferty, and Salim Roukos. 1992. Development and Evaluation of a Broad-Coverage Probabilistic Grammar of English-Language Computer Manuals. In 20th Meeting of the Association for Computational Linguistics (ACL&apos; 92), Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>David Magertnan</author>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
</authors>
<title>Deducing linguistic structure from the statistics of large corpora.</title>
<date>1990</date>
<booktitle>In DARPA Speech and Natural Language Workshop.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>Hidden Valley, Pennsylvania,</location>
<contexts>
<context position="5050" citStr="Brill et al., 1990" startWordPosition="762" endWordPosition="765">ntence structures observed in naturally occurring text. After having described the training material used, we report experiments using several subsets of the available training material and evaluate the effect of the training size on the bracketing performance. Then, we describe a method for reducing the number of parameters in the inferred grammars. Finally, we suggest a stochastic model for inferring labels on the produced binary branching trees. 2 Training Corpus The experiments use texts from the Wall Street Journal Corpus and its partially bracketed version provided by the Penn Treekuik (Brill et al., 1990). Out of 38 600 bracketed sentences (914 000 words), we extracted 34500 sentences (817 000 words) as possible source of training material and 4100 sentences (97 000 words) as source for testing. We experimented with several subsets (350, 1095, 8000 and 34500 sentences) of the available training material. For practical purposes, the part of the tree bank used for training is preprocessed before being used. First, flat portions of parse trees found in the tree bank are turned into a right linear binary branching structure. This enables us to take full advantage of the fact that the extended insi</context>
</contexts>
<marker>Brill, Magertnan, Marcus, Santorini, 1990</marker>
<rawString>Eric Brill, David Magertnan, Mitchell Marcus, and Beatrice Santorini. 1990. Deducing linguistic structure from the statistics of large corpora. In DARPA Speech and Natural Language Workshop. Morgan Kaufmann, Hidden Valley, Pennsylvania, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Nick Waegner</author>
</authors>
<title>Robust Stochastic Parsing Using the Inside-Outside Algorithm.</title>
<date>1992</date>
<booktitle>In AAA! workshop on Statistically-based Techniques in Natural Language Processing.</booktitle>
<contexts>
<context position="1716" citStr="Briscoe and Waegner 1992" startWordPosition="247" endWordPosition="251">rporating hand-crafted rules. These rules are also very often further refined by statistical training. Furthermore, it is widely believed that high performance can only be achieved by disambiguating lexically sensitive phenomena such as prepositional attachment ambiguity, coordination or subcategorization. So far, grammar inference has not been shown to be effective for designing wide coverage parsers. Baker (1979) describes a training algorithm for stochastic context-free grammars (SCFG) which can be used for grammar reestimation (Fujisaki et al. 1989, Sharman et al. 1990, Black et al. 1992, Briscoe and Waegner 1992) or grammar inference from scratch (Lan i and Young 1990). However, the application of SCFGs and the original inside-outside algorithm for grammar inference has been inconclusive for two reasons. First, each iteration of the algorithm on a grammar with n nonterminals requires 0( n31w13 ) time per training sentence w. Second, the inferred grammar imposes bracketings which do not agree with linguistic judgments of sentence structure. Pereira and Schabes (1992) extended the inside-outside algorithm for inferring the parameters of a stochastic context-free grammar to take advantage of constituent </context>
</contexts>
<marker>Briscoe, Waegner, 1992</marker>
<rawString>Ted Briscoe and Nick Waegner. July 1992. Robust Stochastic Parsing Using the Inside-Outside Algorithm. In AAA! workshop on Statistically-based Techniques in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujisaki</author>
<author>F Jelinek</author>
<author>J Cocke</author>
<author>E Black</author>
<author>T Nishino</author>
</authors>
<title>A probabilistic parsing method for sentence disambiguation.</title>
<date>1989</date>
<booktitle>Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh,</location>
<contexts>
<context position="1649" citStr="Fujisaki et al. 1989" startWordPosition="235" endWordPosition="238">d coverage natural Language parsers have been designed by incorporating hand-crafted rules. These rules are also very often further refined by statistical training. Furthermore, it is widely believed that high performance can only be achieved by disambiguating lexically sensitive phenomena such as prepositional attachment ambiguity, coordination or subcategorization. So far, grammar inference has not been shown to be effective for designing wide coverage parsers. Baker (1979) describes a training algorithm for stochastic context-free grammars (SCFG) which can be used for grammar reestimation (Fujisaki et al. 1989, Sharman et al. 1990, Black et al. 1992, Briscoe and Waegner 1992) or grammar inference from scratch (Lan i and Young 1990). However, the application of SCFGs and the original inside-outside algorithm for grammar inference has been inconclusive for two reasons. First, each iteration of the algorithm on a grammar with n nonterminals requires 0( n31w13 ) time per training sentence w. Second, the inferred grammar imposes bracketings which do not agree with linguistic judgments of sentence structure. Pereira and Schabes (1992) extended the inside-outside algorithm for inferring the parameters of </context>
</contexts>
<marker>Fujisaki, Jelinek, Cocke, Black, Nishino, 1989</marker>
<rawString>T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and T. Nishino. 1989. A probabilistic parsing method for sentence disambiguation. Proceedings of the International Workshop on Parsing Technologies, Pittsburgh, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan i</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the Inside-Outside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<contexts>
<context position="1773" citStr="i and Young 1990" startWordPosition="258" endWordPosition="261">ther refined by statistical training. Furthermore, it is widely believed that high performance can only be achieved by disambiguating lexically sensitive phenomena such as prepositional attachment ambiguity, coordination or subcategorization. So far, grammar inference has not been shown to be effective for designing wide coverage parsers. Baker (1979) describes a training algorithm for stochastic context-free grammars (SCFG) which can be used for grammar reestimation (Fujisaki et al. 1989, Sharman et al. 1990, Black et al. 1992, Briscoe and Waegner 1992) or grammar inference from scratch (Lan i and Young 1990). However, the application of SCFGs and the original inside-outside algorithm for grammar inference has been inconclusive for two reasons. First, each iteration of the algorithm on a grammar with n nonterminals requires 0( n31w13 ) time per training sentence w. Second, the inferred grammar imposes bracketings which do not agree with linguistic judgments of sentence structure. Pereira and Schabes (1992) extended the inside-outside algorithm for inferring the parameters of a stochastic context-free grammar to take advantage of constituent bracketing information in the training text. Although the</context>
</contexts>
<marker>i, Young, 1990</marker>
<rawString>K. Lan i and S.J. Young. 1990. The estimation of stochastic context-free grammars using the Inside-Outside algorithm. Computer Speech and Language, 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In 201h Meeting of the Association for Computational Linguistics (ACL&apos; 92),</booktitle>
<location>Newark, Delaware.</location>
<contexts>
<context position="2178" citStr="Pereira and Schabes (1992)" startWordPosition="322" endWordPosition="325">tic context-free grammars (SCFG) which can be used for grammar reestimation (Fujisaki et al. 1989, Sharman et al. 1990, Black et al. 1992, Briscoe and Waegner 1992) or grammar inference from scratch (Lan i and Young 1990). However, the application of SCFGs and the original inside-outside algorithm for grammar inference has been inconclusive for two reasons. First, each iteration of the algorithm on a grammar with n nonterminals requires 0( n31w13 ) time per training sentence w. Second, the inferred grammar imposes bracketings which do not agree with linguistic judgments of sentence structure. Pereira and Schabes (1992) extended the inside-outside algorithm for inferring the parameters of a stochastic context-free grammar to take advantage of constituent bracketing information in the training text. Although they report encouraging experiments (90% bracketing accuracy) on language transcriptions in the Texas Instrument subset of the Air Travel Information System (ATIS), the small size of the corpus (770 bracketed sentences containing a total of 7812 words), its linguistic simplicity, and the computation time required to train the grammar were reasons to believe that these results may not scale up to a larger </context>
<context position="3417" citStr="Pereira and Schabes, 1992" startWordPosition="512" endWordPosition="515">rse corpus. We report grammar inference experiments with this algorithm from the parsed Wall Street Journal corpus. 341 The experiments prove the feasibility and effectiveness of the inside-outside algorithm on a large corpus. Such experiments are made possible by assuming a right branching structure whenever the parsed corpus leaves portions of the parsed tree unspecified. This preprocessing of the corpus makes it fully bracketed. By taking advantage of this fact in the implementation of the inside-outside algorithm, its complexity becomes linear with respect to the input length (as noted by Pereira and Schabes, 1992) and therefore tractable for large corpora. We report experiments using several kinds of initial grammars and a variety of subsets of the corpus as training data. When the entire Wall Street Journal corpus was used as training material, the time required for training has been further reduced by using a parallel implementation of the inside-outside algorithm. The inferred grammar is evaluated by measuring the percentage of compatible brackets of the bracketing imposed by the inferred grammar with the partial bracketing of held out sentences. Surprisingly high bracketing accuracy is achieved wit</context>
<context position="5714" citStr="Pereira and Schabes, 1992" startWordPosition="869" endWordPosition="872">14 000 words), we extracted 34500 sentences (817 000 words) as possible source of training material and 4100 sentences (97 000 words) as source for testing. We experimented with several subsets (350, 1095, 8000 and 34500 sentences) of the available training material. For practical purposes, the part of the tree bank used for training is preprocessed before being used. First, flat portions of parse trees found in the tree bank are turned into a right linear binary branching structure. This enables us to take full advantage of the fact that the extended inside-outside algorithm (as described in Pereira and Schabes, 1992) behaves in linear time when the text is fully bracketed. Then, the syntactic labels are ignored. This allows the reestimation algorithm to distribute its own set of labels based on their actual distribution. We later suggest a method for recovering these labels. The following is an example of a partially parsed sentence found in the Penn Treebank: NP DT NN PP II No price IN NP for DT 11 NNS de new shares The above parse corresponds to the fully bracketed unlabeled parse • VBZ DT No NN has VBN • price IN been VBN for DT set the JJ NNS new shares found in the training corpus. The experiments re</context>
<context position="7125" citStr="Pereira and Schabes (1992)" startWordPosition="1117" endWordPosition="1120">ng material: (DT (NN (IN (DT (JJ NNS)))) (VBZ (VBN VBN))) 3 Inferring Bracketings For the set of experiments described in this section, the initial grammar consists of all 4095 possible ChomVBZ VP has VBN VP been VBN set 342 sky Normal Form rules over 15 nonterminals (Xi, I &lt;i &lt; 15) and 48 terminal symbols (tin, 1 &lt;m &lt;48) for part-of-speech tags (the same set as the one used in the Penn Treebank): Xi XiXk The parameters of the initial stochastic context-free grammar are set randomly while maintaining the proper conditions for stochastic context-free grammars.1 Using the algorithm described in Pereira and Schabes (1992), the current rule probabilities and the parsed training set C are used to estimate the expected frequencies of each rule. Once these frequencies are computed over each bracketed sentence c in the training set, new rule probabilities are assigned in a way that increases the estimated probability of the bracketed training set. This process is iterated until the increase in the estimated probability of the bracketed training text becomes negligible, or equivalently, until the decrease in cross entropy (negative log probability) logP (c) (C,G) = `Ec E d CE C becomes negligible. In the above formu</context>
<context position="9779" citStr="Pereira and Schabes, 1992" startWordPosition="1566" endWordPosition="1569">ed by the inferred grammars obtained after each iteration, we used a Viterbi-style parser to find the most likely analyses of sentences in several test samples, and compared them with the Treebank partial bracketings of the sentences of those samples. For each sample, we counted the percentBracketing Accuracy --- Sentence Accuracy --- A \r, 40 60 80 100 iteration cIP 100 90 80 70 60 50 40 30 20 10 0 0 343 age of brackets of the most likely analysis that are not &amp;quot;crossing&amp;quot; the partial bracketing of the same sentences found in the Treebank. This percentage is called the bracketing accuracy (see Pereira and Schabes, 1992 for the precise definition of this measure). We also computed the percentage of sentences in each sample in which no crossing bracket was found. This percentage is called the sentence accuracy. Figure 2 shows the bracketing and sentence accuracy for the same 84 test sentences. Table 1 shows the bracketing and sentence accuracy for test sentences within various length ranges. High bracketing accuracy is obtained even on relatively long sentences. However, as expected, the sentence accuracy decreases rapidly as the sentences get longer. Length 0-10 0-15 10-19 20-30 Bracketing 94.4% 90.2% 82.5% </context>
<context position="15147" citStr="Pereira and Schabes (1992)" startWordPosition="2444" endWordPosition="2447">at all sentences will be recognized, we found it is very useful in practice. When none of the above procedures enable parsing of the sentence, we used the entire set of parameters of the inferred grammar (this was never the case on the test sentences we considered). For example, the grammar whose performance is depicted in Table 2 defines 4095 parameters. However, the same performance is achieved on these test sets by using only 450 rules (the top 20 binary branching rules X X X for each non-terminal symbol and the top 10 k lexical rules Xi tin for each non-terminal symbol), 5. Implementation Pereira and Schabes (1992) note that the training algorithm behaves in linear time (with respect to the sentence length) when the training material consists of fully bracketed sentences. By taking advantage of this fact, the experiments using a small number of initial rules and a small subset of the available training materials do not require a lot of computation time and can be performed on a single workstation. However, the experiments using larger initial grammars or using more material require more computation. The training algorithm can be parallelized by dividing the training corpus into fixed size blocks of sent</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Pereira, Fernando and Yves Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In 201h Meeting of the Association for Computational Linguistics (ACL&apos; 92), Newark, Delaware.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>