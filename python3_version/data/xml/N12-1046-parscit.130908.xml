<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010506">
<title confidence="0.983281">
Encouraging Consistent Translation Choices
</title>
<author confidence="0.997457">
Ferhan Ture,&apos; Douglas W. Oard,2,4 Philip Resnik3,4
</author>
<affiliation confidence="0.9996486">
&apos;Department of Computer Science
2College of Information Studies
3Department of Linguistics
4Institute for Advanced Computer Studies
University of Maryland, College Park, MD 20740 USA
</affiliation>
<email confidence="0.988267">
fture@cs.umd.edu, oard@umd.edu, resnik@umd.edu
</email>
<sectionHeader confidence="0.995595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998850866666667">
It has long been observed that monolingual text
exhibits a tendency toward “one sense per dis-
course,” and it has been argued that a related
“one translation per discourse” constraint is op-
erative in bilingual contexts as well. In this pa-
per, we introduce a novel method using forced
decoding to confirm the validity of this con-
straint, and we demonstrate that it can be ex-
ploited in order to improve machine translation
quality. Three ways of incorporating such a
preference into a hierarchical phrase-based MT
model are proposed, and the approach where all
three are combined yields the greatest improve-
ments for both Arabic-English and Chinese-
English translation experiments.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962357142857">
In statistical Machine Translation (MT), the state-of-
the-art approach is to translate phrases in the context
of a sentence and to re-order those phrases appro-
priately. Intuitively, it seems as if it should also be
possible to draw on information outside of a single
sentence to further improve translation quality. In
this paper, we challenge the conventional approach
of translating each sentence independently, and ar-
gue that it can indeed also be beneficial to consider
document-scale context when translating text. Mo-
tivated by the success of a “one sense per discourse”
heuristic in Word Sense Disambiguation (WSD), we
explore the potential benefit of leveraging a “one
translation per discourse” heuristic in MT.
The paper is organized as follows. We begin with
related work in Section 2. Next, we provide new
confirmation that the hypothesized one-translation-
per-discourse condition does indeed often hold,
based on a novel analysis using forced decoding
(Section 3). We incorporate this idea into a hierarchi-
cal MT framework by adding three new document-
scale features to the translation model (Section 4).
We then present experimental results demonstrat-
ing solid improvements in translation quality ob-
tained by leveraging these features, both for Arabic-
English (Ar-En) and Chinese-English (Zh-En) trans-
lation (Section 5). Conclusions and future work are
presented in Section 6.
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9999731">
Exploiting discourse-level context has to date
received only limited attention in MT re-
search (e.g., (Gimenez and Marquez, 2007; Liu
et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et
al., 2011)). Exploratory analysis of reference trans-
lations by Carpuat (2009) motivates a hypothesis
that MT systems might benefit from the “one sense
per discourse” heuristic, first introduced by Gale et
al. (1992), which has proven to be effective in the
context of WSD (Yarowsky, 1995). Carpuat’s ap-
proach was to do post-processing on the translation
output to impose a “one translation per discourse”
constraint where the system would otherwise have
made a different choice. A manual evaluation on
a sample of sentences suggested promise from the
technique, which Carpuat suggested in favor of
exploring more integrated approaches.
Xiao et al. (2011) took this one step further and
implement an approach where they identified am-
biguous translations within each document, and at-
</bodyText>
<page confidence="0.652716">
417
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 417–426,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999963444444445">
tempt to fix them by replacing each ambiguity with
the most frequent translation choice. Based on their
error analysis, the authors indicate two shortcomings
when trying to find the correct translation of a given
phrase. First, frequency may not provide sufficient
information to distinguish between translation can-
didates, which is why we take rareness into account
when scoring translation candidates. Another prob-
lem is, like any other heuristic, that there may be
cases where the heuristic fails and there are multi-
ple senses per discourse. Guaranteeing consistency
hurts performance in such situations, which is why
we implement the heuristic as a model feature, and
let the model score decide for each case.
We are aware of a few other analyses that have
shown promising results based on a similar motiva-
tion. For instance, Wasser and Dorr (2008)’s ap-
proach biases the MT system based on term statistics
from relevant documents in comparable corpora. Ma
et al. (2011) show that a translation memory can be
used to find similar source sentences, and consecu-
tively adapt translation choices towards consistency.
Domain adaptation for MT has has also been shown
to be useful in some cases (Bertoldi and Federico,
2009; Hildebrand et al., 2005; Sanchis-Trilles and
Casacuberta, 2010; Tiedemann, 2010; Zhao et al.,
2004), so to the extent we consider documents to be
micro-domains we might expect similar approaches
to be useful at document scale. Indeed, hints that
such ideas may work have been available for some
time. For example, there is clear evidence that the
behavior of human translators can provide evidence
that is often useful for automating WSD (Diab and
Resnik, 2002; Ng et al., 2003). When coupled with
the one-sense-per-discourse heuristic, this suggests
that the reverse may also be true.
</bodyText>
<sectionHeader confidence="0.988262" genericHeader="method">
3 Exploratory analysis
</sectionHeader>
<bodyText confidence="0.999967272727273">
It is well known that writing styles vary by genre,
and in particular that the amount of vocabulary vari-
ation within a document depends to some extent on
the genre (e.g., higher in poetry than in engineering
writing). The degree to which authors tend to make
consistent word choices in any particular genre is,
therefore, an empirical question. In order to gain in-
sight into the extent to which human translators make
consistent vocabulary choices in the types of materi-
als that we wish to translate (in this work, news sto-
ries), we first explore the degree of support for our
one-translation-per-discourse hypothesis in the ref-
erence translations of a standard MT test collection.
We used the Ar-En MT08 data set, which con-
tains 74 newswire documents with a total of 813
sentences, each of which has four reference trans-
lations. Throughout this paper we consistently use
the document (i.e., one news story) as a conve-
nient discourse unit, although of course finer-scale or
broader-scale discourse units might also be explored
in future work. Moreover, throughout this paper we
use the hierarchical phrase-based translation system
(Hiero), which is based on a synchronous context-
free grammar (SCFG) model (Chiang, 2005). In a
text free expansion X —* α in the source language
can occur synchronously with X —* Q in the target
language. In this case, we call α the left hand side
(LHS) of the rule, and Q the right hand side (RHS)
of the rule.
To determine the extent and nature of translation
consistency choices made by human translators, we
randomly selected one of the four sets of reference
translations (first set, with id 0) and we used forced
decoding to find all possible sequences of rules that
could transform the source sentence into the target
sentence. In forced decoding, given a pair of source
and target sentences, and a grammar consisting of
learned translation rules with associated probabili-
ties, the decoder searches all possible derivations for
the one sequence of rules that is most likely (under
the learned translation model) to synchronously pro-
duce the source sentence on the LHS and the target
sentence on the RHS. For instance, consider the fol-
lowing Arabic sentence as input:
</bodyText>
<equation confidence="0.69794">
l 1� �� ul�l.ax�yl a;y1�11 �
</equation>
<footnote confidence="0.704991">
and its uncased reference translation:
there is a link between the three attacks .
</footnote>
<bodyText confidence="0.9996225">
The following four rules, which are part of the SCFG
learned from the the same translation pairs, allows
the decoder to find a sequence of derivations that
“translates” the source-side Arabic sentence into the
</bodyText>
<page confidence="0.995956">
418
</page>
<figureCaption confidence="0.8980676">
Figure 1: Illustration of forced decoding.
target-side reference translation.1
Figure 1 illustrates how the decoder uses these
rules to produce the source and target sides syn-
chronously.
</figureCaption>
<bodyText confidence="0.966580215189874">
As we repeated this procedure for all sentence
pairs, we kept track of all rules that were actually
used by the decoder to generate a reference English
translation from the corresponding Arabic sentences.
Our next step was to identify cases in which the
SCFG could reasonably have produced a substan-
tially different translation. Whenever an Arabic
phrase f occurs multiple times in a document, and f
appears on the LHS of two or more different gram-
mar rules in the SCFG, we count this as a single
“case”.2 These cases correspond to unique (source
phrase f, document d) pairs in which a translation
process using that SCFG could have chosen to pro-
duce two or more different translations of f in d.
Since the multiple appearances of f are distributed
among sentences of d, each counted case may cor-
respond to a number of sentences ranging from 1 to
the number of sentences in that document.
Table 1 shows a small sample of the cases (i.e.,
(source phrase f, document d) pairs) identified as a
result of forced decoding. There were 321 such cases
in our dataset and there were 672 sentences in which
at least one case occurred. This is not an uncommon
phenomenon; these 672 sentences comprise 83% of
1Since our goal was an exploratory analysis, the MT08 test
set was combined with the training set in order to ensure reach-
ability of the reference translations using the learned grammar.
Proper train/dev/test splits were, of course, used for the evalua-
tion results reported in Section 5.
2We define a phrase as any text that constitutes the entire
LHS of a grammar rule.
the test set. However, many of these cases repre-
sent either unlikely choices or inconsequential dif-
ferences, so some post-processing is called for.
Since grammar rules are typically more fine-
grained than is necessary for our purposes (e.g., to
capture various punctuation and determiner differ-
ences that do not affect the “sense” of the transla-
tion), we applied a few simple heuristics to edit the
source and target sides and group all such minor
variations into a single “mega-rule” (e.g., “how”�“,
how”, “third”_“a third”, “want”—“we want”). For
this, we removed nonterminal symbols and punc-
tuation, and considered two target phrases e and
e′ to be different only if edit distance(e, e′) &gt;
max(length(e), length(e′))/2, where the edit dis-
tance is based on character removal and insertion.
For instance, the third example in Table 1 would
have been considered to be translated consistently
as a result of this heuristic, as opposed to the first
example. We also eliminated cases in which no rea-
sonable alternatives were available in the translation
grammar (i.e., cases where the second most probable
rule with the same LHS was assigned a probability
below 0.1 in the grammar). Cases 4 and 5 would
have been removed by this heuristic.
After this filtering and aggregation we were left
with 176 (f, d) pairs in which the translation model
could reasonably have selected between rules that
would have produced substantially different English
translations of f in d (such as cases 1–3 and 6–9).
It was these 176 cases, affecting a total of 512 sen-
tences (63% of test set) for which we then examined
what forced decoding could tell us about translation
consistency.
So now that we know what the human who pro-
duced the reference translations actually did (accord-
ing to forced decoding), and in which cases they
might reasonably have chosen to do something sub-
stantially different (according to the SCFG), we can
ask in which cases the human (effectively) made a
consistent choice of translation rules when encoun-
tering the same Arabic phrase in the same document.
In 128 of the 176 cases, that is what they did (i.e.,
when the same phrase occurred multiple times in a
single document and more than one translation was
reasonably possible, forced decoding indicated that
the human translator translated that phrase in essen-
tially the same way). These cases affected the trans-
</bodyText>
<figure confidence="0.996776791666667">
X16
R1
R1
X16
�4
X7
X12 between X7
R3
is a link
R4
X3
R3
there
R2
R4
X3
a;xJl
ulsl..l:c�ll .
the
three
attacks .
X12
R2
14l�
</figure>
<page confidence="0.910812">
419
</page>
<table confidence="0.489934133333333">
Case Translation counts
Source phrase Doc #
لتقم 566 that killed = 1
killing of = 1
نئاهرلا 782 hostages = 2
نئاهرلا 138 hostage = 1
hostages = 2
ايروك 466 korea = 2
ايروك 763 korea = 2
نم 30 from = 2
نم 7 of = 1
from = 1
نم نهارلا 717 of the current = 2
يتلا 30 the = 1
which =1
</table>
<tableCaption confidence="0.98203">
Table 1: A sample of cases (i.e., (source phrase f, docu-
ment d) pairs) identified as a result of forced decoding.
</tableCaption>
<bodyText confidence="0.999678538461538">
lation of 455 sentences (56% of the test set), suggest-
ing that if we can replicate this human behavior in a
system, it might affect a nontrivial number of trans-
lation choices.
These statistics also suggest, however, that there
may be some risk incurred in such a process, since
in 48 of the 176 cases, the human translator opted
for a substantially different translation. When we
closely examined these 48 instances, we found that
19 (40%) involved changing a content-bearing word
(sometimes to a word with similar meaning). The re-
maining 29 (60%) involved function words or simi-
lar constructions. See Figures 2 and 3 for examples.
</bodyText>
<figureCaption confidence="0.944369">
Figure 2: Examples of differences in lexical choice for
content-bearing words within the same document.
</figureCaption>
<bodyText confidence="0.922497">
We can make several observations based on this
analysis. First, there does indeed seem to be ev-
idence to support the one-translation-per-discourse
heuristic, and to suggest that respecting that heuris-
</bodyText>
<figureCaption confidence="0.9966985">
Figure 3: Examples of differences in lexical choice for
other types of lexical units within the same document.
</figureCaption>
<bodyText confidence="0.999934125">
tic could improve translation outcomes for a substan-
tial number of sentences. Second, even when a ref-
erence translation contains different translations of
the same phrase, this may sometimes be the result of
stylistic choices rather than an intent by the transla-
tor to affect the expressed meaning. If a system were
try to “fix” such cases by enforcing consistent trans-
lation, the resulting translation might be somewhat
more stilted, but perhaps not less accurate or less in-
telligible. Finally, sentence structure conventions or
other language-specific phenomena may sometimes
require the same phrase to be translated differently,
so some way of encouraging consistency while still
allowing the model to consider other contextual fac-
tors might be better than always imposing a hard con-
sistency constraint.
</bodyText>
<sectionHeader confidence="0.995822" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.9999844">
To incorporate document-level features into an MT
system that would otherwise operate with only
sentence-level evidence, we added three super-
sentential “consistency features” to the translation
model. The decoder computes scores for these fea-
tures in two passes over each document; in each pass,
each sentence in the document is decoded. In the
first pass, the decoder keeps track of the number of
occurrences of some aspects of each grammar rule
and stores that information. The consistency fea-
tures are disabled during this pass, and do not affect
decoder scoring. In the second pass, each grammar
rule is assigned as many as three consistency feature
scores, each of which is based on some frozen counts
from the first pass. These features are designed to
introduce a bias towards translation consistency, but
to leave the final decision to the decoder, which of
course also has access to other features from the
translation and language model. At this point we are
more interested in effectiveness than efficiency, so
</bodyText>
<page confidence="0.98844">
420
</page>
<bodyText confidence="0.999692863636364">
we simply note that this approach doubles the run-
ning time of the decoder and that future work on a
more elegant implementation might be productive.
We explore three ways to compute features in this
section. The essential idea behind all of them is to
define some feature function that increases monoton-
ically with an increase in some count that we believe
to be informative, and in which the rate of increase is
damped more strongly as that count increases. Sev-
eral feature functions could satisfy those broad re-
quirements; in this section, we describe three vari-
ants, C1, C2 and C3, and discuss the potential bene-
fits and drawbacks of each.
C1: Counting rules In this variant, we count in-
stances of the same entire grammar rule, where a rule
r contains both the source phrase f and the target
phrase e. During the first pass, whenever a grammar
rule is chosen by the decoder for the one-best output,
the count for that rule is incremented. Given a gram-
mar rule r and the number of times r was counted in
the first pass (given by N{r}), the consistency fea-
ture score is computed as follows:
</bodyText>
<equation confidence="0.980308">
2.2N{r}
C1(r) = (1)
1.2 + N{r}
</equation>
<bodyText confidence="0.999652666666667">
Equation 1 is the term frequency component of the
well known Okapi BM25 term weighting function,
when parameters are set to the conventional values
k = 1.2, b = 0. This is an increasing and con-
cave function in which the count has a diminishing
marginal effect on the feature score. It has proven
to be useful in information retrieval applications, in
which the goal is to model “aboutness” based on term
counts (Robertson et al., 1994). Because our goal is
to demonstrate the potential of consistency features,
it seemed reasonable to work with some simple func-
tion that has a shape like the one we desired. We
leave exploration of optimal damping functions for
future work.
A drawback of this C1 approach is that as we saw
in Section 3, grammar rules in phrase-based MT sys-
tems tend to be somewhat more fine-grained than
seems optimal for constructing a consistency fea-
ture. For instance, consider the following rules that
all translate the same Arabic term:
Based on these grammar rules, we as human read-
ers infer that this Arabic phrase can be translated in
two different ways: as organs or as bodies. An opti-
mal application of the one-translation-per-discourse
heuristic would thus group the rules based on the
presence of one of those words. However, in the C1
variant, each of these rules would be counted sepa-
rately because of differences that in some cases do
not directly affect the choice of content words. For
instance, on the source side, the Arabic token ap-
pears to the right of the nonterminal symbol in R1,
R2 and R3, while it is to the left of the nonterminal in
R4 and R5. On the target side, differences are due to
both nonterminal symbol position and the existence
of determiners. Motivated by many examples like
this, we came up with an alternative way of count-
ing rules.
C2: Counting target tokens To partially address
this sparseness issue, variant C2 focuses only on the
target side. We extract all target tokens whenever a
grammar rule is used by the decoder in a one-best
derivation and increment a counter for each. Since
we are mainly interested in content words (e.g. bod-
ies, organs), we use simple pattern matching to dis-
card nonterminal symbols and punctuation, and we
ignore terms that appear in more than 50% of all doc-
uments (a convenient way of discarding common to-
kens such as the, or, and). This approach separates
the rules in the example above into two groups: rules
with bodies on the target side and rules with organs
on the target side. Upon completion of the first pass,
the consistency feature score for rule r is then de-
termined by first computing a score for each unique
target-side token w using:
</bodyText>
<equation confidence="0.999865">
2.2N{w} D + 1
bm25(w) = 1.2 + N{w} log DF(w) + 0.5 (2)
</equation>
<bodyText confidence="0.998895285714286">
where in this case N{w} maps tokens to their respec-
tive counts in the document, D is the total number
of documents in the collection, and DF (document
frequency) is the number of documents in which the
token occurs. This is a fuller version of the BM25
function in which (in the information retrieval ap-
plication) both high term frequencies and rare terms
</bodyText>
<page confidence="0.994909">
421
</page>
<bodyText confidence="0.992881166666667">
reduce sparsity effects. Similar to C2, the feature of
a rule r is defined by the maximum of scores of all
pairs extracted from r.
are rewarded. We then set the feature score for each
rule r to the maximum score of any of its target-side
terminal tokens:
</bodyText>
<equation confidence="0.9954045">
C2(r) = max
eERHS(r)
</equation>
<bodyText confidence="0.999896076923077">
Our motivation for choosing the maximum is that
when there is more than one content word that sur-
vives the pruning of common terms, we want the
score to be influenced most strongly by the most im-
portant of those terms. Since BM25 term weights
can be thought of as a measure of term importance,
taking the maximum is a simple expedient.
Although counting only target-side tokens yields
coarser granularity than counting rules, ignoring the
source side of the rule risks combining target side
statistics from translations of unrelated source lan-
guage terms. Consider the following grammar rule:
Since the counter for life and support will both be
incremented whenever rule R6 fires in the one-best
decoding during the first pass, problems could arise
if a rule with a different LHS that also contains sup-
port on the RHS were to fire in the same document,
for example:
If we don’t take the source side into account, both oc-
currences of support will be grouped together when
counting and R7 will receive extra score from the
consistency feature whenever R6 is used by the de-
coder. Of course, this problem will only arise when
the LHS of R6 and R7 are present in the same doc-
ument, and how often that happens (and thus how
large the risk from this factor is) is an empirical ques-
tion. We therefore developed a third alternative as a
middle ground between the fine-grained C1 and the
coarse-grained C2.
C3: Counting token translation pairs In this
variant, we count each terminal (source token, tar-
get token) pair that survives pruning. Specifically,
if grammar rule [X] f1f2...fm e1e2...en fires, we
increment the count of every pair (fi, ej), where fi
is aligned to ej. After the first pass, we compute the
feature value of each observed pair, based on this
count and the DF of the target-side of the pair. We
chose to use only the target token in the DF com-
putation (i.e., aggregating over all source tokens) to
</bodyText>
<equation confidence="0.9986135">
C3(r) = max
fELHS(r)
eERHS(r)
⟨f,e⟩ aligned
</equation>
<bodyText confidence="0.997901">
Since each variant has its benefits and drawbacks, we
can include all three in the system and let the tuning
process decide on how each should be weighted.
</bodyText>
<sectionHeader confidence="0.982668" genericHeader="evaluation">
5 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.999980194444444">
We have evaluated the one-translation-per-discourse
feature using the cdec MT system (Dyer et al., 2010).
We started by building a baseline system using stan-
dard features in cdec: lexical and phrase transla-
tion probabilities in both directions, word and arity
penalty features, and a 5-gram language model. We
then added each of the three consistency feature vari-
ants, along with all two-way and the one three-way
combinations of them, thus yielding a total of eight
systems for comparison, including the baseline.
For training the Ar-En system, we used the dataset
from the DARPA GALE evaluation (Olive et al.,
2011), which consists of NIST and LDC releases.
The corpus was filtered to remove sentence pairs
with anomalous length ratios and subsampled to
yield a training set containing 3.4 million parallel
sentence pairs. The Arabic text was preprocessed to
produce two different segmentations (simple punctu-
ation tokenization with orthographic normalization,
and LDC’s ATBv3 representation (Maamouri et al.,
2008)), represented together using cdec’s lattice in-
put format (Dyer et al., 2008).
The Zh-En system was trained on parallel train-
ing text consisting of the non-UN portions and non-
HK Hansards portions of the NIST training corpora.
Chinese was automatically segmented by the Stan-
ford segmenter (Tseng et al., 2005), and traditional
characters were simplified. After subsampling and
filtering, we obtain a training corpus of 1.6 million
parallel sentences.
Both training sets were word-aligned with
GIZA++ (Och and Ney, 2003), using 5 Model 1
and 5 HMM iterations. A SCFG was then ex-
tracted from these alignments using a suffix array
extractor (Chiang, 2007). Evaluation was done with
multi-reference BLEU (Papineni et al., 2002) on test
</bodyText>
<equation confidence="0.9981375">
bm25(e) (3)
bm25((f,e)) (4)
</equation>
<page confidence="0.992249">
422
</page>
<bodyText confidence="0.999450229166667">
sets with four references for each language pair, and
MIRA was used for tuning (Crammer et al., 2006).
In our experiments, we run the first decoding phase
using feature weights that are guessed heuristically
based on weights from previously tuned systems.
All feature weights, including the discourse feature,
were then tuned together, based on the output of
the second decoding phase. For Ar-En parame-
ter tuning, we used the MT06 newswire dataset,
which contains 104 documents and a total of 1,797
sentences. For testing, we used the MT08 dataset
described above (74 documents, 813 sentences).
For Zh-En experiments, the MT02 newswire dataset
(100 documents, 878 sentences) was used for tuning,
and evaluation was done on the MT06 test set (79
documents, 1,664 sentences). For both language
pairs, DF values were computed from the tuning
set for both tuning and evaluation experiments.
When we used NIST’s official metric (BLEU-4)
to compare our results to the official NIST evalu-
ation (NIST, 2006; NIST, 2008), our baseline sys-
tem achieved 54.70 for Ar-En and 31.69 for Zh-
En. Based on reported NIST results, our baseline
would have ranked 4th in the Zh-En MT06 evalua-
tion, and would have outperformed all Ar-En MT08
systems. We used a slightly different IBM-BLEU
metric for the rest of our evaluation. In this case,
the baseline system achieved 53.07 BLEU points
for Ar-En and 30.43 points for Zh-En. Among
more recent papers, the best reported results were
56.87 for Ar-En MT08 (Zhao et al., 2011a) and
35.87 for Zh-En MT06 (Zhao et al., 2011b), although
many papers report BLEU scores below 53 points
for Arabic (Carpuat et al., 2011) and 32 points for
Chinese (Monz, 2011). The systems that outper-
formed our baseline applied novel techniques, and
used larger language models, as well as many non-
standard features. We argue that these novelties are
complementary to our approach, and therefore do not
damage the credibility of our baseline.
Among the single-feature runs, C3 had the best
performance in Ar-En experiments, with 53.84
BLEU points, whereas C2 yielded the best results
for Zh-En with a BLEU score of 30.96. In any case,
all three variants outperformed the baseline (see Ta-
ble 2). When multiple features were combined, we
generally observed an increase in BLEU, suggesting
that our features have usefully different error char-
</bodyText>
<table confidence="0.9974188">
Method BLEU
Ar-En Zh-En
Baseline 53.07 30.43
C1 53.82 30.59
C2 53.70 30.96
C3 53.84 30.54
C12 53.82 30.79
C13 53.82 30.76
C23 53.88 30.63
C123 53.98 31.42
</table>
<tableCaption confidence="0.9876915">
Table 2: Evaluation results: BLEU scores with four ref-
erences for Ar-En and Zh-En experiments.
</tableCaption>
<table confidence="0.999804142857143">
Method # documents
Ar-En Zh-En
Docs 74 79
C1 37 30
C2 37 35
C3 42 36
C123 43 41
</table>
<tableCaption confidence="0.9974565">
Table 3: Doc-level analysis: Number of documents where
each variant outperforms baseline.
</tableCaption>
<bodyText confidence="0.998108652173913">
acteristics. The combination of all three variants,
C123, yielded the best results, nearly 1.0 BLEU point
higher than the baseline for both language pairs.
Evaluation results are summarized in Table 2.
Given our focus on documents, it is natural to
ask what fraction of the documents were helped
or harmed by consistency features. Document-
level BLEU scores for Arabic-to-English transla-
tions show that C3 outperformed the baseline on a
larger number of documents than any other single
feature (42/74=57%), compared with 37/74 (50%)
for both C1 and C2. C123 did better by this measure
as well, with BLEU increasing for 43 of the docu-
ments. There were no documents where the BLEU
score was exactly the same, therefore the BLEU
score declined for the remaining documents. As Ta-
ble 3 indicates, document-level BLEU for the Zh-En
experiments shows similar results.
We can also look at our results in a more fine-
grained way, focusing on differences in how each
system translated the same source-language phrase.
For this analysis, we defined English phrases e
and e′ to be different if edit distance(e, e′) &gt;
</bodyText>
<page confidence="0.9979">
423
</page>
<table confidence="0.995119">
Method Ar-En Zh-En
Cases Test set Cases Test set
C1 77 24% 401 48%
C2 127 35% 686 60%
C3 101 33% 491 53%
Any 197 68% 968 94%
C123 141 41% 651 59%
</table>
<tableCaption confidence="0.9700585">
Table 4: Effect of applying variants of the consistency
feature (Any=C1 or C2 or C3).
</tableCaption>
<bodyText confidence="0.999877602409639">
max(length(e), length(e′))/2. By this way of
counting, there are 197 unique (Arabic phrase, docu-
ment) pairs for which at least one single-feature sys-
tem produced translations differently from the base-
line system. Together, these cases affect 553 sen-
tences (68%) in 67 of the 74 documents, with as
many as 12 differences observed in a single doc-
ument. The number of such differences is even
higher for Chinese-to-English translation, probably
due to lower confidence from the translation model
and longer documents. Table 4 shows the number of
changes by each system, and the percentage of the
test set affected by these changes.
In order to gain greater insight into the effect of
the consistency features, we randomly sampled 60
of the 197 cases and analyzed the influence of the
change to the document BLEU score. In 25 of the
sampled cases, at least one of the three systems made
a change that improved the BLEU score, whereas the
score was adversely affected for at least one system
in 13 cases. BLEU remained unchanged in the re-
maining 22 cases, mostly due to the use of multi-
ple reference translations. When we analyze the ef-
fect of each system separately, we see that C2 was
the most aggressive, making 25 changes that influ-
enced BLEU (16 positive, 9 negative). C1 was the
most conservative, with only 13 such changes (8 pos-
itive, 5 negative). Consistent with the overall BLEU
scores, C3 evidenced the best ratio between benefit
and harm, making 20 changes that affected the score
(16 positive, 4 negative).
Looking at specific cases can yield some insight
into how the consistency features achieve improve-
ments. For example, results improved when trans-
lating the phrase �� ��a, (Eng. organizational,
regulatory), which appears in the context of organi-
zational groups that support terrorist ideology. The
baseline system translated this as organizational in
one case, and regulatory in another. Variants C1
and C2 changed this behavior, so that the translation
was organizational in both cases. One of the refer-
ence translations used organizational in one case and
dropped the phrase in the other, and the other three
translators provided consistent translations (using
organized and organizational). As a result, applying
the one-translation-per-discourse heuristic improved
the multi-reference BLEU score.
On the other hand, here is one of the cases where
our feature hurt performance. The phrase jffl防
MK (Eng. border/frontier troops/guards) appears
in two sentences of a Chinese news story about vio-
lence along the India - Nepal border. All reference
translations consistently used the word border in the
translation, as it is a better choice in this context.
The baseline system translated the phrase as fron-
tier guards and border troops in the two sentences.
All system variants replaced border with frontier to
maintain consistency, and therefore produced worse
translations, causing a decrease in BLEU score.
Examples can, however, also point up limitations
in our ability to measure improvements. In one of
the test documents, the Arabic phrase j
(Eng. sneak, infiltrate, enter without approval) ap-
pears in the context of Turkey trying to enter the Eu-
ropean Union. This was translated by the baseline
system as sneak into in one occurrence and infiltrate
into in another. C1 didn’t change the output, but
C2 and C3 translated the phrase as infiltrate into in
both cases. Although all of the four reference trans-
lators were consistent within their choices, each of
them chose different translations, namely worm its
way, enter, sneak and sneak into. This resulted in
a decrease in BLEU score for the two systems that
chose infiltrate into. This case illustrates a limita-
tion to fine-grained use of BLEU alone as a basis
for analysis, since we might argue that infiltrate into
is no less appropriate than sneak into in this con-
text. In other words, some of the reductions we see
in BLEU may not be actual errors but rather sim-
ply changes that take us outside of the coverage of
the test set. We did not find any cases in our sample
in which improvements in BLEU seemed to reward
changes that adversely affected meaning. From this,
</bodyText>
<page confidence="0.996892">
424
</page>
<bodyText confidence="0.9989466">
we conclude that BLEU is a somewhat conservative
measure when used in this way, and that the actual
overall improvement in translation quality over our
baseline may be somewhat more than our roughly
1.0 measured BLEU improvement would suggest.
</bodyText>
<sectionHeader confidence="0.998456" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999989625">
In this paper, we started with a new way of look-
ing at, and largely supporting, the “one translation
per discourse” hypothesis using forced decoding of
human reference translations. We then leveraged
insights from that analysis to design the transla-
tion model consistency features, obtaining solid im-
provements for both Ar-En and Zh-En translation.
In future work, we plan to explore additional vari-
ants. For example, we can further address sparsity by
incorporating monolingual paraphrase detection on
the source side, the target side or both. We can and
should explore other monotonically increasing con-
cave feature functions in addition to the Okapi BM25
function that we have found to be useful in this work,
we should explore alternatives to our use of the max-
imum function in C2 and C3, and we should con-
sider optimizing to measures other than BLEU (e.g.,
METEOR) that extend the range of rewarded lexical
choices by leveraging monolingual paraphrase evi-
dence.
In designing our features we were guided by our
intuition about which kinds of consistency should be
rewarded. Data can be superior to intuition, how-
ever, and our forced decoding technique might also
be helpful in generating new insights that could help
to guide the design of even more useful features. For
example, our forced decoding clearly points to cases
in which translators have chosen different structural
variants when translating the same phrase, and closer
examination of these cases might help us to automat-
ically detect which kinds of structural variation can
most profitably be moderated using a consistency
feature. We should also note that we have only done
forced decoding to date in one language pair (Ar-
En), and there might be more to be learned about
language-specific issues from doing the same anal-
ysis for additional language pairs.
Finally, the time seems propitious to reconsider
our choice of document-scale as our discourse con-
text. Documents have much to recommend them, but
much of the content that we might wish to translate
(conversational speech, text chat, email threads,...)
doesn’t present the kinds of obvious and unambigu-
ous document boundaries that we find in MT test
collections that are built from news stories. More-
over, some documents (e.g., textbooks) may be too
diverse for an entire document to be the right scale
for consistency. We might also be able to produc-
tively group similar documents into clusters in which
the vocabulary choices are (or should be) mutually
reinforcing.
We therefore end where we began, with many
questions to be answered. Now, however, we have
somewhat different questions – not whether to en-
courage consistency at a super-sentential scale, but
rather when and how best to do that.
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998585">
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA.
</bodyText>
<sectionHeader confidence="0.998458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998778590909091">
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation (StatMT
’09), pages 182–189.
Ralf D. Brown. 2008. Exploiting document-level context
for data-driven machine translation. In Proceedings of
the the Eighth Conference of the Association for Ma-
chine Translation in the Americas (AMTA ’08).
Marine Carpuat, Yuval Marton, and Nizar Habash. 2011.
Improved Arabic-to-English statistical machine trans-
lation by reordering post-verbal subjects for word
alignment. Machine Translation, pages 1–16.
Marine Carpuat. 2009. One translation per discourse. In
Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, DEW ’09,
pages 19–27.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL ’05.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201–228.
</reference>
<page confidence="0.987469">
425
</page>
<reference confidence="0.999691652631579">
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL ’02.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. In Pro-
ceedings of ACL-HLT’08, pages 1012–1020, June.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: a
decoder, alignment, and learning framework for finite-
state and context-free translation models. In ACLDe-
mos ’10, pages 7–12.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, HLT ’91, pages 233–237.
Jesus Gimenez and Llufs Marquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proceedings of StatMT ’07, pages
159–166.
AS Hildebrand, M Eck, S Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of The European Association for Machine
Translation (EAMT ’05).
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2010.
Improving statistical machine translation with mono-
lingual collocation. In ACL ’10.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrimina-
tive learning: a translation memory-inspired approach.
In Proceedings of ACL-HLT’11, pages 1239–1248.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhancing the Arabic Treebank: A Collaborative Ef-
fort toward New Annotation Guidelines. In LREC ’08.
Christof Monz. 2011. Statistical Machine Translation
with Local Language Models. In EMNLP ’11.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In ACL ’03.
NIST. 2006. http://www.itl.nist.gov/iad/mig/tests/mt/2006/.
NIST. 2008. http://www.itl.nist.gov/iad/mig/tests/mt/2008/.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–51.
Joseph Olive, Caitlin Christianson, and John McCary.
2011. Handbook of Natural Language Processing
and Machine Translation: DARPA Global Autonomous
Language Exploitation. Springer Publishing Com-
pany, Inc., 1st edition.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ’02.
Stephen E. Robertson, Steve Walker, Susan Jones, Miche-
line Hancock-Beaulieu, and Mike Gatford. 1994.
Okapi at TREC-3. In TREC.
German Sanchis-Trilles and Francisco Casacuberta.
2010. Bayesian adaptation for statistical machine
translation. In Proceedings of the workshop on Struc-
tural and Syntactic Pattern Recognition (SSPR ’10),
pages 620–629.
Jorg Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the workshop on
Domain Adaptation for Natural Language Processing
(DANLP ’10), pages 8–15.
Huihsin Tseng, Pi-Chuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Michael M. Wasser and Bonnie Dorr. 2008. Ma-
chine translation with cross-lingual information re-
trieval based document relevance scores. Unpublished.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In Machine Translation Summit XIII
(MTS’11).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL ’95.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Lan-
guage model adaptation for statistical machine transla-
tion with structured query models. In COLING ’04.
Bing Zhao, Young-Suk Lee, Xiaoqiang Luo, and Liu Li.
2011a. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In ACL-HLT ’11, pages 846–855.
Yinggong Zhao, Yangsheng Ji, Ning Xi, Shujian Huang,
and Jiajun Chen. 2011b. Language model weight
adaptation based on cross-entropy for statistical ma-
chine translation. In Pacific Asia Conference on Lan-
guage, Information and Computation (PACLIC ’11).
</reference>
<page confidence="0.999173">
426
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.133055">
<title confidence="0.999935">Encouraging Consistent Translation Choices</title>
<author confidence="0.982624">W Philip</author>
<affiliation confidence="0.5049305">of Computer of Information</affiliation>
<note confidence="0.343694333333333">of for Advanced Computer University of Maryland, College Park, MD 20740</note>
<abstract confidence="0.9931314375">It has long been observed that monolingual text exhibits a tendency toward “one sense per discourse,” and it has been argued that a related “one translation per discourse” constraint is operative in bilingual contexts as well. In this paper, we introduce a novel method using forced decoding to confirm the validity of this constraint, and we demonstrate that it can be exploited in order to improve machine translation quality. Three ways of incorporating such a preference into a hierarchical phrase-based MT model are proposed, and the approach where all three are combined yields the greatest improvements for both Arabic-English and Chinese- English translation experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>text chat, email threads,...) doesn’t present the kinds of obvious and unambiguous document boundaries that we find in MT test collections that are built from news stories. Moreover, some documents (e.g., textbooks) may be too diverse for an entire document to be the right scale for consistency. We might also be able to productively group similar documents into clusters in which the vocabulary choices are (or should be) mutually reinforcing.</title>
<marker></marker>
<rawString>(conversational speech, text chat, email threads,...) doesn’t present the kinds of obvious and unambiguous document boundaries that we find in MT test collections that are built from news stories. Moreover, some documents (e.g., textbooks) may be too diverse for an entire document to be the right scale for consistency. We might also be able to productively group similar documents into clusters in which the vocabulary choices are (or should be) mutually reinforcing.</rawString>
</citation>
<citation valid="false">
<title>We therefore end where we began, with many questions to be answered. Now, however, we have somewhat different questions – not whether to encourage consistency at a super-sentential scale, but rather when and how best to do that.</title>
<marker></marker>
<rawString>We therefore end where we began, with many questions to be answered. Now, however, we have somewhat different questions – not whether to encourage consistency at a super-sentential scale, but rather when and how best to do that.</rawString>
</citation>
<citation valid="false">
<title>Acknowledgements This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of DARPA.</title>
<marker></marker>
<rawString>Acknowledgements This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of DARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation (StatMT ’09),</booktitle>
<pages>182--189</pages>
<contexts>
<context position="4839" citStr="Bertoldi and Federico, 2009" startWordPosition="740" endWordPosition="743">ch situations, which is why we implement the heuristic as a model feature, and let the model score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to find similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 Explorator</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proceedings of the Fourth Workshop on Statistical Machine Translation (StatMT ’09), pages 182–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Exploiting document-level context for data-driven machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the the Eighth Conference of the Association for Machine Translation in the Americas (AMTA ’08).</booktitle>
<contexts>
<context position="2600" citStr="Brown, 2008" startWordPosition="392" endWordPosition="393">Section 3). We incorporate this idea into a hierarchical MT framework by adding three new documentscale features to the translation model (Section 4). We then present experimental results demonstrating solid improvements in translation quality obtained by leveraging these features, both for ArabicEnglish (Ar-En) and Chinese-English (Zh-En) translation (Section 5). Conclusions and future work are presented in Section 6. 2 Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gimenez and Marquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might benefit from the “one sense per discourse” heuristic, first introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in f</context>
</contexts>
<marker>Brown, 2008</marker>
<rawString>Ralf D. Brown. 2008. Exploiting document-level context for data-driven machine translation. In Proceedings of the the Eighth Conference of the Association for Machine Translation in the Americas (AMTA ’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
</authors>
<title>Improved Arabic-to-English statistical machine translation by reordering post-verbal subjects for word alignment. Machine Translation,</title>
<date>2011</date>
<pages>1--16</pages>
<contexts>
<context position="25532" citStr="Carpuat et al., 2011" startWordPosition="4258" endWordPosition="4261">baseline system achieved 54.70 for Ar-En and 31.69 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 points for Arabic (Carpuat et al., 2011) and 32 points for Chinese (Monz, 2011). The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features. We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline. Among the single-feature runs, C3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C2 yielded the best results for Zh-En with a BLEU score of 30.96. In any case, all three variants outperformed the baseline (see Table 2). When multiple features were combined, we </context>
</contexts>
<marker>Carpuat, Marton, Habash, 2011</marker>
<rawString>Marine Carpuat, Yuval Marton, and Nizar Habash. 2011. Improved Arabic-to-English statistical machine translation by reordering post-verbal subjects for word alignment. Machine Translation, pages 1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
</authors>
<title>One translation per discourse.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, DEW ’09,</booktitle>
<pages>pages</pages>
<contexts>
<context position="2587" citStr="Carpuat, 2009" startWordPosition="390" endWordPosition="391">rced decoding (Section 3). We incorporate this idea into a hierarchical MT framework by adding three new documentscale features to the translation model (Section 4). We then present experimental results demonstrating solid improvements in translation quality obtained by leveraging these features, both for ArabicEnglish (Ar-En) and Chinese-English (Zh-En) translation (Section 5). Conclusions and future work are presented in Section 6. 2 Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gimenez and Marquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might benefit from the “one sense per discourse” heuristic, first introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat s</context>
</contexts>
<marker>Carpuat, 2009</marker>
<rawString>Marine Carpuat. 2009. One translation per discourse. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, DEW ’09, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL ’05.</booktitle>
<contexts>
<context position="6669" citStr="Chiang, 2005" startWordPosition="1037" endWordPosition="1038"> hypothesis in the reference translations of a standard MT test collection. We used the Ar-En MT08 data set, which contains 74 newswire documents with a total of 813 sentences, each of which has four reference translations. Throughout this paper we consistently use the document (i.e., one news story) as a convenient discourse unit, although of course finer-scale or broader-scale discourse units might also be explored in future work. Moreover, throughout this paper we use the hierarchical phrase-based translation system (Hiero), which is based on a synchronous contextfree grammar (SCFG) model (Chiang, 2005). In a text free expansion X —* α in the source language can occur synchronously with X —* Q in the target language. In this case, we call α the left hand side (LHS) of the rule, and Q the right hand side (RHS) of the rule. To determine the extent and nature of translation consistency choices made by human translators, we randomly selected one of the four sets of reference translations (first set, with id 0) and we used forced decoding to find all possible sequences of rules that could transform the source sentence into the target sentence. In forced decoding, given a pair of source and target</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--201</pages>
<contexts>
<context position="23782" citStr="Chiang, 2007" startWordPosition="3972" endWordPosition="3973">attice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simplified. After subsampling and filtering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a suffix array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test bm25(e) (3) bm25((f,e)) (4) 422 sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the first decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a tota</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="23995" citStr="Crammer et al., 2006" startWordPosition="4005" endWordPosition="4008">atically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simplified. After subsampling and filtering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a suffix array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test bm25(e) (3) bm25((f,e)) (4) 422 sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the first decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a total of 1,797 sentences. For testing, we used the MT08 dataset described above (74 documents, 813 sentences). For Zh-En experiments, the MT02 newswire dataset (100 documents, 878 sentences) was used for tuning, and e</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL ’02.</booktitle>
<contexts>
<context position="5301" citStr="Diab and Resnik, 2002" startWordPosition="814" endWordPosition="817">utively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 Exploratory analysis It is well known that writing styles vary by genre, and in particular that the amount of vocabulary variation within a document depends to some extent on the genre (e.g., higher in poetry than in engineering writing). The degree to which authors tend to make consistent word choices in any particular genre is, therefore, an empirical question. In order to gain insight into the extent to which human translators make consistent vocabulary choices in </context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of ACL ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing Word Lattice Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT’08,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="23208" citStr="Dyer et al., 2008" startWordPosition="3878" endWordPosition="3881">or comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was filtered to remove sentence pairs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs. The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simplified. After subsampling and filtering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a suffix array extractor (Chiang, 2007). Evaluation was done with</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing Word Lattice Translation. In Proceedings of ACL-HLT’08, pages 1012–1020, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Jonathan Weese</author>
<author>Hendra Setiawan</author>
<author>Adam Lopez</author>
<author>Ferhan Ture</author>
<author>Vladimir Eidelman</author>
<author>Juri Ganitkevitch</author>
<author>Phil Blunsom</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: a decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACLDemos ’10,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="22218" citStr="Dyer et al., 2010" startWordPosition="3725" endWordPosition="3728"> pair (fi, ej), where fi is aligned to ej. After the first pass, we compute the feature value of each observed pair, based on this count and the DF of the target-side of the pair. We chose to use only the target token in the DF computation (i.e., aggregating over all source tokens) to C3(r) = max fELHS(r) eERHS(r) ⟨f,e⟩ aligned Since each variant has its benefits and drawbacks, we can include all three in the system and let the tuning process decide on how each should be weighted. 5 Evaluation and Discussion We have evaluated the one-translation-per-discourse feature using the cdec MT system (Dyer et al., 2010). We started by building a baseline system using standard features in cdec: lexical and phrase translation probabilities in both directions, word and arity penalty features, and a 5-gram language model. We then added each of the three consistency feature variants, along with all two-way and the one three-way combinations of them, thus yielding a total of eight systems for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was filtered to remove sentence p</context>
</contexts>
<marker>Dyer, Weese, Setiawan, Lopez, Ture, Eidelman, Ganitkevitch, Blunsom, Resnik, 2010</marker>
<rawString>Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitkevitch, Phil Blunsom, and Philip Resnik. 2010. cdec: a decoder, alignment, and learning framework for finitestate and context-free translation models. In ACLDemos ’10, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="2825" citStr="Gale et al. (1992)" startWordPosition="425" endWordPosition="428">ts in translation quality obtained by leveraging these features, both for ArabicEnglish (Ar-En) and Chinese-English (Zh-En) translation (Section 5). Conclusions and future work are presented in Section 6. 2 Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gimenez and Marquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might benefit from the “one sense per discourse” heuristic, first introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in favor of exploring more integrated approaches. Xiao et al. (2011) took this one step further and implement an approach where they identified ambiguous translations within each document, and at417 2012 Conference of the North A</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language, HLT ’91, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gimenez</author>
<author>Llufs Marquez</author>
</authors>
<title>Context-aware discriminative phrase selection for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of StatMT ’07,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="2554" citStr="Gimenez and Marquez, 2007" startWordPosition="382" endWordPosition="385">ften hold, based on a novel analysis using forced decoding (Section 3). We incorporate this idea into a hierarchical MT framework by adding three new documentscale features to the translation model (Section 4). We then present experimental results demonstrating solid improvements in translation quality obtained by leveraging these features, both for ArabicEnglish (Ar-En) and Chinese-English (Zh-En) translation (Section 5). Conclusions and future work are presented in Section 6. 2 Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gimenez and Marquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might benefit from the “one sense per discourse” heuristic, first introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise fr</context>
</contexts>
<marker>Gimenez, Marquez, 2007</marker>
<rawString>Jesus Gimenez and Llufs Marquez. 2007. Context-aware discriminative phrase selection for statistical machine translation. In Proceedings of StatMT ’07, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AS Hildebrand</author>
<author>M Eck</author>
<author>S Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of The European Association for Machine Translation (EAMT ’05).</booktitle>
<contexts>
<context position="4864" citStr="Hildebrand et al., 2005" startWordPosition="744" endWordPosition="747">e implement the heuristic as a model feature, and let the model score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to find similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 Exploratory analysis It is well kno</context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>AS Hildebrand, M Eck, S Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of The European Association for Machine Translation (EAMT ’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhanyi Liu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Sheng Li</author>
</authors>
<title>Improving statistical machine translation with monolingual collocation.</title>
<date>2010</date>
<booktitle>In ACL ’10.</booktitle>
<contexts>
<context position="2572" citStr="Liu et al., 2010" startWordPosition="386" endWordPosition="389"> analysis using forced decoding (Section 3). We incorporate this idea into a hierarchical MT framework by adding three new documentscale features to the translation model (Section 4). We then present experimental results demonstrating solid improvements in translation quality obtained by leveraging these features, both for ArabicEnglish (Ar-En) and Chinese-English (Zh-En) translation (Section 5). Conclusions and future work are presented in Section 6. 2 Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gimenez and Marquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might benefit from the “one sense per discourse” heuristic, first introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, </context>
</contexts>
<marker>Liu, Wang, Wu, Li, 2010</marker>
<rawString>Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2010. Improving statistical machine translation with monolingual collocation. In ACL ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Yifan He</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Consistent translation using discriminative learning: a translation memory-inspired approach.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT’11,</booktitle>
<pages>1239--1248</pages>
<marker>Ma, He, Way, van Genabith, 2011</marker>
<rawString>Yanjun Ma, Yifan He, Andy Way, and Josef van Genabith. 2011. Consistent translation using discriminative learning: a translation memory-inspired approach. In Proceedings of ACL-HLT’11, pages 1239–1248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Seth Kulick</author>
</authors>
<title>Enhancing the Arabic Treebank: A Collaborative Effort toward New Annotation Guidelines.</title>
<date>2008</date>
<booktitle>In LREC ’08.</booktitle>
<contexts>
<context position="23131" citStr="Maamouri et al., 2008" startWordPosition="3866" endWordPosition="3869"> the one three-way combinations of them, thus yielding a total of eight systems for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was filtered to remove sentence pairs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs. The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simplified. After subsampling and filtering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these align</context>
</contexts>
<marker>Maamouri, Bies, Kulick, 2008</marker>
<rawString>Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008. Enhancing the Arabic Treebank: A Collaborative Effort toward New Annotation Guidelines. In LREC ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
</authors>
<title>Statistical Machine Translation with Local Language Models.</title>
<date>2011</date>
<booktitle>In EMNLP ’11.</booktitle>
<contexts>
<context position="25571" citStr="Monz, 2011" startWordPosition="4267" endWordPosition="4268">9 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 points for Arabic (Carpuat et al., 2011) and 32 points for Chinese (Monz, 2011). The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features. We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline. Among the single-feature runs, C3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C2 yielded the best results for Zh-En with a BLEU score of 30.96. In any case, all three variants outperformed the baseline (see Table 2). When multiple features were combined, we generally observed an increase in BLEU,</context>
</contexts>
<marker>Monz, 2011</marker>
<rawString>Christof Monz. 2011. Statistical Machine Translation with Local Language Models. In EMNLP ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Bin Wang</author>
<author>Yee Seng Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: an empirical study.</title>
<date>2003</date>
<booktitle>In ACL ’03.</booktitle>
<contexts>
<context position="5319" citStr="Ng et al., 2003" startWordPosition="818" endWordPosition="821">on choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 Exploratory analysis It is well known that writing styles vary by genre, and in particular that the amount of vocabulary variation within a document depends to some extent on the genre (e.g., higher in poetry than in engineering writing). The degree to which authors tend to make consistent word choices in any particular genre is, therefore, an empirical question. In order to gain insight into the extent to which human translators make consistent vocabulary choices in the types of mater</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Exploiting parallel texts for word sense disambiguation: an empirical study. In ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<date>2006</date>
<contexts>
<context position="24892" citStr="NIST, 2006" startWordPosition="4149" endWordPosition="4150">r Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a total of 1,797 sentences. For testing, we used the MT08 dataset described above (74 documents, 813 sentences). For Zh-En experiments, the MT02 newswire dataset (100 documents, 878 sentences) was used for tuning, and evaluation was done on the MT06 test set (79 documents, 1,664 sentences). For both language pairs, DF values were computed from the tuning set for both tuning and evaluation experiments. When we used NIST’s official metric (BLEU-4) to compare our results to the official NIST evaluation (NIST, 2006; NIST, 2008), our baseline system achieved 54.70 for Ar-En and 31.69 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 </context>
</contexts>
<marker>NIST, 2006</marker>
<rawString>NIST. 2006. http://www.itl.nist.gov/iad/mig/tests/mt/2006/. NIST. 2008. http://www.itl.nist.gov/iad/mig/tests/mt/2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="23649" citStr="Och and Ney, 2003" startWordPosition="3946" endWordPosition="3949">tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simplified. After subsampling and filtering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a suffix array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test bm25(e) (3) bm25((f,e)) (4) 422 sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the first decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Olive</author>
<author>Caitlin Christianson</author>
<author>John McCary</author>
</authors>
<title>edition.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation.</booktitle>
<publisher>Springer Publishing Company, Inc.,</publisher>
<contexts>
<context position="22731" citStr="Olive et al., 2011" startWordPosition="3809" endWordPosition="3812">n We have evaluated the one-translation-per-discourse feature using the cdec MT system (Dyer et al., 2010). We started by building a baseline system using standard features in cdec: lexical and phrase translation probabilities in both directions, word and arity penalty features, and a 5-gram language model. We then added each of the three consistency feature variants, along with all two-way and the one three-way combinations of them, thus yielding a total of eight systems for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was filtered to remove sentence pairs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs. The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of t</context>
</contexts>
<marker>Olive, Christianson, McCary, 2011</marker>
<rawString>Joseph Olive, Caitlin Christianson, and John McCary. 2011. Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation. Springer Publishing Company, Inc., 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL ’02.</booktitle>
<contexts>
<context position="23853" citStr="Papineni et al., 2002" startWordPosition="3980" endWordPosition="3983">trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simplified. After subsampling and filtering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a suffix array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test bm25(e) (3) bm25((f,e)) (4) 422 sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the first decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a total of 1,797 sentences. For testing, we used the MT08 dataset described a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Susan Jones</author>
<author>Micheline Hancock-Beaulieu</author>
<author>Mike Gatford</author>
</authors>
<title>Okapi at TREC-3. In TREC.</title>
<date>1994</date>
<contexts>
<context position="17078" citStr="Robertson et al., 1994" startWordPosition="2819" endWordPosition="2822">ented. Given a grammar rule r and the number of times r was counted in the first pass (given by N{r}), the consistency feature score is computed as follows: 2.2N{r} C1(r) = (1) 1.2 + N{r} Equation 1 is the term frequency component of the well known Okapi BM25 term weighting function, when parameters are set to the conventional values k = 1.2, b = 0. This is an increasing and concave function in which the count has a diminishing marginal effect on the feature score. It has proven to be useful in information retrieval applications, in which the goal is to model “aboutness” based on term counts (Robertson et al., 1994). Because our goal is to demonstrate the potential of consistency features, it seemed reasonable to work with some simple function that has a shape like the one we desired. We leave exploration of optimal damping functions for future work. A drawback of this C1 approach is that as we saw in Section 3, grammar rules in phrase-based MT systems tend to be somewhat more fine-grained than seems optimal for constructing a consistency feature. For instance, consider the following rules that all translate the same Arabic term: Based on these grammar rules, we as human readers infer that this Arabic ph</context>
</contexts>
<marker>Robertson, Walker, Jones, Hancock-Beaulieu, Gatford, 1994</marker>
<rawString>Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>German Sanchis-Trilles</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Bayesian adaptation for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the workshop on Structural and Syntactic Pattern Recognition (SSPR ’10),</booktitle>
<pages>620--629</pages>
<contexts>
<context position="4903" citStr="Sanchis-Trilles and Casacuberta, 2010" startWordPosition="748" endWordPosition="751"> as a model feature, and let the model score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to find similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 Exploratory analysis It is well known that writing styles vary by genre, a</context>
</contexts>
<marker>Sanchis-Trilles, Casacuberta, 2010</marker>
<rawString>German Sanchis-Trilles and Francisco Casacuberta. 2010. Bayesian adaptation for statistical machine translation. In Proceedings of the workshop on Structural and Syntactic Pattern Recognition (SSPR ’10), pages 620–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorg Tiedemann</author>
</authors>
<title>Context adaptation in statistical machine translation using models with exponentially decaying cache.</title>
<date>2010</date>
<booktitle>In Proceedings of the workshop on Domain Adaptation for Natural Language Processing (DANLP ’10),</booktitle>
<pages>8--15</pages>
<contexts>
<context position="4920" citStr="Tiedemann, 2010" startWordPosition="752" endWordPosition="753">score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to find similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 Exploratory analysis It is well known that writing styles vary by genre, and in particular </context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>Jorg Tiedemann. 2010. Context adaptation in statistical machine translation using models with exponentially decaying cache. In Proceedings of the workshop on Domain Adaptation for Natural Language Processing (DANLP ’10), pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pi-Chuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<date>2005</date>
<note>A con-</note>
<contexts>
<context position="23439" citStr="Tseng et al., 2005" startWordPosition="3916" endWordPosition="3919">rs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs. The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simplified. After subsampling and filtering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a suffix array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test bm25(e) (3) bm25((f,e)) (4) 422 sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the first decod</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pi-Chuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A con-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>