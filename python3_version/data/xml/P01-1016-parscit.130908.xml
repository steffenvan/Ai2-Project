<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000106">
<title confidence="0.994064">
Non-Verbal Cues for Discourse Structure
</title>
<author confidence="0.927372">
Justine Cassell†, Yukiko I. Nakano†, Timothy W. Bickmore†,
Candace L. Sidner$, and Charles Rich$
</author>
<affiliation confidence="0.97081">
†MIT Media Laboratory $Mitsubishi Electric Research Laboratories
</affiliation>
<address confidence="0.990789">
20 Ames Street 201 Broadway
Cambridge, MA 02139 Cambridge, MA 02139
</address>
<email confidence="0.982183">
{justine, yukiko, bickmore}@media.mit.edu {sidner, rich}@merl.com
</email>
<sectionHeader confidence="0.995358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896421052632">
This paper addresses the issue of
designing embodied conversational
agents that exhibit appropriate posture
shifts during dialogues with human
users. Previous research has noted the
importance of hand gestures, eye gaze
and head nods in conversations
between embodied agents and humans.
We present an analysis of human
monologues and dialogues that
suggests that postural shifts can be
predicted as a function of discourse
state in monologues, and discourse and
conversation state in dialogues. On the
basis of these findings, we have
implemented an embodied
conversational agent that uses
Collagen in such a way as to generate
postural shifts.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9998209">
This paper provides empirical support for the
relationship between posture shifts and
discourse structure, and then derives an
algorithm for generating posture shifts in an
animated embodied conversational agent from
discourse states produced by the middleware
architecture known as Collagen [18]. Other
nonverbal behaviors have been shown to be
correlated with the underlying conversational
structure and information structure of discourse.
For example, gaze shifts towards the listener
correlate with a shift in conversational turn
(from the conversational participants’
perspective, they can be seen as a signal that the
floor is available). Gestures correlate with
rhematic content in accompanying language
(from the conversational participants’
perspective, these behaviors can be seen as a
signal that accompanying speech is of high
interest). A better understanding of the role of
nonverbal behaviors in conveying discourse
structures enables improvements in the
naturalness of embodied dialogue systems, such
as embodied conversational agents, as well as
contributing to algorithms for recognizing
discourse structure in speech-understanding
systems. Previous work, however, has not
addressed major body shifts during discourse,
nor has it addressed the nonverbal correlates of
topic shifts.
</bodyText>
<sectionHeader confidence="0.993914" genericHeader="introduction">
2. Background
</sectionHeader>
<bodyText confidence="0.9998547518797">
Only recently have computational linguists
begun to examine the association of nonverbal
behaviors and language. In this section we
review research by non-computational linguists
and discuss how this research has been
employed to formulate algorithms for natural
language generation or understanding.
About three-quarters of all clauses in descriptive
discourse are accompanied by gestures [17], and
within those clauses, the most effortful part of
gestures tends to co-occur with or just before the
phonologically most prominent syllable of the
accompanying speech [13]. It has been shown
that when speech is ambiguous or in a speech
situation with some noise, listeners rely on
gestural cues [22] (and, the higher the noise-to-
signal ratio, the more facilitation by gesture).
Even when gestural content overlaps with
speech (reported to be the case in roughly 50%
of utterances, for descriptive discourse), gesture
often emphasizes information that is also
focused pragmatically by mechanisms like
prosody in speech. In fact, the semantic and
pragmatic compatibility in the gesture-speech
relationship recalls the interaction of words and
graphics in multimodal presentations [11].
On the basis of results such as these, several
researchers have built animated embodied
conversational agents that ally synthesized
speech with animated hand gestures. For
example, Lester et al. [15] generate deictic
gestures and choose referring expressions as a
function of the potential ambiguity and
proximity of objects referred to. Rickel and
Johnson [19]&apos;s pedagogical agent produces a
deictic gesture at the beginning of explanations
about objects. André et al. [1] generate pointing
gestures as a sub-action of the rhetorical action
of labeling, in turn a sub-action of elaborating.
Cassell and Stone [3] generate either speech,
gesture, or a combination of the two, as a
function of the information structure status and
surprise value of the discourse entity.
Head and eye movement has also been examined
in the context of discourse and conversation.
Looking away from one’s interlocutor has been
correlated with the beginning of turns. From the
speaker’s point of view, this look away may
prevent an overload of visual and linguistic
information. On the other hand, during the
execution phase of an utterance, speakers look
more often at listeners. Head nods and eyebrow
raises are correlated with emphasized linguistic
items – such as words accompanied by pitch
accents [7]. Some eye movements occur
primarily at the ends of utterances and at
grammatical boundaries, and appear to function
as synchronization signals. That is, one may
request a response from a listener by looking at
the listener, and suppress the listener’s response
by looking away. Likewise, in order to offer the
floor, a speaker may gaze at the listener at the
end of the utterance. When the listener wants the
floor, s/he may look at and slightly up at the
speaker [10]. It should be noted that turn taking
only partially accounts for eye gaze behavior in
discourse. A better explanation for gaze
behavior integrates turn taking with the
information structure of the propositional
content of an utterance [5]. Specifically, the
beginning of themes are frequently accompanied
by a look-away from the hearer, and the
beginning of rhemes are frequently accompanied
by a look-toward the hearer. When these
categories are co-temporaneous with turn
construction, then they are strongly predictive of
gaze behavior.
Results such as these have led researchers to
generate eye gaze and head movements in
animated embodied conversational agents.
Takeuchi and Nagao, for example, [21] generate
gaze and head nod behaviors in a “talking head.”
Cassell et al. [2] generate eye gaze and head
nods as a function of turn taking behavior, head
turns just before an utterance, and eyebrow
raises as a function of emphasis.
To our knowledge, research on posture shifts
and other gross body movements, has not been
used in the design or implementation of
computational systems. In fact, although a
number of conversational analysts and
ethnomethodologists have described posture
shifts in conversation, their studies have been
qualitative in nature, and difficult to reformulate
as the basis of algorithms for the generation of
language and posture. Nevertheless, researchers
in the non-computational fields have discussed
posture shifts extensively. Kendon [13] reports
a hierarchy in the organization of movement
such that the smaller limbs such as the fingers
and hands engage in more frequent movements,
while the trunk and lower limbs change
relatively rarely.
A number of researchers have noted that
changes in physical distance during interaction
seem to accompany changes in the topic or in
the social relationship between speakers. For
example Condon and Osgton [9] have suggested
that in a speaking individual the changes in
these more slowly changing body parts occur at
the boundaries of the larger units in the flow of
speech. Scheflen (1973) also reports that
posture shifts and other general body
movements appear to mark the points of change
between one major unit of communicative
activity and another. Blom &amp; Gumperz (1972)
identify posture changes and changes in the
spatial relationship between two speakers as
indicators of what they term &amp;quot;situational shifts&amp;quot;
-- momentary changes in the mutual rights and
obligations between speakers accompanied by
shifts in language style. Erickson (1975)
concludes that proxemic shifts seem to be
markers of &apos;important&apos; segments. In his analysis
of college counseling interviews, they occurred
more frequently than any other coded indicator
of segment changes, and were therefore the best
predictor of new segments in the data.
Unfortunately, in none of these studies are
statistics provided, and their analyses rely on
intuitive definitions of discourse segment or
“major shift”. For this reason, we carried out
our own empirical study.
</bodyText>
<sectionHeader confidence="0.91561" genericHeader="method">
3. Empirical Study
</sectionHeader>
<bodyText confidence="0.99992295">
Videotaped “pseudo-monologues” and dialogues
were used as the basis for the current study. In
“pseudo-monologues,” subjects were asked to
describe each of the rooms in their home, then
give directions between four pairs of locations
they knew well (e.g., home and the grocery
store). The experimenter acted as a listener, only
providing backchannel feedback (head nods,
smiles and paraverbals such as &amp;quot;uh-huh&amp;quot;). For
dialogues, two subjects were asked to generate
an idea for a class project that they would both
like to work on, including: 1) what they would
work on; 2) where they would work on it
(including facilities, etc.), and 3) when they
would work on it. Subjects stood in both
conditions and were told to perform their tasks
in 5-10 minutes. The pseudo-monologue
condition (pseudo- because there was in fact an
interlocutor, although he gave backchannel
feedback only and never took the turn) allowed
us to investigate the relationship between
discourse structure and posture shift
independent of turn structure. The two tasks
were constructed to allow us to identify exactly
where discourse segment boundaries would be
placed.
The video data was transcribed and coded for
three features: discourse segment boundaries,
turn boundaries, and posture shifts. A discourse
segment is taken to be an aggregation of
utterances and sub-segments that convey the
discourse segment purpose, which is an
intention that leads to the segment initiation
[12]. In this study we chose initially to look at
high-level discourse segmentation phenomena
rather than those discourse segments embedded
deeper in the discourse. Thus, the time points at
which the assigned task topics were started
served as segmentation points. Turn boundaries
were coded (for dialogues only) as the point in
time in which the start or end of an utterance co-
occurred with a change in speaker, but excluding
backchannel feedback. Turn overlaps were
coded as open-floor time. We defined a posture
shift as a motion or a position shift for a part of
the human body, excluding hands and eyes
(which we have dealt with in other work).
Posture shifts were coded with start and end
time of occurrence (duration), body part in play
(for this paper we divided the body at the
waistline and compared upper body vs. lower
body shifts), and an estimated energy level of
the posture shift. Energy level was normalized
for each subject by taking the largest posture
shift observed for each subject as 100% and
coding all other posture shift energies relative to
the 100% case. Posture shifts that occurred as
part of gesture or were clearly intentionally
generated (e.g., turning one&apos;s body while giving
directions) were not coded.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="method">
4. Results
</sectionHeader>
<bodyText confidence="0.999941">
Data from seven monologues and five dialogues
were transcribed, and then coded and analyzed
independently by two raters. A total of 70.5
minutes of data was analyzed (42.5 minutes of
dialogue and 29.2 minutes of monologue). A
total of 67 discourse segments were identified
(25 in the dialogues and 42 in the monologues),
which constituted 407 turns in the dialogue data.
We used the instructions given to subjects
concerning the topics to discuss as segmentation
boundaries. In future research, we will address
the smaller discourse segmentation. For posture
shift coding, raters coded all posture shifts
independently, and then calculated reliability on
the transcripts of one monologue (5.2 minutes)
and both speakers from one dialogue (8.5
minutes). Agreement on the presence of an
upper body or lower body posture shift in a
particular location (taking location to be a 1-
second window that contains all of or a part of a
posture shift) for these three speakers was 89%
(kappa = .64). For interrater reliability of the
coding of energy level, a Spearman’s rho
revealed a correlation coefficient of .48 (p&lt;.01).
</bodyText>
<subsectionHeader confidence="0.998611">
4.1 Analysis
</subsectionHeader>
<bodyText confidence="0.999755947368421">
Posture shifts occurred regularly throughout the
data (an average of 15 per speaker in both
pseudo-monologues and dialogues). This,
together with the fact that the majority of time
was spent within discourse segments and within
turns (rather than between segments), led us to
normalize our posture shift data for comparison
purposes. For relatively brief intervals (inter-
discourse-segment and inter-turn) normalization
by number of inter-segment occurrences was
sufficient (ps/int), however, for long intervals
(intra-discourse segment and intra-turn) we
needed to normalize by time to obtain
meaningful comparisons. For this normalization
metric we looked at posture-shifts-per-second
(ps/s). This gave us a mean average of .06
posture shifts/second (ps/s) in the monologues
(SD=.07), and .07 posture shifts/second in the
dialogues (SD=.08).
</bodyText>
<tableCaption confidence="0.852957">
Table 4.1.1. Posture WRT Discourse Segments
</tableCaption>
<subsectionHeader confidence="0.993041">
Monologues Dialogues
</subsectionHeader>
<bodyText confidence="0.998406785714286">
ps/s ps/int energy ps/s ps/int energy
0.340 0.837 0.832 0.332 0.533 0.844
0.039 0.701 0.053 0.723
Our initial analysis compared posture shifts
made by the current speaker within discourse
segments (intra-dseg) to those produced at the
boundaries of discourse segments (inter-dseg). It
can be seen (in Table 4.1.1) that posture shifts
occur an order of magnitude more frequently at
discourse segment boundaries than within
discourse segments in both monologues and
dialogues. Posture shifts also tend to be more
energetic at discourse segment boundaries
(F(1,251)=10.4; p&lt;0.001).
</bodyText>
<subsectionHeader confidence="0.480815">
Table 4.1.2 Posture Shifts WRT Turns
</subsectionHeader>
<bodyText confidence="0.98728475">
ps/s ps/int energy
inter-turn 0.140 0.268 0.742
intra-turn 0.022 0.738
Initially, we classified data as being inter- or
intra-turn. Table 4.1.2 shows that turn structure
does have an influence on posture shifts;
subjects were five times more likely to exhibit a
shift at a boundary than within a turn.
</bodyText>
<tableCaption confidence="0.666015">
Table 4.1.3 Posture by Discourse and Turn Breakdown
</tableCaption>
<bodyText confidence="0.999568939393939">
inter-dseg/start-turn
inter-dseg/mid-turn
inter-dseg/end-turn
intra-dseg/start-turn
intra-dseg/mid-turn
intra-dseg/end-turn
An interaction exists between turns and
discourse segments such that discourse segment
boundaries are ten times more likely to co-occur
with turn changes than within turns. Both turn
and discourse structure exhibit an influence on
posture shifts, with discourse having the most
predictive value. Starting a turn while starting a
new discourse segment is marked with a posture
shift roughly 10 times more often than when
starting a turn while staying within discourse
segment. We noticed, however, that posture
shifts appeared to congregate at the beginnings
or ends of turn boundaries, and so our
subsequent analyses examined start-turns, mid-
turns and end-turns. It is clear from these results
that posture is indeed correlated with discourse
state, such that speakers generate a posture shift
when initiating a new discourse segment, which
is often at the boundary between turns.
In addition to looking at the occurrence and
energy of posture shifts we also analyzed the
distributions of upper vs. lower body shifts and
the duration of posture shifts. Speaker upper
body shifts were found to be used more
frequently at the start of turns (48%) than at the
middle of turns (36%) or end of turns (18%)
(F(2,147)=5.39; p&lt;0.005), with no significant
</bodyText>
<figure confidence="0.882039090909091">
inter-
dseg
intra-
dseg
ps/s ps/int
0.562 0.542
0.000 0.000
0.130 0.125
0.067 0.135
0.041
0.053 0.107
</figure>
<bodyText confidence="0.99905775">
dependence on discourse structure. Finally,
speaker posture shift duration was found to
change significantly as a function of both turn
and discourse structure (see Figure 4.1.3). At the
start of turns, posture shift duration is
approximately the same whether a new topic is
introduced or not (2.5 seconds). However, when
ending a turn, speakers move significantly
longer (7.0 seconds) when finishing a topic than
when the topic is continued by the other
interlocutor (2.7 seconds) (F(1,148)=17.9;
p&lt;0.001).
</bodyText>
<figureCaption confidence="0.64471">
Figure 4.1.1 Posture Shift Duration by DSeg and Turn
</figureCaption>
<sectionHeader confidence="0.990281" genericHeader="method">
5. System
</sectionHeader>
<bodyText confidence="0.9996535">
In the following sections we discuss how the
results of the empirical study were integrated
along with Collagen into our existent embodied
conversational agent, Rea.
</bodyText>
<subsectionHeader confidence="0.996158">
5.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.99991909375">
Rea is an embodied conversational agent that
interacts with a user in the real estate agent
domain [2]. The system architecture of Rea is
shown in Figure 5.1. Rea takes input from a
microphone and two cameras in order to sense
the user’s speech and gesture. The UM
interprets and integrates this multimodal input
and outputs a unified semantic representation.
The Understanding Module then sends the
output to Collagen as the Dialogue Manager.
Collagen, as further discussed below, maintains
the state of the dialogue as shared between Rea
and a user. The Reaction Module decides Rea’s
next action based on the discourse state
maintained by Collagen. It also assigns
information structure to output utterances so that
gestures can be appropriately generated. The
semantic representation of the action, including
verbal and non-verbal behaviors, is sent to the
Generation Module which generates surface
linguistic expressions and gestures, including a
set of instructions to achieve synchronization
between animation and speech. These
instructions are executed by a 3D animation
renderer and a text-to-speech system. Table 5.1
shows the associations between discourse and
conversational state that Rea is currently able to
handle. In other work we have discussed how
Rea deals with the association between
information structure and gesture [6]. In the
following sections, we focus on Rea’s
generation of posture shifts.
</bodyText>
<tableCaption confidence="0.985665">
Table 5.1: Discourse functions &amp; non-verbal
behavior cues
</tableCaption>
<table confidence="0.992838785714286">
Discourse Functions non-verbal
level info. behavior cues
Discourse new segment Posture_shift
structure
Conversation turn giving eye_gaze &amp;
structure (stop_gesturing
hand_gesture)
turn keeping (look_away
keep_gesture)
turn taking eye_gaze &amp;
posture_shift
Information emphasize eye_gaze &amp;
structure information beat_and
other_hand_gsts
</table>
<figure confidence="0.705045305555555">
Dialogue
Manager
(Collagen)
Reaction
Module (RM)
Understanding
Module
Speech
Recognition
Vision
Processing
Animation
Renderer
Text to
Speech
Microphone Camera
Animation Speech
Figure5.1: System architecture
Generation Module
Sentence
Realizer
Gesture
Component
8
7
6
5
4
3
2
1
inter
end
mid
DSEG
intra
</figure>
<subsectionHeader confidence="0.989316">
5.2 The Collagen dialogue manager
</subsectionHeader>
<bodyText confidence="0.999954277777778">
CollagenTM is JAVA middleware for building
COLLAborative interface AGENts to work with
users on interface applications. Collagen is
designed with the capability to participate in
collaboration and conversation, based on [12],
[16]. Collagen updates the focus stack and
recipe tree using a combination of the discourse
interpretation algorithm of [16] and plan
recognition algorithms of [14]. It takes as input
user and system utterances and interface actions,
and accesses a library of recipes describing
actions in the domain. After updating the
discourse state, Collagen makes three resources
available to the interface agent: focus of
attention (using the focus stack), segmented
interaction history (of completed segments) and
an agenda of next possible actions created from
the focus stack and recipe tree.
</bodyText>
<subsectionHeader confidence="0.99578">
5.3 Output Generation
</subsectionHeader>
<bodyText confidence="0.9999874375">
The Reaction Module works as a content
planner in the Rea architecture, and also plays
the role of an interface agent in Collagen. It has
access to the discourse state and the agenda
using APIs provided by Collagen. Based on the
results reported above, we describe here how
Rea plans her next nonverbal actions using the
resources that Collagen maintains.
The empirical study revealed that posture shifts
are distributed with respect to discourse segment
and turn boundaries, and that the form of a
posture shift differs according to these co-
determinants. Therefore, generation of posture
shifts in Rea is determined according to these
two factors, with Collagen contributing
information about current discourse state.
</bodyText>
<subsubsectionHeader confidence="0.952897">
5.3.1 Discourse structure information
</subsubsectionHeader>
<bodyText confidence="0.999716727272727">
Any posture shift that occurs between the end of
one discourse segment and the beginning of the
next is defined as an inter-discourse segment
posture shift. In order to elaborate different
generation rules for inter- vs. intra-discourse
segments, Rea judges (D1) whether the next
utterance starts a new topic, or contributes to the
current discourse purpose, (D2) whether the
next utterance is expected to finish a segment.
First, (D1) is calculated by referring to the focus
stack and agenda. In planning a next action, Rea
accesses the goal agenda in Collagen and gets
the content of her next utterance. She also
accesses the focus stack and gets the current
discourse purpose that is shared between her and
the user. By comparing the current purpose and
the purpose of her next utterance, Rea can judge
whether the her next utterance contributes to the
current discourse purpose or not. For example, if
the current discourse purpose is to find a house
to show the user (FindHouse), and the next
utterance that Rea plans to say is as follows,
</bodyText>
<listItem confidence="0.49606875">
(1) (Ask.What (agent Propose.What (user FindHouse
&lt;city ?&gt;)))
Rea says: &amp;quot;What kind of transportation access do you
need?&amp;quot;
</listItem>
<bodyText confidence="0.9856405">
then Rea uses Collagen APIs to compare the
current discourse purpose (FindHouse) to the
purpose of utterance (1). The purpose of this
utterance is to ask the value of the transportation
parameter of FindHouse. Thus, Rea judges that
this utterance contributes to the current
discourse purpose, and continues the same
discourse segment (D1 = continue). On the
other hand, if Rea’s next utterance is about
showing a house,
</bodyText>
<listItem confidence="0.650867666666667">
(2) (Propose.Should (agent ShowHouse (joint
123ElmStreet))
Rea says: &amp;quot;Let&apos;s look at 123 Elm Street.&amp;quot;
</listItem>
<bodyText confidence="0.998479944444445">
then this utterance does not directly contribute
to the current discourse purpose because it does
not ask a parameter of FindHouse, and it
introduces a new discourse purpose ShowHouse.
In this case, Rea judges that there is a discourse
segment boundary between the previous
utterance and the next one (D1 = topic change).
In order to calculate (D2), Rea looks at the plan
tree in Collagen, and judges whether the next
utterance addresses the last goal in the current
discourse purpose. If it is the case, Rea expects
to finish the current discourse segment by the
next utterance (D1 = finish topic). As for
conversational structure, Rea needs to know;
(T1) whether Rea is taking a new turn with the
next utterance, or keeping her current turn for
the next utterance, (T2) whether Rea’s next
utterance requires that the user respond.
</bodyText>
<table confidence="0.824130428571429">
Place of a Case Discourse Conversation Posture shift Posture shift selection
posture shift structure structure decision
information information probability
energy duration body part
beginning of a D1 topic T1 take turn 0.54/int high default upper &amp;
the utterance change
b change keep turn lowertopic - - -
0
c continue take turn 0.13/int low default upper or
lower
d continue keep turn 0.14/sec low short lower
End of the e D2 finish T2 give turn 0.04/int high long lower
utterance topic
f continue give turn 0.11/int low default lower
</table>
<tableCaption confidence="0.858054">
Table 5.3.1:Posture Decision Probabilities for Dialogue
</tableCaption>
<bodyText confidence="0.999723">
First, (T1) is judged by referring to the dialogue
history1. The dialogue history stores both system
utterances and user utterances that occurred in
the dialogue. In the history, each utterance is
stored as a logical form based on an artificial
discourse language [20]. As shown above in
utterance (1), the first argument of the action
indicates the speaker of the utterance; in this
example, it is “agent”. The turn boundary can be
estimated by comparing the speaker of the
previous utterance with the speaker of the next
utterance. If the speaker of the previous
utterance is not Rea, there is a turn boundary
before the next utterance (T1 = take turn). If the
speaker of the previous utterance is Rea, that
means that Rea will keep the same turn for the
next utterance (T1 = keep turn).
Second, (T2) is judged by looking at the type of
Rea’s next utterance. For example, when Rea
asks a question, as in utterance (1), Rea expects
the user to answer the question. In this case, Rea
must convey to the user that the system gives up
the turn (T2 = give up turn).
</bodyText>
<subsubsectionHeader confidence="0.985329">
5.3.2 Deciding and selecting a posture shift
</subsubsectionHeader>
<bodyText confidence="0.995512333333333">
Combining information about discourse
structure (D1, D2) and conversation structure
(T1, T2), the system decides on posture shifts
</bodyText>
<footnote confidence="0.974829">
1 We currently maintain a dialogue history in Rea even
though Collagen has one as well. This is in order to store
and manipulate the information to generate hand gestures
and assign intonational accents. This information will be
integrated into Collagen in the near future.
</footnote>
<bodyText confidence="0.999851954545455">
for the beginning of the utterance and the end of
the utterance. Rea decides to do or not to do a
posture shift by calling a probabilistic function
that looks up the probabilities in Table 5.3.1.
A posture shift for the beginning of the utterance
is decided based on the combination of (D1) and
(T1). For example, if the combined factors
match Case (a), the system decides to generate a
posture shift with 54% probability for the
beginning of the utterance. Note that in Case
(d), that is, Rea keeps the turn without changing
a topic, we cannot calculate a per interval
posture shift rate. Instead, we use a posture shift
rate normalized for time. This rate is used in the
GenerationModule, which calculates the
utterance duration and generates a posture shift
during the utterance based on this posture shift
rate. On the other hand, ending posture shifts
are decided based on the combination of (D2)
and (T2).
For example, if the combined factors match
Case (e), the system decides to generate a
posture shift with 0.04% probability for the
ending of the utterance. When Rea does decide
to activate a posture shift, she then needs to
choose which posture shift to perform. Our
empirical data indicates that the energy level of
the posture shift differs depending on whether
there is a discourse segment boundary or not.
Moreover the duration of a posture shift differs
depending on the place in a turn: start-, mid-, or
end-turn.
Based on these results, we define posture shift
selection rules for energy, duration, and body
part. The correspondence with discourse
information is shown in Table 5.3.1. For
example, in Case (a), the system selects a
posture shift with high energy, using both upper
and lower body. After deciding whether or not
Rea should shift posture and (if so) choosing a
kind of posture shift, Rea sends a command to
the Generation Module to generate a specific
kind of posture shift within a specific time
duration.
</bodyText>
<tableCaption confidence="0.985178">
Table 5.3.2: Posture Decision Probabilities: Monologue
</tableCaption>
<table confidence="0.998242375">
Ca Discourse Posture Posture shift
se structure shift selection
information decision
probability
energy
g D1 change 0.84/int high
topic
h continue 0.04/sec low
</table>
<bodyText confidence="0.9995107">
Posture shifts for pseudo-monologues can be
decided using the same mechanism as that for
dialogue, but omitting conversation structure
information. The probabilities are given in
table Table 5.3.2. For example, if Rea changes
the topic with her next utterance, a posture shift
is generated 84% of the time with high-energy
motion. In other cases, the system randomly
generates low-energy posture shifts 0.04 times
per second.
</bodyText>
<sectionHeader confidence="0.991462" genericHeader="method">
6. Example
</sectionHeader>
<bodyText confidence="0.992748517857143">
Figure 6.1 shows a dialogue between Rea and
the user, and shows how Rea decides to generate
posture shifts. This dialogue consists of two
major segments: finding a house (dialogue), and
showing a house (pseudo-monologue). Based on
this task structure, we defined plan recipes for
Collagen. The first shared discourse purpose
[goal: HaveConversation] is introduced by the
user before the example. Then, in utterance (1),
the user introduces the main part of the
conversation [goal: FindHouse].
The next goal in the agenda, [goal:
IdentifyPreferredCity], should be
accomplished to identify a parameter value for
[goal: FindHouse]. This goal directly
contributes to the current purpose, [goal:
FindHouse]. This case is judged to be a turn
boundary within a discourse segment (Case (c)),
and Rea decides to generate a posture shift at the
beginning of the utterance with 13% probability.
If Rea decides to shift posture she selects a low
energy posture shift using either upper or lower
body. In addition to a posture shift at the
beginning of the utterance, Rea may also choose
to generate a posture shift to end the turn. As
utterance (2) expects the user to take the turn,
and continue to work on the same discourse
purpose, this is Case (f). Thus, the system
generates an end utterance posture shift 11% of
the time. If generated, a low energy posture
shift is chosen. If a beginning and/or ending
posture shifts are generated, they are sent to the
GM, which calculates the schedule of these
multimodal events and generates them.
In utterance (25), Rea introduces a new
discourse purpose [goal : ShowHouse]. Rea,
using a default rule, decides to take the initiative
on this goal. At this point, Rea accesses the
discourse state and confirms that a new goal is
about to start. Rea judges this case as a
discourse segment boundary and also a turn
boundary (Case (a)). Based on this information,
Rea selects a high energy posture shift. An
example of Rea’s high energy posture shift is
shown on the right in Figure 5.2.
As a subdialogue of showing a house, in a
discourse purpose [goal : DiscussFeature], Rea
keeps the turn and continues to describe the
house. We handle this type of interaction as a
pseudo-monologue. Therefore, we can use table
Table 5.3.2 for deciding on posture shifts here.
In utterance (27), Rea starts the discussion about
the house, and takes the initiative. This is judged
as Case (g), and a high energy body motion is
generated 84% of the time.
[Finding a house] &lt; dialogue&gt;
</bodyText>
<listItem confidence="0.763016166666667">
(1) U: I’m looking for a house.
(2) R: (c) Where do you want to live? (f)
(3) U: I like Boston.
(4) R: (c) (d) What kind of transportation
access do you need? (f)
(5) U: I need T access.
</listItem>
<bodyText confidence="0.349912">
....
</bodyText>
<reference confidence="0.767341384615385">
(23) R: (c) (d) How much storage space do
you need? (f)
(24) U: I need to have a storage place in the
basement.
[Showing a house] &lt;Pseudo-monologue&gt;
(25) R: (a) (d) Let’s look at 123 Elm Street. (f)
(26) U: OK.
[Discuss a feature of the house]
(27) R: (g) Let&apos;s discuss a feature of this place.
(28) R: (h) Notice the hardw ood flooring in the
living room.
(29) R: (h) Notice the jacuzzi.
(30) R: (h) Notice the remodeled kitchen
</reference>
<figureCaption confidence="0.948747">
Figure 6.1: Example dialogue
</figureCaption>
<sectionHeader confidence="0.473175" genericHeader="conclusions">
7. Conclusion and Further work
</sectionHeader>
<bodyText confidence="0.99985036">
We have demonstrated a clear relationship
between nonverbal behavior and discourse state,
and shown how this finding can be incorporated
into the generation of language and nonverbal
behaviors for an embodied conversational agent.
Speakers produce posture shifts at 53% of
discourse segment boundaries, more frequently
than they produce those shifts discourse
segment-internally, and with more motion
energy. Furthermore, there is a relationship
between discourse structure and conversational
structure such that when speakers initiate a new
segment at the same time as starting a turn (the
most frequent case by far), they are more likely
to produce a posture shift; while when they end
a discourse segment and a turn at the same time,
their posture shifts last longer than when these
categories do not co-occur.
Although this paper reports results from a
limited number of monologues and dialogues,
the findings are promising. In addition, they
point the way to a number of future directions,
both within the study of posture and discourse,
and more generally within the study of non-
verbal behaviors in computational linguistics.
</bodyText>
<figureCaption confidence="0.773653">
Figure 6.2: Rea demonstrating a low and high energy
posture shift
</figureCaption>
<bodyText confidence="0.999944842105263">
First, given the relationship between
conversational and information structure in [5],
a natural next step is to examine the three-way
relationship between discourse state,
conversational structure (turns), and information
structure (theme/rheme). For the moment, we
have demonstrated that posture shifts may signal
boundaries of units; do they also signal the
information content of units? Next, we need to
look at finer segmentations of the discourse, to
see whether larger and smaller discourse
segments are distinguished through non-verbal
means. Third, the question of listener posture is
an important one. We found that a number of
posture shifts were produced by the participant
who was not speaking. More than half of these
shifts were produced at the same time as a
speaker shift, suggesting a kind of mirroring. In
order to interpret these data, however, a more
sensitive notion of turn structure is required, as
one must be ready to define when exactly
speakers and listeners shift roles. Also, of
course, evaluation of the importance of such
nonverbal behaviors to user interaction is
essential. In a user study of our earlier Gandalf
system [4], users rated the agent&apos;s language
skills significantly higher under test conditions
in which Gandalf deployed conversational
behaviors (gaze, head movement and limited
gesture) than when these behaviors were
disabled. Such an evaluation is also necessary
for the Rea-posture system. But, more
generally, we need to test whether generating
posture shifts of this sort actually serves as a
signal to listeners, for example to initiative
structure in task and dialogue [8]. These
evaluations form part of our future research
plans.
</bodyText>
<sectionHeader confidence="0.911372" genericHeader="acknowledgments">
8. Acknowledgements
</sectionHeader>
<bodyText confidence="0.876413">
This research was supported by MERL, France
Telecom, AT&amp;T, and the other generous sponsors of
the MIT Media Lab. Thanks to the other members of
the Gesture and Narrative Language Group, in
particular Ian Gouldstone and Hannes Vilhjálmsson.
</bodyText>
<sectionHeader confidence="0.918641" genericHeader="references">
9. REFERENCES
</sectionHeader>
<reference confidence="0.999913462365591">
[1] Andre, E., Rist, T., &amp; Muller, J., Employing AI
methods to control the behavior of animated
interface agents, Applied Artificial Intelligence,
vol. 13, pp. 415-448, 1999.
[2] Cassell, J., Bickmore, T., Billinghurst, M.,
Campbell, L., Chang, K., Vilhjalmsson, H., &amp;
Yan, H., Embodiment in Conversational
Interfaces: Rea, Proc. of CHI 99, Pittsburgh, PA,
ACM, 1999.
[3] Cassell, J., Stone, M., &amp; Yan, H., Coordination
and context-dependence in the generation of
embodied conversation, Proc. INLG 2000,
Mitzpe Ramon, Israel, 2000.
[4] Cassell, J. and Thorisson, K. R., The Power of a
Nod and a Glance: Envelope vs. Emotional
Feedback in Animated Conversational Agents,
Applied Art. Intell., vol. 13, pp. 519-538, 1999.
[5] Cassell, J., Torres, O., &amp; Prevost, S., Turn
Taking vs. Discourse Structure: How Best to
Model Multimodal Conversation., in Machine
Conversations, Y. Wilks, Ed. The Hague:
Kluwer, 1999, pp. 143-154.
[6] Cassell, J., Vilhjálmsson, H., &amp; Bickmore, T.,
BEAT: The Behavior Expression Animation
Toolkit, Proc. of SIGGRAPH, ACM Press,
2001.
[7] Chovil, N., Discourse-Oriented Facial Displays
in Conversation, Research on Language and
Social Interaction, vol. 25, pp. 163-194, 1992.
[8] Chu-Carroll, J. &amp; Brown, M., Initiative in
Collaborative Interactions - Its Cues and Effects,
Proc. of AAAI Spring 1997 Symp. on
Computational Models of Mixed Initiative,
1997.
[9] Condon, W. S. &amp; Osgton, W. D., Speech and
body motion synchrony of the speaker-hearer, in
The perception of language, D. Horton &amp; J.
Jenkins, Eds. NY: Academic Press, 1971, pp.
150-184.
[10] Duncan, S., On the structure of speaker-auditor
interaction during speaking turns, Language in
Society, vol. 3, pp. 161-180, 1974.
[11] Green, N., Carenini, G., Kerpedjiev, S., &amp; Roth,
S, A Media-Independent Content Language for
Integrated Text and Graphics Generation, Proc.
of Workshop on Content Visualization and
Intermedia Representations at COLING and
ACL &apos;98, 1998.
[12] Grosz, B. &amp; Sidner, C., Attention, Intentions,
and the Structure of Discourse, Computational
Linguistics, vol. 12, pp. 175-204, 1986.
[13] Kendon, A., Some Relationships between Body
Motion and Speech, in Studies in Dyadic
Communication, A. W. Siegman and B. Pope,
Eds. Elmsford, NY: Pergamon Press, 1972, pp.
177-210.
[14] Lesh, N., Rich, C., &amp; Sidner, C., Using Plan
Recognition in Human-Computer Collaboration,
Proc. of the Conference on User Modelling,
Banff, Canada, NY: Springer Wien, 1999.
[15] Lester, J., Towns, S., Callaway, C., Voerman, J.,
&amp; FitzGerald, P., Deictic and Emotive
Communication in Animated Pedagogical
Agents, in Embodied Conversational Agents, J.
Cassell, J. Sullivan, et. al, Eds. Cambridge: MIT
Press, 2000.
[16] Lochbaum, K., A Collaborative Planning Model
of Intentional Structure, Computational
Linguistics, vol. 24, pp. 525-572, 1998.
[17] McNeill, D., Hand and Mind: What Gestures
Reveal about Thought. Chicago, IL/London,
UK: The University of Chicago Press, 1992.
[18] Rich, C. &amp; Sidner, C. L., COLLAGEN: A
Collaboration Manager for Software Interface
Agents, User Modeling and User-Adapted
Interaction, vol. 8, pp. 315-350, 1998.
[19] Rickel, J. &amp; Johnson, W. L., Task-Oriented
Collaboration with Embodied Agents in Virtual
Worlds, in Embodied Conversational Agents, J.
Cassell, Ed. Cambridge, MA: MIT Press, 2000.
[20] Sidner, C., An Artificial Discourse Language for
Collaborative Negotiation, Proc. of 12th Intnl.
Conf. on Artificial Intelligence (AAAI), Seattle,
WA, MIT Press, 1994.
[21] Takeuchi, A. &amp; Nagao, K., Communicative
facial displays as a new conversational modality,
Proc. of InterCHI &apos;93, Amsterdam, NL, ACM,
1993.
[22] Thompson, L. and Massaro, D., Evaluation and
Integration of Speech and Pointing Gestures
during Referential Understanding, Journal of
Experimental Child Psychology, vol. 42, pp.
144-168, 1986.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960163">
<title confidence="0.995221">Non-Verbal Cues for Discourse Structure</title>
<author confidence="0.997789">Justine Cassell†</author>
<author confidence="0.997789">Yukiko I Nakano†</author>
<author confidence="0.997789">Timothy W Bickmore†</author>
<author confidence="0.997789">Candace L Sidner</author>
<author confidence="0.997789">Charles Rich</author>
<affiliation confidence="0.999957">MIT Media Laboratory $Mitsubishi Electric Research Laboratories</affiliation>
<address confidence="0.999655">20 Ames Street 201 Broadway Cambridge, MA 02139 Cambridge, MA 02139</address>
<email confidence="0.999558">justine@merl.com</email>
<email confidence="0.999558">yukiko@merl.com</email>
<email confidence="0.999558">bickmore}@media.mit.edu{sidner@merl.com</email>
<email confidence="0.999558">rich@merl.com</email>
<abstract confidence="0.9983025">This paper addresses the issue of designing embodied conversational agents that exhibit appropriate posture shifts during dialogues with human users. Previous research has noted the importance of hand gestures, eye gaze and head nods in conversations between embodied agents and humans. We present an analysis of human monologues and dialogues that suggests that postural shifts can be predicted as a function of discourse state in monologues, and discourse and conversation state in dialogues. On the basis of these findings, we have implemented an embodied conversational agent that uses Collagen in such a way as to generate postural shifts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>R</author>
</authors>
<title>(c) (d) How much storage space do you need? (f) (24) U: I need to have a storage place in the basement.</title>
<marker>R, </marker>
<rawString> (23) R: (c) (d) How much storage space do you need? (f) (24) U: I need to have a storage place in the basement.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R</author>
</authors>
<title>(a) (d) Let’s look at 123 Elm Street.</title>
<note>(f) (26) U: OK.</note>
<marker>[Showing a house]</marker>
<rawString>&lt;Pseudo-monologue&gt; (25) R: (a) (d) Let’s look at 123 Elm Street. (f) (26) U: OK.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R</author>
</authors>
<title>(g) Let&apos;s discuss a feature of this place. (28) R: (h) Notice the hardw ood flooring in the living room. (29) R: (h) Notice the jacuzzi. (30) R: (h) Notice the remodeled kitchen</title>
<marker>[Discuss a feature of the house]</marker>
<rawString> (27) R: (g) Let&apos;s discuss a feature of this place. (28) R: (h) Notice the hardw ood flooring in the living room. (29) R: (h) Notice the jacuzzi. (30) R: (h) Notice the remodeled kitchen</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Andre</author>
<author>T Rist</author>
<author>J Muller</author>
</authors>
<title>Employing AI methods to control the behavior of animated interface agents,</title>
<date>1999</date>
<journal>Applied Artificial Intelligence,</journal>
<volume>13</volume>
<pages>415--448</pages>
<contexts>
<context position="3976" citStr="[1]" startWordPosition="568" endWordPosition="568"> compatibility in the gesture-speech relationship recalls the interaction of words and graphics in multimodal presentations [11]. On the basis of results such as these, several researchers have built animated embodied conversational agents that ally synthesized speech with animated hand gestures. For example, Lester et al. [15] generate deictic gestures and choose referring expressions as a function of the potential ambiguity and proximity of objects referred to. Rickel and Johnson [19]&apos;s pedagogical agent produces a deictic gesture at the beginning of explanations about objects. André et al. [1] generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a sub-action of elaborating. Cassell and Stone [3] generate either speech, gesture, or a combination of the two, as a function of the information structure status and surprise value of the discourse entity. Head and eye movement has also been examined in the context of discourse and conversation. Looking away from one’s interlocutor has been correlated with the beginning of turns. From the speaker’s point of view, this look away may prevent an overload of visual and linguistic information. On the other ha</context>
</contexts>
<marker>[1]</marker>
<rawString>Andre, E., Rist, T., &amp; Muller, J., Employing AI methods to control the behavior of animated interface agents, Applied Artificial Intelligence, vol. 13, pp. 415-448, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>T Bickmore</author>
<author>M Billinghurst</author>
<author>L Campbell</author>
<author>K Chang</author>
<author>H Vilhjalmsson</author>
<author>H Yan</author>
</authors>
<date>1999</date>
<booktitle>Embodiment in Conversational Interfaces: Rea, Proc. of CHI 99,</booktitle>
<publisher>ACM,</publisher>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="6038" citStr="[2]" startWordPosition="898" endWordPosition="898">on structure of the propositional content of an utterance [5]. Specifically, the beginning of themes are frequently accompanied by a look-away from the hearer, and the beginning of rhemes are frequently accompanied by a look-toward the hearer. When these categories are co-temporaneous with turn construction, then they are strongly predictive of gaze behavior. Results such as these have led researchers to generate eye gaze and head movements in animated embodied conversational agents. Takeuchi and Nagao, for example, [21] generate gaze and head nod behaviors in a “talking head.” Cassell et al. [2] generate eye gaze and head nods as a function of turn taking behavior, head turns just before an utterance, and eyebrow raises as a function of emphasis. To our knowledge, research on posture shifts and other gross body movements, has not been used in the design or implementation of computational systems. In fact, although a number of conversational analysts and ethnomethodologists have described posture shifts in conversation, their studies have been qualitative in nature, and difficult to reformulate as the basis of algorithms for the generation of language and posture. Nevertheless, resear</context>
<context position="16320" citStr="[2]" startWordPosition="2493" endWordPosition="2493">a new topic is introduced or not (2.5 seconds). However, when ending a turn, speakers move significantly longer (7.0 seconds) when finishing a topic than when the topic is continued by the other interlocutor (2.7 seconds) (F(1,148)=17.9; p&lt;0.001). Figure 4.1.1 Posture Shift Duration by DSeg and Turn 5. System In the following sections we discuss how the results of the empirical study were integrated along with Collagen into our existent embodied conversational agent, Rea. 5.1 System Architecture Rea is an embodied conversational agent that interacts with a user in the real estate agent domain [2]. The system architecture of Rea is shown in Figure 5.1. Rea takes input from a microphone and two cameras in order to sense the user’s speech and gesture. The UM interprets and integrates this multimodal input and outputs a unified semantic representation. The Understanding Module then sends the output to Collagen as the Dialogue Manager. Collagen, as further discussed below, maintains the state of the dialogue as shared between Rea and a user. The Reaction Module decides Rea’s next action based on the discourse state maintained by Collagen. It also assigns information structure to output utt</context>
</contexts>
<marker>[2]</marker>
<rawString>Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L., Chang, K., Vilhjalmsson, H., &amp; Yan, H., Embodiment in Conversational Interfaces: Rea, Proc. of CHI 99, Pittsburgh, PA, ACM, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>M Stone</author>
<author>H Yan</author>
</authors>
<title>Coordination and context-dependence in the generation of embodied conversation,</title>
<date>2000</date>
<booktitle>Proc. INLG</booktitle>
<location>Mitzpe Ramon, Israel,</location>
<contexts>
<context position="4116" citStr="[3]" startWordPosition="590" endWordPosition="590">is of results such as these, several researchers have built animated embodied conversational agents that ally synthesized speech with animated hand gestures. For example, Lester et al. [15] generate deictic gestures and choose referring expressions as a function of the potential ambiguity and proximity of objects referred to. Rickel and Johnson [19]&apos;s pedagogical agent produces a deictic gesture at the beginning of explanations about objects. André et al. [1] generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a sub-action of elaborating. Cassell and Stone [3] generate either speech, gesture, or a combination of the two, as a function of the information structure status and surprise value of the discourse entity. Head and eye movement has also been examined in the context of discourse and conversation. Looking away from one’s interlocutor has been correlated with the beginning of turns. From the speaker’s point of view, this look away may prevent an overload of visual and linguistic information. On the other hand, during the execution phase of an utterance, speakers look more often at listeners. Head nods and eyebrow raises are correlated with emph</context>
</contexts>
<marker>[3]</marker>
<rawString>Cassell, J., Stone, M., &amp; Yan, H., Coordination and context-dependence in the generation of embodied conversation, Proc. INLG 2000, Mitzpe Ramon, Israel, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>K R Thorisson</author>
</authors>
<title>The Power of a Nod and a Glance: Envelope vs.</title>
<date>1999</date>
<journal>Emotional Feedback in Animated Conversational Agents, Applied Art. Intell.,</journal>
<volume>13</volume>
<pages>519--538</pages>
<marker>[4]</marker>
<rawString>Cassell, J. and Thorisson, K. R., The Power of a Nod and a Glance: Envelope vs. Emotional Feedback in Animated Conversational Agents, Applied Art. Intell., vol. 13, pp. 519-538, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>O Torres</author>
<author>S Prevost</author>
</authors>
<title>Turn Taking vs. Discourse Structure: How Best to Model Multimodal Conversation.,</title>
<date>1999</date>
<booktitle>in Machine Conversations, Y. Wilks, Ed. The Hague:</booktitle>
<pages>143--154</pages>
<publisher>Kluwer,</publisher>
<contexts>
<context position="5496" citStr="[5]" startWordPosition="816" endWordPosition="816">to function as synchronization signals. That is, one may request a response from a listener by looking at the listener, and suppress the listener’s response by looking away. Likewise, in order to offer the floor, a speaker may gaze at the listener at the end of the utterance. When the listener wants the floor, s/he may look at and slightly up at the speaker [10]. It should be noted that turn taking only partially accounts for eye gaze behavior in discourse. A better explanation for gaze behavior integrates turn taking with the information structure of the propositional content of an utterance [5]. Specifically, the beginning of themes are frequently accompanied by a look-away from the hearer, and the beginning of rhemes are frequently accompanied by a look-toward the hearer. When these categories are co-temporaneous with turn construction, then they are strongly predictive of gaze behavior. Results such as these have led researchers to generate eye gaze and head movements in animated embodied conversational agents. Takeuchi and Nagao, for example, [21] generate gaze and head nod behaviors in a “talking head.” Cassell et al. [2] generate eye gaze and head nods as a function of turn tak</context>
</contexts>
<marker>[5]</marker>
<rawString>Cassell, J., Torres, O., &amp; Prevost, S., Turn Taking vs. Discourse Structure: How Best to Model Multimodal Conversation., in Machine Conversations, Y. Wilks, Ed. The Hague: Kluwer, 1999, pp. 143-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>H Vilhjálmsson</author>
<author>T Bickmore</author>
</authors>
<title>BEAT: The Behavior Expression Animation Toolkit,</title>
<date>2001</date>
<booktitle>Proc. of SIGGRAPH,</booktitle>
<publisher>ACM Press,</publisher>
<contexts>
<context position="17560" citStr="[6]" startWordPosition="2680" endWordPosition="2680">priately generated. The semantic representation of the action, including verbal and non-verbal behaviors, is sent to the Generation Module which generates surface linguistic expressions and gestures, including a set of instructions to achieve synchronization between animation and speech. These instructions are executed by a 3D animation renderer and a text-to-speech system. Table 5.1 shows the associations between discourse and conversational state that Rea is currently able to handle. In other work we have discussed how Rea deals with the association between information structure and gesture [6]. In the following sections, we focus on Rea’s generation of posture shifts. Table 5.1: Discourse functions &amp; non-verbal behavior cues Discourse Functions non-verbal level info. behavior cues Discourse new segment Posture_shift structure Conversation turn giving eye_gaze &amp; structure (stop_gesturing hand_gesture) turn keeping (look_away keep_gesture) turn taking eye_gaze &amp; posture_shift Information emphasize eye_gaze &amp; structure information beat_and other_hand_gsts Dialogue Manager (Collagen) Reaction Module (RM) Understanding Module Speech Recognition Vision Processing Animation Renderer Text </context>
</contexts>
<marker>[6]</marker>
<rawString>Cassell, J., Vilhjálmsson, H., &amp; Bickmore, T., BEAT: The Behavior Expression Animation Toolkit, Proc. of SIGGRAPH, ACM Press, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chovil</author>
</authors>
<title>Discourse-Oriented Facial Displays in Conversation,</title>
<date>1992</date>
<journal>Research on Language and Social Interaction,</journal>
<volume>25</volume>
<pages>163--194</pages>
<contexts>
<context position="4788" citStr="[7]" startWordPosition="698" endWordPosition="698">unction of the information structure status and surprise value of the discourse entity. Head and eye movement has also been examined in the context of discourse and conversation. Looking away from one’s interlocutor has been correlated with the beginning of turns. From the speaker’s point of view, this look away may prevent an overload of visual and linguistic information. On the other hand, during the execution phase of an utterance, speakers look more often at listeners. Head nods and eyebrow raises are correlated with emphasized linguistic items – such as words accompanied by pitch accents [7]. Some eye movements occur primarily at the ends of utterances and at grammatical boundaries, and appear to function as synchronization signals. That is, one may request a response from a listener by looking at the listener, and suppress the listener’s response by looking away. Likewise, in order to offer the floor, a speaker may gaze at the listener at the end of the utterance. When the listener wants the floor, s/he may look at and slightly up at the speaker [10]. It should be noted that turn taking only partially accounts for eye gaze behavior in discourse. A better explanation for gaze beh</context>
</contexts>
<marker>[7]</marker>
<rawString>Chovil, N., Discourse-Oriented Facial Displays in Conversation, Research on Language and Social Interaction, vol. 25, pp. 163-194, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>M Brown</author>
</authors>
<title>Initiative in Collaborative Interactions - Its Cues and Effects,</title>
<date>1997</date>
<booktitle>Proc. of</booktitle>
<publisher>AAAI Spring</publisher>
<marker>[8]</marker>
<rawString>Chu-Carroll, J. &amp; Brown, M., Initiative in Collaborative Interactions - Its Cues and Effects, Proc. of AAAI Spring 1997 Symp. on Computational Models of Mixed Initiative, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W S Condon</author>
<author>W D Osgton</author>
</authors>
<title>Speech and body motion synchrony of the speaker-hearer, in The perception of language,</title>
<date>1971</date>
<pages>150--184</pages>
<publisher>Academic Press,</publisher>
<location>Eds. NY:</location>
<contexts>
<context position="7142" citStr="[9]" startWordPosition="1067" endWordPosition="1067">rmulate as the basis of algorithms for the generation of language and posture. Nevertheless, researchers in the non-computational fields have discussed posture shifts extensively. Kendon [13] reports a hierarchy in the organization of movement such that the smaller limbs such as the fingers and hands engage in more frequent movements, while the trunk and lower limbs change relatively rarely. A number of researchers have noted that changes in physical distance during interaction seem to accompany changes in the topic or in the social relationship between speakers. For example Condon and Osgton [9] have suggested that in a speaking individual the changes in these more slowly changing body parts occur at the boundaries of the larger units in the flow of speech. Scheflen (1973) also reports that posture shifts and other general body movements appear to mark the points of change between one major unit of communicative activity and another. Blom &amp; Gumperz (1972) identify posture changes and changes in the spatial relationship between two speakers as indicators of what they term &amp;quot;situational shifts&amp;quot; -- momentary changes in the mutual rights and obligations between speakers accompanied by shi</context>
</contexts>
<marker>[9]</marker>
<rawString>Condon, W. S. &amp; Osgton, W. D., Speech and body motion synchrony of the speaker-hearer, in The perception of language, D. Horton &amp; J. Jenkins, Eds. NY: Academic Press, 1971, pp. 150-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Duncan</author>
</authors>
<title>On the structure of speaker-auditor interaction during speaking turns,</title>
<date>1974</date>
<journal>Language in Society,</journal>
<volume>3</volume>
<pages>161--180</pages>
<contexts>
<context position="5257" citStr="[10]" startWordPosition="779" endWordPosition="779">isteners. Head nods and eyebrow raises are correlated with emphasized linguistic items – such as words accompanied by pitch accents [7]. Some eye movements occur primarily at the ends of utterances and at grammatical boundaries, and appear to function as synchronization signals. That is, one may request a response from a listener by looking at the listener, and suppress the listener’s response by looking away. Likewise, in order to offer the floor, a speaker may gaze at the listener at the end of the utterance. When the listener wants the floor, s/he may look at and slightly up at the speaker [10]. It should be noted that turn taking only partially accounts for eye gaze behavior in discourse. A better explanation for gaze behavior integrates turn taking with the information structure of the propositional content of an utterance [5]. Specifically, the beginning of themes are frequently accompanied by a look-away from the hearer, and the beginning of rhemes are frequently accompanied by a look-toward the hearer. When these categories are co-temporaneous with turn construction, then they are strongly predictive of gaze behavior. Results such as these have led researchers to generate eye g</context>
</contexts>
<marker>[10]</marker>
<rawString>Duncan, S., On the structure of speaker-auditor interaction during speaking turns, Language in Society, vol. 3, pp. 161-180, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Green</author>
<author>G Carenini</author>
<author>S Kerpedjiev</author>
<author>S Roth</author>
</authors>
<title>A Media-Independent Content Language for Integrated Text and Graphics Generation,</title>
<date>1998</date>
<booktitle>Proc. of Workshop on Content Visualization and Intermedia Representations at COLING and ACL &apos;98,</booktitle>
<contexts>
<context position="3501" citStr="[11]" startWordPosition="497" endWordPosition="497">s been shown that when speech is ambiguous or in a speech situation with some noise, listeners rely on gestural cues [22] (and, the higher the noise-tosignal ratio, the more facilitation by gesture). Even when gestural content overlaps with speech (reported to be the case in roughly 50% of utterances, for descriptive discourse), gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech. In fact, the semantic and pragmatic compatibility in the gesture-speech relationship recalls the interaction of words and graphics in multimodal presentations [11]. On the basis of results such as these, several researchers have built animated embodied conversational agents that ally synthesized speech with animated hand gestures. For example, Lester et al. [15] generate deictic gestures and choose referring expressions as a function of the potential ambiguity and proximity of objects referred to. Rickel and Johnson [19]&apos;s pedagogical agent produces a deictic gesture at the beginning of explanations about objects. André et al. [1] generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a sub-action of elaborating. Cassel</context>
</contexts>
<marker>[11]</marker>
<rawString>Green, N., Carenini, G., Kerpedjiev, S., &amp; Roth, S, A Media-Independent Content Language for Integrated Text and Graphics Generation, Proc. of Workshop on Content Visualization and Intermedia Representations at COLING and ACL &apos;98, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
<author>Attention</author>
</authors>
<title>Intentions, and the Structure of Discourse,</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<pages>175--204</pages>
<contexts>
<context position="9736" citStr="[12]" startWordPosition="1472" endWordPosition="1472">ve backchannel feedback only and never took the turn) allowed us to investigate the relationship between discourse structure and posture shift independent of turn structure. The two tasks were constructed to allow us to identify exactly where discourse segment boundaries would be placed. The video data was transcribed and coded for three features: discourse segment boundaries, turn boundaries, and posture shifts. A discourse segment is taken to be an aggregation of utterances and sub-segments that convey the discourse segment purpose, which is an intention that leads to the segment initiation [12]. In this study we chose initially to look at high-level discourse segmentation phenomena rather than those discourse segments embedded deeper in the discourse. Thus, the time points at which the assigned task topics were started served as segmentation points. Turn boundaries were coded (for dialogues only) as the point in time in which the start or end of an utterance cooccurred with a change in speaker, but excluding backchannel feedback. Turn overlaps were coded as open-floor time. We defined a posture shift as a motion or a position shift for a part of the human body, excluding hands and e</context>
<context position="18589" citStr="[12]" startWordPosition="2816" endWordPosition="2816">ucture information beat_and other_hand_gsts Dialogue Manager (Collagen) Reaction Module (RM) Understanding Module Speech Recognition Vision Processing Animation Renderer Text to Speech Microphone Camera Animation Speech Figure5.1: System architecture Generation Module Sentence Realizer Gesture Component 8 7 6 5 4 3 2 1 inter end mid DSEG intra 5.2 The Collagen dialogue manager CollagenTM is JAVA middleware for building COLLAborative interface AGENts to work with users on interface applications. Collagen is designed with the capability to participate in collaboration and conversation, based on [12], [16]. Collagen updates the focus stack and recipe tree using a combination of the discourse interpretation algorithm of [16] and plan recognition algorithms of [14]. It takes as input user and system utterances and interface actions, and accesses a library of recipes describing actions in the domain. After updating the discourse state, Collagen makes three resources available to the interface agent: focus of attention (using the focus stack), segmented interaction history (of completed segments) and an agenda of next possible actions created from the focus stack and recipe tree. 5.3 Output G</context>
</contexts>
<marker>[12]</marker>
<rawString>Grosz, B. &amp; Sidner, C., Attention, Intentions, and the Structure of Discourse, Computational Linguistics, vol. 12, pp. 175-204, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Some Relationships between Body Motion and Speech,</title>
<date>1972</date>
<booktitle>in Studies in Dyadic Communication, A. W. Siegman</booktitle>
<pages>177--210</pages>
<publisher>Pergamon Press,</publisher>
<location>Eds. Elmsford, NY:</location>
<contexts>
<context position="2890" citStr="[13]" startWordPosition="405" endWordPosition="405">es of topic shifts. 2. Background Only recently have computational linguists begun to examine the association of nonverbal behaviors and language. In this section we review research by non-computational linguists and discuss how this research has been employed to formulate algorithms for natural language generation or understanding. About three-quarters of all clauses in descriptive discourse are accompanied by gestures [17], and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech [13]. It has been shown that when speech is ambiguous or in a speech situation with some noise, listeners rely on gestural cues [22] (and, the higher the noise-tosignal ratio, the more facilitation by gesture). Even when gestural content overlaps with speech (reported to be the case in roughly 50% of utterances, for descriptive discourse), gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech. In fact, the semantic and pragmatic compatibility in the gesture-speech relationship recalls the interaction of words and graphics in multimodal present</context>
<context position="6730" citStr="[13]" startWordPosition="1001" endWordPosition="1001">before an utterance, and eyebrow raises as a function of emphasis. To our knowledge, research on posture shifts and other gross body movements, has not been used in the design or implementation of computational systems. In fact, although a number of conversational analysts and ethnomethodologists have described posture shifts in conversation, their studies have been qualitative in nature, and difficult to reformulate as the basis of algorithms for the generation of language and posture. Nevertheless, researchers in the non-computational fields have discussed posture shifts extensively. Kendon [13] reports a hierarchy in the organization of movement such that the smaller limbs such as the fingers and hands engage in more frequent movements, while the trunk and lower limbs change relatively rarely. A number of researchers have noted that changes in physical distance during interaction seem to accompany changes in the topic or in the social relationship between speakers. For example Condon and Osgton [9] have suggested that in a speaking individual the changes in these more slowly changing body parts occur at the boundaries of the larger units in the flow of speech. Scheflen (1973) also r</context>
</contexts>
<marker>[13]</marker>
<rawString>Kendon, A., Some Relationships between Body Motion and Speech, in Studies in Dyadic Communication, A. W. Siegman and B. Pope, Eds. Elmsford, NY: Pergamon Press, 1972, pp. 177-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Lesh</author>
<author>C Rich</author>
<author>C Sidner</author>
</authors>
<title>Using Plan Recognition</title>
<date>1999</date>
<booktitle>in Human-Computer Collaboration, Proc. of the Conference on User Modelling,</booktitle>
<publisher>Springer</publisher>
<location>Banff, Canada, NY:</location>
<contexts>
<context position="18755" citStr="[14]" startWordPosition="2841" endWordPosition="2841">rer Text to Speech Microphone Camera Animation Speech Figure5.1: System architecture Generation Module Sentence Realizer Gesture Component 8 7 6 5 4 3 2 1 inter end mid DSEG intra 5.2 The Collagen dialogue manager CollagenTM is JAVA middleware for building COLLAborative interface AGENts to work with users on interface applications. Collagen is designed with the capability to participate in collaboration and conversation, based on [12], [16]. Collagen updates the focus stack and recipe tree using a combination of the discourse interpretation algorithm of [16] and plan recognition algorithms of [14]. It takes as input user and system utterances and interface actions, and accesses a library of recipes describing actions in the domain. After updating the discourse state, Collagen makes three resources available to the interface agent: focus of attention (using the focus stack), segmented interaction history (of completed segments) and an agenda of next possible actions created from the focus stack and recipe tree. 5.3 Output Generation The Reaction Module works as a content planner in the Rea architecture, and also plays the role of an interface agent in Collagen. It has access to the disc</context>
</contexts>
<marker>[14]</marker>
<rawString>Lesh, N., Rich, C., &amp; Sidner, C., Using Plan Recognition in Human-Computer Collaboration, Proc. of the Conference on User Modelling, Banff, Canada, NY: Springer Wien, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lester</author>
<author>S Towns</author>
<author>C Callaway</author>
<author>J Voerman</author>
<author>P FitzGerald</author>
</authors>
<title>Deictic and Emotive Communication in Animated Pedagogical Agents, in Embodied Conversational Agents,</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge:</location>
<contexts>
<context position="3702" citStr="[15]" startWordPosition="527" endWordPosition="527">ven when gestural content overlaps with speech (reported to be the case in roughly 50% of utterances, for descriptive discourse), gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech. In fact, the semantic and pragmatic compatibility in the gesture-speech relationship recalls the interaction of words and graphics in multimodal presentations [11]. On the basis of results such as these, several researchers have built animated embodied conversational agents that ally synthesized speech with animated hand gestures. For example, Lester et al. [15] generate deictic gestures and choose referring expressions as a function of the potential ambiguity and proximity of objects referred to. Rickel and Johnson [19]&apos;s pedagogical agent produces a deictic gesture at the beginning of explanations about objects. André et al. [1] generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a sub-action of elaborating. Cassell and Stone [3] generate either speech, gesture, or a combination of the two, as a function of the information structure status and surprise value of the discourse entity. Head and eye movement has als</context>
</contexts>
<marker>[15]</marker>
<rawString>Lester, J., Towns, S., Callaway, C., Voerman, J., &amp; FitzGerald, P., Deictic and Emotive Communication in Animated Pedagogical Agents, in Embodied Conversational Agents, J. Cassell, J. Sullivan, et. al, Eds. Cambridge: MIT Press, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lochbaum</author>
</authors>
<title>A Collaborative Planning Model of Intentional Structure,</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>525--572</pages>
<contexts>
<context position="18595" citStr="[16]" startWordPosition="2817" endWordPosition="2817"> information beat_and other_hand_gsts Dialogue Manager (Collagen) Reaction Module (RM) Understanding Module Speech Recognition Vision Processing Animation Renderer Text to Speech Microphone Camera Animation Speech Figure5.1: System architecture Generation Module Sentence Realizer Gesture Component 8 7 6 5 4 3 2 1 inter end mid DSEG intra 5.2 The Collagen dialogue manager CollagenTM is JAVA middleware for building COLLAborative interface AGENts to work with users on interface applications. Collagen is designed with the capability to participate in collaboration and conversation, based on [12], [16]. Collagen updates the focus stack and recipe tree using a combination of the discourse interpretation algorithm of [16] and plan recognition algorithms of [14]. It takes as input user and system utterances and interface actions, and accesses a library of recipes describing actions in the domain. After updating the discourse state, Collagen makes three resources available to the interface agent: focus of attention (using the focus stack), segmented interaction history (of completed segments) and an agenda of next possible actions created from the focus stack and recipe tree. 5.3 Output Generat</context>
</contexts>
<marker>[16]</marker>
<rawString>Lochbaum, K., A Collaborative Planning Model of Intentional Structure, Computational Linguistics, vol. 24, pp. 525-572, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Hand and Mind: What Gestures Reveal about Thought.</title>
<date>1992</date>
<publisher>The University of Chicago Press,</publisher>
<location>Chicago, IL/London, UK:</location>
<contexts>
<context position="2714" citStr="[17]" startWordPosition="378" endWordPosition="378">g discourse structure in speech-understanding systems. Previous work, however, has not addressed major body shifts during discourse, nor has it addressed the nonverbal correlates of topic shifts. 2. Background Only recently have computational linguists begun to examine the association of nonverbal behaviors and language. In this section we review research by non-computational linguists and discuss how this research has been employed to formulate algorithms for natural language generation or understanding. About three-quarters of all clauses in descriptive discourse are accompanied by gestures [17], and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech [13]. It has been shown that when speech is ambiguous or in a speech situation with some noise, listeners rely on gestural cues [22] (and, the higher the noise-tosignal ratio, the more facilitation by gesture). Even when gestural content overlaps with speech (reported to be the case in roughly 50% of utterances, for descriptive discourse), gesture often emphasizes information that is also focused pragmatically by mechanisms </context>
</contexts>
<marker>[17]</marker>
<rawString>McNeill, D., Hand and Mind: What Gestures Reveal about Thought. Chicago, IL/London, UK: The University of Chicago Press, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rich</author>
<author>C L Sidner</author>
</authors>
<title>COLLAGEN: A Collaboration Manager for Software Interface Agents, User Modeling and User-Adapted Interaction,</title>
<date>1998</date>
<volume>8</volume>
<pages>315--350</pages>
<contexts>
<context position="1302" citStr="[18]" startWordPosition="183" endWordPosition="183">ests that postural shifts can be predicted as a function of discourse state in monologues, and discourse and conversation state in dialogues. On the basis of these findings, we have implemented an embodied conversational agent that uses Collagen in such a way as to generate postural shifts. 1. Introduction This paper provides empirical support for the relationship between posture shifts and discourse structure, and then derives an algorithm for generating posture shifts in an animated embodied conversational agent from discourse states produced by the middleware architecture known as Collagen [18]. Other nonverbal behaviors have been shown to be correlated with the underlying conversational structure and information structure of discourse. For example, gaze shifts towards the listener correlate with a shift in conversational turn (from the conversational participants’ perspective, they can be seen as a signal that the floor is available). Gestures correlate with rhematic content in accompanying language (from the conversational participants’ perspective, these behaviors can be seen as a signal that accompanying speech is of high interest). A better understanding of the role of nonverba</context>
</contexts>
<marker>[18]</marker>
<rawString>Rich, C. &amp; Sidner, C. L., COLLAGEN: A Collaboration Manager for Software Interface Agents, User Modeling and User-Adapted Interaction, vol. 8, pp. 315-350, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rickel</author>
<author>W L Johnson</author>
</authors>
<title>Task-Oriented Collaboration with Embodied Agents in Virtual Worlds,</title>
<date>2000</date>
<journal>in Embodied Conversational Agents, J. Cassell, Ed.</journal>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="3864" citStr="[19]" startWordPosition="551" endWordPosition="551">tion that is also focused pragmatically by mechanisms like prosody in speech. In fact, the semantic and pragmatic compatibility in the gesture-speech relationship recalls the interaction of words and graphics in multimodal presentations [11]. On the basis of results such as these, several researchers have built animated embodied conversational agents that ally synthesized speech with animated hand gestures. For example, Lester et al. [15] generate deictic gestures and choose referring expressions as a function of the potential ambiguity and proximity of objects referred to. Rickel and Johnson [19]&apos;s pedagogical agent produces a deictic gesture at the beginning of explanations about objects. André et al. [1] generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a sub-action of elaborating. Cassell and Stone [3] generate either speech, gesture, or a combination of the two, as a function of the information structure status and surprise value of the discourse entity. Head and eye movement has also been examined in the context of discourse and conversation. Looking away from one’s interlocutor has been correlated with the beginning of turns. From the speak</context>
</contexts>
<marker>[19]</marker>
<rawString>Rickel, J. &amp; Johnson, W. L., Task-Oriented Collaboration with Embodied Agents in Virtual Worlds, in Embodied Conversational Agents, J. Cassell, Ed. Cambridge, MA: MIT Press, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sidner</author>
</authors>
<title>An Artificial Discourse Language for Collaborative Negotiation,</title>
<date>1994</date>
<booktitle>Proc. of 12th Intnl. Conf. on Artificial Intelligence (AAAI),</booktitle>
<publisher>MIT Press,</publisher>
<location>Seattle, WA,</location>
<contexts>
<context position="23337" citStr="[20]" startWordPosition="3580" endWordPosition="3580">erance change b change keep turn lowertopic - - - 0 c continue take turn 0.13/int low default upper or lower d continue keep turn 0.14/sec low short lower End of the e D2 finish T2 give turn 0.04/int high long lower utterance topic f continue give turn 0.11/int low default lower Table 5.3.1:Posture Decision Probabilities for Dialogue First, (T1) is judged by referring to the dialogue history1. The dialogue history stores both system utterances and user utterances that occurred in the dialogue. In the history, each utterance is stored as a logical form based on an artificial discourse language [20]. As shown above in utterance (1), the first argument of the action indicates the speaker of the utterance; in this example, it is “agent”. The turn boundary can be estimated by comparing the speaker of the previous utterance with the speaker of the next utterance. If the speaker of the previous utterance is not Rea, there is a turn boundary before the next utterance (T1 = take turn). If the speaker of the previous utterance is Rea, that means that Rea will keep the same turn for the next utterance (T1 = keep turn). Second, (T2) is judged by looking at the type of Rea’s next utterance. For exa</context>
</contexts>
<marker>[20]</marker>
<rawString>Sidner, C., An Artificial Discourse Language for Collaborative Negotiation, Proc. of 12th Intnl. Conf. on Artificial Intelligence (AAAI), Seattle, WA, MIT Press, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Takeuchi</author>
<author>K Nagao</author>
</authors>
<title>Communicative facial displays as a new conversational modality,</title>
<date>1993</date>
<booktitle>Proc. of InterCHI &apos;93,</booktitle>
<publisher>ACM,</publisher>
<location>Amsterdam, NL,</location>
<contexts>
<context position="5961" citStr="[21]" startWordPosition="884" endWordPosition="884">better explanation for gaze behavior integrates turn taking with the information structure of the propositional content of an utterance [5]. Specifically, the beginning of themes are frequently accompanied by a look-away from the hearer, and the beginning of rhemes are frequently accompanied by a look-toward the hearer. When these categories are co-temporaneous with turn construction, then they are strongly predictive of gaze behavior. Results such as these have led researchers to generate eye gaze and head movements in animated embodied conversational agents. Takeuchi and Nagao, for example, [21] generate gaze and head nod behaviors in a “talking head.” Cassell et al. [2] generate eye gaze and head nods as a function of turn taking behavior, head turns just before an utterance, and eyebrow raises as a function of emphasis. To our knowledge, research on posture shifts and other gross body movements, has not been used in the design or implementation of computational systems. In fact, although a number of conversational analysts and ethnomethodologists have described posture shifts in conversation, their studies have been qualitative in nature, and difficult to reformulate as the basis o</context>
</contexts>
<marker>[21]</marker>
<rawString>Takeuchi, A. &amp; Nagao, K., Communicative facial displays as a new conversational modality, Proc. of InterCHI &apos;93, Amsterdam, NL, ACM, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Thompson</author>
<author>D Massaro</author>
</authors>
<title>Evaluation and Integration of Speech and Pointing Gestures during Referential Understanding,</title>
<date>1986</date>
<journal>Journal of Experimental Child Psychology,</journal>
<volume>42</volume>
<pages>144--168</pages>
<contexts>
<context position="3018" citStr="[22]" startWordPosition="428" endWordPosition="428">iors and language. In this section we review research by non-computational linguists and discuss how this research has been employed to formulate algorithms for natural language generation or understanding. About three-quarters of all clauses in descriptive discourse are accompanied by gestures [17], and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech [13]. It has been shown that when speech is ambiguous or in a speech situation with some noise, listeners rely on gestural cues [22] (and, the higher the noise-tosignal ratio, the more facilitation by gesture). Even when gestural content overlaps with speech (reported to be the case in roughly 50% of utterances, for descriptive discourse), gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech. In fact, the semantic and pragmatic compatibility in the gesture-speech relationship recalls the interaction of words and graphics in multimodal presentations [11]. On the basis of results such as these, several researchers have built animated embodied conversational agents that </context>
</contexts>
<marker>[22]</marker>
<rawString>Thompson, L. and Massaro, D., Evaluation and Integration of Speech and Pointing Gestures during Referential Understanding, Journal of Experimental Child Psychology, vol. 42, pp. 144-168, 1986.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>