<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000374">
<title confidence="0.990205">
Automatic Segmentation of Multiparty Dialogue
</title>
<author confidence="0.993162">
Pei-Yun Hsueh
</author>
<affiliation confidence="0.994427">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.954485">
Edinburgh, EH8 9LW, GB
</address>
<email confidence="0.999242">
p.hsueh@ed.ac.uk
</email>
<author confidence="0.988042">
Johanna D. Moore
</author>
<affiliation confidence="0.992642">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.953869">
Edinburgh, EH8 9LW, GB
</address>
<email confidence="0.999268">
J.Moore@ed.ac.uk
</email>
<author confidence="0.992239">
Steve Renals
</author>
<affiliation confidence="0.9949055">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.954865">
Edinburgh, EH8 9LW, GB
</address>
<email confidence="0.999436">
s.renals@ed.ac.uk
</email>
<sectionHeader confidence="0.995653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999891827586207">
In this paper, we investigate the prob-
lem of automatically predicting segment
boundaries in spoken multiparty dialogue.
We extend prior work in two ways. We
first apply approaches that have been pro-
posed for predicting top-level topic shifts
to the problem of identifying subtopic
boundaries. We then explore the impact
on performance of using ASR output as
opposed to human transcription. Exam-
ination of the effect of features shows
that predicting top-level and predicting
subtopic boundaries are two distinct tasks:
(1) for predicting subtopic boundaries,
the lexical cohesion-based approach alone
can achieve competitive results, (2) for
predicting top-level boundaries, the ma-
chine learning approach that combines
lexical-cohesion and conversational fea-
tures performs best, and (3) conversational
cues, such as cue phrases and overlapping
speech, are better indicators for the top-
level prediction task. We also find that
the transcription errors inevitable in ASR
output have a negative impact on models
that combine lexical-cohesion and conver-
sational features, but do not change the
general preference of approach for the two
tasks.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998913125">
Text segmentation, i.e., determining the points at
which the topic changes in a stream of text, plays
an important role in applications such as topic
detection and tracking, summarization, automatic
genre detection and information retrieval and ex-
traction (Pevzner and Hearst, 2002). In recent
work, researchers have applied these techniques
to corpora such as newswire feeds, transcripts of
radio broadcasts, and spoken dialogues, in order
to facilitate browsing, information retrieval, and
topic detection (Allan et al., 1998; van Mulbregt
et al., 1999; Shriberg et al., 2000; Dharanipragada
et al., 2000; Blei and Moreno, 2001; Christensen
et al., 2005). In this paper, we focus on segmenta-
tion of multiparty dialogues, in particular record-
ings of small group meetings. We compare mod-
els based solely on lexical information, which are
common in approaches to automatic segmentation
of text, with models that combine lexical and con-
versational features. Because tasks as diverse as
browsing, on the one hand, and summarization, on
the other, require different levels of granularity of
segmentation, we explore the performance of our
models for two tasks: hypothesizing where ma-
jor topic changes occur and hypothesizing where
more subtle nested topic shifts occur.
In addition, because we do not wish to make the
assumption that high quality transcripts of meet-
ing records, such as those produced by human
transcribers, will be commonly available, we re-
quire algorithms that operate directly on automatic
speech recognition (ASR) output.
</bodyText>
<sectionHeader confidence="0.99768" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9967344">
Prior research on segmentation of spoken “docu-
ments” uses approaches that were developed for
text segmentation, and that are based solely on
textual cues. These include algorithms based on
lexical cohesion (Galley et al., 2003; Stokes et
al., 2004), as well as models using annotated fea-
tures (e.g., cue phrases, part-of-speech tags, coref-
erence relations) that have been determined to cor-
relate with segment boundaries (Gavalda et al.,
1997; Beeferman et al., 1999). Blei et al. (2001)
</bodyText>
<page confidence="0.996766">
273
</page>
<bodyText confidence="0.999438040816327">
and van Mulbregt et al. (1999) use topic lan-
guage models and variants of the hidden Markov
model (HMM) to identify topic segments. Recent
systems achieve good results for predicting topic
boundaries when trained and tested on human
transcriptions. For example, Stokes et al. (2004)
report an error rate (Pk) of 0.25 on segmenting
broadcast news stories using unsupervised lexical
cohesion-based approaches. However, topic seg-
mentation of multiparty dialogue seems to be a
considerably harder task. Galley et al. (2003) re-
port an error rate (Pk) of 0.319 for the task of pre-
dicting major topic segments in meetings.&apos;
Although recordings of multiparty dialogue
lack the distinct segmentation cues commonly
found in text (e.g., headings, paragraph breaks,
and other typographic cues) or news story segmen-
tation (e.g., the distinction between anchor and
interview segments), they contain conversation-
based features that may be of use for automatic
segmentation. These include silence, overlap rate,
speaker activity change (Galley et al., 2003), and
cross-speaker linking information, such as adja-
cency pairs (Zechner and Waibel, 2000). Many
of these features can be expected to be compli-
mentary. For segmenting spontaneous multiparty
dialogue into major topic segments, Galley et
al. (2003) have shown that a model integrating lex-
ical and conversation-based features outperforms
one based on solely lexical cohesion information.
However, the automatic segmentation models
in prior work were developed for predicting top-
level topic segments. In addition, compared to
read speech and two-party dialogue, multi-party
dialogues typically exhibit a considerably higher
word error rate (WER) (Morgan et al., 2003).
We expect that incorrectly recognized words will
impair the robustness of lexical cohesion-based
approaches and extraction of conversation-based
discourse cues and other features. Past research
on broadcast news story segmentation using ASR
transcription has shown performance degradation
from 5% to 38% using different evaluation metrics
(van Mulbregt et al., 1999; Shriberg et al., 2000;
Blei and Moreno, 2001). However, no prior study
has reported directly on the extent of this degra-
dation on the performance of a more subtle topic
segmentation task and in spontaneous multiparty
dialogue. In this paper, we extend prior work by
</bodyText>
<subsectionHeader confidence="0.678008">
&apos;For the definition of Pk and Wd, please refer to section
3.4.1
</subsectionHeader>
<bodyText confidence="0.99998125">
investigating the effect of using ASR output on the
models that have previously been proposed. In ad-
dition, we aim to find useful features and models
for the subtopic prediction task.
</bodyText>
<sectionHeader confidence="0.961644" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.993662">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999962866666667">
In this study, we used the ICSI meeting corpus
(LDC2004S02). Seventy-five natural meetings of
ICSI research groups were recorded using close-
talking far field head-mounted microphones and
four desktop PZM microphones. The corpus in-
cludes human transcriptions of all meetings. We
added ASR transcriptions of all 75 meetings which
were produced by Hain (2005), with an average
WER of roughly 30%.
The ASR system used a vocabulary of 50,000
words, together with a trigram language model
trained on a combination of in-domain meeting
data, related texts found by web search, conver-
sational telephone speech (CTS) transcripts and
broadcast news transcripts (about 109 words in to-
tal), resulting in a test-set perplexity of about 80.
The acoustic models comprised a set of context-
dependent hidden Markov models, using gaussian
mixture model output distributions. These were
initially trained on CTS acoustic training data, and
were adapted to the ICSI meetings domain using
maximum a posteriori (MAP) adaptation. Further
adaptation to individual speakers was achieved us-
ing vocal tract length normalization and maximum
likelihood linear regression. A four-fold cross-
validation technique was employed: four recog-
nizers were trained, with each employing 75% of
the ICSI meetings as acoustic and language model
training data, and then used to recognize the re-
maining 25% of the meetings.
</bodyText>
<subsectionHeader confidence="0.999776">
3.2 Fine-grained and coarse-grained topics
</subsectionHeader>
<bodyText confidence="0.999983666666667">
We characterize a dialogue as a sequence of top-
ical segments that may be further divided into
subtopic segments. For example, the 60 minute
meeting Bed003, whose theme is the planning of
a research project on automatic speech recognition
can be described by 4 major topics, from “open-
ing” to “general discourse features for higher lay-
ers” to “how to proceed” to “closing”. Depending
on the complexity, each topic can be further di-
vided into a number of subtopics. For example,
“how to proceed” can be subdivided to 4 subtopic
segments, “segmenting off regions of features”,
</bodyText>
<page confidence="0.995924">
274
</page>
<bodyText confidence="0.999917384615385">
“ad-hoc probabilities”, “data collection” and “ex-
perimental setup”.
Three human annotators at our site used a tai-
lored tool to perform topic segmentation in which
they could choose to decompose a topic into
subtopics, with at most three levels in the resulting
hierarchy. Topics are described to the annotators
as what people in a meeting were talking about.
Annotators were asked to provide a free text la-
bel for each topic segment; they were encour-
aged to use keywords drawn from the transcrip-
tion in these labels, and we provided some stan-
dard labels for non-content topics, such as “open-
ing” and “chitchat”, to impose consistency. For
our initial experiments with automatic segmenta-
tion at different levels of granularity, we flattened
the subtopic structure and consider only two levels
of segmentation–top-level topics and all subtopics.
To establish reliability of our annotation proce-
dure, we calculated kappa statistics between the
annotations of each pair of coders. Our analy-
sis indicates human annotators achieve κ = 0.79
agreement on top-level segment boundaries and
κ = 0.73 agreement on subtopic boundaries. The
level of agreement confirms good replicability of
the annotation procedure.
</bodyText>
<subsectionHeader confidence="0.998948">
3.3 Probabilistic models
</subsectionHeader>
<bodyText confidence="0.999972777777778">
Our goal is to investigate the impact of ASR er-
rors on the selection of features and the choice of
models for segmenting topics at different levels of
granularity. We compare two segmentation mod-
els: (1) an unsupervised lexical cohesion-based
model (LM) using solely lexical cohesion infor-
mation, and (2) feature-based combined models
(CM) that are trained on a combination of lexical
cohesion and conversational features.
</bodyText>
<subsectionHeader confidence="0.975638">
3.3.1 Lexical cohesion-based model
</subsectionHeader>
<bodyText confidence="0.982618927272727">
In this study, we use Galley et al.’s (2003)
LCSeg algorithm, a variant of TextTiling (Hearst,
1997). LCSeg hypothesizes that a major topic
shift is likely to occur where strong term repeti-
tions start and end. The algorithm works with two
adjacent analysis windows, each of a fixed size
which is empirically determined. For each utter-
ance boundary, LCSeg calculates a lexical cohe-
sion score by computing the cosine similarity at
the transition between the two windows. Low sim-
ilarity indicates low lexical cohesion, and a sharp
change in lexical cohesion score indicates a high
probability of an actual topic boundary. The prin-
cipal difference between LCSeg and TextTiling is
that LCSeg measures similarity in terms of lexical
chains (i.e., term repetitions), whereas TextTiling
computes similarity using word counts.
3.3.2 Integrating lexical and
conversation-based features
We also used machine learning approaches that
integrate features into a combined model, cast-
ing topic segmentation as a binary classification
task. Under this supervised learning scheme, a
training set in which each potential topic bound-
ary2 is labelled as either positive (POS) or neg-
ative (NEG) is used to train a classifier to pre-
dict whether each unseen example in the test set
belongs to the class POS or NEG. Our objective
here is to determine whether the advantage of in-
tegrating lexical and conversational features also
improves automatic topic segmentation at the finer
granularity of subtopic levels, as well as when
ASR transcriptions are used.
For this study, we trained decision trees (c4.5)
to learn the best indicators of topic boundaries.
We first used features extracted with the optimal
window size reported to perform best in Galley et
al. (2003) for segmenting meeting transcripts into
major topical units. In particular, this study uses
the following features: (1) lexical cohesion fea-
tures: the raw lexical cohesion score and proba-
bility of topic shift indicated by the sharpness of
change in lexical cohesion score, and (2) conver-
sational features: the number of cue phrases in
an analysis window of 5 seconds preceding and
following the potential boundary, and other inter-
actional features, including similarity of speaker
activity (measured as a change in probability dis-
tribution of number of words spoken by each
speaker) within 5 seconds preceding and follow-
ing each potential boundary, the amount of over-
lapping speech within 30 seconds following each
potential boundary, and the amount of silence be-
tween speaker turns within 30 seconds preceding
each potential boundary.
</bodyText>
<subsectionHeader confidence="0.915152">
3.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.980769714285714">
To compare to prior work, we perform a 25-
fold leave-one-out cross validation on the set of
25 ICSI meetings that were used in Galley et
2In this study, the end of each speaker turn is a potential
segment boundary. If there is a pause of more than 1 second
within a single speaker turn, the turn is divided at the begin-
ning of the pause creating a potential segment boundary.
</bodyText>
<page confidence="0.990401">
275
</page>
<bodyText confidence="0.999094285714286">
al. (2003). We repeated the procedure to eval-
uate the accuracy using the lexical cohesion and
combined models on both human and ASR tran-
scriptions. In each evaluation, we trained the au-
tomatic segmentation models for two tasks: pre-
dicting subtopic boundaries (SUB) and predicting
only top-level boundaries (TOP).
</bodyText>
<subsectionHeader confidence="0.506467">
3.4.1 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.99994725">
In order to be able to compare our results di-
rectly with previous work, we first report our re-
sults using the standard error rate metrics of Pk
and Wd. Pk (Beeferman et al., 1999) is the prob-
ability that two utterances drawn randomly from a
document (in our case, a meeting transcript) are in-
correctly identified as belonging to the same topic
segment. WindowDiff (Wd) (Pevzner and Hearst,
2002) calculates the error rate by moving a sliding
window across the meeting transcript counting the
number of times the hypothesized and reference
segment boundaries are different.
</bodyText>
<sectionHeader confidence="0.548026" genericHeader="method">
3.4.2 Baseline
</sectionHeader>
<bodyText confidence="0.999950538461538">
To compute a baseline, we follow Kan (2003)
and Hearst (1997) in using Monte Carlo simu-
lated segments. For the corpus used as training
data in the experiments, the probability of a poten-
tial topic boundary being an actual one is approxi-
mately 2.2% for all subtopic segments, and 0.69%
for top-level topic segments. Therefore, the Monte
Carlo simulation algorithm predicts that a speaker
turn is a segment boundary with these probabilities
for the two different segmentation tasks. We exe-
cuted the algorithm 10,000 times on each meeting
and averaged the scores to form the baseline for
our experiments.
</bodyText>
<sectionHeader confidence="0.776156" genericHeader="method">
3.4.3 Topline
</sectionHeader>
<bodyText confidence="0.999915090909091">
For the 24 meetings that were used in training,
we have top-level topic boundaries annotated by
coders at Columbia University (Col) and in our lab
at Edinburgh (Edi). We take the majority opinion
on each segment boundary from the Col annota-
tors as reference segments. For the Edi annota-
tions of top-level topic segments, where multiple
annotations exist, we choose one randomly. The
topline is then computed as the Pk score compar-
ing the Col majority annotation to the Edi annota-
tion.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.9364645">
4.1 Experiment 1: Predicting top-level and
subtopic segment boundaries
</subsectionHeader>
<bodyText confidence="0.999414434782609">
The meetings in the ICSI corpus last approxi-
mately 1 hour and have an average of 8-10 top-
level topic segments. In order to facilitate meet-
ing browsing and question-answering, we believe
it is useful to include subtopic boundaries in or-
der to narrow in more accurately on the portion
of the meeting that contains the information the
user needs. Therefore, we performed experiments
aimed at analysing how the LM and CM seg-
mentation models behave in predicting segment
boundaries at the two different levels of granular-
ity.
All of the results are reported on the test set.
Table 1 shows the performance of the lexical co-
hesion model (LM) and the combined model (CM)
integrating the lexical cohesion and conversational
features discussed in Section 3.3.2.3 For the task
of predicting top-level topic boundaries from hu-
man transcripts, CM outperforms LM. LM tends
to over-predict on the top-level, resulting in a
higher false alarm rate. However, for the task of
predicting subtopic shifts, LM alone is consider-
ably better than CM.
</bodyText>
<table confidence="0.999746">
Error Rate Transcript ASR
Models Pk Wd Pk Wd
LM SUB 32.31% 38.18% 32.91% 37.13%
(LCSeg) TOP 36.50% 46.57% 38.02% 48.18%
CM SUB 36.90% 38.68% 38.19% n/a
(C4.5) TOP 28.35% 29.52% 28.38% n/a
</table>
<tableCaption confidence="0.856674">
Table 1: Performance comparison ofprobabilistic
segmentation models.
</tableCaption>
<bodyText confidence="0.999295">
In order to support browsing during the meeting
or shortly thereafter, automatic topic segmentation
will have to operate on the transcriptions produced
by ASR. First note from Table 1 that the prefer-
ence of models for segmentation at the two differ-
ent levels of granularity is the same for ASR and
human transcriptions. CM is better for predicting
top-level boundaries and LM is better for predict-
ing subtopic boundaries. This suggests that these
</bodyText>
<tableCaption confidence="0.982668666666667">
3We do not report Wd scores for the combined model
(CM) on ASR output because this model predicted 0 segment
boundaries when operating on ASR output. In our experi-
ence, CM routinely underpredicted the number of segment
boundaries, and due to the nature of the Wd metric, it should
not be used when there are 0 hypothesized topic boundaries.
</tableCaption>
<page confidence="0.998045">
276
</page>
<bodyText confidence="0.984802166666666">
are two distinct tasks, regardless of whether the
system operates on human produced transcription
or ASR output. Subtopics are better characterized
by lexical cohesion, whereas top-level topic shifts
are signalled by conversational features as well as
lexical-cohesion based features.
4.1.1 Effect of feature combinations:
predicting from human transcripts
Next, we wish to determine which features in
the combined model are most effective for predict-
ing topic segments at the two levels of granularity.
Table 2 gives the average Pk for all 25 meetings
in the test set, using the features described in Sec-
tion 3.3.2. We group the features into four classes:
(1) lexical cohesion-based features (LF): including
lexical cohesion value (LCV) and estimated pos-
terior probability (LCP); (2) interaction features
(IF): the amount of overlapping speech (OVR),
the amount of silence between speaker segments
(GAP), similarity of speaker activity (ACT); (3)
cue phrase feature (CUE); and (4) all available fea-
tures (ALL). For comparison we also report the
baseline (see Section 3.4.2) generated by Monte
Carlo algorithm (MC-B). All of the models us-
ing one or more features from these classes out-
perform the baseline model. A one-way ANOVA
revealed this reliable effect on the top-level seg-
mentation (F(7,192) = 17.46,p &lt; 0.01) as well
as on the subtopic segmentation task (F(7,192) =
5.862,p &lt; 0.01).
</bodyText>
<table confidence="0.998209833333333">
TRANSCRIPT Error Rate(Pk)
Feature set SUB TOP
MC-B 46.61% 48.43%
LF(LCV+LCP) 38.13% 29.92%
IF(ACT+OVR+GAP) 38.87% 30.11%
IF+CUE 38.87% 30.11%
LF+ACT 38.70% 30.10%
LF+OVR 38.56% 29.48%
LF+GAP 38.50% 29.87%
LF+IF 38.11% 29.61%
LF+CUE 37.46% 29.18%
ALL(LF+IF+CUE) 36.90% 28.35%
</table>
<tableCaption confidence="0.973478">
Table 2: Effect of different feature combinations
</tableCaption>
<bodyText confidence="0.982737942307692">
for predicting topic boundaries from human tran-
scripts. MC-B is the randomly generated baseline.
As shown in Table 2, the best performing model
for predicting top-level segments is the one us-
ing all of the features (ALL). This is not surpris-
ing, because these were the features that Galley
et al. (2003) found to be most effective for pre-
dicting top-level segment boundaries in their com-
bined model. Looking at the results in more de-
tail, we see that when we begin with LF features
alone and add other features one by one, the only
model (other than ALL) that achieves significant4
improvement (p &lt; 0.05) over LF is LF+CUE,
the model that combines lexical cohesion features
with cue phrases.
When we look at the results for predicting
subtopic boundaries, we again see that the best
performing model is the one using all features
(ALL). Models using lexical-cohesion features
alone (LF) and lexical cohesion features with cue
phrases (LF+CUE) both yield significantly better
results than using interactional features (IF) alone
(p &lt; 0.01), or using them with cue phrase features
(IF+CUE) (p &lt; 0.01). Again, none of the interac-
tional features used in combination with LF sig-
nificantly improves performance. Indeed, adding
speaker activity change (LF+ACT) degrades the
performance (p &lt; 0.05).
Therefore, we conclude that for predicting both
top-level and subtopic boundaries from human
transcriptions, the most important features are the
lexical cohesion based features (LF), followed
by cue phrases (CUE), with interactional features
contributing to improved performance only when
used in combination with LF and CUE.
However, a closer look at the Pk scores in Ta-
ble 2, adds further evidence to our hypothesis that
predicting subtopics may be a different task from
predicting top-level topics. Subtopic shifts oc-
cur more frequently, and often without clear con-
versational cues. This is suggested by the fact
that absolute performance on subtopic prediction
degrades when any of the interactional features
are combined with the lexical cohesion features.
In contrast, the interactional features slightly im-
prove performance when predicting top-level seg-
ments. Moreover, the fact that the feature OVR
has a positive impact on the model for predicting
top-level topic boundaries, but does not improve
the model for predicting subtopic boundaries re-
veals that having less overlapping speech is a more
prominent phenomenon in major topic shifts than
</bodyText>
<footnote confidence="0.8427294">
4Because we do not wish to make assumptions about the
underlying distribution of error rates, and error rates are not
measured on an interval level, we use a non-parametric sign
test throughout these experiments to compute statistical sig-
nificance.
</footnote>
<page confidence="0.99023">
277
</page>
<bodyText confidence="0.917710227272727">
in subtopic shifts.
4.1.2 Effect of feature combinations:
predicting from ASR output
Features extracted from ASR transcripts are dis-
tinct from those extracted from human transcripts
in at least three ways: (1) incorrectly recognized
words incur erroneous lexical cohesion features
(LF), (2) incorrectly recognized words incur erro-
neous cue phrase features (CUE), and (3) the ASR
system recognizes less overlapping speech (OVR).
In contrast to the finding that integrating conver-
sational features with lexical cohesion features is
useful for prediction from human transcripts, Ta-
ble 3 shows that when operating on ASR output,
neither adding interactional nor cue phrase fea-
tures improves the performance of the model using
only lexical cohesion features. In fact, the model
using all features (ALL) is significantly worse than
the model using only lexical cohesion based fea-
tures (LF). This suggests that we must explore new
features that can lessen the perplexity introduced
by ASR outputs in order to train a better model.
</bodyText>
<table confidence="0.9987722">
ASR Error Rate(Pk)
Feature set SUB TOP
MC-B 43.41% 45.22%
LF(LCV+LCP) 36.83% 25.27%
IF(ACT+OVR+GAP) 36.83% 25.27%
IF+CUE 36.83% 25.27%
LF+GAP 36.67% 24.62%
LF+IF 36.83% 28.24%
LF+CUE 37.42% 25.27%
ALL(LF+IF+CUE) 38.19% 28.38%
</table>
<tableCaption confidence="0.928568">
Table 3: Effect of different feature combinations
for predicting topic boundaries from ASR output.
</tableCaption>
<subsectionHeader confidence="0.9667215">
4.2 Experiment 2: Statistically learned cue
phrases
</subsectionHeader>
<bodyText confidence="0.9999814">
In prior work, Galley et al. (2003) empirically
identified cue phrases that are indicators of seg-
ment boundaries, and then eliminated all cues that
had not previously been identified as cue phrases
in the literature. Here, we conduct an experiment
to explore how different ways of identifying cue
phrases can help identify useful new features for
the two boundary prediction tasks.
In each fold of the 25-fold leave-one-out cross
validation, we use a modified5 Chi-square test to
</bodyText>
<subsectionHeader confidence="0.624987">
5In order to satisfy the mathematical assumptions under-
</subsectionHeader>
<bodyText confidence="0.999970735294118">
calculate statistics for each word (unigram) and
word pair (bi-gram) that occurred in the 24 train-
ing meetings. We then rank unigrams and bigrams
according to their Chi-square scores, filtering out
those with values under 6.64, the threshold for the
Chi-square statistic at the 0.01 significance level.
The unigrams and bigrams in this ranked list are
the learned cue phrases. We then use the occur-
rence counts of cue phrases in an analysis window
around each potential topic boundary in the test
meeting as a feature.
Table 4 shows the performance of models that
use statistically learned cue phrases in their feature
sets compared with models using no cue phrase
features and Galley’s model, which only uses cue
phrases that correspond to those identified in the
literature (Col-cue). We see that for predicting
subtopics, models using the cue word features
(1gram) and the combination of cue words and bi-
grams (1+2gram) yield a 15% and 8.24% improve-
ment over models using no cue features (NOCUE)
(p &lt; 0.01) respectively, while models using only
cue phrases found in the literature (Col-cue) im-
prove performance by just 3.18%. In contrast, for
predicting top-level topics, the model using cue
phrases from the literature (Col-cue) achieves a
4.2% improvement, and this is the only model that
produces statistically significantly better results
than the model using no cue phrases (NOCUE).
The superior performance of models using statis-
tically learned cue phrases as features for predict-
ing subtopic boundaries suggests there may exist a
different set of cue phrases that serve as segmen-
tation cues for subtopic boundaries.
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999964">
As observed in the corpus of meetings, the lack
of macro-level segment units (e.g., story breaks,
paragraph breaks) makes the task of segmenting
spontaneous multiparty dialogue, such as meet-
ings, different from segmenting text or broadcast
news. Compared to the task of segmenting expos-
itory texts reported in Hearst (1997) with a 39.1%
chance of each paragraph end being a target topic
boundary, the chance of each speaker turn be-
ing a top-level or sub-topic boundary in our ICSI
corpus is just 2.2% and 0.69%. The imbalanced
class distribution has a negative effect on the per-
</bodyText>
<footnote confidence="0.99235775">
lying the test, we removed cases with an expected value that
is under a threshold (in this study, we use 1), and we apply
Yate’s correction, (|ObservedV alue − ExpectedValue |−
0.5)2/ExpectedV alue.
</footnote>
<page confidence="0.979657">
278
</page>
<table confidence="0.975631">
NOCUE Col-cue 1gram 2gram 1+2gram MC-B Topline
SUB 38.11% 36.90% 32.39% 36.86% 34.97% 46.61% n/a
TOP 29.61% 28.35% 28.95% 29.20% 29.27% 48.43% 13.48%
</table>
<tableCaption confidence="0.68481725">
Table 4: Performance of models trained with cue phrases from the literature (Col-cue) and cue phrases
learned from statistical tests, including cue words (1gram), cue word pairs (2gram), and cue phrases
composed of both words and word pairs (1+2gram). NOCUE is the model using no cue phrase features.
The Topline is the agreement of human annotators on top-level segments.
</tableCaption>
<figure confidence="0.998525117647059">
Error Rate (Pk)
0.44
0.42
0.38
0.36
0.34
0.32
0.28
0.26
0.4
0.3
TRAN−ALL
TRAN−TOP
ASR−ALL
ASR−TOP
0 10 20 30 40 50 60 70 80
Training Set Size (In meetings)
</figure>
<figureCaption confidence="0.99904">
Figure 1: Performance of the combined model
over the increase of the training set size.
</figureCaption>
<bodyText confidence="0.999979090909091">
formance of machine learning approaches. In a
pilot study, we investigated sampling techniques
that rebalance the class distribution in the train-
ing set. We found that sampling techniques pre-
viously reported in Liu et al (2004) as useful for
dealing with an imbalanced class distribution in
the task of disfluency detection and sentence seg-
mentation do not work for this particular data set.
The implicit assumption of some classifiers (such
as pruned decision trees) that the class distribution
of the test set matches that of the training set, and
that the costs of false positives and false negatives
are equivalent, may account for the failure of these
sampling techniques to yield improvements in per-
formance, when measured using Pk and Wd.
Another approach that copes with the im-
balanced class prediction problem but does not
change the natural class distribution is to increase
the size of the training set. We conducted an ex-
periment in which we incrementally increased the
training set size by randomly choosing ten meet-
ings each time until all meetings were selected.
We executed the process three times and averaged
the scores to obtain the results shown in Figure 1.
However, increasing training set size adds to the
perplexity in the training phase. We see that in-
creasing the size of the training set only improves
the accuracy of segment boundary prediction for
predicting top-level topics on ASR output. The
figure also indicates that training a model to pre-
dict top-level boundaries requires no more than fif-
teen meetings in the training set to reach a reason-
able level of performance.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999925275862069">
Discovering major topic shifts and finding nested
subtopics are essential for the success of speech
document browsing and retrieval. Meeting records
contain rich information, in both content and con-
versation behavioral form, that enable automatic
topic segmentation at different levels of granular-
ity. The current study demonstrates that the two
tasks – predicting top-level and subtopic bound-
aries – are distinct in many ways: (1) for pre-
dicting subtopic boundaries, the lexical cohesion-
based approach achieves results that are com-
petitive with the machine learning approach that
combines lexical and conversational features; (2)
for predicting top-level boundaries, the machine
learning approach performs the best; and (3) many
conversational cues, such as overlapping speech
and cue phrases discussed in the literature, are
better indicators for top-level topic shifts than
for subtopic shifts, but new features such as cue
phrases can be learned statistically for the subtopic
prediction task. Even in the presence of a rela-
tively higher word error rate, using ASR output
makes no difference to the preference of model for
the two tasks. The conversational features also did
not help improve the performance for predicting
from ASR output.
In order to further identify useful features for
automatic segmentation of meetings at different
levels of granularity, we will explore the use of
</bodyText>
<page confidence="0.991419">
279
</page>
<bodyText confidence="0.9998905">
multimodal, i.e., acoustic and visual, cues. In ad-
dition, in the current study, we only extracted fea-
tures from within the analysis windows immedi-
ately preceding and following each potential topic
boundary; we will explore models that take into
account features of longer range dependencies.
</bodyText>
<sectionHeader confidence="0.998914" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.994275222222222">
Many thanks to Jean Carletta for her invaluable
help in managing the data, and for advice and
comments on the work reported in this paper.
Thanks also to the AMI ASR group for produc-
ing the ASR transcriptions, and to the anonymous
reviewers for their helpful comments. This work
was supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811).
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940285714286">
J. Allan, J.G. Carbonell, G. Doddington, J. Yamron,
and Y. Yang. 1998. Topic detection and tracking pi-
lot study: Final report. In Proceedings of the DARPA
Broadcast News Transcription and Understanding
Workshop.
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statis-
tical models for text segmentation. Machine Learn-
ing, 34:177–210.
D. M. Blei and P. J. Moreno. 2001. Topic segmentation
with an aspect hidden Markov model. In Proceed-
ings of the 24th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. ACM Press.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2005. Maximum entropy segmentation of broad-
cast news. In Proceedings of the IEEE International
Conference on Acoustic, Speech, and Signal Pro-
cessing, Philadelphia, USA.
S. Dharanipragada, M. Franz, J.S. McCarley, K. Pap-
ineni, S. Roukos, T. Ward, and W. J. Zhu. 2000.
Statistical methods for topic segmentation. In Pro-
ceedings of the International Conference on Spoken
Language Processing, pages 516–519.
M. Galley, K. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
M. Gavalda, K. Zechner, and G. Aist. 1997. High per-
formance segmentation of spontaneous speech using
part of speech and trigger word information. In Pro-
ceedings of the Fifth ANLP Conference, pages 12–
15.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, and S. Renals. 2005. Tran-
scription of conference room meetings: an investi-
gation. In Proceedings of Interspeech.
M. Hearst. 1997. Texttiling: Segmenting text into mul-
tiparagraph subtopic passages. Computational Lin-
guistics, 25(3):527–571.
M. Kan. 2003. Automatic text summarization as
applied to information retrieval: Using indicative
and informative summaries. Ph.D. thesis, Columbia
University, New York USA.
Y. Liu, E. Shriberg, A. Stolcke, and M. Harper. 2004.
Using machine learning to cope with imbalanced
classes in natural sppech: Evidence from sentence
boundary and disfluency detection. In Proceedings
of the Intl. Conf. Spoken Language Processing.
N. Morgan, D. Baron, S. Bhagat, H. Carvey,
R. Dhillon, J. Edwards, D. Gelbart, A. Janin,
A. Krupski, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, , and C. Wooters. 2003. Meetings about meet-
ings: research at icsi on speech in multiparty conver-
sations. In Proceedings of the IEEE International
Conference on Acoustic, Speech, and Signal Pro-
cessing.
L. Pevzner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19–36.
E. Shriberg, A. Stolcke, D. Hakkani-tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Commu-
nications, 31(1-2):127–254.
N. Stokes, J. Carthy, and A.F. Smeaton. 2004. Se-
lect: a lexical cohesion based news story segmenta-
tion system. AI Communications, 17(1):3–12, Jan-
uary.
P. van Mulbregt, J. Carp, L. Gillick, S. Lowe, and
J. Yamron. 1999. Segmentation of automatically
transcribed broadcast news text. In Proceedings of
the DARPA Broadcast News Workshop, pages 77–
80. Morgan Kaufman Publishers.
Klaus Zechner and Alex Waibel. 2000. DIASUMM:
Flexible summarization of spontaneous dialogues in
unrestricted domains. In Proceedings of COLING-
2000, pages 968–974.
</reference>
<page confidence="0.997138">
280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915467">
<title confidence="0.999868">Automatic Segmentation of Multiparty Dialogue</title>
<author confidence="0.999277">Pei-Yun Hsueh</author>
<affiliation confidence="0.999966">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.997457">Edinburgh, EH8 9LW, GB</address>
<email confidence="0.984507">p.hsueh@ed.ac.uk</email>
<author confidence="0.99898">Johanna D Moore</author>
<affiliation confidence="0.9999535">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.997576">Edinburgh, EH8 9LW, GB</address>
<email confidence="0.993388">J.Moore@ed.ac.uk</email>
<author confidence="0.990314">Steve Renals</author>
<affiliation confidence="0.9999055">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.997693">Edinburgh, EH8 9LW, GB</address>
<email confidence="0.997419">s.renals@ed.ac.uk</email>
<abstract confidence="0.998530933333333">In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue. We extend prior work in two ways. We first apply approaches that have been proposed for predicting top-level topic shifts to the problem of identifying subtopic boundaries. We then explore the impact on performance of using ASR output as opposed to human transcription. Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries, the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries, the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues, such as cue phrases and overlapping speech, are better indicators for the toplevel prediction task. We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features, but do not change the general preference of approach for the two tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J G Carbonell</author>
<author>G Doddington</author>
<author>J Yamron</author>
<author>Y Yang</author>
</authors>
<title>Topic detection and tracking pilot study: Final report.</title>
<date>1998</date>
<booktitle>In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<contexts>
<context position="2040" citStr="Allan et al., 1998" startWordPosition="291" endWordPosition="294">es, but do not change the general preference of approach for the two tasks. 1 Introduction Text segmentation, i.e., determining the points at which the topic changes in a stream of text, plays an important role in applications such as topic detection and tracking, summarization, automatic genre detection and information retrieval and extraction (Pevzner and Hearst, 2002). In recent work, researchers have applied these techniques to corpora such as newswire feeds, transcripts of radio broadcasts, and spoken dialogues, in order to facilitate browsing, information retrieval, and topic detection (Allan et al., 1998; van Mulbregt et al., 1999; Shriberg et al., 2000; Dharanipragada et al., 2000; Blei and Moreno, 2001; Christensen et al., 2005). In this paper, we focus on segmentation of multiparty dialogues, in particular recordings of small group meetings. We compare models based solely on lexical information, which are common in approaches to automatic segmentation of text, with models that combine lexical and conversational features. Because tasks as diverse as browsing, on the one hand, and summarization, on the other, require different levels of granularity of segmentation, we explore the performance</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>J. Allan, J.G. Carbonell, G. Doddington, J. Yamron, and Y. Yang. 1998. Topic detection and tracking pilot study: Final report. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--177</pages>
<contexts>
<context position="3534" citStr="Beeferman et al., 1999" startWordPosition="526" endWordPosition="529">by human transcribers, will be commonly available, we require algorithms that operate directly on automatic speech recognition (ASR) output. 2 Previous Work Prior research on segmentation of spoken “documents” uses approaches that were developed for text segmentation, and that are based solely on textual cues. These include algorithms based on lexical cohesion (Galley et al., 2003; Stokes et al., 2004), as well as models using annotated features (e.g., cue phrases, part-of-speech tags, coreference relations) that have been determined to correlate with segment boundaries (Gavalda et al., 1997; Beeferman et al., 1999). Blei et al. (2001) 273 and van Mulbregt et al. (1999) use topic language models and variants of the hidden Markov model (HMM) to identify topic segments. Recent systems achieve good results for predicting topic boundaries when trained and tested on human transcriptions. For example, Stokes et al. (2004) report an error rate (Pk) of 0.25 on segmenting broadcast news stories using unsupervised lexical cohesion-based approaches. However, topic segmentation of multiparty dialogue seems to be a considerably harder task. Galley et al. (2003) report an error rate (Pk) of 0.319 for the task of predi</context>
<context position="13340" citStr="Beeferman et al., 1999" startWordPosition="2072" endWordPosition="2075">peaker turn, the turn is divided at the beginning of the pause creating a potential segment boundary. 275 al. (2003). We repeated the procedure to evaluate the accuracy using the lexical cohesion and combined models on both human and ASR transcriptions. In each evaluation, we trained the automatic segmentation models for two tasks: predicting subtopic boundaries (SUB) and predicting only top-level boundaries (TOP). 3.4.1 Evaluation metrics In order to be able to compare our results directly with previous work, we first report our results using the standard error rate metrics of Pk and Wd. Pk (Beeferman et al., 1999) is the probability that two utterances drawn randomly from a document (in our case, a meeting transcript) are incorrectly identified as belonging to the same topic segment. WindowDiff (Wd) (Pevzner and Hearst, 2002) calculates the error rate by moving a sliding window across the meeting transcript counting the number of times the hypothesized and reference segment boundaries are different. 3.4.2 Baseline To compute a baseline, we follow Kan (2003) and Hearst (1997) in using Monte Carlo simulated segments. For the corpus used as training data in the experiments, the probability of a potential </context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34:177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>P J Moreno</author>
</authors>
<title>Topic segmentation with an aspect hidden Markov model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="2142" citStr="Blei and Moreno, 2001" startWordPosition="308" endWordPosition="311">mentation, i.e., determining the points at which the topic changes in a stream of text, plays an important role in applications such as topic detection and tracking, summarization, automatic genre detection and information retrieval and extraction (Pevzner and Hearst, 2002). In recent work, researchers have applied these techniques to corpora such as newswire feeds, transcripts of radio broadcasts, and spoken dialogues, in order to facilitate browsing, information retrieval, and topic detection (Allan et al., 1998; van Mulbregt et al., 1999; Shriberg et al., 2000; Dharanipragada et al., 2000; Blei and Moreno, 2001; Christensen et al., 2005). In this paper, we focus on segmentation of multiparty dialogues, in particular recordings of small group meetings. We compare models based solely on lexical information, which are common in approaches to automatic segmentation of text, with models that combine lexical and conversational features. Because tasks as diverse as browsing, on the one hand, and summarization, on the other, require different levels of granularity of segmentation, we explore the performance of our models for two tasks: hypothesizing where major topic changes occur and hypothesizing where mo</context>
<context position="5677" citStr="Blei and Moreno, 2001" startWordPosition="844" endWordPosition="847">d for predicting toplevel topic segments. In addition, compared to read speech and two-party dialogue, multi-party dialogues typically exhibit a considerably higher word error rate (WER) (Morgan et al., 2003). We expect that incorrectly recognized words will impair the robustness of lexical cohesion-based approaches and extraction of conversation-based discourse cues and other features. Past research on broadcast news story segmentation using ASR transcription has shown performance degradation from 5% to 38% using different evaluation metrics (van Mulbregt et al., 1999; Shriberg et al., 2000; Blei and Moreno, 2001). However, no prior study has reported directly on the extent of this degradation on the performance of a more subtle topic segmentation task and in spontaneous multiparty dialogue. In this paper, we extend prior work by &apos;For the definition of Pk and Wd, please refer to section 3.4.1 investigating the effect of using ASR output on the models that have previously been proposed. In addition, we aim to find useful features and models for the subtopic prediction task. 3 Method 3.1 Data In this study, we used the ICSI meeting corpus (LDC2004S02). Seventy-five natural meetings of ICSI research group</context>
</contexts>
<marker>Blei, Moreno, 2001</marker>
<rawString>D. M. Blei and P. J. Moreno. 2001. Topic segmentation with an aspect hidden Markov model. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Christensen</author>
<author>B Kolluru</author>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>Maximum entropy segmentation of broadcast news.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustic, Speech, and Signal Processing,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="2169" citStr="Christensen et al., 2005" startWordPosition="312" endWordPosition="315">ining the points at which the topic changes in a stream of text, plays an important role in applications such as topic detection and tracking, summarization, automatic genre detection and information retrieval and extraction (Pevzner and Hearst, 2002). In recent work, researchers have applied these techniques to corpora such as newswire feeds, transcripts of radio broadcasts, and spoken dialogues, in order to facilitate browsing, information retrieval, and topic detection (Allan et al., 1998; van Mulbregt et al., 1999; Shriberg et al., 2000; Dharanipragada et al., 2000; Blei and Moreno, 2001; Christensen et al., 2005). In this paper, we focus on segmentation of multiparty dialogues, in particular recordings of small group meetings. We compare models based solely on lexical information, which are common in approaches to automatic segmentation of text, with models that combine lexical and conversational features. Because tasks as diverse as browsing, on the one hand, and summarization, on the other, require different levels of granularity of segmentation, we explore the performance of our models for two tasks: hypothesizing where major topic changes occur and hypothesizing where more subtle nested topic shif</context>
</contexts>
<marker>Christensen, Kolluru, Gotoh, Renals, 2005</marker>
<rawString>H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals. 2005. Maximum entropy segmentation of broadcast news. In Proceedings of the IEEE International Conference on Acoustic, Speech, and Signal Processing, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dharanipragada</author>
<author>M Franz</author>
<author>J S McCarley</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>Statistical methods for topic segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>516--519</pages>
<contexts>
<context position="2119" citStr="Dharanipragada et al., 2000" startWordPosition="304" endWordPosition="307">asks. 1 Introduction Text segmentation, i.e., determining the points at which the topic changes in a stream of text, plays an important role in applications such as topic detection and tracking, summarization, automatic genre detection and information retrieval and extraction (Pevzner and Hearst, 2002). In recent work, researchers have applied these techniques to corpora such as newswire feeds, transcripts of radio broadcasts, and spoken dialogues, in order to facilitate browsing, information retrieval, and topic detection (Allan et al., 1998; van Mulbregt et al., 1999; Shriberg et al., 2000; Dharanipragada et al., 2000; Blei and Moreno, 2001; Christensen et al., 2005). In this paper, we focus on segmentation of multiparty dialogues, in particular recordings of small group meetings. We compare models based solely on lexical information, which are common in approaches to automatic segmentation of text, with models that combine lexical and conversational features. Because tasks as diverse as browsing, on the one hand, and summarization, on the other, require different levels of granularity of segmentation, we explore the performance of our models for two tasks: hypothesizing where major topic changes occur and</context>
</contexts>
<marker>Dharanipragada, Franz, McCarley, Papineni, Roukos, Ward, Zhu, 2000</marker>
<rawString>S. Dharanipragada, M. Franz, J.S. McCarley, K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2000. Statistical methods for topic segmentation. In Proceedings of the International Conference on Spoken Language Processing, pages 516–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>E Fosler-Lussier</author>
<author>H Jing</author>
</authors>
<title>Discourse segmentation of multiparty conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3294" citStr="Galley et al., 2003" startWordPosition="488" endWordPosition="491">hesizing where major topic changes occur and hypothesizing where more subtle nested topic shifts occur. In addition, because we do not wish to make the assumption that high quality transcripts of meeting records, such as those produced by human transcribers, will be commonly available, we require algorithms that operate directly on automatic speech recognition (ASR) output. 2 Previous Work Prior research on segmentation of spoken “documents” uses approaches that were developed for text segmentation, and that are based solely on textual cues. These include algorithms based on lexical cohesion (Galley et al., 2003; Stokes et al., 2004), as well as models using annotated features (e.g., cue phrases, part-of-speech tags, coreference relations) that have been determined to correlate with segment boundaries (Gavalda et al., 1997; Beeferman et al., 1999). Blei et al. (2001) 273 and van Mulbregt et al. (1999) use topic language models and variants of the hidden Markov model (HMM) to identify topic segments. Recent systems achieve good results for predicting topic boundaries when trained and tested on human transcriptions. For example, Stokes et al. (2004) report an error rate (Pk) of 0.25 on segmenting broad</context>
<context position="4599" citStr="Galley et al., 2003" startWordPosition="690" endWordPosition="693">c segmentation of multiparty dialogue seems to be a considerably harder task. Galley et al. (2003) report an error rate (Pk) of 0.319 for the task of predicting major topic segments in meetings.&apos; Although recordings of multiparty dialogue lack the distinct segmentation cues commonly found in text (e.g., headings, paragraph breaks, and other typographic cues) or news story segmentation (e.g., the distinction between anchor and interview segments), they contain conversationbased features that may be of use for automatic segmentation. These include silence, overlap rate, speaker activity change (Galley et al., 2003), and cross-speaker linking information, such as adjacency pairs (Zechner and Waibel, 2000). Many of these features can be expected to be complimentary. For segmenting spontaneous multiparty dialogue into major topic segments, Galley et al. (2003) have shown that a model integrating lexical and conversation-based features outperforms one based on solely lexical cohesion information. However, the automatic segmentation models in prior work were developed for predicting toplevel topic segments. In addition, compared to read speech and two-party dialogue, multi-party dialogues typically exhibit a</context>
<context position="11590" citStr="Galley et al. (2003)" startWordPosition="1781" endWordPosition="1784">labelled as either positive (POS) or negative (NEG) is used to train a classifier to predict whether each unseen example in the test set belongs to the class POS or NEG. Our objective here is to determine whether the advantage of integrating lexical and conversational features also improves automatic topic segmentation at the finer granularity of subtopic levels, as well as when ASR transcriptions are used. For this study, we trained decision trees (c4.5) to learn the best indicators of topic boundaries. We first used features extracted with the optimal window size reported to perform best in Galley et al. (2003) for segmenting meeting transcripts into major topical units. In particular, this study uses the following features: (1) lexical cohesion features: the raw lexical cohesion score and probability of topic shift indicated by the sharpness of change in lexical cohesion score, and (2) conversational features: the number of cue phrases in an analysis window of 5 seconds preceding and following the potential boundary, and other interactional features, including similarity of speaker activity (measured as a change in probability distribution of number of words spoken by each speaker) within 5 seconds</context>
<context position="19027" citStr="Galley et al. (2003)" startWordPosition="2986" endWordPosition="2989">PT Error Rate(Pk) Feature set SUB TOP MC-B 46.61% 48.43% LF(LCV+LCP) 38.13% 29.92% IF(ACT+OVR+GAP) 38.87% 30.11% IF+CUE 38.87% 30.11% LF+ACT 38.70% 30.10% LF+OVR 38.56% 29.48% LF+GAP 38.50% 29.87% LF+IF 38.11% 29.61% LF+CUE 37.46% 29.18% ALL(LF+IF+CUE) 36.90% 28.35% Table 2: Effect of different feature combinations for predicting topic boundaries from human transcripts. MC-B is the randomly generated baseline. As shown in Table 2, the best performing model for predicting top-level segments is the one using all of the features (ALL). This is not surprising, because these were the features that Galley et al. (2003) found to be most effective for predicting top-level segment boundaries in their combined model. Looking at the results in more detail, we see that when we begin with LF features alone and add other features one by one, the only model (other than ALL) that achieves significant4 improvement (p &lt; 0.05) over LF is LF+CUE, the model that combines lexical cohesion features with cue phrases. When we look at the results for predicting subtopic boundaries, we again see that the best performing model is the one using all features (ALL). Models using lexical-cohesion features alone (LF) and lexical cohe</context>
<context position="22857" citStr="Galley et al. (2003)" startWordPosition="3578" endWordPosition="3581">orse than the model using only lexical cohesion based features (LF). This suggests that we must explore new features that can lessen the perplexity introduced by ASR outputs in order to train a better model. ASR Error Rate(Pk) Feature set SUB TOP MC-B 43.41% 45.22% LF(LCV+LCP) 36.83% 25.27% IF(ACT+OVR+GAP) 36.83% 25.27% IF+CUE 36.83% 25.27% LF+GAP 36.67% 24.62% LF+IF 36.83% 28.24% LF+CUE 37.42% 25.27% ALL(LF+IF+CUE) 38.19% 28.38% Table 3: Effect of different feature combinations for predicting topic boundaries from ASR output. 4.2 Experiment 2: Statistically learned cue phrases In prior work, Galley et al. (2003) empirically identified cue phrases that are indicators of segment boundaries, and then eliminated all cues that had not previously been identified as cue phrases in the literature. Here, we conduct an experiment to explore how different ways of identifying cue phrases can help identify useful new features for the two boundary prediction tasks. In each fold of the 25-fold leave-one-out cross validation, we use a modified5 Chi-square test to 5In order to satisfy the mathematical assumptions undercalculate statistics for each word (unigram) and word pair (bi-gram) that occurred in the 24 trainin</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing. 2003. Discourse segmentation of multiparty conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gavalda</author>
<author>K Zechner</author>
<author>G Aist</author>
</authors>
<title>High performance segmentation of spontaneous speech using part of speech and trigger word information.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth ANLP Conference,</booktitle>
<pages>12--15</pages>
<contexts>
<context position="3509" citStr="Gavalda et al., 1997" startWordPosition="522" endWordPosition="525">uch as those produced by human transcribers, will be commonly available, we require algorithms that operate directly on automatic speech recognition (ASR) output. 2 Previous Work Prior research on segmentation of spoken “documents” uses approaches that were developed for text segmentation, and that are based solely on textual cues. These include algorithms based on lexical cohesion (Galley et al., 2003; Stokes et al., 2004), as well as models using annotated features (e.g., cue phrases, part-of-speech tags, coreference relations) that have been determined to correlate with segment boundaries (Gavalda et al., 1997; Beeferman et al., 1999). Blei et al. (2001) 273 and van Mulbregt et al. (1999) use topic language models and variants of the hidden Markov model (HMM) to identify topic segments. Recent systems achieve good results for predicting topic boundaries when trained and tested on human transcriptions. For example, Stokes et al. (2004) report an error rate (Pk) of 0.25 on segmenting broadcast news stories using unsupervised lexical cohesion-based approaches. However, topic segmentation of multiparty dialogue seems to be a considerably harder task. Galley et al. (2003) report an error rate (Pk) of 0.</context>
</contexts>
<marker>Gavalda, Zechner, Aist, 1997</marker>
<rawString>M. Gavalda, K. Zechner, and G. Aist. 1997. High performance segmentation of spontaneous speech using part of speech and trigger word information. In Proceedings of the Fifth ANLP Conference, pages 12– 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hain</author>
<author>J Dines</author>
<author>G Garau</author>
<author>M Karafiat</author>
<author>D Moore</author>
<author>V Wan</author>
<author>R Ordelman</author>
<author>S Renals</author>
</authors>
<title>Transcription of conference room meetings: an investigation.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<marker>Hain, Dines, Garau, Karafiat, Moore, Wan, Ordelman, Renals, 2005</marker>
<rawString>T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore, V. Wan, R. Ordelman, and S. Renals. 2005. Transcription of conference room meetings: an investigation. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multiparagraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="9949" citStr="Hearst, 1997" startWordPosition="1523" endWordPosition="1524"> of the annotation procedure. 3.3 Probabilistic models Our goal is to investigate the impact of ASR errors on the selection of features and the choice of models for segmenting topics at different levels of granularity. We compare two segmentation models: (1) an unsupervised lexical cohesion-based model (LM) using solely lexical cohesion information, and (2) feature-based combined models (CM) that are trained on a combination of lexical cohesion and conversational features. 3.3.1 Lexical cohesion-based model In this study, we use Galley et al.’s (2003) LCSeg algorithm, a variant of TextTiling (Hearst, 1997). LCSeg hypothesizes that a major topic shift is likely to occur where strong term repetitions start and end. The algorithm works with two adjacent analysis windows, each of a fixed size which is empirically determined. For each utterance boundary, LCSeg calculates a lexical cohesion score by computing the cosine similarity at the transition between the two windows. Low similarity indicates low lexical cohesion, and a sharp change in lexical cohesion score indicates a high probability of an actual topic boundary. The principal difference between LCSeg and TextTiling is that LCSeg measures simi</context>
<context position="13810" citStr="Hearst (1997)" startWordPosition="2148" endWordPosition="2149">esults directly with previous work, we first report our results using the standard error rate metrics of Pk and Wd. Pk (Beeferman et al., 1999) is the probability that two utterances drawn randomly from a document (in our case, a meeting transcript) are incorrectly identified as belonging to the same topic segment. WindowDiff (Wd) (Pevzner and Hearst, 2002) calculates the error rate by moving a sliding window across the meeting transcript counting the number of times the hypothesized and reference segment boundaries are different. 3.4.2 Baseline To compute a baseline, we follow Kan (2003) and Hearst (1997) in using Monte Carlo simulated segments. For the corpus used as training data in the experiments, the probability of a potential topic boundary being an actual one is approximately 2.2% for all subtopic segments, and 0.69% for top-level topic segments. Therefore, the Monte Carlo simulation algorithm predicts that a speaker turn is a segment boundary with these probabilities for the two different segmentation tasks. We executed the algorithm 10,000 times on each meeting and averaged the scores to form the baseline for our experiments. 3.4.3 Topline For the 24 meetings that were used in trainin</context>
<context position="25319" citStr="Hearst (1997)" startWordPosition="3972" endWordPosition="3973">n the model using no cue phrases (NOCUE). The superior performance of models using statistically learned cue phrases as features for predicting subtopic boundaries suggests there may exist a different set of cue phrases that serve as segmentation cues for subtopic boundaries. 5 Discussion As observed in the corpus of meetings, the lack of macro-level segment units (e.g., story breaks, paragraph breaks) makes the task of segmenting spontaneous multiparty dialogue, such as meetings, different from segmenting text or broadcast news. Compared to the task of segmenting expository texts reported in Hearst (1997) with a 39.1% chance of each paragraph end being a target topic boundary, the chance of each speaker turn being a top-level or sub-topic boundary in our ICSI corpus is just 2.2% and 0.69%. The imbalanced class distribution has a negative effect on the perlying the test, we removed cases with an expected value that is under a threshold (in this study, we use 1), and we apply Yate’s correction, (|ObservedV alue − ExpectedValue |− 0.5)2/ExpectedV alue. 278 NOCUE Col-cue 1gram 2gram 1+2gram MC-B Topline SUB 38.11% 36.90% 32.39% 36.86% 34.97% 46.61% n/a TOP 29.61% 28.35% 28.95% 29.20% 29.27% 48.43%</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M. Hearst. 1997. Texttiling: Segmenting text into multiparagraph subtopic passages. Computational Linguistics, 25(3):527–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kan</author>
</authors>
<title>Automatic text summarization as applied to information retrieval: Using indicative and informative summaries.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University,</institution>
<location>New York USA.</location>
<contexts>
<context position="13792" citStr="Kan (2003)" startWordPosition="2145" endWordPosition="2146">o compare our results directly with previous work, we first report our results using the standard error rate metrics of Pk and Wd. Pk (Beeferman et al., 1999) is the probability that two utterances drawn randomly from a document (in our case, a meeting transcript) are incorrectly identified as belonging to the same topic segment. WindowDiff (Wd) (Pevzner and Hearst, 2002) calculates the error rate by moving a sliding window across the meeting transcript counting the number of times the hypothesized and reference segment boundaries are different. 3.4.2 Baseline To compute a baseline, we follow Kan (2003) and Hearst (1997) in using Monte Carlo simulated segments. For the corpus used as training data in the experiments, the probability of a potential topic boundary being an actual one is approximately 2.2% for all subtopic segments, and 0.69% for top-level topic segments. Therefore, the Monte Carlo simulation algorithm predicts that a speaker turn is a segment boundary with these probabilities for the two different segmentation tasks. We executed the algorithm 10,000 times on each meeting and averaged the scores to form the baseline for our experiments. 3.4.3 Topline For the 24 meetings that we</context>
</contexts>
<marker>Kan, 2003</marker>
<rawString>M. Kan. 2003. Automatic text summarization as applied to information retrieval: Using indicative and informative summaries. Ph.D. thesis, Columbia University, New York USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>M Harper</author>
</authors>
<title>Using machine learning to cope with imbalanced classes in natural sppech: Evidence from sentence boundary and disfluency detection.</title>
<date>2004</date>
<booktitle>In Proceedings of the Intl. Conf. Spoken Language Processing.</booktitle>
<contexts>
<context position="26771" citStr="Liu et al (2004)" startWordPosition="4214" endWordPosition="4217">d word pairs (1+2gram). NOCUE is the model using no cue phrase features. The Topline is the agreement of human annotators on top-level segments. Error Rate (Pk) 0.44 0.42 0.38 0.36 0.34 0.32 0.28 0.26 0.4 0.3 TRAN−ALL TRAN−TOP ASR−ALL ASR−TOP 0 10 20 30 40 50 60 70 80 Training Set Size (In meetings) Figure 1: Performance of the combined model over the increase of the training set size. formance of machine learning approaches. In a pilot study, we investigated sampling techniques that rebalance the class distribution in the training set. We found that sampling techniques previously reported in Liu et al (2004) as useful for dealing with an imbalanced class distribution in the task of disfluency detection and sentence segmentation do not work for this particular data set. The implicit assumption of some classifiers (such as pruned decision trees) that the class distribution of the test set matches that of the training set, and that the costs of false positives and false negatives are equivalent, may account for the failure of these sampling techniques to yield improvements in performance, when measured using Pk and Wd. Another approach that copes with the imbalanced class prediction problem but does</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Harper, 2004</marker>
<rawString>Y. Liu, E. Shriberg, A. Stolcke, and M. Harper. 2004. Using machine learning to cope with imbalanced classes in natural sppech: Evidence from sentence boundary and disfluency detection. In Proceedings of the Intl. Conf. Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Morgan</author>
<author>D Baron</author>
<author>S Bhagat</author>
<author>H Carvey</author>
<author>R Dhillon</author>
<author>J Edwards</author>
<author>D Gelbart</author>
<author>A Janin</author>
<author>A Krupski</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Meetings about meetings: research at icsi on speech in multiparty conversations.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustic, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="5263" citStr="Morgan et al., 2003" startWordPosition="786" endWordPosition="789">h as adjacency pairs (Zechner and Waibel, 2000). Many of these features can be expected to be complimentary. For segmenting spontaneous multiparty dialogue into major topic segments, Galley et al. (2003) have shown that a model integrating lexical and conversation-based features outperforms one based on solely lexical cohesion information. However, the automatic segmentation models in prior work were developed for predicting toplevel topic segments. In addition, compared to read speech and two-party dialogue, multi-party dialogues typically exhibit a considerably higher word error rate (WER) (Morgan et al., 2003). We expect that incorrectly recognized words will impair the robustness of lexical cohesion-based approaches and extraction of conversation-based discourse cues and other features. Past research on broadcast news story segmentation using ASR transcription has shown performance degradation from 5% to 38% using different evaluation metrics (van Mulbregt et al., 1999; Shriberg et al., 2000; Blei and Moreno, 2001). However, no prior study has reported directly on the extent of this degradation on the performance of a more subtle topic segmentation task and in spontaneous multiparty dialogue. In t</context>
</contexts>
<marker>Morgan, Baron, Bhagat, Carvey, Dhillon, Edwards, Gelbart, Janin, Krupski, Peskin, Pfau, Shriberg, Stolcke, 2003</marker>
<rawString>N. Morgan, D. Baron, S. Bhagat, H. Carvey, R. Dhillon, J. Edwards, D. Gelbart, A. Janin, A. Krupski, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, , and C. Wooters. 2003. Meetings about meetings: research at icsi on speech in multiparty conversations. In Proceedings of the IEEE International Conference on Acoustic, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pevzner</author>
<author>M Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="1795" citStr="Pevzner and Hearst, 2002" startWordPosition="256" endWordPosition="259"> as cue phrases and overlapping speech, are better indicators for the toplevel prediction task. We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features, but do not change the general preference of approach for the two tasks. 1 Introduction Text segmentation, i.e., determining the points at which the topic changes in a stream of text, plays an important role in applications such as topic detection and tracking, summarization, automatic genre detection and information retrieval and extraction (Pevzner and Hearst, 2002). In recent work, researchers have applied these techniques to corpora such as newswire feeds, transcripts of radio broadcasts, and spoken dialogues, in order to facilitate browsing, information retrieval, and topic detection (Allan et al., 1998; van Mulbregt et al., 1999; Shriberg et al., 2000; Dharanipragada et al., 2000; Blei and Moreno, 2001; Christensen et al., 2005). In this paper, we focus on segmentation of multiparty dialogues, in particular recordings of small group meetings. We compare models based solely on lexical information, which are common in approaches to automatic segmentati</context>
<context position="13556" citStr="Pevzner and Hearst, 2002" startWordPosition="2107" endWordPosition="2110">s on both human and ASR transcriptions. In each evaluation, we trained the automatic segmentation models for two tasks: predicting subtopic boundaries (SUB) and predicting only top-level boundaries (TOP). 3.4.1 Evaluation metrics In order to be able to compare our results directly with previous work, we first report our results using the standard error rate metrics of Pk and Wd. Pk (Beeferman et al., 1999) is the probability that two utterances drawn randomly from a document (in our case, a meeting transcript) are incorrectly identified as belonging to the same topic segment. WindowDiff (Wd) (Pevzner and Hearst, 2002) calculates the error rate by moving a sliding window across the meeting transcript counting the number of times the hypothesized and reference segment boundaries are different. 3.4.2 Baseline To compute a baseline, we follow Kan (2003) and Hearst (1997) in using Monte Carlo simulated segments. For the corpus used as training data in the experiments, the probability of a potential topic boundary being an actual one is approximately 2.2% for all subtopic segments, and 0.69% for top-level topic segments. Therefore, the Monte Carlo simulation algorithm predicts that a speaker turn is a segment bo</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>L. Pevzner and M. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Hakkani-tur</author>
<author>G Tur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics.</title>
<date>2000</date>
<journal>Speech Communications,</journal>
<pages>31--1</pages>
<contexts>
<context position="2090" citStr="Shriberg et al., 2000" startWordPosition="300" endWordPosition="303"> approach for the two tasks. 1 Introduction Text segmentation, i.e., determining the points at which the topic changes in a stream of text, plays an important role in applications such as topic detection and tracking, summarization, automatic genre detection and information retrieval and extraction (Pevzner and Hearst, 2002). In recent work, researchers have applied these techniques to corpora such as newswire feeds, transcripts of radio broadcasts, and spoken dialogues, in order to facilitate browsing, information retrieval, and topic detection (Allan et al., 1998; van Mulbregt et al., 1999; Shriberg et al., 2000; Dharanipragada et al., 2000; Blei and Moreno, 2001; Christensen et al., 2005). In this paper, we focus on segmentation of multiparty dialogues, in particular recordings of small group meetings. We compare models based solely on lexical information, which are common in approaches to automatic segmentation of text, with models that combine lexical and conversational features. Because tasks as diverse as browsing, on the one hand, and summarization, on the other, require different levels of granularity of segmentation, we explore the performance of our models for two tasks: hypothesizing where </context>
<context position="5653" citStr="Shriberg et al., 2000" startWordPosition="840" endWordPosition="843">rior work were developed for predicting toplevel topic segments. In addition, compared to read speech and two-party dialogue, multi-party dialogues typically exhibit a considerably higher word error rate (WER) (Morgan et al., 2003). We expect that incorrectly recognized words will impair the robustness of lexical cohesion-based approaches and extraction of conversation-based discourse cues and other features. Past research on broadcast news story segmentation using ASR transcription has shown performance degradation from 5% to 38% using different evaluation metrics (van Mulbregt et al., 1999; Shriberg et al., 2000; Blei and Moreno, 2001). However, no prior study has reported directly on the extent of this degradation on the performance of a more subtle topic segmentation task and in spontaneous multiparty dialogue. In this paper, we extend prior work by &apos;For the definition of Pk and Wd, please refer to section 3.4.1 investigating the effect of using ASR output on the models that have previously been proposed. In addition, we aim to find useful features and models for the subtopic prediction task. 3 Method 3.1 Data In this study, we used the ICSI meeting corpus (LDC2004S02). Seventy-five natural meeting</context>
</contexts>
<marker>Shriberg, Stolcke, Hakkani-tur, Tur, 2000</marker>
<rawString>E. Shriberg, A. Stolcke, D. Hakkani-tur, and G. Tur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communications, 31(1-2):127–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stokes</author>
<author>J Carthy</author>
<author>A F Smeaton</author>
</authors>
<title>Select: a lexical cohesion based news story segmentation system.</title>
<date>2004</date>
<journal>AI Communications,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="3316" citStr="Stokes et al., 2004" startWordPosition="492" endWordPosition="495">topic changes occur and hypothesizing where more subtle nested topic shifts occur. In addition, because we do not wish to make the assumption that high quality transcripts of meeting records, such as those produced by human transcribers, will be commonly available, we require algorithms that operate directly on automatic speech recognition (ASR) output. 2 Previous Work Prior research on segmentation of spoken “documents” uses approaches that were developed for text segmentation, and that are based solely on textual cues. These include algorithms based on lexical cohesion (Galley et al., 2003; Stokes et al., 2004), as well as models using annotated features (e.g., cue phrases, part-of-speech tags, coreference relations) that have been determined to correlate with segment boundaries (Gavalda et al., 1997; Beeferman et al., 1999). Blei et al. (2001) 273 and van Mulbregt et al. (1999) use topic language models and variants of the hidden Markov model (HMM) to identify topic segments. Recent systems achieve good results for predicting topic boundaries when trained and tested on human transcriptions. For example, Stokes et al. (2004) report an error rate (Pk) of 0.25 on segmenting broadcast news stories usin</context>
</contexts>
<marker>Stokes, Carthy, Smeaton, 2004</marker>
<rawString>N. Stokes, J. Carthy, and A.F. Smeaton. 2004. Select: a lexical cohesion based news story segmentation system. AI Communications, 17(1):3–12, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P van Mulbregt</author>
<author>J Carp</author>
<author>L Gillick</author>
<author>S Lowe</author>
<author>J Yamron</author>
</authors>
<title>Segmentation of automatically transcribed broadcast news text.</title>
<date>1999</date>
<booktitle>In Proceedings of the DARPA Broadcast News Workshop,</booktitle>
<pages>77--80</pages>
<publisher>Morgan Kaufman Publishers.</publisher>
<marker>van Mulbregt, Carp, Gillick, Lowe, Yamron, 1999</marker>
<rawString>P. van Mulbregt, J. Carp, L. Gillick, S. Lowe, and J. Yamron. 1999. Segmentation of automatically transcribed broadcast news text. In Proceedings of the DARPA Broadcast News Workshop, pages 77– 80. Morgan Kaufman Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Alex Waibel</author>
</authors>
<title>DIASUMM: Flexible summarization of spontaneous dialogues in unrestricted domains.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING2000,</booktitle>
<pages>968--974</pages>
<contexts>
<context position="4690" citStr="Zechner and Waibel, 2000" startWordPosition="703" endWordPosition="706">t al. (2003) report an error rate (Pk) of 0.319 for the task of predicting major topic segments in meetings.&apos; Although recordings of multiparty dialogue lack the distinct segmentation cues commonly found in text (e.g., headings, paragraph breaks, and other typographic cues) or news story segmentation (e.g., the distinction between anchor and interview segments), they contain conversationbased features that may be of use for automatic segmentation. These include silence, overlap rate, speaker activity change (Galley et al., 2003), and cross-speaker linking information, such as adjacency pairs (Zechner and Waibel, 2000). Many of these features can be expected to be complimentary. For segmenting spontaneous multiparty dialogue into major topic segments, Galley et al. (2003) have shown that a model integrating lexical and conversation-based features outperforms one based on solely lexical cohesion information. However, the automatic segmentation models in prior work were developed for predicting toplevel topic segments. In addition, compared to read speech and two-party dialogue, multi-party dialogues typically exhibit a considerably higher word error rate (WER) (Morgan et al., 2003). We expect that incorrectl</context>
</contexts>
<marker>Zechner, Waibel, 2000</marker>
<rawString>Klaus Zechner and Alex Waibel. 2000. DIASUMM: Flexible summarization of spontaneous dialogues in unrestricted domains. In Proceedings of COLING2000, pages 968–974.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>