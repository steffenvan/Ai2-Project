<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.211854">
<title confidence="0.9607815">
SurfShop: combing a product ontology with topic model results for online
window-shopping.
</title>
<author confidence="0.985544">
Zofia Stankiewicz and Satoshi Sekine
</author>
<affiliation confidence="0.991953">
Rakuten Institute of Technology, New York
</affiliation>
<address confidence="0.9934195">
215 Park Avenue South
New York, NY 10003, USA
</address>
<email confidence="0.999761">
{zofia.stankiewicz,satoshi.b.sekine}@mail.rakuten.com
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999547647058823">
At present, online shopping is typically a
search-oriented activity where a user gains ac-
cess to products which best match their query.
Instead, we propose a surf-oriented online
shopping paradigm, which links associated
products allowing users to ”wander around”
the online store and enjoy browsing a variety
of items. As an initial step in creating this ex-
perience, we constructed a prototype of an on-
line shopping interface which combines pro-
duct ontology information with topic model
results to allow users to explore items from the
food and kitchen domain. As a novel task for
topic model application, we also discuss pos-
sible approaches to the task of selecting the
best product categories to illustrate the hidden
topics discovered for our product domain.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976326086957">
Query based search remains the primary method of
access to large collections of data. However, new in-
terfacing options offered by mobile and touchscreen
applications lead to decreased reliance on typed
search queries. This trend further fuels the need for
technologies which allow users to browse and ex-
plore large amounts of data from a variety of view-
points. Online store product databases are a repre-
sentative example of such a data source. At present,
online shopping is typically a search-oriented ac-
tivity. Aside from suggestions of closely matching
products from a recommender system, internet shop-
pers have little opportunity to look around an online
store and explore a variety of related items. This ob-
servation led us to define a novel task of creating a
surf-oriented online shopping interface which facil-
itates browsing and access to multiple types of prod-
ucts. We created the prototype SurfShop application
in order to test whether we can combine knowledge
from a product ontology with topic modeling for a
better browsing experience.
Our aim is to design an application which offers
access to a variety of products, while providing a co-
herent and interesting presentation. While the pro-
duct ontology provides information on product types
which are semantically close (for example spaghetti
and penne), it does not provide information about
associations such as pasta and tomato sauce, which
may be mentioned implicitly in product descrip-
tions. In order to obtain semantically varied product
groupings from the data we integrated topic model
results into the application to display products which
are related through hidden topics.
The data used for this project consists of a snap-
shot from the product database of a Japanese Inter-
net shopping mall Rakuten Ichiba obtained in April
20111. We limited our prototype application to
the food and kitchen domain consisting of approx-
imately 4 million products. The textual information
available for each product includes a title and a short
description. Furthermore, each product is assigned
to a leaf category in the product hierarchy tree.
We use standard LDA (Blei et al., 2003) as the
topic model and our prototype can be treated as
an example of applied topic modeling. Although
there exist browsers of document collections based
</bodyText>
<footnote confidence="0.999634">
1For a version of Rakuten product data made available for
research purposes see http://rit.rakuten.co.jp/rdr/index en.html.
</footnote>
<page confidence="0.991237">
13
</page>
<note confidence="0.5794615">
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 13–16,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999994043478261">
on topic modeling 2, they have been constructed as
direct model result visualizations. In contrast, we
incorporate the LDA results into the output by com-
bining them with product category information and
search to produce a full blown application with a
topic model serving as one of its components. We
provide a more detailed overview of the entire sys-
tem in section 2.
In LDA literature, the topics discovered by the
model are typically represented by top n most prob-
able words for a given topic. In integrating topic
model results into our application we faced a chal-
lenge of creating theme pages which correspond
to hidden topics, and selecting product categories
which best illustrate a given topic. In section 3
we discuss a preliminary evaluation of the applica-
tion’s theme pages which suggests that combining
topic knowledge with ontology structure can lead to
more coherent product category groupings, and that
topic interpretation and labeling based solely on top
n words may not be sufficient for some applied tasks.
We conclude by summarizing plans for further de-
velopment of our prototype.
</bodyText>
<sectionHeader confidence="0.897056" genericHeader="method">
2 System overview
</sectionHeader>
<bodyText confidence="0.999976894736842">
The initial input to the SurfShop system consists of a
product database and a product ontology with node
labels. All products were indexed for fast retrieval
by the application3. A chart of application compo-
nents is presented in Figure1.
Raw product descriptions from our data would
constitute a large corpus including meta-data such
as shipping or manufacturer information, which are
not relevant to our task. Thus, fitting a topic model
over this corpus is not guaranteed to provide use-
ful information about related product types. There-
fore, we decided to aggregate the product informa-
tion into a collection of product category documents,
where each document corresponds to a node in the
product ontology tree (1088 nodes total). Each doc-
ument consists of sentences extracted from product
descriptions which potentially describe its relation-
ship to other product categories (based on the oc-
currence of category name labels). We can then use
</bodyText>
<footnote confidence="0.999247">
2For an example see http://www.sccs.swarthmore.edu/users/
08/ajb/tmve/wiki100k/browse/topic-list.html.
3We used an Apache Solr index and a JavaScript Ajax-Solr
library from https://github.com/evolvingweb/ajax-solr.
</footnote>
<figureCaption confidence="0.99996">
Figure 1: System overview
</figureCaption>
<bodyText confidence="0.999847375">
this artificially constructed corpus as input to LDA
to discover hidden topics in the collection4.
The topic model results, as well as product ontol-
ogy information are combined with product search
in order to build pages for our SurfShop application.
In the prototype the user can move between search,
browsing related categories, as well as browsing the-
matic product groupings. In the search mode, we use
the query and the top n search results to infer which
product category is most relevant to the query. This
allows us to display links to related category groups
next to the search results.
Given a product category, the users can also ex-
plore a related category map, such as the one shown
in Figure 2 for cheese. They can browse example
products in each related category by clicking on the
category to load product information into the right
column on the page. To provide example products, a
query is issued under the relevant ontology node us-
ing the product category label and topic keywords, to
ensure that we display items relevant to the current
page. The product browsing functionality is simi-
lar for theme pages which are discussed in the next
section.
</bodyText>
<footnote confidence="0.993144">
4For LDA we used the Lingpipe package (http://alias-
i.com/lingpipe/).
</footnote>
<page confidence="0.996973">
14
</page>
<figureCaption confidence="0.99052875">
Figure 2: Related category page example. Category and
theme labels have been translated into English.
Figure 3: Theme page fragment. Category and theme
labels have been translated into English.
</figureCaption>
<sectionHeader confidence="0.992817" genericHeader="method">
3 Theme pages
</sectionHeader>
<bodyText confidence="0.99998548">
An example of a breakfast theme page view is shown
in Figure 3. It includes clusters of product categories
which exemplify the page theme, such as bread and
jam or cheese and dairy. Each theme page corre-
sponds to a hidden topic discovered by the LDA
model5. Human interpretation of topic models has
been a focus of some recent work (Chang et al.,
2009; Newman et al., 2010; Mimno et al., 2010).
However, previous approaches concentrate on repre-
senting a topic by its top n most probable words. In
contrast, our goal is to illustrate a topic by choosing
the most representative documents from the collec-
tion, which also correspond to product categories as-
sociated with the topic. Since this is a novel task, we
decided to concentrate on the issue of building and
evaluating theme pages before conducting broader
user studies of the prototype.
There are a few possible ways to select documents
which best represent a topic. The simplest would be
to consider the rank of this topic in the document.
Alternatively, since the model provides an estimate
of topic probability given a document, the proba-
bility that a product category document belongs to
a topic could be calculated straightforwardly using
the Bayes rule6. Yet another option for finding cat-
</bodyText>
<footnote confidence="0.6342904">
5We empirically set the number of topics to 100. We re-
moved top 10% most general topics, as defined by the number
of documents which include the topic in its top 10.
6We made an additional simplifying assumption that all doc-
uments are equiprobable.
</footnote>
<bodyText confidence="0.9993254">
egories related to a given topic would be to assign
a score based on KL divergence between the topic
word multinomial and a product category multino-
mial, with the probability of each word w in the vo-
cabulary defined as follows for a given category:
</bodyText>
<equation confidence="0.9991125">
P (w) = � (P(w|ti) * P(ti|cj)) (1)
t
</equation>
<bodyText confidence="0.999988347826087">
Finally, we hypothesized that product ontology
structure may be helpful in creating the theme pages,
since if one product category is representative of
the topic, its sibling categories are also likely to be.
Conversely, if a category is the only candidate for a
given topic among its neighbors in the tree, it is less
likely to be relevant. Therefore, we clustered the
topic category candidates based on their distance in
the ontology, and retained only the clusters with the
highest average scores.
To evaluate which of the above methods is more
effective, we gave the following task to a group of
three Japanese annotators. For each topic we created
a list of category candidates which included product
categories where the topic ranked 1-3 (methods 1-3
in Table 1), top 25 Bayes score and KL divergence
score categories (methods 4 and 5), as well as the
categories based on ontology distance clusters com-
bined with the Bayes score averages for cluster reli-
ability (method 6). Each annotator was given a list
of top ten keywords for each of the topics and asked
to choose a suitable label based on the keywords.
Subsequently, they were asked to select product cat-
</bodyText>
<page confidence="0.989238">
15
</page>
<table confidence="0.999159857142857">
Scoring method Precision Recall F-score
1.Rank1 73.83% 43.21% 54.16%
2.Rank1+2 50.91% 59.56% 54.54%
3.Rank1+2+3 41.71% 73.08% 52.77%
4.Top25 KL 53.54% 70.44% 60.45%
5.Top25 Bayes 53.56% 71.25% 60.76%
6.Bayes+Ont 66.71% 69.17% 67.48%
</table>
<tableCaption confidence="0.999947">
Table 1: Result average for three annotators on Task 1.
</tableCaption>
<bodyText confidence="0.999663162162162">
egories from the candidate list which fit the topic
label they decided on.
In this manner, each annotator created their own
”golden standard” of best categories which allowed
us to compare the performance of different ap-
proaches to category selection. The amount of ac-
cepted categories varied, however a performance
comparison of candidate sets showed consistent
trends across annotators, which allows us to present
averages over annotator scores in Table 1. Rank
based selection increases in recall as lower ranks
are included but the precision of the results de-
creases. KL divergence and Bayes rule based scores
are comparable. Finally, combining the ontology
information with Bayes scoring improves the pre-
cision, while retaining the recall similar to that of
the top 25 Bayes score approach. We chose this last
method to create theme pages.
We also wanted to verify how the presence of top
topic words affects topic interpretation. In another
task, shown in Table 2, the same group of annota-
tors was presented only with product category lists
which combined method 5 and method 6 candidates
from the previous task. They were asked to assign a
topic label which summarized the majority of those
categories, as well as mark the categories which did
not fit the topic. Even though the annotators had
previously seen the same data, they tended to as-
sign broader labels than those based on the top topic
words, and included more categories as suitable for
a given topic. For example, for the breakfast theme
shown in Figure 3, one annotator labeled the topic
dairy products based on topic words, and bread and
dairy products based on the product category exam-
ples. The results of Task 2 led us to use manually
assigned theme page labels based on the product cat-
egory groupings rather than the topic keywords.
</bodyText>
<table confidence="0.990138">
Scoring method Precision Recall F-score
5.Top25 Bayes 71.28% 81.83% 76.03%
6.Bayes+Ont 84.11% 75.46% 79.38%
</table>
<tableCaption confidence="0.99932">
Table 2: Result average for three annotators on Task 2.
</tableCaption>
<bodyText confidence="0.999770571428572">
The differences in results between Task 1 and
Task 2 indicate that, while top topic keywords aid
interpretation, they may suggest a narrower theme
than the documents selected to represent the topic
and thus may not be optimal for some applications.
This underscores the need for further research on hu-
man evaluation methods for topic models.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="method">
4 Future work
</sectionHeader>
<bodyText confidence="0.9999805">
We demonstrated a prototype SurfShop system
which employs product ontology structure and LDA
model results to link associated product types and
provide an entertaining browsing experience.
In the future we plan to replace the LDA compo-
nent with a model which can directly account for the
links found through the product ontology tree, such
a version of the relational topic model (Chang and
Blei, 2009). In addition, we hope that further explo-
ration of theme page construction can contribute to
the development of topic visualization and evalua-
tion methods.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999105235294118">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn. Res.
3, 993-1022.
Jonathan Chang and David Blei. 2009. Relational topic
models for document networks. Proc. of Conf. on AI
and Statistics, 81-88.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D.
Blei. 2009. Reading tea leaves: How humans interpret
topic models. NIPS, 1-9.
David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum 2011. Optimizing
semantic coherence in topic models. EMNLP, 2011.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, 100-108.
</reference>
<page confidence="0.998662">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.218401">
<title confidence="0.6334735">SurfShop: combing a product ontology with topic model results for window-shopping.</title>
<author confidence="0.424839">Stankiewicz</author>
<affiliation confidence="0.991936">Rakuten Institute of Technology, New</affiliation>
<address confidence="0.9960985">215 Park Avenue New York, NY 10003,</address>
<abstract confidence="0.9990425">At present, online shopping is typically a search-oriented activity where a user gains access to products which best match their query. we propose a shopping paradigm, which links associated products allowing users to ”wander around” the online store and enjoy browsing a variety of items. As an initial step in creating this experience, we constructed a prototype of an online shopping interface which combines product ontology information with topic model results to allow users to explore items from the food and kitchen domain. As a novel task for topic model application, we also discuss possible approaches to the task of selecting the best product categories to illustrate the hidden topics discovered for our product domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="3195" citStr="Blei et al., 2003" startWordPosition="498" endWordPosition="501">the data we integrated topic model results into the application to display products which are related through hidden topics. The data used for this project consists of a snapshot from the product database of a Japanese Internet shopping mall Rakuten Ichiba obtained in April 20111. We limited our prototype application to the food and kitchen domain consisting of approximately 4 million products. The textual information available for each product includes a title and a short description. Furthermore, each product is assigned to a leaf category in the product hierarchy tree. We use standard LDA (Blei et al., 2003) as the topic model and our prototype can be treated as an example of applied topic modeling. Although there exist browsers of document collections based 1For a version of Rakuten product data made available for research purposes see http://rit.rakuten.co.jp/rdr/index en.html. 13 Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 13–16, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics on topic modeling 2, they have been constructed as direct model result visualizations. In contrast, we incorporate the LDA results into the output by combining them</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3, 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>David Blei</author>
</authors>
<title>Relational topic models for document networks.</title>
<date>2009</date>
<booktitle>Proc. of Conf. on AI and Statistics,</booktitle>
<pages>81--88</pages>
<marker>Chang, Blei, 2009</marker>
<rawString>Jonathan Chang and David Blei. 2009. Relational topic models for document networks. Proc. of Conf. on AI and Statistics, 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>J Boyd-Graber</author>
<author>S Gerrish</author>
<author>C Wang</author>
<author>D Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<journal>NIPS,</journal>
<pages>1--9</pages>
<contexts>
<context position="7731" citStr="Chang et al., 2009" startWordPosition="1220" endWordPosition="1223">sed the Lingpipe package (http://aliasi.com/lingpipe/). 14 Figure 2: Related category page example. Category and theme labels have been translated into English. Figure 3: Theme page fragment. Category and theme labels have been translated into English. 3 Theme pages An example of a breakfast theme page view is shown in Figure 3. It includes clusters of product categories which exemplify the page theme, such as bread and jam or cheese and dairy. Each theme page corresponds to a hidden topic discovered by the LDA model5. Human interpretation of topic models has been a focus of some recent work (Chang et al., 2009; Newman et al., 2010; Mimno et al., 2010). However, previous approaches concentrate on representing a topic by its top n most probable words. In contrast, our goal is to illustrate a topic by choosing the most representative documents from the collection, which also correspond to product categories associated with the topic. Since this is a novel task, we decided to concentrate on the issue of building and evaluating theme pages before conducting broader user studies of the prototype. There are a few possible ways to select documents which best represent a topic. The simplest would be to cons</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D. Blei. 2009. Reading tea leaves: How humans interpret topic models. NIPS, 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<publisher>EMNLP,</publisher>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum 2011. Optimizing semantic coherence in topic models. EMNLP, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>100--108</pages>
<contexts>
<context position="7752" citStr="Newman et al., 2010" startWordPosition="1224" endWordPosition="1227">kage (http://aliasi.com/lingpipe/). 14 Figure 2: Related category page example. Category and theme labels have been translated into English. Figure 3: Theme page fragment. Category and theme labels have been translated into English. 3 Theme pages An example of a breakfast theme page view is shown in Figure 3. It includes clusters of product categories which exemplify the page theme, such as bread and jam or cheese and dairy. Each theme page corresponds to a hidden topic discovered by the LDA model5. Human interpretation of topic models has been a focus of some recent work (Chang et al., 2009; Newman et al., 2010; Mimno et al., 2010). However, previous approaches concentrate on representing a topic by its top n most probable words. In contrast, our goal is to illustrate a topic by choosing the most representative documents from the collection, which also correspond to product categories associated with the topic. Since this is a novel task, we decided to concentrate on the issue of building and evaluating theme pages before conducting broader user studies of the prototype. There are a few possible ways to select documents which best represent a topic. The simplest would be to consider the rank of this</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 100-108.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>