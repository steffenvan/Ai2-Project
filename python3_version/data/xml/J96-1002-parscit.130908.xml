<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999481">
A Maximum Entropy Approach
to Natural Language Processing
</title>
<author confidence="0.999501">
Adam L. Bergert Stephen A. Della PietraI
</author>
<affiliation confidence="0.99357">
Columbia University Renaissance Technologies
</affiliation>
<author confidence="0.828869">
Vincent J. Della PietraI
</author>
<sectionHeader confidence="0.307355" genericHeader="abstract">
Renaissance Technologies
</sectionHeader>
<bodyText confidence="0.998774285714286">
The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only
recently, however, have computers become powerful enough to permit the widescale application
of this concept to real world problems in statistical estimation and pattern recognition. In this
paper, we describe a method for statistical modeling based on maximum entropy. We present
a maximum-likelihood approach for automatically constructing maximum entropy models and
describe how to implement this approach efficiently, using as examples several problems in natural
language processing.
</bodyText>
<sectionHeader confidence="0.99023" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99919715">
Statistical modeling addresses the problem of constructing a stochastic model to predict
the behavior of a random process. In constructing this model, we typically have at our
disposal a sample of output from the process. Given this sample, which represents an
incomplete state of knowledge about the process, the modeling problem is to parlay
this knowledge into a representation of the process. We can then use this representation
to make predictions about the future behavior about the process.
Baseball managers (who rank among the better paid statistical modelers) employ
batting averages, compiled from a history of at-bats, to gauge the likelihood that a
player will succeed in his next appearance at the plate. Thus informed, they manipu-
late their lineups accordingly. Wall Street speculators (who rank among the best paid
statistical modelers) build models based on past stock price movements to predict to-
morrow&apos;s fluctuations and alter their portfolios to capitalize on the predicted future.
At the other end of the pay scale reside natural language researchers, who design
language and acoustic models for use in speech recognition systems and related ap-
plications.
The past few decades have witnessed significant progress toward increasing the
predictive capacity of statistical models of natural language. In language modeling, for
instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al. (1994)
have used automatically inferred link grammars to model long range correlations in
language. In parsing, Black et al. (1992) have described how to extract grammatical
</bodyText>
<footnote confidence="0.94407325">
* This research, supported in part by ARPA under grant ONR N00014-91—C-0135, was conducted while
the authors were at the IBM T. J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY 10598
t Now at Computer Science Department, Columbia University.
t Now at Renaissance Technologies, Stony Brook, NY
</footnote>
<note confidence="0.8082955">
© 1996 Association for Computational Linguistics
Computational Linguistics Volume 22, Number 1
</note>
<bodyText confidence="0.99735295">
rules from annotated text automatically and incorporate these rules into statistical
models of grammar. In speech recognition, Lucassen and Mercer (1984) have intro-
duced a technique for automatically discovering relevant features for the translation
of word spelling to word pronunciation.
These efforts, while varied in specifics, all confront two essential tasks of statistical
modeling. The first task is to determine a set of statistics that captures the behavior of
a random process. Given a set of statistics, the second task is to corral these facts into
an accurate model of the process—a model capable of predicting the future output
of the process. The first task is one of feature selection; the second is one of model
selection. In the following pages we present a unified approach to these two tasks
based on the maximum entropy philosophy.
In Section 2 we give an overview of the maximum entropy philosophy and work
through a motivating example. In Section 3 we describe the mathematical structure of
maximum entropy models and give an efficient algorithm for estimating the parame-
ters of such models. In Section 4 we discuss feature selection, and present an automatic
method for discovering facts about a process from a sample of output from the process.
We then present a series of refinements to the method to make it practical to imple-
ment. Finally, in Section 5 we describe the application of maximum entropy ideas to
several tasks in stochastic language processing: bilingual sense disambiguation, word
reordering, and sentence segmentation.
</bodyText>
<sectionHeader confidence="0.977483" genericHeader="method">
2. A Maximum Entropy Overview
</sectionHeader>
<bodyText confidence="0.999961333333333">
We introduce the concept of maximum entropy through a simple example. Suppose we
wish to model an expert translator&apos;s decisions concerning the proper French rendering
of the English word in. Our model p of the expert&apos;s decisions assigns to each French
word or phrase f an estimate, p(f ), of the probability that the expert would choose f as
a translation of in. To guide us in developing p, we collect a large sample of instances
of the expert&apos;s decisions. Our goal is to extract a set of facts about the decision-making
process from the sample (the first task of modeling) that will aid us in constructing a
model of this process (the second task).
One obvious clue we might glean from the sample is the list of allowed trans-
lations. For example, we might discover that the expert translator always chooses
among the following five French phrases: {dans, en, a, au cours de, pendant}. With this
information in hand, we can impose our first constraint on our model p:
</bodyText>
<equation confidence="0.990104">
p(dans) + p(en) + p(a) + p(au cours de) + p(pendant) =1
</equation>
<bodyText confidence="0.999918636363636">
This equation represents our first statistic of the process; we can now proceed to
search for a suitable model that obeys this equation. Of course, there are an infinite
number of models p for which this identity holds. One model satisfying the above
equation is p(dans) = 1; in other words, the model always predicts dans. Another
model obeying this constraint predicts pendant with a probability of 1/2, and a with a
probability of 1/2. But both of these models offend our sensibilities: knowing only that
the expert always chose from among these five French phrases, how can we justify
either of these probability distributions? Each seems to be making rather bold assump-
tions, with no empirical justification. Put another way, these two models assume more
than we actually know about the expert&apos;s decision-making process. All we know is
that the expert chose exclusively from among these five French phrases; given this,
</bodyText>
<page confidence="0.996547">
40
</page>
<note confidence="0.957839">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<bodyText confidence="0.903496">
the most intuitively appealing model is the following:
</bodyText>
<equation confidence="0.9974136">
p(dans) = 1/5
p(en) = 1/5
p(a) = 1/5
p(au cours de) = 1/5
p(pendant) = 1/5
</equation>
<bodyText confidence="0.999895">
This model, which allocates the total probability evenly among the five possible
phrases, is the most uniform model subject to our knowledge. It is not, however, the
most uniform overall; that model would grant an equal probability to every possible
French phrase.
We might hope to glean more clues about the expert&apos;s decisions from our sample.
Suppose we notice that the expert chose either dims or en 30% of the time. We could
apply this knowledge to update our model of the translation process by requiring that
p satisfy two constraints:
</bodyText>
<equation confidence="0.999949">
p(dans) + p(en) = 3/10
p(dans) + p(en) + p(a) + p(au cours de) + p(pendant) = 1
</equation>
<bodyText confidence="0.9996535">
Once again there are many probability distributions consistent with these two
constraints. In the absence of any other knowledge, a reasonable choice for p is again
the most uniform—that is, the distribution which allocates its probability as evenly as
possible, subject to the constraints:
</bodyText>
<equation confidence="0.9941038">
p(dans) = 3/20
p(en) = 3/20
P(a) = 7/30
p(au cours de) = 7/30
p(pendant) = 7/30
</equation>
<bodyText confidence="0.999881666666667">
Say we inspect the data once more, and this time notice another interesting fact:
in half the cases, the expert chose either dans or a. We can incorporate this information
into our model as a third constraint:
</bodyText>
<equation confidence="0.999917333333333">
p(dans) + p(en) = 3/10
p(dans) + p(en) + p(a) + p(au cours de) + p(pendant) = 1
p(dans) + p(a) --= 1/2
</equation>
<bodyText confidence="0.999497">
We can once again look for the most uniform p satisfying these constraints, but
now the choice is not as obvious. As we have added complexity, we have encountered
two difficulties at once. First, what exactly is meant by &amp;quot;uniform,&amp;quot; and how can we
measure the uniformity of a model? Second, having determined a suitable answer to
</bodyText>
<page confidence="0.999051">
41
</page>
<note confidence="0.873366">
Computational Linguistics Volume 22, Number 1
</note>
<bodyText confidence="0.99992347826087">
these questions, how do we go about finding the most uniform model subject to a set
of constraints like those we have described?
The maximum entropy method answers both of these questions, as we will demon-
strate in the next few pages. Intuitively, the principle is simple: model all that is known
and assume nothing about that which is unknown. In other words, given a collection
of facts, choose a model consistent with all the facts, but otherwise as uniform as
possible. This is precisely the approach we took in selecting our model p at each step
in the above example.
The maximum entropy concept has a long history. Adopting the least complex
hypothesis possible is embodied in Occam&apos;s razor (&amp;quot;Nunquam ponenda est pluralitas
sine necesitate.&amp;quot;) and even appears earlier, in the Bible and the writings of Herotodus
(Jaynes 1990). Laplace might justly be considered the father of maximum entropy,
having enunciated the underlying theme 200 years ago in his &amp;quot;Principle of Insufficient
Reason:&amp;quot; when one has no information to distinguish between the probability of two
events, the best strategy is to consider them equally likely (Guiasu and Shenitzer
1985). As E. T. Jaynes, a more recent pioneer of maximum entropy, put it (Jaynes
1990):
... the fact that a certain probability distribution maximizes entropy
subject to certain constraints representing our incomplete information,
is the fundamental property which justifies use of that distribution
for inference; it agrees with everything that is known, but carefully
avoids assuming anything that is not known. It is a transcription into
mathematics of an ancient principle of wisdom ...
</bodyText>
<sectionHeader confidence="0.980731" genericHeader="method">
3. Maximum Entropy Modeling
</sectionHeader>
<bodyText confidence="0.999988">
We consider a random process that produces an output value y, a member of a finite set
Y. For the translation example just considered, the process generates a translation of the
word in, and the output y can be any word in the set {dans, en, a, au cours de, pendant}.
In generating y, the process may be influenced by some contextual information x, a
member of a finite set X. In the present example, this information could include the
words in the English sentence surrounding in.
Our task is to construct a stochastic model that accurately represents the behavior
of the random process. Such a model is a method of estimating the conditional prob-
ability that, given a context x, the process will output y. We will denote by p(yjx) the
probability that the model assigns to y in context x. With a slight abuse of notation, we
will also use p(y1x) to denote the entire conditional probability distribution provided
by the model, with the interpretation that y and x are placeholders rather than specific
instantiations. The proper interpretation should be clear from the context. We will de-
note by P the set of all conditional probability distributions. Thus a model p(y1x) is,
by definition, just an element of P.
</bodyText>
<subsectionHeader confidence="0.998221">
3.1 Training Data
</subsectionHeader>
<bodyText confidence="0.999966571428571">
To study the process, we observe the behavior of the random process for some time,
collecting a large number of samples (xi, Yl), (x2, y2),. , (xN, yN). In the example we
have been considering, each sample would consist of a phrase x containing the words
surrounding in, together with the translation y of in that the process produced. For
now, we can imagine that these training samples have been generated by a human
expert who was presented with a number of random phrases containing in and asked
to choose a good translation for each. When we discuss real-world applications in
</bodyText>
<page confidence="0.996279">
42
</page>
<note confidence="0.754121">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<bodyText confidence="0.996624142857143">
Section 5, we will show how such samples can be automatically extracted from a
bilingual corpus.
We can summarize the training sample in terms of its empirical probability distri-
bution /3, defined by
number of times that (x, y) occurs in the sample
Typically, a particular pair (x, y) will either not occur at all in the sample, or will occur
at most a few times.
</bodyText>
<subsectionHeader confidence="0.999991">
3.2 Statistics, Features and Constraints
</subsectionHeader>
<bodyText confidence="0.999978272727273">
Our goal is to construct a statistical model of the process that generated the training
sample p(x,y). The building blocks of this model will be a set of statistics of the
training sample. In the current example we have employed several such statistics: the
frequency with which in translated to either dans or en was 3/10; the frequency with
which it translated to either dans or au cours de was 1/2; and so on. These particular
statistics were independent of the context, but we could also consider statistics that
depend on the conditioning information x. For instance, we might notice that, in the
training sample, if April is the word following in, then the translation of in is en with
frequency 9/10.
To express the fact that in translates as en when April is the following word, we
can introduce the indicator function:
</bodyText>
<equation confidence="0.985499">
f (x ) 1 if y = en and April follows in
, Y =
0 otherwise
</equation>
<bodyText confidence="0.999936">
The expected value off with respect to the empirical distribution /3(x, y) is exactly the
statistic we are interested in. We denote this expected value by
</bodyText>
<equation confidence="0.9944255">
p(f)=-_Eil(X,Y)f(X/Y) (1)
x,Y
</equation>
<bodyText confidence="0.999746666666667">
We can express any statistic of the sample as the expected value of an appropriate
binary-valued indicator function f. We call such function a feature function or feature
for short. (As with probability distributions, we will sometimes abuse notation and
use f (x, y) to denote both the value of f at a particular pair (x, y) as well as the entire
function f.)
When we discover a statistic that we feel is useful, we can acknowledge its im-
portance by requiring that our model accord with it. We do this by constraining the
expected value that the model assigns to the corresponding feature function f. The
expected value off with respect to the model p(ylx) is
</bodyText>
<equation confidence="0.9935385">
p(f) Ei3(x)pcylxv(x,y) (2)
x,Y
</equation>
<bodyText confidence="0.999690666666667">
where /77(x) is the empirical distribution of x in the training sample. We constrain this
expected value to be the same as the expected value of f in the training sample. That
is, we require
</bodyText>
<equation confidence="0.999624">
P(f) = p(f) (3)
</equation>
<page confidence="0.999553">
43
</page>
<figure confidence="0.891718">
Computational Linguistics Volume 22, Number 1
Combining (1), (2) and (3) yields the more explicit equation
Ep(x)pcylxv(x,y)--- x,y P(x, (x, y)
x,y
</figure>
<bodyText confidence="0.999762076923077">
We call the requirement (3) a constraint equation or simply a constraint. By re-
stricting attention to those models p(y1x) for which (3) holds, we are eliminating from
consideration those models that do not agree with the training sample on how often
the output of the process should exhibit the feature f.
To sum up so far, we now have a means of representing statistical phenomena
inherent in a sample of data (namely, 13(f)), and also a means of requiring that our
model of the process exhibit these phenomena (namely, p(f) =1)(f)).
One final note about features and constraints bears repeating: although the words
&amp;quot;feature&amp;quot; and &amp;quot;constraint&amp;quot; are often used interchangeably in discussions of maximum
entropy, we will be vigilant in distinguishing the two and urge the reader to do
likewise. A feature is a binary-valued function of (x, y); a constraint is an equation
between the expected value of the feature function in the model and its expected
value in the training data.
</bodyText>
<subsectionHeader confidence="0.998633">
3.3 The Maximum Entropy Principle
</subsectionHeader>
<bodyText confidence="0.999276333333333">
Suppose that we are given n feature functions f;, which determine statistics we feel
are important in modeling the process. We would like our model to accord with these
statistics. That is, we would like p to lie in the subset C of P defined by
</bodyText>
<equation confidence="0.9205">
C p E &apos;P I p(fi) p(fi) for i E {1, 2, . , n } (4)
</equation>
<bodyText confidence="0.999124777777778">
Figure 1 provides a geometric interpretation of this setup. Here P is the space of all
(unconditional) probability distributions on three points, sometimes called a simplex.
If we impose no constraints (depicted in (a)), then all probability models are allowable.
Imposing one linear constraint CI restricts us to those p E P that lie on the region
defined by CI, as shown in (b). A second linear constraint could determine p exactly, if
the two constraints are satisfiable; this is the case in (c), where the intersection of CI and
C2 is non-empty. Alternatively, a second linear constraint could be inconsistent with the
first—for instance, the first might require that the probability of the first point is 1/3
and the second that the probability of the third point is 3/4—this is shown in (d). In the
present setting, however, the linear constraints are extracted from the training sample
and cannot, by construction, be inconsistent. Furthermore, the linear constraints in our
applications will not even come close to determining p E P uniquely as they do in (c);
instead, the set C C1 n C n n C of allowable models will be infinite.
Among the models p E C, the maximum entropy philosophy dictates that we select
the most uniform distribution. But now we face a question left open in Section 2: what
does &amp;quot;uniform&amp;quot; mean?
A mathematical measure of the uniformity of a conditional distribution p(ylx) is
provided by the conditional entropyl
</bodyText>
<equation confidence="0.9827585">
H(p) – Er3(x)p(ylx) log P(Ylx) (5)
x,Y
</equation>
<footnote confidence="0.956983">
1 A more common notation for the conditional entropy is H(Y X), where Y and X are random variables
with joint distribution /3 (x)p(y1x). To emphasize the dependence of the entropy on the probability
distribution p, we have adopted the alternate notation H(p).
</footnote>
<page confidence="0.99872">
44
</page>
<figure confidence="0.943659666666667">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
(a) (b)
(c) (d)
</figure>
<figureCaption confidence="0.997617">
Figure 1
</figureCaption>
<bodyText confidence="0.81892">
Four different scenarios in constrained optimization. P represents the space of all probability
distributions. In (a), no constraints are applied, and all p E P are allowable. In (b), the
constraint C1 narrows the set of allowable models to those that lie on the line defined by the
linear constraint. In (c), two consistent constraints C1 and C2 define a single model p E C1 n C2.
In (d), the two constraints are inconsistent (i.e., C1 11 C3 = 0); no p E P can satisfy them both.
The entropy is bounded from below by zero, the entropy of a model with no uncer-
tainty at all, and from above by log Y, the entropy of the uniform distribution over
all possible lyi values of y. With this definition in hand, we are ready to present the
principle of maximum entropy.
</bodyText>
<subsectionHeader confidence="0.856196">
Maximum Entropy Principle
</subsectionHeader>
<bodyText confidence="0.999842">
To select a model from a set C of allowed probability distributions,
choose the model p„ E C with maximum entropy H(p):
</bodyText>
<equation confidence="0.9947755">
p„ = argmax H(p) (6)
pEC
</equation>
<bodyText confidence="0.974373">
It can be shown that p, is always well-defined; that is, there is always a unique model
p„ with maximum entropy in any constrained set C.
</bodyText>
<subsectionHeader confidence="0.990354">
3.4 Parametric Form
</subsectionHeader>
<bodyText confidence="0.9991325">
The maximum entropy principle presents us with a problem in constrained optimiza-
tion: find the p„ E C that maximizes H(p). In simple cases, we can find the solution to
</bodyText>
<page confidence="0.997685">
45
</page>
<note confidence="0.423335">
Computational Linguistics Volume 22, Number 1
</note>
<bodyText confidence="0.999685111111111">
this problem analytically. This was true for the example presented in Section 2 when
we imposed the first two constraints on p. Unfortunately, the solution to the general
problem of maximum entropy cannot be written explicitly, and we need a more in-
direct approach. (The reader is invited to try to calculate the solution for the same
example when the third constraint is imposed.)
To address the general problem, we apply the method of Lagrange multipliers
from the theory of constrained optimization. The relevant steps are outlined here;
the reader is referred to Della Pietra et al. (1995) for a more thorough discussion of
constrained optimization as applied to maximum entropy.
</bodyText>
<listItem confidence="0.716418727272727">
• We will refer to the original constrained optimization problem,
find p* argmax H(p)
pEC
as the primal problem.
• For each feature f, we introduce a parameter A, (a Lagrange multiplier).
We define the Lagrangian A(p, A) by
A(p, A) H(p) + E Ai (P(fi) kfi)) (7)
• Holding A fixed, we compute the unconstrained maximum of the
Lagrangian A(p, A) over all p E P. We denote by pA the p where A(p, A)
achieves its maximum and by AP (A) the value at this maximum:
p), argmax A (p, A) (8)
</listItem>
<equation confidence="0.9623795">
pEP
111(A) A(pA, A) (9)
</equation>
<bodyText confidence="0.9927265">
We call W(A) the dual function. The functions pA and (A) may be
calculated explicitly using simple calculus. We find
</bodyText>
<equation confidence="0.989645333333333">
p),(yjx) ZA1(x) exp A,f,(x, y)) (10)
W(A) =— EP(x) log ZA (x) E A,13(fi) (11)
where Z(x) is a normalizing constant determined by the requirement
that E (ylx) = 1 for all X:
z,s(x) = E exp (E y)) (12)
Y
</equation>
<listItem confidence="0.9840465">
• Finally, we pose the unconstrained dual optimization problem:
Find A* = argmax W(A)
</listItem>
<page confidence="0.668051">
A
46
</page>
<note confidence="0.754109">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<bodyText confidence="0.996876">
At first glance it is not clear what these machinations achieve. However, a funda-
mental principle in the theory of Lagrange multipliers, called generically the Kuhn-
Tucker theorem, asserts that under suitable assumptions, the primal and dual problems
are, in fact, closely related. This is the case in the present situation. Although a de-
tailed account of this relationship is beyond the scope of this paper, it is easy to state
the final result: Suppose that A* is the solution of the dual problem. Then px. is the
solution of the primal problem; that is pA. In other words,
The maximum entropy model subject to the constraints C has the para-
metric form&apos; pk, of (10), where the parameter values A* can be deter-
mined by maximizing the dual function W(A).
The most important practical consequence of this result is that any algorithm for
finding the maximum A* of W(A) can be used to find the maximum p. of H(p) for
p E C.
</bodyText>
<subsectionHeader confidence="0.971747">
3.5 Relation to Maximum Likelihood
</subsectionHeader>
<bodyText confidence="0.9312345">
The log-likelihood Lp(p) of the empirical distribution p as predicted by a model p is
defined by3
</bodyText>
<equation confidence="0.9563165">
Lp(p) a log Hp(yix)ixx,0= E p(x, y) log p(y1x) (13)
x,y x,y
</equation>
<bodyText confidence="0.996437">
It is easy to check that the dual function lIi(A) of the previous section is, in fact, just
the log-likelihood for the exponential model pA; that is
</bodyText>
<equation confidence="0.98036">
W(A) = L(p) (14)
</equation>
<bodyText confidence="0.931801916666667">
With this interpretation, the result of the previous section can be rephrased as:
The model p E C with maximum entropy is the model in the para-
metric family pA(y1x) that maximizes the likelihood of the training
sample /3.
This result provides an added justification for the maximum entropy principle: If
the notion of selecting a model p* on the basis of maximum entropy isn&apos;t compelling
enough, it so happens that this same p is also the model that can best account for the
training sample, from among all models of the same parametric form (10).
Table 1 summarizes the primal-dual framework we have established.
2 It might be that the dual function W(A) does not achieve its maximum at any finite A*. In this case, the
maximum entropy model will not have the form pA for any A. However, it will be the limit of models
of this form, as indicated by the following result, whose proof we omit:
</bodyText>
<footnote confidence="0.830624666666667">
Suppose At, is any sequence such that W(An) converges to the maximum of T(A). Then pk,
converges to p,.
3 We will henceforth abbreviate L(p) by L(p) when the empirical distribution 17 is clear from context.
</footnote>
<page confidence="0.996725">
47
</page>
<note confidence="0.416752">
Computational Linguistics Volume 22, Number 1
</note>
<tableCaption confidence="0.985124">
Table 1
</tableCaption>
<bodyText confidence="0.994106">
The duality of maximum entropy and maximum likelihood is an example
of the more general phenomenon of duality in constrained optimization.
</bodyText>
<subsectionHeader confidence="0.615798">
Primal Dual
</subsectionHeader>
<bodyText confidence="0.945342333333333">
problem argmaxpEc H(p) argmaxx W (A)
description maximum entropy maximum likelihood
type of search constrained optimization unconstrained optimization
search domain p E C real-valued vectors {Al, A2 • • •}
solution A*
Kuhn-Tucker theorem: p. = p),*
</bodyText>
<subsectionHeader confidence="0.999863">
3.6 Computing the Parameters
</subsectionHeader>
<bodyText confidence="0.9999705">
For all but the most simple problems, the A* that maximize xi&apos; (A) cannot be found
analytically. Instead, we must resort to numerical methods. From the perspective of
numerical optimization, the function 4f (A) is well behaved, since it is smooth and
convex-11 in A. Consequently, a variety of numerical methods can be used to calcu-
late A*. One simple method is coordinate-wise ascent, in which A* is computed by
iteratively maximizing 41(A) one coordinate at a time. When applied to the maximum
entropy problem, this technique yields the popular Brown algorithm (Brown 1959).
Other general purpose methods that can be used to maximize T(A) include gradient
ascent and conjugate gradient.
An optimization method specifically tailored to the maximum entropy problem
is the iterative scaling algorithm of Darroch and Ratcliff (1972). We present here a
version of this algorithm specifically designed for the problem at hand; a proof of the
monotonicity and convergence of the algorithm is given in Della Pietra et al. (1995).
The algorithm is applicable whenever the feature functions f,(x,y) are nonnegative:
</bodyText>
<equation confidence="0.949494">
f, y) &gt; 0 for all i, x, and y (15)
</equation>
<bodyText confidence="0.999744666666667">
This is, of course, true for the binary-valued feature functions we are considering here.
The algorithm generalizes the Darroch-Ratcliff procedure, which requires, in addition
to the nonnegativity, that the feature functions satisfy Ei f,(x, y) = 1 for all x, y.
</bodyText>
<figureCaption confidence="0.312836">
Algorithm 1: Improved Iterative Scaling
</figureCaption>
<bodyText confidence="0.701244">
Input: Feature functions fi,f2, ...fn; empirical distribution 19(x, y)
Output: Optimal parameter values A*,; optimal model N.
</bodyText>
<listItem confidence="0.990354333333333">
1. Start with A, = 0 for all i E {1, 2, ... , n}
2. Do for each i E {1,2, ..., n}:
a. Let AA be the solution to
</listItem>
<equation confidence="0.9573805">
Ef(x)pcyixffi(x,y)exp(AA,f#(x,y)) = 13(fi)
x,y
</equation>
<bodyText confidence="0.886553">
where
</bodyText>
<equation confidence="0.954312">
f# (x,y)E--- fi(x,Y)
i=1
</equation>
<page confidence="0.983397">
48
</page>
<note confidence="0.606507">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<listItem confidence="0.8913325">
b. Update the value of A, according to: A, 4- A, + AA,
3. Go to step 2 if not all the A, have converged
</listItem>
<bodyText confidence="0.634005">
The key step in the algorithm is step (2a), the computation of the increments A A,
that solve (16). If f# (x, y) is constant (f* (x, y) = M for all x, y, say) then AA, is given
explicitly as
</bodyText>
<equation confidence="0.969368333333333">
1 Kfi)
log
M PA (fi)
</equation>
<bodyText confidence="0.961931666666667">
If f# (x, y) is not constant, then AA, must be computed numerically. A simple and
effective way of doing this is by Newton&apos;s method. This method computes the solution
a* of an equation g(a*) = 0 iteratively by the recurrence
</bodyText>
<equation confidence="0.849565">
g(an) (18)
an
g` (an)
</equation>
<bodyText confidence="0.965978">
with an appropriate choice for ao and suitable attention paid to the domain of g.
</bodyText>
<sectionHeader confidence="0.871594" genericHeader="method">
4. Feature Selection
</sectionHeader>
<bodyText confidence="0.999996166666667">
Earlier we divided the statistical modeling problem into two steps: finding appropriate
facts about the data, and incorporating these facts into the model. Up to this point we
have proceeded by assuming that the first task was somehow performed for us. Even
in the simple example of Section 2, we did not explicitly state how we selected those
particular constraints. That is, why is the fact that dans or a was chosen by the expert
translator 50% of the time any more important than countless other facts contained in
the data? In fact, the principle of maximum entropy does not directly concern itself
with the issue of feature selection, it merely provides a recipe for combining constraints
into a model. But the feature selection problem is critical, since the universe of possible
constraints is typically in the thousands or even millions. In this section we introduce a
method for automatically selecting the features to be included in a maximum entropy
model, and then offer a series of refinements to ease the computational burden.
</bodyText>
<subsectionHeader confidence="0.999666">
4.1 Motivation
</subsectionHeader>
<bodyText confidence="0.9999939375">
We begin by specifying a large collection Y of candidate features. We do not require
a priori that these features are actually relevant or useful. Instead, we let the pool be
as large as practically possible. Only a small subset of this collection of features will
eventually be employed in our final model.
If we had a training sample of infinite size, we could determine the &amp;quot;true&amp;quot; expected
value for a candidate feature f E ,T simply by computing the fraction of events in the
sample for which f (x, y) = 1. In real-life applications, however, we are provided with
only a small sample of N events, which cannot be trusted to represent the process
fully and accurately. Specifically, we cannot expect that for every feature f EF, the
estimate of /3(f) we derive from this sample will be close to its value in the limit as
n grows large. Employing a larger (or even just a different) sample of data from the
same process might result in different estimates of P(f) for many candidate features.
We would like to include in the model only a subset S of the full set of candidate
features F. We will call S the set of active features. The choice of S must capture
as much information about the random process as possible, yet only include features
whose expected values can be reliably estimated.
</bodyText>
<page confidence="0.994515">
49
</page>
<figure confidence="0.843696">
Computational Linguistics Volume 22, Number 1
</figure>
<figureCaption confidence="0.992458">
Figure 2
</figureCaption>
<bodyText confidence="0.9999248125">
A nested sequence of subsets C(Si) D C(S2) D C(S3) ... of P corresponding to increasingly
large sets of features Si C 52 C $3.
To find S, we adopt an incremental approach to feature selection, similar to the
strategy used for growing decision trees (Bahl et al. 1989). The idea is to build up S by
successively adding features. The choice of feature to add at each step is determined
by the training data. Let us denote the set of models determined by the feature set
S as C(S). &amp;quot;Adding&amp;quot; a feature f is shorthand for requiring that the set of allowable
models all satisfy the equality f9(f) = p(f). Only some members of C(S) will satisfy
this equality; the ones that do we denote by C(S U f).
Thus, each time a candidate feature is added to S, another linear constraint is
imposed on the space C(S) of models allowed by the features in S. As a result, C(S)
shrinks; the model 13* in C with the greatest entropy reflects ever-increasing knowl-
edge and thus, hopefully, becomes a more accurate representation of the process. This
narrowing of the space of permissible models was represented in Figure 1 by a series
of intersecting lines (hyperplanes, in general) in a probability simplex. Perhaps more
intuitively, we could represent it by a series of nested subsets of P, as in Figure 2.
</bodyText>
<subsectionHeader confidence="0.99991">
4.2 Basic Feature Selection
</subsectionHeader>
<bodyText confidence="0.999488">
The basic incremental growth procedure may be outlined as follows. Every stage of
the process is characterized by a set of active features S. These determine a space of
models
</bodyText>
<equation confidence="0.89329575">
C(S) fp E P I p(f) =13(f) for all f E Sl (19)
The optimal model in this space, denoted by p5, is the model with the greatest entropy:
p argmax H(p) (20)
S pEC(S)
</equation>
<page confidence="0.933573">
50
</page>
<note confidence="0.679695">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<bodyText confidence="0.771079">
By adding feature f to S, we obtain a new set of active features S U f. Following (19),
this set of features determines a set of models
C(S &apos;1)E.,- fp E P I p(f) = /3(f) for all fESU (21)
The optimal model in this space of models is
</bodyText>
<equation confidence="0.900392428571429">
PSuf argmaxH(p) (22)
p€C(Sul)
Adding the feature f allows the model p S,
uf to better account for the training sample;
this results in a gain AL(S,i) in the log-likelihood of the training data
AL (S, i) _= L(p) — L(p) (23)
At each stage of the model-construction process, our goal is to select the candidate
</equation>
<bodyText confidence="0.99486225">
feature f E .F which maximizes the gain AL(S,h; that is, we select the candidate
feature which, when adjoined to the set of active features S, produces the greatest
increase in likelihood of the training sample. This strategy is implemented in the
algorithm below.
</bodyText>
<listItem confidence="0.8791698">
Algorithm 2: Basic Feature Selection
Input: Collection j of candidate features; empirical distribution P(x, y)
Output: Set S of active features; model ps incorporating these features
1. Start with S = 0; thus ps is uniform
2. Do for each candidate feature f E F:
</listItem>
<bodyText confidence="0.561658">
Compute the model psuf using Algorithm 1
Compute the gain in the log-likelihood from adding this feature
using (23)
</bodyText>
<listItem confidence="0.9998626">
3. Check the termination condition
4. Select the feature.? with maximal gain AL(S,i)
5. Adjoin to S
6. Compute ps using Algorithm 1
7. Go to step 2
</listItem>
<bodyText confidence="0.999845857142857">
One issue left unaddressed by this algorithm is the termination condition. Obvi-
ously, we would like a condition which applies exactly when all the &amp;quot;useful&amp;quot; features
have been selected. One reasonable stopping criterion is to subject each proposed fea-
ture to cross-validation on a sample of data withheld from the initial data set. If the
feature does not lead to an increase in likelihood of the withheld sample of data,
the feature is discarded. We will have more to say about the stopping criterion in
Section 5.3.
</bodyText>
<page confidence="0.986158">
51
</page>
<note confidence="0.413664">
Computational Linguistics Volume 22, Number 1
</note>
<subsectionHeader confidence="0.997246">
4.3 Approximate Gains
</subsectionHeader>
<bodyText confidence="0.998880055555555">
Algorithm 2 is not a practical method for incremental feature selection. For each can-
didate feature f E .T. considered in step 2, we must compute the maximum entropy
model p a task that is computationally costly even with the efficient iterative scaling
algorithm &apos;introduced earlier. We therefore introduce a modification to the algorithm,
making it greedy but much more feasible. We replace the computation of the gain
AL(S,f ) of a feature f with an approximation, which we will denote by -,AL(S,f).
Recall that a model ps has a set of parameters A, one for each feature in S. The
model p contains this set of parameters, plus a single new parameter a, ,
corre-
sponding to I-.4 Given this structure, we might hope that the optimal values for A do
not change as the feature f is adjoined to S. Were this the case, imposing an addi-
tional constraint would require only optimizing the single parameter a to maximize
the likelihood. Unfortunately, when a new constraint is imposed, the optimal values
of all parameters change.
However, to make the feature-ranking computation tractable, we make the approx-
imation that the addition of a feature f affects only a, leaving the A-values associated
with other features unchanged. That is, when determining the gain off over the model
Ps&apos; we pretend that the best model containing features S U f has the form
</bodyText>
<table confidence="0.791412142857143">
1 (24)
P°s p s(y1
J = Z (x)
x)eaf(x•Y), for some real valued a
e ,
where Ze,(x) -=- Eps(yix)e-f(x,Y) (25)
Y
</table>
<bodyText confidence="0.7293615">
The only parameter distinguishing models of the form (24) is a. Among these models,
we are interested in the one that maximizes the approximate gain
</bodyText>
<equation confidence="0.962647666666667">
Gs,f (a) L(af) - UP s)
= - E p(x) log Za (x) + ap(f )
x
</equation>
<bodyText confidence="0.891858">
We will denote the gain of this model by
</bodyText>
<equation confidence="0.67565">
max Gs, f (a)
a
</equation>
<bodyText confidence="0.891412375">
and the optimal model by
suf argmax Gs j (a)
19S,1
Despite the rather unwieldy notation, the idea is simple. Computing the approxi-
mate gain in likelihood from adding feature f to ps has been reduced to a simple one-
dimensional optimization problem over the single parameter a, which can be solved
by any popular line-search technique, such as Newton&apos;s method. This yields a great
savings in computational complexity over computing the exact gain, an n-dimensional
</bodyText>
<footnote confidence="0.96313">
4 Another way to think of this is that the models psuf and ps have the same number of parameters, but
a =-- 0 for ps,
</footnote>
<page confidence="0.980081">
52
</page>
<figure confidence="0.93803325">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
(a)
L(p) (b)
Lip}
</figure>
<figureCaption confidence="0.972314">
Figure 3
</figureCaption>
<bodyText confidence="0.999913777777778">
The likelihood L(p) is a convex function of its parameters. If we start from a one-constraint
model whose optimal parameter value is A = Ao and consider the increase in Lp(p) from
adjoining a second constraint with the parameter a, the exact answer requires a search over
(A, a). We can simplify this task by holding A =- Ao constant and performing a line search over
the possible values of the new parameter a. In (a), the darkened line represents the search
space we restrict attention to. In (b), we show the reduced problem: a line search over a.
optimization problem requiring more sophisticated methods such as conjugate gradi-
ent. But the savings comes at a price: for any particular feature f, we are probably
underestimating its gain, and there is a reasonable chance that we will select a feature
f whose approximate gain AL(S, f) was highest and pass over the feature f with
maximal gain AL (S, f).
A graphical representation of this approximation is provided in Figure 3. Here the
log-likelihood is represented as an arbitrary convex function over two parameters: A
corresponds to the &amp;quot;old&amp;quot; parameter, and a to the &amp;quot;new&amp;quot; parameter. Holding A fixed
and adjusting a to maximize the log-likelihood involves a search over the darkened
line, rather than a search over the entire space of (A, a).
The actual algorithms, along with the appropriate mathematical framework, are
presented in the appendix.
</bodyText>
<sectionHeader confidence="0.700369" genericHeader="method">
5. Case Studies
</sectionHeader>
<bodyText confidence="0.999972583333333">
In the next few pages we discuss several applications of maximum entropy modeling
within Candide, a fully automatic French-to-English machine translation system under
development at IBM. Over the past few years, we have used Candide as a test bed for
exploring the efficacy of various techniques in modeling problems arising in machine
translation.
We begin in Section 5.1 with a review of the general theory of statistical translation,
describing in some detail the models employed in Candide. In Section 5.2 we describe
how we have applied maximum entropy modeling to predict the French translation of
an English word in context. In Section 5.3 we describe maximum entropy models that
predict differences between French word order and English word order. In Section 5.4
we describe a maximum entropy model that predicts how to divide a French sentence
into short segments that can be translated sequentially.
</bodyText>
<page confidence="0.989911">
53
</page>
<figure confidence="0.859459">
Computational Linguistics Volume 22, Number 1
Thei dog2 ate3 my4 homework5
Lei chiert2 a3 mange4 mes5 devoirs6
</figure>
<figureCaption confidence="0.989212">
Figure 4
</figureCaption>
<bodyText confidence="0.967645">
Alignment of a French-English sentence pair. The subscripts give the position of each word in
its sentence. Here al -= 1, a2 = 2, a3 = a4 = 3, a5 = 4, and a6 = 5.
</bodyText>
<subsectionHeader confidence="0.99379">
5.1 Review of Statistical Translation
</subsectionHeader>
<bodyText confidence="0.9914845">
When presented with a French sentence F, Candide&apos;s task is to find the English sen-
tence E which is most likely given F:
</bodyText>
<equation confidence="0.718718">
= argmax p(E1F) (29)
By Bayes&apos; theorem, this is equivalent to finding E such that
F = argmax p(FIE)p(E) (30)
</equation>
<bodyText confidence="0.999789727272727">
Candide estimates p(E)—the probability that a string E of English words is a well-
formed English sentence—using a parametric model of the English language, com-
monly referred to as a language model. The system estimates p(FIE)—the probability
that a French sentence F is a translation of E—using a parametric model of the process
of English-to-French translation known as a translation model. These two models,
plus a search strategy for finding the E that maximizes (30) for some F, comprise the
engine of the translation system.
We now briefly describe the translation model for the probability P(FIE); a more
thorough account is provided in Brown et al. (1991). We imagine that an English sen-
tence E generates a French sentence F in two steps. First, each word in E independently
generates zero or more French words. These words are then ordered to give a French
sentence F. We denote the ith word of E by e, and the jth word of F by yi. (We em-
ploy yi rather than the more intuitive fi to avoid confusion with the feature function
notation.) We denote the number of words in the sentence E by 1E1 and the number
of words in the sentence F by IF 1. The generative process yields not only the French
sentence F but also an association of the words of F with the words of E. We call this
association an alignment, and denote it by A. An alignment A is parametrized by a
sequence of IFI numbers al, with 1 &lt; a, &lt; IE l. For every word position j in F, aj is the
word position in E of the English word that generates yl. Figure 4 depicts a typical
alignment.
The probability p(FIE) that F is the translation of E is expressed as the sum over
all possible alignments A between E and F of the probability of F and A given E:
</bodyText>
<equation confidence="0.8081365">
p(FE) = Ep(F,AIE) (31)
A
</equation>
<bodyText confidence="0.9791355">
The sum in equation (31) is computationally unwieldy; it involves a sum over all Ft Fl
possible alignments between the words in the two sentences. We sometimes make the
</bodyText>
<page confidence="0.991763">
54
</page>
<note confidence="0.878733">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<bodyText confidence="0.9960265">
simplifying assumption that there exists one extremely probable alignment A, called
the &amp;quot;Viterbi alignment,&amp;quot; for which
</bodyText>
<equation confidence="0.997278333333333">
p(FIE) p(F, AIE) (32)
Given some alignment A (Viterbi or otherwise) between E and F, the probability
p(F, AIE) is given by
jEl Ft
p(F, AIE) = 11 p(n(ei)lei) • Hp(yileaj) • d(AlE, (33)
i=1
</equation>
<bodyText confidence="0.999624">
where n(e) denotes the number of French words aligned with e,. In this expression
</bodyText>
<listItem confidence="0.9986852">
• p(nle) is the probability that the English word e generates n French
words,
• p(yle) is the probability that the English word e generates the French
word y; and
• d(AlE,F) is the probability of the particular order of French words.
</listItem>
<bodyText confidence="0.999897633333333">
We call the model described by equations (31) and (33) the basic translation model.
We take the probabilities p(nle) and p(yle) as the fundamental parameters of the
model, and parametrize the distortion probability in terms of simpler distributions.
Brown et al. (1991) describe a method of estimating these parameters to maximize the
likelihood of a large bilingual corpus of English and French sentences. Their method is
based on the Estimation-Maximization (EM) algorithm, a well-known iterative tech-
nique for maximum likelihood training of a model involving hidden statistics. For the
basic translation model, the hidden information is the alignment A between E and F.
We employed the EM algorithm to estimate the parameters of the basic trans-
lation model so as to maximize the likelihood of a bilingual corpus obtained from
the proceedings of the Canadian Parliament. For historical reasons, these proceedings
are sometimes called &amp;quot;Hansards.&amp;quot; Our Hansard corpus contains 3.6 million English-
French sentence pairs, for a total of a little under 100 million words in each language.
Table 2 shows our parameter estimates for the translation probabilities p(ylin). The ba-
sic translation model has worked admirably: given only the bilingual corpus, with no
additional knowledge of the languages or any relation between them, it has uncovered
some highly plausible translations.
Nevertheless, the basic translation model has one major shortcoming: it does not
take the English context into account. That is, the model does not account for surround-
ing English words when predicting the appropriate French rendering of an English
word. As we pointed out in Section 3, this is not how successful translation works.
The best French translation of in is a function of the surrounding English words: if a
month&apos;s time are the subsequent words, pendant might be more likely, but if the fiscal year
1992 are what follows, then dans is more likely. The basic model is blind to context,
always assigning a probability of 0.3004 to dans and 0.0044 to pendant.
This can yield errors when Candide is called upon to translate a French sentence.
Examples of two such errors are shown in Figure 5. In the first example, the system has
chosen an English sentence in which the French word superieures has been rendered as
superior when greater or higher is a preferable translation. With no knowledge of context,
an expert translator is also quite likely to select superior as the English word generating
</bodyText>
<page confidence="0.990115">
55
</page>
<table confidence="0.468229">
Computational Linguistics Volume 22, Number 1
</table>
<tableCaption confidence="0.933845">
Table 2
</tableCaption>
<table confidence="0.941431235294117">
Most frequent French translations of in as
estimated using EM-training. (OTHER)
represents a catch-all classifier for any
French phrase not listed, none of which
had a probability exceeding 0.0043.
Translation Probability
dans 0.3004
0.2275
de 0.1428
en 0.1361
pour 0.0349
(OTHER) 0.0290
au cours de 0.0233
0.0154
Sur 0.0123
par 0.0101
pendant 0.0044
</table>
<figure confidence="0.6986765">
Je dirais meme que les chances sont superieures a 50%.
I would even say that the odds are superior to 50%.
11 semble que Bank of Boston ait pratiquement acheve son reexamen de Shawmut.
He appears that Bank of Boston has almost completed its review of Shawmut.
</figure>
<figureCaption confidence="0.983060666666667">
Figure 5
Typical errors encountered in using EM-based model of Brown et al. in a French-to-English
translation system.
</figureCaption>
<bodyText confidence="0.999564">
superieures. But an expert privy to the fact that 50% was among the next few words
might be more inclined to select greater or higher. Similarly, in the second example, the
incorrect rendering of II as He might have been avoided had the translation model
used the fact that the word following it is appears.
</bodyText>
<subsectionHeader confidence="0.999557">
5.2 Context-Dependent Word Models
</subsectionHeader>
<bodyText confidence="0.999918416666667">
In the hope of rectifying these errors, we consider the problem of context-sensitive
modeling of word translation. We envision, in practice, a separate maximum entropy
model, p e(y1x), for each English word e, where pe(Y1x) represents the probability that an
expert translator would choose y as the French rendering of e, given the surrounding
English context x. This is just a slightly recast version of a longstanding problem
in computational linguistics, namely, sense disambiguation—the determination of a
word&apos;s sense from its context.
We begin with a training sample of English-French sentence pairs (E, F) randomly
extracted from the Hansard corpus, such that E contains the English word in. For each
sentence pair, we use the basic translation model to compute the Viterbi alignment A
between E and F. Using this alignment, we then construct an (x, y) training event. The
event consists of a context x containing the six words in E surrounding in and a future
</bodyText>
<page confidence="0.998005">
56
</page>
<note confidence="0.932175">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<tableCaption confidence="0.970028">
Table 3
</tableCaption>
<bodyText confidence="0.979908125">
Several actual training events for the maximum entropy translation model for
in, extracted from the transcribed proceedings of the Canadian Parliament.
Translation e_3 e-2 e—i e±i e4.2 e+3
dans the committee stated a letter to
work was required respect of the
au cours de the fiscal year
dans by the government the same postal
de of diphtheria reported Canada , by
not given notice the ordinary way
y equal to the French word which is (according to the Viterbi alignment A) aligned
with in. A few actual examples of such events for in are depicted in Table 3.
Next we define the set of candidate features. For this application, we employ
features that are indicator functions of simply described sets. Specifically, we consider
functions f (x, y) that are one if y is some particular French word and the context
x contains a given English word, and are zero otherwise. We employ the following
notation to represent these features:
</bodyText>
<equation confidence="0.6333788">
1 if y = en and April E
0 otherwise
•
f2(x/Y) 1 if y = pendant and weeks E • • •
0 otherwise
</equation>
<bodyText confidence="0.998496083333333">
Here fi = 1 when April follows in and en is the translation of in; f2 -= 1 when weeks is
one of the three words following in and pendant is the translation.
The set of features under consideration is vast, but may be expressed in abbrevi-
ated form in Table 4. In the table, the symbol 0 is a placeholder for a possible French
word and the symbol is a placeholder for a possible English word. The feature fi
mentioned above is thus derived from template 2 with 0 = en and 0 = April; the
feature f2 is derived from template 5 with 0 = pendant and 0 = weeks. If there are IVE I
total English words and IVA total French words, there are IVA template-1 features,
and IVE I . IV.T features of templates 2, 3, 4, and 5.
Template 1 features give rise to constraints that enforce equality between the prob-
ability of any French translation y of in according to the model and the probability of
that translation in the empirical distribution. Examples of such constraints are
</bodyText>
<equation confidence="0.999283">
p(y = dans) = 13(y = dans)
13(Y = a) = -13(Y = a)
p(y --= de) = /3(y = de)
p(y = en) = 13(y = en)
</equation>
<page confidence="0.997005">
57
</page>
<note confidence="0.539157">
Computational Linguistics Volume 22, Number 1
</note>
<tableCaption confidence="0.989271">
Table 4
</tableCaption>
<bodyText confidence="0.960701625">
Feature templates for word-translation modeling. lye is the size of the
English vocabulary; IVFI the size of the French vocabulary.
A maximum entropy model that uses only template 1 features predicts each French
translation y with the probability p(y) determined by the empirical data. This is exactly
the distribution employed by the basic translation model.
Since template 1 features are independent of x, the maximum entropy model that
employs only constraints derived from template 1 features takes no account of contex-
tual information in assigning a probability to y. When we include constraints derived
from template 2 features, we take our first step towards a context-dependent model.
Rather than simply constraining the expected probability of a French word y to equal
its empirical probability, these constraints require that the expected joint probability of
the English word immediately following in and the French rendering of in be equal
to its empirical probability An example of a template 2 constraint is
p(y = pendant, e+1 = several) = p(y = pendant, e+i = several)
A maximum entropy model that incorporates this constraint will predict the transla-
tions of in in a manner consistent with whether or not the following word is several.
In particular, if in the empirical sample the presence of several led to a greater prob-
ability for pendant, this will be reflected in a maximum entropy model incorporating
this constraint. We have thus taken our first step toward context-sensitive translation
modeling.
Templates 3, 4, and 5 consider, each in a different way, various parts of the context.
For instance, template 5 constraints allow us to model how an expert translator is
biased by the appearance of a word somewhere in the three words following the word
being translated. If house appears within the next three words (e.g., the phrases in the
house and in the red house), then dans might be a more likely translation. On the other
hand, if year appears within the same window of words (as in in the year 1941 or in
that fateful year), then au cours de might be more likely. Together, the five constraint
templates allow the model to condition its assignment of probabilities on a window
of six words around eo, the word in question.
We constructed a maximum entropy model p,n(y1x) by the iterative model-growing
method described in Section 4. The automatic feature selection algorithm first selected
a template 1 constraint for each of the translations of in seen in the sample (12 in
</bodyText>
<figure confidence="0.999089533333333">
•
•
•
•
•
•
1 IVY l Y = 0
2 IY7-1 • We I y = and 0 E
3 IV.F I • IVe I y = and 0 E
4 IY.T. IjVe y = 0 and 0 E
5 IY.7 • lye I y = and 0 E
•
•
Number of
Template Actual Features f (x, y) = 1 if and only if ...
</figure>
<page confidence="0.991055">
58
</page>
<note confidence="0.92687">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<tableCaption confidence="0.991844">
Table 5
</tableCaption>
<bodyText confidence="0.99718">
Maximum entropy model to predict French translation of in. Features shown
here were the first features selected not from template 1. [verb marker] denotes a
morphological marker inserted to indicate the presence of a verb as the next
word.
</bodyText>
<listItem confidence="0.979994571428571">
Feature f (x, AL(S, f) L(p)
y=a and Canada E 0.0415 —2.9674
•
y=a and House E 0.0361 —2.9281
•
yr--en and the E 0.0221 —2.8944
•
y=pour and order E 0.0224 —2.8703
•
y=dans and speech E 0.0190 —2.8525
• • •
y=dans and area E 0.0153 —2.8377
• • •
y=de and increase E 0.0151 —2.8209
• • •
y=[verb marker] and my E 0.0141 —2.8034
•
y=dans and case E 0.0116 —2.7918
• • •
y=au cours de and year E 0.0104 —2.7792
• • •
</listItem>
<bodyText confidence="0.999795764705882">
all), thus constraining the model&apos;s expected probability of each of these translations
to their empirical probabilities. The next few constraints selected by the algorithm are
shown in Table 5. The first column gives the identity of the feature whose expected
value is constrained; the second column gives AL(S, f ), the approximate increase in
the model&apos;s log-likelihood on the data as a result of imposing this constraint; the third
column gives L(p), the log-likelihood after adjoining the feature and recomputing the
model.
Let us consider the fifth row in the table. This constraint requires that the model&apos;s
expected probability of dans, if one of the three words to the right of in is the word
speech, is equal to that in the empirical sample. Before imposing this constraint on the
model during the iterative model-growing process, the log-likelihood of the current
model on the empirical sample was —2.8703 bits. The feature selection algorithm de-
scribed in Section 4 calculated that if this constraint were imposed on the model, the
log-likelihood would rise by approximately 0.019059 bits; since this value was higher
than for any other constraint considered, the constraint was selected. After applying
iterative scaling to recompute the parameters of the new model, the likelihood of the
empirical sample rose to —2.8525 bits, an increase of 0.0178 bits.
</bodyText>
<page confidence="0.997879">
59
</page>
<note confidence="0.534447">
Computational Linguistics Volume 22, Number 1
</note>
<tableCaption confidence="0.664312">
Table 6
Maximum entropy model to predict French translation of to run:
top-ranked features not from template 1.
</tableCaption>
<listItem confidence="0.946422823529412">
Feature f (x, y) ,-AL(S,f) L(p)
y=epuiser and out E 0.0252 -4.8499
• • •
y=manquer and out E 0.0221 -4.8201
• • •
y=ecouler and time E 0.0157 -4.7969
• • •
y=accumuler and up E 0.0149 -4.7771
•
y=nous and we E 0.0140 -4.7582
•
y=aller and counter E 0.0131 -4.7445
• • •
y=candidat and for E 0.0124 -4.7295
• • •
y=diriger and the E 0.0123 -4.7146
• • •
</listItem>
<bodyText confidence="0.999079125">
Table 6 lists the first few selected features for the model for translating the En-
glish word run. The &amp;quot;Hansard flavor&amp;quot;—the rather specific domain of parliamentary
discourse related to Canadian affairs—is easy to detect in many of the features in this
table.
It is not hard to incorporate the maximum entropy word translation models into a
translation model p(FIE) for a French sentence given an English sentence. We merely
replace the simple context-independent models p(ye) used in the basic translation
model (33) with the more general context-dependent models pe(y1x):
</bodyText>
<equation confidence="0.962417333333333">
IEl Fl
P(F, AIE) 11 p(n(eolei) (Y iixaj) • d(AlE,
i=1 j=1
</equation>
<bodyText confidence="0.977261">
where xa, denotes the context of the English word eaj.
Figure 6 illustrates how using this improved translation model in the Candide
system led to improved translations for the two sample sentences given earlier.
</bodyText>
<subsectionHeader confidence="0.810388">
5.3 Segmentation
</subsectionHeader>
<bodyText confidence="0.999526285714286">
Though an ideal machine translation system could devour input sentences of unre-
stricted length, a typical stochastic system must cut the French sentences into polite
lengths before digesting them. If the processing time is exponential in the length of the
input passage (as is the case with the Candide system), then failing to split the French
sentences into reasonably-sized segments would result in an exponential slowdown
in translation.
Thus, a common task in machine translation is to find safe positions at which
</bodyText>
<page confidence="0.989277">
60
</page>
<note confidence="0.744404">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<footnote confidence="0.398979666666667">
Je dirais meme que les chances sont superieures a 50%.
I would even say that the odds are greater than 50%.
Ii semble que Bank of Boston nit pratiquement acheve son reexamen de Shawmut.
It appears that Bank of Boston has almost completed its review of Shawmut .
Figure 6
Improved French-to-English translations resulting from maximum entropy-based system.
</footnote>
<note confidence="0.1832765">
Thei dog2 ate3 my4 homework5
Lei chien2 a3 II mange4 mess 11 devoirs6
</note>
<figureCaption confidence="0.998674">
Figure 7
</figureCaption>
<bodyText confidence="0.997976230769231">
Example of an unsafe segmentation. A word in the translated sentence (e3) is aligned to words
(y3 and y4) in two different segments of the input sentence.
to split input sentences in order to speed the translation process. &amp;quot;Safe&amp;quot; is a vague
term; one might, for instance, reasonably define a safe segmentation as one which
results in coherent blocks of words. For our purposes, however, a safe segmentation
is dependent on the Viterbi alignment A between the input French sentence F and its
English translation E.
We define a rift as a position j in F such that for all k &lt;j, ak &lt;a1 and for all k &gt; j,
ak &gt; al. In other words, the words to the left of the French word yi are generated by
words to the left of the English word eal, and the words to the right of yi are generated
by words to the right of ea,. In the alignment of figure 4, for example, there are rifts
at positions j = 1, 2, 4, 5 in the French sentence. One visual method of determining
whether a rift occurs after the French word j is to try to trace a line from the last
letter of yj up to the last letter of eai; if the line can be drawn without intersecting any
alignment lines, position f is a rift.
Using our definition of rifts, we can redefine a safe segmentation as one in which
the segment boundaries are located only at rifts. Figure 7 illustrates an unsafe seg-
mentation, in which a segment boundary (denoted by the 11 symbol) lies between
a and mange, where there is no rift. Figure 8, on the other hand, illustrates a safe
segmentation.
The reader will notice that a safe segmentation does not necessarily result in se-
mantically coherent segments: mes and devoirs are certainly part of one logical unit,
yet are separated in this safe segmentation. Once such a safe segmentation has been
applied to the French sentence, we can make the assumption while searching for the
appropriate English translation that no word in the translated English sentence will
have to account for French words located in multiple segments. Disallowing inter-
</bodyText>
<page confidence="0.994257">
61
</page>
<figure confidence="0.964947">
Computational Linguistics Volume 22, Number 1
&apos;Thei dog2 ate3 mY 4 homework5
Lei II chien2 a3 mange4 mes5 II devoirs6
</figure>
<figureCaption confidence="0.781217">
Figure 8
</figureCaption>
<bodyText confidence="0.376811">
Example of a safe segmentation.
</bodyText>
<equation confidence="0.509292">
ea, —3 eai+3 tag(ea,-3) tag(ea,+3) class(ea,_3) class(ea,+3)
rift?
Figure 9
(x, y) for sentence segmentation.
</equation>
<bodyText confidence="0.999901419354839">
segment alignments dramatically reduces the scale of the computation involved in
generating a translation, particularly for large sentences. We can consider each seg-
ment sequentially while generating the translation, working from left to right in the
French sentence.
We now describe a maximum entropy model that assigns to each location in a
French sentence a score that is a measure of the safety in cutting the sentence at
that location. We begin as in the word translation problem, with a training sample of
English-French sentence pairs (E, F) randomly extracted from the Hansard corpus. For
each sentence pair we use the basic translation model to compute the Viterbi alignment
A between E and F. We also use a stochastic part-of-speech tagger as described in
Merialdo (1990) to label each word in F with its part of speech. For each position j in F
we then construct a (x, y) training event. The value y is rift if a rift belongs at position
j and is no-rift otherwise. The context information x is reminiscent of that employed
in the word translation application described earlier. It includes a six-word window
of French words: three to the left of yi and three to the right of yl. It also includes the
part-of-speech tags for these words, and the classes of these words as derived from a
mutual-information clustering scheme described in Brown et al. (1990). The complete
(x, y) pair is illustrated in Figure 9.
In creating p(riftlx), we are (at least in principle) modeling the decisions of an
expert French segmenter. We have a sample of his work in the training sample j3(x, y),
and we measure the worth of a model by the log-likelihood Li3(p). During the itera-
tive model-growing procedure, the algorithm selects constraints on the basis of how
much they increase this objective function. As the algorithm proceeds, more and more
constraints are imposed on the model p, bringing it into ever-stricter compliance with
the empirical data p&amp;quot; (x,y). This is useful to a point; insofar as the empirical data em-
bodies the expert knowledge of the French segmenter, we would like to incorporate
this knowledge into a model. But the data contains only so much expert knowledge;
the algorithm should terminate when it has extracted this knowledge. Otherwise, the
model p(y Ix) will begin to fit itself to quirks in the empirical data.
A standard approach in statistical modeling, to avoid the problem of overfitting
the training data, is to employ cross-validation techniques. Separate the training data
</bodyText>
<page confidence="0.993018">
62
</page>
<figure confidence="0.862440333333333">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
20 40 60 80 100 120
Number of Features
</figure>
<figureCaption confidence="0.592332333333333">
Figure 10
Change in log-likelihood during segmenting model-growing. (Overtraining begins to occur at
about 40 features.)
</figureCaption>
<bodyText confidence="0.99928747826087">
P(x, y) into a training portion, Pr, and a withheld portion, Ph. Use only pr in the model-
growing process; that is, select features based on how much they increase the likeli-
hood L;), (p). As the algorithm progresses, Lip, (p) thus increases monotonically. As long
as each new constraint imposed allows p to better account for the random process
that generated both Pr and P h, the quantity Lph (p) also increases. At the point when
overfitting begins, however, the new constraints no longer help p model the random
process, but instead require p to model the noise in the sample Pr itself. At this point,
Lp, (p) continues to rise, but Li,, (p) no longer does. It is at this point that the algorithm
should terminate.
Figure 10 illustrates the change in log-likelihood of training data 11-„ (p) and with-
held data Lph(p). Had the algorithm terminated when the log-likelihood of the withheld
data stopped increasing, the final model p would contain slightly less than 40 features.
We have employed this segmenting model as a component in a French-English ma-
chine translation system in the following manner: The model assigns to each position
in the French sentence a score, p(rif t I x), which is a measure of how appropriate a
split would be at that location. A dynamic programming algorithm then selects, given
the &amp;quot;appropriateness&amp;quot; score at each position and the requirement that no segment may
contain more than 10 words, an optimal (or, at least, reasonable) splitting of the sen-
tence. Figure 11 shows the system&apos;s segmentation of four sentences selected at random
from the Hansard data. We remind the reader to keep in mind when evaluating Figure
11 that the segmenter&apos;s task is not to produce logically coherent blocks of words, but
to divide the sentence into blocks which can be translated sequentially from left to
right.
</bodyText>
<page confidence="0.994393">
63
</page>
<figure confidence="0.9733258">
Computational Linguistics Volume 22, Number 1
Monsieur l&apos;Orateur
j&apos;aimerais poser une question au
Ministre des Transports.
A quelle date le
nouveau reglement devrait ii entrer en vigeur?
Quels furent les criteres utilises
pour revaluation
de ces biens.
Nous
</figure>
<figureCaption confidence="0.680415375">
savons que si nous pouvions controler la folle avoine
dans l&apos;ouest du Canada, en
un an nous
augmenterions notre rendement en
cereales de 1 milliard de dollars.
Figure 11
Maximum entropy segmenter behavior on four sentences selected at random from the
Hansard data.
</figureCaption>
<subsectionHeader confidence="0.999681">
5.4 Word Reordering
</subsectionHeader>
<bodyText confidence="0.9999524">
Translating a French sentence into English involves not only selecting appropriate
English renderings of the words in the French sentence, but also selecting an ordering
for the English words. This order is often very different from the French word order.
One way Candide captures word-order differences in the two languages is to allow for
alignments with crossing lines. In addition, Candide performs, during a preprocessing
stage, a reordering step that shuffles the words in the input French sentence into an
order more closely resembling English word order.
One component of this word reordering step deals with French phrases which have
the NOUN de NOUN form. For some NOUN de NOUN phrases, the best English transla-
tion is nearly word for word: conflit d&apos;interet, for example, is almost always rendered as
conflict of interest. For other phrases, however, the best translation is obtained by inter-
changing the two nouns and dropping the de. The French phrase taux d&apos;interet, for ex-
ample, is best rendered as interest rate. Table 7 gives several examples of NOUN de NOUN
phrases together with their most appropriate English translations.
In this section we describe a maximum entropy model that, given a French NOUN
de NOUN phrase, estimates the probability that the best English translation involves
an interchange of the two nouns. We begin with a sample of English-French sen-
tence pairs (E, F) randomly extracted from the Hansard corpus, such that F contains
a NOUN de NOUN phrase. For each sentence pair we use the basic translation model to
compute the Viterbi alignment A between the words in E and F. Using A we construct
an (x, y) training event as follows: We let the context x be the pair of French nouns
(NouNL, NouNR). We let y be no-interchange if the English translation is a word-for-
word translation of the French phrase and y = interchange if the order of the nouns
in the English and French phrases are interchanged.
We define candidate features based upon the template features shown in Table 8.
</bodyText>
<page confidence="0.998519">
64
</page>
<note confidence="0.921862">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<tableCaption confidence="0.955559">
Table 7
</tableCaption>
<figure confidence="0.902910818181818">
NOUN de NOUN phrases and their English equivalents.
Word-for-word Phrases
somme d&apos;argent sum of money
pays d&apos;origin country of origin
question de privilege question of privilege
conflit d&apos;interet conflict of interest
Interchanged Phrases
bureau de poste post office
taux d&apos;interet interest rate
compagnie d&apos;assurance insurance company
gardien de prison prison guard
</figure>
<bodyText confidence="0.999488714285714">
In this table, the symbol 0 is a placeholder for either interchange or no-interchange
and the symbols 01 and 02 are placeholders for possible French words. If there are
IVA total French words, there are 21V.F1 possible features of templates 1 and 2 and
21v,i2 features of template 3.
Template 1 features consider only the left noun. We expect these features to be
relevant when the decision of whether to interchange the nouns is influenced by the
identity of the left noun. For example, including the template 1 feature
</bodyText>
<figure confidence="0.508612">
1 if y=inter change and NouNL= systeme
0 otherwise
</figure>
<bodyText confidence="0.95825425">
gives the model sensitivity to the fact that the nouns in French NOUN de NOUN phrases
beginning with systeme (such as systeme de surveillance and systeme de quota) are more
likely to be interchanged in the English translation. Similarly, including the template 1
feature
</bodyText>
<equation confidence="0.899655333333333">
f (x 1 if y=no-interchange and NouNL= mois
, y) =
0 otherwise
</equation>
<bodyText confidence="0.9985366875">
gives the model sensitivity to the fact that French NOUN de NOUN phrases beginning
with mois, such as mois de mai (month of May) are more likely to be translated word for
word.
Template 3 features are useful in dealing with translating NOUN de NOUN phrases
in which the interchange decision is influenced by both nouns. For example, NOUN de
NOUN phrases ending in interet are sometimes translated word for word, as in conflit
d&apos;interet (conflict of interest) and are sometimes interchanged, as in taux d&apos;interet (interest
rate).
We used the feature-selection algorithm of section 4 to construct a maximum en-
tropy model from candidate features derived from templates 1, 2, and 3. The model
was grown on 10,000 training events randomly selected from the Hansard corpus. The
final model contained 358 constraints.
To test the model, we constructed a NOUN de NOUN word-reordering module which
interchanges the order of the nouns if p( interchange lx) &gt; 0.5 and keeps the order the
same otherwise. Table 9 compares performance on a suite of test data against a baseline
NOUN de NOUN reordering module that never swaps the word order.
</bodyText>
<page confidence="0.998705">
65
</page>
<note confidence="0.609254">
Computational Linguistics Volume 22, Number 1
</note>
<tableCaption confidence="0.756546">
Table 8
Template features for NOUN de NOUN model.
</tableCaption>
<table confidence="0.9845484">
Number of
Template Actual Features f (x, y) = 1 if and only if ...
1 = and NouNc = 0
2 21V .T = and NouNR =_- 0
3 211).12 = and NouNL = Di and NouNR = 02
</table>
<tableCaption confidence="0.992974">
Table 9
</tableCaption>
<table confidence="0.982388142857143">
NOUN de NOUN model performance: simple approach vs. maximum entropy.
Test data Simple Model Maximum Entropy
Accuracy Model Accuracy
50,229 not interchanged 100% 93.5%
21,326 interchanged 0% 49.2%
71,555 total 70.2% 80.4%
.006 .018 .043 .195 2)6 .224 440 .555 .723 .845 .911 .922 .997
</table>
<figureCaption confidence="0.638176">
smaller... p(Mterchange) ...larger
Figure 12
</figureCaption>
<bodyText confidence="0.958918111111111">
Predictions of the NOUN de NOUN interchange model on phrases selected from a corpus unseen
during the training process.
Table 12 shows some randomly-chosen NOUN de NOUN phrases extracted from
this test suite along with p(interchange1x), the probability assigned by the model to
inversion. On the right are phrases such as saison d&apos;hiver for which the model strongly
predicted an inversion. On the left are phrases the model strongly prefers not to
interchange, such as somme d&apos;argent, abus de privilege and chambre de commerce. Perhaps
most intriguing are those phrases that lie in the middle, such as faux d&apos;inflation, which
can translate either to inflation rate or rate of inflation.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="method">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999668">
We began by introducing the building blocks of maximum entropy modeling—real-
valued features and constraints built from these features. We then discussed the max-
imum entropy principle. This principle instructs us to choose, among all the models
</bodyText>
<page confidence="0.966596">
66
</page>
<note confidence="0.882003">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<bodyText confidence="0.9999628125">
consistent with the constraints, the model with the greatest entropy. We observed that
this model was a member of an exponential family with one adjustable parameter for
each constraint. The optimal values of these parameters are obtained by maximizing
the likelihood of the training data. Thus two different philosophical approaches—
maximum entropy and maximum likelihood—yield the same result: the model with
the greatest entropy consistent with the constraints is the same as the exponential
model which best predicts the sample of data.
We next discussed algorithms for constructing maximum entropy models, concen-
trating our attention on the two main problems facing would-be modelers: selecting
a set of features to include in a model, and computing the parameters of a model
containing these features. The general feature-selection process is too slow in practice,
and we presented several techniques for making the algorithm feasible.
In the second part of this paper we described several applications of our algo-
rithms, concerning modeling tasks arising in Candide, an automatic machine transla-
tion system under development at IBM. These applications demonstrate the efficacy
of maximum entropy techniques for performing context-sensitive modeling.
</bodyText>
<sectionHeader confidence="0.992199" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995598">
The authors wish to thank Harry Printz and
John Lafferty for suggestions and comments
on a preliminary draft of this paper, and
Jerome Bellegarda for providing expert
French knowledge.
</bodyText>
<sectionHeader confidence="0.989646" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999268072463768">
Bahl, L.; Brown, P.; de Souza, P.; and Mercer,
R. (1989). A tree-based statistical language
model for natural language speech
recognition. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 37(7).
Berger, A.; Brown, P.; Della Pietra, S.; Della
Pietra, V.; Gillett, J.; Lafferty, J.; Printz, H.;
and Ureg, L. (1994). The Candide System
for Machine Translation. In Proceedings,
ARPA Conference on Human Language
Technology, Plainsborough, New Jersey.
Black, E.; jelinek, F.; Lafferty, J.; Magerman,
D.; Mercer, R.; and Roukos, S. (1992).
Towards History-based Grammars: Using
Richer Models for Probabilistic Parsing.
In Proceedings, DARPA Speech and Natural
Language Workshop, Arden House, New
York.
Brown, D. (1959). A Note on
Approximations to Discrete Probability
Distributions. Information and Control,
2:386-392.
Brown, P.; Della Pietra, S.; Della Pietra, V.;
and Mercer, R. (1993). The Mathematics of
Statistical Machine Translation: Parameter
Estimation. Computational Linguistics,
19(2):263-311.
Brown, P.; Cocke, J.; Della Pietra, S.; Della
Pietra, V.; Jelinek, F.; Lafferty, J.; Mercer,
R.; and Roossin, P. (1990). A Statistical
Approach to Machine Translation.
Computational Linguistics, 16:79-85.
Brown, P.; Della Pietra, V.; de Souza, P.; and
Mercer, R. (1990). Class-based N-Gram
Models of Natural Language. Proceedings,
IBM Natural Language ITL, 283-298.
Brown, P.; Della Pietra, S.; Della Pietra, V.;
and Mercer, R. (1991). A Statistical
Approach to Sense Disambiguation in
Machine Translation. In Proceedings,
DARPA Speech and Natural Language
Workshop, 146-151.
Cover, T. and Thomas, J. (1991). Elements of
Information Theory. John Wiley &amp; Sons.
Csiszdr, I. (1975). I-Divergence Geometry of
Probability Distributions and
Minimization Problems, The Annals of
Probability, 3(1):146-158.
ibid. (1989). A Geometric Interpretation of
Darroch and Ratcliff&apos;s Generalized
Iterative Scaling. The Annals of Statistics,
17(3):1409-1413.
Csiszdr, L. and Tusnady, G. (1984).
Information Geometry and Alternating
Minimization Procedures. Statistics &amp;
Decisions, Supplemental Issue, no. 1,
205-237.
Darroch, J. N. and Ratcliff, D. (1972).
Generalized Iterative Scaling for
Log-linear Models. Annals of Mathematical
Statistics, no. 43,1470-1480.
Della Pietra, S.; Della Pietra, V.; Gillett, J.;
Lafferty, J.; Printz, H.; and UreA, L. (1994).
&amp;quot;Inference and Estimation of a
Long-range Trigram Model.&amp;quot; In Lecture
Notes in Artificial Intelligence, 862.
Springer-Verlag, 78-92.
Della Pietra, S.; Della Pietra, V.; and
Lafferty, J. (1995). &amp;quot;Inducing features of
</reference>
<page confidence="0.975943">
67
</page>
<note confidence="0.348865">
Computational Linguistics Volume 22, Number 1
</note>
<reference confidence="0.999685795454545">
random fields&amp;quot; CMU Technical Report
CMU-CS-95-144.
Dempster, A. P.; Laird, N. M.; and Rubin,
D. B. (1977). Maximum Likelihood from
Incomplete Data via the EM Algorithm.
Journal of the Royal Statistical Society,
39(B):1-38.
Guiasu, S. and Shenitzer, A. (1985). The
Principle of Maximum Entropy. The
Mathematical Intelligencer, 7(1).
Jaynes, E. T. (1990) &amp;quot;Notes on Present Status
and Future Prospects.&amp;quot; In Maximum
Entropy and Bayesian Methods, edited by
W. T. Grandy and L. H. Schick. Kluwer,
1-13.
Jelinek, F. and Mercer, R. L. (1980).
Interpolated Estimation of Markov Source
Parameters from Sparse Data. In
Proceedings, Workshop on Pattern Recognition
in Practice, Amsterdam, The Netherlands.
Lucassen, J. and Mercer, R. (1984). An
Information Theoretic Approach to
Automatic Determination of Phonemic
Baseforms. In Proceedings, IEEE
International Conference on Acoustics, Speech
and Signal Processing, San Diego, CA,
42.5.1-42.5.4.
Merialdo, B. (1990). Tagging Text with a
Probabilistic Model. In Proceedings, IBM
Natural Language ITL, Paris, France,
161-172.
Nadas, A.; Mercer, R.; Bahl, L.; Bakis, R.;
Cohen, P.; Cole, A.-; Jelinek, E; and Lewis,
B. (1981). Continuous Speech Recognition
with Automatically Selected Acoustic
Prototypes Obtained by either
Bootstrapping or Clustering. In
Proceedings, IEEE International Conference on
Acoustics, Speech and Signal Processing,
Atlanta, GA, 1153-1155.
Sokolnikoff, I. S. and Redheffer, R. M.
(1966). Mathematics of Physics and Modern
Engineering, Second Edition, McGraw-Hill
Book Company.
</reference>
<page confidence="0.999279">
68
</page>
<note confidence="0.875285">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
</note>
<sectionHeader confidence="0.798245" genericHeader="method">
Appendix: Efficient Algorithms for Feature Selection
</sectionHeader>
<subsectionHeader confidence="0.649544">
Computing the Approximate Gain of One Feature
</subsectionHeader>
<bodyText confidence="0.999983166666667">
This section picks up where section 4 left off, describing in some detail a set of algo-
rithms to implement the feature selection process efficiently.
We first describe an iterative algorithm for computing -,AL(S,f) max„ Gsf (a)
for a candidate feature f. The algorithm is based on the fact that the maximum of
G5,1(a) occurs (except in rare cases) at the unique value a* at which the derivative
G&apos;8,1 (al is zero. To find this zero, we apply Newton&apos;s iterative root-finding method.
An important twist is that we do not use the updates obtained by applying Newton&apos;s
method directly in the variable a. This is because there is no guarantee that Gsf (an)
increases monotonically for such updates. Instead, we use updates derived by applying
Newton&apos;s method in the variables e or e-a. A convexity argument shows that using
these updates, the sequence of Gs,f (an) converges monotonically to the maximum
approximate gain -AL(S,f) Gsj (a*) and that an increases monotonically to a*.
The value a* that maximizes Gs ,1 (a) can be found by solving the equation G&apos;8,1 (a*)
= 0. Moreover, if an is any sequence for which GI s j (an) converges monotonically to 0,
then Gsff (an) will increase monotonically. This is a consequence of the convexity of
Gsj(a) in a.
We can solve an equation g(a) =- 0 by Newton&apos;s method, which produces a se-
quence an by the recurrence given in (18), repeated here for convenience:
</bodyText>
<equation confidence="0.994809666666667">
g(an)
an+.1 = an , (34)
g (an)
</equation>
<bodyText confidence="0.983633722222222">
If we start with ao sufficiently close to a, then the sequence an will converge to a*
and g(a) will converge to zero. In general, though, the g(an) will not be monotonic.
However, it can be shown that the sequence is monotonic in the following important
cases: if a() &lt; a* and g(a) is either decreasing and convex-U or increasing and convex-
n.
The function G&apos; s,1 (a) is neither convex-fl or convex-U as a function of a. However,
it can be shown (by taking derivatives) that G&apos;sj (a) is decreasing and convex-U in
e&apos;, and is increasing and convex-fl in e-° . Thus, if a* &gt; 0 so that e° &lt; e*, we can
apply Newton&apos;s method in e to obtain a sequence of an for which G&apos;sj (an) increases
monotonically to zero. Similarly, if a* &lt;0 so that e° &lt; e- a* , we can apply Newton&apos;s
method in e-a to obtain a sequence an for which Gisf (an) decreases monotonically to
zero. In either case, Gs ,f(an) increases monotonically to its maximum Gs,f (a*).
The updates resulting from Newton&apos;s method applied in the variable era , for r = 1
or r = -1 are easily computed:
= an + -log (1 1 G&apos;8( an) (35)
r r G&amp;quot; s ,f (an) )
In order to solve the recurrence in (35), we need to compute G&apos;sj and Gusj. The
zeroth, first, and second derivatives of G are
</bodyText>
<equation confidence="0.998956">
Gs (a) = - EI3(x) log Za (x) + aij(f ) (36)
cs,f(a) = ii(f) - Ep(x)/9f(f ix) (37)
</equation>
<page confidence="0.995795">
69
</page>
<figure confidence="0.877637818181818">
Computational Linguistics Volume 22, Number 1
G&amp;quot;s,f (a) =- 13(x)Pc,f ((f — pts,f (f I x))21x) (38)
where
pos,j(hix).-_Epcsf(yix)h(x,y) (39)
With these in place, we are ready to enumerate
Algorithm 3: Computing the Gain of a Single Feature
Input: Empirical distribution fr(x, y); initial model PS; candidate feature f
Output: Approximate gain --,AL(S,f) of feature f
1. Let
= I 1 if p(f) ps (f ) (40)
–1 otherwise
</figure>
<reference confidence="0.819133166666667">
2. Set ao &lt;– 0
3. Repeat the following until Gs,f (an) has converged:
Compute an±i from an using (35)
Compute Gs,f(an±i ) using (26)
4. Set – AL(S, f) Gs,f(an)
Computing Approximate Gains in Parallel
</reference>
<bodyText confidence="0.999704888888889">
For the purpose of incremental model growing as outlined in Algorithm 2, we need to
compute the maximum approximate gain AL(S,f) for each candidate feature f E F.
One obvious approach is to cycle through all candidate features and apply Algorithm 3
for each one sequentially. Since Algorithm 3 requires one pass through every event
in the training sample per iteration, this could entail millions of passes through the
training sample. Because a significant cost often exists for reading the training data—if
the data cannot be stored in memory but must be accessed from disk, for example—an
algorithm that passes a minimal number of times through the data may be of some
utility. We now give a parallel algorithm specifically tailored to this scenario.
</bodyText>
<reference confidence="0.967263714285714">
Algorithm 4: Computing Approximate Gains for A Collection of Features
Input: Collection .7. of candidate features; empirical distribution /3(x, y);
initial model ps
Output: Approximate gain –AL(S,f) for each candidate feature f E
1. For each f E F, calculate /3(f), the expected value of f in the training data
2. For each x, determine the set .F(x) c F of f that are active for x:
.F(x) {f ETIf(x,Y)Ps(Yix)P(x)&gt; 0 for some yl (41)
</reference>
<page confidence="0.978745">
70
</page>
<figure confidence="0.846440611111111">
Berger, Della Pietra, and Della Pietra A Maximum Entropy Approach
3. For each f, let
{ 1 if i3(f) 5_ Ps(i)
r(f) =
—1 otherwise
4. For each f E T, initialize a(f) 4- 0
5. Repeat the following until a(f) converges for each f E .i:
(a) For each f E T, set
G&apos; (f) — P(f)
G&apos;&apos;(f) &lt;— 0
(b) For each x, do the following:
For each f E T(x), update G&apos; (f) and G&amp;quot; (f) by
G&apos; (f) 4— G&apos; (f) — 13(x)pgi(f Ix)
G&amp;quot; (i) &apos;-- G&amp;quot; (f) — 1)(x — PS ,f(i lx))2ix)
where pg/(flx) Ey pg,1(y1x)f (x, y)
(c) For each f E 1., update a(f) by
ce(f ) &lt;— a(f) + 77(.1--n- log (1 — r(fl ) GG&apos;„(f(f)) )
6. For each f E T, substitute a(f) into (26) to determine —AL(S, f).
</figure>
<figureCaption confidence="0.221911333333333">
Convergence for this algorithm is guaranteed just as it was for algorithm 3—after
each iteration of step 5, the value of a(f) for each candidate feature f is closer to its
optimal value a* (f) and, more importantly, the gain Gsj is closer to the maximal gain
</figureCaption>
<figure confidence="0.523403">
(42)
</figure>
<page confidence="0.983187">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.244263">
<title confidence="0.9971405">A Maximum Entropy Approach to Natural Language Processing</title>
<author confidence="0.989945">Adam L Bergert Stephen A Della PietraI</author>
<affiliation confidence="0.912151">Columbia University Renaissance Technologies</affiliation>
<author confidence="0.866716">Vincent J Della PietraI</author>
<title confidence="0.60498">Renaissance Technologies</title>
<abstract confidence="0.937266571428572">The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>P Brown</author>
<author>P de Souza</author>
<author>R Mercer</author>
</authors>
<title>A tree-based statistical language model for natural language speech recognition.</title>
<date>1989</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>37</volume>
<issue>7</issue>
<marker>Bahl, Brown, de Souza, Mercer, 1989</marker>
<rawString>Bahl, L.; Brown, P.; de Souza, P.; and Mercer, R. (1989). A tree-based statistical language model for natural language speech recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>P Brown</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V Gillett</author>
<author>J Lafferty</author>
<author>J Printz</author>
<author>H</author>
<author>L Ureg</author>
</authors>
<title>The Candide System for Machine Translation.</title>
<date>1994</date>
<booktitle>In Proceedings, ARPA Conference on Human Language Technology,</booktitle>
<location>Plainsborough, New Jersey.</location>
<marker>Berger, Brown, Pietra, S, Gillett, Lafferty, Printz, H, Ureg, 1994</marker>
<rawString>Berger, A.; Brown, P.; Della Pietra, S.; Della Pietra, V.; Gillett, J.; Lafferty, J.; Printz, H.; and Ureg, L. (1994). The Candide System for Machine Translation. In Proceedings, ARPA Conference on Human Language Technology, Plainsborough, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>F jelinek</author>
<author>J Lafferty</author>
<author>D Magerman</author>
<author>R Mercer</author>
<author>S Roukos</author>
</authors>
<title>Towards History-based Grammars: Using Richer Models for Probabilistic Parsing.</title>
<date>1992</date>
<booktitle>In Proceedings, DARPA Speech and Natural Language Workshop,</booktitle>
<location>Arden House, New York.</location>
<contexts>
<context position="2357" citStr="Black et al. (1992)" startWordPosition="347" endWordPosition="350">lter their portfolios to capitalize on the predicted future. At the other end of the pay scale reside natural language researchers, who design language and acoustic models for use in speech recognition systems and related applications. The past few decades have witnessed significant progress toward increasing the predictive capacity of statistical models of natural language. In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al. (1994) have used automatically inferred link grammars to model long range correlations in language. In parsing, Black et al. (1992) have described how to extract grammatical * This research, supported in part by ARPA under grant ONR N00014-91—C-0135, was conducted while the authors were at the IBM T. J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY 10598 t Now at Computer Science Department, Columbia University. t Now at Renaissance Technologies, Stony Brook, NY © 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 1 rules from annotated text automatically and incorporate these rules into statistical models of grammar. In speech recognition, Lucassen and Mercer (1984) ha</context>
</contexts>
<marker>Black, jelinek, Lafferty, Magerman, Mercer, Roukos, 1992</marker>
<rawString>Black, E.; jelinek, F.; Lafferty, J.; Magerman, D.; Mercer, R.; and Roukos, S. (1992). Towards History-based Grammars: Using Richer Models for Probabilistic Parsing. In Proceedings, DARPA Speech and Natural Language Workshop, Arden House, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Brown</author>
</authors>
<title>A Note on Approximations to Discrete Probability Distributions. Information and Control,</title>
<date>1959</date>
<pages>2--386</pages>
<contexts>
<context position="23866" citStr="Brown 1959" startWordPosition="4040" endWordPosition="4041">= p),* 3.6 Computing the Parameters For all but the most simple problems, the A* that maximize xi&apos; (A) cannot be found analytically. Instead, we must resort to numerical methods. From the perspective of numerical optimization, the function 4f (A) is well behaved, since it is smooth and convex-11 in A. Consequently, a variety of numerical methods can be used to calculate A*. One simple method is coordinate-wise ascent, in which A* is computed by iteratively maximizing 41(A) one coordinate at a time. When applied to the maximum entropy problem, this technique yields the popular Brown algorithm (Brown 1959). Other general purpose methods that can be used to maximize T(A) include gradient ascent and conjugate gradient. An optimization method specifically tailored to the maximum entropy problem is the iterative scaling algorithm of Darroch and Ratcliff (1972). We present here a version of this algorithm specifically designed for the problem at hand; a proof of the monotonicity and convergence of the algorithm is given in Della Pietra et al. (1995). The algorithm is applicable whenever the feature functions f,(x,y) are nonnegative: f, y) &gt; 0 for all i, x, and y (15) This is, of course, true for the</context>
</contexts>
<marker>Brown, 1959</marker>
<rawString>Brown, D. (1959). A Note on Approximations to Discrete Probability Distributions. Information and Control, 2:386-392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V</author>
<author>R Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Brown, Pietra, S, V, Mercer, 1993</marker>
<rawString>Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mercer, R. (1993). The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V Jelinek</author>
<author>F Lafferty</author>
<author>J Mercer</author>
<author>R</author>
<author>P Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--79</pages>
<contexts>
<context position="57392" citStr="Brown et al. (1990)" startWordPosition="9881" endWordPosition="9884">r as described in Merialdo (1990) to label each word in F with its part of speech. For each position j in F we then construct a (x, y) training event. The value y is rift if a rift belongs at position j and is no-rift otherwise. The context information x is reminiscent of that employed in the word translation application described earlier. It includes a six-word window of French words: three to the left of yi and three to the right of yl. It also includes the part-of-speech tags for these words, and the classes of these words as derived from a mutual-information clustering scheme described in Brown et al. (1990). The complete (x, y) pair is illustrated in Figure 9. In creating p(riftlx), we are (at least in principle) modeling the decisions of an expert French segmenter. We have a sample of his work in the training sample j3(x, y), and we measure the worth of a model by the log-likelihood Li3(p). During the iterative model-growing procedure, the algorithm selects constraints on the basis of how much they increase this objective function. As the algorithm proceeds, more and more constraints are imposed on the model p, bringing it into ever-stricter compliance with the empirical data p&amp;quot; (x,y). This is </context>
</contexts>
<marker>Brown, Cocke, Pietra, S, Jelinek, Lafferty, Mercer, R, Roossin, 1990</marker>
<rawString>Brown, P.; Cocke, J.; Della Pietra, S.; Della Pietra, V.; Jelinek, F.; Lafferty, J.; Mercer, R.; and Roossin, P. (1990). A Statistical Approach to Machine Translation. Computational Linguistics, 16:79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>V de Souza</author>
<author>P</author>
<author>R Mercer</author>
</authors>
<date>1990</date>
<booktitle>Class-based N-Gram Models of Natural Language. Proceedings, IBM Natural Language ITL,</booktitle>
<pages>283--298</pages>
<marker>Brown, Pietra, de Souza, P, Mercer, 1990</marker>
<rawString>Brown, P.; Della Pietra, V.; de Souza, P.; and Mercer, R. (1990). Class-based N-Gram Models of Natural Language. Proceedings, IBM Natural Language ITL, 283-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V</author>
<author>R Mercer</author>
</authors>
<title>A Statistical Approach to Sense Disambiguation in Machine Translation.</title>
<date>1991</date>
<booktitle>In Proceedings, DARPA Speech and Natural Language Workshop,</booktitle>
<pages>146--151</pages>
<contexts>
<context position="37785" citStr="Brown et al. (1991)" startWordPosition="6472" endWordPosition="6475">t a string E of English words is a wellformed English sentence—using a parametric model of the English language, commonly referred to as a language model. The system estimates p(FIE)—the probability that a French sentence F is a translation of E—using a parametric model of the process of English-to-French translation known as a translation model. These two models, plus a search strategy for finding the E that maximizes (30) for some F, comprise the engine of the translation system. We now briefly describe the translation model for the probability P(FIE); a more thorough account is provided in Brown et al. (1991). We imagine that an English sentence E generates a French sentence F in two steps. First, each word in E independently generates zero or more French words. These words are then ordered to give a French sentence F. We denote the ith word of E by e, and the jth word of F by yi. (We employ yi rather than the more intuitive fi to avoid confusion with the feature function notation.) We denote the number of words in the sentence E by 1E1 and the number of words in the sentence F by IF 1. The generative process yields not only the French sentence F but also an association of the words of F with the </context>
<context position="39997" citStr="Brown et al. (1991)" startWordPosition="6878" endWordPosition="6881">i) • Hp(yileaj) • d(AlE, (33) i=1 where n(e) denotes the number of French words aligned with e,. In this expression • p(nle) is the probability that the English word e generates n French words, • p(yle) is the probability that the English word e generates the French word y; and • d(AlE,F) is the probability of the particular order of French words. We call the model described by equations (31) and (33) the basic translation model. We take the probabilities p(nle) and p(yle) as the fundamental parameters of the model, and parametrize the distortion probability in terms of simpler distributions. Brown et al. (1991) describe a method of estimating these parameters to maximize the likelihood of a large bilingual corpus of English and French sentences. Their method is based on the Estimation-Maximization (EM) algorithm, a well-known iterative technique for maximum likelihood training of a model involving hidden statistics. For the basic translation model, the hidden information is the alignment A between E and F. We employed the EM algorithm to estimate the parameters of the basic translation model so as to maximize the likelihood of a bilingual corpus obtained from the proceedings of the Canadian Parliame</context>
</contexts>
<marker>Brown, Pietra, S, V, Mercer, 1991</marker>
<rawString>Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mercer, R. (1991). A Statistical Approach to Sense Disambiguation in Machine Translation. In Proceedings, DARPA Speech and Natural Language Workshop, 146-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. and Thomas, J. (1991). Elements of Information Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Csiszdr</author>
</authors>
<date>1975</date>
<booktitle>I-Divergence Geometry of Probability Distributions and Minimization Problems, The Annals of Probability,</booktitle>
<pages>3--1</pages>
<marker>Csiszdr, 1975</marker>
<rawString>Csiszdr, I. (1975). I-Divergence Geometry of Probability Distributions and Minimization Problems, The Annals of Probability, 3(1):146-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ibid</author>
</authors>
<title>A Geometric Interpretation of Darroch and Ratcliff&apos;s Generalized Iterative Scaling. The Annals of Statistics,</title>
<date>1989</date>
<pages>17--3</pages>
<marker>ibid, 1989</marker>
<rawString>ibid. (1989). A Geometric Interpretation of Darroch and Ratcliff&apos;s Generalized Iterative Scaling. The Annals of Statistics, 17(3):1409-1413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Csiszdr</author>
<author>G Tusnady</author>
</authors>
<title>Information Geometry and Alternating Minimization Procedures.</title>
<date>1984</date>
<journal>Statistics &amp; Decisions, Supplemental Issue,</journal>
<volume>1</volume>
<pages>205--237</pages>
<marker>Csiszdr, Tusnady, 1984</marker>
<rawString>Csiszdr, L. and Tusnady, G. (1984). Information Geometry and Alternating Minimization Procedures. Statistics &amp; Decisions, Supplemental Issue, no. 1, 205-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized Iterative Scaling for Log-linear Models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<volume>no.</volume>
<pages>43--1470</pages>
<contexts>
<context position="24121" citStr="Darroch and Ratcliff (1972)" startWordPosition="4075" endWordPosition="4078"> 4f (A) is well behaved, since it is smooth and convex-11 in A. Consequently, a variety of numerical methods can be used to calculate A*. One simple method is coordinate-wise ascent, in which A* is computed by iteratively maximizing 41(A) one coordinate at a time. When applied to the maximum entropy problem, this technique yields the popular Brown algorithm (Brown 1959). Other general purpose methods that can be used to maximize T(A) include gradient ascent and conjugate gradient. An optimization method specifically tailored to the maximum entropy problem is the iterative scaling algorithm of Darroch and Ratcliff (1972). We present here a version of this algorithm specifically designed for the problem at hand; a proof of the monotonicity and convergence of the algorithm is given in Della Pietra et al. (1995). The algorithm is applicable whenever the feature functions f,(x,y) are nonnegative: f, y) &gt; 0 for all i, x, and y (15) This is, of course, true for the binary-valued feature functions we are considering here. The algorithm generalizes the Darroch-Ratcliff procedure, which requires, in addition to the nonnegativity, that the feature functions satisfy Ei f,(x, y) = 1 for all x, y. Algorithm 1: Improved It</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>Darroch, J. N. and Ratcliff, D. (1972). Generalized Iterative Scaling for Log-linear Models. Annals of Mathematical Statistics, no. 43,1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V Gillett</author>
<author>J Lafferty</author>
<author>J Printz</author>
<author>H</author>
<author>L UreA</author>
</authors>
<title>Inference and Estimation of a Long-range Trigram Model.&amp;quot;</title>
<date>1994</date>
<booktitle>In Lecture Notes in Artificial Intelligence,</booktitle>
<volume>862</volume>
<pages>78--92</pages>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="2232" citStr="Pietra et al. (1994)" startWordPosition="328" endWordPosition="331"> the best paid statistical modelers) build models based on past stock price movements to predict tomorrow&apos;s fluctuations and alter their portfolios to capitalize on the predicted future. At the other end of the pay scale reside natural language researchers, who design language and acoustic models for use in speech recognition systems and related applications. The past few decades have witnessed significant progress toward increasing the predictive capacity of statistical models of natural language. In language modeling, for instance, Bahl et al. (1989) have used decision tree models and Della Pietra et al. (1994) have used automatically inferred link grammars to model long range correlations in language. In parsing, Black et al. (1992) have described how to extract grammatical * This research, supported in part by ARPA under grant ONR N00014-91—C-0135, was conducted while the authors were at the IBM T. J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY 10598 t Now at Computer Science Department, Columbia University. t Now at Renaissance Technologies, Stony Brook, NY © 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 1 rules from annotated text autom</context>
</contexts>
<marker>Pietra, S, Gillett, Lafferty, Printz, H, UreA, 1994</marker>
<rawString>Della Pietra, S.; Della Pietra, V.; Gillett, J.; Lafferty, J.; Printz, H.; and UreA, L. (1994). &amp;quot;Inference and Estimation of a Long-range Trigram Model.&amp;quot; In Lecture Notes in Artificial Intelligence, 862. Springer-Verlag, 78-92.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V</author>
</authors>
<marker>Pietra, S, V, </marker>
<rawString>Della Pietra, S.; Della Pietra, V.; and</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields&amp;quot;</title>
<date>1995</date>
<tech>CMU Technical Report CMU-CS-95-144.</tech>
<marker>Lafferty, 1995</marker>
<rawString>Lafferty, J. (1995). &amp;quot;Inducing features of random fields&amp;quot; CMU Technical Report CMU-CS-95-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A. P.; Laird, N. M.; and Rubin, D. B. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Guiasu</author>
<author>A Shenitzer</author>
</authors>
<title>The Principle of Maximum Entropy.</title>
<date>1985</date>
<journal>The Mathematical Intelligencer,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="9384" citStr="Guiasu and Shenitzer 1985" startWordPosition="1522" endWordPosition="1525">odel p at each step in the above example. The maximum entropy concept has a long history. Adopting the least complex hypothesis possible is embodied in Occam&apos;s razor (&amp;quot;Nunquam ponenda est pluralitas sine necesitate.&amp;quot;) and even appears earlier, in the Bible and the writings of Herotodus (Jaynes 1990). Laplace might justly be considered the father of maximum entropy, having enunciated the underlying theme 200 years ago in his &amp;quot;Principle of Insufficient Reason:&amp;quot; when one has no information to distinguish between the probability of two events, the best strategy is to consider them equally likely (Guiasu and Shenitzer 1985). As E. T. Jaynes, a more recent pioneer of maximum entropy, put it (Jaynes 1990): ... the fact that a certain probability distribution maximizes entropy subject to certain constraints representing our incomplete information, is the fundamental property which justifies use of that distribution for inference; it agrees with everything that is known, but carefully avoids assuming anything that is not known. It is a transcription into mathematics of an ancient principle of wisdom ... 3. Maximum Entropy Modeling We consider a random process that produces an output value y, a member of a finite set</context>
</contexts>
<marker>Guiasu, Shenitzer, 1985</marker>
<rawString>Guiasu, S. and Shenitzer, A. (1985). The Principle of Maximum Entropy. The Mathematical Intelligencer, 7(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Jaynes</author>
</authors>
<title>Notes on Present Status and Future Prospects.&amp;quot; In Maximum Entropy and Bayesian Methods,</title>
<date>1990</date>
<pages>1--13</pages>
<publisher>Kluwer,</publisher>
<note>edited by</note>
<contexts>
<context position="9058" citStr="Jaynes 1990" startWordPosition="1474" endWordPosition="1475">next few pages. Intuitively, the principle is simple: model all that is known and assume nothing about that which is unknown. In other words, given a collection of facts, choose a model consistent with all the facts, but otherwise as uniform as possible. This is precisely the approach we took in selecting our model p at each step in the above example. The maximum entropy concept has a long history. Adopting the least complex hypothesis possible is embodied in Occam&apos;s razor (&amp;quot;Nunquam ponenda est pluralitas sine necesitate.&amp;quot;) and even appears earlier, in the Bible and the writings of Herotodus (Jaynes 1990). Laplace might justly be considered the father of maximum entropy, having enunciated the underlying theme 200 years ago in his &amp;quot;Principle of Insufficient Reason:&amp;quot; when one has no information to distinguish between the probability of two events, the best strategy is to consider them equally likely (Guiasu and Shenitzer 1985). As E. T. Jaynes, a more recent pioneer of maximum entropy, put it (Jaynes 1990): ... the fact that a certain probability distribution maximizes entropy subject to certain constraints representing our incomplete information, is the fundamental property which justifies use </context>
</contexts>
<marker>Jaynes, 1990</marker>
<rawString>Jaynes, E. T. (1990) &amp;quot;Notes on Present Status and Future Prospects.&amp;quot; In Maximum Entropy and Bayesian Methods, edited by W. T. Grandy and L. H. Schick. Kluwer, 1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated Estimation of Markov Source Parameters from Sparse Data.</title>
<date>1980</date>
<booktitle>In Proceedings, Workshop on Pattern Recognition in Practice,</booktitle>
<location>Amsterdam, The Netherlands.</location>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek, F. and Mercer, R. L. (1980). Interpolated Estimation of Markov Source Parameters from Sparse Data. In Proceedings, Workshop on Pattern Recognition in Practice, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lucassen</author>
<author>R Mercer</author>
</authors>
<title>An Information Theoretic Approach to Automatic Determination of Phonemic Baseforms.</title>
<date>1984</date>
<booktitle>In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>42--5</pages>
<location>San Diego, CA,</location>
<contexts>
<context position="2954" citStr="Lucassen and Mercer (1984)" startWordPosition="435" endWordPosition="438"> parsing, Black et al. (1992) have described how to extract grammatical * This research, supported in part by ARPA under grant ONR N00014-91—C-0135, was conducted while the authors were at the IBM T. J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY 10598 t Now at Computer Science Department, Columbia University. t Now at Renaissance Technologies, Stony Brook, NY © 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 1 rules from annotated text automatically and incorporate these rules into statistical models of grammar. In speech recognition, Lucassen and Mercer (1984) have introduced a technique for automatically discovering relevant features for the translation of word spelling to word pronunciation. These efforts, while varied in specifics, all confront two essential tasks of statistical modeling. The first task is to determine a set of statistics that captures the behavior of a random process. Given a set of statistics, the second task is to corral these facts into an accurate model of the process—a model capable of predicting the future output of the process. The first task is one of feature selection; the second is one of model selection. In the follo</context>
</contexts>
<marker>Lucassen, Mercer, 1984</marker>
<rawString>Lucassen, J. and Mercer, R. (1984). An Information Theoretic Approach to Automatic Determination of Phonemic Baseforms. In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing, San Diego, CA, 42.5.1-42.5.4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging Text with a Probabilistic Model.</title>
<date>1990</date>
<booktitle>In Proceedings, IBM Natural Language ITL,</booktitle>
<pages>161--172</pages>
<location>Paris,</location>
<contexts>
<context position="56806" citStr="Merialdo (1990)" startWordPosition="9777" endWordPosition="9778">ent sequentially while generating the translation, working from left to right in the French sentence. We now describe a maximum entropy model that assigns to each location in a French sentence a score that is a measure of the safety in cutting the sentence at that location. We begin as in the word translation problem, with a training sample of English-French sentence pairs (E, F) randomly extracted from the Hansard corpus. For each sentence pair we use the basic translation model to compute the Viterbi alignment A between E and F. We also use a stochastic part-of-speech tagger as described in Merialdo (1990) to label each word in F with its part of speech. For each position j in F we then construct a (x, y) training event. The value y is rift if a rift belongs at position j and is no-rift otherwise. The context information x is reminiscent of that employed in the word translation application described earlier. It includes a six-word window of French words: three to the left of yi and three to the right of yl. It also includes the part-of-speech tags for these words, and the classes of these words as derived from a mutual-information clustering scheme described in Brown et al. (1990). The complete</context>
</contexts>
<marker>Merialdo, 1990</marker>
<rawString>Merialdo, B. (1990). Tagging Text with a Probabilistic Model. In Proceedings, IBM Natural Language ITL, Paris, France, 161-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nadas</author>
<author>R Mercer</author>
<author>L Bahl</author>
<author>R Bakis</author>
<author>P Cohen</author>
<author>A- Cole</author>
<author>E Jelinek</author>
<author>B Lewis</author>
</authors>
<title>Continuous Speech Recognition with Automatically Selected Acoustic Prototypes Obtained by either Bootstrapping or Clustering. In</title>
<date>1981</date>
<booktitle>Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>1153--1155</pages>
<location>Atlanta, GA,</location>
<marker>Nadas, Mercer, Bahl, Bakis, Cohen, Cole, Jelinek, Lewis, 1981</marker>
<rawString>Nadas, A.; Mercer, R.; Bahl, L.; Bakis, R.; Cohen, P.; Cole, A.-; Jelinek, E; and Lewis, B. (1981). Continuous Speech Recognition with Automatically Selected Acoustic Prototypes Obtained by either Bootstrapping or Clustering. In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing, Atlanta, GA, 1153-1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Sokolnikoff</author>
<author>R M Redheffer</author>
</authors>
<date>1966</date>
<booktitle>Mathematics of Physics and Modern Engineering, Second Edition,</booktitle>
<publisher>McGraw-Hill Book Company.</publisher>
<marker>Sokolnikoff, Redheffer, 1966</marker>
<rawString>Sokolnikoff, I. S. and Redheffer, R. M. (1966). Mathematics of Physics and Modern Engineering, Second Edition, McGraw-Hill Book Company.</rawString>
</citation>
<citation valid="false">
<title>Set ao &lt;– 0 3. Repeat the following until Gs,f (an) has converged: Compute an±i from an using (35) Compute Gs,f(an±i ) using (26) 4. Set – AL(S,</title>
<journal>f) Gs,f(an)</journal>
<marker></marker>
<rawString>2. Set ao &lt;– 0 3. Repeat the following until Gs,f (an) has converged: Compute an±i from an using (35) Compute Gs,f(an±i ) using (26) 4. Set – AL(S, f) Gs,f(an)</rawString>
</citation>
<citation valid="false">
<title>Computing Approximate Gains in Parallel Algorithm 4: Computing Approximate Gains for A Collection of Features Input: Collection .7. of candidate features; empirical distribution /3(x, y); initial model ps Output: Approximate gain –AL(S,f) for each candidate feature f E 1. For each f E F, calculate /3(f), the expected value of f in the training data 2. For each x, determine the set .F(x) c F of f that are active for x: .F(x) {f ETIf(x,Y)Ps(Yix)P(x)&gt; 0 for some yl (41)</title>
<marker></marker>
<rawString>Computing Approximate Gains in Parallel Algorithm 4: Computing Approximate Gains for A Collection of Features Input: Collection .7. of candidate features; empirical distribution /3(x, y); initial model ps Output: Approximate gain –AL(S,f) for each candidate feature f E 1. For each f E F, calculate /3(f), the expected value of f in the training data 2. For each x, determine the set .F(x) c F of f that are active for x: .F(x) {f ETIf(x,Y)Ps(Yix)P(x)&gt; 0 for some yl (41)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>