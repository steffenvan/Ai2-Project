<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006054">
<title confidence="0.9985185">
Fixed Length Word Suffix for Factored
Statistical Machine Translation
</title>
<author confidence="0.992474">
Narges Sharif Razavian Stephan Vogel
</author>
<affiliation confidence="0.998288666666667">
School of Computer Science School of Computer Science
Carnegie Mellon Universiy Carnegie Mellon Universiy
Pittsburgh, USA Pittsburgh, USA
</affiliation>
<email confidence="0.996153">
nsharifr@cs.cmu.edu stephan.vogel@cs.cmu.edu
</email>
<sectionHeader confidence="0.993429" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985033125">
Factored Statistical Machine Translation ex-
tends the Phrase Based SMT model by al-
lowing each word to be a vector of factors.
Experiments have shown effectiveness of
many factors, including the Part of Speech
tags in improving the grammaticality of the
output. However, high quality part of
speech taggers are not available in open
domain for many languages. In this paper
we used fixed length word suffix as a new
factor in the Factored SMT, and were able
to achieve significant improvements in three
set of experiments: large NIST Arabic to
English system, medium WMT Spanish to
English system, and small TRANSTAC
English to Iraqi system.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999837109375">
Statistical Machine Translation(SMT) is current-
ly the state of the art solution to the machine
translation. Phrase based SMT is also among the
top performing approaches available as of today.
This approach is a purely lexical approach, using
surface forms of the words in the parallel corpus
to generate the translations and estimate proba-
bilities. It is possible to incorporate syntactical
information into this framework through differ-
ent ways. Source side syntax based re-ordering
as preprocessing step, dependency based reorder-
ing models, cohesive decoding features are
among many available successful attempts for
the integration of syntax into the translation
model. Factored translation modeling is another
way to achieve this goal. These models allow
each word to be represented as a vector of factors
rather than a single surface form. Factors can
represent richer expression power on each word.
Any factors such as word stems, gender, part of
speech, tense, etc. can be easily used in this
framework.
Previous work in factored translation modeling
have reported consistent improvements from Part
of Speech(POS) tags, morphology, gender, and
case factors (Koehn et. a. 2007). In another work,
Birch et. al. 2007 have achieved improvement
using Combinational Categorial Grammar (CCG)
super-tag factors. Creating the factors is done as
a preprocessing step, and so far, most of the ex-
periments have assumed existence of external
tools for the creation of these factors (i. e. Part of
speech taggers, CCG parsers, etc.). Unfortunately
high quality language processing tools, especial-
ly for the open domain, are not available for most
languages.
While linguistically identifiable representations
(i.e. POS tags, CCG supertags, etc) have been
very frequently used as factors in many applica-
tions including MT, simpler representations have
also been effective in achieving the same result
in other application areas. Grzymala-Busse and
Old 1997, DINCER et.al. 2008, were able to use
fixed length suffixes as features for training a
POS tagging. In another work Saberi and Perrot
1999 showed that reversing middle chunks of the
words while keeping the first and last part intact,
does not decrease listeners’ recognition ability.
This result is very relevant to Machine Transla-
tion, suggesting that inaccurate context which is
usually modeled with n-gram language models,
can still be as effective as accurate surface forms.
Another research (Rawlinson 1997) confirms this
finding; this time in textual domain, observing
that randomization of letters in the middle of
words has little or no effect on the ability of
skilled readers to understand the text. These re-
sults suggest that the inexpensive representation-
al factors which do not need unavailable tools
might also be worth investigating.
These results encouraged us to introduce lan-
guage independent simple factors for machine
translation. In this paper, following the work of
Grzymala-Busse et. al. we used fixed length suf-
</bodyText>
<page confidence="0.975005">
147
</page>
<note confidence="0.4418835">
Proceedings of the ACL 2010 Conference Short Papers, pages 147–150,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943733333333">
fix as word factor, to lower the perplexity of the
language model, and have the factors roughly
function as part of speech tags, thus increasing
the grammaticality of the translation results. We
were able to obtain consistent, significant im-
provements over our baseline in 3 different expe-
riments, large NIST Arabic to English system,
medium WMT Spanish to English system, and
small TRANSTAC English to Iraqi system.
The rest of this paper is as follows. Section 2
briefly reviews the Factored Translation Models.
In section 3 we will introduce our model, and
section 4 will contain the experiments and the
analysis of the results, and finally, we will con-
clude this paper in section 5.
</bodyText>
<sectionHeader confidence="0.983696" genericHeader="method">
2 Factored Translation Model
</sectionHeader>
<bodyText confidence="0.9961965">
Statistical Machine Translation uses the log li-
near combination of a number of features, to
compute the highest probable hypothesis as the
translation.
</bodyText>
<equation confidence="0.554138">
e = argmaxe p(e|f) = argmaxe p exp Σi=1n Ai hi(e,f)
</equation>
<bodyText confidence="0.999242888888889">
In phrase based SMT, assuming the source and
target phrase segmentation as {(fi,ei)}, the most
important features include: the Language Model
feature hlm(e,f) = plm(e); the phrase translation
feature ht(e,f) defined as product of translation
probabilities, lexical probabilities and phrase pe-
nalty; and the reordering probability, hd(e,f),
usually defined as πi=1n d(starti,endi-1) over the
source phrase reordering events.
Factored Translation Model, recently intro-
duced by (Koehn et. al. 2007), allow words to
have a vector representation. The model can then
extend the definition of each of the features from
a uni-dimensional value to an arbitrary joint and
conditional combination of features. Phrase
based SMT is in fact a special case of Factored
SMT.
The factored features are defined as an exten-
sion of phrase translation features. The function
τ(fj,ej), which was defined for a phrase pair be-
fore, can now be extended as a log linear combi-
nation Ef τf(fjf,ejf). The model also allows for a
generation feature, defining the relationship be-
tween final surface form and target factors. Other
features include additional language model fea-
tures over individual factors, and factored reor-
dering features.
</bodyText>
<figureCaption confidence="0.919206">
Figure 1 shows an example of a possible fac-
tored model.
Figure 1: An example of a Factored Translation and
Generation Model
</figureCaption>
<bodyText confidence="0.999791">
In this particular model, words on both source
and target side are represented as a vector of four
factors: surface form, lemma, part of speech
(POS) and the morphology. The target phrase is
generated as follows: Source word lemma gene-
rates target word lemma. Source word&apos;s Part of
speech and morphology together generate the
target word&apos;s part of speech and morphology, and
from its lemma, part of speech and morphology
the surface form of the target word is finally gen-
erated. This model has been able to result in
higher translation BLEU score as well as gram-
matical coherency for English to German, Eng-
lish to Spanish, English to Czech, English to
Chinese, Chinese to English and German to Eng-
lish.
</bodyText>
<sectionHeader confidence="0.982918" genericHeader="method">
3 Fixed Length Suffix Factors for Fac-
tored Translation Modeling
</sectionHeader>
<bodyText confidence="0.999267695652174">
Part of speech tagging, constituent and depen-
dency parsing, combinatory categorical grammar
super tagging are used extensively in most appli-
cations when syntactic representations are
needed. However training these tools require
medium size treebanks and tagged data, which
for most languages will not be available for a
while. On the other hand, many simple words
features, such as their character n-grams, have in
fact proven to be comparably as effective in
many applications.
(Keikha et. al. 2008) did an experiment on text
classification on noisy data, and compared sever-
al word representations. They compared surface
form, stemmed words, character n-grams, and
semantic relationships, and found that for noisy
and open domain text, character-ngrams outper-
form other representations when used for text
classification. In another work (Dincer et al
2009) showed that using fixed length word end-
ing outperforms whole word representation for
training a part of speech tagger for Turkish lan-
guage.
</bodyText>
<page confidence="0.995276">
148
</page>
<figureCaption confidence="0.918467333333333">
Based on this result, we proposed a suffix fac-
tored model for translation, which is shown in
Figure 2.
</figureCaption>
<figure confidence="0.6562715">
about 8K distinct words. Table 1 shows the result
(BLEU Score) of the system compared to the
baseline.
LM
Word Language Model
Suffix Language Model
</figure>
<figureCaption confidence="0.99693375">
Figure 2: Suffix Factored model: Source word de-
termines factor vectors (target word, target word suf-
fix) and each factor will be associated with its
language model.
</figureCaption>
<bodyText confidence="0.9999595">
Based on this model, the final probability of
the translation hypothesis will be the log linear
combination of phrase probabilities, reordering
model probabilities, and each of the language
</bodyText>
<equation confidence="0.9599305">
models’ probabilities.
P(e|f) ~ plm-word(eword)* plm-suffix(esuffix)
n
* Σi=1 p(eword-j &amp; esuffix-j|fj)
I&apos;
* Σi=I p(fj  |eword-j &amp; esuffix-j)
</equation>
<bodyText confidence="0.9996041">
Where plm-word is the n-gram language model
probability over the word surface sequence, with
the language model built from the surface forms.
Similarly, plm-suffix(esuffix) is the language model
probability over suffix sequences. p(eword-j &amp;
esuffix-j|fj) and p(fj  |eword-j &amp; esuffix-j) are translation
probabilities for each phrase pair i , used in by
the decoder. This probability is estimated after
the phrase extraction step which is based on
grow-diag heuristic at this stage.
</bodyText>
<sectionHeader confidence="0.997752" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999968666666667">
We used Moses implementation of the factored
model for training the feature weights, and SRI
toolkit for building n-gram language models. The
baseline for all systems included the moses sys-
tem with lexicalized re-ordering, SRI 5-gram
language models.
</bodyText>
<subsectionHeader confidence="0.993673">
4.1 Small System from Dialog Domain:
English to Iraqi
</subsectionHeader>
<bodyText confidence="0.999917285714286">
This system was TRANSTAC system, which
was built on about 650K sentence pairs with the
average sentence length of 5.9 words. After
choosing length 3 for suffixes, we built a new
parallel corpus, and SRI 5-gram language models
for each factor. Vocabulary size for the surface
form was 110K whereas the word suffixes had
</bodyText>
<table confidence="0.996675333333333">
System Tune on Test on Test on
Set- Set- Set-
July07 June08 Nov08
Baseline 27.74 21.73 15.62
Factored 28.83 22.84 16.41
Improvement 1.09 1.11 0.79
</table>
<tableCaption confidence="0.97171">
Table 1: BLEU score, English to Iraqi Transtac sys-
tem, comparing Factored and Baseline systems.
</tableCaption>
<bodyText confidence="0.999888">
As you can see, this improvement is consistent
over multiple unseen datasets. Arabic cases and
numbers show up as the word suffix. Also, verb
numbers usually appear partly as word suffix and
in some cases as word prefix. Defining a lan-
guage model over the word endings increases the
probability of sequences which have this case
and number agreement, favoring correct agree-
ments over the incorrect ones.
</bodyText>
<subsectionHeader confidence="0.986746">
4.2 Medium System on Travel Domain:
Spanish to English
</subsectionHeader>
<bodyText confidence="0.99987375">
This system is the WMT08 system, on a corpus
of 1.2 million sentence pairs with average sen-
tence length 27.9 words. Like the previous expe-
riment, we defined the 3 character suffix of the
words as the second factor, and built the lan-
guage model and reordering model on the joint
event of (surface, suffix) pairs. We built 5-gram
language models for each factor. The system had
about 97K distinct vocabulary in the surface lan-
guage model, which was reduced to 8K using the
suffix corpus. Having defined the baseline, the
system results are as follows.
</bodyText>
<table confidence="0.9991432">
System Tune- Test set-
WMT06 WMT08
Baseline 33.34 32.53
Factored 33.60 32.84
Improvement 0.26 0.32
</table>
<tableCaption confidence="0.971764">
Table 2: BLEU score, Spanish to English WMT sys-
tem, comparing Factored and Baseline systems.
</tableCaption>
<bodyText confidence="0.8976465">
Here, we see improvement with the suffix fac-
tors compared to the baseline system. Word end-
ings in English language are major indicators of
word’s part of speech in the sentence. In fact
</bodyText>
<figure confidence="0.965569333333333">
Source Target
Word Word 4
Suffix 4
</figure>
<page confidence="0.994873">
149
</page>
<bodyText confidence="0.9997949">
most common stemming algorithm, Porter’s
Stemmer, works by removing word’s suffix.
Having a language model on these suffixes push-
es the common patterns of these suffixes to the
top, making the more grammatically coherent
sentences to achieve a better probability.
improvements over the baseline. This result, ob-
tained from the language independent and inex-
pensive factor, shows promising new
opportunities for all language pairs.
</bodyText>
<sectionHeader confidence="0.850441" genericHeader="conclusions">
References
</sectionHeader>
<subsectionHeader confidence="0.854003">
4.3 Large NIST 2009 System: Arabic to
English
</subsectionHeader>
<bodyText confidence="0.999947375">
We used NIST2009 system as our baseline in
this experiment. The corpus had about 3.8 Mil-
lion sentence pairs, with average sentence length
of 33.4 words. The baseline defined the lexica-
lized reordering model. As before we defined 3
character long word endings, and built 5-gram
SRI language models for each factor. The result
of this experiment is shown in table 3.
</bodyText>
<table confidence="0.995416">
System Tune Test on Test on Test
on Dev07 Dev07 on
MT06 News Weblog MT08
Wire
Baseline 43.06 48.87 37.84 41.70
Factored 44.20 50.39 39.93 42.74
Improve 1.14 1.52 2.09 1.04
ment
</table>
<tableCaption confidence="0.9759075">
Table 3: BLEU score, Arabic to English NIST 2009
system, comparing Factored and Baseline systems.
</tableCaption>
<bodyText confidence="0.999973909090909">
This result confirms the positive effect of the
suffix factors even on large systems. As men-
tioned before we believe that this result is due to
the ability of the suffix to reduce the word into a
very simple but rough grammatical representa-
tion. Defining language models for this factor
forces the decoder to prefer sentences with more
probable suffix sequences, which is believed to
increase the grammaticality of the result. Future
error analysis will show us more insight of the
exact effect of this factor on the outcome.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.986916">
In this paper we introduced a simple yet very
effective factor: fixed length word suffix, to use
in Factored Translation Models. This simple fac-
tor has been shown to be effective as a rough
replacement for part of speech. We tested our
factors in three experiments in a small, English to
Iraqi system, a medium sized system of Spanish
to English, and a large system, NIST09 Arabic to
English. We observed consistent and significant
</bodyText>
<reference confidence="0.997614142857143">
Birch, A., Osborne, M., and Koehn, P. CCG supertags
in factored statistical machine translation. Proceed-
ings of the Second Workshop on Statistical Ma-
chine Translation, pages 9–16, Prague, Czech
Republic. Association for Computational Linguis-
tics, 2007.
Dincer T., Karaoglan B. and Kisla T., A Suffix Based
Part-Of-Speech Tagger For Turkish, Fifth Interna-
tional Conference on Information Technology:
New Generations, 2008.
Grzymala-Busse J.W., Old L.J. A machine learning
experiment to determine part of speech from word-
endings, Lecture Notes in Computer Science,
Communications Session 6B Learning and Discov-
ery Systems, 1997.
Keikha M., Sharif Razavian N, Oroumchian F., and
Seyed Razi H., Document Representation and
Quality of Text: An Analysis, Chapter 12, Survey
of Text Mining II, Springer London, 2008.
Koehn Ph., Hoang H., Factored Translation Models,
Proceedings of 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), 2007.
Rawlinson G. E., The significance of letter position in
word recognition, PhD Thesis, Psychology De-
partment, University of Nottingham, Nottingham
UK, 1976.
Saberi K and Perrot D R, Cognitive restoration of
reversed speech, Nature (London) 1999.
</reference>
<page confidence="0.998255">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.485226">
<title confidence="0.9993185">Fixed Length Word Suffix for Factored Statistical Machine Translation</title>
<author confidence="0.999622">Narges Sharif Razavian Stephan Vogel</author>
<affiliation confidence="0.999406">School of Computer Science School of Computer Science Carnegie Mellon Universiy Carnegie Mellon Universiy</affiliation>
<address confidence="0.815962">Pittsburgh, USA Pittsburgh, USA</address>
<email confidence="0.999433">nsharifr@cs.cmu.edustephan.vogel@cs.cmu.edu</email>
<abstract confidence="0.960865529411765">Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Birch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<location>Prague, Czech Republic.</location>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Birch, A., Osborne, M., and Koehn, P. CCG supertags in factored statistical machine translation. Proceedings of the Second Workshop on Statistical Machine Translation, pages 9–16, Prague, Czech Republic. Association for Computational Linguistics, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dincer</author>
<author>B Karaoglan</author>
<author>T Kisla</author>
</authors>
<title>A Suffix Based Part-Of-Speech Tagger For Turkish,</title>
<date>2008</date>
<booktitle>Fifth International Conference on Information Technology: New Generations,</booktitle>
<marker>Dincer, Karaoglan, Kisla, 2008</marker>
<rawString>Dincer T., Karaoglan B. and Kisla T., A Suffix Based Part-Of-Speech Tagger For Turkish, Fifth International Conference on Information Technology: New Generations, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Grzymala-Busse</author>
<author>L J Old</author>
</authors>
<title>A machine learning experiment to determine part of speech from wordendings,</title>
<date>1997</date>
<booktitle>Lecture Notes in Computer Science, Communications Session 6B Learning and Discovery Systems,</booktitle>
<contexts>
<context position="2901" citStr="Grzymala-Busse and Old 1997" startWordPosition="435" endWordPosition="438">the factors is done as a preprocessing step, and so far, most of the experiments have assumed existence of external tools for the creation of these factors (i. e. Part of speech taggers, CCG parsers, etc.). Unfortunately high quality language processing tools, especially for the open domain, are not available for most languages. While linguistically identifiable representations (i.e. POS tags, CCG supertags, etc) have been very frequently used as factors in many applications including MT, simpler representations have also been effective in achieving the same result in other application areas. Grzymala-Busse and Old 1997, DINCER et.al. 2008, were able to use fixed length suffixes as features for training a POS tagging. In another work Saberi and Perrot 1999 showed that reversing middle chunks of the words while keeping the first and last part intact, does not decrease listeners’ recognition ability. This result is very relevant to Machine Translation, suggesting that inaccurate context which is usually modeled with n-gram language models, can still be as effective as accurate surface forms. Another research (Rawlinson 1997) confirms this finding; this time in textual domain, observing that randomization of le</context>
</contexts>
<marker>Grzymala-Busse, Old, 1997</marker>
<rawString>Grzymala-Busse J.W., Old L.J. A machine learning experiment to determine part of speech from wordendings, Lecture Notes in Computer Science, Communications Session 6B Learning and Discovery Systems, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Keikha</author>
<author>Sharif Razavian N</author>
<author>F Oroumchian</author>
<author>Seyed Razi H</author>
</authors>
<title>Document Representation and Quality of Text: An Analysis, Chapter 12, Survey of Text Mining II,</title>
<date>2008</date>
<publisher>Springer</publisher>
<location>London,</location>
<marker>Keikha, N, Oroumchian, H, 2008</marker>
<rawString>Keikha M., Sharif Razavian N, Oroumchian F., and Seyed Razi H., Document Representation and Quality of Text: An Analysis, Chapter 12, Survey of Text Mining II, Springer London, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn Ph</author>
<author>H Hoang</author>
</authors>
<title>Factored Translation Models,</title>
<date>2007</date>
<booktitle>Proceedings of 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<marker>Ph, Hoang, 2007</marker>
<rawString>Koehn Ph., Hoang H., Factored Translation Models, Proceedings of 45th Annual Meeting of the Association for Computational Linguistics (ACL), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Rawlinson</author>
</authors>
<title>The significance of letter position in word recognition,</title>
<date>1976</date>
<tech>PhD Thesis,</tech>
<institution>Psychology Department, University of Nottingham,</institution>
<location>Nottingham UK,</location>
<marker>Rawlinson, 1976</marker>
<rawString>Rawlinson G. E., The significance of letter position in word recognition, PhD Thesis, Psychology Department, University of Nottingham, Nottingham UK, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Saberi</author>
<author>D R Perrot</author>
</authors>
<title>Cognitive restoration of reversed speech,</title>
<date>1999</date>
<location>Nature (London)</location>
<contexts>
<context position="3040" citStr="Saberi and Perrot 1999" startWordPosition="459" endWordPosition="462">ese factors (i. e. Part of speech taggers, CCG parsers, etc.). Unfortunately high quality language processing tools, especially for the open domain, are not available for most languages. While linguistically identifiable representations (i.e. POS tags, CCG supertags, etc) have been very frequently used as factors in many applications including MT, simpler representations have also been effective in achieving the same result in other application areas. Grzymala-Busse and Old 1997, DINCER et.al. 2008, were able to use fixed length suffixes as features for training a POS tagging. In another work Saberi and Perrot 1999 showed that reversing middle chunks of the words while keeping the first and last part intact, does not decrease listeners’ recognition ability. This result is very relevant to Machine Translation, suggesting that inaccurate context which is usually modeled with n-gram language models, can still be as effective as accurate surface forms. Another research (Rawlinson 1997) confirms this finding; this time in textual domain, observing that randomization of letters in the middle of words has little or no effect on the ability of skilled readers to understand the text. These results suggest that t</context>
</contexts>
<marker>Saberi, Perrot, 1999</marker>
<rawString>Saberi K and Perrot D R, Cognitive restoration of reversed speech, Nature (London) 1999.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>