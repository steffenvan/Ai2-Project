<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000330">
<title confidence="0.988522">
Distribution-Based Pruning of Backoff Language Models
</title>
<author confidence="0.982704">
Jianfeng Gao
</author>
<affiliation confidence="0.97454">
Microsoft Research China
</affiliation>
<address confidence="0.897881">
No. 49 Zhichun Road Haidian District
100080, China,
</address>
<email confidence="0.994676">
jfgao@microsoft.com
</email>
<author confidence="0.736412">
Kai-Fu Lee
</author>
<affiliation confidence="0.751959">
Microsoft Research China
</affiliation>
<address confidence="0.7321815">
No. 49 Zhichun Road Haidian District
100080, China,
</address>
<email confidence="0.991137">
kfl@microsoft.com
</email>
<sectionHeader confidence="0.993818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997629166666667">
We propose a distribution-based pruning of
n-gram backoff language models. Instead
of the conventional approach of pruning
n-grams that are infrequent in training data,
we prune n-grams that are likely to be
infrequent in a new document. Our method
is based on the n-gram distribution i.e. the
probability that an n-gram occurs in a new
document. Experimental results show that
our method performed 7-9% (word
perplexity reduction) better than
conventional cutoff methods.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997005">
Statistical language modelling (SLM) has been
successfully applied to many domains such as
speech recognition (Jelinek, 1990), information
retrieval (Miller et al., 1999), and spoken
language understanding (Zue, 1995). In
particular, n-gram language model (LM) has
been demonstrated to be highly effective for
these domains. N-gram LM estimates the
probability of a word given previous words,
P(wn|w1,...,wn-1).
In applying an SLM, it is usually the case
that more training data will improve a language
model. However, as training data size
increases, LM size increases, which can lead to
models that are too large for practical use.
To deal with the problem, count cutoff
(Jelinek, 1990) is widely used to prune
language models. The cutoff method deletes
from the LM those n-grams that occur
infrequently in the training data. The cutoff
method assumes that if an n-gram is infrequent
in training data, it is also infrequent in testing
data. But in the real world, training data rarely
matches testing data perfectly. Therefore, the
count cutoff method is not perfect.
In this paper, we propose a
distribution-based cutoff method. This
approach estimates if an n-gram is “likely to be
infrequent in testing data”. To determine this
likelihood, we divide the training data into
partitions, and use a cross-validation-like
approach. Experiments show that this method
performed 7-9% (word perplexity reduction)
better than conventional cutoff methods.
In section 2, we discuss prior SLM research,
including backoff bigram LM, perplexity, and
related works on LM pruning methods. In
section 3, we propose a new criterion for LM
pruning based on n-gram distribution, and
discuss in detail how to estimate the
distribution. In section 4, we compare our
method with count cutoff, and present
experimental results in perplexity. Finally, we
present our conclusions in section 5.
</bodyText>
<sectionHeader confidence="0.817589" genericHeader="method">
2 Backoff Bigram and Cutoff
</sectionHeader>
<bodyText confidence="0.999738333333333">
One of the most successful forms of SLM is the
n-gram LM. N-gram LM estimates the
probability of a word given the n-1 previous
words, P(wn|w1,...,wn-1). In practice, n is usually
set to 2 (bigram), or 3 (trigram). For simplicity,
we restrict our discussion to bigram, P(wn|wn-1),
which assumes that the probability of a word
depends only on the identity of the immediately
preceding word. But our approach extends to
any n-gram.
Perplexity is the most common metric for
evaluating a bigram LM. It is defined as,
</bodyText>
<equation confidence="0.937269">
N
− Y—logP(w&amp;quot;w&amp;quot;)
PP =2 NY—
1
(1)
</equation>
<bodyText confidence="0.999979815384615">
where N is the length of the testing data. The
perplexity can be roughly interpreted as the
geometric mean of the branching factor of the
document when presented to the language
model. Clearly, lower perplexities are better.
One of the key issues in language modelling
is the problem of data sparseness. To deal with
the problem, (Katz, 1987) proposed a backoff
scheme, which is widely used in bigram
language modelling. Backoff scheme estimates
the probability of an unseen bigram by utilizing
unigram estimates. It is of the form:
where c(wi-1,wi) is the frequency of word pair
(wi-1,wi) in training data, Pd represents the
Good-Turing discounted estimate for seen
word pairs, and Q(wi-1) is a normalization
factor.
Due to the memory limitation in realistic
applications, only a finite set of word pairs have
conditional probabilities P(wnjwn-1) explicitly
represented in the model, especially when the
model is trained on a large corpus. The
remaining word pairs are assigned a probability
by back-off (i.e. unigram estimates). The goal
of bigram pruning is to remove uncommon
explicit bigram estimates P(wnjwn-1) from the
model to reduce the number of parameters,
while minimizing the performance loss.
The most common way to eliminate unused
count is by means of count cutoffs (Jelinek,
1990). A cutoff is chosen, say 2, and all
probabilities stored in the model with 2 or
fewer counts are removed. This method
assumes that there is not much difference
between a bigram occurring once, twice, or not
at all. Just by excluding those bigrams with a
small count from a model, a significant saving
in memory can be achieved. In a typical
training corpus, roughly 65% of unique bigram
sequences occur only once.
Recently, several improvements over count
cutoffs have been proposed. (Seymore and
Rosenfeld, 1996) proposed a different pruning
scheme for backoff models, where bigrams are
ranked by a weighted difference of the log
probability estimate before and after pruning.
Bigrams with difference less than a threshold
are pruned.
(Stolcke, 1998) proposed a criterion for
pruning based on the relative entropy between
the original and the pruned model. The relative
entropy measure can be expressed as a relative
change in training data perplexity. All bigrams
that change perplexity by less than a threshold
are removed from the model. Stolcke also
concluded that, for practical purpose, the
method in (Seymore and Rosenfeld, 1996) is a
very good approximation to this method.
All previous cutoff methods described
above use a similar criterion for pruning, that is,
the difference (or information loss) between the
original estimate and the backoff estimate.
After ranking, all bigrams with difference small
enough will be pruned, since they contain no
more information.
</bodyText>
<sectionHeader confidence="0.99891" genericHeader="method">
3 Distribution-Based Cutoff
</sectionHeader>
<bodyText confidence="0.999990903225806">
As described in the previous section, previous
cutoff methods assume that training data covers
testing data. Bigrams that are infrequent in
training data are also assumed to be infrequent
in testing data, and will be cutoff. But in the
real world, no matter how large the training
data, it is still always very sparse compared to
all data in the world. Furthermore, training data
will be biased by its mixture of domain, time, or
style, etc. For example, if we use newspaper in
training, a name like “Lewinsky” may have
high frequency in certain years but not others; if
we use Gone with the Wind in training,
“Scarlett O’Hara” will have disproportionately
high probability and will not be cutoff.
We propose another approach to pruning.
We aim to keep bigrams that are more likely to
occur in a new document. We therefore propose
a new criterion for pruning parameters from
bigram models, based on the bigram
distribution i.e. the probability that a bigram
will occur in a new document. All bigrams with
the probability less than a threshold are
removed.
We estimate the probability that a bigram
occurs in a new document by dividing training
data into partitions, called subunits, and use a
cross-validation-like approach. In the
remaining part of this section, we firstly
investigate several methods for term
distribution modelling, and extend them to
</bodyText>
<equation confidence="0.963782666666667">
rPdk  |wia) c(wia,wi) °
P(w  |will) °I
0
i
i
C�w
a)P(w
) otherwise
(2)
</equation>
<bodyText confidence="0.999603666666667">
bigram distribution modelling. Then we
investigate the effects of the definition of the
subunit, and experiment with various ways to
divide a training set into subunits. Experiments
show that this not only allows a much more
efficient computation for bigram distribution
modelling, but also results in a more general
bigram model, in spite of the domain, style, or
temporal bias of training data.
</bodyText>
<subsectionHeader confidence="0.800095">
3.1 Measure of Generality
Probability
</subsectionHeader>
<bodyText confidence="0.999794961538462">
In this section, we will discuss in detail how to
estimate the probability that a bigram occurs in
a new document. For simplicity, we define a
document as the subunit of the training corpus.
In the next section, we will loosen this
constraint.
Term distribution models estimate the
probability Pi(k), the proportion of times that of
a word wi appears k times in a document. In
bigram distribution models, we wish to model
the probability that a word pair (wi-1 ,wi) occurs
in a new document. The probability can be
expressed as the measure of the generality of a
bigram. Thus, in what follows, it is denoted by
Pgen(wi-1,wi). The higher the Pgen(wi-1,wi) is, for
one particular document, the less informative
the bigram is, but for all documents, the more
general the bigram is.
We now consider several methods for term
distribution modelling, which are widely used
in Information Retrieval, and extend them to
bigram distribution modelling. These methods
include models based on the Poisson
distribution (Mood et al., 1974), inverse
document frequency (Salton and Michael,
1983), and Katz’s K mixture (Katz, 1996).
</bodyText>
<subsubsectionHeader confidence="0.985435">
3.1.1 The Poisson Distribution
</subsubsectionHeader>
<bodyText confidence="0.9997592">
The standard probabilistic model for the
distribution of a certain type of event over units
of a fixed size (such as periods of time or
volumes of liquid) is the Poisson distribution,
which is defined as follows:
</bodyText>
<equation confidence="0.983533833333333">
i
( ) ( ; )
k P k
= λ −λ
= e
i
</equation>
<bodyText confidence="0.98696">
In the most common model of the Poisson
distribution in IR, the parameter λi&gt;0 is the
average number of occurrences of wi per
</bodyText>
<equation confidence="0.954665">
cfi
document, that is λ i = , where cfi is the
N
</equation>
<bodyText confidence="0.999667555555555">
number of documents containing wi, and N is
the total number of documents in the collection.
In our case, the event we are interested in is the
occurrence of a particular word pair (wi-1,wi)
and the fixed unit is the document. We can use
the Poisson distribution to estimate an answer
to the question: what is the probability that a
word pair occurs in a document. Therefore, we
get
</bodyText>
<equation confidence="0.944509857142857">
−
Pgen wi wi P i e λ i
( − 1 , ) =1 − (0; ) =1 −
λ (4)
It turns out that using Poisson distribution, we
have Pgen(wi-1,wi) ∝ c(wi-1,wi). This means that
this criterion is equivalent to count cutoff.
</equation>
<subsubsectionHeader confidence="0.951902">
3.1.2 Inverse Document Frequency (IDF)
</subsubsectionHeader>
<bodyText confidence="0.99655675">
IDF is a widely used measure of specificity
(Salton and Michael, 1983). It is the reverse of
generality. Therefore we can also derive
generality from IDF. IDF is defined as follows:
</bodyText>
<equation confidence="0.973695">
IDFi = log( N) (5)
dfi
</equation>
<bodyText confidence="0.87678875">
where, in the case of bigram distribution, N is
the total number of documents, and dfi is the
number of documents that the contain word
N
pair (wi-1,wi). The formula log = gives full
dfi
weight to a word pair (wi-1,wi) that occurred in
one document. Therefore, let’s assume,
</bodyText>
<equation confidence="0.998014">
P w w (6)
gen i
( , ) ∝
− 1 i IDFi
</equation>
<bodyText confidence="0.999907571428571">
It turns out that based on IDF, our criterion is
equivalent to the count cutoff weighted by the
reverse of IDF. Unfortunately, experiments
show that using (6) directly does not get any
improvement. In fact, it is even worse than
count cutoff methods. Therefore, we use the
following form instead,
</bodyText>
<equation confidence="0.991068">
Pi
λ k
i
(3)
!
k
C w w
( )
i− 1 , i
C w w
( )
i− 1, i
P w w ∝ (7)
gen ( , )
i− 1 i
IDFα
i
K
Pgen(wim,wi) =1 −∑ Pi(k
=1
k
) (13)
</equation>
<bodyText confidence="0.9996405">
where α is a weighting factor tuned to
maximize the performance.
</bodyText>
<subsubsectionHeader confidence="0.477507">
3.1.3 KMixture
</subsubsectionHeader>
<bodyText confidence="0.977843210526316">
As stated in (Manning and Schütze, 1999), the
Poisson estimates are good for non-content
words, but not for content words. Several
improvements over Poisson have been
proposed. These include two-Poisson Model
(Harter, 1975) and Katz’s K mixture model
(Katz, 1996). The K mixture is the better. It is
also a simpler distribution that fits empirical
distributions of content words as well as
non-content words. Therefore, we try to use K
mixture for bigram distribution modelling.
According to (Katz, 1996), K mixture model
estimates the probability that word wi appears k
times in a document as follows:
α βPi (k)=(1 −α)δk,0 + β+1( β+1)k (8)
where δk,0=1 iff k=0 and δk,0=0 otherwise. α
and β are parameters that can be fit using the
observed mean λ and the observed inverse
document frequency IDF as follow:
</bodyText>
<equation confidence="0.9835854">
cf
λ = (9)
N
N
IDF = log (10)
df
β=λ×2IDF −1 =cfdff (11)
λ
α = (12)
β
</equation>
<bodyText confidence="0.99993745">
where again, cf is the total number of
occurrence of word wi in the collection, df is the
number of documents in the collection that wi
occurs in, and N is the total number of
documents.
The bigram distribution model is a variation
of the above K mixture model, where we
estimate the probability that a word pair
(wi-1,wi) , occurs in a document by:
where K is dependent on the size of the subunit,
the larger the subunit, the larger the value (in
our experiments, we set K from 1 to 3), and
Pi(k) is the probability of word pair (wi-1,wi)
occurs k times in a document. Pi(k) is estimated
by equation (8), where α , and βare estimated
by equations (9) to (12). Accordingly, cf is the
total number of occurrence of a word pair
(wi-1,wi) in the collection, df is the number of
documents that contain (wi-1,wi), and N is the
total number of documents.
</bodyText>
<subsubsectionHeader confidence="0.883228">
3.1.4 Comparison
</subsubsectionHeader>
<bodyText confidence="0.9288064">
Our experiments show that K mixture is the
best among the three in most cases. Some
partial experimental results are shown in table
1. Therefore, in section 4, all experimental
results are based on K mixture method.
</bodyText>
<table confidence="0.998387625">
Size of Bigram Word Perplexity
(Number of
Bigrams)
Poisson IDF K
Mixture
2000000 693.29 682.13 633.23
5000000 631.64 628.84 603.70
10000000 598.42 598.45 589.34
</table>
<tableCaption confidence="0.996351">
Table 1: Word perplexity comparison of
different bigram distribution models.
</tableCaption>
<subsectionHeader confidence="0.998128">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.999373666666667">
The bigram distribution model suggests a
simple thresholding algorithm for bigram
backoff model pruning:
</bodyText>
<listItem confidence="0.999525">
1. Select a threshold θ.
2. Compute the probability that each bigram
occurs in a document individually by
equation (13).
3. Remove all bigrams whose probability to
occur in a document is less than θ, and
recomputed backoff weights.
</listItem>
<sectionHeader confidence="0.998288" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99994864">
In this section, we report the experimental
results on bigram pruning based on distribution
versus count cutoff pruning method.
In conventional approaches, a document is
defined as the subunit of training data for term
distribution estimating. But for a very large
training corpus that consists of millions of
documents, the estimation for the bigram
distribution is very time-consuming. To cope
with this problem, we use a cluster of
documents as the subunit. As the number of
clusters can be controlled, we can define an
efficient computation method, and optimise the
clustering algorithm.
In what follows, we will report the
experimental results with document and cluster
being defined as the subunit, respectively. In
our experiments, documents are clustered in
three ways: by similar domain, style, or time.
In all experiments described below, we use an
open testing data consisting of 15 million
characters that have been proofread and
balanced among domain, style and time.
Training data are obtained from newspaper
(People’s Daily) and novels.
</bodyText>
<subsectionHeader confidence="0.994091">
4.1 Using Documents as Subunits
</subsectionHeader>
<figureCaption confidence="0.939420333333333">
Figure 1 shows the results when we define a
document as the subunit. We used
approximately 450 million characters of
People’s Daily training data (1996), which
consists of 39708 documents.
Figure 1: Word perplexity comparison of cutoff
</figureCaption>
<bodyText confidence="0.537022">
pruning and distribution based bigram pruning
using a document as the subunit.
</bodyText>
<subsectionHeader confidence="0.9947995">
4.2 Using Clusters by Domain as
Subunits
</subsectionHeader>
<bodyText confidence="0.971180304347826">
Figure 2 shows the results when we define a
domain cluster as the subunit. We also used
approximately 450 million characters of
People’s Daily training data (1996). To cluster
the documents, we used an SVM classifier
developed by Platt (Platt, 1998) to cluster
documents of similar domains together
automatically, and obtain a domain hierarchy
incrementally. We also added a constraint to
balance the size of each cluster, and finally we
obtained 105 clusters. It turns out that using
domain clusters as subunits performs almost as
well as the case of documents as subunits.
Furthermore, we found that by using the
pruning criterion based on bigram distribution,
a lot of domain-specific bigrams are pruned. It
then results in a relatively domain-independent
language model. Therefore, we call this
pruning method domain subtraction based
pruning.
Figure 2: Word perplexity comparison of cutoff
pruning and distribution based bigram pruning
using a domain cluster as the subunit.
</bodyText>
<subsectionHeader confidence="0.999528">
4.3 Using Clusters by Style as
Subunits
</subsectionHeader>
<bodyText confidence="0.999711583333333">
Figure 3 shows the results when we define a
style cluster as the subunit. For this experiment,
we used 220 novels written by different writers,
each approximately 500 kilonbytes in size, and
defined each novel as a style cluster. Just like in
domain clustering, we found that by using the
pruning criterion based on bigram distribution,
a lot of style-specific bigrams are pruned. It
then results in a relatively style-independent
language model. Therefore, we call this
pruning method style subtraction based
pruning.
</bodyText>
<figure confidence="0.997889970588235">
Size (Number of Bigrams)
Millions
10
9
8
7
6
5
4
3
2
0
1
550 600 650 700 750 800
Word Perplexity
Count Cutoff
Distribution Cutoff
Size (Number of Bigrams)
Millions
10
9
8
7
6
5
4
3
2
0
1
550 600 650 700 750 800
Word Perplexity
Count Cutoff
Distribution Cutoff
</figure>
<figureCaption confidence="0.891895">
Figure 3: Word perplexity comparison of cutoff
pruning and distribution based bigram pruning
using a style cluster as the subunit.
</figureCaption>
<subsectionHeader confidence="0.976778">
4.4 Using Clusters by Time as
Subunits
</subsectionHeader>
<bodyText confidence="0.999249954545455">
In practice, it is relatively easier to collect large
training text from newspaper. For example,
many Chinese SLMs are trained from
newspaper, which has high quality and
consistent in style. But the disadvantage is the
temporal term phenomenon. In other words,
some bigrams are used frequently during one
time period, and then never used again.
Figure 4 shows the results when we define a
temporal cluster as the subunit. In this
experiment, we used approximately 9,200
million characters of People’s Daily training
data (1978--1997). We simply clustered the
document published in the same month of the
same year as a cluster. Therefore, we obtained
240 clusters in total. Similarly, we found that
by using the pruning criterion based on bigram
distribution, a lot of time-specific bigrams are
pruned. It then results in a relatively
time-independent language model. Therefore,
we call this pruning method temporal
subtraction based pruning.
</bodyText>
<figureCaption confidence="0.593033333333333">
Figure 4: Word perplexity comparison of cutoff
pruning and distribution based bigram pruning
using a temporal cluster as the subunit.
</figureCaption>
<subsectionHeader confidence="0.987003">
4.3 Summary
</subsectionHeader>
<bodyText confidence="0.998663166666667">
In our research lab, we are particularly
interested in the problem of pinyin to Chinese
character conversion, which has a memory
limitation of 2MB for programs. At 2MB
memory, our method leads to 7-9% word
perplexity reduction, as displayed in table 2.
</bodyText>
<table confidence="0.998051555555556">
Subunit Word Perplexity
Reduction
Document 9.3%
Document Cluster by 7.8%
Domain
Document Cluster by 7.1%
Style
Document Cluster by 7.3%
Time
</table>
<tableCaption confidence="0.986419">
Table 2: Word perplexity reduction for bigram
</tableCaption>
<bodyText confidence="0.960244">
of size 2M.
As shown in figure 1-4, although as the size of
language model is decreased, the perplexity
rises sharply, the models created with the
bigram distribution based pruning have
consistently lower perplexity values than for
the count cutoff method. Furthermore, when
modelling bigram distribution on document
clusters, our pruning method results in a more
general n-gram backoff model, which resists to
domain, style or temporal bias of training data.
</bodyText>
<sectionHeader confidence="0.999662" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.973324">
In this paper, we proposed a novel approach for
n-gram backoff models pruning: keep n-grams
</bodyText>
<figure confidence="0.998861774193549">
Millions
Size (Number of Bigrams)
10
9
8
7
6
5
4
3
2
0
1
500 520 540 560 580 600 620 640 660 680 700
Word Perplexity
Count Cutoff
Distribution Cutoff
Millions
Size (Number of Bigrams)
0.6
0.4
0.2
1.4
1.2
.8
0
1
1200 1300 1400 1500 1600 1700 1800 1900 2000
Word Perplexity
Count Cutoff
Distribution Cutoff
</figure>
<bodyText confidence="0.9890381875">
that are more likely to occur in a new
document. We then developed a criterion for
pruning parameters from n-gram models, based
on the n-gram distribution i.e. the probability
that an n-gram occurs in a document. All
n-grams with the probability less than a
threshold are removed. Experimental results
show that the distribution-based pruning
method performed 7-9% (word perplexity
reduction) better than conventional cutoff
methods. Furthermore, when modelling n-gram
distribution on document clusters created
according to domain, style, or time, the pruning
method results in a more general n-gram
backoff model, in spite of the domain, style or
temporal bias of training data.
</bodyText>
<sectionHeader confidence="0.9976" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999457">
We would like to thank Mingjjing Li, Zheng
Chen, Ming Zhou, Chang-Ning Huang and
other colleagues from Microsoft Research,
Jian-Yun Nie from the University of Montreal,
Canada, Charles Ling from the University of
Western Ontario, Canada, and Lee-Feng Chien
from Academia Sinica, Taiwan, for their help
in developing the ideas and implementation in
this paper. We would also like to thank Jian
Zhu for her help in our experiments.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941282608696">
F. Jelinek, “Self-organized language modeling for
speech recognition”, in Readings in Speech
Recognition, A. Waibel and K.F. Lee, eds.,
Morgan-Kaufmann, San Mateo, CA, 1990, pp.
450-506.
D. Miller, T. Leek, R. M. Schwartz, “A hidden
Markov model information retrieval system”, in
Proc. 22nd International Conference on Research
and Development in Information Retrieval,
Berkeley, CA, 1999, pp. 214-221.
V.W. Zue, “Navigating the information
superhighway using spoken language interfaces”,
IEEE Expert, S. M. Katz, “Estimation of
probabilities from sparse data for the language
model component of a speech recognizer”, IEEE
Transactions on Acoustic, Speech and Signal
Processing, ASSP-35(3): 400-401, March, 1987.
K. Seymore, R. Rosenfeld, “Scalable backoff
language models”, in Porc. International
Conference on Speech and Language Processing,
Vol1. Philadelphia,PA,1996, pp.232-235
A. Stolcke, “Entropy-based Pruning of Backoff
Language Models” in Proc. DRAPA News
Transcriptionand Understanding Workshop,
Lansdowne, VA. 1998. pp.270-274
M. Mood, A. G. Franklin, and C. B. Duane,
“Introduction to the theory of statistics”, New
York: McGraw-Hill, 3rd edition, 1974.
G. Salton, and J. M. Michael, “Introduction to
Modern Information Retrieval”, New York:
McGraw-Hill, 1983.
S. M. Katz, “Distribution of content words and
phrases in text and language modeling”, Natural
Language Engineering, 1996(2): 15-59
C. D. Manning, and H. Schütze, “Foundations of
Statistical Natural Language Processing”, The
MIT Press, 1999.
S. Harter, “A probabilistic approach to automatic
keyword indexing: Part II. An algorithm for
probabilistic indexing”, Journal of the American
Society for Information Science, 1975(26):
280-289
J. Platt, “How to Implement SVMs”, IEEE
Intelligent System Magazine, Trends and
Controversies, Marti Hearst, ed., vol 13, no 4,
1998.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.860872">
<title confidence="0.993283">Distribution-Based Pruning of Backoff Language Models</title>
<author confidence="0.979466">Jianfeng Gao</author>
<affiliation confidence="0.999618">Microsoft Research China</affiliation>
<address confidence="0.9778645">No. 49 Zhichun Road Haidian District 100080, China,</address>
<email confidence="0.999931">jfgao@microsoft.com</email>
<author confidence="0.973993">Kai-Fu Lee</author>
<affiliation confidence="0.99949">Microsoft Research China</affiliation>
<address confidence="0.977337">No. 49 Zhichun Road Haidian District 100080, China,</address>
<email confidence="0.999955">kfl@microsoft.com</email>
<abstract confidence="0.999395769230769">We propose a distribution-based pruning of backoff language models. Instead of the conventional approach of pruning that are infrequent in training data, prune that are likely to be infrequent in a new document. Our method based on the distribution the that an occurs in a new document. Experimental results show that our method performed 7-9% (word perplexity reduction) better than conventional cutoff methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition”,</title>
<date>1990</date>
<booktitle>in Readings in Speech Recognition,</booktitle>
<pages>450--506</pages>
<editor>A. Waibel and K.F. Lee, eds., Morgan-Kaufmann,</editor>
<location>San Mateo, CA,</location>
<contexts>
<context position="892" citStr="Jelinek, 1990" startWordPosition="124" endWordPosition="125"> We propose a distribution-based pruning of n-gram backoff language models. Instead of the conventional approach of pruning n-grams that are infrequent in training data, we prune n-grams that are likely to be infrequent in a new document. Our method is based on the n-gram distribution i.e. the probability that an n-gram occurs in a new document. Experimental results show that our method performed 7-9% (word perplexity reduction) better than conventional cutoff methods. 1 Introduction Statistical language modelling (SLM) has been successfully applied to many domains such as speech recognition (Jelinek, 1990), information retrieval (Miller et al., 1999), and spoken language understanding (Zue, 1995). In particular, n-gram language model (LM) has been demonstrated to be highly effective for these domains. N-gram LM estimates the probability of a word given previous words, P(wn|w1,...,wn-1). In applying an SLM, it is usually the case that more training data will improve a language model. However, as training data size increases, LM size increases, which can lead to models that are too large for practical use. To deal with the problem, count cutoff (Jelinek, 1990) is widely used to prune language mod</context>
<context position="4509" citStr="Jelinek, 1990" startWordPosition="703" endWordPosition="704">Q(wi-1) is a normalization factor. Due to the memory limitation in realistic applications, only a finite set of word pairs have conditional probabilities P(wnjwn-1) explicitly represented in the model, especially when the model is trained on a large corpus. The remaining word pairs are assigned a probability by back-off (i.e. unigram estimates). The goal of bigram pruning is to remove uncommon explicit bigram estimates P(wnjwn-1) from the model to reduce the number of parameters, while minimizing the performance loss. The most common way to eliminate unused count is by means of count cutoffs (Jelinek, 1990). A cutoff is chosen, say 2, and all probabilities stored in the model with 2 or fewer counts are removed. This method assumes that there is not much difference between a bigram occurring once, twice, or not at all. Just by excluding those bigrams with a small count from a model, a significant saving in memory can be achieved. In a typical training corpus, roughly 65% of unique bigram sequences occur only once. Recently, several improvements over count cutoffs have been proposed. (Seymore and Rosenfeld, 1996) proposed a different pruning scheme for backoff models, where bigrams are ranked by a</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>F. Jelinek, “Self-organized language modeling for speech recognition”, in Readings in Speech Recognition, A. Waibel and K.F. Lee, eds., Morgan-Kaufmann, San Mateo, CA, 1990, pp. 450-506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Miller</author>
<author>T Leek</author>
<author>R M Schwartz</author>
</authors>
<title>A hidden Markov model information retrieval system”, in</title>
<date>1999</date>
<booktitle>Proc. 22nd International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>214--221</pages>
<location>Berkeley, CA,</location>
<contexts>
<context position="937" citStr="Miller et al., 1999" startWordPosition="128" endWordPosition="131">g of n-gram backoff language models. Instead of the conventional approach of pruning n-grams that are infrequent in training data, we prune n-grams that are likely to be infrequent in a new document. Our method is based on the n-gram distribution i.e. the probability that an n-gram occurs in a new document. Experimental results show that our method performed 7-9% (word perplexity reduction) better than conventional cutoff methods. 1 Introduction Statistical language modelling (SLM) has been successfully applied to many domains such as speech recognition (Jelinek, 1990), information retrieval (Miller et al., 1999), and spoken language understanding (Zue, 1995). In particular, n-gram language model (LM) has been demonstrated to be highly effective for these domains. N-gram LM estimates the probability of a word given previous words, P(wn|w1,...,wn-1). In applying an SLM, it is usually the case that more training data will improve a language model. However, as training data size increases, LM size increases, which can lead to models that are too large for practical use. To deal with the problem, count cutoff (Jelinek, 1990) is widely used to prune language models. The cutoff method deletes from the LM th</context>
</contexts>
<marker>Miller, Leek, Schwartz, 1999</marker>
<rawString>D. Miller, T. Leek, R. M. Schwartz, “A hidden Markov model information retrieval system”, in Proc. 22nd International Conference on Research and Development in Information Retrieval, Berkeley, CA, 1999, pp. 214-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V W Zue</author>
</authors>
<title>Navigating the information superhighway using spoken language interfaces”,</title>
<date>1987</date>
<journal>IEEE</journal>
<volume>35</volume>
<issue>3</issue>
<pages>400--401</pages>
<marker>Zue, 1987</marker>
<rawString>V.W. Zue, “Navigating the information superhighway using spoken language interfaces”, IEEE Expert, S. M. Katz, “Estimation of probabilities from sparse data for the language model component of a speech recognizer”, IEEE Transactions on Acoustic, Speech and Signal Processing, ASSP-35(3): 400-401, March, 1987.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Seymore</author>
<author>R Rosenfeld</author>
</authors>
<title>Scalable backoff language models”,</title>
<booktitle>in Porc. International Conference on Speech and Language Processing, Vol1. Philadelphia,PA,1996,</booktitle>
<pages>232--235</pages>
<marker>Seymore, Rosenfeld, </marker>
<rawString>K. Seymore, R. Rosenfeld, “Scalable backoff language models”, in Porc. International Conference on Speech and Language Processing, Vol1. Philadelphia,PA,1996, pp.232-235</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Entropy-based Pruning of Backoff Language Models” in</title>
<date>1998</date>
<booktitle>Proc. DRAPA News Transcriptionand Understanding Workshop,</booktitle>
<pages>270--274</pages>
<location>Lansdowne, VA.</location>
<contexts>
<context position="5261" citStr="Stolcke, 1998" startWordPosition="825" endWordPosition="826">is not much difference between a bigram occurring once, twice, or not at all. Just by excluding those bigrams with a small count from a model, a significant saving in memory can be achieved. In a typical training corpus, roughly 65% of unique bigram sequences occur only once. Recently, several improvements over count cutoffs have been proposed. (Seymore and Rosenfeld, 1996) proposed a different pruning scheme for backoff models, where bigrams are ranked by a weighted difference of the log probability estimate before and after pruning. Bigrams with difference less than a threshold are pruned. (Stolcke, 1998) proposed a criterion for pruning based on the relative entropy between the original and the pruned model. The relative entropy measure can be expressed as a relative change in training data perplexity. All bigrams that change perplexity by less than a threshold are removed from the model. Stolcke also concluded that, for practical purpose, the method in (Seymore and Rosenfeld, 1996) is a very good approximation to this method. All previous cutoff methods described above use a similar criterion for pruning, that is, the difference (or information loss) between the original estimate and the bac</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>A. Stolcke, “Entropy-based Pruning of Backoff Language Models” in Proc. DRAPA News Transcriptionand Understanding Workshop, Lansdowne, VA. 1998. pp.270-274</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mood</author>
<author>A G Franklin</author>
<author>C B Duane</author>
</authors>
<title>Introduction to the theory of statistics”,</title>
<date>1974</date>
<location>New York:</location>
<note>McGraw-Hill, 3rd edition,</note>
<contexts>
<context position="8899" citStr="Mood et al., 1974" startWordPosition="1417" endWordPosition="1420">e wish to model the probability that a word pair (wi-1 ,wi) occurs in a new document. The probability can be expressed as the measure of the generality of a bigram. Thus, in what follows, it is denoted by Pgen(wi-1,wi). The higher the Pgen(wi-1,wi) is, for one particular document, the less informative the bigram is, but for all documents, the more general the bigram is. We now consider several methods for term distribution modelling, which are widely used in Information Retrieval, and extend them to bigram distribution modelling. These methods include models based on the Poisson distribution (Mood et al., 1974), inverse document frequency (Salton and Michael, 1983), and Katz’s K mixture (Katz, 1996). 3.1.1 The Poisson Distribution The standard probabilistic model for the distribution of a certain type of event over units of a fixed size (such as periods of time or volumes of liquid) is the Poisson distribution, which is defined as follows: i ( ) ( ; ) k P k = λ −λ = e i In the most common model of the Poisson distribution in IR, the parameter λi&gt;0 is the average number of occurrences of wi per cfi document, that is λ i = , where cfi is the N number of documents containing wi, and N is the total numb</context>
</contexts>
<marker>Mood, Franklin, Duane, 1974</marker>
<rawString>M. Mood, A. G. Franklin, and C. B. Duane, “Introduction to the theory of statistics”, New York: McGraw-Hill, 3rd edition, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>J M Michael</author>
</authors>
<title>Introduction to Modern Information Retrieval”,</title>
<date>1983</date>
<location>New York: McGraw-Hill,</location>
<contexts>
<context position="8954" citStr="Salton and Michael, 1983" startWordPosition="1424" endWordPosition="1427"> (wi-1 ,wi) occurs in a new document. The probability can be expressed as the measure of the generality of a bigram. Thus, in what follows, it is denoted by Pgen(wi-1,wi). The higher the Pgen(wi-1,wi) is, for one particular document, the less informative the bigram is, but for all documents, the more general the bigram is. We now consider several methods for term distribution modelling, which are widely used in Information Retrieval, and extend them to bigram distribution modelling. These methods include models based on the Poisson distribution (Mood et al., 1974), inverse document frequency (Salton and Michael, 1983), and Katz’s K mixture (Katz, 1996). 3.1.1 The Poisson Distribution The standard probabilistic model for the distribution of a certain type of event over units of a fixed size (such as periods of time or volumes of liquid) is the Poisson distribution, which is defined as follows: i ( ) ( ; ) k P k = λ −λ = e i In the most common model of the Poisson distribution in IR, the parameter λi&gt;0 is the average number of occurrences of wi per cfi document, that is λ i = , where cfi is the N number of documents containing wi, and N is the total number of documents in the collection. In our case, the eve</context>
</contexts>
<marker>Salton, Michael, 1983</marker>
<rawString>G. Salton, and J. M. Michael, “Introduction to Modern Information Retrieval”, New York: McGraw-Hill, 1983.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S M Katz</author>
</authors>
<title>Distribution of content words and phrases in text and language modeling”,</title>
<journal>Natural Language Engineering,</journal>
<volume>1996</volume>
<issue>2</issue>
<pages>15--59</pages>
<marker>Katz, </marker>
<rawString>S. M. Katz, “Distribution of content words and phrases in text and language modeling”, Natural Language Engineering, 1996(2): 15-59</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<date>1999</date>
<journal>Foundations of Statistical Natural Language Processing”, The</journal>
<publisher>MIT Press,</publisher>
<contexts>
<context position="11122" citStr="Manning and Schütze, 1999" startWordPosition="1857" endWordPosition="1860">red in one document. Therefore, let’s assume, P w w (6) gen i ( , ) ∝ − 1 i IDFi It turns out that based on IDF, our criterion is equivalent to the count cutoff weighted by the reverse of IDF. Unfortunately, experiments show that using (6) directly does not get any improvement. In fact, it is even worse than count cutoff methods. Therefore, we use the following form instead, Pi λ k i (3) ! k C w w ( ) i− 1 , i C w w ( ) i− 1, i P w w ∝ (7) gen ( , ) i− 1 i IDFα i K Pgen(wim,wi) =1 −∑ Pi(k =1 k ) (13) where α is a weighting factor tuned to maximize the performance. 3.1.3 KMixture As stated in (Manning and Schütze, 1999), the Poisson estimates are good for non-content words, but not for content words. Several improvements over Poisson have been proposed. These include two-Poisson Model (Harter, 1975) and Katz’s K mixture model (Katz, 1996). The K mixture is the better. It is also a simpler distribution that fits empirical distributions of content words as well as non-content words. Therefore, we try to use K mixture for bigram distribution modelling. According to (Katz, 1996), K mixture model estimates the probability that word wi appears k times in a document as follows: α βPi (k)=(1 −α)δk,0 + β+1( β+1)k (8)</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>C. D. Manning, and H. Schütze, “Foundations of Statistical Natural Language Processing”, The MIT Press, 1999.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Harter</author>
</authors>
<title>A probabilistic approach to automatic keyword indexing: Part II. An algorithm for probabilistic indexing”,</title>
<journal>Journal of the American Society for Information Science,</journal>
<volume>1975</volume>
<issue>26</issue>
<pages>280--289</pages>
<marker>Harter, </marker>
<rawString>S. Harter, “A probabilistic approach to automatic keyword indexing: Part II. An algorithm for probabilistic indexing”, Journal of the American Society for Information Science, 1975(26): 280-289</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>How to Implement SVMs”,</title>
<date>1998</date>
<journal>IEEE Intelligent System Magazine, Trends</journal>
<volume>13</volume>
<editor>and Controversies, Marti Hearst, ed.,</editor>
<contexts>
<context position="15334" citStr="Platt, 1998" startWordPosition="2559" endWordPosition="2560">bunits Figure 1 shows the results when we define a document as the subunit. We used approximately 450 million characters of People’s Daily training data (1996), which consists of 39708 documents. Figure 1: Word perplexity comparison of cutoff pruning and distribution based bigram pruning using a document as the subunit. 4.2 Using Clusters by Domain as Subunits Figure 2 shows the results when we define a domain cluster as the subunit. We also used approximately 450 million characters of People’s Daily training data (1996). To cluster the documents, we used an SVM classifier developed by Platt (Platt, 1998) to cluster documents of similar domains together automatically, and obtain a domain hierarchy incrementally. We also added a constraint to balance the size of each cluster, and finally we obtained 105 clusters. It turns out that using domain clusters as subunits performs almost as well as the case of documents as subunits. Furthermore, we found that by using the pruning criterion based on bigram distribution, a lot of domain-specific bigrams are pruned. It then results in a relatively domain-independent language model. Therefore, we call this pruning method domain subtraction based pruning. F</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>J. Platt, “How to Implement SVMs”, IEEE Intelligent System Magazine, Trends and Controversies, Marti Hearst, ed., vol 13, no 4, 1998.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>