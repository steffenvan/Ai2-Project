<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9420875">
A FORMAL LEXICON IN THE MEANING-TEXT THEORY
(OR How TO Do LEXICA WITH WORDS)
</note>
<author confidence="0.245996">
Igor A. Mel&apos;&apos;euk
</author>
<table confidence="0.787847875">
Departement de linguistique
Universite de Montréal
C.P. 6128, Succ. &amp;quot;A&amp;quot;
Montréal, P.Q. H3C 3J7 Canada
Alain Polguere
Odyssey Research Associates
Place Outremont, 1290 Van Horne
Montréal, P.Q. H2V 1K6 Canada
</table>
<bodyText confidence="0.996856928571429">
The goal of this paper is to present a particular type of lexicon, elaborated within a formal theory of
natural language called Meaning-Text Theory (MTT). This theory puts strong emphasis on the
development of highly structured lexica. Computational linguistics does of course recognize the
importance of the lexicon in language processing. However, MTT probably goes further in this direction
than various well-known approaches within computational linguistics; it assigns to the lexicon a central
place, so that the rest of linguistic description is supposed to pivot around the lexicon. It is in this spirit
that MTT views the model of natural language: the Meaning-Text Model, or MTM. It is believed that
a very rich lexicon presenting individual information about lexemes in a consistent and detailed way
facilitates the general task of computational linguistics by dividing it into two more or less autonomous
subtasks: a linguistic and a computational one. The MTM lexicon, embodying a vast amount of linguistic
information, can be used in different computational applications.
We will present here a short outline of the lexicon in question as well as of its interaction with other
components of the MTM, with special attention to computational implications of the Meaning-Text
Theory.
</bodyText>
<sectionHeader confidence="0.996837333333333" genericHeader="method">
1. LEVELS OF UTTERANCE REPRESENTATION IN
MEANING-TEXT THEORY AND THE MEANING-TEXT
MODEL OF NATURAL LANGUAGE.
</sectionHeader>
<bodyText confidence="0.979817">
The goal of the present paper is two-fold:
</bodyText>
<listItem confidence="0.7371812">
1) To present a specific viewpoint on the role of
lexica in &amp;quot;intelligent&amp;quot; systems designed to process texts
in natural language and based on access to meaning.
2) To present a specific format for such a lexicon —
so-called Explanatory Combinatorial Dictionary (ECD).
</listItem>
<bodyText confidence="0.99970615">
We believe that a rich enough lexicon, which could
enable us to solve the major problem of computational
linguistics — that of presenting all necessary informa-
tion about natural language in compact form, should be
anchored in a formal and comprehensive theory of
language. The lexicon to be discussed, that is ECD, has
been conceived and developed within the framework of
a particular linguistic theory — more specifically, Mean-
ing-Text Theory or MTT (Mel&apos;a&amp;quot;uk 1974, 1981, 1988:43-
101). Note that this is by no means a theory of how
linguistic knowledge could or should be applied in the
context of any computational task. The MTT is a theory
of how to describe and formally present linguistic
knowledge, a theory of linguistic description; therefore,
its contribution to computational linguistics is only a
partial one: to take care exclusively of the linguistic part
of the general endeavor.
We cannot present here the Meaning-Text Theory in
detail, so we will limit ourselves to a brief characteri-
zation of the following two aspects, which are of
</bodyText>
<footnote confidence="0.84631375">
Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 87 /030261-275$03.00
</footnote>
<note confidence="0.6430565">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 261
Igor Mertuk and Main Polguere A Formal Lexicon in Meaning-Text Theory
</note>
<bodyText confidence="0.994265">
particular relevance to this paper: the system of linguis-
tic representations the theory makes use of, and the
linguistic Meaning-Text model which it presupposes.
</bodyText>
<subsectionHeader confidence="0.865126">
1.1. UTTERANCE REPRESENTATIONS IN THE MTT.
</subsectionHeader>
<bodyText confidence="0.995712083333333">
We have to warn our reader that limitations of space
force us to have recourse to drastic simplifications,
perhaps even too drastic sometimes. Thus, an MTT
utterance representation is in fact a set of formal objects
called structures, their vocation being the characteriza-
tion of separate aspects of the phenomena to be de-
scribed. But in this paper we use the term representa-
tion to refer to the main one of the structures which
compose a representation (since we disregard the other
structures).
In Meaning-Text Theory, an utterance&apos; is repre-
sented at seven levels:
</bodyText>
<listItem confidence="0.721336083333333">
1) The Sem(antic) R(epresentation) of utterance U is,
roughly speaking, a network which depicts the linguistic
meaning of U without taking into consideration the way
this meaning is expressed in U (distribution of meaning
between words and constructions, and the like). Thus a
SemR represents in fact the meaning of the whole family
of utterances synonymous with each other.2 The nodes
of a SemR network are labeled with semantic units, a
semantic unit being a specific sense of a lexeme in the
language in question. The arcs of the network are
labeled with distinctive numbers which identify dif-
ferent arguments of a predicate. Thus,
</listItem>
<bodyText confidence="0.890782529411765">
a 1 P 2
o o o
is equivalent to the more familiar notation P( a , b). As
the reader can see, our SemR is based on predicate-
argument relations (although we do not use the linear
notation of predicate calculus nor predicate calculus as
such).
2) The D(eep-)Synt(actic) R(epresentation) of U is,
roughly speaking, a dependency tree whose nodes are
not linearly ordered (because linear order is taken to be
a means of expressing syntactic structure rather than
being part of it). The nodes of a DSyntR are labeled with
meaningful lexemes of U, which are supplied with
The vague term utterance is used on purpose. In the present paper
we take the sentence as our basic unit, but the MTT is not restricted
to sentences. In principle, it can deal with sequences of sentences,
although up to now, within the MTT, the way to represent such
sequences and the rules to process them have not yet been developed
(in contrast with such works as, e.g., McKeown (1985)).
2 The term meaning is to be construed here in the narrowest sense—
as referring to strictly linguistic meaning, i.e. the meaning (of utter-
ances) which is given to any native speaker just by the mastery of his
language. We say this to avoid a misunderstanding: our meaning has
nothing to do with &amp;quot;actual&amp;quot; meaning, which is aimed at by such
questions as &apos;What do you mean by that?&apos; or &apos;What is the meaning of
this paper?&apos; This restricted character of meaning in our interpretation
will become clear when we discuss semantic representation in the
MIT (see below).
meaning-bearing morphological values (such as number
in nouns or tense in verbs; in our example below, we
indicate such values for the top node only). The
branches of a DSyntR carry the names of universal
DSynt-relations, which are few in number (less than
ten).
</bodyText>
<listItem confidence="0.932668428571429">
3) The S(urface-)Synt(actic) R(epresentation) of U is
also a dependency tree of the same formal type but, its
nodes are labeled with all actual lexemic occurrences of
U (including all structural words), and branches of it
carry the names of a few dozen specific SSynt-relations,
which correspond to the actual syntactic constructions
of a particular language.
</listItem>
<bodyText confidence="0.99957225">
The distinction between Deep- and Surface- suble-
vels is related to the fact that some syntactic phenom-
ena (basically, cooccurrence restrictions) are more
linked to meaning, while other syntactic phenomena
(word order, agreement, and the like) are more relevant
from the viewpoint of actual text. Phenomena of the
first type are captured in the DSyntR, which is geared to
meaning, and those of the second type, in the SSyntR,
geared to text.
In the same vein and with the same purpose, MTT
introduces two sublevels — deep vs. surface — in
morphology and phonology:
</bodyText>
<listItem confidence="0.9993515">
4) D(eep-)Morph(ological) R(epresentation).
5) S(urface-)Morph(ological) R(epresentation).
6) D(eep-)Phon(etic) R(epresentation),
or phonological representation.
7) S(urface-)Phon(etic) R(epresentation),
or phonetic representation proper.
</listItem>
<bodyText confidence="0.997926">
We will not justify the Deep- vs. Surface- dichotomy
or, more generally, the composition and organization of
our set of representation levels. Instead of this, we will
try to link our somewhat abstract statements to a
specific example. Namely, we will quote a French
sentence along with its representations on the first three
levels. We will not consider here the morphological and
phonological representations of this sentence, since
they are not relevant to our goal. Note that throughout
this paper we will use examples borrowed from French
(since we had the corresponding information available
only in that language, while to work out the English
examples would require special research, which we are
in no position to undertake); but to facilitate the reading
of all the examples, we will supply approximate English
glosses for all French lexical items.
Let us consider French sentence (1):
</bodyText>
<listItem confidence="0.899724">
(1) Fr. Alcide (X) a aide Mordecai&apos; (Y) a passer son
bac (Z) par ses conseils avertis (W)
</listItem>
<bodyText confidence="0.760758666666667">
`Alcide helped Mordecai pass his [high school leav-
ing] exams with his judicious advice&apos;.
At the Sem-level, sentence (1) appears as Figure 1:
</bodyText>
<page confidence="0.925679">
262 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<figure confidence="0.594847">
SemR of Sentence (1)
</figure>
<figureCaption confidence="0.728814">
Figure 1
</figureCaption>
<figure confidence="0.975919380952381">
1
3
Igor Mel&apos;Cuk and Main Polguere
&apos;aider 2&apos;
[ help 2 1
o
\
A Formal Lexicon in Meaning-Text Theory
&apos;avant&apos;
o [ before ]
2-------21 ° &apos;maintenant&apos;
[ now ]
1
o
&apos;Alcide&apos;
&apos;Mordecai&apos; 1 &apos;passer le bac&apos;
[ pass one&apos;s exams ]
2 0 1 o -2
&apos;conseils avertis&apos; &apos;concerner&apos;
[ judicious advice ] [ concern ]
1
</figure>
<bodyText confidence="0.999887068965517">
In this figure, SI is a dummy to indicate an unspecified
meaning (it is not specified what exactly the advice from
Alcide is). In principle, every lexemic sense must be
identified by a number; for the sake of simplicity we do
this here for one node only: AIDER 2, which will be
discussed in 2.1.3. To avoid unnecessary complications,
we have grouped certain semantic elements together
(&apos;passer le bac&apos; and `conseils avertis&apos;). The same short-
cut is used in the next two figures.
At the DSynt-level, sentence (1) appears as follows:
The subscript &amp;quot;present perfect&amp;quot; on AIDER 2 corre-
sponds, in the SemR, to the subnetwork &apos;before now&apos;:
this is roughly the meaning of the French present
perfect (called &amp;quot;passé compose&amp;quot;). During the transition
to the SSynt-level, this subscript triggers a DSynt-rule
that introduces the appropriate auxiliary verb (in our
case, AVOIR &apos;have&apos;) along with the auxiliary SSynt-
relation, linking it to the lexical verb (i.e., AIDER 2) in
the participial form: see Fig. 3. Note that the first
DSynt-dependent of the verb AIDER 2 in the DSyntR
(ALCIDE) must be linked as the grammatical subject to
the AVOIR node in the SSyntR, while all other depen-
dents of AIDER 2 remain dependents of its participial
form.
At the SSynt-level, we get Figure 3:
Now we will move to the characterization of the
formal device, a set of rules, which ensures the transi-
tion between the representations of the above levels:
the Meaning-Text Model (MTM).
</bodyText>
<subsectionHeader confidence="0.982051">
1.2. THE MEANING-TEXT MODEL
</subsectionHeader>
<bodyText confidence="0.99980525">
The MTM is nothing else but what is currently called
grammar (we avoid this usage because we would like to
distinguish and even contrast grammar vs. lexicon, both
being parts of a linguistic model). More specifically, it is
</bodyText>
<table confidence="0.888223666666667">
AIDER 2 present perfect
[ help 2 1
o
o I III IV
ALCI DE o o
MORDECAI SES CONSEILS AYERTIS
[ his judicious advice ]
o
PASSER SON BAC
</table>
<tableCaption confidence="0.196619">
[ pass his exams ]
DSy ntR of Sentence (1)
</tableCaption>
<figureCaption confidence="0.920445">
Figure 2
</figureCaption>
<figure confidence="0.990272689655172">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 263
Igor Mereuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory
AVOIR present
[ have ]
ALCIDE
MORDECAI
SSyntR of Sentence (1)
Figure 3
predicative
auxiliary
direct
objective
o
prepositional
o/
[ his judicious advice ]
PASSER SON BAC
[ pass his exams ]
indirect I
objective
1 ,
o A
1 prepositional
o
SES CONSEILS AVERT IS
AIDER 2
[ help 2 ]
oblique
o PAR
</figure>
<bodyText confidence="0.959805888888889">
a set of formal rules, with complex internal organiza-
tion, which, so to speak, translate the initial SemR into
a final SPhonR (or into a written text) and vice versa. Of
course, in doing so, the rules pass through all interme-
diate representations. Therefore the MTM is subdivided
into components such that each one deals with the
correspondence between two adjacent levels n and n +
1; given seven levels of representation, there are six
components:
</bodyText>
<listItem confidence="0.999539916666667">
1) The Semantic Component (transition between SemR
and DSyntR);
2) The Deep-Syntactic Component (transition between
DSyntR and SSyntR);
3) The Surface-Syntactic Component (transition be-
tween SSyntR and DMorphR);
4) The Deep-Morphological Component (transition be-
tween DMorphR and SMorphR);
5) The Surface-Morphological Component (transition
between SMorphR and DPhonR);
6) The Deep-Phonetic Component (transition between
DPhonR and SPhonR).
</listItem>
<bodyText confidence="0.975927441176471">
Each component has a roughly identical internal struc-
ture; namely, it contains three types of rules:
— well-formedness rules for representations of the
source level;
— well-formedness rules for representations of the
target level;
— transition rules proper.
The well-formedness rules serve simultaneously both to
check the correctness of the representation in question
and to contol the application of the transition rules. Let
it be emphasized that until now the MTM (and the MTT
in general) does not contain the data necessary to
effectively carry out the said control; thus the develop-
ment and organization of these data constitutes, from a
computational viewpoint, the main problem in the ap-
plication of the MTT.
To sum up, the synthesis of a sentence appears in the
Meaning-Text framework as a series of subsequent
transitions, or translations, from one representation to
the next one, beginning with SemR; the analysis takes
of course the opposite direction, starting with the
SPhonR or with the written text. This is, however, only
a logical description of what happens. In a real compu-
tational implementation, it is often necessary, in order
to take a decision concerning a particular level of
representation, to consider several other levels simulta-
neously.
As the reader has probably realized, the MTM be-
longs to so-called stratificational models of language,
launched about a quarter of a century ago (Lamb 1966,
Sgall 1967, Mel&apos;euk 1974); note that at present we can
see a clear tendency to introduce certain ideas of the
stratificational approach into theoretical and computa-
tional linguistics (compare, e.g., the distinction of sev-
</bodyText>
<page confidence="0.951123">
264 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.683057">
Igor Mereuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory
</note>
<bodyText confidence="0.993117571428571">
eral levels of linguistic representation in Bresnan&apos;s
Lexical Functional Grammar).
Since our goal in the paper is to discuss the role of a
formal lexicon in a linguistic model, and, as we will try
to show, our formal lexicon is a part of the semantic
component (of the MTM), we will concentrate only on
this portion of the model.
</bodyText>
<sectionHeader confidence="0.979436" genericHeader="method">
2. EXPLANATORY COMBINATORIAL DICTIONARY.
</sectionHeader>
<bodyText confidence="0.999955666666667">
A lexicographic unit in the ECD, i.e. a dictionary
entry, covers one lexical item — a word or a set phrase
— taken in one well-specified sense. All such items
called lexemes (respectively, phrasemes) are described
in a rigorous and uniform way, so that a dictionary entry
is divided into three major zones: the semantic zone, the
syntactic zone, and the lexical cooccurrence zone. (We
leave out of consideration all other zones, as irrelevant
to the main purpose of this paper.)
</bodyText>
<sectionHeader confidence="0.373182" genericHeader="method">
2.1. SEMANTIC ZONE.
2.1.1. THE LEXICOGRAPHIC DEFINITION AS THE BASIS OF
AN ECD ENTRY.
</sectionHeader>
<bodyText confidence="0.987556166666667">
An ECD entry is centered around the definition of the
head word, i.e. the representation of its meaning, or its
SemR. All particularities of the ECD, as far as the
semantic domain is concerned, follow from the fact that
meaning is taken to be the first and foremost motivation
for everything else in the dictionary: relevant properties
of lexical items are discovered and described contingent
upon their definitions. In order to make this important
property more obvious, let us compare the MTT ap-
proach to the lexicon with certain current approaches
known in computational linguistics. The approaches we
mean here are rather syntax-motivated, so that they
view the lexicon as an appendix to the grammar, the
latter being equated with the formulation of syntactic
well-formedness. In MTT, it is exactly the opposite: the
grammar is considered to be an appendix to the lexicon,
an appendix which, on the one hand, expresses useful
generalizations over the lexicon, and on the other hand,
embodies all procedures necessary for manipulating
lexical data. As we have said, the lexicon and the
grammar taken together constitute the MT model, with
the lexicon at its foundations.
ECD definitions possess, among other things, the
following two properties, which distinguish them from
3 Let it be emphasized that the abstract concept of an ECD as
presented below is by no means the same thing as an actual ECD
implemented in a specific form, such as, e.g., Mel&apos;6uk etal. (1984) and
Mel&apos;euk and 2ho1kovsky (1984). The particular concept of ECD
which we propose here is no more than a logical constraint on an
infinity of possible ways to build concrete ECDs. On the one hand, we
insist on the concept of ECD only, without touching upon the methods
of its realization; on the other hand, the actual ECDs of Russian and
French, mentioned above, are not well adapted, under their present
form, to computational treatment.
the definitions of many lexica used in computational
linguistics:
</bodyText>
<listItem confidence="0.831420111111111">
a) An ECD definition must be adequate in the sense
that all possible correct usages of the lexeme defined
are covered by it and all incorrect usages are excluded.
In other terms, all the components of an ECD definition
are necessary and the set of these is sufficient for the
task just stated.
b) In various computational or formal approaches,
lexicographic definitions are written in terms of a small
set of prefabricated elements. It looks as if the resear-
</listItem>
<bodyText confidence="0.974334585365853">
cher&apos;s goal were to make his definitional language as
different as possible from his object language (cf. con-
ceptual dependencies in Schank (1972), logical repre-
sentations in Dowty (1979), and the like). One major
drawback of this type of approach is that it does not
provide for a direct and explicit expression of lexical
cohesion in the language under analysis: possible rela-
tionships among lexical items have to be inferred from
their definitions in a very indirect way. In sharp contrast
to this, the ECD defines a lexeme L of language L in
terms of other lexemes L 1 , L2, . . Li, of L in such a
way that the meaning of an L, is simpler than that of L,
which precludes circularity (for further discussion, see
2.1.2.a). As a result, in the ECD, semantic relationships
among lexemes are established and explicitly stated
directly via their definitions.
2.1.2. FORMAL CHARACTER OF AN ECD DEFINITION.
An ECD definition is a decomposition of the meaning of
the corresponding lexeme. It is a semantic network
whose nodes are labeled either with semantic units
(actually, lexemes) of L, or with variables, and whose
arcs are labeled with distinctive numbers which identify
different arguments of a predicate. A lexical label rep-
resents the definition (the meaning) of the correspond-
ing lexeme, rather than the lexeme itself. Therefore,
each node of a definitional network stands, in its turn,
for another network, whose nodes are replaceable by
their corresponding networks, and so forth, until the
bottom level primitives are reached. For practical rea-
sons, though, one can take as primitives the lexemes of
the level at which the researcher decides to stop the
process of decomposing. This approach is directly re-
lated to the pioneering work of Wierzbicka: see, e.g.,
Wierzbicka (1980).
The elaboration of ECD definitions must satisfy a
number of principles, of which we will mention here the
following two:
a) The Decomposition Principle requires that a lex-
eme be defined in terms of lexemes which are semanti-
cally simpler than it; as mentioned above, this precludes
circularity. This principle, if applied consistently, will
</bodyText>
<footnote confidence="0.65154">
4 By possible we mean &apos;possible in any imaginable context&apos; — with
the obvious exception of contexts involving either the phonetic form
(as, e.g., in poetry) or metalinguistic use of lexical items.
</footnote>
<figure confidence="0.923968">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 265
Igor Mel&apos;kuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory
&apos;employ er &apos;
[ use I
&apos;secourir 1•
[ succour ]
&apos;secourir 2&apos;
12
2o
(resources ]
X
</figure>
<subsectionHeader confidence="0.750862">
Definition of the French verb SECOURIR 2
</subsectionHeader>
<bodyText confidence="0.953838714285714">
What we mean by a (lexicographic) definition of a lexical
meaning is an equivalence between this lexical meaning
itself taken together with its Sem-actants (the righthand
part of the figure) and its semantic decomposition observing
the Maximal Block Principle (the lefthand part).
In English, the network in the lefthand part can be read as:
&apos;X uses Z which is X&apos;s resources in order to SECOURIR 1 Y&apos;
</bodyText>
<figureCaption confidence="0.662217">
Figure 4
</figureCaption>
<bodyText confidence="0.999934705882353">
lead to a set of semantic primitives. As is easily seen,
we do not begin by postulating a set of semantic
primitives: we hope to have them discovered by a long
and painstaking process of semantic decomposition
applied to thousands of actual lexical items. But let it be
underscored that within the MTT framework it is not
vital to have semantic primitives at hand in order to be
able to proceed successfully with linguistic description.
b) The Maximal Block Principle requires that, within
the definition of lexeme L, any subnetwork which is the
definition of L&apos; be replaced by L&apos;; this ensures the
graduality of decomposition, which contributes to mak-
ing explicit interlexical links in the language. In fact this
principle forbids writing a definition in terms of seman-
tic primitives if semantic units of a higher level are
available; this makes our definitions immediately grasp-
able and more workable.
</bodyText>
<sectionHeader confidence="0.6943895" genericHeader="method">
2.1.3. AN EXAMPLE OF THE INTERACTION AMONG ECD
DEFINITIONS.
</sectionHeader>
<bodyText confidence="0.9999104">
To illustrate the way the vocabulary of L is semantically
hierarchized using ECD definitions, we will consider
some definitions involving semantically related lexemes.
Let us take the French verb SECOURIR, roughly
&apos;succour&apos;. (Although this English gloss is not a very
familiar word in Modern English, we cannot find a
better equivalent, since there is no single English lexical
item that covers exactly the meaning of this French
verb. However we believe that this is a good example
because it shows how different the lexicalizations of the
same meaning in two different languages can be. We
hope that our semantic description of SECOURIR will
eventually make its meaning clear to the reader.) SE-
COURIR corresponds to two lexemes the meaning of
which can be illustrated by the following examples:
</bodyText>
<listItem confidence="0.521196">
(2) SECOURIR 1
Ce vaccin a secouru de nombreux enfants
&apos;This vaccine helped/saved many children&apos;.
</listItem>
<bodyText confidence="0.912049466666667">
SECOURIR 2
Le medecin a secouru de nombreux enfants avec ce
vaccin
&apos;The doctor helped/saved many children with this
vaccine&apos;.
There exists an obvious semantic relation between the
two lexemes: a causative one; thus SECOURIR 2
means &apos;to use something for it to SECOURIR 1 some-
one&apos;, that is, very roughly, &apos;to cause something to
SECOURIR 1 someone&apos; (the concept of causation is
implicit in `use&apos;). In a more formal way, we can repre-
sent the meaning of SECOURIR 2 by a semantic
network which includes SECOURIR 1 (Figure 4).
SECOURIR 1, in its turn, can be defined in terms of
AIDER 1, roughly &apos;help&apos; (as in La lumiere aide les
</bodyText>
<figure confidence="0.9659079">
o I
&apos;ressources&apos;
266 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
Igor MePeuk and Main Polguere A Formal Lexicon in Meaning-Text Theory
&apos;aider 1 &apos;
(help 1 ]
o
0 4----- o
Y &apos;survivre&apos;
[ survive ]
&apos;necessaire&apos;
1 72 (necessary ]
&lt; &gt;
&apos;secourir 1 &apos;
o
/ N2N
Definition of the French verb SECOURIR 1
In English, the network can be read as:
&apos; X helps 1 Y to survive, this help1 being
necessary for Y&apos;s survival&apos;
</figure>
<figureCaption confidence="0.960119">
Figure 5
</figureCaption>
<bodyText confidence="0.965392266666667">
plantes dans leur croissance, lit. &apos;Light helps plants in
their growth&apos;), plus the following two important seman-
tic components: &apos;survive&apos; and &apos;necessary&apos;. More specif-
ically, X SECOURT 1 limeans &apos;X AIDE 1 [=helps 1]Y
to survive, X&apos;s helping 1 Y being necessary for Y&apos;s
survival&apos;. The node `secourir 1&apos; in the network of Fig. 4
can be replaced by its own definition, i.e. by the
network of Fig. 5, giving Fig. 6. It is obvious that,
generally speaking, the inverse substitution is also pos-
sible: a network representing a definition of a lexical
meaning can be replaced by a single node representing
this meaning. This type of network manipulation must
be carried out automatically by substitution procedures
based on lexico-semantic rules of the form:
SemR, &lt;=&gt; MEANING OF LEXEME Li
</bodyText>
<figure confidence="0.396696461538461">
&apos;employer&apos;
(use ]
o 1
&apos;ressources&apos;
[ resources ]
&apos;aider 1&apos; 10
[ help 1 1./....
cl&apos; 2
2 I\1
Y &apos;survivre&apos;
(survive ]
&apos;necessaire&apos;
[ necessary ]
</figure>
<figureCaption confidence="0.27160125">
Deeper semantic decomposition for &apos;X SECOURT 2 Y&apos;
(with the definition of SECOURIR 1 substituted
for the latter)
Figure 6
</figureCaption>
<bodyText confidence="0.8244718">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 267
Igor Mel&apos;euk and Alain Polguere A Formal Lexicon in Meaning-Text Theory
Such a rule is nothing less than the central element of a
lexical entry; more precisely, the core of its semantic
zone .5
Note that the same causative relation that exists
between SECOURIR 1 and SECOURIR 2 exists also
between AIDER 1 and AIDER 2 (which appeared in our
first example, sentence (1)), as can be illustrated by the
definition of AIDER 2 in Fig. 7:
</bodyText>
<subsubsectionHeader confidence="0.40039">
2.1.4. ON INTERPRETING THE SEMR.
</subsubsectionHeader>
<bodyText confidence="0.999366714285714">
As we have already said, the SemR in the MTT is aimed
at representing the linguistic meaning, i.e., the common
core of all synonymous utterances; it serves to reduce
the enormous synonymy of natural language to a stan-
dard and easily manageable form — but it has no direct
link with the real world. Therefore, the full-fledged
description of linguistic behavior should include another
</bodyText>
<figure confidence="0.97678048">
&apos;moment&apos;
[ time ] &apos;moment&apos;
o
004 1 0[ time ]
&apos;aider 2&apos;
2 3 \
o o
X Z
1
&lt;=&gt;
&apos;effectuer&apos;
[ make ] o
1/X
2
&apos;aider 1&apos;
(help 1 ]
[ resources ]
&apos;ressources.
W
o 1
.ox
3
&apos;employer&apos;
[ use ]
Definition of the French verb AIDER 2
</figure>
<figureCaption confidence="0.7469488">
In English, the network can be read as follows:
&apos;Y carrying out Z, X uses his resources W in order
for W to help 1 Y to carry out Z; the use of resources
by X and the carrying out of Z by Y are simultaneous&apos;.
Figure 7
</figureCaption>
<bodyText confidence="0.972550954545455">
To sum up the semantic inclusion relations among the
four lexemes discussed, let us present these in the
following obvious form:
`secourir 2&apos; includes `secourir 1&apos;, which includes
&apos;aider 1&apos;, which is included in &apos;aider 2&apos;;
at the same time, there is an approximate proportionality:
`secourir 2&apos; / `secourir 1&apos; &apos;aider 2&apos; / &apos;aider 1&apos;.
5 In our framework, a (lexicographic) definition is a semantic rule
which is part of the semantic component of the MTM. Semantic rules
of this type serve to reduce a SemR network to a more compact form
(if applied from left to right) and/or to expand a SemR network (if
applied from right to left). A second type of semantic rule are rules
matching a lexeme meaning with the corresponding lexeme, i.e. rules
that carry out the transition between a SemR and a DSyntR (or vice
versa). Generally speaking, all the rules of the MTM are subdivided
into these two types: (a) rules establishing correspondences between
units of the same linguistic level, or equivalence rules (semantic rules
of the type illustrated in this paper in Figs. 4, 5, 7; paraphrasing rules
— see (9) below); (b) rules establishing correspondences between
units of two adjacent linguistic levels, or manifestation rules (e.g.,
Fig. 9). We are in no position to go into further details, but we would
like to stake out this important distinction.
representation — the representation of the state of affairs
in the real world, and another model — the Reality-
Meaning Model. The latter ensures the transition between
the results of perception etc. and the SemR, which is the
starting/end point of linguistic activity proper.
Our exclusion of real world knowledge from the
SemR and therefore from the MTM opposes our ap-
proach to others, such as, e.g., Montague Grammar,
which claims that the rules associating semantic repre-
sentations with sentences and the rules interpreting the
same representations in terms of extralinguistic reality
(set-theoretical interpretation) are of the same nature
and can be integrated within the same model. We insist
on distinguishing the SemR proper from the real world/
knowledge representation because we feel that the
description of linguistic activity in the strict sense of the
term is very different from the description of perceptual
or logical activity.
To illustrate the relation between a SemR and the
corresponding real-world representation, we can take
the example of machine translation. We think that the
only common denominator of two texts (in different
</bodyText>
<page confidence="0.890255">
268 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.834401">
Igor MerCuk and Alain Polguire A Formal Lexicon in Meaning-Text Theory
</note>
<bodyText confidence="0.999233857142857">
languages) equivalent under translation is the represen-
tation of the state of affairs referred to, not a SemR of
either text. The SemR is not an ideal interlingua —
although it can be effectively used to bring closer the
source and the target languages — because a SemR, by
definition, reflects the idiomatic characteristics of the
language in question.
</bodyText>
<subsectionHeader confidence="0.986112">
2.2. SYNTACTIC ZONE.
</subsectionHeader>
<bodyText confidence="0.9999544">
This zone stores the data on the syntactic behavior of
the head lexeme — more specifically, on its capacity to
participate in various syntactic configurations. Along
with the part of speech (= syntactic category), the
syntactic zone presents two major types of information:
</bodyText>
<listItem confidence="0.825885">
— Syntactic features.
</listItem>
<bodyText confidence="0.978672666666667">
— The government pattern.
These two types are distinguished by their bearing on
the semantic actants (= arguments) of the head lexeme.
</bodyText>
<subsectionHeader confidence="0.492874">
2.2.1. SYNTACTIC FEATURES.
</subsectionHeader>
<bodyText confidence="0.999734">
A syntactic feature of lexeme L specifies particular
syntactic structures which accept L but which are not
directly related to the semantic actants appearing in its
definition (though this does not preclude a syntactic
feature from being linked to the meaning of L in a
different way). For instance, the feature &amp;quot;geogr&amp;quot; sin-
gles out English common nouns capable of appearing in
the construction
</bodyText>
<equation confidence="0.7884115">
Detdef + NI geogr ± of ± xT2
&amp;quot; proper,
</equation>
<bodyText confidence="0.990729333333333">
having the meaning &apos;the N1 called N2&apos;
Such nouns are, e.g., city, island, republic (but not
mountain, peninsula, river, . .
</bodyText>
<listItem confidence="0.672969">
(3) a. the city of London
</listItem>
<figure confidence="0.925573333333333">
the island of Borneo
the Republic of Poland
b. *the Mountain of Montblanc &lt;the Montblanc
Mountain&gt;
*the Peninsula of Kamtchatka &lt;the Kamtchatka
Peninsula&gt;
</figure>
<figureCaption confidence="0.609989">
*the River of Saint-Lawrence &lt;the Saint-Law-
rence River&gt;.
</figureCaption>
<bodyText confidence="0.9996884">
Note that &amp;quot;geogr&amp;quot; will block such an expression as
*Mountain of London, but not because London is not a
mountain; this will happen because a mountain does not
syntactically admit its name in the form of of X. The
London Mountain will not be precluded by this feature
since it is syntactically perfectly correct.
Syntactic features, which do not presuppose strictly
disjoint sets, provide for a more flexible and multifacet-
ted subclassification of lexemes than do parts of speech,
which induce a strict partition of the lexical stock.
</bodyText>
<subsubsectionHeader confidence="0.508151">
2.2.2. GOVERNMENT PATTERN.
</subsubsectionHeader>
<bodyText confidence="0.99566725">
The government pattern of lexeme L specifies the
correspondence between L&apos;s semantic actants and their
realization at the DSynt-level, SSynt-level and DMorph-
level. It is a rectangular matrix with three rows:
</bodyText>
<equation confidence="0.438821">
— the upper one contains semantic actants of L
</equation>
<bodyText confidence="0.980182869565217">
— the middle one indicates the DSynt-roles
(ULM,...) played by the manifestations of the Sem-
actants on the DSynt-level with respect to L;
— the lower one indicates structural words and
morphological forms necessary for the manifesta-
tions of the same Sem-actants on the SSynt- and
DMorph-levels contingent on L.
The number of columns in this matrix is equal to the
number of Sem-actants of L. Each column specifies the
correspondence between a Sem-actant and its realiza-
tions on closer-to-surface levels.
To illustrate, we will use the government pattern of
the French lexeme AIDER 2, the definition of which has
been presented in Fig. 7. This definition involves four
semantic actants (X, Y, Z and W), all of which are
implemented in sentence (1), see the end of Section 1.1.
The government pattern of AIDER 2 has the form
shown in Fig. 8.
Let us now discuss the way the government pattern
of a lexeme is used to ensure the appropriate corre-
spondence between the representations of adjacent
levels (within the framework of a transition from the
SemR toward the sentence). We will focus on a frag-
ment of sentence (1), more specifically, on the cor-
respondence between the semantic actant Z of AIDER
2 and its syntactic correlates on both syntactic lev-
els.
In the transition between the SemR of (1) and its
DSyntR (see Figs. 1 and 2), the government pattern of
AIDER 2 establishes that Z is DSynt-actant III of this
lexeme. The correspondence &amp;quot;Z &lt;=&gt; III&amp;quot; is specified
in the 3rd column of the government pattern (Fig. 8),
which will be our working column.
The transition between the DSyntR and the SSyntR
of (1), Figs. 2 and 3, may be thought of as being carried
out in three steps:
1) Since the dependent of the DSynt-relation III,
headed by AIDER 2, is a verb,6 namely PASSER
&apos;pass&apos;, we select lines 2 (a V) and 5 (pour V) of column
III (only these two lines imply a verb occurrence).
2) We make our choice between lines 2 and 5. (In this
specific case, the choice is made according to the
restrictions imposed on the government pattern in ques-
tion, but not given in this paper. More specifically, the
preposition pour seems to imply the participation of X,
agent of AIDER 2, in activity Z; since, however,
</bodyText>
<footnote confidence="0.8411998">
6 The indication of the part of speech of a given lexeme, as well as its
government pattern, its syntactic features etc., do not appear in the
representation itself but are stored in the dictionary entry of the
lexeme, which must be made easily available to all computational
procedures.
</footnote>
<table confidence="0.353303909090909">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 269
Igor Mereuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory
X Y 1 V
I II III IV
1 . N 1 . N 1. a N 1. de N
2. a v 2. avec N
3. dans N
3. par N
4. Adv
4. pour N
5. pour V
</table>
<subsectionHeader confidence="0.577045">
Government pattern of the French verb AIDER 2
</subsectionHeader>
<bodyText confidence="0.7719484">
X, Y, Z and W are the semantic actants of AIDER 2: X is the person who
helps, Y the person who receives help, Z the activity of Y in which he
needs help, and W the resources by which X helps Y.
I, II, III and IV are the Deep-Syntactic actants of AIDER 2: I refers to
the noun phrase that expresses X etc.
</bodyText>
<figureCaption confidence="0.856947">
Figure 8
</figureCaption>
<bodyText confidence="0.941536290322581">
PASSER SON BAC &apos;pass one&apos;s exams&apos; is a purely
individual action, one has to choose the preposition a,
line 2.)
3) The selection of the preposition having been made,
a DSynt-rule (cf. Figure 9) introduces (into the SSyntR)
the lexical node A, with the concomitant SSynt-
relations: &amp;quot;indirect objective&amp;quot; — from AIDER 2 to A,
and &amp;quot;prepositional&amp;quot; — from A to PASSER (SON
BAC).
This rule provides a general frame for the expression
of DSynt- relations, but it is the government pattern of
a specific lexeme, in this case A = AIDER 2, that
supplies the specific preposition, in this case PREP = A.
Thus a particular government pattern serves to instan-
tiate in an appropriate way the variables in certain
DSynt-rules.
In general, a government pattern has associated with
it a number of restrictions concerning the cooccurrence
and the realization of actants:
— an actant cannot appear together with/without
another actant;
— a given surface form of an actant determines the
surface form of another actant;
— a given realization of an actant is possible only
under given conditions, semantic or otherwise (cf. the
restriction mentioned above, step 2, concerning the
choice between a and pour);
— etc.
These restrictions function as filters screening possi-
ble forms and combinations of actants on the DSynt-, as
well as on the SSynt-level.
</bodyText>
<subsectionHeader confidence="0.459347">
2.2.3. A FEW REMARKS ON THE GOVERNMENT
PATTERN VS. THE FEATURE APPROACH.
</subsectionHeader>
<bodyText confidence="0.997823222222222">
The notion of government pattern, as an element of
lexical description, is not in itself revolutionary and is
present in a number of linguistic theories. Thus what we
call the government pattern is related to the concept of
subcategorization in generative grammars. For in-
stance, Generalized Phrase Structure Grammar (Gazdar
et al. 1985) utilizes in its rules what are called syntactic
features,7 subcategorizing lexical items according to, in
our terminology, their government pattern. GPSG pos-
tulates the existence of subcategories, for instance
V[32], i.e. a verb of type 32, etc., which constrain the
application of grammar rules by selecting the lexical
items they can deal with. It seems obvious that it is
possible to make generalizations by grouping certain
elements of a single syntactic category on the basis of
certain similarities in their syntactic behavior; we do not
believe, however, that one has to establish those group-
ings according to general syntactic rules rather than
according to individual lexicographic descriptions.
Tackling the problem from the viewpoint of the lexicon
presents at least two advantages:
1) One can describe the syntactic behavior of lexeme
L vis a vis its syntactic actants in L&apos;s lexical entry
without having to examine a rather complex system of
syntactic rules dealing with syntactic features attributed
to L. Thus, syntactic features are a code that needs
interpretation in terms of pre-existing syntactic rules; in
</bodyText>
<footnote confidence="0.880802">
7 Let it be strongly emphasized that the GPSG concept of syntactic
feature does not correspond to syntactic features as used in the MTT
(see 2.2.1 above).
</footnote>
<page confidence="0.871209">
270 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<figure confidence="0.976102111111111">
Igor Merkuk and Main Polguere A Formal Lexicon in Meaning-Text Theory
A
o (III [PREP])
&apos;indirect objective
1 ° PREP
prepositional
o B
SSy ntR
A DSynt-rule for the realization of the DSy nt-relation III
</figure>
<figureCaption confidence="0.512472">
Figure 9
</figureCaption>
<figure confidence="0.98600525">
A
011 [PREP])
III
DSy ntR
</figure>
<bodyText confidence="0.9989941875">
contrast to that, a government pattern is a &amp;quot;speaking&amp;quot;
expression which explicitly specifies the syntactic mi-
cro-structure typical of L — independently of any
syntactic rules that might use it.
2) One avoids postulating disjoint lexemic classes
according to syntactic behavior (note that as a rule one
has no means of evaluating, a priori, how many such
classes there are and what their characteristics would
be).
We could mention also that a very detailed descrip-
tion of lexemes in terms of government patterns allows
one to consider nearly as many distinct syntactic behav-
iors (i.e., subclasses) as there are lexical items. For
French, this seems to be what can be inferred from verb
lists constructed by M. Gross and his associates (Gross
1975).
</bodyText>
<subsectionHeader confidence="0.9341335">
2.3. LEXICAL COMBINATORICS ZONE.
2.3.1. WHAT IS RESTRICTED LEXICAL COOCCURRENCE?
</subsectionHeader>
<bodyText confidence="0.999888833333333">
The main novelty of the ECD is a systematic descrip-
tion of the restricted lexical cooccurrence of every head
lexeme. Restricted lexical cooccurrence can be of two
quite different types.
The first type is illustrated by the impossible cooc-
currence observed in a sentence such as (4):
</bodyText>
<subsubsectionHeader confidence="0.431454">
(4) The telephone was drinking the sexy integrals.
</subsubsectionHeader>
<bodyText confidence="0.9984594">
In (4), certain lexemes cannot cooccur only because of
their meanings and of our knowledge of the world. The
exact translation of (4) will be deviant in any language,
and this means that the restrictions violated in (4) are of
a non-linguistic nature. They should not be covered by
a linguistic description.
In sharp contrast to (4), the phrases in (5) demon-
strate the second type of restricted lexical cooccurr-
ence, i.e. what are considered by MTT to be truly
linguistic restrictions on lexical cooccurrence:
</bodyText>
<equation confidence="0.383908">
(5) a.
Eng. (to) ASK a question
</equation>
<bodyText confidence="0.98598108">
Fr. POSER (litt. &apos;put&apos;) une question
Sp. HACER (&apos;make&apos;) una pregunta
Russ. ZADAT&apos; (&apos;give&apos;) vopros
b.
Eng. (to) LET OUT a cry
Fr. POUSSER (&apos;push&apos;) un cri
Sp. DAR (&apos;give&apos;) un grito
Russ. ISPUSTIT&apos; (let out&apos;) krik
The selection of the appropriate verb to go with a given
noun cannot be done according to the meaning of the
latter; there is nothing in the meaning of &apos;question&apos; to
explain why in English you ask it while in Spanish you
make it, in French you put it and in Russian you give it.
This type of restricted lexical cooccurrence is the prime
target of lexicographic description, which uses for this
purpose so-called lexical functions (see below).
However, we think that our lexicographic descrip-
tion has no place for selectional restrictions formulated
in terms of semantic features. Let us consider an
example of selectional restrictions as used in some
approaches in computational linguistics: McCord&apos;s Slot
Grammar (McCord 1982), which has the advantage of
giving, in a sample lexicon for a database questioning
system, fully explicit information concerning the gov-
ernment pattern of each lexical entry. Thus we have for
</bodyText>
<note confidence="0.431158">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 271
Igor MerCuk and Alain Polguire A Formal Lexicon in Meaning-Text Theory
</note>
<bodyText confidence="0.7240585">
the lexeme COURSE (as in programming course, take
a course etc.):
</bodyText>
<listItem confidence="0.702908">
(6) noun(course, crs(X,Y,_,_), nil, X:crs,
[npobj(in):Y:subject] ).
</listItem>
<bodyText confidence="0.999931133333333">
In this Prolog expression, the last argument of the
five-place predicate &apos;noun&apos; indicates that the comple-
ment (= NP object) of the lexeme COURSE introduced
by the preposition IN (course in ancient history) must
be labeled with the semantic feature &amp;quot;subject&amp;quot;. We,
however, think that such indications should by no
means appear in the description of lexemes as cooccurr-
ence restrictions. Actually, one can give a course on
anything at all (flowerpots, sexual habits of bedbugs,
etc.). Just giving a course on something makes this
something a subject. Therefore, we must indicate that
the Y argument of the predicate &apos;course&apos; is called
subject but we cannot say that in order to be able to fill
in the argument Y of COURSE a noun must be seman-
tically labeled &amp;quot;subject&amp;quot;.8
</bodyText>
<subsectionHeader confidence="0.825092">
2.3.2. THE CONCEPT OF LEXICAL FUNCTION.
</subsectionHeader>
<bodyText confidence="0.999804909090909">
A lexical function f is a dependency that associates
with a lexeme L, called the argument of f, another
lexeme (or a set of (quasi-)synonymous lexemes) L&apos;
which expresses, with respect to L, a very abstract
meaning (which can even be zero) and plays a specific
syntactic role. For instance, for a noun N denoting an
action, the lexical function Operi specifies a verb (se-
mantically empty — or at least emptied) which takes as
its grammatical subject the name of the agent of said
action and as its direct object, the lexeme N itself. Thus
the phrases in (5) are described as follows:
</bodyText>
<equation confidence="0.9018989">
(7) a.
Engl. Opel-, (QUESTION) = ASK
Fr. ()per, (QUESTION) = POSER
Sp. Opel., (PREGUNTA) = HACER
Russ. Opel., (VOPROS) = ZADAT&apos;
b.
Engl. °per, (CRY) = LET OUT
Fr. Operl CRI) = POUSSER
Sp. ()per, (GRITO) = DAR
Russ. Oper, (KRIK) = ISPUSTIT&apos;
</equation>
<bodyText confidence="0.94875556">
There are about 60 lexical functions of the Operi type,
called standard elementary LFs. They and their combi-
nations allow one to describe exhaustively and in a
highly systematic way almost the whole of restricted
lexical cooccurrence in natural languages. Given the
8 Selectional restrictions of the type just indicated are needed in most
cases for possible applications of a grammar, e.g., in order to facilitate
the resolution of syntactic homonymy. We, on the other hand, do not
want to mix the theoretical description of a grammar with tools
appropriate for its applications. Note, however, that we use selectio-
nal restrictions as well, but only if they are necessary for the choice of
the correct linguistic expression: for instance, with a given verb, one
preposition is chosen with human nouns while the other one takes
only abstract nouns, etc. (cf. the choice between pour vs. a in 2.2.2).
importance of lexical functions for &amp;quot;intelligent&amp;quot; linguis-
tic systems, we will offer an abridged list thereof in the
appendix (see also Mel&apos;euk 1982).
In the MTM LFs play a double role:
1) During the production of the text from a given
SemR, LFs control the proper choice of lexical items
linked to the lexeme in question by regular semantic
relations. For instance, if we need to express in French
the meaning &apos;badly wounded&apos; [&apos;badly&apos; Fr. mal,
&apos;wounded&apos; blesse, but &apos;badly wounded&apos; cannot be
translated by *mal blesse], we take the following steps:
</bodyText>
<listItem confidence="0.894251411764706">
(8) a. &apos;wounded&apos; Fr. blesse;
b. &apos;badly&apos; with respect to &apos;wounded&apos; is the LF Magn
[meaning &apos;very&apos;, &apos;intensely&apos;]: Magn ( wounded)
badly ;
c. therefore, &apos;badly wounded&apos; Magn ( blesse) +
blesse;
d. Magn ( blesse ) = grievement ;
e. finally, &apos;badly wounded&apos; grievement blesse.
During the analysis of a text, LFs help to resolve
syntactic homonymy, since they indicate which word has
the greater likelihood of going with which other word.
2) In text production, LFs are used to describe
sentence synonymy, or, more precisely, the derivation
of a set of synonymous sentences from the same DSynt-
structure. This is done by formulating, in terms of LFs,
a number of equivalences of the following type:
(9) Co (v) &lt; = &gt;Operi (S0 ( C0) )—&gt; So ( C0) ,
</listItem>
<bodyText confidence="0.956505333333333">
where Co stands for the head lexeme and So for the
action nominal.
This equivalence can be illustrated by (10):
(10) Alex received [C0] me quite well &lt;=&gt; Alex gave
[Operi (S0 ( Co ) )] me quite a good reception [So ( Co )].
The operation carried out by this type of rule is called
paraphrasing. About sixty paraphrasing rules of the
form (9) are needed to cover all systematic paraphrases
in any language (moreover, there must be about thirty
syntactic rules which describe transformations of trees
and &amp;quot;serve&amp;quot; the rules of the above type). A powerful
paraphrasing system is necessary, not only because it is
interesting in itself, but mainly because without such a
system it seems impossible to produce texts of good
quality for a given SemR: indeed when one is blocked
during a derivation by linguistic restrictions, one can
bypass the obstacle by recourse to paraphrases.
During text analysis, a powerful paraphrasing system
helps to reduce the vast synonymy of natural language
to a standard and therefore more manageable represen-
tation.
</bodyText>
<sectionHeader confidence="0.998319" genericHeader="method">
3. THE ECD IN THE COMPUTATIONAL
IMPLEMENTATION OF THE MEANING-TEXT MODEL.
</sectionHeader>
<bodyText confidence="0.999562333333333">
Within the framework of the global task of automatic
natural language processing (which is involved in, e.g.,
natural language understanding, machine translation,
</bodyText>
<page confidence="0.839695">
272 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.884493">
Igor Mel&apos;euk and Alain Polguere A Formal Lexicon in Meaning-Text Theory&apos;
</note>
<bodyText confidence="0.9999066875">
natural language interfaces, etc.) the MTM addresses
one rather restricted but precise problem: the produc-
tion of the highest possible number of synonymous
utterances for a given meaning, presented in the stan-
dard form of a SemR, or, inversely, the reduction of a
wealth of synonymous utterances to their standard
semantic invariant — their SemR. All the other compo-
nents of a natural language system could then be geared
to a representation of meaning, thus avoiding all the
capricious phenomena of natural language not directly
related to meaning. We would like to discuss below two
issues relevant to this aspect of natural language proc-
essing, an aspect which we consider to be strictly
linguistic, and specifically relevant to the ECD: 1) using
the lexicon in the transition between levels of linguistic
representation and 2) structuring the lexicon.
</bodyText>
<sectionHeader confidence="0.8800045" genericHeader="method">
3.1. USING THE LEXICON IN THE TRANSITION BETWEEN
LEVELS OF REPRESENTATION.
</sectionHeader>
<bodyText confidence="0.99373396969697">
To see how the ECD is applied by the MTM in the
utilization of the latter, let us assume that the MTM has
to deal with a pre-existent SemR which corresponds to
a sentence (we disregard the problem of cutting any
initial SemR, which can represent the global meaning of
a whole text, into &amp;quot;smaller&amp;quot; SemRs, representing the
meanings of separate sentences). We presuppose that
the model is able to associate with such a SemR all
DSyntRs of the sentences which a speaker of the
language in question could produce for the given mean-
ing. In other words, the model carries out the mapping
{SemRi} &lt;= = &gt; {DSyntRk}. This mapping is imple-
mented via the following two operations which are
logically distinct, even though we can conceive of them
as constantly interacting and applying simultaneously:
1) The first operation groups semantic elements of
the initial SemR into clusters each of which corresponds
to a lexical unit of the language. Thus this operation
selects the lexical stock for the target DSyntR and can be
called lexicalization. (As the careful reader may have
noticed, we have already discussed lexicalization with-
out naming it at the end of Sec. 2.1.3.)
2) The second operation organizes the lexical units
supplied by the first operation into a dependency tree
under the control, on the one hand, of the information
found under the dictionary entries for these units (var-
ious constraints on cooccurrence), and, on the other
hand, of semantic links between their sources in the
SemR. Thus this operation constructs the syntactic
structure for the target DSyntR and can be called
syntacticization.
It is obvious that the two operations are not com-
pletely independent of each other; but at present we do
not know how they interact. There is an even more
serious problem: the MTM does not offer procedural
tools ensuring different groupings of semantic network
elements, in order to join them into lexicalizable clus-
ters. In other words, the MTM in its current state lacks
mechanisms and devices for semantic network analysis.
Nevertheless, whatever these mechanisms may be, they
must rely in an essential way on the ECD, from which
they have to draw all necessary semantic and syntactic
information. After all, semantic units in a network, i.e.
in a SemR, are nothing else but lexical meanings in the
language under consideration.
Each DSyntR obtained in the way just described
undergoes a set of paraphrasing rules, which involve
lexical functions; an example of such a rule is rule (9).
These rules derive from the initial DSyntR the set of
DSyntRs which are synonymous to it, thus providing
for necessary synonymic flexibility: as we have just
said, under synthesis, the model displays the multitude
of variants, so that the selection of an appropriate one
becomes easier; under analysis, it reduces the host of
variants to a standard representation, which facilitates
its subsequent processing. The paraphrasing rules of the
MTM have already been studied from the computa-
tional viewpoint (Boyer and Lapalme 1985). Note that
theoretically these rules are reversible: although they
have been implemented in the synthesis direction, they
must function as well under analysis.
As for the operations and the corresponding compo-
nents of the MTM which have the task of carrying out
the transition between closer-to-surface levels (from
DSyntR to SSyntR, from SSyntR to DMorphR, etc.),
we will not consider them here for lack of space.
</bodyText>
<sectionHeader confidence="0.562585" genericHeader="method">
3.2. THE OVERALL STRUCTURING APPROACH TO THE
LEXICON AS A TYPICAL TRAIT OF THE ECD.
</sectionHeader>
<bodyText confidence="0.999944166666667">
Computational linguistics today knows many lan-
guage analysis and synthesis systems relying on an
explicit formalized lexicon. Traditionally, however,
these systems use a &amp;quot;flat&amp;quot; declarative representation of
lexical information. More specifically, a lexical entry in
a classical computational lexicon is an independent
proposition — i.e., a structure fully autonomous with
respect to all other entries; such an entry contains a set
of lexical data (concerning the head word) which is
almost always isolated from other similar sets of lexical
data. This boils down to the following principle: gener-
ally speaking, a lexeme is not described as a function of
other lexemes of the language, so that the information
supplied by the lexicon does not relate it to the rest of
the vocabulary. The structure of typical computational
lexica is not relational: they are basically sequences of
entries, each of which is a set of features associated
with the head lexeme. See, for instance, the entry of
McCord&apos;s Slot Grammar lexicon cited in (6) above, or
the lexicon of Pereira&apos;s well-known Chat-80 (Pereira
1983).
This approach can be loosely called non-structuring.9
(Note that this state of affairs recalls what has been
observed in the development of modern semantics:
</bodyText>
<footnote confidence="0.912451">
9 To be sure, this approach presupposes some degree of structuring;
we, however, allow ourselves to use the term to emphasize the
insufficiency of this structuring.
</footnote>
<note confidence="0.5584495">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 273
Igor MePeuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory
</note>
<bodyText confidence="0.999437142857143">
initially, linguists believed that one could do with un-
structured sets of semantic features; later, they em-
barked on a research path with a heavy emphasis on
hierarchization of semantic representations, such as
&amp;quot;logical form&amp;quot;; finally, we see the advent of highly
sophisticated semantic networks. One might expect a
similar evolution in the organization of the lexicon.)
The non-structuring approach has certain advan-
tages, the main one being the possibility of isolating, in
the course of actual text processing, a mini-fragment of
the whole lexicon, sufficient for a specific task and much
easier to manipulate. In order to analyse or synthesize a
text, a system using a non-structured lexicon will have
to deal only with the lexemes actually present in this
text, without being forced to reach out to other entries
in the lexicon. Computationally, this is very practical —
but then the system is restricted to scanty lexical
information. The result is that a non- structured lexicon
prevents the system from performing high-level linguis-
tic jobs, which involves looking for new words and set
phrases, making subtle choices, controlling style, and
the like.
In sharp contrast to this, the ECD is consistently
structured; this means that, with respect to its meaning
and its cooccurrence, a lexeme is specified in terms of
other lexemes. The necessity of such structuring mani-
fests itself in many tasks of which we will consider the
following two, considered previously: lexicalization and
paraphrasing.
The lexicalization of a SemR, i.e. of a semantic
network, is carried out due to the fact that a semantic
unit labeling one of the network&apos;s nodes is actually the
sense of a word — a lexeme, which is fully determined
in the lexicon by a hierarchical set of properties. More
specifically, one of these properties is the participation
of the lexeme in question in the definitions of other
lexemes, as well as the presence of certain lexemes in
its own definition. The screening of semantic nodes with
their properties will hopefully allow the model to carry
out successfully the analysis of SemRs in a given
language — in view of their lexicalization. We think that
it will be possible to generalize this particular way of
exploiting a structured lexicon to other levels of linguis-
tic computation.
The paraphrasing of a DSyntR, i.e. of a DSynt-tree,
which is so important to ensure the transition between
the Sem- and the DSynt-levels (see 2.3.2), is of course
possible only through Lexical Functions, which obvi-
ously represent another aspect of lexical structuring.
To conclude, we would like to mention that the
whole of the information presented in a lexicon of the
ECD format may sometimes seem too detailed with
regard to certain kinds of computational applications.
However, we do not see why, in computational as well
as in more traditional linguistics, a lexical description
could not be as complete, as consistent and as detailed
as possible, so that it could be employed (even if
partially modified) in all imaginable contexts of research
and/or application. Such a description ideally contains
all the lexical information that could be necessary for
any task; for a specific task, one can extract from this
source as much information as is deemed sufficient. A
description of a linguistic phenomenon formulated in a
theoretically consistent and exhaustive way is valuable
for computational linguistics applications because it is
&amp;quot;reusable&amp;quot;: it can be utilized in many different applica-
tions. From this point of view, the rigorous description
of even one texeme in a precise and formal framework
appears as an improvement, in itself, on what has been
achieved in this domain.
</bodyText>
<sectionHeader confidence="0.998413" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999342285714286">
Thanks to Geoffrey Hird for his comments on an earlier
draft of this paper and to Kathleen Connors and Lidija
Iordanskaja for their remarks and suggestions concern-
ing the final draft. We are also grateful to two anony-
mous reviewers of Computational Linguistics, whose
well-directed criticisms helped us considerably in re-
shaping our paper.
</bodyText>
<sectionHeader confidence="0.998663" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999840394736842">
Boyer, Michel and Lapalme, Guy. 1985 Generating Paraphrases from
Meaning- Text Semantic Networks. Computational Intelligence
1(3-4): 103-117.
Dowty, David R. 1979 Word Meaning and Montague Grammar. D.
Reidel, Dordrecht, Holland.
Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey and Sag, Ivan. 1985
Generalized Phrase Structure Grammar. Harvard University
Press, Cambridge, Massachusetts.
Gross, Maurice. 1975 Methodes en Syntaxe. Hermann, Paris, France.
Lamb, Sydney. 1966 Outline of Stratificational Grammar. George-
town University Press, Washington.
McCord, Michael C. 1982 Using Slots and Modifiers in Logic Gram-
mars for Natural Language. Artificial Intelligence 18: 327-367.
McKeown, Kathleen R. 1985 Text Generation. Cambridge University
Press, Cambridge, England.
Merduk, Igor A. 1974 Opyt Teorii Lingvistieeskix Modelej &amp;quot;Smysl-
Tekst&amp;quot;. Nauka, Moscow, USSR.
Meduk, Igor A. 1981 Meaning-Text Models: A Recent Trend in
Soviet Linguistics. Annual Review of Anthropology 10: 27-62.
Merduk, Igor A. 1982 Lexical Functions in Lexicographic Descrip-
tion. In: Proceedings of the VIllth Annual Meeting of the Berkeley
Linguistic Society, Berkeley: UCB, 427-444.
Mel&apos;duk, Igor A. 1988. Dependency Syntax: Theory and Practice.
SUNY Press, New York.
Merduk, Igor A. et al. 1984 Dictionnaire Explicatif et Combinatoire
du Fran gis Contemporain. Recherches Lexico-Semantiques I.
Presses de l&apos;Universitd de Montréal, Montréal, Canada.
Mel&apos;duk, Igor A. and 2holkovsky, Alexander K. 1984 Explanatory
Combinatorial Dictionary of Modern Russian. [in Russian] Wiener
Slawistischer Almanach, Vienna, Austria.
Pereira, Fernando C.N. 1983 Logic for Natural Language Analysis.
Technical Note 275, SRI International, Menlo Park, California.
Schank, Roger C. 1972 Conceptual Dependency: A Theory of Natural
Language Understanding. Cognitive Psychology 3: 552-631.
Sgall, Peter. 1967 Generativini Popis Javyka a Ceslai Declinace.
Academia, Prague, Czechoslovakia.
Wierzbicka, Anna. 1980 Lingua Mentalis. Academic Press, New
York, New York.
</reference>
<page confidence="0.932715">
274 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.430222">
Igor Mel&apos; euk and Alain Polguere
</note>
<sectionHeader confidence="0.93088" genericHeader="references">
APPENDIX
</sectionHeader>
<bodyText confidence="0.937361222222222">
An illustrative list of lexical functions
The argument of a LF is called its key word.
Ao
Derived adjective with the same meaning as the key
word:
Ao ( city) = urban
A1, A2, A3, . .
Typical qualifier for the first, second, third, . . . actant
of the key word:
</bodyText>
<figure confidence="0.819079419354839">
A1 ( surprise ) = surprised
A2 ( surprise) = surprising
Cont
Means &apos;continue&apos;:
ContOperi( contact ) = stay, remain, keep [ in
contact with]
(Here is a typical example of the way LFs can be
&amp;quot;composed&amp;quot; to describe more complex new relations
between lexical items.)
Contr
Contrastive term:
Contr ( top ) = bottom
Contr ( night) = day
Convkim
Conversive, or a lexeme denoting a relation that is the
converse of the one expressed by the argument of the
LF. The indices i, j, k, I indicate the type of argument
permutation which is effected:
Convn ( more ) = less
Theo is MORE religious than Oeht &lt;=&gt; Oeht is
LESS religious than Theo
Conv3214 ( sell) = buy
Ivan SOLD his soul to the Devil for three bucks &lt;=&gt;
The Devil BOUGHT Ivan&apos;s soul from him for three
bucks
Gener
Generic word:
Gener ( anger) = feeling [ of anger]
A Formal Lexicon in Meaning-Text Theory
Gener ( pain ) = sensation, feeling [ of pain ]
Labor
</figure>
<bodyText confidence="0.966033">
Semantically empty word which takes the actants i and
j as its subject and direct object, respectively, and the
key word as its indirect object:
</bodyText>
<equation confidence="0.812235375">
Labor12 ( esteem) = hold [ someone in (high) esteem]
Liqu
Means &apos;liquidate&apos;, &apos;eliminate&apos;:
Liqu ( meeting) = adjourn
Mult
Standard word for a collectivity:
Mutt ( ship) = fleet
Operi, Oper2, . . .
</equation>
<bodyText confidence="0.840686333333333">
See 2.3.2 above. Semantically empty verb which takes
the first, second, . . . actant of the key word as its
subject and the key word as its direct object:
</bodyText>
<equation confidence="0.990169">
Operi ( attention ) = pay
Oper2 ( attention) = attract
Magn
Means &apos;very&apos;, &apos;intense&apos;, &apos;intensely&apos;:
Magn ( escape) = narrow
Magn ( bleed) = profusely
</equation>
<bodyText confidence="0.858808166666667">
So
Derived noun with the same meaning as the key word:
So ( honest) = honesty
St, S2, S3, . . .
Typical noun for the first, second, third, . . . actant of
the key word:
</bodyText>
<equation confidence="0.99132325">
Si ( sell ) = vendor
S2 ( sell) = merchandise
S3 ( sell ) = buyer
S4 ( sell) = price
</equation>
<subsectionHeader confidence="0.562058">
Syn, Sync, Syn, Synn
</subsectionHeader>
<bodyText confidence="0.950450333333333">
Synonymous and quasi-synonymous (&amp;quot;C&amp;quot; means `nar-
rower&apos;; &amp;quot;D&amp;quot; means &apos;broader&apos;; &amp;quot;n&amp;quot; means
&apos;intersecting&apos;):
</bodyText>
<equation confidence="0.9577815">
Syn ( calling) = vocation
Sync ( respect ) = veneration
Syn, ( keen ) = interested
Sync ( escape ) = break out [ of], run away [ from]
</equation>
<page confidence="0.334814">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 275
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995345">LEXICON IN THE MEANING-TEXT WITH WORDS</title>
<author confidence="0.999832">Igor A Mel&apos;&apos;euk</author>
<affiliation confidence="0.962755">Departement de Universite de</affiliation>
<address confidence="0.6837615">C.P. 6128, Succ. Montréal, P.Q. H3C 3J7 Canada</address>
<author confidence="0.894225">Alain Polguere</author>
<affiliation confidence="0.904556">Odyssey Research Associates</affiliation>
<address confidence="0.712662">Place Outremont, 1290 Van Montréal, P.Q. H2V 1K6 Canada</address>
<abstract confidence="0.983806608991827">The goal of this paper is to present a particular type of lexicon, elaborated within a formal theory of natural language called Meaning-Text Theory (MTT). This theory puts strong emphasis on the development of highly structured lexica. Computational linguistics does of course recognize the importance of the lexicon in language processing. However, MTT probably goes further in this direction than various well-known approaches within computational linguistics; it assigns to the lexicon a central place, so that the rest of linguistic description is supposed to pivot around the lexicon. It is in this spirit that MTT views the model of natural language: the Meaning-Text Model, or MTM. It is believed that a very rich lexicon presenting individual information about lexemes in a consistent and detailed way facilitates the general task of computational linguistics by dividing it into two more or less autonomous subtasks: a linguistic and a computational one. The MTM lexicon, embodying a vast amount of linguistic information, can be used in different computational applications. We will present here a short outline of the lexicon in question as well as of its interaction with other components of the MTM, with special attention to computational implications of the Meaning-Text Theory. 1. LEVELS OF UTTERANCE REPRESENTATION IN MEANING-TEXT THEORY AND THE MEANING-TEXT MODEL OF NATURAL LANGUAGE. The goal of the present paper is two-fold: 1) To present a specific viewpoint on the role of lexica in &amp;quot;intelligent&amp;quot; systems designed to process texts in natural language and based on access to meaning. 2) To present a specific format for such a lexicon — so-called Explanatory Combinatorial Dictionary (ECD). We believe that a rich enough lexicon, which could enable us to solve the major problem of computational linguistics — that of presenting all necessary information about natural language in compact form, should be anchored in a formal and comprehensive theory of language. The lexicon to be discussed, that is ECD, has been conceived and developed within the framework of a particular linguistic theory — more specifically, Meaning-Text Theory or MTT (Mel&apos;a&amp;quot;uk 1974, 1981, 1988:43- 101). Note that this is by no means a theory of how linguistic knowledge could or should be applied in the context of any computational task. The MTT is a theory of how to describe and formally present linguistic knowledge, a theory of linguistic description; therefore, its contribution to computational linguistics is only a partial one: to take care exclusively of the linguistic part of the general endeavor. We cannot present here the Meaning-Text Theory in detail, so we will limit ourselves to a brief characterization of the following two aspects, which are of by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided the copies are not made for direct commercial advantage and the and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/ 87 /030261-275$03.00 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 261 Igor Mertuk and Main Polguere A Formal Lexicon in Meaning-Text Theory particular relevance to this paper: the system of linguistic representations the theory makes use of, and the linguistic Meaning-Text model which it presupposes. 1.1. UTTERANCE REPRESENTATIONS IN THE MTT. We have to warn our reader that limitations of space force us to have recourse to drastic simplifications, perhaps even too drastic sometimes. Thus, an MTT utterance representation is in fact a set of formal objects vocation being the characterization of separate aspects of the phenomena to be de- But in this paper we use the term representarefer to the main one of the structures which compose a representation (since we disregard the other structures). In Meaning-Text Theory, an utterance&apos; is represented at seven levels: 1) The Sem(antic) R(epresentation) of utterance U is, roughly speaking, a network which depicts the linguistic meaning of U without taking into consideration the way this meaning is expressed in U (distribution of meaning between words and constructions, and the like). Thus a SemR represents in fact the meaning of the whole family utterances synonymous with each The nodes of a SemR network are labeled with semantic units, a semantic unit being a specific sense of a lexeme in the language in question. The arcs of the network are labeled with distinctive numbers which identify different arguments of a predicate. Thus, a 1 P 2 o o o is equivalent to the more familiar notation P( a , b). As the reader can see, our SemR is based on predicateargument relations (although we do not use the linear notation of predicate calculus nor predicate calculus as such). 2) The D(eep-)Synt(actic) R(epresentation) of U is, roughly speaking, a dependency tree whose nodes are not linearly ordered (because linear order is taken to be a means of expressing syntactic structure rather than being part of it). The nodes of a DSyntR are labeled with meaningful lexemes of U, which are supplied with vague term used on purpose. In the present paper we take the sentence as our basic unit, but the MTT is not restricted to sentences. In principle, it can deal with sequences of sentences, although up to now, within the MTT, the way to represent such sequences and the rules to process them have not yet been developed (in contrast with such works as, e.g., McKeown (1985)). 2The term is be construed here in the narrowest sense— as referring to strictly linguistic meaning, i.e. the meaning (of utterances) which is given to any native speaker just by the mastery of his We say this to avoid a misunderstanding: our nothing to do with &amp;quot;actual&amp;quot; meaning, which is aimed at by such questions as &apos;What do you mean by that?&apos; or &apos;What is the meaning of this paper?&apos; This restricted character of meaning in our interpretation will become clear when we discuss semantic representation in the below). meaning-bearing morphological values (such as number in nouns or tense in verbs; in our example below, we indicate such values for the top node only). The branches of a DSyntR carry the names of universal DSynt-relations, which are few in number (less than ten). 3) The S(urface-)Synt(actic) R(epresentation) of U is also a dependency tree of the same formal type but, its nodes are labeled with all actual lexemic occurrences of U (including all structural words), and branches of it carry the names of a few dozen specific SSynt-relations, which correspond to the actual syntactic constructions of a particular language. The distinction between Deepand Surfacesublevels is related to the fact that some syntactic phenomena (basically, cooccurrence restrictions) are more linked to meaning, while other syntactic phenomena (word order, agreement, and the like) are more relevant from the viewpoint of actual text. Phenomena of the first type are captured in the DSyntR, which is geared to meaning, and those of the second type, in the SSyntR, geared to text. In the same vein and with the same purpose, MTT two sublevels — deep — in morphology and phonology: 4) D(eep-)Morph(ological) R(epresentation). 5) S(urface-)Morph(ological) R(epresentation). 6) D(eep-)Phon(etic) R(epresentation), or phonological representation. 7) S(urface-)Phon(etic) R(epresentation), or phonetic representation proper. will not justify the Deepdichotomy or, more generally, the composition and organization of our set of representation levels. Instead of this, we will try to link our somewhat abstract statements to a specific example. Namely, we will quote a French sentence along with its representations on the first three levels. We will not consider here the morphological and phonological representations of this sentence, since they are not relevant to our goal. Note that throughout this paper we will use examples borrowed from French (since we had the corresponding information available only in that language, while to work out the English examples would require special research, which we are in no position to undertake); but to facilitate the reading of all the examples, we will supply approximate English glosses for all French lexical items. Let us consider French sentence (1): Fr. aide Mordecai&apos; (Y) a passer son ses conseils avertis `Alcide helped Mordecai pass his [high school leaving] exams with his judicious advice&apos;. At the Sem-level, sentence (1) appears as Figure 1: 262 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 SemR of Sentence (1) Figure 1 1 3 Igor Mel&apos;Cuk and Main Polguere &apos;aider 2&apos; [ help 2 1 o \ A Formal Lexicon in Meaning-Text Theory &apos;avant&apos; o [ before ] &apos;maintenant&apos; [ now ] 1 o &apos;Alcide&apos; &apos;Mordecai&apos; 1 &apos;passer le bac&apos; [ pass one&apos;s exams ] 0 1o -2 &apos;conseils avertis&apos; &apos;concerner&apos; [ judicious advice ] [ concern ] 1 In this figure, SI is a dummy to indicate an unspecified meaning (it is not specified what exactly the advice from Alcide is). In principle, every lexemic sense must be identified by a number; for the sake of simplicity we do this here for one node only: AIDER 2, which will be discussed in 2.1.3. To avoid unnecessary complications, we have grouped certain semantic elements together (&apos;passer le bac&apos; and `conseils avertis&apos;). The same shortcut is used in the next two figures. At the DSynt-level, sentence (1) appears as follows: The subscript &amp;quot;present perfect&amp;quot; on AIDER 2 corresponds, in the SemR, to the subnetwork &apos;before now&apos;: this is roughly the meaning of the French present perfect (called &amp;quot;passé compose&amp;quot;). During the transition to the SSynt-level, this subscript triggers a DSynt-rule that introduces the appropriate auxiliary verb (in our case, AVOIR &apos;have&apos;) along with the auxiliary SSyntrelation, linking it to the lexical verb (i.e., AIDER 2) in the participial form: see Fig. 3. Note that the first DSynt-dependent of the verb AIDER 2 in the DSyntR (ALCIDE) must be linked as the grammatical subject to the AVOIR node in the SSyntR, while all other dependents of AIDER 2 remain dependents of its participial form. At the SSynt-level, we get Figure 3: Now we will move to the characterization of the formal device, a set of rules, which ensures the transition between the representations of the above levels: the Meaning-Text Model (MTM). 1.2. THE MEANING-TEXT MODEL The MTM is nothing else but what is currently called avoid this usage because we would like to and even contrast being parts of a linguistic model). More specifically, it is AIDER 2 [ help 2 1 o present perfect o I III IV ALCI DE o o MORDECAI SES CONSEILS AYERTIS [ his judicious advice ] o PASSER SON BAC [ pass his exams ] DSy ntR of Sentence (1) Figure 2 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 263 Igor Mereuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory [ have ] ALCIDE MORDECAI SSyntR of Sentence (1) Figure 3 predicative auxiliary direct objective o prepositional [ his judicious advice ] PASSER SON BAC [ pass his exams ] indirect I objective 1 , o A o SES CONSEILS AVERT IS AIDER 2 [ help 2 ] oblique o PAR a set of formal rules, with complex internal organization, which, so to speak, translate the initial SemR into a final SPhonR (or into a written text) and vice versa. Of course, in doing so, the rules pass through all intermerepresentations. the MTM is subdivided into components such that each one deals with the between two adjacent levels + 1; given seven levels of representation, there are six components: 1) The Semantic Component (transition between SemR and DSyntR); 2) The Deep-Syntactic Component (transition between DSyntR and SSyntR); 3) The Surface-Syntactic Component (transition between SSyntR and DMorphR); 4) The Deep-Morphological Component (transition between DMorphR and SMorphR); 5) The Surface-Morphological Component (transition between SMorphR and DPhonR); 6) The Deep-Phonetic Component (transition between DPhonR and SPhonR). Each component has a roughly identical internal structure; namely, it contains three types of rules: — well-formedness rules for representations of the source level; — well-formedness rules for representations of the target level; — transition rules proper. The well-formedness rules serve simultaneously both to check the correctness of the representation in question and to contol the application of the transition rules. Let it be emphasized that until now the MTM (and the MTT in general) does not contain the data necessary to effectively carry out the said control; thus the development and organization of these data constitutes, from a computational viewpoint, the main problem in the application of the MTT. To sum up, the synthesis of a sentence appears in the Meaning-Text framework as a series of subsequent transitions, or translations, from one representation to the next one, beginning with SemR; the analysis takes of course the opposite direction, starting with the SPhonR or with the written text. This is, however, only a logical description of what happens. In a real computational implementation, it is often necessary, in order to take a decision concerning a particular level of representation, to consider several other levels simultaneously. the reader has probably realized, the beto so-called of language, launched about a quarter of a century ago (Lamb 1966, Sgall 1967, Mel&apos;euk 1974); note that at present we can see a clear tendency to introduce certain ideas of the stratificational approach into theoretical and computalinguistics (compare, e.g., the distinction of sev- 264 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Igor Mereuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory eral levels of linguistic representation in Bresnan&apos;s Lexical Functional Grammar). Since our goal in the paper is to discuss the role of a formal lexicon in a linguistic model, and, as we will try to show, our formal lexicon is a part of the semantic component (of the MTM), we will concentrate only on this portion of the model. 2. EXPLANATORY COMBINATORIAL DICTIONARY. A lexicographic unit in the ECD, i.e. a dictionary entry, covers one lexical item — a word or a set phrase — taken in one well-specified sense. All such items described in a rigorous and uniform way, so that a dictionary entry is divided into three major zones: the semantic zone, the syntactic zone, and the lexical cooccurrence zone. (We leave out of consideration all other zones, as irrelevant to the main purpose of this paper.) ZONE. 2.1.1. THE LEXICOGRAPHIC DEFINITION AS THE BASIS AN ECD ENTRY. An ECD entry is centered around the definition of the head word, i.e. the representation of its meaning, or its SemR. All particularities of the ECD, as far as the semantic domain is concerned, follow from the fact that meaning is taken to be the first and foremost motivation for everything else in the dictionary: relevant properties of lexical items are discovered and described contingent upon their definitions. In order to make this important property more obvious, let us compare the MTT approach to the lexicon with certain current approaches known in computational linguistics. The approaches we mean here are rather syntax-motivated, so that they view the lexicon as an appendix to the grammar, the latter being equated with the formulation of syntactic well-formedness. In MTT, it is exactly the opposite: the grammar is considered to be an appendix to the lexicon, an appendix which, on the one hand, expresses useful generalizations over the lexicon, and on the other hand, embodies all procedures necessary for manipulating lexical data. As we have said, the lexicon and the grammar taken together constitute the MT model, with the lexicon at its foundations. ECD definitions possess, among other things, the following two properties, which distinguish them from 3Let it be emphasized that the abstract concept of an ECD as presented below is by no means the same thing as an actual ECD in a specific form, such as, e.g., Mel&apos;6uk and and The particular concept of ECD which we propose here is no more than a logical constraint on an infinity of possible ways to build concrete ECDs. On the one hand, we insist on the concept of ECD only, without touching upon the methods of its realization; on the other hand, the actual ECDs of Russian and French, mentioned above, are not well adapted, under their present form, to computational treatment. the definitions of many lexica used in computational linguistics: a) An ECD definition must be adequate in the sense that all possible correct usages of the lexeme defined are covered by it and all incorrect usages are excluded. In other terms, all the components of an ECD definition are necessary and the set of these is sufficient for the task just stated. b) In various computational or formal approaches, lexicographic definitions are written in terms of a small set of prefabricated elements. It looks as if the researcher&apos;s goal were to make his definitional language as different as possible from his object language (cf. conceptual dependencies in Schank (1972), logical representations in Dowty (1979), and the like). One major drawback of this type of approach is that it does not provide for a direct and explicit expression of lexical cohesion in the language under analysis: possible relationships among lexical items have to be inferred from their definitions in a very indirect way. In sharp contrast to this, the ECD defines a lexeme L of language L in of other lexemes L 1, . . of L in such a way that the meaning of an L, is simpler than that of L, which precludes circularity (for further discussion, see 2.1.2.a). As a result, in the ECD, semantic relationships among lexemes are established and explicitly stated directly via their definitions. 2.1.2. FORMAL CHARACTER OF AN ECD DEFINITION. An ECD definition is a decomposition of the meaning of the corresponding lexeme. It is a semantic network whose nodes are labeled either with semantic units lexemes) of with variables, and whose arcs are labeled with distinctive numbers which identify different arguments of a predicate. A lexical label represents the definition (the meaning) of the corresponding lexeme, rather than the lexeme itself. Therefore, each node of a definitional network stands, in its turn, for another network, whose nodes are replaceable by their corresponding networks, and so forth, until the bottom level primitives are reached. For practical reasons, though, one can take as primitives the lexemes of the level at which the researcher decides to stop the process of decomposing. This approach is directly related to the pioneering work of Wierzbicka: see, e.g., Wierzbicka (1980). The elaboration of ECD definitions must satisfy a number of principles, of which we will mention here the following two: Decomposition Principle that a lexeme be defined in terms of lexemes which are semantically simpler than it; as mentioned above, this precludes circularity. This principle, if applied consistently, will 4By mean &apos;possible in any imaginable context&apos; — with the obvious exception of contexts involving either the phonetic form (as, e.g., in poetry) or metalinguistic use of lexical items. Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 265 Mel&apos;kuk and Alain Polguere A Formal Lexicon in Theory &apos;employ er &apos; [ use I &apos;secourir 1• [ succour ] &apos;secourir 2&apos; (resources ] X Definition of the French verb SECOURIR 2 What we mean by a (lexicographic) definition of a lexical meaning is an equivalence between this lexical meaning itself taken together with its Sem-actants (the righthand part of the figure) and its semantic decomposition observing the Maximal Block Principle (the lefthand part). In English, the network in the lefthand part can be read as: &apos;X uses Z which is X&apos;s resources in order to SECOURIR 1 Y&apos; Figure 4 to set of semantic primitives. As is easily seen, we do not begin by postulating a set of semantic primitives: we hope to have them discovered by a long and painstaking process of semantic decomposition applied to thousands of actual lexical items. But let it be underscored that within the MTT framework it is not vital to have semantic primitives at hand in order to be able to proceed successfully with linguistic description. Maximal Block Principle that, within the definition of lexeme L, any subnetwork which is the definition of L&apos; be replaced by L&apos;; this ensures the graduality of decomposition, which contributes to making explicit interlexical links in the language. In fact this principle forbids writing a definition in terms of semantic primitives if semantic units of a higher level are available; this makes our definitions immediately graspable and more workable. 2.1.3. AN EXAMPLE OF THE INTERACTION AMONG DEFINITIONS. illustrate the way the vocabulary of semantically hierarchized using ECD definitions, we will consider some definitions involving semantically related lexemes. Let us take the French verb SECOURIR, roughly &apos;succour&apos;. (Although this English gloss is not a very familiar word in Modern English, we cannot find a better equivalent, since there is no single English lexical item that covers exactly the meaning of this French verb. However we believe that this is a good example because it shows how different the lexicalizations of the same meaning in two different languages can be. We hope that our semantic description of SECOURIR will eventually make its meaning clear to the reader.) SE- COURIR corresponds to two lexemes the meaning of which can be illustrated by the following examples: (2) SECOURIR 1 Ce vaccin a secouru de nombreux enfants &apos;This vaccine helped/saved many children&apos;. SECOURIR 2 Le medecin a secouru de nombreux enfants avec ce vaccin &apos;The doctor helped/saved many children with this vaccine&apos;. There exists an obvious semantic relation between the two lexemes: a causative one; thus SECOURIR 2 means &apos;to use something for it to SECOURIR 1 someone&apos;, that is, very roughly, &apos;to cause something to SECOURIR 1 someone&apos; (the concept of causation is implicit in `use&apos;). In a more formal way, we can represent the meaning of SECOURIR 2 by a semantic network which includes SECOURIR 1 (Figure 4). SECOURIR 1, in its turn, can be defined in terms of 1, roughly &apos;help&apos; (as in lumiere aide les o I &apos;ressources&apos; 266 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Igor MePeuk and Main Polguere A Formal Lexicon in Meaning-Text Theory &apos;aider 1 &apos; (help 1 ] o 4----- Y &apos;survivre&apos; [ survive ] &apos;necessaire&apos; 1 72 (necessary ] &lt; &gt; &apos; o Definition of the French verb SECOURIR 1 In English, the network can be read as: &apos; X helps 1 Y to survive, this help1 being necessary for Y&apos;s survival&apos; Figure 5 dans leur croissance, &apos;Light helps plants in their growth&apos;), plus the following two important semantic components: &apos;survive&apos; and &apos;necessary&apos;. More specif- SECOURT 1 &apos;X AIDE 1 [=helps survive, X&apos;s helping being necessary for Y&apos;s survival&apos;. The node `secourir 1&apos; in the network of Fig. 4 can be replaced by its own definition, i.e. by the network of Fig. 5, giving Fig. 6. It is obvious that, generally speaking, the inverse substitution is also possible: a network representing a definition of a lexical meaning can be replaced by a single node representing this meaning. This type of network manipulation must be carried out automatically by substitution procedures based on lexico-semantic rules of the form: &lt;=&gt; MEANING OF LEXEME &apos;employer&apos; (use ] o 1 &apos;ressources&apos; [ resources ] help cl&apos; Y &apos;survivre&apos; (survive ] &apos;necessaire&apos; [ necessary ] Deeper semantic decomposition for &apos;X SECOURT 2 Y&apos; (with the definition of SECOURIR 1 substituted for the latter) Figure 6 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 267 Igor Mel&apos;euk and Alain Polguere A Formal Lexicon in Meaning-Text Theory Such a rule is nothing less than the central element of a lexical entry; more precisely, the core of its semantic Note that the same causative relation that exists between SECOURIR 1 and SECOURIR 2 exists also between AIDER 1 and AIDER 2 (which appeared in our first example, sentence (1)), as can be illustrated by the definition of AIDER 2 in Fig. 7: 2.1.4. ON INTERPRETING THE SEMR. As we have already said, the SemR in the MTT is aimed representing the i.e., the common core of all synonymous utterances; it serves to reduce the enormous synonymy of natural language to a standard and easily manageable form — but it has no direct link with the real world. Therefore, the full-fledged description of linguistic behavior should include another &apos;moment&apos; [ time ] &apos;moment&apos; o 0[ ] &apos;aider 2&apos; 3\ o o X Z 1 &lt;=&gt; &apos;effectuer&apos; [ make ] o 1/X 2 &apos;aider 1&apos; (help 1 ] [ resources W o 1 3 &apos;employer&apos; [ use ] Definition of the French verb AIDER 2 In English, the network can be read as follows: &apos;Y carrying out Z, X uses his resources W in order for W to help 1 Y to carry out Z; the use of resources by X and the carrying out of Z by Y are simultaneous&apos;. Figure 7 To sum up the semantic inclusion relations among the four lexemes discussed, let us present these in the following obvious form: 2&apos; 1&apos;, includes 1&apos;, which is included in 2&apos;; at the same time, there is an approximate proportionality: `secourir 2&apos; / `secourir 1&apos; &apos;aider 2&apos; / &apos;aider 1&apos;. 5In our framework, a (lexicographic) definition is a semantic rule which is part of the semantic component of the MTM. Semantic rules of this type serve to reduce a SemR network to a more compact form (if applied from left to right) and/or to expand a SemR network (if applied from right to left). A second type of semantic rule are rules matching a lexeme meaning with the corresponding lexeme, i.e. rules that carry out the transition between a SemR and a DSyntR (or vice versa). Generally speaking, all the rules of the MTM are subdivided into these two types: (a) rules establishing correspondences between of the same linguistic level, or rules rules of the type illustrated in this paper in Figs. 4, 5, 7; paraphrasing rules — see (9) below); (b) rules establishing correspondences between of two adjacent linguistic levels, or rules Fig. 9). We are in no position to go into further details, but we would like to stake out this important distinction. representation — the representation of the state of affairs in the real world, and another model — the Reality- Meaning Model. The latter ensures the transition between the results of perception etc. and the SemR, which is the starting/end point of linguistic activity proper. Our exclusion of real world knowledge from the SemR and therefore from the MTM opposes our approach to others, such as, e.g., Montague Grammar, which claims that the rules associating semantic representations with sentences and the rules interpreting the same representations in terms of extralinguistic reality (set-theoretical interpretation) are of the same nature and can be integrated within the same model. We insist on distinguishing the SemR proper from the real world/ knowledge representation because we feel that the description of linguistic activity in the strict sense of the term is very different from the description of perceptual or logical activity. To illustrate the relation between a SemR and the corresponding real-world representation, we can take the example of machine translation. We think that the only common denominator of two texts (in different 268 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Igor MerCuk and Alain Polguire A Formal Lexicon in Meaning-Text Theory languages) equivalent under translation is the representation of the state of affairs referred to, not a SemR of either text. The SemR is not an ideal interlingua — although it can be effectively used to bring closer the source and the target languages — because a SemR, by definition, reflects the idiomatic characteristics of the language in question. 2.2. SYNTACTIC ZONE. This zone stores the data on the syntactic behavior of the head lexeme — more specifically, on its capacity to participate in various syntactic configurations. Along with the part of speech (= syntactic category), the syntactic zone presents two major types of information: — Syntactic features. — The government pattern. These two types are distinguished by their bearing on the semantic actants (= arguments) of the head lexeme. 2.2.1. SYNTACTIC FEATURES. A syntactic feature of lexeme L specifies particular syntactic structures which accept L but which are not directly related to the semantic actants appearing in its definition (though this does not preclude a syntactic feature from being linked to the meaning of L in a different way). For instance, the feature &amp;quot;geogr&amp;quot; singles out English common nouns capable of appearing in the construction + NI geogr± of± the meaning &apos;the called nouns are, e.g., island, republic not mountain, peninsula, river, . . a. city of London the island of Borneo the Republic of Poland Mountain of Montblanc &lt;the Montblanc Mountain&gt; *the Peninsula of Kamtchatka &lt;the Kamtchatka Peninsula&gt; *the River of Saint-Lawrence &lt;the Saint-Lawrence River&gt;. Note that &amp;quot;geogr&amp;quot; will block such an expression as of London, not because London is not a this will happen because mountain not its name in the form of X. The Mountain not be precluded by this feature since it is syntactically perfectly correct. Syntactic features, which do not presuppose strictly disjoint sets, provide for a more flexible and multifacetted subclassification of lexemes than do parts of speech, which induce a strict partition of the lexical stock. 2.2.2. GOVERNMENT PATTERN. The government pattern of lexeme L specifies correspondence between L&apos;s semantic actants and their realization at the DSynt-level, SSynt-level and DMorphlevel. It is a rectangular matrix with three rows: — the upper one contains semantic actants of L — the middle one indicates the DSynt-roles (ULM,...) played by the manifestations of the Semactants on the DSynt-level with respect to L; — the lower one indicates structural words and morphological forms necessary for the manifestations of the same Sem-actants on the SSyntand DMorph-levels contingent on L. The number of columns in this matrix is equal to the number of Sem-actants of L. Each column specifies the correspondence between a Sem-actant and its realizations on closer-to-surface levels. To illustrate, we will use the government pattern of the French lexeme AIDER 2, the definition of which has been presented in Fig. 7. This definition involves four semantic actants (X, Y, Z and W), all of which are in sentence the end of Section 1.1. The government pattern of AIDER 2 has the form shown in Fig. 8. Let us now discuss the way the government pattern of a lexeme is used to ensure the appropriate correspondence between the representations of adjacent levels (within the framework of a transition from the SemR toward the sentence). We will focus on a fragment of sentence (1), more specifically, on the correspondence between the semantic actant Z of AIDER 2 and its syntactic correlates on both syntactic levels. the transition between the SemR of its DSyntR (see Figs. 1 and 2), the government pattern of 2 establishes that Z is DSynt-actant this The correspondence &amp;quot;Z &lt;=&gt; specified in the 3rd column of the government pattern (Fig. 8), which will be our working column. The transition between the DSyntR and the SSyntR of (1), Figs. 2 and 3, may be thought of as being carried out in three steps: Since the dependent of the DSynt-relation by AIDER 2, is a namely PASSER we select lines 2 and (pour of column these two lines imply a verb occurrence). 2) We make our choice between lines 2 and 5. (In this specific case, the choice is made according to the restrictions imposed on the government pattern in question, but not given in this paper. More specifically, the to imply the participation of X, agent of AIDER 2, in activity Z; since, however, 6The indication of the part of speech of a given lexeme, as well as its government pattern, its syntactic features etc., do not appear in the representation itself but are stored in the dictionary entry of the lexeme, which must be made easily available to all computational procedures.</abstract>
<note confidence="0.6497545">Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 269 Igor Mereuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory X Y 1 V I II III IV 1 . N 1 . N a 1. de N 2. a v 2. avec N 3. dans N 3. par N 4. Adv 4. pour N 5. pour V Government pattern of the French verb AIDER 2</note>
<abstract confidence="0.988786342222223">X, Y, Z and W are the semantic actants of AIDER 2: X is the person who helps, Y the person who receives help, Z the activity of Y in which he needs help, and W the resources by which X helps Y. I, II, III and IV are the Deep-Syntactic actants of AIDER 2: I refers to the noun phrase that expresses X etc. Figure 8 PASSER SON BAC &apos;pass one&apos;s exams&apos; is a purely action, one has to choose the preposition line 2.) 3) The selection of the preposition having been made, a DSynt-rule (cf. Figure 9) introduces (into the SSyntR) the lexical node A, with the concomitant SSynt- &amp;quot;indirect objective&amp;quot; — from AIDER 2 to &amp;quot;prepositional&amp;quot; — from PASSER (SON BAC). This rule provides a general frame for the expression of DSyntrelations, but it is the government pattern of a specific lexeme, in this case A = AIDER 2, that the specific preposition, in this case = Thus a particular government pattern serves to instantiate in an appropriate way the variables in certain DSynt-rules. In general, a government pattern has associated with it a number of restrictions concerning the cooccurrence and the realization of actants: — an actant cannot appear together with/without another actant; — a given surface form of an actant determines the surface form of another actant; — a given realization of an actant is possible only under given conditions, semantic or otherwise (cf. the restriction mentioned above, step 2, concerning the between a and — etc. These restrictions function as filters screening possible forms and combinations of actants on the DSynt-, as well as on the SSynt-level. A REMARKS ON THE FEATURE APPROACH. The notion of government pattern, as an element of lexical description, is not in itself revolutionary and is present in a number of linguistic theories. Thus what we call the government pattern is related to the concept of subcategorization in generative grammars. For instance, Generalized Phrase Structure Grammar (Gazdar al. utilizes in its rules what are called subcategorizing lexical items according to, in our terminology, their government pattern. GPSG postulates the existence of subcategories, for instance V[32], i.e. a verb of type 32, etc., which constrain the application of grammar rules by selecting the lexical items they can deal with. It seems obvious that it is possible to make generalizations by grouping certain elements of a single syntactic category on the basis of certain similarities in their syntactic behavior; we do not believe, however, that one has to establish those groupings according to general syntactic rules rather than according to individual lexicographic descriptions. Tackling the problem from the viewpoint of the lexicon presents at least two advantages: can describe the syntactic behavior of lexeme L vis a vis its syntactic actants in L&apos;s lexical entry without having to examine a rather complex system of syntactic rules dealing with syntactic features attributed to L. Thus, syntactic features are a code that needs interpretation in terms of pre-existing syntactic rules; in 7Let it be strongly emphasized that the GPSG concept of syntactic feature does not correspond to syntactic features as used in the MTT (see 2.2.1 above). 270 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Merkuk and Main Polguere A Formal Lexicon in Theory A o (III [PREP]) &apos;indirect objective 1° PREP prepositional SSy ntR A DSynt-rule for the realization of the DSy nt-relation III Figure 9 A 011 [PREP]) III DSy ntR contrast to that, a government pattern is a &amp;quot;speaking&amp;quot; expression which explicitly specifies the syntactic mitypical of of any syntactic rules that might use it. 2) One avoids postulating disjoint lexemic classes according to syntactic behavior (note that as a rule one has no means of evaluating, a priori, how many such classes there are and what their characteristics would be). We could mention also that a very detailed description of lexemes in terms of government patterns allows one to consider nearly as many distinct syntactic behaviors (i.e., subclasses) as there are lexical items. For French, this seems to be what can be inferred from verb lists constructed by M. Gross and his associates (Gross 1975). COMBINATORICS ZONE. 2.3.1. WHAT IS RESTRICTED LEXICAL COOCCURRENCE? The main novelty of the ECD is a systematic description of the restricted lexical cooccurrence of every head lexeme. Restricted lexical cooccurrence can be of two quite different types. The first type is illustrated by the impossible cooccurrence observed in a sentence such as (4): telephone was drinking the sexy integrals. In (4), certain lexemes cannot cooccur only because of their meanings and of our knowledge of the world. The exact translation of (4) will be deviant in any language, and this means that the restrictions violated in (4) are of a non-linguistic nature. They should not be covered by a linguistic description. In sharp contrast to (4), the phrases in (5) demonstrate the second type of restricted lexical cooccurrence, i.e. what are considered by MTT to be truly linguistic restrictions on lexical cooccurrence: question POSER (litt. &apos;put&apos;) question HACER (&apos;make&apos;) pregunta ZADAT&apos; (&apos;give&apos;) b. OUT cry POUSSER (&apos;push&apos;) cri DAR (&apos;give&apos;) grito ISPUSTIT&apos; (let out&apos;) The selection of the appropriate verb to go with a given noun cannot be done according to the meaning of the latter; there is nothing in the meaning of &apos;question&apos; to why in English you while in Spanish you in French you and in Russian you This type of restricted lexical cooccurrence is the prime target of lexicographic description, which uses for this so-called functions below). However, we think that our lexicographic description has no place for selectional restrictions formulated in terms of semantic features. Let us consider an example of selectional restrictions as used in some approaches in computational linguistics: McCord&apos;s Slot Grammar (McCord 1982), which has the advantage of giving, in a sample lexicon for a database questioning system, fully explicit information concerning the government pattern of each lexical entry. Thus we have for Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 271 Igor MerCuk and Alain Polguire A Formal Lexicon in Meaning-Text Theory lexeme COURSE (as in course, take a course etc.): (6) noun(course, crs(X,Y,_,_), nil, X:crs, [npobj(in):Y:subject] ). In this Prolog expression, the last argument of the five-place predicate &apos;noun&apos; indicates that the complement (= NP object) of the lexeme COURSE introduced the preposition IN in ancient history) be labeled with the semantic feature &amp;quot;subject&amp;quot;. We, however, think that such indications should by no means appear in the description of lexemes as cooccurrence restrictions. Actually, one can give a course on anything at all (flowerpots, sexual habits of bedbugs, etc.). Just giving a course on something makes this something a subject. Therefore, we must indicate that the Y argument of the predicate &apos;course&apos; is called we cannot say that in order to be able to fill in the argument Y of COURSE a noun must be semanlabeled 2.3.2. THE CONCEPT OF LEXICAL FUNCTION. A lexical function f is a dependency that associates with a lexeme L, called the argument of f, another lexeme (or a set of (quasi-)synonymous lexemes) L&apos; which expresses, with respect to L, a very abstract meaning (which can even be zero) and plays a specific syntactic role. For instance, for a noun N denoting an the lexical function specifies a verb (semantically empty — or at least emptied) which takes as its grammatical subject the name of the agent of said action and as its direct object, the lexeme N itself. Thus the phrases in (5) are described as follows: (7) a. Engl. Opel-, (QUESTION) = ASK Fr. ()per, (QUESTION) = POSER Sp. Opel., (PREGUNTA) = HACER Russ. Opel., (VOPROS) = ZADAT&apos; b. Engl. °per, (CRY) = LET OUT CRI) = POUSSER Sp. ()per, (GRITO) = DAR Russ. Oper, (KRIK) = ISPUSTIT&apos; are about 60 lexical functions of the type, elementary LFs. and their combinations allow one to describe exhaustively and in a highly systematic way almost the whole of restricted lexical cooccurrence in natural languages. Given the 8Selectional restrictions of the type just indicated are needed in most cases for possible applications of a grammar, e.g., in order to facilitate the resolution of syntactic homonymy. We, on the other hand, do not want to mix the theoretical description of a grammar with tools appropriate for its applications. Note, however, that we use selectional restrictions as well, but only if they are necessary for the choice of the correct linguistic expression: for instance, with a given verb, one preposition is chosen with human nouns while the other one takes abstract nouns, etc. (cf. the choice between 2.2.2). importance of lexical functions for &amp;quot;intelligent&amp;quot; linguistic systems, we will offer an abridged list thereof in the appendix (see also Mel&apos;euk 1982). In the MTM LFs play a double role: 1) During the production of the text from a given SemR, LFs control the proper choice of lexical items linked to the lexeme in question by regular semantic relations. For instance, if we need to express in French meaning &apos;badly wounded&apos; [&apos;badly&apos; Fr. &apos;badly wounded&apos; cannot be by blesse], take the following steps: a. &apos;wounded&apos; Fr. &apos;badly&apos; with respect to &apos;wounded&apos; is the LF &apos;very&apos;, &apos;intensely&apos;]: ( badly ; therefore, &apos;badly wounded&apos; ( blesse; Magn ( ) = grievement ; finally, &apos;badly wounded&apos; During the analysis of a text, LFs help to resolve syntactic homonymy, since they indicate which word has the greater likelihood of going with which other word. 2) In text production, LFs are used to describe sentence synonymy, or, more precisely, the derivation of a set of synonymous sentences from the same DSyntstructure. This is done by formulating, in terms of LFs, a number of equivalences of the following type: Co (v) = ( )—&gt; ( , stands for the head lexeme and for the action nominal. This equivalence can be illustrated by (10): received me quite well &lt;=&gt; Alex gave ( ) )] quite a good reception ( )]. The operation carried out by this type of rule is called paraphrasing. About sixty paraphrasing rules of the form (9) are needed to cover all systematic paraphrases in any language (moreover, there must be about thirty syntactic rules which describe transformations of trees and &amp;quot;serve&amp;quot; the rules of the above type). A powerful paraphrasing system is necessary, not only because it is interesting in itself, but mainly because without such a system it seems impossible to produce texts of good quality for a given SemR: indeed when one is blocked during a derivation by linguistic restrictions, one can bypass the obstacle by recourse to paraphrases. During text analysis, a powerful paraphrasing system helps to reduce the vast synonymy of natural language to a standard and therefore more manageable representation. THE IMPLEMENTATION OF THE MEANING-TEXT MODEL. Within the framework of the global task of automatic natural language processing (which is involved in, e.g., natural language understanding, machine translation, 272 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Igor Mel&apos;euk and Alain Polguere A Formal Lexicon in Meaning-Text Theory&apos; natural language interfaces, etc.) the MTM addresses one rather restricted but precise problem: the production of the highest possible number of synonymous utterances for a given meaning, presented in the standard form of a SemR, or, inversely, the reduction of a wealth of synonymous utterances to their standard semantic invariant — their SemR. All the other components of a natural language system could then be geared to a representation of meaning, thus avoiding all the capricious phenomena of natural language not directly related to meaning. We would like to discuss below two issues relevant to this aspect of natural language processing, an aspect which we consider to be strictly linguistic, and specifically relevant to the ECD: 1) using the lexicon in the transition between levels of linguistic representation and 2) structuring the lexicon. 3.1. USING THE LEXICON IN THE TRANSITION LEVELS OF REPRESENTATION. To see how the ECD is applied by the MTM in the utilization of the latter, let us assume that the MTM has to deal with a pre-existent SemR which corresponds to a sentence (we disregard the problem of cutting any initial SemR, which can represent the global meaning of a whole text, into &amp;quot;smaller&amp;quot; SemRs, representing the meanings of separate sentences). We presuppose that the model is able to associate with such a SemR all DSyntRs of the sentences which a speaker of the language in question could produce for the given meaning. In other words, the model carries out the mapping &lt;= = &gt; This mapping is implemented via the following two operations which are logically distinct, even though we can conceive of them as constantly interacting and applying simultaneously: 1) The first operation groups semantic elements of the initial SemR into clusters each of which corresponds to a lexical unit of the language. Thus this operation lexical stock the target DSyntR and can be the careful reader may have noticed, we have already discussed lexicalization without naming it at the end of Sec. 2.1.3.) 2) The second operation organizes the lexical units supplied by the first operation into a dependency tree under the control, on the one hand, of the information found under the dictionary entries for these units (various constraints on cooccurrence), and, on the other hand, of semantic links between their sources in the Thus this operation constructs syntactic the target DSyntR and can be called syntacticization. obvious that the two operations are not completely independent of each other; but at present we do not know how they interact. There is an even more serious problem: the MTM does not offer procedural tools ensuring different groupings of semantic network elements, in order to join them into lexicalizable clusters. In other words, the MTM in its current state lacks mechanisms and devices for semantic network analysis. Nevertheless, whatever these mechanisms may be, they must rely in an essential way on the ECD, from which they have to draw all necessary semantic and syntactic information. After all, semantic units in a network, i.e. in a SemR, are nothing else but lexical meanings in the language under consideration. Each DSyntR obtained in the way just described a set of rules, involve lexical functions; an example of such a rule is rule (9). These rules derive from the initial DSyntR the set of DSyntRs which are synonymous to it, thus providing for necessary synonymic flexibility: as we have just said, under synthesis, the model displays the multitude of variants, so that the selection of an appropriate one becomes easier; under analysis, it reduces the host of variants to a standard representation, which facilitates its subsequent processing. The paraphrasing rules of the MTM have already been studied from the computational viewpoint (Boyer and Lapalme 1985). Note that theoretically these rules are reversible: although they have been implemented in the synthesis direction, they must function as well under analysis. As for the operations and the corresponding components of the MTM which have the task of carrying out the transition between closer-to-surface levels (from DSyntR to SSyntR, from SSyntR to DMorphR, etc.), we will not consider them here for lack of space. 3.2. THE OVERALL STRUCTURING APPROACH TO LEXICON AS A TYPICAL TRAIT OF THE ECD. Computational linguistics today knows many language analysis and synthesis systems relying on an explicit formalized lexicon. Traditionally, however, these systems use a &amp;quot;flat&amp;quot; declarative representation of lexical information. More specifically, a lexical entry in a classical computational lexicon is an independent proposition — i.e., a structure fully autonomous with respect to all other entries; such an entry contains a set of lexical data (concerning the head word) which is almost always isolated from other similar sets of lexical data. This boils down to the following principle: generally speaking, a lexeme is not described as a function of other lexemes of the language, so that the information supplied by the lexicon does not relate it to the rest of the vocabulary. The structure of typical computational lexica is not relational: they are basically sequences of entries, each of which is a set of features associated with the head lexeme. See, for instance, the entry of McCord&apos;s Slot Grammar lexicon cited in (6) above, or the lexicon of Pereira&apos;s well-known Chat-80 (Pereira 1983). approach can be loosely called (Note that this state of affairs recalls what has been observed in the development of modern semantics: 9To be sure, this approach presupposes some degree of structuring; we, however, allow ourselves to use the term to emphasize the insufficiency of this structuring. Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 273 Igor MePeuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory initially, linguists believed that one could do with unsets of semantic features; later, they embarked on a research path with a heavy emphasis on hierarchization of semantic representations, such as &amp;quot;logical form&amp;quot;; finally, we see the advent of highly sophisticated semantic networks. One might expect a similar evolution in the organization of the lexicon.) The non-structuring approach has certain advantages, the main one being the possibility of isolating, in the course of actual text processing, a mini-fragment of the whole lexicon, sufficient for a specific task and much easier to manipulate. In order to analyse or synthesize a text, a system using a non-structured lexicon will have to deal only with the lexemes actually present in this text, without being forced to reach out to other entries in the lexicon. Computationally, this is very practical — but then the system is restricted to scanty lexical information. The result is that a nonstructured lexicon prevents the system from performing high-level linguistic jobs, which involves looking for new words and set phrases, making subtle choices, controlling style, and the like. In sharp contrast to this, the ECD is consistently structured; this means that, with respect to its meaning and its cooccurrence, a lexeme is specified in terms of other lexemes. The necessity of such structuring manifests itself in many tasks of which we will consider the following two, considered previously: lexicalization and paraphrasing. The lexicalization of a SemR, i.e. of a semantic network, is carried out due to the fact that a semantic unit labeling one of the network&apos;s nodes is actually the sense of a word — a lexeme, which is fully determined in the lexicon by a hierarchical set of properties. More specifically, one of these properties is the participation of the lexeme in question in the definitions of other lexemes, as well as the presence of certain lexemes in its own definition. The screening of semantic nodes with their properties will hopefully allow the model to carry out successfully the analysis of SemRs in a given language — in view of their lexicalization. We think that it will be possible to generalize this particular way of exploiting a structured lexicon to other levels of linguistic computation. The paraphrasing of a DSyntR, i.e. of a DSynt-tree, which is so important to ensure the transition between the Semand the DSynt-levels (see 2.3.2), is of course possible only through Lexical Functions, which obviously represent another aspect of lexical structuring. To conclude, we would like to mention that the whole of the information presented in a lexicon of the ECD format may sometimes seem too detailed with regard to certain kinds of computational applications. However, we do not see why, in computational as well as in more traditional linguistics, a lexical description could not be as complete, as consistent and as detailed as possible, so that it could be employed (even if partially modified) in all imaginable contexts of research and/or application. Such a description ideally contains all the lexical information that could be necessary for any task; for a specific task, one can extract from this source as much information as is deemed sufficient. A description of a linguistic phenomenon formulated in a theoretically consistent and exhaustive way is valuable for computational linguistics applications because it is &amp;quot;reusable&amp;quot;: it can be utilized in many different applications. From this point of view, the rigorous description of even one texeme in a precise and formal framework appears as an improvement, in itself, on what has been achieved in this domain. ACKNOWLEDGMENTS Thanks to Geoffrey Hird for his comments on an earlier draft of this paper and to Kathleen Connors and Lidija Iordanskaja for their remarks and suggestions concerning the final draft. We are also grateful to two anonyreviewers of Linguistics, well-directed criticisms helped us considerably in reshaping our paper.</abstract>
<note confidence="0.84197685">REFERENCES Boyer, Michel and Lapalme, Guy. 1985 Generating Paraphrases from Text Semantic Networks. Intelligence 1(3-4): 103-117. David R. 1979 Meaning and Montague Grammar. Reidel, Dordrecht, Holland. Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey and Sag, Ivan. 1985 Phrase Structure Grammar. University Press, Cambridge, Massachusetts. Maurice. 1975 en Syntaxe. Paris, France. Sydney. 1966 of Stratificational Grammar. Georgetown University Press, Washington. Michael C. 1982 Using Slots and Modifiers in Logic Gramfor Natural Language. Intelligence 327-367. Kathleen R. 1985 Generation. University Press, Cambridge, England. Igor A. 1974 Teorii Lingvistieeskix Modelej &amp;quot;Smysl- Moscow, USSR. Meduk, Igor A. 1981 Meaning-Text Models: A Recent Trend in Linguistics. Review of Anthropology 27-62. Merduk, Igor A. 1982 Lexical Functions in Lexicographic Descrip- In: of the VIllth Annual Meeting of the Berkeley Society, UCB, 427-444. Igor A. 1988. Syntax: Theory and Practice. SUNY Press, New York. Igor A. al. Explicatif et Combinatoire du Fran gis Contemporain. Recherches Lexico-Semantiques I. Presses de l&apos;Universitd de Montréal, Montréal, Canada. Igor A. and K. 1984 Dictionary of Modern Russian. Russian] Wiener Slawistischer Almanach, Vienna, Austria. Fernando C.N. 1983 for Natural Language Analysis. Technical Note 275, SRI International, Menlo Park, Schank, Roger C. 1972 Conceptual Dependency: A Theory of Natural Understanding. Psychology 552-631. Peter. 1967 Popis Javyka a Ceslai Declinace. Academia, Prague, Czechoslovakia. Anna. 1980 Mentalis. Press, New York, New York. Computational Linguistics, 13, Numbers 3-4, July-December 1987</note>
<author confidence="0.536393">Igor Mel&apos; euk</author>
<author confidence="0.536393">Alain Polguere</author>
<abstract confidence="0.975764947368421">APPENDIX An illustrative list of lexical functions The argument of a LF is called its key word. Ao Derived adjective with the same meaning as the key word: ( = urban . . Typical qualifier for the first, second, third, . . . actant of the key word: ( ) = surprised ( = surprising Cont Means &apos;continue&apos;: ) = stay, remain, keep [ in contact with] (Here is a typical example of the way LFs can be &amp;quot;composed&amp;quot; to describe more complex new relations between lexical items.) Contr Contrastive term: ( ) = bottom ( = day Conversive, or a lexeme denoting a relation that is the converse of the one expressed by the argument of the LF. The indices i, j, k, I indicate the type of argument permutation which is effected: ( ) = less Theo is MORE religious than Oeht &lt;=&gt; Oeht is LESS religious than Theo ( = buy Ivan SOLD his soul to the Devil for three bucks &lt;=&gt; The Devil BOUGHT Ivan&apos;s soul from him for three bucks Gener Generic word: ( = feeling [ of anger] A Formal Lexicon in Meaning-Text Theory ( ) = sensation, feeling [ of pain ] Labor Semantically empty word which takes the actants i and j as its subject and direct object, respectively, and the key word as its indirect object: ( = hold [ someone in (high) esteem] Liqu Means &apos;liquidate&apos;, &apos;eliminate&apos;: ( = adjourn Mult Standard word for a collectivity: ( = fleet . . . See 2.3.2 above. Semantically empty verb which takes the first, second, . . . actant of the key word as its subject and the key word as its direct object: ( ) = pay ( = attract Magn Means &apos;very&apos;, &apos;intense&apos;, &apos;intensely&apos;: ( = narrow ( = profusely Derived noun with the same meaning as the key word: ( = honesty . . . Typical noun for the first, second, third, . . . actant of the key word: ( sell ) = vendor ( = merchandise ( ) = buyer ( = price Syn, Synonymous and quasi-synonymous (&amp;quot;C&amp;quot; means `narrower&apos;; &amp;quot;D&amp;quot; means &apos;broader&apos;; &amp;quot;n&amp;quot; means &apos;intersecting&apos;): ( = vocation ( ) = veneration ( ) = interested</abstract>
<note confidence="0.6027365">( ) = break out [ of], run away [ from] Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 275</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michel Boyer</author>
<author>Guy Lapalme</author>
</authors>
<title>Generating Paraphrases from Meaning- Text Semantic Networks.</title>
<date>1985</date>
<journal>Computational Intelligence</journal>
<volume>1</volume>
<issue>3</issue>
<pages>103--117</pages>
<contexts>
<context position="50027" citStr="Boyer and Lapalme 1985" startWordPosition="8305" endWordPosition="8308"> of paraphrasing rules, which involve lexical functions; an example of such a rule is rule (9). These rules derive from the initial DSyntR the set of DSyntRs which are synonymous to it, thus providing for necessary synonymic flexibility: as we have just said, under synthesis, the model displays the multitude of variants, so that the selection of an appropriate one becomes easier; under analysis, it reduces the host of variants to a standard representation, which facilitates its subsequent processing. The paraphrasing rules of the MTM have already been studied from the computational viewpoint (Boyer and Lapalme 1985). Note that theoretically these rules are reversible: although they have been implemented in the synthesis direction, they must function as well under analysis. As for the operations and the corresponding components of the MTM which have the task of carrying out the transition between closer-to-surface levels (from DSyntR to SSyntR, from SSyntR to DMorphR, etc.), we will not consider them here for lack of space. 3.2. THE OVERALL STRUCTURING APPROACH TO THE LEXICON AS A TYPICAL TRAIT OF THE ECD. Computational linguistics today knows many language analysis and synthesis systems relying on an exp</context>
</contexts>
<marker>Boyer, Lapalme, 1985</marker>
<rawString>Boyer, Michel and Lapalme, Guy. 1985 Generating Paraphrases from Meaning- Text Semantic Networks. Computational Intelligence 1(3-4): 103-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
</authors>
<title>Word Meaning and</title>
<date>1979</date>
<location>Dordrecht, Holland.</location>
<contexts>
<context position="18041" citStr="Dowty (1979)" startWordPosition="2940" endWordPosition="2941">uate in the sense that all possible correct usages of the lexeme defined are covered by it and all incorrect usages are excluded. In other terms, all the components of an ECD definition are necessary and the set of these is sufficient for the task just stated. b) In various computational or formal approaches, lexicographic definitions are written in terms of a small set of prefabricated elements. It looks as if the researcher&apos;s goal were to make his definitional language as different as possible from his object language (cf. conceptual dependencies in Schank (1972), logical representations in Dowty (1979), and the like). One major drawback of this type of approach is that it does not provide for a direct and explicit expression of lexical cohesion in the language under analysis: possible relationships among lexical items have to be inferred from their definitions in a very indirect way. In sharp contrast to this, the ECD defines a lexeme L of language L in terms of other lexemes L 1 , L2, . . Li, of L in such a way that the meaning of an L, is simpler than that of L, which precludes circularity (for further discussion, see 2.1.2.a). As a result, in the ECD, semantic relationships among lexemes</context>
</contexts>
<marker>Dowty, 1979</marker>
<rawString>Dowty, David R. 1979 Word Meaning and Montague Grammar. D. Reidel, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="36121" citStr="Gazdar et al. 1985" startWordPosition="6017" endWordPosition="6020">ioned above, step 2, concerning the choice between a and pour); — etc. These restrictions function as filters screening possible forms and combinations of actants on the DSynt-, as well as on the SSynt-level. 2.2.3. A FEW REMARKS ON THE GOVERNMENT PATTERN VS. THE FEATURE APPROACH. The notion of government pattern, as an element of lexical description, is not in itself revolutionary and is present in a number of linguistic theories. Thus what we call the government pattern is related to the concept of subcategorization in generative grammars. For instance, Generalized Phrase Structure Grammar (Gazdar et al. 1985) utilizes in its rules what are called syntactic features,7 subcategorizing lexical items according to, in our terminology, their government pattern. GPSG postulates the existence of subcategories, for instance V[32], i.e. a verb of type 32, etc., which constrain the application of grammar rules by selecting the lexical items they can deal with. It seems obvious that it is possible to make generalizations by grouping certain elements of a single syntactic category on the basis of certain similarities in their syntactic behavior; we do not believe, however, that one has to establish those group</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey and Sag, Ivan. 1985 Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>Methodes en Syntaxe. Hermann,</title>
<date>1975</date>
<publisher>Georgetown University Press,</publisher>
<location>Paris, France. Lamb, Sydney.</location>
<contexts>
<context position="38462" citStr="Gross 1975" startWordPosition="6396" endWordPosition="6397">— independently of any syntactic rules that might use it. 2) One avoids postulating disjoint lexemic classes according to syntactic behavior (note that as a rule one has no means of evaluating, a priori, how many such classes there are and what their characteristics would be). We could mention also that a very detailed description of lexemes in terms of government patterns allows one to consider nearly as many distinct syntactic behaviors (i.e., subclasses) as there are lexical items. For French, this seems to be what can be inferred from verb lists constructed by M. Gross and his associates (Gross 1975). 2.3. LEXICAL COMBINATORICS ZONE. 2.3.1. WHAT IS RESTRICTED LEXICAL COOCCURRENCE? The main novelty of the ECD is a systematic description of the restricted lexical cooccurrence of every head lexeme. Restricted lexical cooccurrence can be of two quite different types. The first type is illustrated by the impossible cooccurrence observed in a sentence such as (4): (4) The telephone was drinking the sexy integrals. In (4), certain lexemes cannot cooccur only because of their meanings and of our knowledge of the world. The exact translation of (4) will be deviant in any language, and this means t</context>
</contexts>
<marker>Gross, 1975</marker>
<rawString>Gross, Maurice. 1975 Methodes en Syntaxe. Hermann, Paris, France. Lamb, Sydney. 1966 Outline of Stratificational Grammar. Georgetown University Press, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C McCord</author>
</authors>
<title>Using Slots and Modifiers in Logic Grammars for Natural Language.</title>
<date>1982</date>
<journal>Artificial Intelligence</journal>
<volume>18</volume>
<pages>327--367</pages>
<contexts>
<context position="40359" citStr="McCord 1982" startWordPosition="6709" endWordPosition="6710">; there is nothing in the meaning of &apos;question&apos; to explain why in English you ask it while in Spanish you make it, in French you put it and in Russian you give it. This type of restricted lexical cooccurrence is the prime target of lexicographic description, which uses for this purpose so-called lexical functions (see below). However, we think that our lexicographic description has no place for selectional restrictions formulated in terms of semantic features. Let us consider an example of selectional restrictions as used in some approaches in computational linguistics: McCord&apos;s Slot Grammar (McCord 1982), which has the advantage of giving, in a sample lexicon for a database questioning system, fully explicit information concerning the government pattern of each lexical entry. Thus we have for Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 271 Igor MerCuk and Alain Polguire A Formal Lexicon in Meaning-Text Theory the lexeme COURSE (as in programming course, take a course etc.): (6) noun(course, crs(X,Y,_,_), nil, X:crs, [npobj(in):Y:subject] ). In this Prolog expression, the last argument of the five-place predicate &apos;noun&apos; indicates that the complement (= NP object) of t</context>
</contexts>
<marker>McCord, 1982</marker>
<rawString>McCord, Michael C. 1982 Using Slots and Modifiers in Logic Grammars for Natural Language. Artificial Intelligence 18: 327-367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="5963" citStr="McKeown (1985)" startWordPosition="961" endWordPosition="962">es are not linearly ordered (because linear order is taken to be a means of expressing syntactic structure rather than being part of it). The nodes of a DSyntR are labeled with meaningful lexemes of U, which are supplied with The vague term utterance is used on purpose. In the present paper we take the sentence as our basic unit, but the MTT is not restricted to sentences. In principle, it can deal with sequences of sentences, although up to now, within the MTT, the way to represent such sequences and the rules to process them have not yet been developed (in contrast with such works as, e.g., McKeown (1985)). 2 The term meaning is to be construed here in the narrowest sense— as referring to strictly linguistic meaning, i.e. the meaning (of utterances) which is given to any native speaker just by the mastery of his language. We say this to avoid a misunderstanding: our meaning has nothing to do with &amp;quot;actual&amp;quot; meaning, which is aimed at by such questions as &apos;What do you mean by that?&apos; or &apos;What is the meaning of this paper?&apos; This restricted character of meaning in our interpretation will become clear when we discuss semantic representation in the MIT (see below). meaning-bearing morphological values</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, Kathleen R. 1985 Text Generation. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Merduk</author>
</authors>
<title>Opyt Teorii Lingvistieeskix Modelej &amp;quot;SmyslTekst&amp;quot;.</title>
<date>1974</date>
<publisher>Nauka,</publisher>
<location>Moscow, USSR.</location>
<marker>Merduk, 1974</marker>
<rawString>Merduk, Igor A. 1974 Opyt Teorii Lingvistieeskix Modelej &amp;quot;SmyslTekst&amp;quot;. Nauka, Moscow, USSR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Meduk</author>
</authors>
<title>Meaning-Text Models: A Recent Trend in Soviet Linguistics.</title>
<date>1981</date>
<journal>Annual Review of Anthropology</journal>
<volume>10</volume>
<pages>27--62</pages>
<marker>Meduk, 1981</marker>
<rawString>Meduk, Igor A. 1981 Meaning-Text Models: A Recent Trend in Soviet Linguistics. Annual Review of Anthropology 10: 27-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Merduk</author>
</authors>
<title>Lexical Functions in Lexicographic Description. In:</title>
<date>1982</date>
<booktitle>Proceedings of the VIllth Annual Meeting of the Berkeley Linguistic Society,</booktitle>
<pages>427--444</pages>
<location>Berkeley: UCB,</location>
<marker>Merduk, 1982</marker>
<rawString>Merduk, Igor A. 1982 Lexical Functions in Lexicographic Description. In: Proceedings of the VIllth Annual Meeting of the Berkeley Linguistic Society, Berkeley: UCB, 427-444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel&apos;duk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>SUNY Press,</publisher>
<location>New York.</location>
<marker>Mel&apos;duk, 1988</marker>
<rawString>Mel&apos;duk, Igor A. 1988. Dependency Syntax: Theory and Practice. SUNY Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Merduk</author>
</authors>
<title>Dictionnaire Explicatif et Combinatoire du Fran gis Contemporain. Recherches Lexico-Semantiques I. Presses de l&apos;Universitd de Montréal,</title>
<date>1984</date>
<location>Montréal, Canada.</location>
<marker>Merduk, 1984</marker>
<rawString>Merduk, Igor A. et al. 1984 Dictionnaire Explicatif et Combinatoire du Fran gis Contemporain. Recherches Lexico-Semantiques I. Presses de l&apos;Universitd de Montréal, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel&apos;duk</author>
<author>Alexander K 2holkovsky</author>
</authors>
<date>1984</date>
<booktitle>Explanatory Combinatorial Dictionary of Modern Russian. [in Russian] Wiener Slawistischer Almanach,</booktitle>
<location>Vienna, Austria.</location>
<marker>Mel&apos;duk, 2holkovsky, 1984</marker>
<rawString>Mel&apos;duk, Igor A. and 2holkovsky, Alexander K. 1984 Explanatory Combinatorial Dictionary of Modern Russian. [in Russian] Wiener Slawistischer Almanach, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
</authors>
<title>Logic for Natural Language Analysis.</title>
<date>1983</date>
<booktitle>Technical Note 275, SRI International,</booktitle>
<location>Menlo Park, California.</location>
<contexts>
<context position="51627" citStr="Pereira 1983" startWordPosition="8561" endWordPosition="8562">st always isolated from other similar sets of lexical data. This boils down to the following principle: generally speaking, a lexeme is not described as a function of other lexemes of the language, so that the information supplied by the lexicon does not relate it to the rest of the vocabulary. The structure of typical computational lexica is not relational: they are basically sequences of entries, each of which is a set of features associated with the head lexeme. See, for instance, the entry of McCord&apos;s Slot Grammar lexicon cited in (6) above, or the lexicon of Pereira&apos;s well-known Chat-80 (Pereira 1983). This approach can be loosely called non-structuring.9 (Note that this state of affairs recalls what has been observed in the development of modern semantics: 9 To be sure, this approach presupposes some degree of structuring; we, however, allow ourselves to use the term to emphasize the insufficiency of this structuring. Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 273 Igor MePeuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory initially, linguists believed that one could do with unstructured sets of semantic features; later, they embarked on a research pa</context>
</contexts>
<marker>Pereira, 1983</marker>
<rawString>Pereira, Fernando C.N. 1983 Logic for Natural Language Analysis. Technical Note 275, SRI International, Menlo Park, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
</authors>
<title>Conceptual Dependency: A Theory of Natural Language Understanding.</title>
<date>1972</date>
<journal>Cognitive Psychology</journal>
<volume>3</volume>
<pages>552--631</pages>
<contexts>
<context position="18000" citStr="Schank (1972)" startWordPosition="2934" endWordPosition="2935">uistics: a) An ECD definition must be adequate in the sense that all possible correct usages of the lexeme defined are covered by it and all incorrect usages are excluded. In other terms, all the components of an ECD definition are necessary and the set of these is sufficient for the task just stated. b) In various computational or formal approaches, lexicographic definitions are written in terms of a small set of prefabricated elements. It looks as if the researcher&apos;s goal were to make his definitional language as different as possible from his object language (cf. conceptual dependencies in Schank (1972), logical representations in Dowty (1979), and the like). One major drawback of this type of approach is that it does not provide for a direct and explicit expression of lexical cohesion in the language under analysis: possible relationships among lexical items have to be inferred from their definitions in a very indirect way. In sharp contrast to this, the ECD defines a lexeme L of language L in terms of other lexemes L 1 , L2, . . Li, of L in such a way that the meaning of an L, is simpler than that of L, which precludes circularity (for further discussion, see 2.1.2.a). As a result, in the </context>
</contexts>
<marker>Schank, 1972</marker>
<rawString>Schank, Roger C. 1972 Conceptual Dependency: A Theory of Natural Language Understanding. Cognitive Psychology 3: 552-631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Sgall</author>
</authors>
<title>Generativini Popis Javyka a Ceslai Declinace.</title>
<date>1967</date>
<location>Academia, Prague, Czechoslovakia.</location>
<contexts>
<context position="14128" citStr="Sgall 1967" startWordPosition="2297" endWordPosition="2298">sitions, or translations, from one representation to the next one, beginning with SemR; the analysis takes of course the opposite direction, starting with the SPhonR or with the written text. This is, however, only a logical description of what happens. In a real computational implementation, it is often necessary, in order to take a decision concerning a particular level of representation, to consider several other levels simultaneously. As the reader has probably realized, the MTM belongs to so-called stratificational models of language, launched about a quarter of a century ago (Lamb 1966, Sgall 1967, Mel&apos;euk 1974); note that at present we can see a clear tendency to introduce certain ideas of the stratificational approach into theoretical and computational linguistics (compare, e.g., the distinction of sev264 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Igor Mereuk and Alain Polguere A Formal Lexicon in Meaning-Text Theory eral levels of linguistic representation in Bresnan&apos;s Lexical Functional Grammar). Since our goal in the paper is to discuss the role of a formal lexicon in a linguistic model, and, as we will try to show, our formal lexicon is a part of the se</context>
</contexts>
<marker>Sgall, 1967</marker>
<rawString>Sgall, Peter. 1967 Generativini Popis Javyka a Ceslai Declinace. Academia, Prague, Czechoslovakia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Wierzbicka</author>
</authors>
<title>Lingua Mentalis.</title>
<date>1980</date>
<publisher>Academic Press,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="19641" citStr="Wierzbicka (1980)" startWordPosition="3208" endWordPosition="3209">fy different arguments of a predicate. A lexical label represents the definition (the meaning) of the corresponding lexeme, rather than the lexeme itself. Therefore, each node of a definitional network stands, in its turn, for another network, whose nodes are replaceable by their corresponding networks, and so forth, until the bottom level primitives are reached. For practical reasons, though, one can take as primitives the lexemes of the level at which the researcher decides to stop the process of decomposing. This approach is directly related to the pioneering work of Wierzbicka: see, e.g., Wierzbicka (1980). The elaboration of ECD definitions must satisfy a number of principles, of which we will mention here the following two: a) The Decomposition Principle requires that a lexeme be defined in terms of lexemes which are semantically simpler than it; as mentioned above, this precludes circularity. This principle, if applied consistently, will 4 By possible we mean &apos;possible in any imaginable context&apos; — with the obvious exception of contexts involving either the phonetic form (as, e.g., in poetry) or metalinguistic use of lexical items. Computational Linguistics, Volume 13, Numbers 3-4, July-Decem</context>
</contexts>
<marker>Wierzbicka, 1980</marker>
<rawString>Wierzbicka, Anna. 1980 Lingua Mentalis. Academic Press, New York, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>