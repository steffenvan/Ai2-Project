<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005200">
<title confidence="0.997632">
Using Machine Learning to Explore Human Multimodal Clarification
Strategies
</title>
<author confidence="0.996684">
Verena Rieser
</author>
<affiliation confidence="0.9960975">
Department of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.773598">
Saarbr¨ucken, D-66041
</address>
<email confidence="0.995458">
vrieser@coli.uni-sb.de
</email>
<author confidence="0.995621">
Oliver Lemon
</author>
<affiliation confidence="0.998039">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.925011">
Edinburgh, EH8 9LW, GB
</address>
<email confidence="0.998273">
olemon@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973185185185">
We investigate the use of machine learn-
ing in combination with feature engineer-
ing techniques to explore human multi-
modal clarification strategies and the use
of those strategies for dialogue systems.
We learn from data collected in a Wizard-
of-Oz study where different wizards could
decide whether to ask a clarification re-
quest in a multimodal manner or else use
speech alone. We show that there is a
uniform strategy across wizards which is
based on multiple features in the context.
These are generic runtime features which
can be implemented in dialogue systems.
Our prediction models achieve a weighted
f-score of 85.3% (which is a 25.5% im-
provement over a one-rule baseline). To
assess the effects of models, feature dis-
cretisation, and selection, we also conduct
a regression analysis. We then interpret
and discuss the use of the learnt strategy
for dialogue systems. Throughout the in-
vestigation we discuss the issues arising
from using small initial Wizard-of-Oz data
sets, and we show that feature engineer-
ing is an essential step when learning from
such limited data.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999880676470588">
Good clarification strategies in dialogue systems
help to ensure and maintain mutual understand-
ing and thus play a crucial role in robust conversa-
tional interaction. In dialogue application domains
with high interpretation uncertainty, for example
caused by acoustic uncertainties from a speech
recogniser, multimodal generation and input leads
to more robust interaction (Oviatt, 2002) and re-
duced cognitive load (Oviatt et al., 2004). In this
paper we investigate the use of machine learning
(ML) to explore human multimodal clarification
strategies and the use of those strategies to decide,
based on the current dialogue context, when a di-
alogue system’s clarification request (CR) should
be generated in a multimodal manner.
In previous work (Rieser and Moore, 2005)
we showed that for spoken CRs in human-
human communication people follow a context-
dependent clarification strategy which systemati-
cally varies across domains (and even across Ger-
manic languages). In this paper we investigate
whether there exists a context-dependent “intu-
itive” human strategy for multimodal CRs as well.
To test this hypothesis we gathered data in a
Wizard-of-Oz (WOZ) study, where different wiz-
ards could decide when to show a screen output.
From this data we build prediction models, using
supervised learning techniques together with fea-
ture engineering methods, that may explain the un-
derlying process which generated the data. If we
can build a model which predicts the data quite re-
liably, we can show that there is a uniform strategy
that the majority of our wizards followed in certain
contexts.
</bodyText>
<figureCaption confidence="0.999114">
Figure 1: Methodology and structure
</figureCaption>
<bodyText confidence="0.976079">
The overall method and corresponding structure
of the paper is as shown in figure 1. We proceed
</bodyText>
<page confidence="0.986177">
659
</page>
<note confidence="0.724424">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 659–666,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999843052631579">
as follows. In section 2 we present the WOZ cor-
pus from which we extract a potential context us-
ing “Information State Update” (ISU)-based fea-
tures (Lemon et al., 2005), listed in section 3. We
also address the question how to define a suit-
able “local” context definition for the wizard ac-
tions. We apply the feature engineering methods
described in section 4 to address the questions of
unique thresholds and feature subsets across wiz-
ards. These techniques also help to reduce the
context representation and thus the feature space
used for learning. In section 5 we test different
classifiers upon this reduced context and separate
out the independent contribution of learning al-
gorithms and feature engineering techniques. In
section 6 we discuss and interpret the learnt strat-
egy. Finally we argue for the use of reinforcement
learning to optimise the multimodal clarification
strategy.
</bodyText>
<sectionHeader confidence="0.976338" genericHeader="introduction">
2 The WOZ Corpus
</sectionHeader>
<bodyText confidence="0.999952264705882">
The corpus we are using for learning was col-
lected in a multimodal WOZ study of German
task-oriented dialogues for an in-car music player
application, (Kruijff-Korbayov´a et al., 2005) . Us-
ing data from a WOZ study, rather than from real
system interactions, allows us to investigate how
humans clarify. In this study six people played the
role of an intelligent interface to an MP3 player
and were given access to a database of informa-
tion. 24 subjects were given a set of predefined
tasks to perform using an MP3 player with a mul-
timodal interface. In one part of the session the
users also performed a primary driving task, us-
ing a driving simulator. The wizards were able
to speak freely and display the search results or
the playlist on the screen by clicking on vari-
ous pre-computed templates. The users were also
able to speak, as well as make selections on the
screen. The user’s utterances were immediately
transcribed by a typist. The transcribed user’s
speech was then corrupted by deleting a varying
number of words, simulating understanding prob-
lems at the acoustic level. This (sometimes) cor-
rupted transcription was then presented to the hu-
man wizard. Note that this environment introduces
uncertainty on several levels, for example multiple
matches in the database, lexical ambiguities, and
errors on the acoustic level, as described in (Rieser
et al., 2005). Whenever the wizard produced a
CR, the experiment leader invoked a questionnaire
window on a GUI, where the wizard classified
their CR according to the primary source of the
understanding problem, mapping to the categories
defined by (Traum and Dillenbourg, 1996).
</bodyText>
<subsectionHeader confidence="0.996315">
2.1 The Data
</subsectionHeader>
<bodyText confidence="0.999766">
The corpus gathered with this setup comprises
70 dialogues, 1772 turns and 17076 words. Ex-
ample 1 shows a typical multimodal clarification
sub-dialogue, 1 concerning an uncertain reference
(note that “Venus” is an album name, song title,
and an artist name), where the wizard selects a
screen output while asking a CR.
</bodyText>
<listItem confidence="0.8130155">
(1) User: Please play “Venus”.
Wizard: Does this list contain the song?
[shows list with 20 DB matches]
User: Yes. It’s number 4. [clicks on item 4]
</listItem>
<bodyText confidence="0.9986395">
For each session we gathered logging informa-
tion which consists of e.g., the transcriptions of
the spoken utterances, the wizard’s database query
and the number of results, the screen option cho-
sen by the wizard, classification of CRs, etc. We
transformed the log-files into an XML structure,
consisting of sessions per user, dialogues per task,
and turns.2
</bodyText>
<subsectionHeader confidence="0.99987">
2.2 Data analysis:
</subsectionHeader>
<bodyText confidence="0.999946294117647">
Of the 774 wizard turns 19.6% were annotated
as CRs, resulting in 152 instances for learning,
where our six wizards contributed about equal
proportions. A x2 test on multimodal strategy
(i.e. showing a screen output or not with a CR)
showed significant differences between wizards
(x2(1) = 34.21,p &lt; .000). On the other hand, a
Kruskal-Wallis test comparing user preference for
the multimodal output showed no significant dif-
ference across wizards (H(5)=10.94, p &gt; .05). 3
Mean performance ratings for the wizards’ multi-
modal behaviour ranged from 1.67 to 3.5 on a five-
point Likert scale. Observing significantly differ-
ent strategies which are not significantly different
in terms of user satisfaction, we conjecture that the
wizards converged on strategies which were ap-
propriate in certain contexts. To strengthen this
</bodyText>
<footnote confidence="0.998094555555556">
1Translated from German.
2Where a new “turn” begins at the start of each new user
utterance after a wizard utterance, taking the user utterance as
a most basic unit of dialogue progression as defined in (Paek
and Chickering, 2005).
3The Kruskal-Wallis test is the non-parametric equivalent
to a one-way ANOVA. Since the users indicated their satis-
faction on a 5-point likert scale, an ANOVA which assumes
normality would be invalid.
</footnote>
<page confidence="0.996197">
660
</page>
<bodyText confidence="0.999976083333333">
hypothesis we split the data by wizard and and per-
formed a Kruskal-Wallis test on multimodal be-
haviour per session. Only the two wizards with the
lowest performance score showed no significant
variation across session, whereas the wizards with
the highest scores showed the most varying be-
haviour. These results again indicate a context de-
pendent strategy. In the following we test this hy-
pothesis (that good multimodal clarification strate-
gies are context-dependent) by building a predic-
tion model of the strategy an average wizard took
dependent on certain context features.
</bodyText>
<sectionHeader confidence="0.995438" genericHeader="method">
3 Context/Information-State Features
</sectionHeader>
<bodyText confidence="0.9999793125">
A state or context in our system is a dialogue in-
formation state as defined in (Lemon et al., 2005).
We divide the types of information represented
in the dialogue information state into local fea-
tures (comprising low level and dialogue features),
dialogue history features, and user model fea-
tures. We also defined features reflecting the ap-
plication environment (e.g. driving). All fea-
tures are automatically extracted from the XML
log-files (and are available at runtime in ISU-
based dialogue systems). From these features we
want to learn whether to generate a screen out-
put (graphic-yes), or whether to clarify using
speech only (graphic-no). The case that the
wizard only used screen output for clarification did
not occur.
</bodyText>
<subsectionHeader confidence="0.999092">
3.1 Local Features
</subsectionHeader>
<bodyText confidence="0.99996">
First, we extracted features present in the “lo-
cal” context of a CR, such as the number
of matches returned from the data base query
(DBmatches), how many words were deleted
by the corruption algorithm4 (deletion), what
problem source the wizard indicated in the pop-
up questionnaire (source), the previous user
speech act (userSpeechAct), and the delay be-
tween the last wizard utterance and the user’s reply
(delay). 5
One decision to take for extracting these local
features was how to define the “local” context of
a CR. As shown in table 1, we experimented with
a number of different context definitions. Context
1 defined the local context to be the current turn
only, i.e. the turn containing the CR. Context 2
</bodyText>
<footnote confidence="0.970183">
4Note that this feature is only an approximation of the
ASR confidence score that we would expect in an automated
dialogue system. See (Rieser et al., 2005) for full details.
5We introduced the delay feature to handle clarifications
concerning contact.
</footnote>
<table confidence="0.991095571428572">
id Context (turns) acc/ wf- acc/ wf-score
score ma- Naive Bayes
jority(%) (%)
1 only current turn 83.0/54.9 81.0/68.3
2 current and next 71.3/50.4 72.01/68.2
3 current and previous 60.50/59.8 76.0*/75.3
4 previous, current, next 67.8/48.9 76.9*/ 74.8
</table>
<tableCaption confidence="0.9711745">
Table 1: Comparison of context definitions for lo-
cal features (* denotes p &lt; .05)
</tableCaption>
<bodyText confidence="0.999852571428571">
also considered the current turn and the turn fol-
lowing (and is thus not a “runtime” context). Con-
text 3 considered the current turn and the previous
turn. Context 4 is the maximal definition of a lo-
cal context, namely the previous, current, and next
turn (also not available at runtime). 6
To find the context type which provides the rich-
est information to a classifier, we compared the ac-
curacy achieved in a 10-fold cross validation by
a Naive Bayes classifier (as a standard) on these
data sets against the majority class baseline, us-
ing a paired t-test, we found that that for context
3 and context 4, Naive Bayes shows a significant
improvement (with p &lt; .05 using Bonferroni cor-
rection). In table 1 we also show the weighted
f-scores since they show that the high accuracy
achieved using the first two contexts is due to over-
prediction. We chose to use context 3, since these
features will be available during system runtime
and the learnt strategy could be implemented in an
actual system.
</bodyText>
<subsectionHeader confidence="0.999375">
3.2 Dialogue History Features
</subsectionHeader>
<bodyText confidence="0.999992727272727">
The history features account for events in the
whole dialogue so far, i.e. all information gath-
ered before asking the CR, such as the number of
CRs asked (CRhist), how often the screen output
was already used (screenHist), the corruption
rate so far (delHist), the dialogue duration so
far (duration), and whether the user reacted to
the screen output, either by verbally referencing
(refHist) , e.g. using expressions such as “It’s
item number 4”, or by clicking (clickHist) as
in example 1.
</bodyText>
<subsectionHeader confidence="0.975567">
3.3 User Model Features
</subsectionHeader>
<bodyText confidence="0.999725">
Under “user model features” we consider features
reflecting the wizards’ responsiveness to the be-
</bodyText>
<footnote confidence="0.703991333333333">
6Note that dependent on the context definition a CR
might get annotated differently, since placing the question
and showing the graphic might be asynchronous events.
</footnote>
<page confidence="0.99393">
661
</page>
<bodyText confidence="0.999649066666667">
haviour and situation of the user. Each session
comprised four dialogues with one wizard. The
user model features average the user’s behaviour
in these dialogues so far, such as how responsive
the user is towards the screen output, i.e. how of-
ten this user clicks (clickUser) and how fre-
quently s/he uses verbal references (refUser);
how often the wizard had already shown a screen
output (screenUser) and how many CRs were
already asked (CRuser); how much the user’s
speech was corrupted on average (delUser), i.e.
an approximation of how well this user is recog-
nised; and whether this user is currently driving or
not (driving). This information was available
to the wizard.
</bodyText>
<figure confidence="0.743011571428571">
LOCAL FEATURES
DBmatches: 20
deletion: 0
source: reference resolution
userSpeechAct: command
delay: 0
HISTORY FEATURES
[CRhist, screenHist, delHist,
refHist, clickHist]=0
duration= 10s
USER MODELFEATURES
[clickUser,refUser,screenUser,
CRuser]=0
driving= true
</figure>
<figureCaption confidence="0.9643695">
Figure 2: Features in the context after the first turn
in example 1.
</figureCaption>
<subsectionHeader confidence="0.623932">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999899695652174">
Note that all these features are generic over
information-seeking dialogues where database re-
sults can be displayed on a screen; except for
driving which only applies to hands-and-eyes-
busy situations. Figure 2 shows a context for ex-
ample 1, assuming that it was the first utterance by
this user.
This potential feature space comprises 18 fea-
tures, many of them taking numeric attributes as
values. Considering our limited data set of 152
training instances we run the risk of severe data
sparsity. Furthermore we want to explore which
features of this potential feature space influenced
the wizards’ multimodal strategy. In the next
two sections we describe feature engineering tech-
niques, namely discretising methods for dimen-
sionality reduction and feature selection methods,
which help to reduce the feature space to a sub-
set which is most predictive of multimodal clarifi-
cation. For our experiments we use implementa-
tions of discretisation and feature selection meth-
ods provided by the WEKA toolkit (Witten and
Frank, 2005).
</bodyText>
<sectionHeader confidence="0.9979" genericHeader="method">
4 Feature Engineering
</sectionHeader>
<subsectionHeader confidence="0.999348">
4.1 Discretising Numeric Features
</subsectionHeader>
<bodyText confidence="0.9999739375">
Global discretisation methods divide all contin-
uous features into a smaller number of distinct
ranges before learning starts. This has two advan-
tages concerning the quality of our data for ML.
First, discretisation methods take feature distribu-
tions into account and help to avoid sparse data.
Second, most of our features are highly positively
skewed. Some ML methods (such as the standard
extension of the Naive Bayes classifier to handle
numeric features) assume that numeric attributes
have a normal distribution. We use Proportional
k-Interval (PKI) discretisation as a unsupervised
method, and an entropy-based algorithm (Fayyad
and Irani, 1993) based on the Minimal Description
Length (MDL) principle as a supervised discreti-
sation method.
</bodyText>
<subsectionHeader confidence="0.960541">
4.2 Feature Selection
</subsectionHeader>
<bodyText confidence="0.9999846">
Feature selection refers to the problem of select-
ing an optimum subset of features that are most
predictive of a given outcome. The objective of se-
lection is two-fold: improving the prediction per-
formance of ML models and providing a better un-
derstanding of the underlying concepts that gener-
ated the data. We chose to apply forward selec-
tion for all our experiments given our large fea-
ture set, which might include redundant features.
We use the following feature filtering methods:
correlation-based subset evaluation (CFS) (Hall,
2000) and a decision tree algorithm (rule-based
ML) for selecting features before doing the actual
learning. We also used a wrapper method called
Selective Naive Bayes, which has been shown to
perform reliably well in practice (Langley and
Sage, 1994). We also apply a correlation-based
ranking technique since subset selection models
inner-feature relations at the expense of saying
less about individual feature performance itself.
</bodyText>
<subsectionHeader confidence="0.67826">
4.3 Results for PKI and MDL Discretisation
</subsectionHeader>
<bodyText confidence="0.9981716">
Feature selection and discretisation influence one-
another, i.e. feature selection performs differently
on PKI or MDL discretised data. MDL discreti-
sation reduces our range of feature values dra-
matically. It fails to discretise 10 of 14 nu-
meric features and bars those features from play-
ing a role in the final decision structure because
the same discretised value will be given to all
instances. However, MDL discretisation cannot
replace proper feature selection methods since
</bodyText>
<page confidence="0.998565">
662
</page>
<tableCaption confidence="0.998846">
Table 2: Feature selection on PKI-discretised data (left) and on MDL-discretised data (right)
</tableCaption>
<bodyText confidence="0.999912909090909">
it doesn’t explicitly account for redundancy be-
tween features, nor for non-numerical features.
For the other 4 features which were discretised
there is a binary split around one (fairly low)
threshold: screenHist (.5), refUser (.375),
screenUser (1.0), CRUser (1.25).
Table 2 shows two figures illustrating the dif-
ferent subsets of features chosen by the feature
selection algorithms on discretised data. From
these four subsets we extracted a fifth, using all
the features which were chosen by at least two
of the feature selection methods, i.e. the features
in the overlapping circle regions shown in figure
2. For both data sets the highest ranking fea-
tures are also the ones contained in the overlapping
regions, which are screenUser, refUser
and screenHist. For implementation dialogue
management needs to keep track of whether the
user already saw a screen output in a previous in-
teraction (screenUser), or in the same dialogue
(screenHist), and whether this user (verbally)
reacted to the screen output (refUser).
</bodyText>
<sectionHeader confidence="0.714355" genericHeader="method">
5 Performance of Different Learners and
Feature Engineering
</sectionHeader>
<bodyText confidence="0.999889714285714">
In this section we evaluate the performance of fea-
ture engineering methods in combination with dif-
ferent ML algorithms (where we treat feature op-
timisation as an integral part of the training pro-
cess). All experiments are carried out using 10-
fold cross-validation. We take an approach similar
to (Daelemans et al., 2003) where parameters of
the classifier are optimised with respect to feature
selection. We use a wide range of different multi-
variate classifiers which reflect our hypothesis that
a decision is based on various features in the con-
text, and compare them against two simple base-
line strategies, reflecting deterministic contextual
behaviour.
</bodyText>
<subsectionHeader confidence="0.988381">
5.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999992285714286">
The simplest baseline we can consider is to always
predict the majority class in the data, in our case
graphic-no. This yields a 45.6% wf-score.
This baseline reflects a deterministic wizard strat-
egy never showing a screen output.
A more interesting baseline is obtained by us-
ing a 1-rule classifier. It chooses the feature
which produces the minimum error (which is
refUser for the PKI discretised data set, and
screenHist for the MDL set). We use the im-
plementation of a one-rule classifier provided in
the WEKA toolkit. This yields a 59.8% wf-score.
This baseline reflects a deterministic wizard strat-
egy which is based on a single feature only.
</bodyText>
<subsectionHeader confidence="0.995847">
5.2 Machine Learners
</subsectionHeader>
<bodyText confidence="0.999919466666667">
For learning we experiment with five different
types of supervised classifiers.We chose Naive
Bayes as a joint (generative) probabilistic model,
using the WEKA implementation of (John and Lan-
gley, 1995)’s classifier; Bayesian Networks as a
graphical generative model, again using the WEKA
implementation; and we chose maxEnt as a dis-
criminative (conditional) model, using the Max-
imum Entropy toolkit (Le, 2003). As a rule in-
duction algorithm we used JRIP, the WEKA imple-
mentation of (Cohen, 1995)’s Repeated Incremen-
tal Pruning to Produce Error Reduction (RIPPER).
And for decision trees we used the J4.8 classi-
fier (WEKA’s implementation of the C4.5 system
(Quinlan, 1993)).
</bodyText>
<subsectionHeader confidence="0.999612">
5.3 Comparison of Results
</subsectionHeader>
<bodyText confidence="0.999971166666667">
We experimented using these different classifiers
on raw data, on MDL and PKI discretised data,
and on discretised data using the different fea-
ture selection algorithms. To compare the clas-
sification outcomes we report on two measures:
accuracy and wf-score, which is the weighted
</bodyText>
<page confidence="0.997249">
663
</page>
<table confidence="0.999538857142857">
Feature transformation/ 1-rule Rule Decision maxEnt Naive Bayesian Average
(acc./ wf-score (%)) baseline Induction Tree Bayes Network
raw data 60.5/59.8 76.3/78.3 79.4/78.6 70.0/75.3 76.0/75.3 79.5/72.0 73.62/73.21
PKI + all features 60.5/ 64.6 67.1/66.4 77.4/76.3 70.7/76.7 77.5/81.6 77.3/82.3 71.75/74.65
PKI+ CFS subset 60.5/64.4 68.7/70.7 79.2/76.9 76.7/79.4 78.2/80.6 77.4/80.7 73.45/75.45
PKI+ rule-based ML 60.5/66.5 72.8/76.1 76.0/73.9 75.3/80.2 80.1/78.3 80.8/79.8 74.25/75.80
PKI+ selective Bayes 60.5/64.4 68.2/65.2 78.4/77.9 79.3/78.1 84.6/85.3 84.5/84.6 75.92/75.92
PKI+ subset overlap 60.5/64.4 70.9/70.7 75.9/76.9 76.7/78.2 84.0/80.6 83.7/80.7 75.28/75.25
MDL + all features 60.5/69.9 79.0/78.8 78.0/78.1 71.3/76.8 74.9/73.3 74.7/73.9 73.07/75.13
MDL + CFS subset 60.5/69.9 80.1/78.2 80.6/78.2 76.0/80.2 75.7/75.8 75.7/75.8 74.77/76.35
MDL + rule-based ML 60.5/75.5 80.4/81.6 78.7/80.2 79.3/78.8 82.7/82.9 82.7/82.9 77.38/80.32
MDL + select. Bayes 60.5/75.5 80.4/81.6 78.7/80.8 79.3/80.1 82.7/82.9 82.7/82.9 77.38/80.63
MDL + overlap 60.5/75.5 80.4/81.6 78.7/80.8 79.3/80.1 82.7/82.9 82.7/82.9 77.38/80.63
average 60.5/68.24 74.9/75.38 78.26/78.06 75.27/78.54 79.91/79.96 80.16/79.86
</table>
<tableCaption confidence="0.999899">
Table 3: Average accuracy and wf-scores for models in feature engineering experiments .
</tableCaption>
<bodyText confidence="0.999935303030303">
sum (by class frequency in the data; 39.5%
graphic-yes, 60.5% graphic-no) of the f-
scores of the individual classes. In table 3 we
see fairly stable high performance for Bayesian
models with MDL feature selection. However, the
best performing model is Naive Bayes using wrap-
per methods (selective Bayes) for feature selection
and PKI discretisation. This model achieves a wf-
score of 85.3%, which is a 25.5% improvement
over the 1-rule baseline.
We separately explore the models and feature
engineering techniques and their impact on the
prediction accuracy for each trial/cross-validation.
In the following we separate out the independent
contribution of models and features. To assess
the effects of models, feature discretisation and
selection on performance accuracy, we conduct
a hierarchical regression analysis. The models
alone explain 18.1% of the variation in accuracy
(R2 = .181) whereas discretisation methods only
contribute 0.4% and feature selection 1% (R2 =
.195). All parameters, except for discretisation
methods have a significant impact on modelling
accuracy (P &lt; .001), indicating that feature selec-
tion is an essential step for predicting wizard be-
haviour. The coefficients of the regression model
lead us to the following hypotheses which we ex-
plore by comparing the group means for models,
discretisation, and features selection methods. Ap-
plying a Kruskal-Wallis test with Mann-Whitney
tests as a post-hoc procedure (using Bonferroni
correction for multiple comparisons), we obtained
the following results: 7
</bodyText>
<listItem confidence="0.998367">
• All ML algorithms are significantly better
than the majority and one-rule baselines. All
</listItem>
<footnote confidence="0.996127666666667">
7We cannot report full details here. Supplementary
material is available at www.coli.uni-saarland.de/
˜vrieser/acl06-supplementary.html
</footnote>
<bodyText confidence="0.992694">
except maxEnt are significantly better than
the Rule Induction algorithm. There is no
significant difference in the performance of
Decision Tree, maxEnt, Naive Bayes, and
Bayesian Network classifiers. Multivariate
models being significantly better than the
two baseline models indicates that we have
a strategy that is based on context features.
</bodyText>
<listItem confidence="0.9489159">
• For discretisation methods we found that the
classifiers were performing significantly bet-
ter on MDL discretised data than on PKI or
continuous data. MDL being significantly
better than continuous data indicates that all
wizards behaved as though using thresholds
to make their decisions, and MDL being bet-
ter than PKI supports the hypothesis that de-
cisions were context dependent.
• All feature selection methods (except for
</listItem>
<bodyText confidence="0.897431052631579">
CFS) lead to better performance than using
all of the features. Selective Bayes and rule-
based ML selection performed significantly
better than CFS. Selective Bayes, rule-based
ML, and subset-overlap showed no signifi-
cant differences. These results show that wiz-
ards behaved as though specific features were
important (but they suggest that inner-feature
relations used by CFS are less important).
Discussion of results: These experimental re-
sults show two things. First, the results indi-
cate that we can learn a good prediction model
from our data. We conclude that our six wiz-
ards did not behave arbitrarily, but selected their
strategy according to certain contextual features.
By separating out the individual contributions of
models and feature engineering techniques, we
have shown that wizard behaviour is based on
multiple features. In sum, Decision Tree, max-
</bodyText>
<page confidence="0.996882">
664
</page>
<bodyText confidence="0.999662214285714">
Ent, Naive Bayes, and Bayesian Network clas-
sifiers on MDL discretised data using Selective
Bayes and Rule-based ML selection achieved
the best results. The best performing feature
subset was screenUser,screenHist, and
userSpeechAct. The best performing model
uses the richest feature space including the feature
driving.
Second, the regression analysis shows that us-
ing these feature engineering techniques in combi-
nation with improved ML algorithms is an essen-
tial step for learning good prediction models from
the small data sets which are typically available
from multimodal WOZ studies.
</bodyText>
<sectionHeader confidence="0.954643" genericHeader="method">
6 Interpretation of the learnt Strategy
</sectionHeader>
<bodyText confidence="0.999887218181818">
For interpreting the learnt strategies we discuss
Rule Induction and Decision Trees since they are
the easiest to interpret (and to implement in stan-
dard rule-based dialogue systems). For both we
explain the results obtained by MDL and selective
Bayes, since this combination leads to the best per-
formance.
Rule induction: Figure 3 shows a reformula-
tion of the rules from which the learned classifier
is constructed. The feature screenUser plays
a central role. These rules (in combination with
the low thresholds) say that if you have already
shown a screen output to this particular user in
any previous turn (i.e. screenUser &gt; 1), then
do so again if the previous user speech act was
a command (i.e. userSpeechAct=command)
or if you have already shown a screen out-
put in a previous turn in this dialogue (i.e.
screenHist&gt;0.5). Otherwise don’t show
screen output when asking a clarification.
Decision tree: Figure 4 shows the decision tree
learnt by the classifier J4.8. The five rules
contained in this tree also heavily rely on the
user model as well as the previous screen his-
tory. The rules constructed by the first two nodes
(screenUser, screenHist) may lead to a
repetitive strategy since the right branch will result
in the same action (graphic-yes) in all future
actions. The only variation is introduced by the
speech act, collapsing the tree to the same rule set
as in figure 3. Note that this rule-set is based on
domain independent features.
Discussion: Examining the classifications made
by our best performing Bayesian models we found
that the learnt conditional probability distribu-
tions produce similar feature-value mappings to
the rules described above. The strategy learnt
by the classifiers heavily depends on features ob-
tained in previous interactions, i.e. user model fea-
tures. Furthermore these strategies can lead to
repetitive action, i.e. if a screen output was once
shown to this user, and the user has previously
used or referred to the screen, the screen will be
used over and over again.
For learning a strategy which varies in context
but adapts in more subtle ways (e.g. to the user
model), we would need to explore many more
strategies through interactions with users to find
an optimal one. One way to reduce costs for build-
ing such an optimised strategy is to apply Rein-
forcement Learning (RL) with simulated users. In
future work we will begin with the strategy learnt
by supervised learning (which reflects sub-optimal
average wizard behaviour) and optimise it for dif-
ferent user models and reward structures.
</bodyText>
<figureCaption confidence="0.989401">
Figure 4: Five-rule tree from J4.8 (“inf” = oc)
</figureCaption>
<sectionHeader confidence="0.963241" genericHeader="conclusions">
7 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999987176470588">
We showed that humans use a context-dependent
strategy for asking multimodal clarification re-
quests by learning such a strategy from WOZ data.
Only the two wizards with the lowest performance
scores showed no significant variation across ses-
sions, leading us to hypothesise that the better wiz-
ards converged on a context-dependent strategy.
We were able to discover a runtime context based
on which all wizards behaved uniformly, using
feature discretisation methods and feature selec-
tion methods on dialogue context features. Based
on these features we were able to predict how
an ‘average’ wizard would behave in that context
with an accuracy of 84.6% (wf-score of 85.3%,
which is a 25.5% improvement over a one rule-
based baseline). We explained the learned strate-
gies and showed that they can be implemented in
</bodyText>
<page confidence="0.987811">
665
</page>
<figure confidence="0.877914">
IF screenUser&gt;1 AND (userSpeechAct=command OR screenHist&gt;0.5) THEN graphic=yes
ELSE graphic=no
</figure>
<figureCaption confidence="0.999899">
Figure 3: Reformulation of the rules learnt by JRIP
</figureCaption>
<bodyText confidence="0.999988352941176">
rule-based dialogue systems based on domain in-
dependent features. We also showed that feature
engineering is essential for achieving significant
performance gains when using large feature spaces
with the small data sets which are typical of di-
alogue WOZ studies. By interpreting the learnt
strategies we found them to be sub-optimal. In
current research, RL is applied to optimise strate-
gies and has been shown to lead to dialogue strate-
gies which are better than those present in the orig-
inal data (Henderson et al., 2005). The next step
towards a RL-based system is to add task-level and
reward-level annotations to calculate reward func-
tions, as discussed in (Rieser et al., 2005). We
furthermore aim to learn more refined clarifica-
tion strategies indicating the problem source and
its severity.
</bodyText>
<sectionHeader confidence="0.996095" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9837445">
The authors would like thank the ACL reviewers,
Alissa Melinger, and Joel Tetreault for help and dis-
cussion. This work is supported by the TALK project,
www.talk-project.org, and the International Post-
Graduate College for Language Technology and Cognitive
Systems, Saarbr¨ucken.
</bodyText>
<sectionHeader confidence="0.997909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999387059701493">
William W. Cohen. 1995. Fast effective rule induction.
In Proceedings of the 12th ICML-95.
Walter Daelemans, V´eronique Hoste, Fien De Meul-
der, and Bart Naudts. 2003. Combined optimization
of feature selection and algorithm parameter interac-
tion in machine learning of language. In Proceed-
ings of the 14th ECML-03.
Usama Fayyad and Keki Irani. 1993. Multi-
interval discretization of continuousvalued attributes
for classification learning. In Proc. IJCAI-93.
Mark Hall. 2000. Correlation-based feature selection
for discrete and numeric class machine learning. In
Proc. 17th Int Conf. on Machine Learning.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2005. Hybrid Reinforcement/Supervised
Learning for Dialogue Policies from COMMUNI-
CATOR data. In IJCAI workshop on Knowledge and
Reasoning in Practical Dialogue Systems,.
George John and Pat Langley. 1995. Estimating con-
tinuous distributions in bayesian classifiers. In Pro-
ceedings of the 11th UAI-95. Morgan Kaufmann.
Ivana Kruijff-Korbayov´a, Nate Blaylock, Ciprian Ger-
stenberger, Verena Rieser, Tilman Becker, Michael
Kaisser, Peter Poller, and Jan Schehl. 2005. An ex-
periment setup for collecting data for adaptive out-
put planning in a multimodal dialogue system. In
10th European Workshop on NLG.
Pat Langley and Stephanie Sage. 1994. Induction of
selective bayesian classifiers. In Proceedings of the
10th UAI-94.
Zhang Le. 2003. Maximum entropy modeling toolkit
for Python and C++.
Oliver Lemon, Kallirroi Georgila, James Henderson,
Malte Gabsdil, Ivan Meza-Ruiz, and Steve Young.
2005. Deliverable d4.1: Integration of learning and
adaptivity with the ISU approach.
Sharon Oviatt, Rachel Coulston, and Rebecca
Lunsford. 2004. When do we interact mul-
timodally? Cognitive load and multimodal
communication patterns. In Proceedings of the 6th
ICMI-04.
Sharon Oviatt. 2002. Breaking the robustness bar-
rier: Recent progress on the design of robust mul-
timodal systems. In Advances in Computers. Aca-
demic Press.
Tim Paek and David Maxwell Chickering. 2005.
The markov assumption in spoken dialogue manage-
ment. In Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Verena Rieser and Johanna Moore. 2005. Implica-
tions for Generating Clarification Requests in Task-
oriented Dialogues. In Proceedings of the 43rd ACL.
Verena Rieser, Ivana Kruijff-Korbayov´a, and Oliver
Lemon. 2005. A corpus collection and annota-
tion framework for learning multimodal clarification
strategies. In Proceedings of the 6th SIGdial Work-
shop on Discourse and Dialogue.
David Traum and Pierre Dillenbourg. 1996. Mis-
communication in multi-modal collaboration. In
Proceedings of the Workshop on Detecting, Repair-
ing, and Preventing Human-Machine Miscommuni-
cation. AAAI-96.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques
(Second Edition). Morgan Kaufmann.
</reference>
<page confidence="0.99843">
666
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961074">
<title confidence="0.9992385">Using Machine Learning to Explore Human Multimodal Clarification Strategies</title>
<author confidence="0.999978">Verena Rieser</author>
<affiliation confidence="0.9999705">Department of Computational Linguistics Saarland University</affiliation>
<address confidence="0.993795">Saarbr¨ucken, D-66041</address>
<email confidence="0.985442">vrieser@coli.uni-sb.de</email>
<author confidence="0.998592">Oliver Lemon</author>
<affiliation confidence="0.999968">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.997832">Edinburgh, EH8 9LW, GB</address>
<email confidence="0.996879">olemon@inf.ed.ac.uk</email>
<abstract confidence="0.999610857142857">We investigate the use of machine learning in combination with feature engineering techniques to explore human multimodal clarification strategies and the use of those strategies for dialogue systems. We learn from data collected in a Wizardof-Oz study where different wizards could decide whether to ask a clarification request in a multimodal manner or else use speech alone. We show that there is a uniform strategy across wizards which is based on multiple features in the context. These are generic runtime features which can be implemented in dialogue systems. Our prediction models achieve a weighted f-score of 85.3% (which is a 25.5% improvement over a one-rule baseline). To assess the effects of models, feature discretisation, and selection, we also conduct a regression analysis. We then interpret and discuss the use of the learnt strategy for dialogue systems. Throughout the investigation we discuss the issues arising from using small initial Wizard-of-Oz data sets, and we show that feature engineering is an essential step when learning from such limited data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the 12th ICML-95.</booktitle>
<contexts>
<context position="19800" citStr="Cohen, 1995" startWordPosition="3145" endWordPosition="3146">his baseline reflects a deterministic wizard strategy which is based on a single feature only. 5.2 Machine Learners For learning we experiment with five different types of supervised classifiers.We chose Naive Bayes as a joint (generative) probabilistic model, using the WEKA implementation of (John and Langley, 1995)’s classifier; Bayesian Networks as a graphical generative model, again using the WEKA implementation; and we chose maxEnt as a discriminative (conditional) model, using the Maximum Entropy toolkit (Le, 2003). As a rule induction algorithm we used JRIP, the WEKA implementation of (Cohen, 1995)’s Repeated Incremental Pruning to Produce Error Reduction (RIPPER). And for decision trees we used the J4.8 classifier (WEKA’s implementation of the C4.5 system (Quinlan, 1993)). 5.3 Comparison of Results We experimented using these different classifiers on raw data, on MDL and PKI discretised data, and on discretised data using the different feature selection algorithms. To compare the classification outcomes we report on two measures: accuracy and wf-score, which is the weighted 663 Feature transformation/ 1-rule Rule Decision maxEnt Naive Bayesian Average (acc./ wf-score (%)) baseline Indu</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William W. Cohen. 1995. Fast effective rule induction. In Proceedings of the 12th ICML-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>V´eronique Hoste</author>
<author>Fien De Meulder</author>
<author>Bart Naudts</author>
</authors>
<title>Combined optimization of feature selection and algorithm parameter interaction in machine learning of language.</title>
<date>2003</date>
<booktitle>In Proceedings of the 14th ECML-03.</booktitle>
<marker>Daelemans, Hoste, De Meulder, Naudts, 2003</marker>
<rawString>Walter Daelemans, V´eronique Hoste, Fien De Meulder, and Bart Naudts. 2003. Combined optimization of feature selection and algorithm parameter interaction in machine learning of language. In Proceedings of the 14th ECML-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Usama Fayyad</author>
<author>Keki Irani</author>
</authors>
<title>Multiinterval discretization of continuousvalued attributes for classification learning.</title>
<date>1993</date>
<booktitle>In Proc. IJCAI-93.</booktitle>
<contexts>
<context position="15176" citStr="Fayyad and Irani, 1993" startWordPosition="2418" endWordPosition="2421">s divide all continuous features into a smaller number of distinct ranges before learning starts. This has two advantages concerning the quality of our data for ML. First, discretisation methods take feature distributions into account and help to avoid sparse data. Second, most of our features are highly positively skewed. Some ML methods (such as the standard extension of the Naive Bayes classifier to handle numeric features) assume that numeric attributes have a normal distribution. We use Proportional k-Interval (PKI) discretisation as a unsupervised method, and an entropy-based algorithm (Fayyad and Irani, 1993) based on the Minimal Description Length (MDL) principle as a supervised discretisation method. 4.2 Feature Selection Feature selection refers to the problem of selecting an optimum subset of features that are most predictive of a given outcome. The objective of selection is two-fold: improving the prediction performance of ML models and providing a better understanding of the underlying concepts that generated the data. We chose to apply forward selection for all our experiments given our large feature set, which might include redundant features. We use the following feature filtering methods</context>
</contexts>
<marker>Fayyad, Irani, 1993</marker>
<rawString>Usama Fayyad and Keki Irani. 1993. Multiinterval discretization of continuousvalued attributes for classification learning. In Proc. IJCAI-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
</authors>
<title>Correlation-based feature selection for discrete and numeric class machine learning.</title>
<date>2000</date>
<booktitle>In Proc. 17th Int Conf. on Machine Learning.</booktitle>
<contexts>
<context position="15832" citStr="Hall, 2000" startWordPosition="2524" endWordPosition="2525">) principle as a supervised discretisation method. 4.2 Feature Selection Feature selection refers to the problem of selecting an optimum subset of features that are most predictive of a given outcome. The objective of selection is two-fold: improving the prediction performance of ML models and providing a better understanding of the underlying concepts that generated the data. We chose to apply forward selection for all our experiments given our large feature set, which might include redundant features. We use the following feature filtering methods: correlation-based subset evaluation (CFS) (Hall, 2000) and a decision tree algorithm (rule-based ML) for selecting features before doing the actual learning. We also used a wrapper method called Selective Naive Bayes, which has been shown to perform reliably well in practice (Langley and Sage, 1994). We also apply a correlation-based ranking technique since subset selection models inner-feature relations at the expense of saying less about individual feature performance itself. 4.3 Results for PKI and MDL Discretisation Feature selection and discretisation influence oneanother, i.e. feature selection performs differently on PKI or MDL discretised</context>
</contexts>
<marker>Hall, 2000</marker>
<rawString>Mark Hall. 2000. Correlation-based feature selection for discrete and numeric class machine learning. In Proc. 17th Int Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
</authors>
<title>Hybrid Reinforcement/Supervised Learning for Dialogue Policies from COMMUNICATOR data.</title>
<date>2005</date>
<booktitle>In IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems,.</booktitle>
<marker>Henderson, Lemon, Georgila, 2005</marker>
<rawString>James Henderson, Oliver Lemon, and Kallirroi Georgila. 2005. Hybrid Reinforcement/Supervised Learning for Dialogue Policies from COMMUNICATOR data. In IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George John</author>
<author>Pat Langley</author>
</authors>
<title>Estimating continuous distributions in bayesian classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of the 11th UAI-95.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="19506" citStr="John and Langley, 1995" startWordPosition="3095" endWordPosition="3099"> baseline is obtained by using a 1-rule classifier. It chooses the feature which produces the minimum error (which is refUser for the PKI discretised data set, and screenHist for the MDL set). We use the implementation of a one-rule classifier provided in the WEKA toolkit. This yields a 59.8% wf-score. This baseline reflects a deterministic wizard strategy which is based on a single feature only. 5.2 Machine Learners For learning we experiment with five different types of supervised classifiers.We chose Naive Bayes as a joint (generative) probabilistic model, using the WEKA implementation of (John and Langley, 1995)’s classifier; Bayesian Networks as a graphical generative model, again using the WEKA implementation; and we chose maxEnt as a discriminative (conditional) model, using the Maximum Entropy toolkit (Le, 2003). As a rule induction algorithm we used JRIP, the WEKA implementation of (Cohen, 1995)’s Repeated Incremental Pruning to Produce Error Reduction (RIPPER). And for decision trees we used the J4.8 classifier (WEKA’s implementation of the C4.5 system (Quinlan, 1993)). 5.3 Comparison of Results We experimented using these different classifiers on raw data, on MDL and PKI discretised data, and </context>
</contexts>
<marker>John, Langley, 1995</marker>
<rawString>George John and Pat Langley. 1995. Estimating continuous distributions in bayesian classifiers. In Proceedings of the 11th UAI-95. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivana Kruijff-Korbayov´a</author>
<author>Nate Blaylock</author>
<author>Ciprian Gerstenberger</author>
<author>Verena Rieser</author>
<author>Tilman Becker</author>
<author>Michael Kaisser</author>
<author>Peter Poller</author>
<author>Jan Schehl</author>
</authors>
<title>An experiment setup for collecting data for adaptive output planning in a multimodal dialogue system.</title>
<date>2005</date>
<booktitle>In 10th European Workshop on NLG.</booktitle>
<marker>Kruijff-Korbayov´a, Blaylock, Gerstenberger, Rieser, Becker, Kaisser, Poller, Schehl, 2005</marker>
<rawString>Ivana Kruijff-Korbayov´a, Nate Blaylock, Ciprian Gerstenberger, Verena Rieser, Tilman Becker, Michael Kaisser, Peter Poller, and Jan Schehl. 2005. An experiment setup for collecting data for adaptive output planning in a multimodal dialogue system. In 10th European Workshop on NLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pat Langley</author>
<author>Stephanie Sage</author>
</authors>
<title>Induction of selective bayesian classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 10th UAI-94.</booktitle>
<contexts>
<context position="16078" citStr="Langley and Sage, 1994" startWordPosition="2561" endWordPosition="2564">-fold: improving the prediction performance of ML models and providing a better understanding of the underlying concepts that generated the data. We chose to apply forward selection for all our experiments given our large feature set, which might include redundant features. We use the following feature filtering methods: correlation-based subset evaluation (CFS) (Hall, 2000) and a decision tree algorithm (rule-based ML) for selecting features before doing the actual learning. We also used a wrapper method called Selective Naive Bayes, which has been shown to perform reliably well in practice (Langley and Sage, 1994). We also apply a correlation-based ranking technique since subset selection models inner-feature relations at the expense of saying less about individual feature performance itself. 4.3 Results for PKI and MDL Discretisation Feature selection and discretisation influence oneanother, i.e. feature selection performs differently on PKI or MDL discretised data. MDL discretisation reduces our range of feature values dramatically. It fails to discretise 10 of 14 numeric features and bars those features from playing a role in the final decision structure because the same discretised value will be gi</context>
</contexts>
<marker>Langley, Sage, 1994</marker>
<rawString>Pat Langley and Stephanie Sage. 1994. Induction of selective bayesian classifiers. In Proceedings of the 10th UAI-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Le</author>
</authors>
<title>Maximum entropy modeling toolkit for Python and C++.</title>
<date>2003</date>
<contexts>
<context position="19714" citStr="Le, 2003" startWordPosition="3129" endWordPosition="3130">a one-rule classifier provided in the WEKA toolkit. This yields a 59.8% wf-score. This baseline reflects a deterministic wizard strategy which is based on a single feature only. 5.2 Machine Learners For learning we experiment with five different types of supervised classifiers.We chose Naive Bayes as a joint (generative) probabilistic model, using the WEKA implementation of (John and Langley, 1995)’s classifier; Bayesian Networks as a graphical generative model, again using the WEKA implementation; and we chose maxEnt as a discriminative (conditional) model, using the Maximum Entropy toolkit (Le, 2003). As a rule induction algorithm we used JRIP, the WEKA implementation of (Cohen, 1995)’s Repeated Incremental Pruning to Produce Error Reduction (RIPPER). And for decision trees we used the J4.8 classifier (WEKA’s implementation of the C4.5 system (Quinlan, 1993)). 5.3 Comparison of Results We experimented using these different classifiers on raw data, on MDL and PKI discretised data, and on discretised data using the different feature selection algorithms. To compare the classification outcomes we report on two measures: accuracy and wf-score, which is the weighted 663 Feature transformation/</context>
</contexts>
<marker>Le, 2003</marker>
<rawString>Zhang Le. 2003. Maximum entropy modeling toolkit for Python and C++.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Malte Gabsdil</author>
<author>Ivan Meza-Ruiz</author>
<author>Steve Young</author>
</authors>
<title>Deliverable d4.1: Integration of learning and adaptivity with the ISU approach.</title>
<date>2005</date>
<contexts>
<context position="3447" citStr="Lemon et al., 2005" startWordPosition="528" endWordPosition="531">an build a model which predicts the data quite reliably, we can show that there is a uniform strategy that the majority of our wizards followed in certain contexts. Figure 1: Methodology and structure The overall method and corresponding structure of the paper is as shown in figure 1. We proceed 659 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 659–666, Sydney, July 2006. c�2006 Association for Computational Linguistics as follows. In section 2 we present the WOZ corpus from which we extract a potential context using “Information State Update” (ISU)-based features (Lemon et al., 2005), listed in section 3. We also address the question how to define a suitable “local” context definition for the wizard actions. We apply the feature engineering methods described in section 4 to address the questions of unique thresholds and feature subsets across wizards. These techniques also help to reduce the context representation and thus the feature space used for learning. In section 5 we test different classifiers upon this reduced context and separate out the independent contribution of learning algorithms and feature engineering techniques. In section 6 we discuss and interpret the </context>
<context position="8657" citStr="Lemon et al., 2005" startWordPosition="1373" endWordPosition="1376">ehaviour per session. Only the two wizards with the lowest performance score showed no significant variation across session, whereas the wizards with the highest scores showed the most varying behaviour. These results again indicate a context dependent strategy. In the following we test this hypothesis (that good multimodal clarification strategies are context-dependent) by building a prediction model of the strategy an average wizard took dependent on certain context features. 3 Context/Information-State Features A state or context in our system is a dialogue information state as defined in (Lemon et al., 2005). We divide the types of information represented in the dialogue information state into local features (comprising low level and dialogue features), dialogue history features, and user model features. We also defined features reflecting the application environment (e.g. driving). All features are automatically extracted from the XML log-files (and are available at runtime in ISUbased dialogue systems). From these features we want to learn whether to generate a screen output (graphic-yes), or whether to clarify using speech only (graphic-no). The case that the wizard only used screen output for</context>
</contexts>
<marker>Lemon, Georgila, Henderson, Gabsdil, Meza-Ruiz, Young, 2005</marker>
<rawString>Oliver Lemon, Kallirroi Georgila, James Henderson, Malte Gabsdil, Ivan Meza-Ruiz, and Steve Young. 2005. Deliverable d4.1: Integration of learning and adaptivity with the ISU approach.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Oviatt</author>
<author>Rachel Coulston</author>
<author>Rebecca Lunsford</author>
</authors>
<title>When do we interact multimodally? Cognitive load and multimodal communication patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th ICMI-04.</booktitle>
<contexts>
<context position="1836" citStr="Oviatt et al., 2004" startWordPosition="271" endWordPosition="274">scuss the issues arising from using small initial Wizard-of-Oz data sets, and we show that feature engineering is an essential step when learning from such limited data. 1 Introduction Good clarification strategies in dialogue systems help to ensure and maintain mutual understanding and thus play a crucial role in robust conversational interaction. In dialogue application domains with high interpretation uncertainty, for example caused by acoustic uncertainties from a speech recogniser, multimodal generation and input leads to more robust interaction (Oviatt, 2002) and reduced cognitive load (Oviatt et al., 2004). In this paper we investigate the use of machine learning (ML) to explore human multimodal clarification strategies and the use of those strategies to decide, based on the current dialogue context, when a dialogue system’s clarification request (CR) should be generated in a multimodal manner. In previous work (Rieser and Moore, 2005) we showed that for spoken CRs in humanhuman communication people follow a contextdependent clarification strategy which systematically varies across domains (and even across Germanic languages). In this paper we investigate whether there exists a context-dependen</context>
</contexts>
<marker>Oviatt, Coulston, Lunsford, 2004</marker>
<rawString>Sharon Oviatt, Rachel Coulston, and Rebecca Lunsford. 2004. When do we interact multimodally? Cognitive load and multimodal communication patterns. In Proceedings of the 6th ICMI-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Oviatt</author>
</authors>
<title>Breaking the robustness barrier: Recent progress on the design of robust multimodal systems.</title>
<date>2002</date>
<booktitle>In Advances in Computers.</booktitle>
<publisher>Academic Press.</publisher>
<contexts>
<context position="1787" citStr="Oviatt, 2002" startWordPosition="264" endWordPosition="265">ystems. Throughout the investigation we discuss the issues arising from using small initial Wizard-of-Oz data sets, and we show that feature engineering is an essential step when learning from such limited data. 1 Introduction Good clarification strategies in dialogue systems help to ensure and maintain mutual understanding and thus play a crucial role in robust conversational interaction. In dialogue application domains with high interpretation uncertainty, for example caused by acoustic uncertainties from a speech recogniser, multimodal generation and input leads to more robust interaction (Oviatt, 2002) and reduced cognitive load (Oviatt et al., 2004). In this paper we investigate the use of machine learning (ML) to explore human multimodal clarification strategies and the use of those strategies to decide, based on the current dialogue context, when a dialogue system’s clarification request (CR) should be generated in a multimodal manner. In previous work (Rieser and Moore, 2005) we showed that for spoken CRs in humanhuman communication people follow a contextdependent clarification strategy which systematically varies across domains (and even across Germanic languages). In this paper we in</context>
</contexts>
<marker>Oviatt, 2002</marker>
<rawString>Sharon Oviatt. 2002. Breaking the robustness barrier: Recent progress on the design of robust multimodal systems. In Advances in Computers. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Paek</author>
<author>David Maxwell Chickering</author>
</authors>
<title>The markov assumption in spoken dialogue management.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="7737" citStr="Paek and Chickering, 2005" startWordPosition="1227" endWordPosition="1230">ference across wizards (H(5)=10.94, p &gt; .05). 3 Mean performance ratings for the wizards’ multimodal behaviour ranged from 1.67 to 3.5 on a fivepoint Likert scale. Observing significantly different strategies which are not significantly different in terms of user satisfaction, we conjecture that the wizards converged on strategies which were appropriate in certain contexts. To strengthen this 1Translated from German. 2Where a new “turn” begins at the start of each new user utterance after a wizard utterance, taking the user utterance as a most basic unit of dialogue progression as defined in (Paek and Chickering, 2005). 3The Kruskal-Wallis test is the non-parametric equivalent to a one-way ANOVA. Since the users indicated their satisfaction on a 5-point likert scale, an ANOVA which assumes normality would be invalid. 660 hypothesis we split the data by wizard and and performed a Kruskal-Wallis test on multimodal behaviour per session. Only the two wizards with the lowest performance score showed no significant variation across session, whereas the wizards with the highest scores showed the most varying behaviour. These results again indicate a context dependent strategy. In the following we test this hypoth</context>
</contexts>
<marker>Paek, Chickering, 2005</marker>
<rawString>Tim Paek and David Maxwell Chickering. 2005. The markov assumption in spoken dialogue management. In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="19977" citStr="Quinlan, 1993" startWordPosition="3172" endWordPosition="3173">ervised classifiers.We chose Naive Bayes as a joint (generative) probabilistic model, using the WEKA implementation of (John and Langley, 1995)’s classifier; Bayesian Networks as a graphical generative model, again using the WEKA implementation; and we chose maxEnt as a discriminative (conditional) model, using the Maximum Entropy toolkit (Le, 2003). As a rule induction algorithm we used JRIP, the WEKA implementation of (Cohen, 1995)’s Repeated Incremental Pruning to Produce Error Reduction (RIPPER). And for decision trees we used the J4.8 classifier (WEKA’s implementation of the C4.5 system (Quinlan, 1993)). 5.3 Comparison of Results We experimented using these different classifiers on raw data, on MDL and PKI discretised data, and on discretised data using the different feature selection algorithms. To compare the classification outcomes we report on two measures: accuracy and wf-score, which is the weighted 663 Feature transformation/ 1-rule Rule Decision maxEnt Naive Bayesian Average (acc./ wf-score (%)) baseline Induction Tree Bayes Network raw data 60.5/59.8 76.3/78.3 79.4/78.6 70.0/75.3 76.0/75.3 79.5/72.0 73.62/73.21 PKI + all features 60.5/ 64.6 67.1/66.4 77.4/76.3 70.7/76.7 77.5/81.6 7</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Johanna Moore</author>
</authors>
<title>Implications for Generating Clarification Requests in Taskoriented Dialogues.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="2172" citStr="Rieser and Moore, 2005" startWordPosition="324" endWordPosition="327">onal interaction. In dialogue application domains with high interpretation uncertainty, for example caused by acoustic uncertainties from a speech recogniser, multimodal generation and input leads to more robust interaction (Oviatt, 2002) and reduced cognitive load (Oviatt et al., 2004). In this paper we investigate the use of machine learning (ML) to explore human multimodal clarification strategies and the use of those strategies to decide, based on the current dialogue context, when a dialogue system’s clarification request (CR) should be generated in a multimodal manner. In previous work (Rieser and Moore, 2005) we showed that for spoken CRs in humanhuman communication people follow a contextdependent clarification strategy which systematically varies across domains (and even across Germanic languages). In this paper we investigate whether there exists a context-dependent “intuitive” human strategy for multimodal CRs as well. To test this hypothesis we gathered data in a Wizard-of-Oz (WOZ) study, where different wizards could decide when to show a screen output. From this data we build prediction models, using supervised learning techniques together with feature engineering methods, that may explain </context>
</contexts>
<marker>Rieser, Moore, 2005</marker>
<rawString>Verena Rieser and Johanna Moore. 2005. Implications for Generating Clarification Requests in Taskoriented Dialogues. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Ivana Kruijff-Korbayov´a</author>
<author>Oliver Lemon</author>
</authors>
<title>A corpus collection and annotation framework for learning multimodal clarification strategies.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<marker>Rieser, Kruijff-Korbayov´a, Lemon, 2005</marker>
<rawString>Verena Rieser, Ivana Kruijff-Korbayov´a, and Oliver Lemon. 2005. A corpus collection and annotation framework for learning multimodal clarification strategies. In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Pierre Dillenbourg</author>
</authors>
<title>Miscommunication in multi-modal collaboration.</title>
<date>1996</date>
<booktitle>In Proceedings of the Workshop on Detecting, Repairing, and Preventing Human-Machine Miscommunication. AAAI-96.</booktitle>
<contexts>
<context position="5826" citStr="Traum and Dillenbourg, 1996" startWordPosition="917" endWordPosition="920"> number of words, simulating understanding problems at the acoustic level. This (sometimes) corrupted transcription was then presented to the human wizard. Note that this environment introduces uncertainty on several levels, for example multiple matches in the database, lexical ambiguities, and errors on the acoustic level, as described in (Rieser et al., 2005). Whenever the wizard produced a CR, the experiment leader invoked a questionnaire window on a GUI, where the wizard classified their CR according to the primary source of the understanding problem, mapping to the categories defined by (Traum and Dillenbourg, 1996). 2.1 The Data The corpus gathered with this setup comprises 70 dialogues, 1772 turns and 17076 words. Example 1 shows a typical multimodal clarification sub-dialogue, 1 concerning an uncertain reference (note that “Venus” is an album name, song title, and an artist name), where the wizard selects a screen output while asking a CR. (1) User: Please play “Venus”. Wizard: Does this list contain the song? [shows list with 20 DB matches] User: Yes. It’s number 4. [clicks on item 4] For each session we gathered logging information which consists of e.g., the transcriptions of the spoken utterances,</context>
</contexts>
<marker>Traum, Dillenbourg, 1996</marker>
<rawString>David Traum and Pierre Dillenbourg. 1996. Miscommunication in multi-modal collaboration. In Proceedings of the Workshop on Detecting, Repairing, and Preventing Human-Machine Miscommunication. AAAI-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques (Second Edition).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14467" citStr="Witten and Frank, 2005" startWordPosition="2313" endWordPosition="2316">ering our limited data set of 152 training instances we run the risk of severe data sparsity. Furthermore we want to explore which features of this potential feature space influenced the wizards’ multimodal strategy. In the next two sections we describe feature engineering techniques, namely discretising methods for dimensionality reduction and feature selection methods, which help to reduce the feature space to a subset which is most predictive of multimodal clarification. For our experiments we use implementations of discretisation and feature selection methods provided by the WEKA toolkit (Witten and Frank, 2005). 4 Feature Engineering 4.1 Discretising Numeric Features Global discretisation methods divide all continuous features into a smaller number of distinct ranges before learning starts. This has two advantages concerning the quality of our data for ML. First, discretisation methods take feature distributions into account and help to avoid sparse data. Second, most of our features are highly positively skewed. Some ML methods (such as the standard extension of the Naive Bayes classifier to handle numeric features) assume that numeric attributes have a normal distribution. We use Proportional k-In</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques (Second Edition). Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>