<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.537034">
SCFG Decoding Without Binarization
</title>
<author confidence="0.786663">
Mark Hopkins and Greg Langmead
</author>
<affiliation confidence="0.770953">
SDL Language Weaver, Inc.
</affiliation>
<address confidence="0.8766725">
6060 Center Drive, Suite 150
Los Angeles, CA 90045
</address>
<email confidence="0.999484">
{mhopkins,glangmead}@languageweaver.com
</email>
<sectionHeader confidence="0.994805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999320090909091">
Conventional wisdom dictates that syn-
chronous context-free grammars (SCFGs)
must be converted to Chomsky Normal Form
(CNF) to ensure cubic time decoding. For ar-
bitrary SCFGs, this is typically accomplished
via the synchronous binarization technique of
(Zhang et al., 2006). A drawback to this ap-
proach is that it inflates the constant factors as-
sociated with decoding, and thus the practical
running time. (DeNero et al., 2009) tackle this
problem by defining a superset of CNF called
Lexical Normal Form (LNF), which also sup-
ports cubic time decoding under certain im-
plicit assumptions. In this paper, we make
these assumptions explicit, and in doing so,
show that LNF can be further expanded to
a broader class of grammars (called “scope-
3”) that also supports cubic-time decoding.
By simply pruning non-scope-3 rules from a
GHKM-extracted grammar, we obtain better
translation performance than synchronous bi-
narization.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998197625">
At the heart of bottom-up chart parsing (Younger,
1967) is the following combinatorial problem. We
have a context-free grammar (CFG) rule (for in-
stance, S → NP VP PP) and an input sentence of
length n (for instance, “on the fast jet ski of mr
smith”). During chart parsing, we need to apply the
rule to all relevant subspans of the input sentence.
See Figure 1. For this particular rule, there are (n+1)
</bodyText>
<page confidence="0.697444">
4
</page>
<bodyText confidence="0.992466333333333">
application contexts, i.e. ways to choose the sub-
spans. Since the asymptotic running time of chart
parsing is at least linear in this quantity, it will take
</bodyText>
<figure confidence="0.48121825">
on the fast jet ski of mr smith
NP VP PP
NP VP PP
...
NP VP PP
choice choice choice choice
point point point point
...
NP VP PP
NP VP PP
(are Figure 1: A demonstration of application contexts. There
n�1) application contexts for the CFG rule “S → NP
4
VP PP”, where n is the length of the input sentence.
at least O((n+1)) = O(n4) time if we include this
4
</figure>
<bodyText confidence="0.9938904375">
rule in our grammar.
Fortunately, we can take advantage of the fact that
any CFG has an equivalent representation in Chom-
sky Normal Form (CNF). In CNF, all rules have
the form X → Y Z or X → x, where x is a termi-
nal and X, Y, Z are nonterminals. If a rule has the
form X → Y Z, then there are only (n3 1) applica-
tion contexts, thus the running time of chart parsing
is O((n31)) = O(n 3) when applied to CNF gram-
mars.
A disadvantage to CNF conversion is that it in-
creases both the overall number of rules and the
overall number of nonterminals. This inflation of
the “grammar constant” does not affect the asymp-
totic runtime, but can have a significant impact on
the performance in practice. For this reason, (DeN-
</bodyText>
<page confidence="0.982962">
646
</page>
<note confidence="0.891832">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 646–655,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.592642166666667">
on the fast jet ski of mr smith
the JJ NPB of NNP
the JJ NPB of NNP
the JJ NPB of NNP
the JJ NPB of NNP
choice point choice point
</table>
<figureCaption confidence="0.949197">
Figure 2: A demonstration of application contexts for
rules with lexical anchors. There are O(n) application
</figureCaption>
<bodyText confidence="0.989818925925926">
contexts for CFG rule “S —* the NPB of NNP”, and
O(n&apos;) application contexts for CFG rule “S —* the JJ
NPB of NNP”, if we assume that the input sentence has
length n and contains no repeated words.
ero et al., 2009) provide a relaxation of CNF called
Lexical Normal Form (LNF). LNF is a superclass of
CNF that also allows rules whose right-hand sides
have no consecutive nonterminals. The intuition is
that the terminals provide anchors that limit the ap-
plicability of a given rule. For instance, consider the
rule NP —* the NPB of NNP. See Figure 2. Because
the terminals constrain our choices, there are only
two different application contexts. The implicit as-
sumption is that input sentences will not repeat the
same word more than a small constant number of
times. If we make the explicit assumption that all
words of an input sentence are unique, then there
are O(n2) application contexts for a “no consecu-
tive nonterminals” rule. Thus under this assumption,
the running time of chart parsing is still O(n3) when
applied to LNF grammars.
But once we make this assumption explicit, it be-
comes clear that we can go even further than LNF
and still maintain the cubic bound on the runtime.
Consider the rule NP —* the JJ NPB of NNP. This
rule is not LNF, but there are still only O(n2) ap-
plication contexts, due to the anchoring effect of the
terminals. In general, for a rule of the form X —* &apos;y,
there are at most O(np) application contexts, where
p is the number of consecutive nonterminal pairs in
the string X ·&apos;y· X (where X is an arbitrary nontermi-
nal). We refer to p as the scope of a rule. Thus chart
parsing runs in time O(nsc WG)), where scope(G)
is the maximum scope of any of the rules in CFG G.
Specifically, any scope-3 grammar can be decoded
in cubic time.
Like (DeNero et al., 2009), the target of our in-
terest is synchronous context-free grammar (SCFG)
decoding with rules extracted using the GHKM al-
gorithm (Galley et al., 2004). In practice, it turns out
that only a small percentage of the lexical rules in
our system have scope greater than 3. By simply re-
moving these rules from the grammar, we can main-
tain the cubic running time of chart parsing without
any kind of binarization. This has three advantages.
First, we do not inflate the grammar constant. Sec-
ond, unlike (DeNero et al., 2009), we maintain the
synchronous property of the grammar, and thus can
integrate language model scoring into chart parsing.
Finally, a system without binarized rules is consid-
erably simpler to build and maintain. We show that
this approach gives us better practical performance
than a mature system that binarizes using the tech-
nique of (Zhang et al., 2006).
</bodyText>
<sectionHeader confidence="0.98554" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.987858421052632">
Assume we have a global vocabulary of symbols,
containing the reserved substitution symbol Q. De-
fine a sentence as a sequence of symbols. We will
typically use space-delimited quotations to represent
example sentences, e.g. “the fast jet ski” rather than
(the, fast, jet, ski). We will use the dot operator to
represent the concatenation of sentences, e.g. “the
fast” · “jet ski” = “the fast jet ski”.
Define the rank of a sentence as the count
of its Q symbols. We will use the no-
tation SUB(s, s1, ..., sk) to denote the substitu-
tion of k sentences s1, ..., sk into a k-rank sen-
tence s. For instance, if s = “the Q Q of
Q”, then SUB(s, “fast”, “jet ski”, “mr smith”) =
“the fast jet ski of mr smith”.
To refer to a subsentence, define a span as a pair
[a, b] of nonnegative integers such that a &lt; b. For
a sentence s = (s1, s2, ..., sn) and a span [a, b] such
that b &lt; n, define s [a,b] = (sa+1, ..., sb).
</bodyText>
<figure confidence="0.67382275">
on the fast jet ski of mr smith
the NPB of NNP
the NPB of NNP
choice point
</figure>
<page confidence="0.948633">
647
</page>
<equation confidence="0.928245857142857">
C1
JJ -&gt; fast NN -&gt; jet ski NNP -&gt; mr smith
C2
NP -&gt; the JJ NN of NNP
PP -&gt; on NP
NP -&gt; the JJ NN of NNP
PP -&gt; on NP
PP -&gt; &lt; on NP1, sur NP1&gt;
NP -&gt; &lt; the JJ1 NN2 of NNP3, le NN2 JJ1 de NNP3 &gt;
JJ -&gt; &lt; fast, vite &gt; NN -&gt; &lt; jet ski, jet ski &gt; NNP -&gt; &lt; mr smith, m smith &gt;
JJ -&gt; fast NN -&gt; jet ski NNP -&gt; mr smith
C3
C4
C5
</equation>
<figureCaption confidence="0.835382">
Figure 3: An example CFG derivation (above) and an ex-
ample SCFG derivation (below). Both derive the sen-
</figureCaption>
<bodyText confidence="0.7186515">
tence SUB(“on ♦”, SUB( “the ♦ ♦ of ♦”, “fast”, “jet
ski”, “mr smith”) ) = “on the fast jet ski of mr smith”.
The SCFG derivation simultaneously derives the auxil-
iary sentence “sur le jet ski vite de m smith”.
</bodyText>
<sectionHeader confidence="0.989694" genericHeader="method">
3 Minimum Derivation Cost
</sectionHeader>
<bodyText confidence="0.99975124">
Chart parsing solves a problem which we will re-
fer to as Minimum Derivation Cost. Because we
want our results to be applicable to both CFG decod-
ing and SCFG decoding with an integrated language
model, we will provide a somewhat more abstract
formulation of chart parsing than usual.
In Figure 3, we show an example of a CFG deriva-
tion. A derivation is a tree of CFG rules, constructed
so that the preconditions (the RHS nonterminals) of
any rule match the postconditions (the LHS nonter-
minal) of its child rules. The purpose of a derivation
is to derive a sentence, which is obtained through
recursive substitution. In the example, we substitute
“fast”, “jet ski”, and “mr smith” into the lexical pat-
tern “the ♦ ♦ of ♦” to obtain “the fast jet ski of mr
smith”. Then we substitute this result into the lexi-
cal pattern “on ♦” to obtain “on the fast jet ski of mr
smith”.
The cost of a derivation is simply the sum of the
base costs of its rules. Thus the cost of the CFG
derivation in Figure 3 is C1 + C2 + C3 + C4 + C5,
where C1 is the base cost of rule “PP → on NP”, etc.
Notice that this cost can be distributed locally to the
nodes of the derivation (Figure 4).
An SCFG derivation is similar to a CFG deriva-
</bodyText>
<figureCaption confidence="0.97158075">
Figure 4: The cost of the CFG derivation in Figure 3 is
C1 + C2 + C3 + C4 + C5, where C1 is the base cost
of rule “PP → on NP”, etc. Notice that this cost can be
distributed locally to the nodes of the derivation.
</figureCaption>
<bodyText confidence="0.999988208333333">
tion, except that it simultaneously derives two sen-
tences. For instance, the SCFG derivation in Fig-
ure 3 derives the sentence pair ( “on the fast jet ski
of mr smith”, “sur le jet ski vite de m smith” ). In
machine translation, often we want the cost of the
SCFG derivation to include a language model cost
for this second sentence. For example, the cost of the
SCFG derivation in Figure 3 might be C1+C2+C3+
C4+C5+LM(sur le)+LM(le jet)+LM(jet ski)+
LM(ski de) + LM(de m) + LM(m smith), where
LM is the negative log of a 2-gram language model.
This new cost function can also be distributed lo-
cally to the nodes of the derivation, as shown in Fig-
ure 5. However, in order to perform the local com-
putations, we need to pass information (in this case,
the LM boundary words) up the tree. We refer to
this extra information as carries. Formally, define a
carry as a sentence of rank 0.
In order to provide a chart parsing formulation
that applies to both CFG decoding and SCFG de-
coding with an integrated language model, we need
abstract definitions of rule and derivation that cap-
ture the above concepts of pattern, postcondition,
preconditions, cost, and carries.
</bodyText>
<subsectionHeader confidence="0.997249">
3.1 Rules
</subsectionHeader>
<bodyText confidence="0.9813265">
Define a rule as a tuple (k, s*, X, 7r, F, c), where k is
a nonnegative integer called the rank, s* is a rank-k
</bodyText>
<page confidence="0.970398">
648
</page>
<figure confidence="0.939349631578947">
C2 + LM(le jet)
+ LM(ski vite)
+ LM(vite de)
+ LM(de m)
NP -&gt; &lt; the JJ1 NN2 of NNP3,
le NN2 JJ1 de NNP3 &gt;
jet * ski
vite * vite
m * smith
C1 + LM(sur le)
PP -&gt; &lt; on NP1,
sur NP1&gt;
le * smith
JJ -&gt; &lt; fast,
vite &gt;
C3
NN -&gt; &lt; jet ski,
jet ski &gt;
C4 + LM(jet ski)
NNP -&gt; &lt; mr smith,
m smith &gt;
C5 + LM(m smith)
postcondition preconditions postcondition preconditions rank
rank = 3
NP -&gt; the JJ NN of NNP
NP -&gt; &lt; the JJ1 NN2 of NNP3,
le NN2 JJ1 de NNP3 &gt;
pattern the ◊ ◊ of ◊
carry Γ( “” , ”” , ”” ) = “”
function
cost c( “” , ”” , ”” ) = C
function
pattern the ◊ ◊ of ◊
carry Γ( “u*v” , ”w*x” , ”y*z” )
function = “le * z”
cost c( “u*v” , ”w*x” , ”y*z” )
function = C + LM(w|le) + LM(u|x)
+ LM(de|v) + LM(y|de)
</figure>
<figureCaption confidence="0.9930956">
Figure 5: The cost of the SCFG derivation in Figure 3
(with an integrated language model score) can also be dis-
tributed to the nodes of the derivation, but to perform the
local computations, information must be passed up the
tree. We refer to this extra information as a carry.
</figureCaption>
<bodyText confidence="0.9999906">
sentence called the pattern 1, X is a symbol called
the postcondition, 7r is a k-length sentence called the
preconditions, F is a function (called the carry func-
tion) that maps a k-length list of carries to a carry,
and c is a function (called the cost function) that
maps a k-length list of carries to a real number. Fig-
ure 6 shows a CFG and an SCFG rule, deconstructed
according to this definition. 2 Note that the CFG rule
has trivial cost and carry functions that map every-
thing to a constant. We refer to such rules as simple.
We will use post(r) to refer to the postcondition
of rule r, and pre(r, i) to refer to the ith precondition
of rule r.
Finally, define a grammar as a finite set of rules.
A grammar is simple if all its rules are simple.
</bodyText>
<subsectionHeader confidence="0.999199">
3.2 Derivations
</subsectionHeader>
<bodyText confidence="0.848958363636364">
For a grammar R, define deriv(R) as the smallest set
that contains every tuple (r, S1, ..., Sk) satisfying the
following conditions:
1For simplicity, we also impose the condition that “0” is not
a valid pattern. This is tantamount to disallowing unary rules.
2One possible point of confusion is why the pattern of the
SCFG rule refers only to the primary sentence, and not the aux-
iliary sentence. To reconstruct the auxiliary sentence from an
SCFG derivation in practice, one would need to augment the
abstract definition of rule with an auxiliary pattern. However
this is not required for our theoretical results.
</bodyText>
<figureCaption confidence="0.882577666666667">
Figure 6: Deconstruction of a CFG rule (left) and SCFG
rule (right) according to the definition of rule in Sec-
tion 3.1. The carry function of the SCFG rule computes
</figureCaption>
<bodyText confidence="0.839635666666667">
boundary words for a 2-gram language model. In the cost
functions, C is a real number and LM returns the negative
log of a language model query.
</bodyText>
<listItem confidence="0.99047175">
• r E R is a k-rank rule
• Si E deriv(R) for all 1 &lt; i &lt; k
• pre(r, i) = post(ri) for all 1 &lt; i &lt; k, where ri
is the first element of tuple Si.
</listItem>
<bodyText confidence="0.969523">
An R–derivation is an element of deriv(R). Con-
sider a derivation S = (r, S1, ..., Sk), where rule
r = (k, s*, X, 7r, F, c). Define the following prop-
erties:
</bodyText>
<equation confidence="0.997494333333333">
post(S) = post(r)
sent(S) = SUB(s*, sent(S1), ..., sent(Sk))
carry(S) = F(carry(S1), ..., carry(Sk))
k
cost(S) = c(carry(S1), ..., carry(Sk)) + cost(Sj)
j=1
</equation>
<bodyText confidence="0.99342725">
In words, we say that derivation S derives sen-
tence sent(S). If for some span Q of a particular sen-
tence s, it holds that sent(S) = sQ, then we will say
that S is a derivation over span Q.
</bodyText>
<subsectionHeader confidence="0.999142">
3.3 Problem Statement
</subsectionHeader>
<bodyText confidence="0.9846435">
The Minimum Derivation Cost problem is the fol-
lowing. Given a set R of rules and an input sentence
</bodyText>
<page confidence="0.986205">
649
</page>
<figure confidence="0.830989">
0 1 2 3 4 5 6 7 8
</figure>
<figureCaption confidence="0.848083">
on the fast jet ski of mr smith
the 0 0 of 0
Figure 7: An application context for the pattern “the ♦ ♦
of ♦” and the sentence “on the fast jet ski of mr smith”.
</figureCaption>
<bodyText confidence="0.883122">
s, find the minimum cost of any R–derivation that
derives s. In other words, compute:
</bodyText>
<equation confidence="0.667876">
MinDCost(R, s) ,
</equation>
<sectionHeader confidence="0.993803" genericHeader="method">
4 Application Contexts
</sectionHeader>
<bodyText confidence="0.957122695652174">
Chart parsing solves Minimum Derivation Cost via
dynamic programming. It works by building deriva-
tions over increasingly larger spans of the input sen-
tence s. Consider just one of these spans Q. How do
we build a derivation over that span?
Recall that a derivation takes the form
(r, S1, ..., Sk). Given the rule r and its pattern
s∗, we need to choose the subderivations Si such
that SUB(s∗, sent(S1), ...,sent(Sk)) = sσ. To do
so, we must match the pattern to the span, so that
we know which subspans we need to build the
subderivations over. Figure 7 shows a matching
of the pattern “the ♦ ♦ of ♦” to span [1, 8] of the
sentence “on the fast jet ski of mr smith”. It tells
us that we can build a derivation over span [1, 8] by
choosing this rule and subderivations over subspans
[2, 3], [3, 5], and [6, 8].
We refer to these matchings as application con-
texts. Formally, given two sentences s∗ and s
of respective lengths m and n, define an (s∗, s)–
context as an monotonically increasing sequence
(x0, x1, ..., xm) of integers between 0 and n such
that for all i:
</bodyText>
<equation confidence="0.979839">
s∗−
[i =� ♦ implies that s∗[i−1,i] = s[xi−1,xi]
</equation>
<bodyText confidence="0.9393014">
The context shown in Figure 7 is (1, 2, 3, 5, 6, 8).
Use cxt(s∗, s) to denote the set of all (s∗, s)–
contexts.
An (s∗, s)–context x = (x0, x1, ..., xm) has the
following properties:
</bodyText>
<equation confidence="0.9976435">
span(x; s∗, s) = [x0, xm]
subspans(x; s∗, s) = ([x0, x1],..., [xm−1, xm])
</equation>
<bodyText confidence="0.9796195">
Moreover, define varspans(x; s∗, s) as the sub-
sequence of subspans(x; s∗, s) including only
[xi−1, xi] such that s∗[i−1,i] = ♦. For the context
x shown in Figure 7:
</bodyText>
<equation confidence="0.999836">
span(x; s∗, s) = [1, 8]
subspans(x; s∗, s) = ([1, 2], [2, 3], [3, 5], [5, 6], [6,8])
varspans(x; s∗, s) = ([2, 3], [3, 5], [6, 8])
</equation>
<bodyText confidence="0.99398775">
An application context x E cxt(s∗, s) tells us that
we can build a derivation over span(x) by choosing
a rule with pattern s∗ and subderivations over each
span in varspans(x; s∗, s).
</bodyText>
<sectionHeader confidence="0.97701" genericHeader="method">
5 Chart Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.990635833333333">
We are now ready to describe the chart parsing al-
gorithm. Consider a span Q of our input sentence
s and assume that we have computed and stored all
derivations over any subspan of Q. A naive way to
compute the minimum cost derivation over span Q is
to consider every possible derivation:
</bodyText>
<listItem confidence="0.986383833333333">
1. Choose a rule r = (k, s∗, X, 7r, F, c).
2. Choose an application context x E cxt(s∗, s)
such that span(x; s∗, s) = Q.
3. For each subspan Qi E varspans(x; s∗, s),
choose a subderivation Si such that post(Si) =
pre(r, i).
</listItem>
<bodyText confidence="0.981374">
The key observation here is the following. In or-
der to score such a derivation, we did not actually
need to know each subderivation in its entirety. We
merely needed to know the following information
about it: (a) the subspan that it derives, (b) its post-
condition, (c) its carry.
</bodyText>
<figure confidence="0.7441985">
min cost(S)
δ∈deriv(R)|sent(δ)=s
</figure>
<page confidence="0.9877">
650
</page>
<bodyText confidence="0.997634428571428">
Chart parsing takes advantage of the above obser-
vation to avoid building all possible derivations. In-
stead it groups together derivations that share a com-
mon subspan, postcondition, and carry, and records
only the minimum cost for each equivalence class.
It records this cost in an associative map referred to
as the chart.
Specifically, assume that we have computed and
stored the minimum cost of every derivation class
(σ&apos;, X&apos;, γ&apos;), where X&apos; is a postcondition, γ&apos; is a
carry, and σ&apos; is a proper subspan of σ. Chart pars-
ing computes the minimum cost of every derivation
class (σ, X, γ) by adapting the above naive method
as follows:
</bodyText>
<listItem confidence="0.989749857142857">
1. Choose a rule r = (k, s*, X, π, Γ, c).
2. Choose an application context x E cxt(s*, s)
such that span(x; s*, s) = σ.
3. For each subspan σi E varspans(x; s*, s),
choose a derivation class (σi, Xi, γi) from the
chart such that Xi = pre(r, i).
4. Update3 the cost of derivation class
</listItem>
<equation confidence="0.888921">
(σ, p ost(r), Γ(γ1, ..., γk)) with:
k
c(γ1, ..., γk) + chart[σi, Xi, γi]
i=1
</equation>
<bodyText confidence="0.999878666666667">
where chart[σi,Xi,γi] refers to the stored
cost of derivation class (σi, Xi, γi).
By iteratively applying the above method to all sub-
spans of size 1, 2, etc., chart parsing provides an
efficient solution for the Minimum Derivation Cost
problem.
</bodyText>
<sectionHeader confidence="0.981241" genericHeader="method">
6 Runtime Analysis
</sectionHeader>
<bodyText confidence="0.9998492">
At the heart of chart parsing is a single operation:
the updating of a value in the chart. The running
time is linear in the number of these chart updates.
4 The typical analysis counts the number of chart
updates per span. Here we provide an alternative
</bodyText>
<footnote confidence="0.727819333333333">
3Here, update means “replace the cost associated with the
class if the new cost is lower.”
4This assumes that you can linearly enumerate the relevant
updates. One convenient way to do this is to frame the enumer-
ation problem as a search space, e.g. (Hopkins and Langmead,
2009)
</footnote>
<bodyText confidence="0.977768322580645">
analysis that counts the number of chart updates per
rule. This provides us with a finer bound with prac-
tical implications.
Let r be a rule with rank k and pattern s*. Con-
sider the chart updates involving rule r. There is
(potentially) an update for every choice of (a) span,
(b) application context, and (c) list of k derivation
classes. If we let (t be the set of possible carries,
then this means there are at most |cxt(s*, s) |· |(t|k
updates involving rule r. 5 If we are doing beam de-
coding (i.e. after processing a span, the chart keeps
only the B items of lowest cost), then there are at
most |cxt(s*, s) |· Bk updates.
We can simplify the above by providing an upper
bound for |cxt(s*, s)|. Define an ambiguity as the
sentence “♦ ♦”, and define scope(s*) as the number
of ambiguities in the sentence “♦” ·s*· “♦”. The
following bound holds:
Lemma 1. Assume that a zero-rank sentence s does
not contain the same symbol more than once. Then
|cxt(s*,s) |&lt;—
Proof. Suppose s* and s have respective lengths m
and n. Consider (x0, x1, ..., xm) E cxt(s*, s). Let
I be the set of integers i between 1 and m such that
s*i =� ♦ and let I+ be the set of integers i between
0 and m − 1 such that s*i+1 =� ♦. If i E I, then we
know the value of xi, namely it is the unique integer
j such that sj = s*i. Similarly, if i E I+, then the
value of xi must be the unique integer j such that
sj = s*i+1. Thus the only nondetermined elements
of context xi are those for which i E� I U I+. Hence
</bodyText>
<equation confidence="0.907342">
|cxt(s*, s) ||s|�0,1,...,m��I�I� = |s|scope(s*).
</equation>
<bodyText confidence="0.998349333333333">
Hence, under the assumption that the input sen-
tence s does not contain the same symbol more than
once, then there are at most |s|scope(s*) · |(t|k chart
updates involving a rule with pattern s*.
For a rule r with pattern s*, define scope(r) =
scope(s*). For a grammar R, define scope(R) =
maxrER scope(r) and rank(R) = maxrER rank(r).
Given a grammar R and an input sentence s,
the above lemma tells us that chart parsing makes
</bodyText>
<footnote confidence="0.996796">
5For instance, in SCFG decoding with an integrated j-gram
language model, a carry consists of 2(j − 1) boundary words.
Generally it is assumed that there are O(n) possible choices for
a boundary word, and hence O(n2U−1)) possible carries.
</footnote>
<equation confidence="0.195556">
|s|scope(s*).
</equation>
<page confidence="0.991261">
651
</page>
<bodyText confidence="0.982284">
O(|s|scope(R) · g|rank(R)) chart updates. If we re-
strict ourselves to beam /search, than chart parsing
makes O(|s|scope(R)) chart updates. 6
</bodyText>
<subsectionHeader confidence="0.99796">
6.1 On the Uniqueness Assumption
</subsectionHeader>
<bodyText confidence="0.999968090909091">
In practice, it will not be true that each input sen-
tence contains only unique symbols, but it is not too
far removed from the practical reality of many use
cases, for which relatively few symbols repeat them-
selves in a given sentence. The above lemma can
also be relaxed to assume only that there is a con-
stant upper bound on the multiplicity of a symbol
in the input sentence. This does not affect the O-
bound on the number of chart updates, as long as we
further assume a constant limit on the length of rule
patterns.
</bodyText>
<sectionHeader confidence="0.957969" genericHeader="method">
7 Scope Reduction
</sectionHeader>
<bodyText confidence="0.99041675">
From this point of view, CNF binarization can be
viewed as a specific example of scope reduction.
Suppose we have a grammar R of scope p. See Fig-
ure 8. If we can find a grammar R� of scope p� &lt; p
which is “similar” to grammar R, then we can de-
code in O(nP) rather than O(nP) time.
We can frame the problem by assuming the fol-
lowing parameters:
</bodyText>
<listItem confidence="0.987663666666667">
• a grammar R
• a desired scope p
• a loss function A that returns a (non-negative
real-valued) score for any two grammars R and
R; if A(R, R) = 0, then the grammars are con-
sidered to be equivalent
</listItem>
<bodyText confidence="0.964526666666667">
A scope reduction method with loss λ finds a gram-
mar R� such that scope( R) &lt; p and A(R, R) = λ.
A scope reduction method is lossless when its loss
is 0.
In the following sections, we will use the loss
function:
A(R, R) = |MinDCost(R, s) − MinDCost( R, s)|
where s is a fixed input sentence. Observe that if
A(R, R) = 0, then the solution to the Minimum
</bodyText>
<footnote confidence="0.959716">
6Assuming rank(R) is bounded by a constant.
</footnote>
<figureCaption confidence="0.986653">
Figure 8: The “scope reduction” problem. Given a gram-
mar of large scope, find a similar grammar of reduced
scope.
</figureCaption>
<figure confidence="0.507891">
Derivation Cost problem is the same for both R and
R. 7
</figure>
<subsectionHeader confidence="0.982157">
7.1 CNF Binarization
</subsectionHeader>
<bodyText confidence="0.983319181818182">
A rule r is CNF if its pattern is “0 Q” or “x”, where x
is any non-substitution symbol. A grammar is CNF
if all of its rules are CNF. Note that the maximum
scope of a CNF grammar is 3.
CNF binarization is a deterministic process that
maps a simple grammar to a CNF grammar. Since
binarization takes subcubic time, we can decode
with any grammar R in O(n3) time by converting
R to CNF grammar R, and then decoding with R.
This is a lossless scope reduction method.
What if grammar R is not simple? For SCFG
grammars, (Zhang et al., 2006) provide a scope
reduction method called synchronous binarization
with quantifiable loss. Synchronous binarization se-
lects a “binarizable” subgrammar R&apos; of grammar R,
and then converts R&apos; into a CNF grammar R. The
cost and carry functions of these new rules are con-
structed such that the conversion from R&apos; to R is
a lossless scope reduction. Thus the total loss of
the method is |MinDCost(R, s)−MinDCost(R&apos;, s)|.
Fortunately, they find in practice that R&apos; usually con-
tains the great majority of the rules of R, thus they
</bodyText>
<footnote confidence="0.993952666666667">
7Note that if we want the actual derivation and not just its
cost, then we need to specify a more finely grained loss func-
tion. This is omitted for clarity and left as an exercise.
</footnote>
<figure confidence="0.986466682926829">
CNF LNF
Scope 3
All Grammars
652
a b 0 0 c
0 a 0 0 b
a 0 b 0 0
0 0 a 0 b
0 0 0 a b
a 0 0 0 b
a 0 0 b 0 c
a 0 0 0 0 b
a 0 0 b c 0 0 d
a 0 0 0 0 b 0 c 0 d
a 0 0 b 0 0 c 0 0 d
a 0 0
0 0 a
a 0 0 b
0 a 0 0
0 0 a 0
0 0 0 a
a 0 0 0
a b 0 0
0 0 a b
a 0 0 b 0
a 0 0 b c
0 1 2 3 4 5 6 7 8
P
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
% of rules with scope &lt;= P
AE Lexical CE Lexical AE Nonlexical CE Nonlexical
</figure>
<figureCaption confidence="0.995363">
Figure 9: A selection of rule patterns that are scope &lt; 3
but not LNF or CNF.
</figureCaption>
<bodyText confidence="0.993117333333334">
assert that this loss is negligable.
A drawback of their technique is that the resulting
CNF grammar contains many more rules and post-
conditions than the original grammar. These con-
stant factors do not impact asymptotic performance,
but do impact practical performance.
</bodyText>
<subsectionHeader confidence="0.993653">
7.2 Lexical Normal Form
</subsectionHeader>
<bodyText confidence="0.95315">
Concerned about this inflation of the grammar con-
stant, (DeNero et al., 2009) consider a superset of
CNF called Lexical Normal Form (LNF). A rule is
LNF if its pattern does not contain an ambiguity as
a proper subsentence (recall that an ambiguity was
defined to be the sentence “0 0”). Like CNF, the
maximum scope of an LNF grammar is 3. In the
worst case, the pattern s* is “0 0”, in which case
there are three ambiguities in the sentence “0” ·s*·
“0” .
(DeNero et al., 2009) provide a lossless scope
reduction method that maps a simple grammar to
an LNF grammar, thus enabling cubic-time decod-
ing. Their principal objective is to provide a scope
reduction method for SCFG that introduces fewer
postconditions than (Zhang et al., 2006). However
unlike (Zhang et al., 2006), their method only ad-
dresses simple grammars. Thus they cannot inte-
grate LM scoring into their decoding, requiring them
to rescore the decoder output with a variant of cube
growing (Huang and Chiang, 2007).
Figure 10: Breakdown of rules by scope (average per sen-
tence in our test sets). In practice, most of the lexical rules
applicable to a given sentence (95% for Arabic-English
and 85% for Chinese-English) are scope 3 or less.
</bodyText>
<subsectionHeader confidence="0.982981">
7.3 Scope Pruning
</subsectionHeader>
<bodyText confidence="0.999872851851852">
To exercise the power of the ideas presented in this
paper, we experimented with a third (and very easy)
scope reduction method called scope pruning. If we
consider the entire space of scope-3 grammars, we
see that it contains a much richer set of rules than
those permitted by CNF or LNF. See Figure 9 for
examples. Scope pruning is a lossy scope reduc-
tion method that simply takes an arbitrary grammar
and prunes all rules with scope greater than 3. By
not modifying any rules, we preserve their cost and
carry functions (enabling integrated LM decoding),
without increasing the grammar constant. The prac-
tical question is: how many rules are we typically
pruning from the original grammar?
We experimented with two pretrained syntax-
based machine translation systems with rules ex-
tracted via the GHKM algorithm (Galley et al.,
2004). The first was an Arabic-English system, with
rules extracted from 200 million words of parallel
data from the NIST 2008 data collection, and with
a 4-gram language model trained on 1 billion words
of monolingual English data from the LDC Giga-
word corpus. We evaluated this system’s perfor-
mance on the NIST 2008 test corpus, which con-
sists of 1357 Arabic sentences from a mixture of
newswire and web domains, with four English refer-
ence translations. The second system was a Chinese-
</bodyText>
<page confidence="0.996423">
653
</page>
<figure confidence="0.994398458333333">
Arabic-English Chinese-English
40
37
36
39
35
38
34
33
37
BLEU-4
BLEU-4
32
36
31
30
35
29
34
28
33
27
0 2000 4000 6000 8000 0 2000 4000 6000 8000
Words per minute Words per minute
</figure>
<figureCaption confidence="0.97931">
Figure 11: Speed-quality tradeoff curves comparing the baseline scope reduction method of synchronous binarization
(dark gray diamonds) with scope-3 pruning (light gray squares).
</figureCaption>
<bodyText confidence="0.998615941176471">
English system, with rules extracted from 16 million
words of parallel data from the mainland-news do-
main of the LDC corpora, and with a 4-gram lan-
guage model trained on monolingual English data
from the AFP and Xinhua portions of the LDC Gi-
gaword corpus. We evaluated this system’s perfor-
mance on the NIST 2003 test corpus, which con-
sists of 919 Chinese sentences, with four English
reference translations. For both systems, we report
BLEU scores (Papineni et al., 2002) on untokenized,
recapitalized output.
In practice, how many rules have scope greater
than 3? To answer this question, it is useful to dis-
tinguish between lexical rules (i.e. rules whose pat-
terns contain at least one non-substitution symbol)
and non-lexical rules. Only a subset of lexical rules
are potentially applicable to a given input sentence.
Figure 10 shows the scope profile of these applicable
rules (averaged over all sentences in our test sets).
Most of the lexical rules applicable to a given sen-
tence (95% for Arabic-English, 85% for Chinese-
English) are scope 3 or less. 8 Note, however, that
scope pruning also prunes a large percentage of non-
lexical rules.
Figure 11 compares scope pruning with the base-
line technique of synchronous binarization. To gen-
erate these speed-quality tradeoff curves, we de-
coded the test sets with 380 different beam settings.
We then plotted the hull of these 380 points, by elim-
inating any points that were dominated by another
(i.e. had better speed and quality). We found that
this simple approach to scope reduction produced
a better speed-quality tradeoff than the much more
complex synchronous binarization. 9
</bodyText>
<footnote confidence="0.999791666666667">
8For contrast, the corresponding numbers for LNF are 64%
and 53%, respectively.
9We also tried a hybrid approach in which we scope-pruned
</footnote>
<page confidence="0.998538">
654
</page>
<sectionHeader confidence="0.997817" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.974971">
In this paper, we made the following contributions:
</bodyText>
<listItem confidence="0.818328916666667">
• We provided an abstract formulation of chart
parsing that generalizes CFG decoding and
SCFG decoding with an integrated LM.
• We framed scope reduction as a first-class ab-
stract problem, and showed that CNF binariza-
tion and LNF binarization are two specific solu-
tions to this problem, each with their respective
advantages and disadvantages.
• We proposed a third scope reduction technique
called scope pruning, and we showed that it can
outperform synchronous CNF binarization for
particular use cases.
</listItem>
<bodyText confidence="0.9996801">
Moreover, this work gives formal expression to the
extraction heuristics of hierarchical phrase-based
translation (Chiang, 2007), whose directive not to
extract SCFG rules with adjacent nonterminals can
be viewed as a preemptive pruning of rules with
scope greater than 2 (more specifically, the prun-
ing of non-LNF lexical rules). In general, this work
provides a framework in which different approaches
to tractability-focused grammar construction can be
compared and discussed.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99014909375">
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Main Conference.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Mark Hopkins and Greg Langmead. 2009. Cube pruning
as heuristic search. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL.
the lexical rules and synchronously binarized the non-lexical
rules. This had a similar performance to scope-pruning all
rules. The opposite approach of scope-pruning the lexical rules
and synchronously binarizing the non-lexical rules had a similar
performance to synchronous binarization.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318.
Daniel Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189–208.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Confer-
ence, pages 256–263.
</reference>
<page confidence="0.99888">
655
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.710292">
<title confidence="0.999577">SCFG Decoding Without Binarization</title>
<author confidence="0.997301">Mark Hopkins</author>
<author confidence="0.997301">Greg</author>
<affiliation confidence="0.980059">SDL Language Weaver,</affiliation>
<address confidence="0.9773525">6060 Center Drive, Suite Los Angeles, CA</address>
<abstract confidence="0.989494347826087">Conventional wisdom dictates that synchronous context-free grammars (SCFGs) must be converted to Chomsky Normal Form (CNF) to ensure cubic time decoding. For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al., 2006). A drawback to this approach is that it inflates the constant factors associated with decoding, and thus the practical running time. (DeNero et al., 2009) tackle this problem by defining a superset of CNF called Lexical Normal Form (LNF), which also supports cubic time decoding under certain implicit assumptions. In this paper, we make these assumptions explicit, and in doing so, show that LNF can be further expanded to a broader class of grammars (called “scope- 3”) that also supports cubic-time decoding. By simply pruning non-scope-3 rules from a GHKM-extracted grammar, we obtain better translation performance than synchronous binarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="25545" citStr="Chiang, 2007" startWordPosition="4887" endWordPosition="4888">rn s* is “0 0”, in which case there are three ambiguities in the sentence “0” ·s*· “0” . (DeNero et al., 2009) provide a lossless scope reduction method that maps a simple grammar to an LNF grammar, thus enabling cubic-time decoding. Their principal objective is to provide a scope reduction method for SCFG that introduces fewer postconditions than (Zhang et al., 2006). However unlike (Zhang et al., 2006), their method only addresses simple grammars. Thus they cannot integrate LM scoring into their decoding, requiring them to rescore the decoder output with a variant of cube growing (Huang and Chiang, 2007). Figure 10: Breakdown of rules by scope (average per sentence in our test sets). In practice, most of the lexical rules applicable to a given sentence (95% for Arabic-English and 85% for Chinese-English) are scope 3 or less. 7.3 Scope Pruning To exercise the power of the ideas presented in this paper, we experimented with a third (and very easy) scope reduction method called scope pruning. If we consider the entire space of scope-3 grammars, we see that it contains a much richer set of rules than those permitted by CNF or LNF. See Figure 9 for examples. Scope pruning is a lossy scope reductio</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Mohit Bansal</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Efficient parsing for transducer grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference.</booktitle>
<contexts>
<context position="4999" citStr="DeNero et al., 2009" startWordPosition="884" endWordPosition="887">e runtime. Consider the rule NP —* the JJ NPB of NNP. This rule is not LNF, but there are still only O(n2) application contexts, due to the anchoring effect of the terminals. In general, for a rule of the form X —* &apos;y, there are at most O(np) application contexts, where p is the number of consecutive nonterminal pairs in the string X ·&apos;y· X (where X is an arbitrary nonterminal). We refer to p as the scope of a rule. Thus chart parsing runs in time O(nsc WG)), where scope(G) is the maximum scope of any of the rules in CFG G. Specifically, any scope-3 grammar can be decoded in cubic time. Like (DeNero et al., 2009), the target of our interest is synchronous context-free grammar (SCFG) decoding with rules extracted using the GHKM algorithm (Galley et al., 2004). In practice, it turns out that only a small percentage of the lexical rules in our system have scope greater than 3. By simply removing these rules from the grammar, we can maintain the cubic running time of chart parsing without any kind of binarization. This has three advantages. First, we do not inflate the grammar constant. Second, unlike (DeNero et al., 2009), we maintain the synchronous property of the grammar, and thus can integrate langua</context>
<context position="24642" citStr="DeNero et al., 2009" startWordPosition="4727" endWordPosition="4730">b 0 0 0 0 a b a 0 0 b 0 a 0 0 b c 0 1 2 3 4 5 6 7 8 P 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 % of rules with scope &lt;= P AE Lexical CE Lexical AE Nonlexical CE Nonlexical Figure 9: A selection of rule patterns that are scope &lt; 3 but not LNF or CNF. assert that this loss is negligable. A drawback of their technique is that the resulting CNF grammar contains many more rules and postconditions than the original grammar. These constant factors do not impact asymptotic performance, but do impact practical performance. 7.2 Lexical Normal Form Concerned about this inflation of the grammar constant, (DeNero et al., 2009) consider a superset of CNF called Lexical Normal Form (LNF). A rule is LNF if its pattern does not contain an ambiguity as a proper subsentence (recall that an ambiguity was defined to be the sentence “0 0”). Like CNF, the maximum scope of an LNF grammar is 3. In the worst case, the pattern s* is “0 0”, in which case there are three ambiguities in the sentence “0” ·s*· “0” . (DeNero et al., 2009) provide a lossless scope reduction method that maps a simple grammar to an LNF grammar, thus enabling cubic-time decoding. Their principal objective is to provide a scope reduction method for SCFG th</context>
</contexts>
<marker>DeNero, Bansal, Pauls, Klein, 2009</marker>
<rawString>John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. 2009. Efficient parsing for transducer grammars. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="5147" citStr="Galley et al., 2004" startWordPosition="908" endWordPosition="911">g effect of the terminals. In general, for a rule of the form X —* &apos;y, there are at most O(np) application contexts, where p is the number of consecutive nonterminal pairs in the string X ·&apos;y· X (where X is an arbitrary nonterminal). We refer to p as the scope of a rule. Thus chart parsing runs in time O(nsc WG)), where scope(G) is the maximum scope of any of the rules in CFG G. Specifically, any scope-3 grammar can be decoded in cubic time. Like (DeNero et al., 2009), the target of our interest is synchronous context-free grammar (SCFG) decoding with rules extracted using the GHKM algorithm (Galley et al., 2004). In practice, it turns out that only a small percentage of the lexical rules in our system have scope greater than 3. By simply removing these rules from the grammar, we can maintain the cubic running time of chart parsing without any kind of binarization. This has three advantages. First, we do not inflate the grammar constant. Second, unlike (DeNero et al., 2009), we maintain the synchronous property of the grammar, and thus can integrate language model scoring into chart parsing. Finally, a system without binarized rules is considerably simpler to build and maintain. We show that this appr</context>
<context position="26623" citStr="Galley et al., 2004" startWordPosition="5067" endWordPosition="5070"> it contains a much richer set of rules than those permitted by CNF or LNF. See Figure 9 for examples. Scope pruning is a lossy scope reduction method that simply takes an arbitrary grammar and prunes all rules with scope greater than 3. By not modifying any rules, we preserve their cost and carry functions (enabling integrated LM decoding), without increasing the grammar constant. The practical question is: how many rules are we typically pruning from the original grammar? We experimented with two pretrained syntaxbased machine translation systems with rules extracted via the GHKM algorithm (Galley et al., 2004). The first was an Arabic-English system, with rules extracted from 200 million words of parallel data from the NIST 2008 data collection, and with a 4-gram language model trained on 1 billion words of monolingual English data from the LDC Gigaword corpus. We evaluated this system’s performance on the NIST 2008 test corpus, which consists of 1357 Arabic sentences from a mixture of newswire and web domains, with four English reference translations. The second system was a Chinese653 Arabic-English Chinese-English 40 37 36 39 35 38 34 33 37 BLEU-4 BLEU-4 32 36 31 30 35 29 34 28 33 27 0 2000 4000</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>Cube pruning as heuristic search.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="18494" citStr="Hopkins and Langmead, 2009" startWordPosition="3494" endWordPosition="3497">ing provides an efficient solution for the Minimum Derivation Cost problem. 6 Runtime Analysis At the heart of chart parsing is a single operation: the updating of a value in the chart. The running time is linear in the number of these chart updates. 4 The typical analysis counts the number of chart updates per span. Here we provide an alternative 3Here, update means “replace the cost associated with the class if the new cost is lower.” 4This assumes that you can linearly enumerate the relevant updates. One convenient way to do this is to frame the enumeration problem as a search space, e.g. (Hopkins and Langmead, 2009) analysis that counts the number of chart updates per rule. This provides us with a finer bound with practical implications. Let r be a rule with rank k and pattern s*. Consider the chart updates involving rule r. There is (potentially) an update for every choice of (a) span, (b) application context, and (c) list of k derivation classes. If we let (t be the set of possible carries, then this means there are at most |cxt(s*, s) |· |(t|k updates involving rule r. 5 If we are doing beam decoding (i.e. after processing a span, the chart keeps only the B items of lowest cost), then there are at mos</context>
</contexts>
<marker>Hopkins, Langmead, 2009</marker>
<rawString>Mark Hopkins and Greg Langmead. 2009. Cube pruning as heuristic search. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL. the</booktitle>
<contexts>
<context position="25545" citStr="Huang and Chiang, 2007" startWordPosition="4885" endWordPosition="4888"> the pattern s* is “0 0”, in which case there are three ambiguities in the sentence “0” ·s*· “0” . (DeNero et al., 2009) provide a lossless scope reduction method that maps a simple grammar to an LNF grammar, thus enabling cubic-time decoding. Their principal objective is to provide a scope reduction method for SCFG that introduces fewer postconditions than (Zhang et al., 2006). However unlike (Zhang et al., 2006), their method only addresses simple grammars. Thus they cannot integrate LM scoring into their decoding, requiring them to rescore the decoder output with a variant of cube growing (Huang and Chiang, 2007). Figure 10: Breakdown of rules by scope (average per sentence in our test sets). In practice, most of the lexical rules applicable to a given sentence (95% for Arabic-English and 85% for Chinese-English) are scope 3 or less. 7.3 Scope Pruning To exercise the power of the ideas presented in this paper, we experimented with a third (and very easy) scope reduction method called scope pruning. If we consider the entire space of scope-3 grammars, we see that it contains a much richer set of rules than those permitted by CNF or LNF. See Figure 9 for examples. Scope pruning is a lossy scope reductio</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of ACL. the lexical rules and synchronously binarized the non-lexical rules. This had a similar performance to scope-pruning all rules. The opposite approach of scope-pruning the lexical rules and synchronously binarizing the non-lexical rules had a similar performance to synchronous binarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="27940" citStr="Papineni et al., 2002" startWordPosition="5292" endWordPosition="5295">deoff curves comparing the baseline scope reduction method of synchronous binarization (dark gray diamonds) with scope-3 pruning (light gray squares). English system, with rules extracted from 16 million words of parallel data from the mainland-news domain of the LDC corpora, and with a 4-gram language model trained on monolingual English data from the AFP and Xinhua portions of the LDC Gigaword corpus. We evaluated this system’s performance on the NIST 2003 test corpus, which consists of 919 Chinese sentences, with four English reference translations. For both systems, we report BLEU scores (Papineni et al., 2002) on untokenized, recapitalized output. In practice, how many rules have scope greater than 3? To answer this question, it is useful to distinguish between lexical rules (i.e. rules whose patterns contain at least one non-substitution symbol) and non-lexical rules. Only a subset of lexical rules are potentially applicable to a given input sentence. Figure 10 shows the scope profile of these applicable rules (averaged over all sentences in our test sets). Most of the lexical rules applicable to a given sentence (95% for Arabic-English, 85% for ChineseEnglish) are scope 3 or less. 8 Note, however</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="1184" citStr="Younger, 1967" startWordPosition="177" endWordPosition="178">the practical running time. (DeNero et al., 2009) tackle this problem by defining a superset of CNF called Lexical Normal Form (LNF), which also supports cubic time decoding under certain implicit assumptions. In this paper, we make these assumptions explicit, and in doing so, show that LNF can be further expanded to a broader class of grammars (called “scope3”) that also supports cubic-time decoding. By simply pruning non-scope-3 rules from a GHKM-extracted grammar, we obtain better translation performance than synchronous binarization. 1 Introduction At the heart of bottom-up chart parsing (Younger, 1967) is the following combinatorial problem. We have a context-free grammar (CFG) rule (for instance, S → NP VP PP) and an input sentence of length n (for instance, “on the fast jet ski of mr smith”). During chart parsing, we need to apply the rule to all relevant subspans of the input sentence. See Figure 1. For this particular rule, there are (n+1) 4 application contexts, i.e. ways to choose the subspans. Since the asymptotic running time of chart parsing is at least linear in this quantity, it will take on the fast jet ski of mr smith NP VP PP NP VP PP ... NP VP PP choice choice choice choice p</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Daniel Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="5869" citStr="Zhang et al., 2006" startWordPosition="1031" endWordPosition="1034">greater than 3. By simply removing these rules from the grammar, we can maintain the cubic running time of chart parsing without any kind of binarization. This has three advantages. First, we do not inflate the grammar constant. Second, unlike (DeNero et al., 2009), we maintain the synchronous property of the grammar, and thus can integrate language model scoring into chart parsing. Finally, a system without binarized rules is considerably simpler to build and maintain. We show that this approach gives us better practical performance than a mature system that binarizes using the technique of (Zhang et al., 2006). 2 Preliminaries Assume we have a global vocabulary of symbols, containing the reserved substitution symbol Q. Define a sentence as a sequence of symbols. We will typically use space-delimited quotations to represent example sentences, e.g. “the fast jet ski” rather than (the, fast, jet, ski). We will use the dot operator to represent the concatenation of sentences, e.g. “the fast” · “jet ski” = “the fast jet ski”. Define the rank of a sentence as the count of its Q symbols. We will use the notation SUB(s, s1, ..., sk) to denote the substitution of k sentences s1, ..., sk into a k-rank senten</context>
<context position="23092" citStr="Zhang et al., 2006" startWordPosition="4372" endWordPosition="4375">cope. Derivation Cost problem is the same for both R and R. 7 7.1 CNF Binarization A rule r is CNF if its pattern is “0 Q” or “x”, where x is any non-substitution symbol. A grammar is CNF if all of its rules are CNF. Note that the maximum scope of a CNF grammar is 3. CNF binarization is a deterministic process that maps a simple grammar to a CNF grammar. Since binarization takes subcubic time, we can decode with any grammar R in O(n3) time by converting R to CNF grammar R, and then decoding with R. This is a lossless scope reduction method. What if grammar R is not simple? For SCFG grammars, (Zhang et al., 2006) provide a scope reduction method called synchronous binarization with quantifiable loss. Synchronous binarization selects a “binarizable” subgrammar R&apos; of grammar R, and then converts R&apos; into a CNF grammar R. The cost and carry functions of these new rules are constructed such that the conversion from R&apos; to R is a lossless scope reduction. Thus the total loss of the method is |MinDCost(R, s)−MinDCost(R&apos;, s)|. Fortunately, they find in practice that R&apos; usually contains the great majority of the rules of R, thus they 7Note that if we want the actual derivation and not just its cost, then we nee</context>
<context position="25302" citStr="Zhang et al., 2006" startWordPosition="4845" endWordPosition="4848">l Normal Form (LNF). A rule is LNF if its pattern does not contain an ambiguity as a proper subsentence (recall that an ambiguity was defined to be the sentence “0 0”). Like CNF, the maximum scope of an LNF grammar is 3. In the worst case, the pattern s* is “0 0”, in which case there are three ambiguities in the sentence “0” ·s*· “0” . (DeNero et al., 2009) provide a lossless scope reduction method that maps a simple grammar to an LNF grammar, thus enabling cubic-time decoding. Their principal objective is to provide a scope reduction method for SCFG that introduces fewer postconditions than (Zhang et al., 2006). However unlike (Zhang et al., 2006), their method only addresses simple grammars. Thus they cannot integrate LM scoring into their decoding, requiring them to rescore the decoder output with a variant of cube growing (Huang and Chiang, 2007). Figure 10: Breakdown of rules by scope (average per sentence in our test sets). In practice, most of the lexical rules applicable to a given sentence (95% for Arabic-English and 85% for Chinese-English) are scope 3 or less. 7.3 Scope Pruning To exercise the power of the ideas presented in this paper, we experimented with a third (and very easy) scope re</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 256–263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>