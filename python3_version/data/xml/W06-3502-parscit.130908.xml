<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.097714">
<title confidence="0.9967165">
Backbone Extraction and Pruning for Speeding Up a Deep Parser for
Dialogue Systems
</title>
<author confidence="0.957558">
Myroslava O. Dzikovska
</author>
<affiliation confidence="0.9796905">
Human Communication Research Centre
University of Edinburgh
</affiliation>
<address confidence="0.949215">
Edinburgh, EH8 9LW, United Kingdom,
</address>
<email confidence="0.999482">
mdzikovs@inf.ed.ac.uk
</email>
<author confidence="0.968605">
Carolyn P. Ros´e
</author>
<affiliation confidence="0.940782">
Carnegie Mellon University
Language Technologies Institute,
</affiliation>
<address confidence="0.639039">
Pittsburgh, PA 15213,
</address>
<email confidence="0.999184">
cprose@cs.cmu.edu
</email>
<sectionHeader confidence="0.994144" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996000384615385">
In this paper we discuss issues related to
speeding up parsing with wide-coverage
unification grammars. We demonstrate
that state-of-the-art optimisation tech-
niques based on backbone parsing before
unification do not provide a general so-
lution, because they depend on specific
properties of the grammar formalism that
do not hold for all unification based gram-
mars. As an alternative, we describe an
optimisation technique that combines am-
biguity packing at the constituent structure
level with pruning based on local features.
</bodyText>
<sectionHeader confidence="0.999023" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999065090909091">
In this paper we investigate the problem of scaling
up a deep unification-based parser developed specif-
ically for the purpose of robust interpretation in dia-
logue systems by improving its speed and coverage
for longer utterances. While typical sentences in di-
alogue contexts are shorter than in expository text
domains, longer utterances are important in discus-
sion oriented domains. For example, in educational
applications of dialogue it is important to elicit deep
explanation from students and then offer focused
feedback based on the details of what students say.
The choice of instructional dialogue as a target ap-
plication influenced the choice of parser we needed
to use for interpretation in a dialogue system. Sev-
eral deep, wide-coverage parsers are currently avail-
able (Copestake and Flickinger, 2000; Ros´e, 2000;
Baldridge, 2002; Maxwell and Kaplan, 1994), but
many of these have not been designed with issues re-
lated to interpretation in a dialogue context in mind.
The TRIPS grammar (Dzikovska et al., 2005) is a
wide-coverage unification grammar that has been
used very successfully in several task-oriented di-
alogue systems. It supports interpretation of frag-
ments and lexical semantic features (see Section 2
for a more detailed discussion), and provides addi-
tional robustness through “robust” rules that cover
common grammar mistakes found in dialogue such
as missing articles or incorrect agreement. These
enhancements help parsing dialogue (both spoken
and typed), but they significantly increase grammar
ambiguity, a common concern in building grammars
for robust parsing (Schneider and McCoy, 1998). It
is specifically these robustness-efficiency trade-offs
that we address in this paper.
Much work has been done related to enhanc-
ing the efficiency of deep interpretation systems
(Copestake and Flickinger, 2000; Swift et al., 2004;
Maxwell and Kaplan, 1994), which forms the foun-
dation that we build on in this work. For example,
techniques for speeding up unification in HPSG lead
to dramatic improvements in efficiency (Kiefer et
al., 1999). Likewise ambiguity packing and CFG
backbone parsing (Maxwell and Kaplan, 1994; van
Noord, 1997) are known to increase parsing effi-
ciency. However, as we show in this paper, these
techniques depend on specific grammar properties
that do not hold for all grammars. This claim is con-
sistent with observations of Carroll (1994) that pars-
ing software optimisation techniques tend to be lim-
ited in their applicability to the individual grammars
they were developed for. While we used TRIPS as
our example unification-based grammar, this inves-
tigation is important not only for this project, but in
the general context of speeding up a wide-coverage
unification grammar which incorporates fragment
</bodyText>
<page confidence="0.955024">
9
</page>
<note confidence="0.901062">
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 9–16,
New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99941375">
rules and lexical semantics, which may not be im-
mediately provided by other available systems.
In the remainder of the paper, we begin with a
brief description of the TRIPS parser and grammar,
and motivate the choice of LCFLEX parsing algo-
rithm to provide a fast parsing foundation. We then
discuss the backbone extraction and pruning tech-
niques that we used, and evaluate them in compar-
ison with the original parsing algorithm. We con-
clude with discussion of some implications for im-
plementing grammars that build deep syntactic and
semantic representations.
</bodyText>
<sectionHeader confidence="0.986407" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999977711864407">
The work reported in this paper was done as part
of the process of developing a dialogue system that
incorporates deep natural language understanding.
We needed a grammar that provides lexical seman-
tic interpretation, supports parsing fragmentary ut-
terance in dialogue, and could be used to start de-
velopment without large quantities of corpus data.
TRIPS fulfilled our requirements better than sim-
ilar alternatives, such as LINGO ERG (Copestake
and Flickinger, 2000) or XLE (Maxwell and Kaplan,
1994).
TRIPS produces logical forms which include se-
mantic classes and roles in a domain-independent
frame-based formalism derived from FrameNet and
VerbNet (Dzikovska et al., 2004; Kipper et al.,
2000). Lexical semantic features are known to be
helpful in both deep (Tetreault, 2005) and shal-
low interpretation tasks (Narayanan and Harabagiu,
2004). Apart from TRIPS, these have not been in-
tegrated into existing deep grammars. While both
LINGO-ERG and XLE include semantic features
related to scoping, in our applications the avail-
ability of semantic classes and semantic role as-
signments was more important to interpretation,
and these features are not currently available from
those parsers. Finally, TRIPS provides a domain-
independent parse selection model, as well as rules
for interpreting discourse fragments (as was also
done in HPSG (Schlangen and Lascarides, 2003), a
feature actively used in interpretation.
While TRIPS provides the capabilities we need,
its parse times for long sentences (above 15 words
long) are intolerably long. We considered two pos-
sible techniques for speeding up parsing: speeding
up unification using the techniques similar to the
LINGO system (Copestake and Flickinger, 2000),
or using backbone extraction (Maxwell and Ka-
plan, 1994; Ros´e and Lavie, 2001; Briscoe and Car-
roll, 1994). TRIPS already uses a fast unification
algorithm similar to quasi-destructive unification,
avoiding copying during unification.&apos; However,
the TRIPS grammar retains the notion of phrase
structure, and thus it was more natural to chose to
use backbone extraction with ambiguity packing to
speed up the parsing.
As a foundation for our optimisation work, we
started with the freely available LCFLEX parser
(Ros´e and Lavie, 2001). LCFLEX is an all-paths
parser that uses left-corner prediction and ambigu-
ity packing to make all-paths parsing tractable, and
which was shown to be efficient for long sentences
with somewhat less complex unification augmented
context-free grammars. We show that all-paths pars-
ing with LCFLEX is not tractable for the ambiguity
level in the TRIPS grammar, but that by introduc-
ing a pruning method that uses ambiguity packing to
guide pruning decisions, we can achieve significant
improvements in both speed and coverage compared
to the original TRIPS parser.
</bodyText>
<sectionHeader confidence="0.897214" genericHeader="method">
3 The TRIPS and LCFLEX algorithms
</sectionHeader>
<subsectionHeader confidence="0.99756">
3.1 The TRIPS parser
</subsectionHeader>
<bodyText confidence="0.998797842105263">
The TRIPS parser we use as a baseline is a bottom-
up chart parser with lexical entries and rules repre-
sented as attribute-value structures. To achieve pars-
ing efficiency, TRIPS uses a best-first beam search
algorithm based on the scores from a parse selection
model (Dzikovska et al., 2005; Elsner et al., 2005).
The constituents on the parser’s agenda are grouped
into buckets based on their scores. At each step, the
bucket with the highest scoring constituents is se-
lected to build/extend chart edges. The parsing stops
once N requested analyses are found. This guaran-
tees that the parser returns the N-best list of analyses
according to the parse selection model used, unless
the parser reaches the chart size limit.
&apos;Other enhancements used by LINGO depend on disallow-
ing disjunctive features, and relying instead on the type system.
The TRIPS grammar is untyped and uses disjunctive features,
and converting it to a typed system would require as yet unde-
termined amount of additional work.
</bodyText>
<page confidence="0.996948">
10
</page>
<bodyText confidence="0.999982821428571">
In addition to best-first parsing, the TRIPS parser
uses a chart size limit, to prevent the parser from
running too long on unparseable utterances, similar
to (Frank et al., 2003). TRIPS is much slower pro-
cessing utterances not covered in the grammar, be-
cause it continues its search until it reaches the chart
limit. Thus, a lower chart limit improves parsing
efficiency. However, we show in our evaluation that
the chart limit necessary to obtain good performance
in most cases is too low to find parses for utterances
with 15 or more words, even if they are covered by
the grammar.
The integration of lexical semantics in the TRIPS
lexicon has a major impact on parsing in TRIPS.
Each word in the TRIPS lexicon is associated with a
semantic type from a domain-independent ontology.
This enables word sense disambiguation and seman-
tic role labelling for the logical form produced by
the grammar. Multiple word senses result in addi-
tional ambiguity on top of syntactic ambiguity, but it
is controlled in part with the use of weak selectional
restrictions, similar to the restrictions employed by
the VerbNet lexicon (Kipper et al., 2000). Check-
ing semantic restrictions is an integral part of TRIPS
parsing, and removing them significantly decreases
speed and increases ambiguity of the TRIPS parser
(Dzikovska, 2004). We show that it also has an im-
pact on parsing with a CFG backbone in Section 4.1.
</bodyText>
<subsectionHeader confidence="0.995905">
3.2 LCFLEX
</subsectionHeader>
<bodyText confidence="0.999855357142857">
The LCFLEX parser (Ros´e and Lavie, 2001) is an
all-paths robust left corner chart parser designed to
incorporate various robustness techniques such as
word skipping, flexible unification, and constituent
insertion. Its left corner chart parsing algorithm
is similar to that described by Briscoe and Carroll
(1994). The system supports grammatical specifi-
cation in a unification framework that consists of
context-free grammar rules augmented with feature
bundles associated with the non-terminals of the
rules. LCFLEX can be used in two parsing modes:
either context-free parsing can be done first, fol-
lowed by applying the unification rules, or unifica-
tion can be done interleaved with context-free pars-
ing. The context free backbone allows for efficient
left corner predictions using a pre-compiled left cor-
ner prediction table, such as that described in (van
Noord, 1997). To enhance its efficiency, it incor-
porates a provably optimal ambiguity packing algo-
rithm (Lavie and Ros´e, 2004).
These efficiency techniques make feasible all-
path parsing with the LCFLEX CARMEL grammar
(Ros´e, 2000). However, CARMEL was engineered
with fast all-paths parsing in mind, resulting in cer-
tain compromises in terms of coverage. For exam-
ple, it has only very limited coverage for noun-noun
compounding, or headless noun phrases, which are a
major source of ambiguity with the TRIPS grammar.
</bodyText>
<sectionHeader confidence="0.699425" genericHeader="method">
4 Combining LCFLEX and TRIPS
</sectionHeader>
<subsectionHeader confidence="0.999587">
4.1 Adding CFG Backbone
</subsectionHeader>
<bodyText confidence="0.999957882352941">
A simplified TRIPS grammar rule for verb phrases
and a sample verb entry are shown in Figure 1. The
features for building semantic representations are
omitted for brevity. Each constituent has an assigned
category that corresponds to its phrasal type, and a
set of (complex-valued) features.
The backbone extraction algorithm is reason-
ably straightforward, with CFG non-terminals cor-
responding directly to TRIPS constituent categories.
To each CFG rule we attach a corresponding TRIPS
unification rule. After parsing is complete, the
parses found are scored and ordered with the parse
selection model, and therefore parsing accuracy in
all-paths mode is the same or better than TRIPS ac-
curacy for the same model.
For constituents with subcategorized arguments
(verbs, nouns, adverbial prepositions), our back-
bone generation algorithm takes the subcategoriza-
tion frame into account. For example, the TRIPS
VP rule will split into 27 CFG rules corresponding
to different subcategorization frames: VP —* V intr,
VP —* V NP NP, VP —* V NP CP NP CP, etc. For
each lexical entry, its appropriate CFG category is
determined based on the subcategorization frame
from TRIPS lexical representation. This improves
parsing efficiency using the prediction algorithms in
TFLEX operating on the CFG backbone. The ver-
sion of the TRIPS grammar used in testing con-
tained 379 grammar rules with 21 parts of speech
(terminal symbols) and 31 constituent types (non-
terminal symbols), which were expanded into 1121
CFG rules with 85 terminals and 36 non-terminals
during backbone extraction.
We found, however, that the previously used tech-
</bodyText>
<page confidence="0.987092">
11
</page>
<figure confidence="0.916305125">
(a) ((VP (SUBJ ?!subj) (CLASS ?lf))
-vp1-role .99
(V (LF ?lf) (SUBJ ?!subj) (DOBJ ?dobj)
(IOBJ ?iobj) (COMP3 ?comp3))
?iobj ?dobj ?comp3)
(b) ((V (agr 3s) (LF LF::Filling)
(SUBJ (NP (agr 3s)))
(DOBJ (NP (case obj))) (IOBJ -) (COMP3 -)))
</figure>
<figureCaption confidence="0.994581">
Figure 1: (a) A simplified VP rule from the TRIPS
grammar; (b) a simplified verb entry for a transitive
verb. Question marks denote variables.
</figureCaption>
<bodyText confidence="0.999940236363636">
nique of context-free parsing first followed by full
re-unification was not suitable for parsing with the
TRIPS grammar. The CFG structure extracted from
the TRIPS grammar contains 43 loops resulting
from lexical coercion rules or elliptical construc-
tions. A small number of loops from lexical coer-
cion were both obvious and easy to avoid, because
they are in the form N—* N. However, there were
longer loops, for example, NP —* SPEC for sen-
tences like “John’s car” and SPEC —* NP for head-
less noun phrases in sentences like “I want three”.
LCFLEX uses a re-unification algorithm that asso-
ciates a set of unification rules with each CFG pro-
duction, which are reapplied at a later stage. To
be able to apply a unification rule corresponding to
N—* N production, it has to be explicitly present in
the chart, leading to an infinite number of N con-
stituents produced. Applying the extra CFG rules
expanding the loops during re-unification would
complicate the algorithm significantly. Instead, we
implemented loop detection during CFG parsing.
The feature structures prevent loops in unifica-
tion, and we considered including certain grammat-
ical features into backbone extraction as done in
(Briscoe and Carroll, 1994). However, in the TRIPS
grammar the feature values responsible for break-
ing loops belonged to multi-valued features (6 val-
ued in the worst case), with values which may de-
pend on other multiple-valued features in daughter
constituents. Thus adding the extra features resulted
in major backbone size increases because of cate-
gory splitting. This can be remedied with additional
pre-compilation (Kiefer and Krieger, 2004), how-
ever, this requires that all lexical entries be known
in advance. One nice feature of the TRIPS lex-
icon is that it includes a mechanism for dynami-
cally adding lexical entries for unknown words from
wide-coverage lexicons such as VerbNet (Kipper et
al., 2000), which would be impractical to use in pre-
compilation.
Therefore, to use CFG parsing before unification
in our system, we implemented a loop detector that
checked the CFG structure to disallow loops. How-
ever, the next problem that we encountered is mas-
sive ambiguity in the CFG structure. Even a very
short phrase such as “a train” had over 700 possi-
ble CFG analyses, and took 910 msec to parse com-
pared to 10 msec with interleaved unification. CFG
ambiguity is so high because noun phrase fragments
are allowed as top-level categories, and lexical am-
biguity is compounded with semantic ambiguity and
robust rules normally disallowed by features during
unification. Thus, in our combined algorithm we had
to use unification interleaved with parsing to filter
out the CFG constituents.
</bodyText>
<subsectionHeader confidence="0.99683">
4.2 Ambiguity Packing
</subsectionHeader>
<bodyText confidence="0.99672184">
For building semantic representations in parallel
with parsing, ambiguity packing presents a set of
known problems (Oepen and Carroll, 2000). One
possible solution is to exclude semantic features dur-
ing an initial unification stage, use ambiguity pack-
ing, and re-unify with semantic features in a post-
processing stage. In our case, we found this strategy
difficult to implement, since selectional restrictions
are used to limit the ambiguity created by multiple
word senses during syntactic parsing. Therefore, we
chose to do ambiguity packing on the CFG structure
only, keeping the multiple feature structures associ-
ated with each packed CFG constituent.
To begin to evaluate the contribution of ambiguity
packing on efficiency, we ran a test on the first 39 ut-
terances in a hold out set not used in the formal eval-
uation below. Sentences ranged from 1 to 17 words
in length, 16 of which had 6 or more words. On this
set, the average parse time without ambiguity pack-
ing was 10 seconds per utterance, and 30 seconds per
utterance on utterances with 6 or more words. With
ambiguity packing turned on, the average parse time
decreased to 5 seconds per utterance, and 13.5 sec-
onds per utterance on the utterances with more than
6 words. While this evaluation showed that ambi-
</bodyText>
<page confidence="0.997304">
12
</page>
<bodyText confidence="0.9990695">
guity packing improves parsing efficiency, we deter-
mined that further enhancements were necessary.
</bodyText>
<subsectionHeader confidence="0.999282">
4.3 Pruning
</subsectionHeader>
<bodyText confidence="0.999991207792208">
We added a pruning technique based on the scor-
ing model discussed above and ambiguity packing
to enhance system performance. As an illustration,
consider an example from a corpus used in our eval-
uation where the TRIPS grammar generates a large
number of analyses, “we have a heart attack vic-
tim at marketplace mall”. The phrase “a heart at-
tack victim” has at least two interpretations,“a [N1
heart [N1 attack [N1 victim]]]” and “a [N1 [N1 heart
[N1 attack]] [N1 victim]]”. The prepositional phrase
“at marketplace mall” can attach either to the noun
phrase or to the verb. Overall, this results in 4 basic
interpretations, with additional ambiguity resulting
from different possible senses of “have”.
The best-first parsing algorithm in TRIPS uses
parse selection scores to suppress less likely inter-
pretations. In our example, the TRIPS parser will
chose the higher-scoring one of the two interpreta-
tions for “a heart attack victim”, and use it first. For
this NP the features associated with both interpreta-
tions are identical with respect to further processing,
thus TRIPS will never come back to the other in-
terpretation, effectively pruning it. “At” also has 2
possible interpretations due to word sense ambigu-
ity: LF::TIME-LOC and LF::SPATIAL-LOC. The
former has a slightly higher preference, and TRIPS
will try it first. But then it will be unable to find an
interpretation for “at Marketplace Mall”, and back-
track to LF::SPATIAL-LOC to find a correct parse.
Without chart size limits the parser is guaran-
teed to find a parse eventually through backtracking.
However, this algorithm does not work quite as well
with chart size limits. If there are many similarly-
scored constituents in the chart for different parts of
the utterance, the best-first algorithm expands them
first, and the the chart size limit tends to interfere
before TRIPS can backtrack to an appropriate lower-
scoring analysis.
Ambiguity packing offers an opportunity to make
pruning more strategic by focusing specifically on
competing interpretations for the same utterance
span. The simplest pruning idea would be for ev-
ery ambiguity packed constituent to eliminate the in-
terpretations with low TRIPS scores. However, we
need to make sure that we don’t prune constituents
that are required higher up in the tree to make a
parse. Consider our example again.
The constituent for “at” will be ambiguity
packed with its two meanings. But if we prune
LF::SPATIAL-LOC at that point, the parse for “at
Marketplace Mall” will fail later. Formally, the com-
peting interpretations for “at” have non-local fea-
tures, namely, the subcategorized complement (time
versus location) is different for those interpretations,
and is checked higher up in the parse. But for “a
heart attack victim” the ambiguity-packed interpre-
tations differ only in local features. All features as-
sociated with this NP checked higher up come from
the head noun “victim” and are identical in all inter-
pretations. Therefore we can eliminate the low scor-
ing interpretations with little risk of discarding those
essential for finding a complete parse. Thus, for
any constituent where ambiguity-packed non-head
daughters differ only in local features, we prune
the interpretations coming from them to a specified
prune beam width based on their TRIPS scores.
This pruning heuristic based on local features
can be generalised to different unification grammars.
For example, in HPSG pruning would be safe at all
points where a head is combined with ambiguity-
packed non-head constituents, due to the locality
principle. In the TRIPS grammar, if a trips rule
uses subcategorization features, the same locality
principle holds. This heuristic has perfect precision
though not complete recall, but, as our evaluation
shows, it is sufficient to significantly improve per-
formance in comparison with the TRIPS parser.
</bodyText>
<sectionHeader confidence="0.998125" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9998925">
The purpose of our evaluation is to explore the ex-
tent to which we can achieve a better balance be-
tween parse time and coverage using backbone pars-
ing with pruning compared to the original best-first
algorithm. For our comparison we used an excerpt
from the Monroe corpus that has been used in previ-
ous TRIPS research on parsing speed and accuracy
(Swift et al., 2004) consisting of dialogues s2, s4,
s16 and s17. Dialogue s2 was a hold out set used for
pilot testing and setting parameters. The other three
dialogues were set aside for testing. Altogether, the
test set contained 1042 utterances, ranging from 1 to
</bodyText>
<page confidence="0.997862">
13
</page>
<bodyText confidence="0.999566416666667">
45 words in length (mean 5.38 words/utt, st. dev. 5.7
words/utt). Using our hold-out set, we determined
that a beam width of three was an optimal setting.
Thus, we compared TFLEX using a beam width of 3
to three different versions of TRIPS that varied only
in terms of the maximum chart size, giving us a ver-
sion that is significantly faster than TFLEX overall,
one that has parse times that are statistically indis-
tinguishable from TFLEX, and one that is signifi-
cantly slower. We show that while lower chart sizes
in TRIPS yield speed ups in parse time, they come
with a cost in terms of coverage.
</bodyText>
<subsectionHeader confidence="0.996852">
5.1 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999985277777778">
Because our goal is to explore the parse time versus
coverage trade-offs of two different parsing architec-
tures, the two evaluation measures that we report are
average parse time per sentence and probability of
finding at least one parse, the latter being a measure
estimating the effect of parse algorithm on parsing
coverage.
Since the scoring model is the same in TRIPS and
TFLEX, then as long as TFLEX can find at least one
parse (which happened in all but 1 instances on our
held-out set), the set returned will include the one
produced by TRIPS. We spot-checked the TFLEX
utterances in the test set for which TRIPS could
not find a parse to verify that the parses produced
were reasonable. The parses produced by TFLEX on
these sentences were typically acceptable, with er-
rors mainly stemming from attachment disambigua-
tion problems.
</bodyText>
<subsectionHeader confidence="0.963372">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.998662357142857">
We first compared parsers in terms of probability of
producing at least one parse (see Figure 2). Since
the distribution of sentence lengths in the test corpus
was heavily skewed toward shorter sentences, we
grouped sentences into equivalence classes based on
a range of sentence lengths with a 5-word increment,
with all of the sentences over 20 words aggregated
in the same class. Given a large number of short sen-
tences, there was no significant difference overall in
likelihood to find a parse. However, on sentences
greater than 10 words long, TFLEX is significantly
more likely to produce a parse than any of the TRIPS
parsers (evaluated using a binary logistic regression,
N = 334, G = 16.8, DF = 1, p &lt; .001). Fur-
</bodyText>
<table confidence="0.9945858">
Parser &lt;= 20 words &gt;= 6 words
TFLEX 6.2 (20.2) 29.1 (96.3)
TRIPS-1500 2.3 (5.4) 6.9 (8.2)
TRIPS-5000 7.7 (30.2) 28.1 (56.4)
TRIPS-10000 22.7 (134.4) 107.6 (407.4)
</table>
<tableCaption confidence="0.98649">
Table 1: The average parse times for TRIPS and
TFLEX on utterances 6 words or more.
</tableCaption>
<bodyText confidence="0.996540051282051">
thermore, for sentences greater than 20 words long,
no form of TRIPS parser ever returned a complete
parse.
Next we compared the parsers in terms of aver-
age parse time on the whole data set across equiva-
lence classes of sentences, assigned based on Aggre-
gated Sentence Length (see Figure 2 and Table 1).
An ANOVA with Parser and Aggregated Sentence
Length as independent variables and Parse Time as
the dependent variable showed a significant effect
of Parser on Parse Time (F(3,4164) = 270.03,
p &lt; .001). Using a Bonferroni post-hoc analysis, we
determined that TFLEX is significantly faster than
TRIPS-10000 (p &lt; .001), statistically indistinguish-
able in terms of parse time from TRIPS-5000, and
significantly slower than TRIPS-1500 (p &lt; .001).
Since none of the TRIPS parsers ever returned a
parse for sentences greater than 20 words long, we
recomputed this analysis excluding the latter. We
still find a significant effect of Parser on Parse Time
(F(3,4068) = 18.6, p &lt; .001). However, a post-
hoc analysis reveals that parse times for TFLEX,
TRIPS-1500, and TRIPS-5000 are statistically in-
distinguishable for this subset, whereas TFLEX is
significantly faster than TRIPS-10000 (p &lt; .001).
See Table 1 for for parse times of all four parsers.
Since TFLEX and TRIPS both spent 95% of their
computational effort on sentences with 6 or more
words, we also include results for this subset of the
corpus.
Thus, TFLEX presents a superior balance of cov-
erage and efficiency especially for long sentences
(10 words or more) since for these sentences it is
significantly more likely to find a parse than any ver-
sion of TRIPS, even a version where the chart size is
expanded to an extent that it becomes significantly
slower (i.e., TRIPS-10000). And while TRIPS-1500
is consistently faster than the other parsers, it is
not significantly faster than TFLEX on sentences 20
</bodyText>
<page confidence="0.997903">
14
</page>
<figureCaption confidence="0.8009295">
Figure 2: Parse times and probability of getting a parse depending on (aggregated) sentence lengths. 5
denotes sentences with 5 or fewer words, 25 sentences with more than 20 words.
</figureCaption>
<bodyText confidence="0.998683">
words long or less, which is the subset of sentences
for which it is able to find a parse.
</bodyText>
<subsectionHeader confidence="0.938338">
5.3 Discussion and Future Work
</subsectionHeader>
<bodyText confidence="0.999990333333333">
The most obvious lesson learned in this experience
is that the speed up techniques developed for specific
grammars and unification formalisms do not transfer
easily to other unification grammars. The features
that make TRIPS interesting – the inclusion of lex-
ical semantics, and the rules for parsing fragments
– also make it less amenable to using existing effi-
ciency techniques.
Grammars with an explicit CFG backbone nor-
mally restrict the grammar writer from writing
grammar loops, a restriction not imposed by gen-
eral unification grammars. As we showed, there
can be a substantial number of loops in a CFG due
to the need to cover various elliptical constructions,
which makes CFG parsing not interleaved with uni-
fication less attractive in cases where we want to
avoid expensive CFG precompilation. Moreover, as
we found with the TRIPS grammar, in the context
of robust parsing with lexical semantics the ambigu-
ity in a CFG backbone grows large enough to make
CFG parsing followed by unification inefficient. We
described an alternative technique that uses pruning
based on a parse selection model.
Another option for speeding up parsing that we
have not discussed in detail is using a typed gram-
mar without disjunction and speeding up unification
as done in HPSG grammars (Kiefer et al., 1999). In
order to do this, we must first address the issue of
integrating the type of lexical semantics that we re-
quire with HPSG’s type system. Adding lexical se-
mantics while retaining the speed benefits obtained
through this type system would require that the se-
mantic type ontology be expressed in the same for-
malism as the syntactic types. We plan to further
explore this option in our future work.
Though longer sentences were relatively rare
in our test set, using the system in an educa-
tional domain (our ultimate goal) means that the
longer sentences are particularly important, because
they often correspond to significant instructional
events, specifically answers to deep questions such
as “why” and “how” questions. Our evaluation has
been designed to show system performance with ut-
terances of different length, which would roughly
correspond to the performance in interpreting short
and long student answers. Since delays in respond-
ing can de-motivate the student and decrease the
quality of the dialogue, improving handling of long
utterances can have an important effect on the sys-
tem performance. Evaluating this in practice is a
possible direction for future work.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999969857142857">
We described a combination of efficient parsing
techniques to improve parsing speed and coverage
with the TRIPS deep parsing grammar. We showed
that context-free parsing was inefficient on a back-
bone extracted from an existing unification gram-
mar, and demonstrated how to make all-path pars-
ing more tractable by a new pruning algorithm based
</bodyText>
<page confidence="0.992441">
15
</page>
<bodyText confidence="0.999925714285714">
on ambiguity packing and local features, general-
isable to other unification grammars. We demon-
strated that our pruning algorithm provides better
efficiency-coverage balance than the best-first pars-
ing with chart limits utilised by the TRIPS parser,
and discussed the design implications for other ro-
bust parsing grammars.
</bodyText>
<sectionHeader confidence="0.99473" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9980986">
We thank Mary Swift and James Allen for their
help with the TRIPS code and useful comments.
This material is based on work supported by grants
from the Office of Naval Research under numbers
N000140510048 and N000140510043.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985387804878">
J. Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
T. Briscoe and J. Carroll. 1994. Generalized proba-
bilistic LR parsing of natural language (corpora) with
unification-based grammars. Computational Linguis-
tics, 19(1):25–59.
J. Carroll. 1994. Relating complexity to practical per-
formance in parsing with wide-coverage unification
grammars. In Proceedings ofACL-2004.
A. Copestake and D. Flickinger. 2000. An open
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings ofLREC-2000, Athens, Greece.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004.
Building a computational lexicon and ontology with
framenet. In LREC workshop on Building Lexical Re-
sources from Semantically Annotated Corpora.
M. Dzikovska, M. Swift, J. Allen, and W. de Beaumont.
2005. Generic parsing for multi-domain semantic in-
terpretation. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT-05).
M. O. Dzikovska. 2004. A Practical Semantic Represen-
tation For Natural Language Parsing. Ph.D. thesis,
University of Rochester.
M. Elsner, M. Swift, J. Allen, and D. Gildea. 2005. On-
line statistics for a unification-based dialogue parser.
In Proceedings of the 9th International Workshop on
Parsing Technologies (IWPT-05).
A. Frank, B. Kiefer, B. Crysmann, M. Becker, and
U. Schafer. 2003. Integrated shallow and deep pars-
ing: TopP meets HPSG. In Proceedings ofACL 2003.
B. Kiefer and H.-U. Krieger. 2004. A context-free ap-
proximation of head-driven phrase structure grammar.
In H. Bunt, J. Carroll, and G. Satta, editors, New De-
velopments in Parsing Technology. Kluwer.
B. Kiefer, H. Krieger, J. Carroll, and R. Malouf. 1999.
Bag of useful techniques for efficient and robust pars-
ing. In Proceedings ofACL 1999.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of the 7th Conference on Artificial Intelligence and of
the 12th Conference on Innovative Applications ofAr-
tificial Intelligence.
A. Lavie and C. P. Ros´e. 2004. Optimal ambiguity pack-
ing in context-free parsers with interleaved unification.
In H. Bunt, J. Carroll, and G. Satta, editors, Current Is-
sues in Parsing Technologies. Kluwer Academic Press.
J. T. Maxwell and R. M. Kaplan. 1994. The interface
between phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571–590.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proceedings of
International Conference on Computational Linguis-
tics (COLING 2004), Geneva, Switzerland.
S. Oepen and J. Carroll. 2000. Ambiguity packing in
constraint-based parsing - practical results. In Pro-
ceedings ofNAACL’00.
C. P. Ros´e and A. Lavie. 2001. Balancing robustness
and efficiency in unification-augmented context-free
parsers for large practical applications. In J. Junqua
and G. Van Noord, editors, Robustness in Language
and Speech Technology. Kluwer Academic Press.
C. Ros´e. 2000. A framework for robust semantic in-
terpretation. In Proceedings 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics.
D. Schlangen and A. Lascarides. 2003. The interpreta-
tion of non-sentential utterances in dialogue. In Pro-
ceedings of the 4th SIGdial Workshop on Discourse
and Dialogue, Japan, May.
D. Schneider and K. F. McCoy. 1998. Recognizing syn-
tactic errors in the writing of second language learners.
In Proceedings of COLING-ACL’98.
M. Swift, J. Allen, and D. Gildea. 2004. Skeletons in
the parser: Using a shallow parser to improve deep
parsing. In Proceedings of COLING-04.
J. Tetreault. 2005. Empirical Evaluations of Pronoun
Resolution. Ph.D. thesis, University of Rochester.
G. van Noord. 1997. An efficient implementation of
the head-corner parser. Computational Linguistics,
23(3):425–456.
</reference>
<page confidence="0.998694">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.737268">
<title confidence="0.9981015">Backbone Extraction and Pruning for Speeding Up a Deep Parser Dialogue Systems</title>
<author confidence="0.975805">O Myroslava</author>
<affiliation confidence="0.9963255">Human Communication Research University of</affiliation>
<address confidence="0.881309">Edinburgh, EH8 9LW, United</address>
<email confidence="0.982008">mdzikovs@inf.ed.ac.uk</email>
<author confidence="0.983082">P Carolyn</author>
<affiliation confidence="0.9746255">Carnegie Mellon Language Technologies</affiliation>
<address confidence="0.928469">Pittsburgh, PA</address>
<email confidence="0.999304">cprose@cs.cmu.edu</email>
<abstract confidence="0.999151928571429">In this paper we discuss issues related to speeding up parsing with wide-coverage unification grammars. We demonstrate that state-of-the-art optimisation techniques based on backbone parsing before unification do not provide a general solution, because they depend on specific properties of the grammar formalism that do not hold for all unification based grammars. As an alternative, we describe an optimisation technique that combines ambiguity packing at the constituent structure level with pruning based on local features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Baldridge</author>
</authors>
<title>Lexically Specified Derivational Control in Combinatory Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1734" citStr="Baldridge, 2002" startWordPosition="252" endWordPosition="253">es. While typical sentences in dialogue contexts are shorter than in expository text domains, longer utterances are important in discussion oriented domains. For example, in educational applications of dialogue it is important to elicit deep explanation from students and then offer focused feedback based on the details of what students say. The choice of instructional dialogue as a target application influenced the choice of parser we needed to use for interpretation in a dialogue system. Several deep, wide-coverage parsers are currently available (Copestake and Flickinger, 2000; Ros´e, 2000; Baldridge, 2002; Maxwell and Kaplan, 1994), but many of these have not been designed with issues related to interpretation in a dialogue context in mind. The TRIPS grammar (Dzikovska et al., 2005) is a wide-coverage unification grammar that has been used very successfully in several task-oriented dialogue systems. It supports interpretation of fragments and lexical semantic features (see Section 2 for a more detailed discussion), and provides additional robustness through “robust” rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement. These enhancements he</context>
</contexts>
<marker>Baldridge, 2002</marker>
<rawString>J. Baldridge. 2002. Lexically Specified Derivational Control in Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="6207" citStr="Briscoe and Carroll, 1994" startWordPosition="938" endWordPosition="942"> Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a feature actively used in interpretation. While TRIPS provides the capabilities we need, its parse times for long sentences (above 15 words long) are intolerably long. We considered two possible techniques for speeding up parsing: speeding up unification using the techniques similar to the LINGO system (Copestake and Flickinger, 2000), or using backbone extraction (Maxwell and Kaplan, 1994; Ros´e and Lavie, 2001; Briscoe and Carroll, 1994). TRIPS already uses a fast unification algorithm similar to quasi-destructive unification, avoiding copying during unification.&apos; However, the TRIPS grammar retains the notion of phrase structure, and thus it was more natural to chose to use backbone extraction with ambiguity packing to speed up the parsing. As a foundation for our optimisation work, we started with the freely available LCFLEX parser (Ros´e and Lavie, 2001). LCFLEX is an all-paths parser that uses left-corner prediction and ambiguity packing to make all-paths parsing tractable, and which was shown to be efficient for long sent</context>
<context position="9974" citStr="Briscoe and Carroll (1994)" startWordPosition="1548" endWordPosition="1551">VerbNet lexicon (Kipper et al., 2000). Checking semantic restrictions is an integral part of TRIPS parsing, and removing them significantly decreases speed and increases ambiguity of the TRIPS parser (Dzikovska, 2004). We show that it also has an impact on parsing with a CFG backbone in Section 4.1. 3.2 LCFLEX The LCFLEX parser (Ros´e and Lavie, 2001) is an all-paths robust left corner chart parser designed to incorporate various robustness techniques such as word skipping, flexible unification, and constituent insertion. Its left corner chart parsing algorithm is similar to that described by Briscoe and Carroll (1994). The system supports grammatical specification in a unification framework that consists of context-free grammar rules augmented with feature bundles associated with the non-terminals of the rules. LCFLEX can be used in two parsing modes: either context-free parsing can be done first, followed by applying the unification rules, or unification can be done interleaved with context-free parsing. The context free backbone allows for efficient left corner predictions using a pre-compiled left corner prediction table, such as that described in (van Noord, 1997). To enhance its efficiency, it incorpo</context>
<context position="14308" citStr="Briscoe and Carroll, 1994" startWordPosition="2239" endWordPosition="2242">ociates a set of unification rules with each CFG production, which are reapplied at a later stage. To be able to apply a unification rule corresponding to N—* N production, it has to be explicitly present in the chart, leading to an infinite number of N constituents produced. Applying the extra CFG rules expanding the loops during re-unification would complicate the algorithm significantly. Instead, we implemented loop detection during CFG parsing. The feature structures prevent loops in unification, and we considered including certain grammatical features into backbone extraction as done in (Briscoe and Carroll, 1994). However, in the TRIPS grammar the feature values responsible for breaking loops belonged to multi-valued features (6 valued in the worst case), with values which may depend on other multiple-valued features in daughter constituents. Thus adding the extra features resulted in major backbone size increases because of category splitting. This can be remedied with additional pre-compilation (Kiefer and Krieger, 2004), however, this requires that all lexical entries be known in advance. One nice feature of the TRIPS lexicon is that it includes a mechanism for dynamically adding lexical entries fo</context>
</contexts>
<marker>Briscoe, Carroll, 1994</marker>
<rawString>T. Briscoe and J. Carroll. 1994. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
</authors>
<title>Relating complexity to practical performance in parsing with wide-coverage unification grammars.</title>
<date>1994</date>
<booktitle>In Proceedings ofACL-2004.</booktitle>
<contexts>
<context position="3284" citStr="Carroll (1994)" startWordPosition="491" endWordPosition="492">ep interpretation systems (Copestake and Flickinger, 2000; Swift et al., 2004; Maxwell and Kaplan, 1994), which forms the foundation that we build on in this work. For example, techniques for speeding up unification in HPSG lead to dramatic improvements in efficiency (Kiefer et al., 1999). Likewise ambiguity packing and CFG backbone parsing (Maxwell and Kaplan, 1994; van Noord, 1997) are known to increase parsing efficiency. However, as we show in this paper, these techniques depend on specific grammar properties that do not hold for all grammars. This claim is consistent with observations of Carroll (1994) that parsing software optimisation techniques tend to be limited in their applicability to the individual grammars they were developed for. While we used TRIPS as our example unification-based grammar, this investigation is important not only for this project, but in the general context of speeding up a wide-coverage unification grammar which incorporates fragment 9 Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 9–16, New York City, June 2006. c�2006 Association for Computational Linguistics rules and lexical semantics, which may not be immediately provided </context>
<context position="6207" citStr="Carroll, 1994" startWordPosition="940" endWordPosition="942">IPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a feature actively used in interpretation. While TRIPS provides the capabilities we need, its parse times for long sentences (above 15 words long) are intolerably long. We considered two possible techniques for speeding up parsing: speeding up unification using the techniques similar to the LINGO system (Copestake and Flickinger, 2000), or using backbone extraction (Maxwell and Kaplan, 1994; Ros´e and Lavie, 2001; Briscoe and Carroll, 1994). TRIPS already uses a fast unification algorithm similar to quasi-destructive unification, avoiding copying during unification.&apos; However, the TRIPS grammar retains the notion of phrase structure, and thus it was more natural to chose to use backbone extraction with ambiguity packing to speed up the parsing. As a foundation for our optimisation work, we started with the freely available LCFLEX parser (Ros´e and Lavie, 2001). LCFLEX is an all-paths parser that uses left-corner prediction and ambiguity packing to make all-paths parsing tractable, and which was shown to be efficient for long sent</context>
<context position="9974" citStr="Carroll (1994)" startWordPosition="1550" endWordPosition="1551">con (Kipper et al., 2000). Checking semantic restrictions is an integral part of TRIPS parsing, and removing them significantly decreases speed and increases ambiguity of the TRIPS parser (Dzikovska, 2004). We show that it also has an impact on parsing with a CFG backbone in Section 4.1. 3.2 LCFLEX The LCFLEX parser (Ros´e and Lavie, 2001) is an all-paths robust left corner chart parser designed to incorporate various robustness techniques such as word skipping, flexible unification, and constituent insertion. Its left corner chart parsing algorithm is similar to that described by Briscoe and Carroll (1994). The system supports grammatical specification in a unification framework that consists of context-free grammar rules augmented with feature bundles associated with the non-terminals of the rules. LCFLEX can be used in two parsing modes: either context-free parsing can be done first, followed by applying the unification rules, or unification can be done interleaved with context-free parsing. The context free backbone allows for efficient left corner predictions using a pre-compiled left corner prediction table, such as that described in (van Noord, 1997). To enhance its efficiency, it incorpo</context>
<context position="14308" citStr="Carroll, 1994" startWordPosition="2241" endWordPosition="2242">t of unification rules with each CFG production, which are reapplied at a later stage. To be able to apply a unification rule corresponding to N—* N production, it has to be explicitly present in the chart, leading to an infinite number of N constituents produced. Applying the extra CFG rules expanding the loops during re-unification would complicate the algorithm significantly. Instead, we implemented loop detection during CFG parsing. The feature structures prevent loops in unification, and we considered including certain grammatical features into backbone extraction as done in (Briscoe and Carroll, 1994). However, in the TRIPS grammar the feature values responsible for breaking loops belonged to multi-valued features (6 valued in the worst case), with values which may depend on other multiple-valued features in daughter constituents. Thus adding the extra features resulted in major backbone size increases because of category splitting. This can be remedied with additional pre-compilation (Kiefer and Krieger, 2004), however, this requires that all lexical entries be known in advance. One nice feature of the TRIPS lexicon is that it includes a mechanism for dynamically adding lexical entries fo</context>
</contexts>
<marker>Carroll, 1994</marker>
<rawString>J. Carroll. 1994. Relating complexity to practical performance in parsing with wide-coverage unification grammars. In Proceedings ofACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>An open source grammar development environment and broadcoverage English grammar using HPSG.</title>
<date>2000</date>
<booktitle>In Proceedings ofLREC-2000,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="1704" citStr="Copestake and Flickinger, 2000" startWordPosition="246" endWordPosition="249">ng its speed and coverage for longer utterances. While typical sentences in dialogue contexts are shorter than in expository text domains, longer utterances are important in discussion oriented domains. For example, in educational applications of dialogue it is important to elicit deep explanation from students and then offer focused feedback based on the details of what students say. The choice of instructional dialogue as a target application influenced the choice of parser we needed to use for interpretation in a dialogue system. Several deep, wide-coverage parsers are currently available (Copestake and Flickinger, 2000; Ros´e, 2000; Baldridge, 2002; Maxwell and Kaplan, 1994), but many of these have not been designed with issues related to interpretation in a dialogue context in mind. The TRIPS grammar (Dzikovska et al., 2005) is a wide-coverage unification grammar that has been used very successfully in several task-oriented dialogue systems. It supports interpretation of fragments and lexical semantic features (see Section 2 for a more detailed discussion), and provides additional robustness through “robust” rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect ag</context>
<context position="4857" citStr="Copestake and Flickinger, 2000" startWordPosition="733" endWordPosition="736"> parsing algorithm. We conclude with discussion of some implications for implementing grammars that build deep syntactic and semantic representations. 2 Motivation The work reported in this paper was done as part of the process of developing a dialogue system that incorporates deep natural language understanding. We needed a grammar that provides lexical semantic interpretation, supports parsing fragmentary utterance in dialogue, and could be used to start development without large quantities of corpus data. TRIPS fulfilled our requirements better than similar alternatives, such as LINGO ERG (Copestake and Flickinger, 2000) or XLE (Maxwell and Kaplan, 1994). TRIPS produces logical forms which include semantic classes and roles in a domain-independent frame-based formalism derived from FrameNet and VerbNet (Dzikovska et al., 2004; Kipper et al., 2000). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semant</context>
<context position="6100" citStr="Copestake and Flickinger, 2000" startWordPosition="921" endWordPosition="924">gnments was more important to interpretation, and these features are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a feature actively used in interpretation. While TRIPS provides the capabilities we need, its parse times for long sentences (above 15 words long) are intolerably long. We considered two possible techniques for speeding up parsing: speeding up unification using the techniques similar to the LINGO system (Copestake and Flickinger, 2000), or using backbone extraction (Maxwell and Kaplan, 1994; Ros´e and Lavie, 2001; Briscoe and Carroll, 1994). TRIPS already uses a fast unification algorithm similar to quasi-destructive unification, avoiding copying during unification.&apos; However, the TRIPS grammar retains the notion of phrase structure, and thus it was more natural to chose to use backbone extraction with ambiguity packing to speed up the parsing. As a foundation for our optimisation work, we started with the freely available LCFLEX parser (Ros´e and Lavie, 2001). LCFLEX is an all-paths parser that uses left-corner prediction a</context>
</contexts>
<marker>Copestake, Flickinger, 2000</marker>
<rawString>A. Copestake and D. Flickinger. 2000. An open source grammar development environment and broadcoverage English grammar using HPSG. In Proceedings ofLREC-2000, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M O Dzikovska</author>
<author>M D Swift</author>
<author>J F Allen</author>
</authors>
<title>Building a computational lexicon and ontology with framenet.</title>
<date>2004</date>
<booktitle>In LREC workshop on Building Lexical Resources from Semantically Annotated Corpora.</booktitle>
<contexts>
<context position="5066" citStr="Dzikovska et al., 2004" startWordPosition="764" endWordPosition="767">e process of developing a dialogue system that incorporates deep natural language understanding. We needed a grammar that provides lexical semantic interpretation, supports parsing fragmentary utterance in dialogue, and could be used to start development without large quantities of corpus data. TRIPS fulfilled our requirements better than similar alternatives, such as LINGO ERG (Copestake and Flickinger, 2000) or XLE (Maxwell and Kaplan, 1994). TRIPS produces logical forms which include semantic classes and roles in a domain-independent frame-based formalism derived from FrameNet and VerbNet (Dzikovska et al., 2004; Kipper et al., 2000). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more important to interpretation, and these features are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules </context>
</contexts>
<marker>Dzikovska, Swift, Allen, 2004</marker>
<rawString>M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004. Building a computational lexicon and ontology with framenet. In LREC workshop on Building Lexical Resources from Semantically Annotated Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dzikovska</author>
<author>M Swift</author>
<author>J Allen</author>
<author>W de Beaumont</author>
</authors>
<title>Generic parsing for multi-domain semantic interpretation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT-05).</booktitle>
<marker>Dzikovska, Swift, Allen, de Beaumont, 2005</marker>
<rawString>M. Dzikovska, M. Swift, J. Allen, and W. de Beaumont. 2005. Generic parsing for multi-domain semantic interpretation. In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M O Dzikovska</author>
</authors>
<title>A Practical Semantic Representation For Natural Language Parsing.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="9565" citStr="Dzikovska, 2004" startWordPosition="1484" endWordPosition="1485">IPS lexicon is associated with a semantic type from a domain-independent ontology. This enables word sense disambiguation and semantic role labelling for the logical form produced by the grammar. Multiple word senses result in additional ambiguity on top of syntactic ambiguity, but it is controlled in part with the use of weak selectional restrictions, similar to the restrictions employed by the VerbNet lexicon (Kipper et al., 2000). Checking semantic restrictions is an integral part of TRIPS parsing, and removing them significantly decreases speed and increases ambiguity of the TRIPS parser (Dzikovska, 2004). We show that it also has an impact on parsing with a CFG backbone in Section 4.1. 3.2 LCFLEX The LCFLEX parser (Ros´e and Lavie, 2001) is an all-paths robust left corner chart parser designed to incorporate various robustness techniques such as word skipping, flexible unification, and constituent insertion. Its left corner chart parsing algorithm is similar to that described by Briscoe and Carroll (1994). The system supports grammatical specification in a unification framework that consists of context-free grammar rules augmented with feature bundles associated with the non-terminals of the </context>
</contexts>
<marker>Dzikovska, 2004</marker>
<rawString>M. O. Dzikovska. 2004. A Practical Semantic Representation For Natural Language Parsing. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>M Swift</author>
<author>J Allen</author>
<author>D Gildea</author>
</authors>
<title>Online statistics for a unification-based dialogue parser.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT-05).</booktitle>
<contexts>
<context position="7559" citStr="Elsner et al., 2005" startWordPosition="1153" endWordPosition="1156">table for the ambiguity level in the TRIPS grammar, but that by introducing a pruning method that uses ambiguity packing to guide pruning decisions, we can achieve significant improvements in both speed and coverage compared to the original TRIPS parser. 3 The TRIPS and LCFLEX algorithms 3.1 The TRIPS parser The TRIPS parser we use as a baseline is a bottomup chart parser with lexical entries and rules represented as attribute-value structures. To achieve parsing efficiency, TRIPS uses a best-first beam search algorithm based on the scores from a parse selection model (Dzikovska et al., 2005; Elsner et al., 2005). The constituents on the parser’s agenda are grouped into buckets based on their scores. At each step, the bucket with the highest scoring constituents is selected to build/extend chart edges. The parsing stops once N requested analyses are found. This guarantees that the parser returns the N-best list of analyses according to the parse selection model used, unless the parser reaches the chart size limit. &apos;Other enhancements used by LINGO depend on disallowing disjunctive features, and relying instead on the type system. The TRIPS grammar is untyped and uses disjunctive features, and converti</context>
</contexts>
<marker>Elsner, Swift, Allen, Gildea, 2005</marker>
<rawString>M. Elsner, M. Swift, J. Allen, and D. Gildea. 2005. Online statistics for a unification-based dialogue parser. In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Frank</author>
<author>B Kiefer</author>
<author>B Crysmann</author>
<author>M Becker</author>
<author>U Schafer</author>
</authors>
<title>Integrated shallow and deep parsing: TopP meets HPSG.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="8426" citStr="Frank et al., 2003" startWordPosition="1295" endWordPosition="1298"> This guarantees that the parser returns the N-best list of analyses according to the parse selection model used, unless the parser reaches the chart size limit. &apos;Other enhancements used by LINGO depend on disallowing disjunctive features, and relying instead on the type system. The TRIPS grammar is untyped and uses disjunctive features, and converting it to a typed system would require as yet undetermined amount of additional work. 10 In addition to best-first parsing, the TRIPS parser uses a chart size limit, to prevent the parser from running too long on unparseable utterances, similar to (Frank et al., 2003). TRIPS is much slower processing utterances not covered in the grammar, because it continues its search until it reaches the chart limit. Thus, a lower chart limit improves parsing efficiency. However, we show in our evaluation that the chart limit necessary to obtain good performance in most cases is too low to find parses for utterances with 15 or more words, even if they are covered by the grammar. The integration of lexical semantics in the TRIPS lexicon has a major impact on parsing in TRIPS. Each word in the TRIPS lexicon is associated with a semantic type from a domain-independent onto</context>
</contexts>
<marker>Frank, Kiefer, Crysmann, Becker, Schafer, 2003</marker>
<rawString>A. Frank, B. Kiefer, B. Crysmann, M. Becker, and U. Schafer. 2003. Integrated shallow and deep parsing: TopP meets HPSG. In Proceedings ofACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer</author>
<author>H-U Krieger</author>
</authors>
<title>A context-free approximation of head-driven phrase structure grammar. In</title>
<date>2004</date>
<booktitle>New Developments in Parsing Technology.</booktitle>
<editor>H. Bunt, J. Carroll, and G. Satta, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="14726" citStr="Kiefer and Krieger, 2004" startWordPosition="2303" endWordPosition="2306">oop detection during CFG parsing. The feature structures prevent loops in unification, and we considered including certain grammatical features into backbone extraction as done in (Briscoe and Carroll, 1994). However, in the TRIPS grammar the feature values responsible for breaking loops belonged to multi-valued features (6 valued in the worst case), with values which may depend on other multiple-valued features in daughter constituents. Thus adding the extra features resulted in major backbone size increases because of category splitting. This can be remedied with additional pre-compilation (Kiefer and Krieger, 2004), however, this requires that all lexical entries be known in advance. One nice feature of the TRIPS lexicon is that it includes a mechanism for dynamically adding lexical entries for unknown words from wide-coverage lexicons such as VerbNet (Kipper et al., 2000), which would be impractical to use in precompilation. Therefore, to use CFG parsing before unification in our system, we implemented a loop detector that checked the CFG structure to disallow loops. However, the next problem that we encountered is massive ambiguity in the CFG structure. Even a very short phrase such as “a train” had o</context>
</contexts>
<marker>Kiefer, Krieger, 2004</marker>
<rawString>B. Kiefer and H.-U. Krieger. 2004. A context-free approximation of head-driven phrase structure grammar. In H. Bunt, J. Carroll, and G. Satta, editors, New Developments in Parsing Technology. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer</author>
<author>H Krieger</author>
<author>J Carroll</author>
<author>R Malouf</author>
</authors>
<title>Bag of useful techniques for efficient and robust parsing.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="2959" citStr="Kiefer et al., 1999" startWordPosition="437" endWordPosition="440">rsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, a common concern in building grammars for robust parsing (Schneider and McCoy, 1998). It is specifically these robustness-efficiency trade-offs that we address in this paper. Much work has been done related to enhancing the efficiency of deep interpretation systems (Copestake and Flickinger, 2000; Swift et al., 2004; Maxwell and Kaplan, 1994), which forms the foundation that we build on in this work. For example, techniques for speeding up unification in HPSG lead to dramatic improvements in efficiency (Kiefer et al., 1999). Likewise ambiguity packing and CFG backbone parsing (Maxwell and Kaplan, 1994; van Noord, 1997) are known to increase parsing efficiency. However, as we show in this paper, these techniques depend on specific grammar properties that do not hold for all grammars. This claim is consistent with observations of Carroll (1994) that parsing software optimisation techniques tend to be limited in their applicability to the individual grammars they were developed for. While we used TRIPS as our example unification-based grammar, this investigation is important not only for this project, but in the ge</context>
<context position="27574" citStr="Kiefer et al., 1999" startWordPosition="4432" endWordPosition="4435">s CFG parsing not interleaved with unification less attractive in cases where we want to avoid expensive CFG precompilation. Moreover, as we found with the TRIPS grammar, in the context of robust parsing with lexical semantics the ambiguity in a CFG backbone grows large enough to make CFG parsing followed by unification inefficient. We described an alternative technique that uses pruning based on a parse selection model. Another option for speeding up parsing that we have not discussed in detail is using a typed grammar without disjunction and speeding up unification as done in HPSG grammars (Kiefer et al., 1999). In order to do this, we must first address the issue of integrating the type of lexical semantics that we require with HPSG’s type system. Adding lexical semantics while retaining the speed benefits obtained through this type system would require that the semantic type ontology be expressed in the same formalism as the syntactic types. We plan to further explore this option in our future work. Though longer sentences were relatively rare in our test set, using the system in an educational domain (our ultimate goal) means that the longer sentences are particularly important, because they ofte</context>
</contexts>
<marker>Kiefer, Krieger, Carroll, Malouf, 1999</marker>
<rawString>B. Kiefer, H. Krieger, J. Carroll, and R. Malouf. 1999. Bag of useful techniques for efficient and robust parsing. In Proceedings ofACL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>Classbased construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of the 7th Conference on Artificial Intelligence and of the 12th Conference on Innovative Applications ofArtificial Intelligence.</booktitle>
<contexts>
<context position="5088" citStr="Kipper et al., 2000" startWordPosition="768" endWordPosition="771">a dialogue system that incorporates deep natural language understanding. We needed a grammar that provides lexical semantic interpretation, supports parsing fragmentary utterance in dialogue, and could be used to start development without large quantities of corpus data. TRIPS fulfilled our requirements better than similar alternatives, such as LINGO ERG (Copestake and Flickinger, 2000) or XLE (Maxwell and Kaplan, 1994). TRIPS produces logical forms which include semantic classes and roles in a domain-independent frame-based formalism derived from FrameNet and VerbNet (Dzikovska et al., 2004; Kipper et al., 2000). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more important to interpretation, and these features are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting disco</context>
<context position="9385" citStr="Kipper et al., 2000" startWordPosition="1456" endWordPosition="1459">es with 15 or more words, even if they are covered by the grammar. The integration of lexical semantics in the TRIPS lexicon has a major impact on parsing in TRIPS. Each word in the TRIPS lexicon is associated with a semantic type from a domain-independent ontology. This enables word sense disambiguation and semantic role labelling for the logical form produced by the grammar. Multiple word senses result in additional ambiguity on top of syntactic ambiguity, but it is controlled in part with the use of weak selectional restrictions, similar to the restrictions employed by the VerbNet lexicon (Kipper et al., 2000). Checking semantic restrictions is an integral part of TRIPS parsing, and removing them significantly decreases speed and increases ambiguity of the TRIPS parser (Dzikovska, 2004). We show that it also has an impact on parsing with a CFG backbone in Section 4.1. 3.2 LCFLEX The LCFLEX parser (Ros´e and Lavie, 2001) is an all-paths robust left corner chart parser designed to incorporate various robustness techniques such as word skipping, flexible unification, and constituent insertion. Its left corner chart parsing algorithm is similar to that described by Briscoe and Carroll (1994). The syste</context>
<context position="14989" citStr="Kipper et al., 2000" startWordPosition="2348" endWordPosition="2351">e for breaking loops belonged to multi-valued features (6 valued in the worst case), with values which may depend on other multiple-valued features in daughter constituents. Thus adding the extra features resulted in major backbone size increases because of category splitting. This can be remedied with additional pre-compilation (Kiefer and Krieger, 2004), however, this requires that all lexical entries be known in advance. One nice feature of the TRIPS lexicon is that it includes a mechanism for dynamically adding lexical entries for unknown words from wide-coverage lexicons such as VerbNet (Kipper et al., 2000), which would be impractical to use in precompilation. Therefore, to use CFG parsing before unification in our system, we implemented a loop detector that checked the CFG structure to disallow loops. However, the next problem that we encountered is massive ambiguity in the CFG structure. Even a very short phrase such as “a train” had over 700 possible CFG analyses, and took 910 msec to parse compared to 10 msec with interleaved unification. CFG ambiguity is so high because noun phrase fragments are allowed as top-level categories, and lexical ambiguity is compounded with semantic ambiguity and</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>K. Kipper, H. T. Dang, and M. Palmer. 2000. Classbased construction of a verb lexicon. In Proceedings of the 7th Conference on Artificial Intelligence and of the 12th Conference on Innovative Applications ofArtificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>C P Ros´e</author>
</authors>
<title>Optimal ambiguity packing in context-free parsers with interleaved unification.</title>
<date>2004</date>
<editor>In H. Bunt, J. Carroll, and G. Satta, editors, Current</editor>
<publisher>Kluwer Academic Press.</publisher>
<marker>Lavie, Ros´e, 2004</marker>
<rawString>A. Lavie and C. P. Ros´e. 2004. Optimal ambiguity packing in context-free parsers with interleaved unification. In H. Bunt, J. Carroll, and G. Satta, editors, Current Issues in Parsing Technologies. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Maxwell</author>
<author>R M Kaplan</author>
</authors>
<title>The interface between phrasal and functional constraints.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="1761" citStr="Maxwell and Kaplan, 1994" startWordPosition="254" endWordPosition="257"> sentences in dialogue contexts are shorter than in expository text domains, longer utterances are important in discussion oriented domains. For example, in educational applications of dialogue it is important to elicit deep explanation from students and then offer focused feedback based on the details of what students say. The choice of instructional dialogue as a target application influenced the choice of parser we needed to use for interpretation in a dialogue system. Several deep, wide-coverage parsers are currently available (Copestake and Flickinger, 2000; Ros´e, 2000; Baldridge, 2002; Maxwell and Kaplan, 1994), but many of these have not been designed with issues related to interpretation in a dialogue context in mind. The TRIPS grammar (Dzikovska et al., 2005) is a wide-coverage unification grammar that has been used very successfully in several task-oriented dialogue systems. It supports interpretation of fragments and lexical semantic features (see Section 2 for a more detailed discussion), and provides additional robustness through “robust” rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement. These enhancements help parsing dialogue (both s</context>
<context position="3038" citStr="Maxwell and Kaplan, 1994" startWordPosition="448" endWordPosition="451">mmar ambiguity, a common concern in building grammars for robust parsing (Schneider and McCoy, 1998). It is specifically these robustness-efficiency trade-offs that we address in this paper. Much work has been done related to enhancing the efficiency of deep interpretation systems (Copestake and Flickinger, 2000; Swift et al., 2004; Maxwell and Kaplan, 1994), which forms the foundation that we build on in this work. For example, techniques for speeding up unification in HPSG lead to dramatic improvements in efficiency (Kiefer et al., 1999). Likewise ambiguity packing and CFG backbone parsing (Maxwell and Kaplan, 1994; van Noord, 1997) are known to increase parsing efficiency. However, as we show in this paper, these techniques depend on specific grammar properties that do not hold for all grammars. This claim is consistent with observations of Carroll (1994) that parsing software optimisation techniques tend to be limited in their applicability to the individual grammars they were developed for. While we used TRIPS as our example unification-based grammar, this investigation is important not only for this project, but in the general context of speeding up a wide-coverage unification grammar which incorpor</context>
<context position="4891" citStr="Maxwell and Kaplan, 1994" startWordPosition="739" endWordPosition="742">cussion of some implications for implementing grammars that build deep syntactic and semantic representations. 2 Motivation The work reported in this paper was done as part of the process of developing a dialogue system that incorporates deep natural language understanding. We needed a grammar that provides lexical semantic interpretation, supports parsing fragmentary utterance in dialogue, and could be used to start development without large quantities of corpus data. TRIPS fulfilled our requirements better than similar alternatives, such as LINGO ERG (Copestake and Flickinger, 2000) or XLE (Maxwell and Kaplan, 1994). TRIPS produces logical forms which include semantic classes and roles in a domain-independent frame-based formalism derived from FrameNet and VerbNet (Dzikovska et al., 2004; Kipper et al., 2000). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more impor</context>
<context position="6156" citStr="Maxwell and Kaplan, 1994" startWordPosition="929" endWordPosition="933">s are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a feature actively used in interpretation. While TRIPS provides the capabilities we need, its parse times for long sentences (above 15 words long) are intolerably long. We considered two possible techniques for speeding up parsing: speeding up unification using the techniques similar to the LINGO system (Copestake and Flickinger, 2000), or using backbone extraction (Maxwell and Kaplan, 1994; Ros´e and Lavie, 2001; Briscoe and Carroll, 1994). TRIPS already uses a fast unification algorithm similar to quasi-destructive unification, avoiding copying during unification.&apos; However, the TRIPS grammar retains the notion of phrase structure, and thus it was more natural to chose to use backbone extraction with ambiguity packing to speed up the parsing. As a foundation for our optimisation work, we started with the freely available LCFLEX parser (Ros´e and Lavie, 2001). LCFLEX is an all-paths parser that uses left-corner prediction and ambiguity packing to make all-paths parsing tractable</context>
</contexts>
<marker>Maxwell, Kaplan, 1994</marker>
<rawString>J. T. Maxwell and R. M. Kaplan. 1994. The interface between phrasal and functional constraints. Computational Linguistics, 19(4):571–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>S Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics (COLING 2004),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="5235" citStr="Narayanan and Harabagiu, 2004" startWordPosition="790" endWordPosition="793">n, supports parsing fragmentary utterance in dialogue, and could be used to start development without large quantities of corpus data. TRIPS fulfilled our requirements better than similar alternatives, such as LINGO ERG (Copestake and Flickinger, 2000) or XLE (Maxwell and Kaplan, 1994). TRIPS produces logical forms which include semantic classes and roles in a domain-independent frame-based formalism derived from FrameNet and VerbNet (Dzikovska et al., 2004; Kipper et al., 2000). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more important to interpretation, and these features are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a feature actively used in interpretation. While TRIPS provides the capa</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>S. Narayanan and S. Harabagiu. 2004. Question answering based on semantic structures. In Proceedings of International Conference on Computational Linguistics (COLING 2004), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Ambiguity packing in constraint-based parsing - practical results.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL’00.</booktitle>
<contexts>
<context position="15936" citStr="Oepen and Carroll, 2000" startWordPosition="2500" endWordPosition="2503">a train” had over 700 possible CFG analyses, and took 910 msec to parse compared to 10 msec with interleaved unification. CFG ambiguity is so high because noun phrase fragments are allowed as top-level categories, and lexical ambiguity is compounded with semantic ambiguity and robust rules normally disallowed by features during unification. Thus, in our combined algorithm we had to use unification interleaved with parsing to filter out the CFG constituents. 4.2 Ambiguity Packing For building semantic representations in parallel with parsing, ambiguity packing presents a set of known problems (Oepen and Carroll, 2000). One possible solution is to exclude semantic features during an initial unification stage, use ambiguity packing, and re-unify with semantic features in a postprocessing stage. In our case, we found this strategy difficult to implement, since selectional restrictions are used to limit the ambiguity created by multiple word senses during syntactic parsing. Therefore, we chose to do ambiguity packing on the CFG structure only, keeping the multiple feature structures associated with each packed CFG constituent. To begin to evaluate the contribution of ambiguity packing on efficiency, we ran a t</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>S. Oepen and J. Carroll. 2000. Ambiguity packing in constraint-based parsing - practical results. In Proceedings ofNAACL’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
<author>A Lavie</author>
</authors>
<title>Balancing robustness and efficiency in unification-augmented context-free parsers for large practical applications.</title>
<date>2001</date>
<booktitle>Robustness in Language and Speech Technology.</booktitle>
<editor>In J. Junqua and G. Van Noord, editors,</editor>
<publisher>Kluwer Academic Press.</publisher>
<marker>Ros´e, Lavie, 2001</marker>
<rawString>C. P. Ros´e and A. Lavie. 2001. Balancing robustness and efficiency in unification-augmented context-free parsers for large practical applications. In J. Junqua and G. Van Noord, editors, Robustness in Language and Speech Technology. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ros´e</author>
</authors>
<title>A framework for robust semantic interpretation.</title>
<date>2000</date>
<booktitle>In Proceedings 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Ros´e, 2000</marker>
<rawString>C. Ros´e. 2000. A framework for robust semantic interpretation. In Proceedings 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schlangen</author>
<author>A Lascarides</author>
</authors>
<title>The interpretation of non-sentential utterances in dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Japan,</location>
<contexts>
<context position="5761" citStr="Schlangen and Lascarides, 2003" startWordPosition="870" endWordPosition="873">e helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more important to interpretation, and these features are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a feature actively used in interpretation. While TRIPS provides the capabilities we need, its parse times for long sentences (above 15 words long) are intolerably long. We considered two possible techniques for speeding up parsing: speeding up unification using the techniques similar to the LINGO system (Copestake and Flickinger, 2000), or using backbone extraction (Maxwell and Kaplan, 1994; Ros´e and Lavie, 2001; Briscoe and Carroll, 1994). TRIPS already uses a fast unification algorithm similar to quasi-destructive unification, avoiding copying during unification.&apos; However, the TRIPS gramm</context>
</contexts>
<marker>Schlangen, Lascarides, 2003</marker>
<rawString>D. Schlangen and A. Lascarides. 2003. The interpretation of non-sentential utterances in dialogue. In Proceedings of the 4th SIGdial Workshop on Discourse and Dialogue, Japan, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schneider</author>
<author>K F McCoy</author>
</authors>
<title>Recognizing syntactic errors in the writing of second language learners.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98.</booktitle>
<contexts>
<context position="2514" citStr="Schneider and McCoy, 1998" startWordPosition="367" endWordPosition="370">ar (Dzikovska et al., 2005) is a wide-coverage unification grammar that has been used very successfully in several task-oriented dialogue systems. It supports interpretation of fragments and lexical semantic features (see Section 2 for a more detailed discussion), and provides additional robustness through “robust” rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement. These enhancements help parsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, a common concern in building grammars for robust parsing (Schneider and McCoy, 1998). It is specifically these robustness-efficiency trade-offs that we address in this paper. Much work has been done related to enhancing the efficiency of deep interpretation systems (Copestake and Flickinger, 2000; Swift et al., 2004; Maxwell and Kaplan, 1994), which forms the foundation that we build on in this work. For example, techniques for speeding up unification in HPSG lead to dramatic improvements in efficiency (Kiefer et al., 1999). Likewise ambiguity packing and CFG backbone parsing (Maxwell and Kaplan, 1994; van Noord, 1997) are known to increase parsing efficiency. However, as we </context>
</contexts>
<marker>Schneider, McCoy, 1998</marker>
<rawString>D. Schneider and K. F. McCoy. 1998. Recognizing syntactic errors in the writing of second language learners. In Proceedings of COLING-ACL’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swift</author>
<author>J Allen</author>
<author>D Gildea</author>
</authors>
<title>Skeletons in the parser: Using a shallow parser to improve deep parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04.</booktitle>
<contexts>
<context position="2747" citStr="Swift et al., 2004" startWordPosition="402" endWordPosition="405">e detailed discussion), and provides additional robustness through “robust” rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement. These enhancements help parsing dialogue (both spoken and typed), but they significantly increase grammar ambiguity, a common concern in building grammars for robust parsing (Schneider and McCoy, 1998). It is specifically these robustness-efficiency trade-offs that we address in this paper. Much work has been done related to enhancing the efficiency of deep interpretation systems (Copestake and Flickinger, 2000; Swift et al., 2004; Maxwell and Kaplan, 1994), which forms the foundation that we build on in this work. For example, techniques for speeding up unification in HPSG lead to dramatic improvements in efficiency (Kiefer et al., 1999). Likewise ambiguity packing and CFG backbone parsing (Maxwell and Kaplan, 1994; van Noord, 1997) are known to increase parsing efficiency. However, as we show in this paper, these techniques depend on specific grammar properties that do not hold for all grammars. This claim is consistent with observations of Carroll (1994) that parsing software optimisation techniques tend to be limit</context>
<context position="21406" citStr="Swift et al., 2004" startWordPosition="3389" endWordPosition="3392">zation features, the same locality principle holds. This heuristic has perfect precision though not complete recall, but, as our evaluation shows, it is sufficient to significantly improve performance in comparison with the TRIPS parser. 5 Evaluation The purpose of our evaluation is to explore the extent to which we can achieve a better balance between parse time and coverage using backbone parsing with pruning compared to the original best-first algorithm. For our comparison we used an excerpt from the Monroe corpus that has been used in previous TRIPS research on parsing speed and accuracy (Swift et al., 2004) consisting of dialogues s2, s4, s16 and s17. Dialogue s2 was a hold out set used for pilot testing and setting parameters. The other three dialogues were set aside for testing. Altogether, the test set contained 1042 utterances, ranging from 1 to 13 45 words in length (mean 5.38 words/utt, st. dev. 5.7 words/utt). Using our hold-out set, we determined that a beam width of three was an optimal setting. Thus, we compared TFLEX using a beam width of 3 to three different versions of TRIPS that varied only in terms of the maximum chart size, giving us a version that is significantly faster than TF</context>
</contexts>
<marker>Swift, Allen, Gildea, 2004</marker>
<rawString>M. Swift, J. Allen, and D. Gildea. 2004. Skeletons in the parser: Using a shallow parser to improve deep parsing. In Proceedings of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
</authors>
<title>Empirical Evaluations of Pronoun Resolution.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="5170" citStr="Tetreault, 2005" startWordPosition="783" endWordPosition="784">rammar that provides lexical semantic interpretation, supports parsing fragmentary utterance in dialogue, and could be used to start development without large quantities of corpus data. TRIPS fulfilled our requirements better than similar alternatives, such as LINGO ERG (Copestake and Flickinger, 2000) or XLE (Maxwell and Kaplan, 1994). TRIPS produces logical forms which include semantic classes and roles in a domain-independent frame-based formalism derived from FrameNet and VerbNet (Dzikovska et al., 2004; Kipper et al., 2000). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). Apart from TRIPS, these have not been integrated into existing deep grammars. While both LINGO-ERG and XLE include semantic features related to scoping, in our applications the availability of semantic classes and semantic role assignments was more important to interpretation, and these features are not currently available from those parsers. Finally, TRIPS provides a domainindependent parse selection model, as well as rules for interpreting discourse fragments (as was also done in HPSG (Schlangen and Lascarides, 2003), a featu</context>
</contexts>
<marker>Tetreault, 2005</marker>
<rawString>J. Tetreault. 2005. Empirical Evaluations of Pronoun Resolution. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>An efficient implementation of the head-corner parser.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>van Noord, 1997</marker>
<rawString>G. van Noord. 1997. An efficient implementation of the head-corner parser. Computational Linguistics, 23(3):425–456.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>