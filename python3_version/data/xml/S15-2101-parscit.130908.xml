<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000241">
<title confidence="0.9984295">
Swiss-Chocolate: Combining Flipout Regularization and Random Forests
with Artificially Built Subsystems to Boost Text-Classification for Sentiment
</title>
<author confidence="0.986544">
Fatih Uzdilli
</author>
<affiliation confidence="0.837114">
Zurich University of Applied Sciences
Switzerland
</affiliation>
<email confidence="0.958072">
uzdi@zhaw.ch
</email>
<author confidence="0.996192">
Pascal Julmy
</author>
<affiliation confidence="0.8362025">
Zurich University of Applied Sciences
Switzerland
</affiliation>
<email confidence="0.992744">
pascal.julmy@gmail.com
</email>
<author confidence="0.699117">
Martin Jaggi
</author>
<affiliation confidence="0.3753325">
ETH Zurich
Switzerland
</affiliation>
<email confidence="0.868653">
jaggi@inf.ethz.ch
</email>
<author confidence="0.9975">
Leon Derczynski
</author>
<affiliation confidence="0.999531">
University of Sheffield
</affiliation>
<address confidence="0.648411">
UK
</address>
<email confidence="0.99595">
leon@dcs.shef.ac.uk
</email>
<author confidence="0.970137">
Dominic Egger
</author>
<affiliation confidence="0.819098">
Zurich University of Applied Sciences
Switzerland
</affiliation>
<email confidence="0.92679">
dominicegger@bluewin.ch
</email>
<author confidence="0.997528">
Mark Cieliebak
</author>
<affiliation confidence="0.8360245">
Zurich University of Applied Sciences
Switzerland
</affiliation>
<email confidence="0.993363">
ciel@zhaw.ch
</email>
<sectionHeader confidence="0.995573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999631733333333">
We describe a classifier for predicting
message-level sentiment of English micro-
blog messages from Twitter. This paper
describes our submission to the SemEval-
2015 competition (Task 10). Our approach is
to combine several variants of our previous
year’s SVM system into one meta-classifier,
which was then trained using a random forest.
The main idea is that the meta-classifier
allows the combination of the strengths and
overcome some of the weaknesses of the
artificially-built individual classifiers, and
adds additional non-linearity. We were also
able to improve the linear classifiers by using
a new regularization technique we call flipout.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999788545454545">
With the availability of huge amounts of user gener-
ated text online, the interest in automatic sentiment
analysis of text has greatly increased recently in both
academia and industry.
The goal is to classify a tweet (on the full mes-
sage level) into the three classes positive, negative,
and neutral. In this paper, we describe our approach
using a modified SVM based classifier on short text
as in Twitter messages. Our system has participated
in the SemEval-2015 Task 10 competition, “Senti-
ment Analysis in Twitter, Subtask–B Message Po-
</bodyText>
<footnote confidence="0.69182425">
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.99470353125">
larity Classification” (Rosenthal et al., 2015). Previ-
ous iterations of the evaluation were run in 2013 and
2014.
Our Results in the Competition. Our system was
ranked 8th out of 40 participants, with an F1-score
of 62.61 on the Twitter-2015 test set. The 2015 win-
ning team obtained an average F1-score of 64.84.
The detailed rankings of our approach were: 4th
rank on the LiveJournal data; 6th on the SMS data
(2013); 10th on Twitter-2013; 12th on Twitter-2014;
and 25th on Twitter Sarcasm. See (Rosenthal et al.,
2015) for full details and all results.
Data. In the competition, tweets for training and
development were provided as tweet IDs. A frac-
tion (10-15%) of the tweets were no longer avail-
able on Twitter, which made results of the compe-
tition not fully comparable. For testing, in addition
to last year’s data (tweets, SMS, LiveJournal), new
tweets were provided. An overview of the data that
we were able to download is given in Table 1.
Our Approach. Our system is based on two main
ideas. First, we propose a new regularization tech-
nique called flipout, which post-processes a trained
classifier model for better generalization perfor-
mance. Details of this are given in Section 2. Sec-
ond, we combine multiple classifiers with a meta-
classifier, to yield better performance than each sin-
gle sub-classifier (D¨urr et al., 2014; Cieliebak et al.,
2014). To achieve this, we extended our existing
system (Jaggi et al., 2014). The result is simple:
a large collection of features used in a linear SVM
classifier. We replicated that system with several dif-
</bodyText>
<page confidence="0.971385">
608
</page>
<bodyText confidence="0.830246142857143">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 608–612,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
ferent choices of features and parameters. The out-
put of all those artificially built classifiers is then
feed as input to a random forest classifier, which
generated final classification results, and gave our
system additional non-linear output capabilities.
</bodyText>
<tableCaption confidence="0.9982245">
Table 1: Overview of the data we found available for
training, development and testing.
</tableCaption>
<table confidence="0.999815111111111">
Dataset Total Posit. Negat. Neutr.
Train (Tweets) 8224 3058 1210 3956
Dev (Tweets) 1417 494 286 637
Test: Twitter2015 2390 1038 365 987
Test: Twitter2014 1853 982 202 669
Test: Twitter2013 3813 1572 601 1640
Test: SMS2013 2093 492 394 1207
Test: Tw2014Sarcasm 86 33 40 13
Test: LiveJournal2014 1142 427 304 411
</table>
<sectionHeader confidence="0.948074" genericHeader="method">
2 Flipout Regularization
</sectionHeader>
<bodyText confidence="0.999974111111111">
We propose a new kind of post-
processing/regularization technique to improve
classification accuracy in a setting with several
different available datasets. The intuition comes
from the setting of transfer learning. Many words in
the training data do not occur in the same context
as in the target data (as for example caused by
topic shifts, such as in the evaluation task’s scenario
here). By finding suitable replacements for some
input words, the generalization performance of
a pre-trained linear classifier can be improved.
Since this post-processing of a pre-trained classifier
overrides potentially many of its weights, the
post-processing has an additional regularizing effect
with respect to the original training set, in addition
to the transfer effect towards the target dataset.
We follow a greedy approach to find the best
word-replacements which is as follows:
</bodyText>
<listItem confidence="0.809331235294118">
1. Split the dataset into 4 parts, here called flip-
train, flipdev1, flipdev2 and fliptest.
2. Train a classifier (e.g. SVM) on the set fliptrain,
using the original full set of features.
3. Calculate prediction score on datasets flipdev1
and flipdev2.
4. Pick a subset 5 of words from the vocabulary
of fliptrain. This is the wordpool for the flipout
trick.
5. For each word w1 E 5:
• For each word w2 E 5:
Consider the modified classifier using
the replacement (flip) of input words
w1 H w2. Compute its predic-
tion score on the validation datasets
flipdev1 and flipdev2.
• Keep the replacement w1 H w∗2 which
</listItem>
<bodyText confidence="0.992457230769231">
resulted in the maximum improvement
for the word w1, in the sense of
min(improvement on flipdev1, improve-
ment on flipdev2),
One would expect that this approach would re-
place words of the original set (fliptrain) with words
having a better discriminative power on the new set
(flipdev). In reality, it turned out that words with-
out an obvious relation to each other were replaced
such as: 2nd H may, about H I’m, we H day, etc.
The reason we have separated the development sets
(flipdev1 and flipdev2) is to better avoid potential
overfitting.
</bodyText>
<sectionHeader confidence="0.995403" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999973583333333">
For our system, we preprocessed the tweets and ex-
tracted textual features. Using different subsets of
these features and flipout, we train different lin-
ear classifiers resulting in sentiment classification
systems which are intrinsically different from each
other. These “subsystems” were then combined into
a meta-classifier using a random forest (Breiman,
2001). The random forest uses the outputs of in-
dividual classifiers as features and the labels on the
training data as input for training. Afterwards, in
the test phase, the random forest makes predictions
using the outputs of the same individual classifiers.
</bodyText>
<subsectionHeader confidence="0.997283">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9993825">
The tweets were preprocessed with standard meth-
ods before extracting the features.
</bodyText>
<listItem confidence="0.999328875">
• URLs and usernames are each normalized to a
replacement token
• Tokenizer: We used ArkTweetNLP (Owoputi
et al., 2013) which is suitable for tweets. All
text was transformed to lowercase (except for
special features relying on case information).
• Negation encoding: The negated context of a
sentence is marked as in (Pang et al., 2002), us-
</listItem>
<page confidence="0.996616">
609
</page>
<bodyText confidence="0.9921715">
ing the list of negation words from Christopher
Potts’ sentiment tutorial1.
</bodyText>
<subsectionHeader confidence="0.873704">
3.2 Features for the Subsystems
</subsectionHeader>
<bodyText confidence="0.99967075">
The subsystems use different subsets of the features
we introduce here. Most of them are the same as in
our last years submission (Jaggi et al., 2014). New
additions are marked with a + sign.
</bodyText>
<sectionHeader confidence="0.794417" genericHeader="method">
Features:
</sectionHeader>
<listItem confidence="0.99595906060606">
• n-grams: presence of word n-grams (n =
1...4)
• POS-n-grams: presence of word n-grams with
one or more words replaced by the POS-
Tag (Jaggi et al., 2014). The ArkTweetNLP
structured prediction POS tagger provided by
(Owoputi et al., 2013) together with their pro-
vided standard model (model.20120919) suit-
able for Twitter data was used (n = 3...5)
• non-contiguous n-gram: presence of word n-
grams with one or more words replaced by a
wildcard (n = 3...5)
• character n-grams: presence of charac-
ter n-grams (n = 3...6) weighted in-
creasingly by their length (weights 0.7 ·
{1.0,1.1,1.2,1.4,1.6,1.9} for length 3, 4,...)
• # upper cased: number of tokens written with
all characters in upper case
• # of hashtags
• # of POS tags: for each POS-tag the number of
occurrences
• continuous punctuation: number of continu-
ous exclamation marks, number of continuous
question marks (max)
• last token punctuation: whether the last token
contains an exclamation mark or question mark
or a period
• # elongated words number of words which re-
peat the same character more than two times
• # negated tokens the number of words occur-
ring in a negation context
• Lexicons: For each lexicon (NRC-emotion,
BingLiu, MQA, NRC-HashtagSentiment,
</listItem>
<footnote confidence="0.813756">
Sentiment140, Sentiment140-3-class,
RottenTomatoes-3-class):
1http://sentiment.christopherpotts.net/
lingstruc.html
</footnote>
<bodyText confidence="0.991495857142857">
- total tokens for each class (positive, neg-
ative and neutral for 3-class lexicons)
- score of last token for each class
- maximum score over all tokens for each
class
- total score over all tokens for each class
- +score of last token regardless of the class
- +maximum score over all tokens for all
classes together
- +total score over all tokens
For the 2-class lexicons, we flip the score of
tokens occurring in the negation scope. The 3-
class lexicons are already trained with marked
negations (Jaggi et al., 2014).
</bodyText>
<listItem confidence="0.851959">
• +lemma n-grams: presence of lemma n-grams
(n = 1...4), by using the Standford Core NLP
lemmatizer.
• +cluster unigram: whether a word from each
cluster in the CMU tweet clusters occurs or not
• +GloVe: GloVe word embeddings (Penning-
ton et al., 2014) are a newer version of the
word2vec embedding by (Mikolov et al., 2013),
using a matrix factorization instead of deep
learning. We used the sum, minimum and max-
imum of the GloVe-vectors for the tokens oc-
curing in the tweet.
</listItem>
<subsectionHeader confidence="0.995398">
3.3 Subsystems
</subsectionHeader>
<bodyText confidence="0.9159355">
For the subsystems we used different linear classifier
variants trained using the LibLinear package (Fan
et al., 2008), all being multi-class classifiers for the
three classes in a one-against-all setting.
Subsystem 1. We combined all features to a sin-
gle feature vector using an `1-regularized squared
loss SVM classifer and flipout regularization as de-
scribed in Section 2. We trained the SVM and op-
timized the regularization parameter using 10-fold
cross-validation on the training set. The remain-
ing sets were used for flipout: dev as flipdev1 and
twitter-test13 as flipdev2 and twitter-test14 for test-
ing. We chose the word pool S for flipout as the
most frequent 50 words in fliptrain.
Subsystem 2. The same as subsystem 1 but with-
out flipout. The system was trained on train+dev and
the SVM regularization parameter C was optimized
against the test sets.
</bodyText>
<page confidence="0.99334">
610
</page>
<bodyText confidence="0.981078166666667">
Subsystem 3. The same as subsystem 2 but using
Logistic Regression instead of SVM.
Subsystem 4. The same as subsystem 2 but with-
out any lexicon features.
Subsystem 5. The same as subsystem 2 but using
only the GloVe word-embedding features.
</bodyText>
<subsectionHeader confidence="0.995964">
3.4 Meta-Classifier
</subsectionHeader>
<bodyText confidence="0.999965272727273">
Each subsystem outputs three real values corre-
sponding to the three sentiment classes. In addition,
it outputs the categorical value for the predicted sen-
timent class. Our meta-classifier used these 4 values
as input features. We trained a random forest using
the Weka Java-library on the train data, although the
subsystems are trained on the same data. To avoid
overfitting, we regularized the random forest against
the test sets by trying different values for number of
trees, maximum depth of the forest and the number
of features used per random selection.
</bodyText>
<sectionHeader confidence="0.99818" genericHeader="evaluation">
4 Results
</sectionHeader>
<table confidence="0.9987126">
Twitter15 62.70 62.07 62.41 53.72 58.11 62.73
Twitter14 69.44 69.07 69.34 61.60 63.42 70.19
Twitter13 69.64 69.05 69.49 61.73 61.84 69.70
LiveJournal14 73.54 74.14 74.29 62.32 62.67 74.48
Sarcarsm14 52.94 52.15 50.69 56.15 56.17 49.83
</table>
<tableCaption confidence="0.99969">
Table 2: Results of our subsystems and final system.
</tableCaption>
<bodyText confidence="0.996649108108108">
Overall Performance. Looking at the overall per-
formance, we managed to increase the scores on ev-
ery test set compared to our previous year’s submis-
sion. Table 2 shows the scores of our individual sub-
systems as well as the final system on each test set.
Note that our results in the official submission are
slightly different from Table 2, because of a mistake
we made in the class assignments in our random for-
est input, which is fixed here.
Classifiers. Subsystems 2 and 3 only differ in the
choice of the linear classifier. Our results here show
that logistic regression slightly outperforms SVM.
Flipout Regularization. Flipout proved very use-
ful. Subsystem 1 (with flipout) reached from 0.37 to
0.79 higher F1 than Subsystem 2 (without flipout).
Features from Unsupervised Learning: Lexicons
and Word-Embeddings. Subsystem 4 does not
use any of the lexicon features which were con-
structed on a separate unlabeled large corpus. The
large decrease in performance shows the importance
of the lexicons. Also we can see that Subsystem 5
(which only uses the GloVe word-embedding fea-
tures) results in a very small variation of its scores on
the different test sets, compared to the other systems.
This confirms our expectation that features gener-
ated from unsupervised training on a large data set
will generalize better, i.e. are more robust to topic
and domain changes.
Meta-Classifier. The final system compared with
Subsystem 1 shows the gain from performing meta-
classification. On last year’s Twitter test set, we
obtain an improvement of 0.75 F1-Score. On this
year’s test set (which was hidden), we achieved an
improvement of 0.03, which was low. However, we
are encouraged by the large improvement of 0.94
F1-Score on out-of-domain data (Live Journal) –
which was not seen during training.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999890833333333">
We have described a classifier to predict the sen-
timent of short texts such as tweets. Our system
is built upon the approach of our previous sys-
tems (Jaggi et al., 2014) and (D¨urr et al., 2014), with
several modifications and extensions in features and
regularization. We have seen that our system signif-
icantly improves upon last year’s approach, achiev-
ing a gain of 2.65 points in F1 score on last year’s
test data.
We showed that our newly introduced flipout reg-
ularization technique improved the score on our sys-
tem. To be able to make general statements we need
to further investigate its behavior on different data
sets. We also showed that artificially-built subsys-
tems can be used to improve upon the best classifier
using meta-classification. A question which remains
is how one could automatize the meta-classification
approach to built the most beneficial subsystems.
</bodyText>
<figure confidence="0.998444833333333">
Final System
Subsystem 1
Subsystem 5
Subsystem 4
Subsystem 3
Subsystem 2
</figure>
<page confidence="0.987237">
611
</page>
<sectionHeader confidence="0.994779" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999875375">
Leo Breiman. (2001). Random Forests. Machine
Learning, 45(1):5–32.
Mark Cieliebak, Oliver D¨urr, and Fatih Uzdilli
(2014). Meta-Classifiers Easily Improve Com-
mercial Sentiment Detection Tools. In LREC
2014 - Proceedings of the 9th International Con-
ference on Language Resources and Evaluation.
Oliver D¨urr, Fatih Uzdilli, and Mark Cieliebak
(2014). JOINT FORCES: Unite Competing Sen-
timent Classifiers with Random Forest. In Se-
mEval 2014 - Proceedings of the 8th International
Workshop on Semantic Evaluation, pages 366–
369, Dublin, Ireland.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin (2008). LI-
BLINEAR: A Library for Large Linear Classifi-
cation. JMLR, 9:1871–1874.
Martin Jaggi, Fatih Uzdilli, and Mark Cieliebak
(2014). Swiss-Chocolate: Sentiment Detection
using Sparse SVMs and Part-Of-Speech n-Grams.
In SemEval 2014 - Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation, pages
601–604, Dublin, Ireland.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever
(2013). Exploiting Similarities among Languages
for Machine Translation. arXiv.org.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith (2013). Improved Part-Of-Speech Tagging
for Online Conversational Text with Word Clus-
ters. In Proceedings of NAACL-HLT, pages 380–
390.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan
(2002). Thumbs up? Sentiment Classification us-
ing Machine Learning Techniques. In ACL 2002 -
Conference of the Association for Computational
Linguistics, pages 79–86, Morristown, NJ, USA.
Jeffrey Pennington, Richard Socher and Christopher
D Manning (2014). GloVe: Global Vectors for
Word Representation, EMNLP 2014 - Conference
on Empirical Methods in Natural Language Pro-
cessing.
Sara Rosenthal, Preslav Nakov, Svetlana Kir-
itchenko, Saif M Mohammad, Alan Ritter and
Veselin Stoyanov (2015). SemEval-2015 Task 10:
Sentiment Analysis in Twitter. In Proceedings of
the 9th International Workshop on Semantic Eval-
uation, Denver, Colorado.
</reference>
<page confidence="0.997514">
612
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.021045">
<title confidence="0.9982355">Swiss-Chocolate: Combining Flipout Regularization and Random Forests with Artificially Built Subsystems to Boost Text-Classification for Sentiment</title>
<author confidence="0.932989">Fatih Uzdilli</author>
<affiliation confidence="0.999033">Zurich University of Applied</affiliation>
<address confidence="0.513662">Switzerland</address>
<email confidence="0.962041">uzdi@zhaw.ch</email>
<author confidence="0.984218">Pascal Julmy</author>
<affiliation confidence="0.7957275">Zurich University of Applied Switzerland</affiliation>
<email confidence="0.998862">pascal.julmy@gmail.com</email>
<author confidence="0.999958">Martin Jaggi</author>
<affiliation confidence="0.736492">ETH Switzerland</affiliation>
<email confidence="0.955752">jaggi@inf.ethz.ch</email>
<author confidence="0.99021">Leon Derczynski</author>
<affiliation confidence="0.756097">University of Sheffield UK</affiliation>
<email confidence="0.990353">leon@dcs.shef.ac.uk</email>
<author confidence="0.999618">Dominic Egger</author>
<affiliation confidence="0.999124">Zurich University of Applied</affiliation>
<address confidence="0.668619">Switzerland</address>
<email confidence="0.960632">dominicegger@bluewin.ch</email>
<author confidence="0.999531">Mark Cieliebak</author>
<affiliation confidence="0.999018">Zurich University of Applied</affiliation>
<address confidence="0.513004">Switzerland</address>
<email confidence="0.963998">ciel@zhaw.ch</email>
<abstract confidence="0.998953375">We describe a classifier for predicting message-level sentiment of English microblog messages from Twitter. This paper describes our submission to the SemEval- 2015 competition (Task 10). Our approach is to combine several variants of our previous year’s SVM system into one meta-classifier, which was then trained using a random forest. The main idea is that the meta-classifier allows the combination of the strengths and overcome some of the weaknesses of the artificially-built individual classifiers, and adds additional non-linearity. We were also able to improve the linear classifiers by using a new regularization technique we call flipout.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random Forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="6861" citStr="Breiman, 2001" startWordPosition="1063" endWordPosition="1064">hat words without an obvious relation to each other were replaced such as: 2nd H may, about H I’m, we H day, etc. The reason we have separated the development sets (flipdev1 and flipdev2) is to better avoid potential overfitting. 3 System Description For our system, we preprocessed the tweets and extracted textual features. Using different subsets of these features and flipout, we train different linear classifiers resulting in sentiment classification systems which are intrinsically different from each other. These “subsystems” were then combined into a meta-classifier using a random forest (Breiman, 2001). The random forest uses the outputs of individual classifiers as features and the labels on the training data as input for training. Afterwards, in the test phase, the random forest makes predictions using the outputs of the same individual classifiers. 3.1 Preprocessing The tweets were preprocessed with standard methods before extracting the features. • URLs and usernames are each normalized to a replacement token • Tokenizer: We used ArkTweetNLP (Owoputi et al., 2013) which is suitable for tweets. All text was transformed to lowercase (except for special features relying on case information</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. (2001). Random Forests. Machine Learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Cieliebak</author>
<author>Oliver D¨urr</author>
<author>Fatih Uzdilli</author>
</authors>
<title>Meta-Classifiers Easily Improve Commercial Sentiment Detection Tools.</title>
<date>2014</date>
<booktitle>In LREC 2014 - Proceedings of the 9th International Conference on Language Resources and Evaluation.</booktitle>
<marker>Cieliebak, D¨urr, Uzdilli, 2014</marker>
<rawString>Mark Cieliebak, Oliver D¨urr, and Fatih Uzdilli (2014). Meta-Classifiers Easily Improve Commercial Sentiment Detection Tools. In LREC 2014 - Proceedings of the 9th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver D¨urr</author>
<author>Fatih Uzdilli</author>
<author>Mark Cieliebak</author>
</authors>
<title>JOINT FORCES: Unite Competing Sentiment Classifiers with Random Forest.</title>
<date>2014</date>
<booktitle>In SemEval 2014 - Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<pages>366--369</pages>
<location>Dublin, Ireland.</location>
<marker>D¨urr, Uzdilli, Cieliebak, 2014</marker>
<rawString>Oliver D¨urr, Fatih Uzdilli, and Mark Cieliebak (2014). JOINT FORCES: Unite Competing Sentiment Classifiers with Random Forest. In SemEval 2014 - Proceedings of the 8th International Workshop on Semantic Evaluation, pages 366– 369, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<pages>9--1871</pages>
<publisher>JMLR,</publisher>
<contexts>
<context position="10342" citStr="Fan et al., 2008" startWordPosition="1636" endWordPosition="1639"> 2014). • +lemma n-grams: presence of lemma n-grams (n = 1...4), by using the Standford Core NLP lemmatizer. • +cluster unigram: whether a word from each cluster in the CMU tweet clusters occurs or not • +GloVe: GloVe word embeddings (Pennington et al., 2014) are a newer version of the word2vec embedding by (Mikolov et al., 2013), using a matrix factorization instead of deep learning. We used the sum, minimum and maximum of the GloVe-vectors for the tokens occuring in the tweet. 3.3 Subsystems For the subsystems we used different linear classifier variants trained using the LibLinear package (Fan et al., 2008), all being multi-class classifiers for the three classes in a one-against-all setting. Subsystem 1. We combined all features to a single feature vector using an `1-regularized squared loss SVM classifer and flipout regularization as described in Section 2. We trained the SVM and optimized the regularization parameter using 10-fold cross-validation on the training set. The remaining sets were used for flipout: dev as flipdev1 and twitter-test13 as flipdev2 and twitter-test14 for testing. We chose the word pool S for flipout as the most frequent 50 words in fliptrain. Subsystem 2. The same as s</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin (2008). LIBLINEAR: A Library for Large Linear Classification. JMLR, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jaggi</author>
<author>Fatih Uzdilli</author>
<author>Mark Cieliebak</author>
</authors>
<title>Swiss-Chocolate: Sentiment Detection using Sparse SVMs and Part-Of-Speech n-Grams.</title>
<date>2014</date>
<booktitle>In SemEval 2014 - Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<pages>601--604</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="3448" citStr="Jaggi et al., 2014" startWordPosition="517" endWordPosition="520">s data (tweets, SMS, LiveJournal), new tweets were provided. An overview of the data that we were able to download is given in Table 1. Our Approach. Our system is based on two main ideas. First, we propose a new regularization technique called flipout, which post-processes a trained classifier model for better generalization performance. Details of this are given in Section 2. Second, we combine multiple classifiers with a metaclassifier, to yield better performance than each single sub-classifier (D¨urr et al., 2014; Cieliebak et al., 2014). To achieve this, we extended our existing system (Jaggi et al., 2014). The result is simple: a large collection of features used in a linear SVM classifier. We replicated that system with several dif608 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 608–612, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics ferent choices of features and parameters. The output of all those artificially built classifiers is then feed as input to a random forest classifier, which generated final classification results, and gave our system additional non-linear output capabilities. Table 1: Overview of th</context>
<context position="7820" citStr="Jaggi et al., 2014" startWordPosition="1217" endWordPosition="1220">g the features. • URLs and usernames are each normalized to a replacement token • Tokenizer: We used ArkTweetNLP (Owoputi et al., 2013) which is suitable for tweets. All text was transformed to lowercase (except for special features relying on case information). • Negation encoding: The negated context of a sentence is marked as in (Pang et al., 2002), us609 ing the list of negation words from Christopher Potts’ sentiment tutorial1. 3.2 Features for the Subsystems The subsystems use different subsets of the features we introduce here. Most of them are the same as in our last years submission (Jaggi et al., 2014). New additions are marked with a + sign. Features: • n-grams: presence of word n-grams (n = 1...4) • POS-n-grams: presence of word n-grams with one or more words replaced by the POSTag (Jaggi et al., 2014). The ArkTweetNLP structured prediction POS tagger provided by (Owoputi et al., 2013) together with their provided standard model (model.20120919) suitable for Twitter data was used (n = 3...5) • non-contiguous n-gram: presence of word ngrams with one or more words replaced by a wildcard (n = 3...5) • character n-grams: presence of character n-grams (n = 3...6) weighted increasingly by their</context>
<context position="9731" citStr="Jaggi et al., 2014" startWordPosition="1532" endWordPosition="1535">timent140-3-class, RottenTomatoes-3-class): 1http://sentiment.christopherpotts.net/ lingstruc.html - total tokens for each class (positive, negative and neutral for 3-class lexicons) - score of last token for each class - maximum score over all tokens for each class - total score over all tokens for each class - +score of last token regardless of the class - +maximum score over all tokens for all classes together - +total score over all tokens For the 2-class lexicons, we flip the score of tokens occurring in the negation scope. The 3- class lexicons are already trained with marked negations (Jaggi et al., 2014). • +lemma n-grams: presence of lemma n-grams (n = 1...4), by using the Standford Core NLP lemmatizer. • +cluster unigram: whether a word from each cluster in the CMU tweet clusters occurs or not • +GloVe: GloVe word embeddings (Pennington et al., 2014) are a newer version of the word2vec embedding by (Mikolov et al., 2013), using a matrix factorization instead of deep learning. We used the sum, minimum and maximum of the GloVe-vectors for the tokens occuring in the tweet. 3.3 Subsystems For the subsystems we used different linear classifier variants trained using the LibLinear package (Fan et</context>
<context position="14189" citStr="Jaggi et al., 2014" startWordPosition="2268" endWordPosition="2271">n changes. Meta-Classifier. The final system compared with Subsystem 1 shows the gain from performing metaclassification. On last year’s Twitter test set, we obtain an improvement of 0.75 F1-Score. On this year’s test set (which was hidden), we achieved an improvement of 0.03, which was low. However, we are encouraged by the large improvement of 0.94 F1-Score on out-of-domain data (Live Journal) – which was not seen during training. 5 Conclusion We have described a classifier to predict the sentiment of short texts such as tweets. Our system is built upon the approach of our previous systems (Jaggi et al., 2014) and (D¨urr et al., 2014), with several modifications and extensions in features and regularization. We have seen that our system significantly improves upon last year’s approach, achieving a gain of 2.65 points in F1 score on last year’s test data. We showed that our newly introduced flipout regularization technique improved the score on our system. To be able to make general statements we need to further investigate its behavior on different data sets. We also showed that artificially-built subsystems can be used to improve upon the best classifier using meta-classification. A question which</context>
</contexts>
<marker>Jaggi, Uzdilli, Cieliebak, 2014</marker>
<rawString>Martin Jaggi, Fatih Uzdilli, and Mark Cieliebak (2014). Swiss-Chocolate: Sentiment Detection using Sparse SVMs and Part-Of-Speech n-Grams. In SemEval 2014 - Proceedings of the 8th International Workshop on Semantic Evaluation, pages 601–604, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
</authors>
<title>and Ilya Sutskever</title>
<date>2013</date>
<tech>arXiv.org.</tech>
<marker>Mikolov, Le, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever (2013). Exploiting Similarities among Languages for Machine Translation. arXiv.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved Part-Of-Speech Tagging for Online Conversational Text with Word Clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith (2013). Improved Part-Of-Speech Tagging for Online Conversational Text with Word Clusters. In Proceedings of NAACL-HLT, pages 380– 390.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bo Pang</author>
</authors>
<title>Lillian Lee and Shivakumar Vaithyanathan (2002). Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<booktitle>In ACL 2002 -Conference of the Association for Computational Linguistics,</booktitle>
<pages>79--86</pages>
<location>Morristown, NJ, USA.</location>
<marker>Pang, </marker>
<rawString>Bo Pang, Lillian Lee and Shivakumar Vaithyanathan (2002). Thumbs up? Sentiment Classification using Machine Learning Techniques. In ACL 2002 -Conference of the Association for Computational Linguistics, pages 79–86, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global Vectors for Word Representation,</title>
<date>2014</date>
<booktitle>Conference on Empirical Methods in Natural Language Processing.</booktitle>
<location>EMNLP</location>
<contexts>
<context position="9984" citStr="Pennington et al., 2014" startWordPosition="1575" endWordPosition="1579">ens for each class - total score over all tokens for each class - +score of last token regardless of the class - +maximum score over all tokens for all classes together - +total score over all tokens For the 2-class lexicons, we flip the score of tokens occurring in the negation scope. The 3- class lexicons are already trained with marked negations (Jaggi et al., 2014). • +lemma n-grams: presence of lemma n-grams (n = 1...4), by using the Standford Core NLP lemmatizer. • +cluster unigram: whether a word from each cluster in the CMU tweet clusters occurs or not • +GloVe: GloVe word embeddings (Pennington et al., 2014) are a newer version of the word2vec embedding by (Mikolov et al., 2013), using a matrix factorization instead of deep learning. We used the sum, minimum and maximum of the GloVe-vectors for the tokens occuring in the tweet. 3.3 Subsystems For the subsystems we used different linear classifier variants trained using the LibLinear package (Fan et al., 2008), all being multi-class classifiers for the three classes in a one-against-all setting. Subsystem 1. We combined all features to a single feature vector using an `1-regularized squared loss SVM classifer and flipout regularization as describe</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher and Christopher D Manning (2014). GloVe: Global Vectors for Word Representation, EMNLP 2014 - Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
</authors>
<title>Alan Ritter and Veselin Stoyanov (2015). SemEval-2015 Task 10: Sentiment Analysis in Twitter.</title>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation,</booktitle>
<location>Denver, Colorado.</location>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, </marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter and Veselin Stoyanov (2015). SemEval-2015 Task 10: Sentiment Analysis in Twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, Denver, Colorado.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>