<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992561">
A Modified Joint Source-Channel Model for Transliteration
</title>
<author confidence="0.8498985">
Asif Ekbal
Comp. Sc. &amp; Engg. Deptt.
</author>
<affiliation confidence="0.715415">
Jadavpur University
India
</affiliation>
<address confidence="0.316307">
ekbal_asif12@
</address>
<email confidence="0.649568">
yahoo.co.in
</email>
<author confidence="0.393164">
Sudip Kumar Naskar
Comp. Sc. &amp; Engg. Deptt.
</author>
<affiliation confidence="0.393683">
Jadavpur University
India
</affiliation>
<email confidence="0.3922615">
sudip_naskar@
hotmail.com
</email>
<author confidence="0.526681">
Sivaji Bandyopadhyay
Comp. Sc. &amp; Engg. Deptt.
</author>
<affiliation confidence="0.4797565">
Jadavpur University
India
</affiliation>
<email confidence="0.3994985">
sivaji_cse_ju@
yahoo.com
</email>
<sectionHeader confidence="0.998241" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999827631578947">
In Natural Language Processing (NLP)
application areas such as information retrieval,
question answering systems and machine
translation, there is an increasing need to
translate OOV words from one language to
another. They are translated through
transliteration, the method of translating into
another language by expressing the original
foreign words using characters of the target
language preserving the pronunciation in their
original languages. Thus, the central problem in
transliteration is predicting the pronunciation of
the original word. Transliteration between two
languages, that use the same set of alphabets, is
trivial: the word is left as it is. However, for
languages that use different alphabet sets, the
names must be transliterated or rendered in the
target language alphabets.
Technical terms and named entities make up
the bulk of these OOV words. Named entities
hold a very important place in NLP applications.
Proper identification, classification and
translation of named entities are very crucial in
many NLP applications and pose a very big
challenge to NLP researchers. Named entities are
usually not found in bilingual dictionaries and
they are very productive in nature. Translation of
named entities is a tricky task: it involves both
translation and transliteration. Transliteration is
commonly used for named entities, even when
the words could be translated. Different types of
named entities are translated differently.
Numerical and temporal expressions typically
use a limited set of vocabulary words (e.g.,
names of months, days of the week etc.) and can
be translated fairly easily using simple
translation patterns. The named entity machine
transliteration algorithms presented in this work
</bodyText>
<sectionHeader confidence="0.507781" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.941985194444445">
Most machine transliteration systems
transliterate out of vocabulary (OOV)
words through intermediate phonemic
mapping. A framework has been
presented that allows direct
orthographical mapping between two
languages that are of different origins
employing different alphabet sets. A
modified joint source–channel model
along with a number of alternatives have
been proposed. Aligned transliteration
units along with their context are
automatically derived from a bilingual
training corpus to generate the
collocational statistics. The transliteration
units in Bengali words take the pattern
C+M where C represents a vowel or a
consonant or a conjunct and M represents
the vowel modifier or matra. The English
transliteration units are of the form C*V*
where C represents a consonant and V
represents a vowel. A Bengali-English
machine transliteration system has been
developed based on the proposed models.
The system has been trained to
transliterate person names from Bengali
to English. It uses the linguistic
knowledge of possible conjuncts and
diphthongs in Bengali and their
equivalents in English. The system has
been evaluated and it has been observed
that the modified joint source-channel
model performs best with a Word
Agreement Ratio of 69.3% and a
Transliteration Unit Agreement Ratio of
89.8%.
</bodyText>
<page confidence="0.98135">
191
</page>
<note confidence="0.7246105">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 191–198,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999948745454546">
focus on person names, locations and
organizations. A machine transliteration system
that is trained on person names is very important
in a multilingual country like India where large
name collections like census data, electoral roll
and railway reservation information must be
available to multilingual citizens of the country
in their vernacular. In the present work, the
various proposed models have been evaluated on
a training corpus of person names.
A hybrid neural network and knowledge-based
system to generate multiple English spellings for
Arabic personal names is described in (Arbabi et
al., 1994). (Knight and Graehl, 1998) developed
a phoneme-based statistical model using finite
state transducer that implements transformation
rules to do back-transliteration. (Stalls and
Knight, 1998) adapted this approach for back
transliteration from Arabic to English for English
names. A spelling-based model is described in
(Al-Onaizan and Knight, 2002a; Al-Onaizan and
Knight, 2002c) that directly maps English letter
sequences into Arabic letter sequences with
associated probability that are trained on a small
English/Arabic name list without the need for
English pronunciations. The phonetics-based and
spelling-based models have been linearly
combined into a single transliteration model in
(Al-Onaizan and Knight, 2002b) for
transliteration of Arabic named entities into
English.
Several phoneme-based techniques have been
proposed in the recent past for machine
transliteration using transformation-based
learning algorithm (Meng et al., 2001; Jung et
al., 2000; Vigra and Khudanpur, 2003).
(Abduljaleel and Larkey, 2003) have presented a
simple statistical technique to train an English-
Arabic transliteration model from pairs of names.
The two-stage training procedure first learns
which n-gram segments should be added to
unigram inventory for the source language, and
then a second stage learns the translation model
over this inventory. This technique requires no
heuristic or linguistic knowledge of either
language.
(Goto et al., 2003) described an English-
Japanese transliteration method in which an
English word is divided into conversion units
that are partial English character strings in an
English word and each English conversion unit is
converted into a partial Japanese Katakana
character string. It calculates the likelihood of a
particular choice of letters of chunking into
English conversion units for an English word by
linking them to Katakana characters using
syllables. Thus the English conversion units
consider phonetic aspects. It considers the
English and Japanese contextual information
simultaneously to calculate the plausibility of
conversion from each English conversion unit to
various Japanese conversion units using a single
probability model based on the maximum
entropy method.
(Haizhou et al., 2004) presented a framework
that allows direct orthographical mapping
between English and Chinese through a joint
source-channel model, called n-gram
transliteration model. The orthographic
alignment process is automated using the
maximum likelihood approach, through the
Expectation Maximization algorithm to derive
aligned transliteration units from a bilingual
dictionary. The joint source-channel model tries
to capture how source and target names can be
generated simultaneously, i.e., the context
information in both the source and the target
sides are taken into account.
A tuple n-gram transliteration model (Marino
et al., 2005; Crego et al., 2005) has been log-
linearly combined with feature functions to
develop a statistical machine translation system
for Spanish-to-English and English-to-Spanish
translation tasks. The model approximates the
joint probability between source and target
languages by using trigrams.
The present work differs from (Goto et al.,
2003; Haizhou et al., 2004) in the sense that
identification of the transliteration units in the
source language is done using regular
expressions and no probabilistic model is used.
The proposed modified joint source-channel
model is similar to the model proposed by (Goto
et. al., 2003) but it differs in the way the
transliteration units and the contextual
information are defined in the present work. No
linguistic knowledge is used in (Goto et al.,
2003; Haizhou et al., 2004) whereas the present
work uses linguistic knowledge in the form of
possible conjuncts and diphthongs in Bengali.
The paper is organized as follows. The
machine transliteration problem has been
formulated under both noisy-channel model and
joint source-channel model in Section 2. A
number of transliteration models based on
collocation statistics including the modified joint
source-channel model and their evaluation
scheme have been proposed in Section 3. The
Bengali-English machine transliteration scenario
has been presented in Section 4. The proposed
</bodyText>
<page confidence="0.991672">
192
</page>
<bodyText confidence="0.995446666666667">
models have been evaluated and the result of
evaluation is reported in Section 5. The
conclusion is drawn in Section 6.
</bodyText>
<sectionHeader confidence="0.604197" genericHeader="introduction">
2 Machine Transliteration and Joint
</sectionHeader>
<subsectionHeader confidence="0.748705">
Source-Channel Model
</subsectionHeader>
<bodyText confidence="0.999315294117647">
A transliteration system takes as input a character
string in the source language and generates a
character string in the target language as output.
The process can be conceptualized as two levels
of decoding: segmentation of the source string
into transliteration units; and relating the source
language transliteration units with units in the
target language, by resolving different
combinations of alignments and unit mappings.
The problem of machine transliteration has been
studied extensively in the paradigm of the noisy
channel model.
For a given Bengali name B as the observed
channel output, we have to find out the most
likely English transliteration E that maximizes
P(EIB). Applying Bayes’ rule, it means to find
E to maximize
</bodyText>
<equation confidence="0.888395">
P(B,E) = P(BIE) * P(E) (1)
</equation>
<bodyText confidence="0.998094209302326">
with equivalent effect. This is equivalent to
modelling two probability distributions: P(B|E),
the probability of transliterating E to B through a
noisy channel, which is also called
transformation rules, and P(E), the probability
distribution of source, which reflects what is
considered good English transliteration in
general. Likewiswe, in English to Bengali (E2B)
transliteration, we could find B that maximizes
P(B,E) = P(EIB) * P(B) (2)
for a given English name. In equations (1) and
(2), P(B) and P(E) are usually estimated using n-
gram language models. Inspired by research
results of grapheme-to-phoneme research in
speech synthesis literature, many have suggested
phoneme-based approaches to resolving P(BIE)
and P(EIB), which approximates the probability
distribution by introducing a phonemic
representation. In this way, names in the source
language, say B, are converted into an
intermediate phonemic representation P, and then
the phonemic representation is further converted
into the target language, say English E. In
Bengali to English (B2E) transliteration, the
phoneme-based approach can be formulated as
P(EIB) = P(EIP) * P(PIB) and conversely we
have P(BIE) = P(BIP) * P(PIE) for E2B back-
transliteration.
However, phoneme-based approaches are
limited by a major constraint that could
compromise transliteration precision. The
phoneme-based approach requires derivation of
proper phonemic representation for names of
different origins. One may need to prepare
multiple language-dependent grapheme-to-
phoneme(G2P) and phoneme-to-grapheme(P2G)
conversion systems accordingly, and that is not
easy to achieve.
In view of close coupling of the source and
target transliteration units, a joint source-channel
model, or n-gram transliteration model (TM) has
been proposed in (Haizhou et al., 2004). For K
alligned transliteration units, we have
</bodyText>
<equation confidence="0.897546166666667">
P(B,E) = P( b1, b2 bk, e1, e2 ek )
= P (&lt;b,e&gt;1, &lt;b,e&gt;2, &lt;b,e&gt;k)
K
=
fl P ( &lt;b,e&gt;kI &lt;b,e&gt;1k-1) (3)
k=1
</equation>
<bodyText confidence="0.990991714285714">
which provides an alternative to the phoneme-
based approach for resolving equations (1) and
(2) by eliminating the intermediate phonemic
representation.
Unlike the noisy-channel model, the joint
source-channel model does not try to capture
how source names can be mapped to target
names, but rather how source and target names
can be generated simultaneously. In other words,
a joint probability model is estimated that can be
easily marginalized in order to yield conditional
probability models for both transliteration and
back-transliteration.
Suppose that we have a Bengali name a =
x1x2 xm and an English transliteration R =
y1y2 yn where xi, i = 1: m are Bengali
transliteration units and yj, j = 1: n are English
transliteration units. An English transliteration
unit may correspond to zero, one or more than
one transliteration unit in Bengali. Often the
values of m and n are different.
</bodyText>
<equation confidence="0.896633">
x1 x2x3 xi-1xixi+1....xm
y1 y2 yi yn
</equation>
<bodyText confidence="0.9990094">
where there exists an alignment Y with &lt;b,e&gt;1
= &lt;x1,y1&gt;; &lt;b,e&gt;2 = &lt;x2x3, y2&gt;; .... and &lt;b,e&gt;k =
&lt;xm,yn&gt;. A transliteration unit correspondence
&lt;b, e&gt; is called a transliteration pair. Thus B2E
transliteration can be formulated as
</bodyText>
<equation confidence="0.885662">
1 = argmax P (a, 1, Y ) (4)
1, Y
</equation>
<bodyText confidence="0.953012">
and similarly the E2B back-transliteration as
</bodyText>
<page confidence="0.940612">
193
</page>
<equation confidence="0.951822">
a = argmax P (a, 1, y ) (5)
a, y
</equation>
<bodyText confidence="0.9996585">
An n-gram transliteration model is defined as
the conditional probability or transliteration
probability of a transliteration pair &lt;b, e&gt;k
depending on its immediate n predecessor pairs:
</bodyText>
<equation confidence="0.8244628">
P (B, E) = P (a, 0, Y)
K
=
fl P ( &lt;b, e&gt;kI &lt;b, e&gt;k-n+1k-1) (6)
k=1
</equation>
<sectionHeader confidence="0.9622405" genericHeader="method">
3 Proposed Models and Evaluation
Scheme
</sectionHeader>
<bodyText confidence="0.999902775510204">
Machine transliteration has been viewed as a
sense disambiguation problem. A number of
transliteration models have been proposed that
can generate the English transliteration from a
Bengali word that is not registered in any
bilingual or pronunciation dictionary. The
Bengali word is divided into Transliteration
Units (TU) that have the pattern C+M, where C
represents a vowel or a consonant or conjunct
and M represents the vowel modifier or matra.
An English word is divided into TUs that have
the pattern C*V*, where C represents a
consonant and V represents a vowel. The TUs
are considered as the lexical units for machine
transliteration. The system considers the Bengali
and English contextual information in the form
of collocated TUs simultaneously to calculate the
plausibility of transliteration from each Bengali
TU to various English candidate TUs and
chooses the one with maximum probability. This
is equivalent to choosing the most appropriate
sense of a word in the source language to identify
its representation in the target language. The
system learns the mappings automatically from
the bilingual training corpus guided by linguistic
features. The output of this mapping process is a
decision-list classifier with collocated TUs in the
source language and their equivalent TUs in
collocation in the target language along with the
probability of each decision obtained from a
training corpus. The machine transliteration of
the input Bengali word is obtained using direct
orthographic mapping by identifying the
equivalent English TU for each Bengali TU in
the input and then placing the English TUs in
order. The various proposed models differ in the
nature of collocational stastistics used during
machine transliteration process: monogram
model with no context, bigram model with
previous (with respect to the current TU to be
transliterated) source TU as the context, bigram
model with next source TU as the context,
bigram model with previous source and target
TUs as the context (this is the joint source
channel model), trigram model with previous and
next source TUs as the context and the modified
joint source-channel model with previous and
next source TUs and the previous target TU as
the context.
</bodyText>
<listItem confidence="0.957768">
• Model A
</listItem>
<bodyText confidence="0.997455333333333">
In this model, no context is considered in
either the source or the target side. This is
essentially the monogram model.
</bodyText>
<equation confidence="0.977103">
K
P(B,E) = IT P(&lt;b,e&gt;k)
k=1
</equation>
<listItem confidence="0.819135">
• Model B
</listItem>
<bodyText confidence="0.99991225">
This is essentially a bigram model with
previous source TU, i.e., the source TU occurring
to the left of the current TU to be transliterated,
as the context.
</bodyText>
<equation confidence="0.823587">
K
P(B,E) = II P(&lt;b,e&gt;k  |bk-1)
k=1
•Model C
</equation>
<bodyText confidence="0.99967275">
This is essentially a bigram model with next
source TU, i.e., the source TU occurring to the
right of the current TU to be transliterated, as the
context.
</bodyText>
<equation confidence="0.978207333333333">
K
P(B,E) = II P(&lt;b,e&gt;kI bk+1 )
k=1
</equation>
<listItem confidence="0.883106">
• Model D
</listItem>
<bodyText confidence="0.999929666666667">
This is essentially the joint source-channel
model where the previous TUs in both the source
and the target sides are considered as the context.
The previous TU on the target side refers to the
transliterated TU to the immediate left of the
current target TU to be transliterated.
</bodyText>
<equation confidence="0.970886">
K
P(B,E) = II P( &lt;b,e&gt;k i  |&lt;b,e&gt;k-1)
k=1
</equation>
<page confidence="0.991598">
194
</page>
<listItem confidence="0.666015">
• Model E
</listItem>
<bodyText confidence="0.999892666666667">
This is basically the trigram model where the
previous and the next source TUs are considered
as the context
</bodyText>
<equation confidence="0.984744333333333">
K
P(B,E) = II P(&lt;b,e&gt;k  |bk-1, bk+1)
k=1
</equation>
<listItem confidence="0.806613">
• Model F
</listItem>
<bodyText confidence="0.99978575">
In this model, the previous and the next TUs in
the source and the previous target TU are
considered as the context. This is the modified
joint source-channel model .
</bodyText>
<equation confidence="0.993041333333333">
K
P(B,E) = II P (&lt;b,e&gt;k  |&lt;b,e&gt;k-1, bk+1)
k=1
</equation>
<bodyText confidence="0.99993525">
The performance of the system is evaluated in
terms of Transliteration Unit Agreement Ratio
(TUAR) and Word Agreement Ratio (WAR)
following the evaluation scheme in (Goto et al.,
2003). The evaluation parameter Character
Agreement Ratio in (Goto et al., 2003) has been
modified to Transliteration Unit Agreement
Ratio as vowel modifier matra symbols in
Bengali words are not independent and must
always follow a consonant or a conjunct in a
Transliteration Unit. Let, B be the input Bengali
word, E be the English transliteration given by
the user in open test and E/ be the system
generates the transliteration..TUAR is defined as,
TUAR = (L-Err)/ L, where L is the number of
TUs in E, and Err is the number of wrongly
transliterated TUs in E/ generated by the system.
WAR is defined as, WAR= (S-Err/) / S, where S
is the test sample size and Err/ is is the number of
erroneous names generated by the system (when
E/ does not match with E). Each of these models
has been evaluated with linguistic knowledge of
the set of possible conjuncts and diphthongs in
Bengali and their equivalents in English. It has
been observed that the Modified Joint Source
Channel Model with linguistic knowledge
performs best in terms of Word Agreement Ratio
and Transliteration Unit Agreement Ratio.
</bodyText>
<sectionHeader confidence="0.9895705" genericHeader="method">
4 Bengali-English Machine
Transliteration
</sectionHeader>
<bodyText confidence="0.992524818181818">
Translation of named entities is a tricky task: it
involves both translation and transliteration.
Transliteration is commonly used for named
entities, even when the words could be translated
[v�-t lfal (janata dal) is translated to Janata Dal
(literal translation) although \SmV (Janata) and
Tfal (Dal) are vocabulary words]. On the other
hand VP K&apos;V NdINVaIf (jadavpur
viswavidyalaya) is translated to Jadavpur
University in which VPKV (Jadavpur) is
transliterated to Jadavpur and SII
(viswavidyalaya) is translated to University.
A bilingual training corpus has been kept that
contains entries mapping Bengali names to their
respective English transliterations. To
automatically analyze the bilingual training
corpus to acquire knowledge in order to map new
Bengali names to English, TUs are extracted
from the Bengali names and the corresponding
English names, and Bengali TUs are associated
with their English counterparts.
Some examples are given below:
</bodyText>
<equation confidence="0.935427">
�������� (abhinandan) --+ [W  |R�  |--T  |&apos;i�  |-]
abhinandan --+ [a  |bhi  |na  |nda  |n ]
T:s5ft (krishnamoorti) --+ [;7  |�s  |5[  |f]
krishnamurthy --+ [ kri  |shna  |mu  |rthy ]
������ (srikant) --+ [ 2R  |;Iit  |!� ]
srikant --+ [ sri  |ka  |nt ]
</equation>
<bodyText confidence="0.892332333333333">
After retrieving the transliteration units from a
Bengali-English name pair, it associates the
Bengali TUs to the English TUs along with the
TUs in context.
For example, it derives the following
transliteration pairs or rules from the name-pair:
�a�����a 1# (rabindranath) --+ rabindranath
Source Language Target Language
previous TU TU next TU previous TU TU
</bodyText>
<equation confidence="0.852696333333333">
- ;a If H - ra
a
~ ~~&amp;quot; H ra bi
��� ��&amp;quot;-fft H bi ndra
�a �t # H ndra na
�t # - H na th
</equation>
<page confidence="0.992636">
195
</page>
<bodyText confidence="0.962707252631579">
But, in some cases, the number of
transliteration units retrieved from the Bengali
and English words may differ. The [ ���e$��%&amp;il
(brijmohan) H brijmohan ] name pair yields 5
TUs in Bengali side and 4 TUs in English side
[ I  |v5  |$��  |%&amp;  |- H bri  |jmo  |ha  |n]. In such
cases, the system cannot align the TUs
automatically and linguistic knowledge is used
to resolve the confusion. A knowledge base that
contains a list of Bengali conjuncts and
diphthongs and their possible English
representations has been kept. The hypothesis
followed in the present work is that the problem
TU in the English side has always the maximum
length. If more than one English TU has the
same length, then system starts its analysis from
the first one. In the above example, the TUs bri
and jmo have the same length. The system
interacts with the knowledge base and ascertains
that bri is valid and jmo cannot be a valid TU in
English since there is no corresponding conjunct
representation in Bengali. So jmo is split up into
2 TUs j and mo, and the system aligns the 5 TUs
as [���  |�  |$��  |%&amp;  |� H bri  |j  |mo  |ha  |n].
Similarly, [&apos;~~~~-ft# (loknath) H loknath] is
initially split as [ &apos;aft  |W  |-qt  |# ] H lo  |kna |
th], and then as [ lo  |k  |na  |th ] since kna has the
maximum length and it does not have any valid
conjunct representation in Bengali.
In some cases, the knowledge of Bengali
diphthong resolves the problem. In the following
example, [fit  |(  |5Tt (raima) H rai  |ma], the
number of TUs on both sides do not
match. The English TU rai is chosen for analysis
as its length is greater than the other TU ma. The
vowel sequence ai corresponds to a diphthong in
Bengali that has two valid representations &lt; Wk)
* &gt;. The first representation signifies that a
matra is associated to the previous character
followed by the character (+ This matches the
present Bengali input. Thus, the English vowel
sequence ai is separated from the TU rai (rai --). r
 |ai) and the intermediate form of the name pair
appears to be [at  |t  |5Tt (raima) H r  |ai  |ma].
Here, a matra is associated with the Bengali TU
that corresponds to English TU r and so there
must be a vowel attached with the TU r. TU ai is
further splitted as a and i (ai --). a  |i) and the first
one (i.e. a) is assimilated with the previous TU
(i.e. r) and finally the name pair appears as: [ V |
( |14t (raima) H ra  |i  |ma].
In the following two examples, the number of
TUs on both sides does not match.
[ &apos;�  |a  |at  |vSY (devraj) H de  |vra  |j ]
[ &apos;,t  |sT  |-qt  |# (somnath) H so  |mna  |th]
It is observed that both vr and mn represent
valid conjuncts in Bengali but these examples
contain the constituent Bengali consonants in
order and not the conjunct representation. During
the training phase, if, for some conjuncts,
examples with conjunct representation are
outnumbered by examples with constituent
consonants representation, the conjunct is
removed from the linguistic knowledge base and
training examples with such conjunct
representation are moved to a Direct example
base which contains the English words and their
Bengali transliteration. The above two name
pairs can then be realigned as
[ &apos;wt  |a  |at  |vSY (devraj) H de  |v  |ra  |j ]
[ &apos;,t  |sT  |-qt  |# (somnath) H so  |m  |na  |th]
Otherwise, if such conjuncts are included in
the linguistic knowledge base, training examples
with constituent consonants representation are to
be moved to the Direct example base.
The Bengali names and their English
transliterations are split into TUs in such a way
that, it results in a one-to-one correspondence
after using the linguistic information. But in
some cases there exits zero-to-one or many-
to-one relationship. An example of Zero-to-One
relationship [(D --). h] is the name-pair [M  |-t
(alla) H a  |lla  |h] while the name-pair [Wt  |( |
��� (aivy) H i  |vy] is an example of Many-to-
One relationship [Wt, ( --). i]. These bilingual
examples should also be included in the Direct
example base.
In some cases, the linguistic knowledge
apparently solves the mapping problem, but not
always. From the name-pair [;d.t (barkha) H
barkha], the system initially generates the
mapping [a  |a  |.~ H ba  |rkha] which is not
one-to-one. Then it consults the linguistic
knowledge base and breaks up the transliteration
unit as (rkha --). rk  |ha ) and generates the final
</bodyText>
<page confidence="0.996838">
196
</page>
<bodyText confidence="0.9971388">
aligned transliteration pair [a  |a  |V H ba  |rk |
ha ] (since it finds out that rk has a valid conjunct
representation in Bengali but not rkh), which is
an incorrect transliteration pair to train the
system. It should have been [a  |a  |V H ba  |r |
kha]. Such type of errors can be detected by
following the alignment process from the target
side during the training phase. Such training
examples may be either manually aligned or
maintained in the Direct Example base.
</bodyText>
<sectionHeader confidence="0.993305" genericHeader="evaluation">
5 Results of the Proposed Models
</sectionHeader>
<bodyText confidence="0.999402235294118">
Approximately 6000 Indian person names have
been collected and their English transliterations
have been stored manually. This set acts as the
training corpus on which the system is trained to
generate the collocational statistics. These
statistics serve as the decision list classifier to
identify the target language TU given the source
language TU and its context. The system also
includes the linguistic knowledge in the form of
valid conjuncts and diphthongs in Bengali and
their English representation.
All the models have been tested with an open
test corpus of about 1200 Bengali names that
contains their English transliterations. The total
number of transliteration units (TU) in these
1200 (Sample Size, i.e., S) Bengali names is
4755 (this is the value of L), i.e., on an average a
Bengali name contains 4 TUs. The test set was
collected from users and it was checked that it
does not contain names that are present in the
training set. The total number of transliteration
unit errors (Err) in the system-generated
transliterations and the total number of words
erroneously generated (Err&apos;) by the system have
been shown in Table 1 for each individual model.
The models are evaluated on the basis of the two
evaluation metrics, Word Agreement Ratio
(WAR) and Transliteration Unit Agreement
Ratio (TUAR). The results of the tests in terms
of the evaluation metrics are shown in Table 2.
The modified joint source-channel model (Model
F) that incorporates linguistic knowledge
performs best among all the models with a Word
Agreement Ratio (WAR) of 69.3% and a
Transliteration Unit Agreement Ratio (TUAR) of
89.8%. The joint source-channel model with
linguistic knowledge (Model D) has not
performed well in the Bengali-English machine
transliteration whereas the trigram model (Model
E) needs further attention as its result are
comparable to the modified joint source-channel
model (Model F). All the models were also tested
for back-transliteration, i.e., English to Bengali
transliteration, with an open test corpus of 1000
English names that contain their Bengali
transliterations. The results of these tests in terms
of the evaluation metrics WAR and TUAR are
shown in Table 3. It is observed that the
modified joint source-channel model performs
best in back-transliteration with a WAR of
67.9% and a TUAR of 89%.
</bodyText>
<table confidence="0.99979525">
Model Error in TUs Error words
(Err) (Err&apos;)
A 990 615
B 795 512
C 880 532
D 814 471
E 604 413
F 486 369
</table>
<tableCaption confidence="0.9742355">
Table 1: Value of Err and Err&apos; for each model
(B2E transliteration)
</tableCaption>
<table confidence="0.99987675">
Model WAR TUAR
(in %) (in %)
A 48.8 79.2
B 57.4 83.3
C 55.7 81.5
D 60.8 82.9
E 65.6 87.3
F 69.3 89.8
</table>
<tableCaption confidence="0.976154">
Table 2: Results with Evaluation Metrics
(B2E transliteration)
</tableCaption>
<table confidence="0.999897625">
Model WAR TUAR
(in %) (in %)
A 49.6 79.8
B 56.2 83.8
C 53.9 82.2
D 58.2 83.2
E 64.7 87.5
F 67.9 89.0
</table>
<tableCaption confidence="0.927794">
Table 3: Results with Evaluation Metrics
(E2B transliteration)
</tableCaption>
<sectionHeader confidence="0.996872" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.9505828">
It has been observed that the modified joint
source-channel model with linguistic knowledge
performs best in terms of Word Agreement Ratio
(WAR) and Transliteration Unit Agreement
Ratio (TUAR). Detailed examination of the
</bodyText>
<page confidence="0.993482">
197
</page>
<bodyText confidence="0.99991345">
evaluation results reveals that Bengali has
separate short and long vowels and the
corresponding matra representation while these
may be represented in English by the same
vowel. It has been observed that most of the
errors are at the matra level i.e., a short matra
might have been replaced by a long matra or vice
versa. More linguistic knowledge is necessary to
disambiguate the short and the long vowels and
the matra representation in Bengali. The system
includes conjuncts and diphthongs as part of the
linguistic knowledge base. Triphthongs or
tetraphthongs usually do not appear in Indian
names. But, inclusion of them will enable the
system to transliterate those few names that may
include them. The models are to be trained
further on sets of additional person names from
other geographic areas. Besides person names,
location and organization names are also to be
used for training the proposed models.
</bodyText>
<sectionHeader confidence="0.984524" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.963044">
Our thanks go to Council of Scientific and
Industrial Research, Human Resource
Development Group, New Delhi, India for
supporting Sudip Kumar Naskar under Senior
Research Fellowship Award (9/96(402) 2003-
EMR-I).
</bodyText>
<sectionHeader confidence="0.995989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996503723076923">
Abdul Jaleel Nasreen and Leah S. Larkey. 2003.
Statistical Transliteration for English-Arabic Cross
Language Information Retrieval. Proceedings of
the Twelfth International Conference on
Information and Knowledge Management (CIKM
2003), New Orleans, USA, 139-146.
Al-Onaizan Y. and Knight K. 2002a. Named Entity
Translation: Extended Abstract. Proceedings of the
Human Language Technology Conference (HLT
2002), 122-124.
Al-Onaizan Y. and Knight K.2002b. Translating
Named Entities Using Monolingual and Bilingual
Resources. Proceedings of the 40th Annual
Meeting of the ACL (ACL 2002), 400-408.
Al-Onaizan Y. and Knight K. 2002c. Machine
Transliteration of Names in Arabic Text.
Proceedings of the ACL Workshop on
Computational Approaches to Semitic Languages.
Arbabi Mansur, Scott M. Fischthal, Vincent C.
Cheng, and Elizabeth Bar. 1994. Algorithms for
Arabic name transliteration. IBM Journal of
Research and Development, 38(2): 183-193.
Crego J.M., Marino J.B. and A. de Gispert. 2005.
Reordered Search and Tuple Unfolding for Ngram-
based SMT. Proceedings of the MT-Summit X,
Phuket, Thailand, 283-289.
Marino J. B., Banchs R., Crego J. M., A. de Gispert,
P. Lambert, J. A. Fonollosa and M. Ruiz, Bilingual
N-gram Statistical Machine Translation.
Proceedings of the MT-Summit X, Phuket,
Thailand, 275-282.
Goto I., N. Kato, N. Uratani, and T. Ehara. 2003.
Transliteration considering Context Information
based on the Maximum Entropy Method.
Proceeding of the MT-Summit IX, New Orleans,
USA, 125–132.
Haizhou Li, Zhang Min, Su Jian. 2004. A Joint
Source-Channel Model for Machine
Transliteration. Proceedings of the 42nd Annual
Meeting of the ACL (ACL 2004), Barcelona,
Spain, 159-166.
Jung Sung Young, Sung Lim Hong, and Eunok Paek.
2000. An English to Korean Transliteration Model
of Extended Markov Window. Proceedings of
COLING 2000, 1, 383-389.
Knight K. and J. Graehl. 1998. Machine
Transliteration, Computational Linguistics, 24(4):
599-612.
Meng Helen M., Wai-Kit Lo, Berlin Chen and Karen
Tang. 2001. Generating Phonetic Cognates to
handle Name Entities in English-Chinese Cross-
language Spoken Document Retrieval. Proceedings
of the Automatic Speech Recognition and
Understanding (ASRU) Workshop, Trento, Italy.
Stalls, Bonnie Glover and Knight K. 1998.
Translating names and technical terms in Arabic
text. Proceedings of the COLING/ACL Workshop
on Computational Approaches to Semitic
Languages, Montral, Canada, 34-41.
Virga Paola and Sanjeev Khudanpur. 2003.
Transliteration of Proper Names in Crosslingual
Information Retrieval. Proceedings of the ACL
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, Sapporo,
Japan, 57-60.
</reference>
<page confidence="0.997717">
198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.049656">
<title confidence="0.7169195">A Modified Joint Source-Channel Model for Transliteration Asif Ekbal</title>
<author confidence="0.558523">Sc</author>
<author confidence="0.558523">Engg Deptt</author>
<affiliation confidence="0.999975">Jadavpur University</affiliation>
<address confidence="0.829906">India</address>
<email confidence="0.7750125">ekbal_asif12@yahoo.co.in</email>
<author confidence="0.938472">Sudip Kumar Naskar</author>
<affiliation confidence="0.8162145">Comp. Sc. &amp; Engg. Deptt. Jadavpur University</affiliation>
<address confidence="0.941054">India</address>
<email confidence="0.9787585">sudip_naskar@hotmail.com</email>
<author confidence="0.954">Sivaji Bandyopadhyay</author>
<affiliation confidence="0.766058">Comp. Sc. &amp; Engg. Deptt. Jadavpur University</affiliation>
<address confidence="0.938693">India</address>
<email confidence="0.925876">sivaji_cse_ju@yahoo.com</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abdul Jaleel Nasreen</author>
<author>Leah S Larkey</author>
</authors>
<title>Statistical Transliteration for English-Arabic Cross Language Information Retrieval.</title>
<date>2003</date>
<booktitle>Proceedings of the Twelfth International Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>139--146</pages>
<location>New Orleans, USA,</location>
<marker>Nasreen, Larkey, 2003</marker>
<rawString>Abdul Jaleel Nasreen and Leah S. Larkey. 2003. Statistical Transliteration for English-Arabic Cross Language Information Retrieval. Proceedings of the Twelfth International Conference on Information and Knowledge Management (CIKM 2003), New Orleans, USA, 139-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Named Entity Translation: Extended Abstract.</title>
<date>2002</date>
<booktitle>Proceedings of the Human Language Technology Conference (HLT</booktitle>
<pages>122--124</pages>
<contexts>
<context position="4517" citStr="Al-Onaizan and Knight, 2002" startWordPosition="652" endWordPosition="655">their vernacular. In the present work, the various proposed models have been evaluated on a training corpus of person names. A hybrid neural network and knowledge-based system to generate multiple English spellings for Arabic personal names is described in (Arbabi et al., 1994). (Knight and Graehl, 1998) developed a phoneme-based statistical model using finite state transducer that implements transformation rules to do back-transliteration. (Stalls and Knight, 1998) adapted this approach for back transliteration from Arabic to English for English names. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. Several phoneme-based techniques have been proposed in the recent past for machine transliteration using transformation-based learning algorithm (Meng et al., 2001; </context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan Y. and Knight K. 2002a. Named Entity Translation: Extended Abstract. Proceedings of the Human Language Technology Conference (HLT 2002), 122-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>Knight K 2002b</author>
</authors>
<title>Translating Named Entities Using Monolingual and Bilingual Resources.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the ACL (ACL</booktitle>
<pages>400--408</pages>
<marker>Al-Onaizan, 2002b, 2002</marker>
<rawString>Al-Onaizan Y. and Knight K.2002b. Translating Named Entities Using Monolingual and Bilingual Resources. Proceedings of the 40th Annual Meeting of the ACL (ACL 2002), 400-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<date>2002</date>
<booktitle>Machine Transliteration of Names in Arabic Text. Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="4517" citStr="Al-Onaizan and Knight, 2002" startWordPosition="652" endWordPosition="655">their vernacular. In the present work, the various proposed models have been evaluated on a training corpus of person names. A hybrid neural network and knowledge-based system to generate multiple English spellings for Arabic personal names is described in (Arbabi et al., 1994). (Knight and Graehl, 1998) developed a phoneme-based statistical model using finite state transducer that implements transformation rules to do back-transliteration. (Stalls and Knight, 1998) adapted this approach for back transliteration from Arabic to English for English names. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. Several phoneme-based techniques have been proposed in the recent past for machine transliteration using transformation-based learning algorithm (Meng et al., 2001; </context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan Y. and Knight K. 2002c. Machine Transliteration of Names in Arabic Text. Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arbabi Mansur</author>
<author>Scott M Fischthal</author>
<author>Vincent C Cheng</author>
<author>Elizabeth Bar</author>
</authors>
<title>Algorithms for Arabic name transliteration.</title>
<date>1994</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>38</volume>
<issue>2</issue>
<pages>183--193</pages>
<marker>Mansur, Fischthal, Cheng, Bar, 1994</marker>
<rawString>Arbabi Mansur, Scott M. Fischthal, Vincent C. Cheng, and Elizabeth Bar. 1994. Algorithms for Arabic name transliteration. IBM Journal of Research and Development, 38(2): 183-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Marino</author>
<author>A de Gispert</author>
</authors>
<title>Reordered Search and Tuple Unfolding for Ngrambased SMT.</title>
<date>2005</date>
<booktitle>Proceedings of the MT-Summit X,</booktitle>
<pages>283--289</pages>
<location>Phuket, Thailand,</location>
<marker>Crego, Marino, de Gispert, 2005</marker>
<rawString>Crego J.M., Marino J.B. and A. de Gispert. 2005. Reordered Search and Tuple Unfolding for Ngrambased SMT. Proceedings of the MT-Summit X, Phuket, Thailand, 283-289.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J B Marino</author>
<author>R Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A Fonollosa</author>
<author>M Ruiz</author>
</authors>
<title>Bilingual N-gram Statistical Machine Translation.</title>
<booktitle>Proceedings of the MT-Summit X,</booktitle>
<pages>275--282</pages>
<location>Phuket, Thailand,</location>
<marker>Marino, Banchs, Crego, de Gispert, Lambert, Fonollosa, Ruiz, </marker>
<rawString>Marino J. B., Banchs R., Crego J. M., A. de Gispert, P. Lambert, J. A. Fonollosa and M. Ruiz, Bilingual N-gram Statistical Machine Translation. Proceedings of the MT-Summit X, Phuket, Thailand, 275-282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Goto</author>
<author>N Kato</author>
<author>N Uratani</author>
<author>T Ehara</author>
</authors>
<date>2003</date>
<booktitle>Transliteration considering Context Information based on the Maximum Entropy Method. Proceeding of the MT-Summit IX,</booktitle>
<pages>125--132</pages>
<location>New Orleans, USA,</location>
<contexts>
<context position="5614" citStr="Goto et al., 2003" startWordPosition="807" endWordPosition="810">roposed in the recent past for machine transliteration using transformation-based learning algorithm (Meng et al., 2001; Jung et al., 2000; Vigra and Khudanpur, 2003). (Abduljaleel and Larkey, 2003) have presented a simple statistical technique to train an EnglishArabic transliteration model from pairs of names. The two-stage training procedure first learns which n-gram segments should be added to unigram inventory for the source language, and then a second stage learns the translation model over this inventory. This technique requires no heuristic or linguistic knowledge of either language. (Goto et al., 2003) described an EnglishJapanese transliteration method in which an English word is divided into conversion units that are partial English character strings in an English word and each English conversion unit is converted into a partial Japanese Katakana character string. It calculates the likelihood of a particular choice of letters of chunking into English conversion units for an English word by linking them to Katakana characters using syllables. Thus the English conversion units consider phonetic aspects. It considers the English and Japanese contextual information simultaneously to calculate</context>
<context position="7380" citStr="Goto et al., 2003" startWordPosition="1058" endWordPosition="1061">gual dictionary. The joint source-channel model tries to capture how source and target names can be generated simultaneously, i.e., the context information in both the source and the target sides are taken into account. A tuple n-gram transliteration model (Marino et al., 2005; Crego et al., 2005) has been loglinearly combined with feature functions to develop a statistical machine translation system for Spanish-to-English and English-to-Spanish translation tasks. The model approximates the joint probability between source and target languages by using trigrams. The present work differs from (Goto et al., 2003; Haizhou et al., 2004) in the sense that identification of the transliteration units in the source language is done using regular expressions and no probabilistic model is used. The proposed modified joint source-channel model is similar to the model proposed by (Goto et. al., 2003) but it differs in the way the transliteration units and the contextual information are defined in the present work. No linguistic knowledge is used in (Goto et al., 2003; Haizhou et al., 2004) whereas the present work uses linguistic knowledge in the form of possible conjuncts and diphthongs in Bengali. The paper </context>
<context position="16586" citStr="Goto et al., 2003" startWordPosition="2544" endWordPosition="2547">erated. K P(B,E) = II P( &lt;b,e&gt;k i |&lt;b,e&gt;k-1) k=1 194 • Model E This is basically the trigram model where the previous and the next source TUs are considered as the context K P(B,E) = II P(&lt;b,e&gt;k |bk-1, bk+1) k=1 • Model F In this model, the previous and the next TUs in the source and the previous target TU are considered as the context. This is the modified joint source-channel model . K P(B,E) = II P (&lt;b,e&gt;k |&lt;b,e&gt;k-1, bk+1) k=1 The performance of the system is evaluated in terms of Transliteration Unit Agreement Ratio (TUAR) and Word Agreement Ratio (WAR) following the evaluation scheme in (Goto et al., 2003). The evaluation parameter Character Agreement Ratio in (Goto et al., 2003) has been modified to Transliteration Unit Agreement Ratio as vowel modifier matra symbols in Bengali words are not independent and must always follow a consonant or a conjunct in a Transliteration Unit. Let, B be the input Bengali word, E be the English transliteration given by the user in open test and E/ be the system generates the transliteration..TUAR is defined as, TUAR = (L-Err)/ L, where L is the number of TUs in E, and Err is the number of wrongly transliterated TUs in E/ generated by the system. WAR is defined</context>
</contexts>
<marker>Goto, Kato, Uratani, Ehara, 2003</marker>
<rawString>Goto I., N. Kato, N. Uratani, and T. Ehara. 2003. Transliteration considering Context Information based on the Maximum Entropy Method. Proceeding of the MT-Summit IX, New Orleans, USA, 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Zhang Min</author>
<author>Su Jian</author>
</authors>
<title>A Joint Source-Channel Model for Machine Transliteration.</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting of the ACL (ACL 2004),</booktitle>
<pages>159--166</pages>
<location>Barcelona,</location>
<marker>Li, Min, Jian, 2004</marker>
<rawString>Haizhou Li, Zhang Min, Su Jian. 2004. A Joint Source-Channel Model for Machine Transliteration. Proceedings of the 42nd Annual Meeting of the ACL (ACL 2004), Barcelona, Spain, 159-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung Sung Young</author>
<author>Sung Lim Hong</author>
<author>Eunok Paek</author>
</authors>
<title>An English to Korean Transliteration Model of Extended Markov Window.</title>
<date>2000</date>
<booktitle>Proceedings of COLING</booktitle>
<volume>1</volume>
<pages>383--389</pages>
<marker>Young, Hong, Paek, 2000</marker>
<rawString>Jung Sung Young, Sung Lim Hong, and Eunok Paek. 2000. An English to Korean Transliteration Model of Extended Markov Window. Proceedings of COLING 2000, 1, 383-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<journal>Machine Transliteration, Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>599--612</pages>
<contexts>
<context position="4195" citStr="Knight and Graehl, 1998" startWordPosition="609" endWordPosition="612">son names, locations and organizations. A machine transliteration system that is trained on person names is very important in a multilingual country like India where large name collections like census data, electoral roll and railway reservation information must be available to multilingual citizens of the country in their vernacular. In the present work, the various proposed models have been evaluated on a training corpus of person names. A hybrid neural network and knowledge-based system to generate multiple English spellings for Arabic personal names is described in (Arbabi et al., 1994). (Knight and Graehl, 1998) developed a phoneme-based statistical model using finite state transducer that implements transformation rules to do back-transliteration. (Stalls and Knight, 1998) adapted this approach for back transliteration from Arabic to English for English names. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models </context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Knight K. and J. Graehl. 1998. Machine Transliteration, Computational Linguistics, 24(4): 599-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meng Helen M</author>
<author>Wai-Kit Lo</author>
<author>Berlin Chen</author>
<author>Karen Tang</author>
</authors>
<title>Generating Phonetic Cognates to handle Name Entities in English-Chinese Crosslanguage Spoken Document Retrieval.</title>
<date>2001</date>
<booktitle>Proceedings of the Automatic Speech Recognition and Understanding (ASRU) Workshop,</booktitle>
<location>Trento, Italy.</location>
<marker>M, Lo, Chen, Tang, 2001</marker>
<rawString>Meng Helen M., Wai-Kit Lo, Berlin Chen and Karen Tang. 2001. Generating Phonetic Cognates to handle Name Entities in English-Chinese Crosslanguage Spoken Document Retrieval. Proceedings of the Automatic Speech Recognition and Understanding (ASRU) Workshop, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Glover Stalls</author>
<author>K Knight</author>
</authors>
<title>Translating names and technical terms in Arabic text.</title>
<date>1998</date>
<booktitle>Proceedings of the COLING/ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>34--41</pages>
<location>Montral, Canada,</location>
<contexts>
<context position="4360" citStr="Stalls and Knight, 1998" startWordPosition="629" endWordPosition="632"> large name collections like census data, electoral roll and railway reservation information must be available to multilingual citizens of the country in their vernacular. In the present work, the various proposed models have been evaluated on a training corpus of person names. A hybrid neural network and knowledge-based system to generate multiple English spellings for Arabic personal names is described in (Arbabi et al., 1994). (Knight and Graehl, 1998) developed a phoneme-based statistical model using finite state transducer that implements transformation rules to do back-transliteration. (Stalls and Knight, 1998) adapted this approach for back transliteration from Arabic to English for English names. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. Several </context>
</contexts>
<marker>Stalls, Knight, 1998</marker>
<rawString>Stalls, Bonnie Glover and Knight K. 1998. Translating names and technical terms in Arabic text. Proceedings of the COLING/ACL Workshop on Computational Approaches to Semitic Languages, Montral, Canada, 34-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Virga Paola</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Transliteration of Proper Names in Crosslingual Information Retrieval.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL 2003 Workshop on Multilingual and Mixedlanguage Named Entity Recognition,</booktitle>
<pages>57--60</pages>
<location>Sapporo, Japan,</location>
<marker>Paola, Khudanpur, 2003</marker>
<rawString>Virga Paola and Sanjeev Khudanpur. 2003. Transliteration of Proper Names in Crosslingual Information Retrieval. Proceedings of the ACL 2003 Workshop on Multilingual and Mixedlanguage Named Entity Recognition, Sapporo, Japan, 57-60.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>