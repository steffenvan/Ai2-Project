<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000634">
<title confidence="0.997527">
Computing Optimal Alignments for the IBM-3 Translation Model
</title>
<author confidence="0.994569">
Thomas Schoenemann
</author>
<affiliation confidence="0.9018095">
Centre for Mathematical Sciences
Lund University, Sweden
</affiliation>
<sectionHeader confidence="0.975671" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993184">
Prior work on training the IBM-3 transla-
tion model is based on suboptimal meth-
ods for computing Viterbi alignments. In
this paper, we present the first method
guaranteed to produce globally optimal
alignments. This not only results in im-
proved alignments, it also gives us the op-
portunity to evaluate the quality of stan-
dard hillclimbing methods. Indeed, hill-
climbing works reasonably well in prac-
tice but still fails to find the global opti-
mum for between 2% and 12% of all sen-
tence pairs and the probabilities can be
several tens of orders of magnitude away
from the Viterbi alignment.
By reformulating the alignment problem
as an Integer Linear Program, we can
use standard machinery from global opti-
mization theory to compute the solutions.
We use the well-known branch-and-cut
method, but also show how it can be cus-
tomized to the specific problem discussed
in this paper. In fact, a large number of
alignments can be excluded from the start
without losing global optimality.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901490566038">
Brown et al. (1993) proposed to approach the
problem of automatic natural language translation
from a statistical viewpoint and introduced five
probability models, known as IBM 1-5. Their
models were single word based, where each
source word could produce at most one target
word.
State-of-the-art statistical translation systems
follow the phrase based approach, e.g. (Och and
Ney, 2000; Marcu and Wong, 2002; Koehn, 2004;
Chiang, 2007; Hoang et al., 2007), and hence al-
low more general alignments. Yet, single word
based models (Brown et al., 1993; Brown et al.,
1995; Vogel et al., 1996) are still highly relevant:
many phrase based systems extract phrases from
the alignments found by training the single word
based models, and those that train phrases directly
usually underperform these systems (DeNero et
al., 2006).
Single word based models can be divided into
two classes. On the one hand, models like IBM-1,
IBM-2 and the HMM are computationally easy to
handle: both marginals and Viterbi alignments can
be computed by dynamic programming or even
simpler techniques.
On the other hand there are fertility based mod-
els, including IBM 3-5 and Model 6. These mod-
els have been shown to be of higher practical rel-
evance than the members of the first class (Och
and Ney, 2003) since they usually produce better
alignments. At the same time, computing Viterbi
alignments for these methods has been shown to
be NP-hard (Udupa and Maji, 2006), and comput-
ing marginals is no easier.
The standard way to handle these models – as
implemented in GIZA++ (Al-Onaizan et al., 1999;
Och and Ney, 2003) – is to use a hillclimbing al-
gorithm. Recently Udupa and Maji (2005) pro-
posed an interesting approximation based on solv-
ing sequences of exponentially large subproblems
by means of dynamic programming and also ad-
dressed the decoding problem. In both cases there
is no way to tell how far away the result is from
the Viterbi alignment.
In this paper we solve the problem of find-
ing IBM-3 Viterbi alignments by means of Inte-
ger Linear Programming (Schrijver, 1986). While
there is no polynomial run-time guarantee, in prac-
tice the applied branch-and-cut framework is fast
enough to find optimal solutions even for the large
Canadian Hansards task (restricted to sentences
with at most 75 words), with a training time of 6
hours on a 2.4 GHz Core 2 Duo (single threaded).
</bodyText>
<page confidence="0.859168">
98
</page>
<note confidence="0.958772">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 98–106,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99908395">
Integer Linear Programming in the context of
machine translation first appeared in the work of
Germann et al. (2004), who addressed the trans-
lation problem (often called decoding) in terms of
a travelings-salesman like formulation. Recently,
DeNero and Klein (2008) addressed the training
problem for phrase-based models by means of
integer linear programming, and proved that the
problem is NP-hard. The main difference to our
work is that they allow only consecutive words in
the phrases. In their formulation, allowing arbi-
trary phrases would require an exponential number
of variables. In contrast, our approach handles the
classical single word based model where any kind
of “phrases” in the source sentence are aligned to
one-word phrases in the target sentence.
Lacoste-Julien et al. (2006) propose an inte-
ger linear program for a symmetrized word-level
alignment model. Their approach also allows to
take the alignments of neighboring words into ac-
count. In contrast to our work, they only have a
very crude fertility model and they are consider-
ing a substantially different model. It should be
noted, however, that a subclass of their problems
can be solved in polynomial time - the problem is
closely related to bipartite graph matching. Less
general approaches based on matching have been
proposed in (Matusov et al., 2004) and (Taskar et
al., 2005).
Recently Bodrumlu et al. (2009) proposed a
very innovative cost function for jointly optimiz-
ing dictionary entries and alignments, which they
minimize using integer linear programming. They
also include a mechanism to derive N-best lists.
However, they mention rather long computation
times for rather small corpora. It is not clear if the
large Hansards tasks could be addressed by their
method.
An overview of integer linear programming ap-
proaches for natural language processing can be
found on http://ilpnlp.wikidot.com/.
To facilitate further research in this area, the
source code will be made publicly available.
Contribution The key contribution of our work
is a method to handle exact fertility models as aris-
ing in the IBM-3 model in a global optimization
framework. This is done by a linear number of
linear consistency constraints. Unlike all previ-
ous works on integer linear programming for ma-
chine translation, we do not solely use binary co-
efficients in the constraint matrix, hence showing
that the full potential of the method has so far not
been explored.
At the same time, our method allows us to give a
detailed analysis of the quality of hillclimbing ap-
proaches. Moreover, we give a more detailed de-
scription of how to obtain a fast problem-tailored
integer solver than in previous publications, and
include a mechanism to a priori exclude some vari-
ables without losing optimality.
</bodyText>
<sectionHeader confidence="0.876837" genericHeader="method">
2 The IBM-3 Translation Model
</sectionHeader>
<bodyText confidence="0.999577666666667">
Given a source sentence fJ1 , the statistical ap-
proach to machine translation is to assign each
possible target sentence eI1 a probability to be an
accurate translation. For convenience in the trans-
lation process, this probability is usually rewritten
as
</bodyText>
<equation confidence="0.9960695">
P(eI1|fJ1 ) = 11) ,
p(fJ 1 ) · p(eI 1) · p(fJ 1 |eI
</equation>
<bodyText confidence="0.999799">
and the training problem is to derive suitable pa-
rameters for the latter term from a bilingual cor-
pus. Here, the probability is expressed by sum-
ming over hidden variables called alignments. The
common assumption in single word based models
is that each source position j produces a single tar-
get position aj ∈ {0, ... , I}, where an artificial 0-
position has been introduced to mark words with-
out a correspondence in the target sentence. The
alignment of a source sentence is then a vector aJ1 ,
and the probability can now be written as
</bodyText>
<equation confidence="0.943513">
X
p(fJ 1 |eI 1) =
ai
</equation>
<bodyText confidence="0.999830142857143">
We will focus on training the IBM-3 model which
is based on the concept of fertilities: given an
alignment aJ1, the fertility Φi(aJ1) = Pj:aj=i 1
of target word i expresses the number of source
words aligned to it. Omitting the dependence on
aJ1 (and defining p(j|0) = 1), the probability is
expressed as
</bodyText>
<equation confidence="0.945832">
h i
Φi! p(Φi|ei)
Y· h i
j p(fj|eaj) · p(j|aj) . (1)
</equation>
<bodyText confidence="0.99981125">
For the probability p(Φ0|J) of the fertility of the
empty word, we use the modification introduced in
(Och and Ney, 2003), see there for details. In sum-
mary, the model comprises a single word based
</bodyText>
<equation confidence="0.9958355">
p(fJ1 , aJ1 |eI1) .
1
p(fJ1 , aJ1 |eI1) = p(Φ0|J) · YI
i=1
</equation>
<page confidence="0.973004">
99
</page>
<bodyText confidence="0.99989175">
translation model, an inverted zero-order align-
ment model and a fertility model. We now discuss
how to find the optimal alignment for given prob-
abilities, i.e. to solve the problem
</bodyText>
<equation confidence="0.5195225">
arg max p(fJ1 ,aJ1 |eI�) (2)
al
</equation>
<bodyText confidence="0.97879175">
for each bilingual sentence pair in the training
set. This is a desirable step in the approximate
EM-algorithm that is commonly used to train the
model.
</bodyText>
<sectionHeader confidence="0.955248" genericHeader="method">
3 Finding IBM-3 Viterbi Alignments via
Integer Linear Programming
</sectionHeader>
<bodyText confidence="0.999902214285714">
Instead of solving (2) directly we consider the
equivalent task of minimizing the negative loga-
rithm of the probability function. A significant
part of the arising cost function is already linear
in terms of the alignment variables, a first step for
the integer linear program (ILP) we will derive.
To model the problem as an ILP, we introduce
two sets of variables. Firstly, for any source po-
sition j ∈ {1, ... , J} and any target position
i ∈ {0, ... , I} we introduce an integer variable
xij ∈ {0,1} which we want to be 1 exactly if
aj = i and 0 otherwise. Since each source posi-
tion must be aligned to exactly one target position,
we arrive at the set of linear constraints
</bodyText>
<equation confidence="0.95825">
X xij = 1 , j = 1,... ,J . (3)
i
</equation>
<bodyText confidence="0.999796666666667">
The negative logarithm of the bottom row of (1) is
now easily written as a linear function in terms of
the variables xij:
</bodyText>
<equation confidence="0.960198">
X cxij · xij ,
i,j
h i
cx ij = − log p(fj|ei) · p(j|i) .
</equation>
<bodyText confidence="0.999955125">
For the part of the cost depending on the fertilities,
we introduce another set of integer variables yif ∈
{0, 1}. Here i ∈ {0,... , I} and f ranges from 0 to
some pre-specified limit on the maximal fertility,
which we set to max(15, J/2) in our experiments
(fertilities &gt; J need not be considered). We want
yif to be 1 if the fertility of i is f, 0 otherwise.
Hence, again these variables must sum to 1:
</bodyText>
<equation confidence="0.8555415">
X yif = 1 , i = 0, ... , I . (4)
f
</equation>
<bodyText confidence="0.9780615">
The associated part of the cost function is written
as
</bodyText>
<equation confidence="0.980398285714286">
X cyif · yif ,
i,f
h i
cif = − log
y f! p(f|ei) , i = 1,... ,I
h i
cy 0f = − log p(Φ0 = f|J) .
</equation>
<bodyText confidence="0.99996575">
It remains to ensure that the variables yif express-
ing the fertilities are consistent with the fertilities
induced by the alignment variables xij. This is
done via the following set of linear constraints:
</bodyText>
<equation confidence="0.996598375">
X Xxij = f · yif , i = 0, ... , I . (5)
j f
Problem (2) is now reduced to solving the integer
linear program
ar minPcxij xij + Pcyi f yi f
1xgz,},1yO i,j i,f
subject to (3), (4), (5)
xij ∈ {0, 1}, yif ∈ {0, 1} ,(6)
</equation>
<bodyText confidence="0.9634705">
with roughly 2I J variables and roughly J + 2I
constraints.
</bodyText>
<sectionHeader confidence="0.862719" genericHeader="method">
4 Solving the Integer Linear Program
</sectionHeader>
<bodyText confidence="0.999537125">
To solve the arising integer linear programming
problem, we first relax the integrality constraints
on the variables to continuous ones:
xij ∈ [0, 1], yif ∈ [0, 1] ,
and obtain a lower bound on the problems by solv-
ing the arising linear programming relaxation via
the dual simplex method.
While in practice this can be done in a matter of
milli-seconds even for sentences with I, J &gt; 50,
the result is frequently a fractional solution. Here
the alignment variables are usually integral but the
fertility variables are not.
In case the LP-relaxation does not produce an
integer solution, the found solution is used as the
initialization of a branch-and-cut framework. Here
one first tries to strengthen the LP-relaxation by
deriving additional inequalities that must be valid
for all integral solutions see e.g. (Schrijver, 1986;
Wolter, 2006) and www.coin-or.org. These
inequalities are commonly called cuts. Then one
applies a branch-and-bound scheme on the inte-
ger variables. In each step of this scheme, addi-
tional inequalities are derived. The process is fur-
ther sped-up by introducing a heuristic to derive an
</bodyText>
<page confidence="0.956637">
100
</page>
<bodyText confidence="0.999958777777778">
upper bound on the cost function. Such bounds are
generally given by feasible integral solutions. We
use our own heuristic as a plug-in to the solver.
It generates solutions by thresholding the align-
ment variables (winner-take-all) and deriving the
induced fertility variables. An initial upper bound
is furthermore given by the alignment found by
hillclimbing.
We suspect that further speed-ups are possible
by using so-called follow-up nodes: e.g. if in the
branch-and-bound an alignment variable xij is set
to 1, one can conclude that the fertility variable
yi0 must be 0. Also, sets of binary variables that
must sum to 1 as in (3) and (4) are known as spe-
cial ordered sets of type I and there are variants
of branch-and-cut that can exploit these proper-
ties. However, in our context they did not result
in speed-ups.
Our code is currently based on the open source
COIN-OR project1 and involves the linear pro-
gramming solver CLP, the integer programming
solver CBC, and the cut generator library CGL.
We have also tested two commercial solvers. For
the problem described in this paper, CBC per-
formed best. Tests on other integer programming
tasks showed however that the Gurobi solver out-
performs CBC on quite a number of problems.
</bodyText>
<sectionHeader confidence="0.942787" genericHeader="method">
5 Speed-ups by Deriving Bounds
</sectionHeader>
<bodyText confidence="0.999707105263158">
It turns out that, depending on the cost function,
some variables may a priori be excluded from the
optimization problem without losing global opti-
mality. That is, they can be excluded even before
the first LP-relaxation is solved.
The affected variables have relatively high cost
coefficients and they are identified by considering
lower bounds and an upper bound on the cost func-
tion. Starting from the lower bounds, one can then
identify variables that when included in a solution
would raise the cost beyond the upper bound.
An upper bound u on the problem is given by
any alignment. We use the one found by hillclimb-
ing. If during the branch-and-cut process tighter
upper bounds become available, the process could
be reapplied (as a so-called column cut generator).
For the lower bounds we use different ones to
exclude alignment variables and to exclude fertil-
ity variables.
</bodyText>
<footnote confidence="0.984703">
1www.coin-or.org
</footnote>
<subsectionHeader confidence="0.987266">
5.1 Excluding Alignment Variables
</subsectionHeader>
<bodyText confidence="0.999915142857143">
To derive a lower bound for the alignment vari-
ables, we first observe that the cost cxij for the
alignment variables are all positive, whereas the
cost cyif for the fertilities are frequently negative,
due to the factorial of f. A rather tight lower
bound on the fertility cost can be derived by solv-
ing the problem
</bodyText>
<equation confidence="0.997513666666667">
lF,1 = min
{Φi}
s.t. Pi Φi = J , (7)
</equation>
<bodyText confidence="0.996732">
which is easily solved by dynamic programming
proceeding along i. A lower bound on the align-
ment cost is given by
</bodyText>
<equation confidence="0.742845333333333">
lA = Pj lA,j ,
where lA,j = min cxij .
i=0,...,I
</equation>
<bodyText confidence="0.996244333333333">
The lower bound is then given by l1 = lF,1 + lA,
and we can be certain that source word j will not
be aligned to target word i if
</bodyText>
<equation confidence="0.469006">
cxij &gt; lA,j + (u − l1) .
</equation>
<subsectionHeader confidence="0.998575">
5.2 Excluding Fertility Variables
</subsectionHeader>
<bodyText confidence="0.999953">
Excluding fertility variables is more difficult as
cost can be negative and we have used a constraint
to derive lF,1 above.
At present we are using a two ways to gener-
ate a lower bound and apply the exclusion process
with each of them sequentially. Both bounds are
looser than l1, but they immensely help to get the
computation times to an acceptable level.
The first bound builds upon l1 as derived above,
but using a looser bound lF,2 for the fertility cost:
</bodyText>
<equation confidence="0.984916">
XlF,2 =
i
</equation>
<bodyText confidence="0.9957818">
This results in a bound l2 = lF,2 + lA, and fertility
variables can now be excluded in a similar manner
as above.
Our second bound is usually much tighter and
purely based on the fertility variables:
</bodyText>
<equation confidence="0.957638916666667">
X min hcy iΦi + min i
l3 = Φi J ⊆{1,...,J} : |J =Φi |cx i (J ) ,
i
with cxi (J ) = X cxij ,
j∈J
PI
i=0
y
ciΦi
min
Φi
cyiΦi .
</equation>
<page confidence="0.979998">
101
</page>
<bodyText confidence="0.9999771">
and where the cost of the empty set is defined as 0.
Although this expression looks rather involved, it
is actually quite easy to compute by simply sorting
the respective cost entries. A fertility variable yif
can now be excluded if the difference between Zf
and the contribution of i to l3 exceeds u − l3.
We consider it likely that more variables can be
excluded by deriving bounds in the spirit of (7),
but with the additional constraint that 4bi = f for
some i and f. We leave this for future work.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.998598685714286">
We have tested our method on three different tasks
involving a total of three different languages and
each in both directions. The first task is the well-
known Canadian Hansards2 task (senate debates)
for French and English. Because of the large
dataset we are currently only considering sentence
pairs where both sentences have at most 75 words.
Longer sentences are usually not useful to derive
model parameters.
The other two datasets are released by the Eu-
ropean Corpus Initiative3. We choose the Union
Bank of Switzerland (UBS) corpus for English and
German and the Avalanche Bulletins, originally
released by SFISAR, for French and German. For
the latter task we have annotated alignments for
150 of the training sentences, where one annota-
tor specified sure and possible alignments. For de-
tails, also on the alignment error rate, see (Och and
Ney, 2003).
All corpora have been preprocessed with
language-specific rules; their statistics are given in
Table 1. We have integrated our method into the
standard toolkit GIZA++4 and are using the train-
ing scheme 15H53545 for all tasks. While we fo-
cus on the IBM-3 stage, we also discuss the quality
of the resulting IBM-4 parameters and alignments.
Experiments were run on a 2.4 GHz Core 2
Duo with 4 GB memory. For most sentence pairs,
the memory consumption of our method is only
marginally more than in standard GIZA++ (600
MB). In the first iteration on the large Hansards
task, however, there are a few very difficult sen-
tence pairs where the solver needs up to 90 min-
utes and 1.5 GB . We observed this in both trans-
lation directions.
</bodyText>
<footnote confidence="0.9952958">
2www.isi.edu/natural-language/
download/hansard/
3The entire CD with many more corpora is available for
currently 50 Euros.
4available at code.google.com/p/giza-pp/ .
</footnote>
<table confidence="0.999911">
Avalanche Bulletin
French German
# sentences 2989
max. sentence length 88 57
total words 64825 45629
vocabulary size 1707 2113
UBS
English German
# sentences 2689
max. sentence length 92 91
total words 62617 53417
vocabulary size 5785 9127
Canadian Hansards (max. 75)
French English
# sentences 180706
max. sentence length 75 75
total words 3730570 3329022
vocabulary size 48065 37633
</table>
<tableCaption confidence="0.9763435">
Table 1: Corpus statistics for all employed (train-
ing) corpora, after preprocessing.
</tableCaption>
<subsectionHeader confidence="0.998772">
6.1 Evaluating Hillclimbing
</subsectionHeader>
<bodyText confidence="0.9990903">
In our first set of experiments, we compute Viterbi
alignments merely to evaluate the quality of the
standard training process. That is, the model
parameters are updated based on the alignments
found by hillclimbing. Table 2 reveals that, as
expected, hillclimbing does not always find the
global optimum: depending on the task and it-
eration number, between 2 and 12 percent of all
hillclimbing alignments are suboptimal. For short
sentences (i.e. I, J &lt; 20) hillclimbing usually
finds the global optimum.
Somewhat more surprisingly, even when a good
and hence quite focused initialization of the IBM-
3 model parameters is given (by training HMMs
first), the probability of the Viterbi alignment can
be up to a factor of 1037 away from the optimum.
This factor occurred on the Hansards task for a
sentence pair with 46 source and 46 target words
and the fertility of the empty word changed from
9 (for hillclimbing) to 5.
</bodyText>
<subsectionHeader confidence="0.999883">
6.2 Hillclimbing vs. Viterbi Alignments
</subsectionHeader>
<bodyText confidence="0.99918875">
We now turn to a training scheme where the
Viterbi alignments are used to actually update the
model parameters, and compare it to the standard
training scheme (based on hillclimbing).
</bodyText>
<page confidence="0.992942">
102
</page>
<table confidence="0.999765740740741">
Candian Hansards (max 75)
French → English
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 10.7% 10.7% 10.8% 11.1% 11.4%
Maximal factor to Viterbi alignment 1.9 · 1037 9.1 · 1017 7.3 · 1014 3.3 · 1012 8.1 · 1014
English → French
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 7.3% 7.5% 7.4% 7.4% 7.5%
Maximal factor to Viterbi alignment 5.6 · 1038 6.6 · 1020 7.6 · 1011 4.3 · 1010 8.3 · 1011
Avalanche Bulletins
French → German
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 7.5% 5.6% 4.9% 4.9% 4.4%
Maximal factor to Viterbi alignment 6.1 · 105 877 368 2.5 · 104 429
German → French
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 4.2% 2.7% 2.5% 2.3% 2.1%
Maximal factor to Viterbi alignment 40 302 44 3.3 · 104 9.2 · 104
Union Bank of Switzerland (UBS)
English → German
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 5.0% 4.0% 3.5% 3.3% 3.2%
Maximal factor to Viterbi alignment 677 22 53 40 32
German → English
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 5.5% 3.3% 2.5% 2.2% 2.3%
Maximal factor to Viterbi alignment 1.4 · 107 808 33 33 1.8 · 104
</table>
<tableCaption confidence="0.8487765">
Table 2: Analysis of Hillclimbing on all considered tasks. All numbers are for the IBM-3 translation
model. Iteration 1 is the first iteration after the transfer from HMM, the final iteration is the transfer to
IBM4. The factors are w.r.t. the original formulation, not the negative logarithm of it and are defined as
the maximal ratio between the Viterbi probability and the hillclimbing probability.
</tableCaption>
<page confidence="0.99914">
103
</page>
<bodyText confidence="0.9945715">
une baisse de la temp´erature a en g´en´eral stabilis´e la couverture neigeuse .
ein Temperaturr¨uckgang hat die Schneedecke im allgemeinen stabilisiert .
</bodyText>
<subsectionHeader confidence="0.69206">
Standard training (hillclimbing).
</subsectionHeader>
<bodyText confidence="0.864443">
une baisse de la temp´erature a en g´en´eral stabilis´e la couverture neigeuse .
ein Temperaturr¨uckgang hat die Schneedecke im allgemeinen stabilisiert .
</bodyText>
<figureCaption confidence="0.7027005">
Proposed training (Viterbi alignments).
Figure 1: Comparison of training schemes. Shown are the alignments of the final IBM-3 iteration.
</figureCaption>
<bodyText confidence="0.999551387096774">
Indeed Table 3 demonstrates that with the new
training scheme, the perplexities of the final IBM-
3 iteration are consistently lower. Yet, this effect
does not carry over to IBM-4 training, where the
perplexities are consistently higher. Either this is
due to overfitting or it is better to use the same
method for alignment computation for both IBM-
3 and IBM-4. After all, both start from the HMM
Viterbi alignments.
Interestingly, the maximal factor between the
hillclimbing alignment and the Viterbi alignment
is now consistently higher on all tasks and in all
iterations. The extreme cases are a factor of 1076
for the Canadian Hansards English —* French task
and 1030 for the Bulletin French —* German task.
Table 4 demonstrates that the alignment error
rates of both schemes are comparable. Indeed, a
manual evaluation of the alignments showed that
most of the changes affect words like articles or
prepositions that are generally hard to translate.
In many cases neither the heuristic nor the Viterbi
alignment could be considered correct. An inter-
esting case where the proposed scheme produced
the better alignment is shown in Figure 1.
In summary, our results give a thorough justi-
fication for the commonly used heuristics. A test
with the original non-deficient empty word model
of the IBM-3 furthermore confirmed the impres-
sion of (Och and Ney, 2003) that overly many
words are aligned to the empty word: the tendency
is even stronger in the Viterbi alignments.
</bodyText>
<subsectionHeader confidence="0.998509">
6.3 Optimizing Running Time
</subsectionHeader>
<bodyText confidence="0.9979175">
The possibilities to influence the run-times of the
branch-and-cut framework are vast: there are nu-
</bodyText>
<table confidence="0.999588875">
Union Bank (UBS) E —* G
Final IBM-3 Final IBM-4
Standard train. 49.21 35.73
Proposed train. 49.00 35.76
Union Bank (UBS) G —* E
Final IBM-3 Final IBM-4
Standard train. 62.38 47.39
Proposed train. 62.08 47.43
Avalanche F —* G
Final IBM-3 Final IBM-4
Standard train. 35.44 21.99
Proposed train. 35.23 22.04
Avalanche G —* F
Final IBM-3 Final IBM-4
Standard train. 34.60 22.78
Proposed train. 34.48 22.76
Canadian Hansards F —* E
Final IBM-3 Final IBM-4
Standard train. 105.28 55.22
Proposed train. 92.09 55.35
Canadian Hansards E —* F
Final IBM-3 Final IBM-4
Standard train. 70.58 37.64
Proposed train. 70.03 37.73
</table>
<tableCaption confidence="0.999581">
Table 3: Analysis of the perplexities in training.
</tableCaption>
<page confidence="0.968028">
104
</page>
<table confidence="0.999214">
French —* German
Final IBM-3 Final IBM-4
Standard train. 24.31% 23.01%
Proposed train. 24.31% 23.24%
German —* French
Final IBM-3 Final IBM-4
Standard train. 33.03% 33.44%
Proposed train. 33.00% 33.27%
</table>
<tableCaption confidence="0.9707925">
Table 4: Alignment error rates on the Avalanche
bulletin task.
</tableCaption>
<bodyText confidence="0.998212783783784">
merous ways to generate cuts and several of them
can be used simultaneously. The CBC-package
also allows to specify how many rounds of cuts
to derive at each node. Then there is the question
of whether to use the bounds derived in Section
5 to a priori exclude variables. Finally, branch-
and-cut need not be done on all variables: since
solving LP-relaxations typically results in integral
alignments, it suffices to do branch-and-cut on the
fertility variables and only add the alignment vari-
ables in case non-integral values arise (this never
happened in our experiments5).
We could not possibly test all combinations
of the listed possibilities, and our primary focus
was to achieve acceptable run-times for the large
Hansards task. Still, in the end we have a quite
uniform picture: the lowest run-times are achieved
by using Gomory Cuts only. Moreover, including
all variables for branching was between 1.5 and 2
times faster than only including fertility variables.
Only by exploiting the bounds derived in Section
5 the run-times for the Hansards task in direction
from English to French became acceptable. We
believe that further speed-ups are possible by de-
riving tighter bounds, and are planning to investi-
gate this in the future.
We end up with roughly 6 hours for the
Hansards task, roughly 3 minutes for the UBS
task, and about 2.5 minutes for the Avalanche task.
In all cases the run-times are much higher than
in the standard GIZA++ training. However, we
are now getting optimality guarantees where pre-
viously one could not even tell how far away one is
from the optimum. And the Viterbi alignments of
several sentence pairs can of course be computed
in parallel.
Lastly, we mention the possibility of setting a
</bodyText>
<footnote confidence="0.6650395">
5In fact, when fixing the fertility variables, the problem
reduces to the polynomial time solvable assignment problem.
</footnote>
<bodyText confidence="0.999669833333333">
limit on the branch-and-cut process, either on the
running time or on the number of nodes. There is
then no longer a guarantee for global optimality,
but at least one is getting a bound on the gap to the
optimum and one can be certain that the training
time will be sufficiently low.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999027333333333">
We present the first method to compute IBM-3
Viterbi alignments with a guarantee of optimal-
ity. In contrast to other works on integer linear
programming for machine translation, our formu-
lation is able to include a precise and very gen-
eral fertility model. The resulting integer linear
program can be solved sufficiently fast in prac-
tice, and we have given many comments on how
problem-specific knowledge can be incorporated
into standard solvers.
The proposed method allows for the first time
to analyze the quality of hillclimbing approaches
for IBM-3 training. It was shown that they can be
very far from the optimum. At the same time, this
seems to happen mostly for difficult sentences that
are not suitable to derive good model parameters.
In future work we want to derive tighter bounds
to a priori exclude variables, combine the method
with the N-best list generation of (Bodrumlu et
al., 2009) and evaluate on a larger set of corpora.
Finally we are planning to test other integer pro-
gramming solvers.
Acknowledgments We thank Fredrik Kahl for
helpful comments and an anonymous reviewer for
pointing out freely available software packages.
This research was funded by the European Re-
search Council (GlobalVision grant no. 209480).
</bodyText>
<sectionHeader confidence="0.998607" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999818071428571">
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.
Smith, and D. Yarowsky. 1999. Statistical ma-
chine translation, Final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi.
2009. A new objective function for word align-
ment. In Proceedings of the Workshop on Inte-
ger Linear Programmingfor Natural Langauge Pro-
cessing (ILP), Boulder, Colorado, June.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263–311, June.
</reference>
<page confidence="0.98328">
105
</page>
<reference confidence="0.999876025641025">
P.F. Brown, J. Cocke, S.A. Della Pietra, V.J. Della
Pietra, F. Jelinek, J. Lai, and R.L. Mercer. 1995.
Method and system for natural language translation.
U.S. patent #5.477.451.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
J. DeNero and D. Klein. 2008. The complexity
of phrase alignment problems. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Columbus, Ohio, June.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models under-
perform surface heuristics. In StatMT ’06: Proceed-
ings of the Workshop on Statistical Machine Trans-
lation, pages 31–38, Morristown, NJ, USA, June.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2004. Fast decoding and optimal decod-
ing for machine translation. Artificial Intelligence,
154(1–2), April.
H. Hoang, A. Birch, C. Callison-Burch, R. Zens,
A. Constantin, M. Federico, N. Bertoldi, C. Dyer,
B. Cowan, W. Shen, C. Moran, and O. Bojar. 2007.
Moses: Open source toolkit for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 177–
180, Prague, Czech Republic, June.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation mod-
els. In Conference of the Association for Machine
Translation in the Americas (AMTA), pages 115–
124, Washington, D.C., October.
S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.
2006. Word alignment via quadratic assignment.
In Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, New York, New York, June.
D. Marcu and W. Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Philadelphia,
Pennsylvania, July.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In International Conference on Computational Lin-
guistics (COLING), Geneva, Switzerland, August.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Annual Meeting
of the Association for Computational Linguistics
(ACL), pages 440–447, Hongkong, China, October.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
A. Schrijver. 1986. Theory of Linear and Integer Pro-
gramming. Wiley-Interscience Series in Discrete
Mathematics and Optimization. John Wiley &amp; Sons.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005.
A discriminative matching approach to word align-
ment. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Vancouver,
Canada, October.
R. Udupa and H.K. Maji. 2005. Theory of align-
ment generators and applications to statistical ma-
chine translation. In The International Joint Con-
ferences on Artificial Intelligence, Edinburgh, Scot-
land, August.
R. Udupa and H.K. Maji. 2006. Computational com-
plexity of statistical machine translation. In Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), Trento, Italy,
April.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Inter-
national Conference on Computational Linguistics
(COLING), pages 836–841, Copenhagen, Denmark,
August.
K. Wolter. 2006. Implementation of Cutting Plane
Separators for Mixed Integer Programs. Master’s
thesis, Technische Universit¨at Berlin, Germany.
</reference>
<page confidence="0.997329">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982449">
<title confidence="0.999225">Computing Optimal Alignments for the IBM-3 Translation Model</title>
<author confidence="0.999937">Thomas Schoenemann</author>
<affiliation confidence="0.9977295">Centre for Mathematical Lund University, Sweden</affiliation>
<abstract confidence="0.999519846153846">Prior work on training the IBM-3 translation model is based on suboptimal methods for computing Viterbi alignments. In this paper, we present the first method guaranteed to produce globally optimal alignments. This not only results in improved alignments, it also gives us the opportunity to evaluate the quality of standard hillclimbing methods. Indeed, hillclimbing works reasonably well in practice but still fails to find the global optimum for between 2% and 12% of all sentence pairs and the probabilities can be several tens of orders of magnitude away from the Viterbi alignment. By reformulating the alignment problem as an Integer Linear Program, we can use standard machinery from global optimization theory to compute the solutions. We use the well-known branch-and-cut method, but also show how it can be customized to the specific problem discussed in this paper. In fact, a large number of alignments can be excluded from the start without losing global optimality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>J Curin</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>J Lafferty</author>
<author>I D Melamed</author>
<author>F J Och</author>
<author>D Purdy</author>
<author>N A Smith</author>
<author>D Yarowsky</author>
</authors>
<date>1999</date>
<note>Statistical machine translation, Final report, JHU workshop. http://www.clsp.jhu.edu/ws99/.</note>
<contexts>
<context position="2716" citStr="Al-Onaizan et al., 1999" startWordPosition="440" endWordPosition="443">ally easy to handle: both marginals and Viterbi alignments can be computed by dynamic programming or even simpler techniques. On the other hand there are fertility based models, including IBM 3-5 and Model 6. These models have been shown to be of higher practical relevance than the members of the first class (Och and Ney, 2003) since they usually produce better alignments. At the same time, computing Viterbi alignments for these methods has been shown to be NP-hard (Udupa and Maji, 2006), and computing marginals is no easier. The standard way to handle these models – as implemented in GIZA++ (Al-Onaizan et al., 1999; Och and Ney, 2003) – is to use a hillclimbing algorithm. Recently Udupa and Maji (2005) proposed an interesting approximation based on solving sequences of exponentially large subproblems by means of dynamic programming and also addressed the decoding problem. In both cases there is no way to tell how far away the result is from the Viterbi alignment. In this paper we solve the problem of finding IBM-3 Viterbi alignments by means of Integer Linear Programming (Schrijver, 1986). While there is no polynomial run-time guarantee, in practice the applied branch-and-cut framework is fast enough to</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, I. D. Melamed, F. J. Och, D. Purdy, N. A. Smith, and D. Yarowsky. 1999. Statistical machine translation, Final report, JHU workshop. http://www.clsp.jhu.edu/ws99/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tugba Bodrumlu</author>
<author>Kevin Knight</author>
<author>Sujith Ravi</author>
</authors>
<title>A new objective function for word alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programmingfor Natural Langauge Processing (ILP),</booktitle>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5084" citStr="Bodrumlu et al. (2009)" startWordPosition="821" endWordPosition="824">coste-Julien et al. (2006) propose an integer linear program for a symmetrized word-level alignment model. Their approach also allows to take the alignments of neighboring words into account. In contrast to our work, they only have a very crude fertility model and they are considering a substantially different model. It should be noted, however, that a subclass of their problems can be solved in polynomial time - the problem is closely related to bipartite graph matching. Less general approaches based on matching have been proposed in (Matusov et al., 2004) and (Taskar et al., 2005). Recently Bodrumlu et al. (2009) proposed a very innovative cost function for jointly optimizing dictionary entries and alignments, which they minimize using integer linear programming. They also include a mechanism to derive N-best lists. However, they mention rather long computation times for rather small corpora. It is not clear if the large Hansards tasks could be addressed by their method. An overview of integer linear programming approaches for natural language processing can be found on http://ilpnlp.wikidot.com/. To facilitate further research in this area, the source code will be made publicly available. Contributio</context>
</contexts>
<marker>Bodrumlu, Knight, Ravi, 2009</marker>
<rawString>Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009. A new objective function for word alignment. In Proceedings of the Workshop on Integer Linear Programmingfor Natural Langauge Processing (ILP), Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1161" citStr="Brown et al. (1993)" startWordPosition="185" endWordPosition="188">ctice but still fails to find the global optimum for between 2% and 12% of all sentence pairs and the probabilities can be several tens of orders of magnitude away from the Viterbi alignment. By reformulating the alignment problem as an Integer Linear Program, we can use standard machinery from global optimization theory to compute the solutions. We use the well-known branch-and-cut method, but also show how it can be customized to the specific problem discussed in this paper. In fact, a large number of alignments can be excluded from the start without losing global optimality. 1 Introduction Brown et al. (1993) proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5. Their models were single word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g. (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007), and hence allow more general alignments. Yet, single word based models (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996) are still highly relevant: m</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J Lai</author>
<author>R L Mercer</author>
</authors>
<title>Method and system for natural language translation.</title>
<date>1995</date>
<journal>U.S.</journal>
<pages>5--477</pages>
<contexts>
<context position="1711" citStr="Brown et al., 1995" startWordPosition="271" endWordPosition="274">without losing global optimality. 1 Introduction Brown et al. (1993) proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5. Their models were single word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g. (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007), and hence allow more general alignments. Yet, single word based models (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996) are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems (DeNero et al., 2006). Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be computed by dynamic programming or even simpler techniques. On the other hand there are fertility based models, including IBM 3-5 and Model 6. These mod</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lai, Mercer, 1995</marker>
<rawString>P.F. Brown, J. Cocke, S.A. Della Pietra, V.J. Della Pietra, F. Jelinek, J. Lai, and R.L. Mercer. 1995. Method and system for natural language translation. U.S. patent #5.477.451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1578" citStr="Chiang, 2007" startWordPosition="249" endWordPosition="250">stomized to the specific problem discussed in this paper. In fact, a large number of alignments can be excluded from the start without losing global optimality. 1 Introduction Brown et al. (1993) proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5. Their models were single word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g. (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007), and hence allow more general alignments. Yet, single word based models (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996) are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems (DeNero et al., 2006). Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be computed by dynamic</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3956" citStr="DeNero and Klein (2008)" startWordPosition="639" endWordPosition="642">utions even for the large Canadian Hansards task (restricted to sentences with at most 75 words), with a training time of 6 hours on a 2.4 GHz Core 2 Duo (single threaded). 98 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 98–106, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Integer Linear Programming in the context of machine translation first appeared in the work of Germann et al. (2004), who addressed the translation problem (often called decoding) in terms of a travelings-salesman like formulation. Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. The main difference to our work is that they allow only consecutive words in the phrases. In their formulation, allowing arbitrary phrases would require an exponential number of variables. In contrast, our approach handles the classical single word based model where any kind of “phrases” in the source sentence are aligned to one-word phrases in the target sentence. Lacoste-Julien et al. (2006) propose an integer linear program for a symmetrized word-level alig</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>J. DeNero and D. Klein. 2008. The complexity of phrase alignment problems. In Annual Meeting of the Association for Computational Linguistics (ACL), Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In StatMT ’06: Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="1964" citStr="DeNero et al., 2006" startWordPosition="310" endWordPosition="313">ngle word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g. (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007), and hence allow more general alignments. Yet, single word based models (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996) are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems (DeNero et al., 2006). Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be computed by dynamic programming or even simpler techniques. On the other hand there are fertility based models, including IBM 3-5 and Model 6. These models have been shown to be of higher practical relevance than the members of the first class (Och and Ney, 2003) since they usually produce better alignments. At the same time, computing Viterbi alignments for these methods has been shown to be NP-hard (</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In StatMT ’06: Proceedings of the Workshop on Statistical Machine Translation, pages 31–38, Morristown, NJ, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>K Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2004</date>
<journal>Artificial Intelligence,</journal>
<volume>154</volume>
<issue>1</issue>
<contexts>
<context position="3807" citStr="Germann et al. (2004)" startWordPosition="618" endWordPosition="621">rijver, 1986). While there is no polynomial run-time guarantee, in practice the applied branch-and-cut framework is fast enough to find optimal solutions even for the large Canadian Hansards task (restricted to sentences with at most 75 words), with a training time of 6 hours on a 2.4 GHz Core 2 Duo (single threaded). 98 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 98–106, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Integer Linear Programming in the context of machine translation first appeared in the work of Germann et al. (2004), who addressed the translation problem (often called decoding) in terms of a travelings-salesman like formulation. Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. The main difference to our work is that they allow only consecutive words in the phrases. In their formulation, allowing arbitrary phrases would require an exponential number of variables. In contrast, our approach handles the classical single word based model where any kind of “phrases” in the source sentence are</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2004</marker>
<rawString>U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada. 2004. Fast decoding and optimal decoding for machine translation. Artificial Intelligence, 154(1–2), April.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>R Zens</author>
<author>A Constantin</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>C Dyer</author>
</authors>
<marker>Hoang, Birch, Callison-Burch, Zens, Constantin, Federico, Bertoldi, Dyer, </marker>
<rawString>H. Hoang, A. Birch, C. Callison-Burch, R. Zens, A. Constantin, M. Federico, N. Bertoldi, C. Dyer,</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>O Bojar</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<marker>Cowan, Shen, Moran, Bojar, 2007</marker>
<rawString>B. Cowan, W. Shen, C. Moran, and O. Bojar. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 177– 180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>115--124</pages>
<location>Washington, D.C.,</location>
<contexts>
<context position="1564" citStr="Koehn, 2004" startWordPosition="247" endWordPosition="248"> it can be customized to the specific problem discussed in this paper. In fact, a large number of alignments can be excluded from the start without losing global optimality. 1 Introduction Brown et al. (1993) proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5. Their models were single word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g. (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007), and hence allow more general alignments. Yet, single word based models (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996) are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems (DeNero et al., 2006). Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be compu</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Conference of the Association for Machine Translation in the Americas (AMTA), pages 115– 124, Washington, D.C., October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Julien</author>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Jordan</author>
</authors>
<title>Word alignment via quadratic assignment.</title>
<date>2006</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<location>New York, New York,</location>
<contexts>
<context position="4488" citStr="Lacoste-Julien et al. (2006)" startWordPosition="722" endWordPosition="725"> decoding) in terms of a travelings-salesman like formulation. Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. The main difference to our work is that they allow only consecutive words in the phrases. In their formulation, allowing arbitrary phrases would require an exponential number of variables. In contrast, our approach handles the classical single word based model where any kind of “phrases” in the source sentence are aligned to one-word phrases in the target sentence. Lacoste-Julien et al. (2006) propose an integer linear program for a symmetrized word-level alignment model. Their approach also allows to take the alignments of neighboring words into account. In contrast to our work, they only have a very crude fertility model and they are considering a substantially different model. It should be noted, however, that a subclass of their problems can be solved in polynomial time - the problem is closely related to bipartite graph matching. Less general approaches based on matching have been proposed in (Matusov et al., 2004) and (Taskar et al., 2005). Recently Bodrumlu et al. (2009) pro</context>
</contexts>
<marker>Lacoste-Julien, Taskar, Klein, Jordan, 2006</marker>
<rawString>S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan. 2006. Word alignment via quadratic assignment. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, New York, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="1551" citStr="Marcu and Wong, 2002" startWordPosition="243" endWordPosition="246">hod, but also show how it can be customized to the specific problem discussed in this paper. In fact, a large number of alignments can be excluded from the start without losing global optimality. 1 Introduction Brown et al. (1993) proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5. Their models were single word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g. (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007), and hence allow more general alignments. Yet, single word based models (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996) are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems (DeNero et al., 2006). Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and W. Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, Pennsylvania, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Symmetric word alignments for statistical machine translation.</title>
<date>2004</date>
<booktitle>In International Conference on Computational Linguistics (COLING),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5025" citStr="Matusov et al., 2004" startWordPosition="811" endWordPosition="814">are aligned to one-word phrases in the target sentence. Lacoste-Julien et al. (2006) propose an integer linear program for a symmetrized word-level alignment model. Their approach also allows to take the alignments of neighboring words into account. In contrast to our work, they only have a very crude fertility model and they are considering a substantially different model. It should be noted, however, that a subclass of their problems can be solved in polynomial time - the problem is closely related to bipartite graph matching. Less general approaches based on matching have been proposed in (Matusov et al., 2004) and (Taskar et al., 2005). Recently Bodrumlu et al. (2009) proposed a very innovative cost function for jointly optimizing dictionary entries and alignments, which they minimize using integer linear programming. They also include a mechanism to derive N-best lists. However, they mention rather long computation times for rather small corpora. It is not clear if the large Hansards tasks could be addressed by their method. An overview of integer linear programming approaches for natural language processing can be found on http://ilpnlp.wikidot.com/. To facilitate further research in this area, t</context>
</contexts>
<marker>Matusov, Zens, Ney, 2004</marker>
<rawString>E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word alignments for statistical machine translation. In International Conference on Computational Linguistics (COLING), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="1529" citStr="Och and Ney, 2000" startWordPosition="239" endWordPosition="242"> branch-and-cut method, but also show how it can be customized to the specific problem discussed in this paper. In fact, a large number of alignments can be excluded from the start without losing global optimality. 1 Introduction Brown et al. (1993) proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5. Their models were single word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g. (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007), and hence allow more general alignments. Yet, single word based models (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996) are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems (DeNero et al., 2006). Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 440–447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2422" citStr="Och and Ney, 2003" startWordPosition="391" endWordPosition="394">om the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems (DeNero et al., 2006). Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be computed by dynamic programming or even simpler techniques. On the other hand there are fertility based models, including IBM 3-5 and Model 6. These models have been shown to be of higher practical relevance than the members of the first class (Och and Ney, 2003) since they usually produce better alignments. At the same time, computing Viterbi alignments for these methods has been shown to be NP-hard (Udupa and Maji, 2006), and computing marginals is no easier. The standard way to handle these models – as implemented in GIZA++ (Al-Onaizan et al., 1999; Och and Ney, 2003) – is to use a hillclimbing algorithm. Recently Udupa and Maji (2005) proposed an interesting approximation based on solving sequences of exponentially large subproblems by means of dynamic programming and also addressed the decoding problem. In both cases there is no way to tell how f</context>
<context position="7831" citStr="Och and Ney, 2003" startWordPosition="1295" endWordPosition="1298">e target sentence. The alignment of a source sentence is then a vector aJ1 , and the probability can now be written as X p(fJ 1 |eI 1) = ai We will focus on training the IBM-3 model which is based on the concept of fertilities: given an alignment aJ1, the fertility Φi(aJ1) = Pj:aj=i 1 of target word i expresses the number of source words aligned to it. Omitting the dependence on aJ1 (and defining p(j|0) = 1), the probability is expressed as h i Φi! p(Φi|ei) Y· h i j p(fj|eaj) · p(j|aj) . (1) For the probability p(Φ0|J) of the fertility of the empty word, we use the modification introduced in (Och and Ney, 2003), see there for details. In summary, the model comprises a single word based p(fJ1 , aJ1 |eI1) . 1 p(fJ1 , aJ1 |eI1) = p(Φ0|J) · YI i=1 99 translation model, an inverted zero-order alignment model and a fertility model. We now discuss how to find the optimal alignment for given probabilities, i.e. to solve the problem arg max p(fJ1 ,aJ1 |eI�) (2) al for each bilingual sentence pair in the training set. This is a desirable step in the approximate EM-algorithm that is commonly used to train the model. 3 Finding IBM-3 Viterbi Alignments via Integer Linear Programming Instead of solving (2) direct</context>
<context position="16599" citStr="Och and Ney, 2003" startWordPosition="2900" endWordPosition="2903">rge dataset we are currently only considering sentence pairs where both sentences have at most 75 words. Longer sentences are usually not useful to derive model parameters. The other two datasets are released by the European Corpus Initiative3. We choose the Union Bank of Switzerland (UBS) corpus for English and German and the Avalanche Bulletins, originally released by SFISAR, for French and German. For the latter task we have annotated alignments for 150 of the training sentences, where one annotator specified sure and possible alignments. For details, also on the alignment error rate, see (Och and Ney, 2003). All corpora have been preprocessed with language-specific rules; their statistics are given in Table 1. We have integrated our method into the standard toolkit GIZA++4 and are using the training scheme 15H53545 for all tasks. While we focus on the IBM-3 stage, we also discuss the quality of the resulting IBM-4 parameters and alignments. Experiments were run on a 2.4 GHz Core 2 Duo with 4 GB memory. For most sentence pairs, the memory consumption of our method is only marginally more than in standard GIZA++ (600 MB). In the first iteration on the large Hansards task, however, there are a few </context>
<context position="22529" citStr="Och and Ney, 2003" startWordPosition="3893" endWordPosition="3896">alignment error rates of both schemes are comparable. Indeed, a manual evaluation of the alignments showed that most of the changes affect words like articles or prepositions that are generally hard to translate. In many cases neither the heuristic nor the Viterbi alignment could be considered correct. An interesting case where the proposed scheme produced the better alignment is shown in Figure 1. In summary, our results give a thorough justification for the commonly used heuristics. A test with the original non-deficient empty word model of the IBM-3 furthermore confirmed the impression of (Och and Ney, 2003) that overly many words are aligned to the empty word: the tendency is even stronger in the Viterbi alignments. 6.3 Optimizing Running Time The possibilities to influence the run-times of the branch-and-cut framework are vast: there are nuUnion Bank (UBS) E —* G Final IBM-3 Final IBM-4 Standard train. 49.21 35.73 Proposed train. 49.00 35.76 Union Bank (UBS) G —* E Final IBM-3 Final IBM-4 Standard train. 62.38 47.39 Proposed train. 62.08 47.43 Avalanche F —* G Final IBM-3 Final IBM-4 Standard train. 35.44 21.99 Proposed train. 35.23 22.04 Avalanche G —* F Final IBM-3 Final IBM-4 Standard train.</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schrijver</author>
</authors>
<date>1986</date>
<booktitle>Theory of Linear and Integer Programming. Wiley-Interscience Series in Discrete Mathematics and Optimization.</booktitle>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="3199" citStr="Schrijver, 1986" startWordPosition="526" endWordPosition="527">6), and computing marginals is no easier. The standard way to handle these models – as implemented in GIZA++ (Al-Onaizan et al., 1999; Och and Ney, 2003) – is to use a hillclimbing algorithm. Recently Udupa and Maji (2005) proposed an interesting approximation based on solving sequences of exponentially large subproblems by means of dynamic programming and also addressed the decoding problem. In both cases there is no way to tell how far away the result is from the Viterbi alignment. In this paper we solve the problem of finding IBM-3 Viterbi alignments by means of Integer Linear Programming (Schrijver, 1986). While there is no polynomial run-time guarantee, in practice the applied branch-and-cut framework is fast enough to find optimal solutions even for the large Canadian Hansards task (restricted to sentences with at most 75 words), with a training time of 6 hours on a 2.4 GHz Core 2 Duo (single threaded). 98 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 98–106, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Integer Linear Programming in the context of machine translation first appeared in the work of Germann et al</context>
<context position="11226" citStr="Schrijver, 1986" startWordPosition="1951" endWordPosition="1952">lving the arising linear programming relaxation via the dual simplex method. While in practice this can be done in a matter of milli-seconds even for sentences with I, J &gt; 50, the result is frequently a fractional solution. Here the alignment variables are usually integral but the fertility variables are not. In case the LP-relaxation does not produce an integer solution, the found solution is used as the initialization of a branch-and-cut framework. Here one first tries to strengthen the LP-relaxation by deriving additional inequalities that must be valid for all integral solutions see e.g. (Schrijver, 1986; Wolter, 2006) and www.coin-or.org. These inequalities are commonly called cuts. Then one applies a branch-and-bound scheme on the integer variables. In each step of this scheme, additional inequalities are derived. The process is further sped-up by introducing a heuristic to derive an 100 upper bound on the cost function. Such bounds are generally given by feasible integral solutions. We use our own heuristic as a plug-in to the solver. It generates solutions by thresholding the alignment variables (winner-take-all) and deriving the induced fertility variables. An initial upper bound is furt</context>
</contexts>
<marker>Schrijver, 1986</marker>
<rawString>A. Schrijver. 1986. Theory of Linear and Integer Programming. Wiley-Interscience Series in Discrete Mathematics and Optimization. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>D Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="5051" citStr="Taskar et al., 2005" startWordPosition="816" endWordPosition="819">ases in the target sentence. Lacoste-Julien et al. (2006) propose an integer linear program for a symmetrized word-level alignment model. Their approach also allows to take the alignments of neighboring words into account. In contrast to our work, they only have a very crude fertility model and they are considering a substantially different model. It should be noted, however, that a subclass of their problems can be solved in polynomial time - the problem is closely related to bipartite graph matching. Less general approaches based on matching have been proposed in (Matusov et al., 2004) and (Taskar et al., 2005). Recently Bodrumlu et al. (2009) proposed a very innovative cost function for jointly optimizing dictionary entries and alignments, which they minimize using integer linear programming. They also include a mechanism to derive N-best lists. However, they mention rather long computation times for rather small corpora. It is not clear if the large Hansards tasks could be addressed by their method. An overview of integer linear programming approaches for natural language processing can be found on http://ilpnlp.wikidot.com/. To facilitate further research in this area, the source code will be mad</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discriminative matching approach to word alignment. In Conference on Empirical Methods in Natural Language Processing (EMNLP), Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>H K Maji</author>
</authors>
<title>Theory of alignment generators and applications to statistical machine translation.</title>
<date>2005</date>
<booktitle>In The International Joint Conferences on Artificial Intelligence,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="2805" citStr="Udupa and Maji (2005)" startWordPosition="457" endWordPosition="460">amming or even simpler techniques. On the other hand there are fertility based models, including IBM 3-5 and Model 6. These models have been shown to be of higher practical relevance than the members of the first class (Och and Ney, 2003) since they usually produce better alignments. At the same time, computing Viterbi alignments for these methods has been shown to be NP-hard (Udupa and Maji, 2006), and computing marginals is no easier. The standard way to handle these models – as implemented in GIZA++ (Al-Onaizan et al., 1999; Och and Ney, 2003) – is to use a hillclimbing algorithm. Recently Udupa and Maji (2005) proposed an interesting approximation based on solving sequences of exponentially large subproblems by means of dynamic programming and also addressed the decoding problem. In both cases there is no way to tell how far away the result is from the Viterbi alignment. In this paper we solve the problem of finding IBM-3 Viterbi alignments by means of Integer Linear Programming (Schrijver, 1986). While there is no polynomial run-time guarantee, in practice the applied branch-and-cut framework is fast enough to find optimal solutions even for the large Canadian Hansards task (restricted to sentence</context>
</contexts>
<marker>Udupa, Maji, 2005</marker>
<rawString>R. Udupa and H.K. Maji. 2005. Theory of alignment generators and applications to statistical machine translation. In The International Joint Conferences on Artificial Intelligence, Edinburgh, Scotland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>H K Maji</author>
</authors>
<title>Computational complexity of statistical machine translation.</title>
<date>2006</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="2585" citStr="Udupa and Maji, 2006" startWordPosition="417" endWordPosition="420">. Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be computed by dynamic programming or even simpler techniques. On the other hand there are fertility based models, including IBM 3-5 and Model 6. These models have been shown to be of higher practical relevance than the members of the first class (Och and Ney, 2003) since they usually produce better alignments. At the same time, computing Viterbi alignments for these methods has been shown to be NP-hard (Udupa and Maji, 2006), and computing marginals is no easier. The standard way to handle these models – as implemented in GIZA++ (Al-Onaizan et al., 1999; Och and Ney, 2003) – is to use a hillclimbing algorithm. Recently Udupa and Maji (2005) proposed an interesting approximation based on solving sequences of exponentially large subproblems by means of dynamic programming and also addressed the decoding problem. In both cases there is no way to tell how far away the result is from the Viterbi alignment. In this paper we solve the problem of finding IBM-3 Viterbi alignments by means of Integer Linear Programming (Sc</context>
</contexts>
<marker>Udupa, Maji, 2006</marker>
<rawString>R. Udupa and H.K. Maji. 2006. Computational complexity of statistical machine translation. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics (COLING),</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="1732" citStr="Vogel et al., 1996" startWordPosition="275" endWordPosition="278">l optimality. 1 Introduction Brown et al. (1993) proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5. Their models were single word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g. (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007), and hence allow more general alignments. Yet, single word based models (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996) are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems (DeNero et al., 2006). Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be computed by dynamic programming or even simpler techniques. On the other hand there are fertility based models, including IBM 3-5 and Model 6. These models have been shown t</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word alignment in statistical translation. In International Conference on Computational Linguistics (COLING), pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wolter</author>
</authors>
<title>Implementation of Cutting Plane Separators for Mixed Integer Programs. Master’s thesis, Technische Universit¨at</title>
<date>2006</date>
<location>Berlin, Germany.</location>
<contexts>
<context position="11241" citStr="Wolter, 2006" startWordPosition="1953" endWordPosition="1954"> linear programming relaxation via the dual simplex method. While in practice this can be done in a matter of milli-seconds even for sentences with I, J &gt; 50, the result is frequently a fractional solution. Here the alignment variables are usually integral but the fertility variables are not. In case the LP-relaxation does not produce an integer solution, the found solution is used as the initialization of a branch-and-cut framework. Here one first tries to strengthen the LP-relaxation by deriving additional inequalities that must be valid for all integral solutions see e.g. (Schrijver, 1986; Wolter, 2006) and www.coin-or.org. These inequalities are commonly called cuts. Then one applies a branch-and-bound scheme on the integer variables. In each step of this scheme, additional inequalities are derived. The process is further sped-up by introducing a heuristic to derive an 100 upper bound on the cost function. Such bounds are generally given by feasible integral solutions. We use our own heuristic as a plug-in to the solver. It generates solutions by thresholding the alignment variables (winner-take-all) and deriving the induced fertility variables. An initial upper bound is furthermore given b</context>
</contexts>
<marker>Wolter, 2006</marker>
<rawString>K. Wolter. 2006. Implementation of Cutting Plane Separators for Mixed Integer Programs. Master’s thesis, Technische Universit¨at Berlin, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>