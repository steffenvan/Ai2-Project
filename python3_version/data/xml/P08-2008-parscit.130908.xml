<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000373">
<title confidence="0.980682">
Novel Semantic Features for Verb Sense Disambiguation
</title>
<author confidence="0.763804">
Dmitriy Dligach
</author>
<affiliation confidence="0.283302333333333">
The Center for Computational
Language and Education
Research
</affiliation>
<address confidence="0.411383333333333">
1777 Exposition Drive
Boulder, Colorado 80301
Dmitriy.Dligach
</address>
<email confidence="0.965487">
@colorado.edu
</email>
<author confidence="0.984177">
Martha Palmer
</author>
<affiliation confidence="0.852722333333333">
Department of Linguistics
University of Colorado
at Boulder
</affiliation>
<address confidence="0.748523666666667">
295 UCB
Boulder, Colorado 80309
Martha.Palmer
</address>
<email confidence="0.965315">
@colorado.edu
</email>
<sectionHeader confidence="0.994918" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99975975">
We propose a novel method for extracting
semantic information about a verb&apos;s arguments
and apply it to Verb Sense Disambiguation
(VSD). We contrast this method with two
popular approaches to retrieving this informa-
tion and show that it improves the perform-
ance of our VSD system and outperforms the
other two approaches
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989816">
The task of Verb Sense Disambiguation (VSD)
consists in automatically assigning a sense to a
verb (target verb) given its context. In a supervised
setting, a VSD system is usually trained on a set of
pre-labeled examples; the goal of this system is to
tag unseen examples with a sense from some sense
inventory.
An automatic VSD system usually has at its
disposal a diverse set of features among which the
semantic features play an important role: verb
sense distinctions often depend on the distinctions
in the semantics of the target verb&apos;s arguments
(Hanks, 1996). Therefore, some method of captur-
ing the semantic knowledge about the verb&apos;s argu-
ments is crucial to the success of a VSD system.
The approaches to obtaining this kind of
knowledge can be based on extracting it from elec-
tronic dictionaries such as WordNet (Fellbaum,
1998), using Named Entity (NE) tags, or a combi-
nation of both (Chen, 2005). In this paper, we pro-
pose a novel method for obtaining semantic
knowledge about words and show how it can be
applied to VSD. We contrast this method with the
other two approaches and compare their perform-
ances in a series of experiments.
</bodyText>
<sectionHeader confidence="0.920554" genericHeader="introduction">
2 Lexical and Syntactic Features
</sectionHeader>
<bodyText confidence="0.999944789473684">
We view VSD as a supervised learning problem,
solving which requires three groups of features:
lexical, syntactic, and semantic. Lexical features
include all open class words; we extract them from
the target sentence and the two surrounding sen-
tences. We also use as features two words on the
right and on the left of the target verb as well as
their POS tags. We extract syntactic features from
constituency parses; they indicate whether the tar-
get verb has a subject/object and what their head
words and POS tags are, whether the target verb is
in a passive or active form, whether the target verb
has a subordinate clause, and whether the target
verb has a PP adjunct. Additionally, we implement
several new syntactic features, which have not
been used in VSD before: the path through the
parse tree from the target verb to the verb&apos;s argu-
ments and the subcategorization frame, as used in
semantic role labeling.
</bodyText>
<sectionHeader confidence="0.987832" genericHeader="method">
3 Semantic Features
</sectionHeader>
<bodyText confidence="0.99653275">
Consider the verb prepare for which our sense in-
ventory defines two senses: (1) to put together,
assemble (e.g. He is going to prepare breakfast for
the whole crowd; I haven&apos;t prepared my lecture
</bodyText>
<page confidence="0.997302">
29
</page>
<reference confidence="0.223424">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 29–32,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999962956521739">
yet); (2) to make ready (e.g. She prepared the chil-
dren for school every morning). Knowing the se-
mantic class of the objects breakfast, lecture and
children is the decisive factor in distinguishing the
two senses and facilitates better generalization
from the training data. One way to obtain this
knowledge is from WordNet (WN) or from the
output of a NE-tagger. However, both approaches
suffer from the same limitation: they collapse mul-
tiple semantic properties of nouns into a finite
number of predefined static classes. E.g., the most
immediate hypernym of breakfast in WN is meal,
while the most immediate hypernym of lecture is
address, which makes these two nouns unrelated.
Yet, breakfast and lecture are both social events
which share some semantic properties: they both
can be attended, hosted, delivered, given, held,
organized etc. To discover these class-like descrip-
tions of nouns, one can observe which verbs take
these nouns as objects. E.g. breakfast can serve as
the object of serve, host, attend, and cook which
are all indicative of breakfast&apos;s semantic proper-
ties.
Given a noun, we can dynamically retrieve
other verbs that take that noun as an object from a
dependency-parsed corpus; we call this kind of
data Dynamic Dependency Neighbors (DDNs)
because it is obtained dynamically and based on
the dependency relations in the neighborhood of
the noun of interest. The top 501 DDNs can be
viewed as a reliable inventory of semantic proper-
ties of the noun. To collect this data, we utilized
two resources: (1) MaltParser (Nivre, 2007) – a
high-efficiency dependency parser; (2) English
Gigaword – a large corpus of 5.7M news articles.
We preprocessed Gigaword with MaltParser, ex-
tracted all pairs of nouns and verbs that were
parsed as participants of the object-verb relation,
and counted the frequency of occurrence of all the
unique pairs. Finally, we indexed the resulting re-
cords of the form &lt;frequency, verb, object&gt; using
the Lucene2 indexing engine.
As an example, consider four nouns: dinner,
breakfast, lecture, child. When used as the objects
of prepare, the first three of them correspond to the
instances of the sense 1 of prepare; the fourth one
</bodyText>
<footnote confidence="0.9740815">
1 In future, we will try to optimize this parameter
2 Available at http://lucene.apache.org/
</footnote>
<bodyText confidence="0.999233066666667">
corresponds to an instance of the sense 2. With the
help of our index, we can retrieve their DDNs.
There is a considerable overlap among the DDNs
of the first three nouns and a much smaller overlap
between child and the first three nouns. E.g., din-
ner and breakfast have 34 DDNs in common,
while dinner and child only share 14.
Once we have set up the framework for the ex-
traction of DDNs, the algorithm for applying them
to VSD is straightforward: (1) find the noun object
of the ambiguous verb (2) extract the DDNs for
that noun (3) sort the DDNs by frequency and keep
the top 50 (4) include these DDNs in the feature
vector so that each of the extracted verbs becomes
a separate feature.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="method">
4 Relevant Work
</sectionHeader>
<bodyText confidence="0.972267363636364">
At the core of our work lies the notion of distribu-
tional similarity (Harris, 1968), which states that
similar words occur in similar contexts. In various
sources, the notion of context ranges from bag-of-
words-like approaches to more structured ones in
which syntax plays a role. Schutze (1998) used
bag-of-words contexts for sense discrimination.
Hindle (1990) grouped nouns into thesaurus-like
lists based on the similarity of their syntactic con-
texts. Our approach is similar with the difference
that we do not group noun arguments into finite
categories, but instead leave the category bounda-
ries blurry and allow overlaps.
The DDNs are essentially a form of world
knowledge which we extract automatically and
apply to VSD. Other researches attacked the prob-
lem of unsupervised extraction of world knowl-
edge: Schubert (2003) reports a method for
extracting general facts about the world from tree-
banked Brown corpus. Lin and Pantel in (2001)
describe their DIRT system for extraction of para-
phrase-like inference rules.
</bodyText>
<sectionHeader confidence="0.998756" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9961802">
We selected a subset of the verbs annotated in the
OntoNotes project (Chen, 2007) that had at least
50 instances. The resulting data set consisted of
46,577 instances of 217 verbs. The predominant
sense baseline for this data is 68%. We used
</bodyText>
<page confidence="0.991522">
30
</page>
<bodyText confidence="0.9920304">
libsvm3 for classification. We computed the accu-
racy and error rate using 5-fold cross-validation.
knowledge of the DDNs provides richer semantic
knowledge than just the knowledge of the object&apos;s
head word.
</bodyText>
<subsectionHeader confidence="0.991148">
5.1 Experiments with a limited set of features
</subsectionHeader>
<bodyText confidence="0.999830296296296">
The main objective of this experiment was to iso-
late the effect of the novel semantic features we
proposed in this paper, i.e. the DDN features. To-
ward that goal, we stripped our system of all the
features but the most essential ones to investigate
whether the DDN features would have a clearly
positive or negative impact on the system perform-
ance. Lexical features are the most essential to our
system: a model that includes only the lexical fea-
tures achieves an accuracy of 80.22, while the ac-
curacy of our full-blown VSD system is 82.88%4.
Since the DDN features have no effect when the
object is not present, we identified 18,930 in-
stances where the target verb had an object (about
41% of all instances) and used only them in the
experiment.
We built three models that included (1) the
lexical features only (2) the lexical and the DDN
features (3) the lexical and the object features. The
object features consist of the head word of the NP
object and the head word&apos;s POS tag. The object is
included since extracting the DDN features re-
quires knowledge of the object; therefore the per-
formance of a model that only includes lexical
features cannot be considered a fair baseline for
studying the effect of the DDN features. Results
are in Table 4.
</bodyText>
<table confidence="0.9992884">
Features Included in Accuracy, % Error Rate, %
Model
Lexical 78.95 21.05
Lexical + Object 79.34 20.66
Lexical + DDN 82.40 17.60
</table>
<tableCaption confidence="0.999956">
Table 4. Experiments with object instances
</tableCaption>
<bodyText confidence="0.999965714285714">
As we see, the model that includes the DDN
features performs more than 3 percentage points
better than the model that only includes the object
features (approximately 15% reduction in error
rate). Also, based on the comparison of the per-
formance of the &amp;quot;lexical features only&amp;quot; and the
&amp;quot;lexical + DDN&amp;quot; models, we can claim that the
</bodyText>
<footnote confidence="0.989388333333333">
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
4 Given this high baseline, we include error rate when report-
ing the results of the experiments as it is more informative
</footnote>
<subsectionHeader confidence="0.639959">
5.2 Integrating the DDN features into a full-
fledged VSD system
</subsectionHeader>
<bodyText confidence="0.999279625">
The objective of this experiment was to investigate
whether the DDN features improve the perform-
ance of a full-fledged VSD system. We built two
models which consisted of (1) the entire set of fea-
tures (2) all the features of the first model exclud-
ing the DDN features. The entire data set (46K
instances) participated in the experiment. Results
are in Table 5.
</bodyText>
<table confidence="0.99921225">
Features Included in Accuracy, % Error Rate, %
Model
All Features – DDN 82.38 17.62
All Features 82.88 17.12
</table>
<tableCaption confidence="0.999831">
Table 5. Performance of the full-fledged VSD system
</tableCaption>
<bodyText confidence="0.9995865">
The DDN features improved performance by
0.5% (3% drop in error rate). The difference be-
tween the accuracies is statistically significant
(p=0.05).
</bodyText>
<subsectionHeader confidence="0.9389">
5.3 Relative Contribution of Various Seman-
tic Fe atures
</subsectionHeader>
<bodyText confidence="0.999466444444445">
The goal of this experiment was to study the rela-
tive contribution of various semantic features to
the performance of our VSD system. We built five
models each of which, in addition to the lexical
and syntactic features, included only certain
type(s) of semantic feature: (1) WN (2) NE (3)
WN and NE (4) DDN (5) no semantic features
(baseline). All 46K instances participated in the
experiment. The results are shown in Table 6.
</bodyText>
<table confidence="0.999218714285714">
Features Included in Model Accuracy, Error Rate,
% %
Lexical + Syntactic 81.82 18.18
Lexical + Syntactic + WN 82.34 17.60
Lexical + Syntactic + NE 82.01 17.99
Lexical + Syntactic + WN + NE 82.38 17.62
Lexical + Syntactic + DDN 82.97 17.03
</table>
<tableCaption confidence="0.983171">
Table 6. Relative Contribution of Semantic Features
</tableCaption>
<bodyText confidence="0.9968455">
The DDN features outperform the other two
types of semantic features used separately and in
conjunction. The difference in performance is sta-
tistically significant (p=0.05).
</bodyText>
<page confidence="0.999874">
31
</page>
<sectionHeader confidence="0.984706" genericHeader="method">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999981086956522">
As we saw, the novel semantic features we pro-
posed are beneficial to the task of VSD: they re-
sulted in a decrease in error rate from 3% to 15%,
depending on the particular experiment. We also
discovered that the DDN features contributed twice
as much as the other two types of semantic features
combined: adding the WN and NE features to the
baseline resulted in about a 3% decrease in error
rate, while adding the DDN features caused a more
than 6% drop.
Our results suggest that DDNs duplicate the ef-
fect of WN and NE: our system achieved the same
performance when all three types of semantic fea-
tures were used and when we discarded WN and
NE features and kept only the DDNs. This finding
is important because such resources as WN and
NE-taggers are domain and language specific
while the DDNs have the advantage of being ob-
tainable from a large collection of texts in the do-
main or language of interest. Thus, the DDNs can
become a crucial part of building a robust VSD
system for a resource-poor domain or language,
given a high-accuracy parser.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999465272727273">
In this paper we only experimented with verbs&apos;
objects, however the concept of DDNs can be eas-
ily extended to other arguments of the target verb.
Also, we only utilized the object-verb relation in
the dependency parses, but the range of potentially
useful relations does not have to be limited only to
it. Finally, we used as features the 50 most fre-
quent verbs that took the noun argument as an ob-
ject. However, the raw frequency is certainly not
the only way to rank the verbs; we plan on explor-
ing other metrics such as Mutual Information.
</bodyText>
<sectionHeader confidence="0.994951" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999274307692308">
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team. Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not
necessarily reflect the views of the National Sci-
ence Foundation. We also thank our colleagues
Rodney Nielsen and Philipp Wetzler for parsing
English Gigaword with MaltParser.
</bodyText>
<sectionHeader confidence="0.995927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999890864864865">
Jinying Chen, Dmitriy Dligach and Martha Palmer.
2007. Towards Large-scale High-Performance Eng-
lish Verb Sense Disambiguation by Using Linguisti-
cally Motivated Features. In International
Conference on Semantic Computing. Issue , 17-19.
Jinying Chen and Martha Palmer. 2005. Towards Ro-
bust High Performance Word Sense Disambiguation
of English Verbs Using Rich Linguistic Features. In
Proceedings of the 2nd International Joint Confer-
ence on Natural Language Processing, Korea.
Christiane Fellbaum. 1998. WordNet - an Electronic
Lexical Database. The MIT Press, Cambridge, Mas-
sachusetts, London, UK.
Patrick Hanks, 1996. Contextual Dependencies and
Lexical Sets. In The Int. Journal of Corpus Linguis-
tics, 1:1
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. New York. Wiley.
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In Proceedings of the 28th
Annual Meeting of Association for Computational
Linguistics. Pages 268-275
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proceedings of
ACM Conference on Knowledge Discovery and Data
Mining. pp. 323-328. San Francisco, CA.
Joakim Nivre, Johan Hall, Jens Nilsson, et. al. Malt-
Parser: A language-independent system for data-
driven dependency parsing. 2007. In Natural Lan-
guage Engineering, 13(2), 95-135.
Lenhart Schubert and Matthew Tong, Extracting and
evaluating general world knowledge from the Brown
corpus. 2003. In Proc. of the HLT/NAACL Workshop
on Text Meaning, May 31, Edmonton, Alberta, Can-
ada.
Hinrich Schutze. 1998. Automatic Word Sense Dis-
crimination. In Computational Linguistics, 24(1):97-
</reference>
<page confidence="0.75122">
123
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.076349">
<title confidence="0.999644">Novel Semantic Features for Verb Sense Disambiguation</title>
<author confidence="0.452009">Dmitriy Dligach</author>
<affiliation confidence="0.580354">The Center for Computational Language and Education Research</affiliation>
<address confidence="0.983265">1777 Exposition Drive Boulder, Colorado 80301</address>
<email confidence="0.6811725">Dmitriy.Dligach@colorado.edu</email>
<author confidence="0.999926">Martha Palmer</author>
<affiliation confidence="0.960915333333333">Department of Linguistics University of Colorado at Boulder</affiliation>
<address confidence="0.997949">295 UCB Boulder, Colorado 80309</address>
<email confidence="0.8205115">Martha.Palmer@colorado.edu</email>
<abstract confidence="0.981896666666667">We propose a novel method for extracting semantic information about a verb&apos;s arguments and apply it to Verb Sense Disambiguation (VSD). We contrast this method with two popular approaches to retrieving this information and show that it improves the performance of our VSD system and outperforms the other two approaches</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>29--32</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 29–32,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Towards Large-scale High-Performance English Verb Sense Disambiguation by Using Linguistically Motivated Features.</title>
<date>2007</date>
<booktitle>In International Conference on Semantic Computing. Issue ,</booktitle>
<pages>17--19</pages>
<marker>Chen, Dligach, Palmer, 2007</marker>
<rawString>Jinying Chen, Dmitriy Dligach and Martha Palmer. 2007. Towards Large-scale High-Performance English Verb Sense Disambiguation by Using Linguistically Motivated Features. In International Conference on Semantic Computing. Issue , 17-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Martha Palmer</author>
</authors>
<title>Towards Robust High Performance Word Sense Disambiguation of English Verbs Using Rich Linguistic Features.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing,</booktitle>
<marker>Chen, Palmer, 2005</marker>
<rawString>Jinying Chen and Martha Palmer. 2005. Towards Robust High Performance Word Sense Disambiguation of English Verbs Using Rich Linguistic Features. In Proceedings of the 2nd International Joint Conference on Natural Language Processing, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet - an Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts, London, UK.</location>
<contexts>
<context position="1524" citStr="Fellbaum, 1998" startWordPosition="236" endWordPosition="237">e goal of this system is to tag unseen examples with a sense from some sense inventory. An automatic VSD system usually has at its disposal a diverse set of features among which the semantic features play an important role: verb sense distinctions often depend on the distinctions in the semantics of the target verb&apos;s arguments (Hanks, 1996). Therefore, some method of capturing the semantic knowledge about the verb&apos;s arguments is crucial to the success of a VSD system. The approaches to obtaining this kind of knowledge can be based on extracting it from electronic dictionaries such as WordNet (Fellbaum, 1998), using Named Entity (NE) tags, or a combination of both (Chen, 2005). In this paper, we propose a novel method for obtaining semantic knowledge about words and show how it can be applied to VSD. We contrast this method with the other two approaches and compare their performances in a series of experiments. 2 Lexical and Syntactic Features We view VSD as a supervised learning problem, solving which requires three groups of features: lexical, syntactic, and semantic. Lexical features include all open class words; we extract them from the target sentence and the two surrounding sentences. We als</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet - an Electronic Lexical Database. The MIT Press, Cambridge, Massachusetts, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>Contextual Dependencies and Lexical Sets. In The Int.</title>
<date>1996</date>
<journal>Journal of Corpus Linguistics,</journal>
<volume>1</volume>
<contexts>
<context position="1251" citStr="Hanks, 1996" startWordPosition="190" endWordPosition="191">forms the other two approaches 1 Introduction The task of Verb Sense Disambiguation (VSD) consists in automatically assigning a sense to a verb (target verb) given its context. In a supervised setting, a VSD system is usually trained on a set of pre-labeled examples; the goal of this system is to tag unseen examples with a sense from some sense inventory. An automatic VSD system usually has at its disposal a diverse set of features among which the semantic features play an important role: verb sense distinctions often depend on the distinctions in the semantics of the target verb&apos;s arguments (Hanks, 1996). Therefore, some method of capturing the semantic knowledge about the verb&apos;s arguments is crucial to the success of a VSD system. The approaches to obtaining this kind of knowledge can be based on extracting it from electronic dictionaries such as WordNet (Fellbaum, 1998), using Named Entity (NE) tags, or a combination of both (Chen, 2005). In this paper, we propose a novel method for obtaining semantic knowledge about words and show how it can be applied to VSD. We contrast this method with the other two approaches and compare their performances in a series of experiments. 2 Lexical and Synt</context>
</contexts>
<marker>Hanks, 1996</marker>
<rawString>Patrick Hanks, 1996. Contextual Dependencies and Lexical Sets. In The Int. Journal of Corpus Linguistics, 1:1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley.</publisher>
<location>New York.</location>
<marker>Harris, 1968</marker>
<rawString>Zelig S. Harris. 1968. Mathematical Structures of Language. New York. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun Classification from Predicate-Argument Structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of Association for Computational Linguistics.</booktitle>
<pages>268--275</pages>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun Classification from Predicate-Argument Structures. In Proceedings of the 28th Annual Meeting of Association for Computational Linguistics. Pages 268-275</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM Conference on Knowledge Discovery and Data Mining.</booktitle>
<pages>323--328</pages>
<location>San Francisco, CA.</location>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery of Inference Rules from Text. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining. pp. 323-328. San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>MaltParser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>In Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>95--135</pages>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, et. al. MaltParser: A language-independent system for datadriven dependency parsing. 2007. In Natural Language Engineering, 13(2), 95-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart Schubert</author>
<author>Matthew Tong</author>
</authors>
<title>Extracting and evaluating general world knowledge from the Brown corpus.</title>
<date>2003</date>
<booktitle>In Proc. of the HLT/NAACL Workshop on Text Meaning, May 31,</booktitle>
<location>Edmonton, Alberta, Canada.</location>
<marker>Schubert, Tong, 2003</marker>
<rawString>Lenhart Schubert and Matthew Tong, Extracting and evaluating general world knowledge from the Brown corpus. 2003. In Proc. of the HLT/NAACL Workshop on Text Meaning, May 31, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>In Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic Word Sense Discrimination. In Computational Linguistics, 24(1):97-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>