<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000072">
<title confidence="0.989457">
Improving Search Results Quality by Customizing Summary Lengths
</title>
<author confidence="0.996588">
Michael Kaisser
</author>
<affiliation confidence="0.998544">
University of Edinburgh
</affiliation>
<address confidence="0.802062">
2 Buccleuch Place
Edinburgh EH8 9LW
</address>
<email confidence="0.998797">
m.kaisser@sms.ed.ac.uk
</email>
<note confidence="0.685459333333333">
Marti A. Hearst
UC Berkeley
102 South Hall
</note>
<address confidence="0.812787">
Berkeley, CA 94705
</address>
<email confidence="0.998155">
hearst@ischool.berkeley.edu
</email>
<author confidence="0.850669">
John B. Lowe
</author>
<affiliation confidence="0.791548">
Powerset, Inc.
</affiliation>
<address confidence="0.9086875">
475 Brannan St.
San Francisco, CA 94107
</address>
<email confidence="0.999589">
johnblowe@gmail.com
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919230769231">
Web search engines today typically show re-
sults as a list of titles and short snippets that
summarize how the retrieved documents are
related to the query. However, recent research
suggests that longer summaries can be prefer-
able for certain types of queries. This pa-
per presents empirical evidence that judges
can predict appropriate search result summary
lengths, and that perceptions of search result
quality can be affected by varying these result
lengths. These findings have important impli-
cations for search results presentation, espe-
cially for natural language queries.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863048780488">
Search results listings on the web have become stan-
dardized as a list of information summarizing the
retrieved documents. This summary information is
often referred to as the document’s surrogate (Mar-
chionini et al., 2008).
In older search systems, such as those used in
news and legal search, the document surrogate typ-
ically consisted of the title and important metadata,
such as date, author, source, and length of the article,
as well as the document’s manually written abstract.
In most cases, the full text content of the document
was not available to the search engine and so no ex-
tracts could be made.
In web search, document surrogates typically
show the web page’s title, a URL, and information
extracted from the full text contents of the docu-
ment. This latter part is referred to by several dif-
ferent names, including summary, abstract, extract,
and snippet. Today it is standard for web search en-
gines to show these summaries as one or two lines
of text, often with ellipses separating sentence frag-
ments. However, there is evidence that the ideal re-
sult length is often longer than the standard snippet
length, and that furthermore, result length depends
on the type of answer being sought.
In this paper, we systematically examine the ques-
tion of search result length preference, comparing
different result lengths for different query types. We
find evidence that desired answer length is sensitive
to query type, and that for some queries longer an-
swers are judged to be of higher quality.
In the following sections we summarize the re-
lated work on result length variation and on query
topic classification. We then describe two studies. In
the first, judges examined queries and made predic-
tions about the expected answer types and the ideal
answer lengths. In the second study, judges rated
answers of different lengths for these queries. The
studies find evidence supporting the idea that differ-
ent query types are best answered with summaries
of different lengths.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.963232">
2.1 Query-biased Summaries
</subsectionHeader>
<bodyText confidence="0.999924142857143">
In the early days of the web, the result summary
consisted of the first few lines of text, due both to
concerns about intellectual property, and because of-
ten that was the only part of the full text that the
search engines retained from their crawls. Eventu-
ally, search engines started showing what are known
variously as query-biased summaries, keyword-in-
</bodyText>
<page confidence="0.970178">
701
</page>
<note confidence="0.696929">
Proceedings ofACL-08: HLT, pages 701–709,
</note>
<page confidence="0.494896">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999823833333333">
context (KWIC) extractions, and user-directed sum-
maries (Tombros and Sanderson, 1998). In these
summaries, sentence fragments, full sentences, or
groups of sentences that contain query terms are ex-
tracted from the full text. Early versions of this idea
were developed in the Snippet Search tool (Peder-
sen et al., 1991) and the Superbook tool’s Table-of-
Contents view (Egan et al., 1989).
A query-biased summary shows sentences that
summarize the ways the query terms are used within
the document. In addition to showing which subsets
of query terms occur in a retrieved document, this
display also exposes the context in which the query
terms appear with respect to one another.
Research suggests that query-biased summaries
are superior to showing the first few sentences from
documents. Tombros &amp; Sanderson (1998), in a
study with 20 participants using TREC ad hoc data,
found higher precision and recall and higher sub-
jective preferences for query-biased summaries over
summaries showing the first few sentences. Simi-
lar results for timing and subjective measurements
were found by White et al. (2003) in a study with
24 participants. White et al. (2003) also describe
experiments with different sentence selection mech-
anisms, including giving more weight to sentences
that contained query words along with text format-
ting.
There are significant design questions surround-
ing how best to formulate and display query-biased
summaries. As with standard document summariza-
tion and extraction, there is an inherent trade-off
between showing long, informative summaries and
minimizing the screen space required by each search
hit. There is also a tension between showing short
snippets that contain all or most of the query terms
and showing coherent stretches of text. If the query
terms do not co-occur near one another, then the ex-
tract has to become very long if full sentences and
all query terms are to be shown. Many web search
engine snippets compromise by showing fragments
instead of sentences.
</bodyText>
<subsectionHeader confidence="0.999155">
2.2 Studies Comparing Results Lengths
</subsectionHeader>
<bodyText confidence="0.984212653061225">
Recently, a few studies have analyzed the results of
varying search summary length.
In the question-answering context (as opposed to
general web search), Lin et al. (2003) conducted
a usability study with 32 computer science students
comparing four types of answer context: exact an-
swer, answer-in-sentence, answer-in-paragraph, and
answer-in-document. To remove effects of incorrect
answers, they used a system that produced only cor-
rect answers, drawn from an online encyclopedia.
Participants viewed answers for 8 question scenar-
ios. Lin et al. (2003) found no significant differ-
ences in task completion times, but they did find dif-
ferences in subjective responses. Most participants
(53%) preferred paragraph-sized chunks, noting that
a sentence wasn’t much more information beyond
the exact answer, and a full document was often-
times too long. That said, 23% preferred full docu-
ments, 20% preferred sentences, and one participant
preferred exact answer, thus suggesting that there is
considerable individual variation.
Paek et al. (2004) experimented with showing dif-
fering amounts of summary information in results
listings, controlling the study design so that only
one result in each list of 10 was relevant. For half
the test questions, the target information was visi-
ble in the original snippet, and for the other half, the
participant needed to use their mouse to view more
information from the relevant search result. They
compared three interface conditions:
(i) a standard search results listing, in which a
mouse click on the title brings up the full text
of the web page,
(ii) “instant” view, for which a mouseclick ex-
panded the document summary to show addi-
tional sentences from the document, and those
sentences contained query terms and the an-
swer to the search task, and
(iii) a “dynamic” view that responded to a mouse
hover, and dynamically expanded the summary
with a few words at a time.
Eleven out of 18 participants preferred instant
view over the other two views, and on average all
participants produced faster and more accurate re-
sults with this view. Seven participants preferred dy-
namic view over the others, but many others found
this view disruptive. The dynamic view suffered
from the problem that, as the text expanded, the
mouse no longer covered the selected results, and
</bodyText>
<page confidence="0.987092">
702
</page>
<bodyText confidence="0.999880433333333">
so an unintended, different search result sometimes
started to expand. Notably, none of the participants
preferred the standard results listing view.
Cutrell &amp; Guan (2007), compared search sum-
maries of varying length: short (1 line of text),
medium (2-3 lines) and long (6-7 lines) using search
engine-produced snippets (it is unclear if the sum-
mary text was contiguous or included ellipses).
They also compared 6 navigational queries (where
the goal is to find a website’s homepage), with 6 in-
formational queries (e.g., “find when the Titanic set
sail for its only voyage and what port it left from,”
“find out how long the Las Vegas monorail is”). In
a study with 22 participants, they found that partic-
ipants were 24 seconds faster on average with the
long view than with the short and medium view. The
also found that participants were 10 seconds slower
on average with the long view for the navigational
tasks. They present eye tracking evidence which
suggests that on the navigational task, the extra text
distracts the eye from the URL. They did not re-
port on subjective responses to the different answer
lengths.
Rose et al. (2007) varied search results summaries
along several dimensions, finding that text choppi-
ness and sentence truncation had negative effects,
and genre cues had positive effects. They did not
find effects for varying summary length, but they
only compared relatively similar summary lengths
(2 vs. 3 vs. 4 lines long).
</bodyText>
<subsectionHeader confidence="0.9984445">
2.3 Categorizing Questions by Expected
Answer Types
</subsectionHeader>
<bodyText confidence="0.999960888888889">
In the field of automated question-answering, much
effort has been expended on automatically deter-
mining the kind of answer that is expected for a
given question. The candidate answer types are
often drawn from the types of questions that have
appeared in the TREC Question Answering track
(Voorhees, 2003). For example, the Webclopedia
project created a taxonomy of 180 types of ques-
tion targets (Hovy et al., 2002), and the FALCON
project (Harabagiu et al., 2003) developed an an-
swer taxonomy with 33 top level categories (such
as PERSON, TIME, REASON, PRODUCT, LOCA-
TION, NUMERICAL VALUE, QUOTATION), and
these were further refined into an unspecified num-
ber of additional categories. Ramakrishnan et al.
(2004) show an automated method for determining
expected answer types using syntactic information
and mapping query terms to WordNet.
</bodyText>
<subsectionHeader confidence="0.999371">
2.4 Categorizing Web Queries
</subsectionHeader>
<bodyText confidence="0.999961023255814">
A different line of research is the query log cate-
gorization problem. In query logs, the queries are
often much more terse and ill-defined than in the
TREC QA track, and, accordingly, the taxonomies
used to classify what is called the query intent have
been much more general.
In an attempt to demonstrate how information
needs for web search differ from the assumptions
of pre-web information retrieval systems, Broder
(2002) created a taxonomy of web search goals, and
then estimated frequency of such goals by a com-
bination of an online survey (3,200 responses, 10%
response rate) and a manual analysis of 1,000 query
from the AltaVista query logs. This taxonomy has
been heavily influential in discussions of query types
on the Web.
Rose &amp; Levinson (2004) followed up on Broder’s
work, again using web query logs, but developing
a taxonomy that differed somewhat from Broder’s.
They manually classified a set of 1,500 AltaVista
search engine log queries. For two sets of 500
queries, the labeler saw just the query and the re-
trieved documents; for the third set the labeler also
saw information about which item(s) the searcher
clicked on. They found that the classifications that
used the extra information about clickthrough did
not change the proportions of assignments to each
category. Because they did not directly compare
judgments with and without click information on
the same queries, this is only weak evidence that
query plus retrieved documents is sufficient to clas-
sify query intent.
Alternatively, queries from web query logs can be
classified according to the topic of the query, inde-
pendent of the type of information need. For ex-
ample, a search involving the topic of weather can
consist of the simple information need of looking
at today’s forecast, or the rich and complex infor-
mation need of studying meteorology. Over many
years, Spink &amp; Jansen et al. (2006; 2007) have man-
ually analyzed samples of query logs to track a num-
ber of different trends. One of the most notable is
the change in topic mix. As an alternative to man-
</bodyText>
<page confidence="0.993888">
703
</page>
<bodyText confidence="0.999876333333333">
ual classification of query topics, Shen et al. (2005)
described an algorithm for automatically classifying
web queries into a set of pre-defined topics. More re-
cently, Broder et al. (2007) presented a highly accu-
rate method (around .7 F-score) for classifying short,
rare queries into a taxonomy of 6,000 categories.
</bodyText>
<sectionHeader confidence="0.983639" genericHeader="method">
3 Study Goals
</sectionHeader>
<bodyText confidence="0.9999294">
Related work suggests that longer results are prefer-
able, but not for all query types. The goal of our
efforts was to determine preferred result length for
search results, depending on type of query. To do
this, we performed two studies:
</bodyText>
<listItem confidence="0.86047475">
1. We asked a set of judges to categorize a large
set of web queries according to their expected
preferred response type and expected preferred
response length.
</listItem>
<bodyText confidence="0.78440275">
2. We then developed high-quality answer pas-
sages of different lengths for a subset of these
queries by selecting appropriate passages from
the online encyclopedia Wikipedia, and asked
judges to rate the quality of these answers.
The results of this study should inform search in-
terface designers about what the best presentation
format is.
</bodyText>
<subsectionHeader confidence="0.999328">
3.1 Using Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.816880157894737">
For these studies, we make use of a web service of-
fered by Amazon.com called Mechanical Turk, in
which participants (called “turkers”) are paid small
sums of money in exchange for work on “Human
Intelligence tasks” (HITs).1 These HITs are gener-
ated from an XML description of the task created
by the investigator (called a “requester”). The par-
ticipants can come from any walk of life, and their
identity is not known to the requesters. We have in
past work found the results produced by these judges
to be of high quality, and have put into place vari-
ous checks to detect fraudulent behavior. Other re-
searchers have investigated the efficacy of language
1Website: http://www.mturk.com. For experiment 1, ap-
proximately 38,000 HITs were completed at a cost of about
$1,500. For experiment 2, approximately 7,300 HITs were
completed for about $170. Turkers were paid between $.01 and
$.05 per HIT depending on task complexity; Amazon imposes
additional charges.
</bodyText>
<listItem confidence="0.8976841">
1. Person(s)
2. Organization(s)
3. Time(s) (date, year, time span etc.)
4. Number or Quantity
5. Geographic Location(s) (e.g., city, lake, address)
6. Place(s) (e.g.,”the White House”, “at a supermar-
ket”)
7. Obtain resource online (e.g., movies, lyrics, books,
magazines, knitting patterns)
8. Website or URL
9. Purchase and product information
10. Gossip and celebrity information
11. Language-related (e.g., translations, definitions,
crossword puzzle answers)
12. General information about a topic
13. Advice
14. Reason or Cause, Explanation
15. Yes/No, with or without explanation or evidence
16. Other
17. Unjudgable
</listItem>
<tableCaption confidence="0.613668333333333">
Table 1: Allowable responses to the question: “What sort
of result or results does the query ask for?” in the first
experiment.
</tableCaption>
<listItem confidence="0.999098571428571">
1. A word or short phrase
2. A sentence
3. One or more paragraphs (i.e. at least several sen-
tences)
4. An article or full document
5. A list
6. Other, or some combination of the above
</listItem>
<tableCaption confidence="0.8126715">
Table 2: Allowable responses to the question: “How long
is the best result for this query?” in the first experiment.
</tableCaption>
<bodyText confidence="0.9474695">
annotation using this service and have found that the
results are of high quality (Su et al., 2007).
</bodyText>
<subsectionHeader confidence="0.999724">
3.2 Estimating Ideal Answer Length and Type
</subsectionHeader>
<bodyText confidence="0.999979833333333">
We developed a set of 12,790 queries, drawn from
Powerset’s in house query database which con-
tains representative subsets of queries from different
search engines’ query logs, as well as hand-edited
query sets used for regression testing. There are a
disproportionally large number of natural language
queries in this set compared with query sets from
typical keyword engines. Such queries are often
complete questions and are sometimes grammatical
fragments (e.g., “date of next US election”) and so
are likely to be amenable to interesting natural lan-
guage processing algorithms, which is an area of in-
</bodyText>
<page confidence="0.998039">
704
</page>
<figureCaption confidence="0.9991455">
Figure 1: Results of the first experiment. The y-axis shows the semantic type of the predicted answer, in the same
order as listed in Table 1; the x-axis shows the preferred length as listed in Table 2. Three bars with length greater
than 1,500 are trimmed to the maximum size to improve readability (GeneralInfo/Paragraphs, GeneralInfo/Article,
and Number/Phrase).
</figureCaption>
<bodyText confidence="0.999960571428571">
terest of our research. The average number of words
per query (as determined by white space separation)
was 5.8 (sd. 2.9) and the average number of char-
acters (including punctuation and white space) was
32.3 (14.9). This is substantially longer than the cur-
rent average for web search query, which was ap-
proximately 2.8 in 2005 (Jansen et al., 2007); this is
due to the existence of natural language queries.
Judges were asked to classify each query accord-
ing to its expected response type into one of 17 cat-
egories (see Table 1). These categories include an-
swer types used in question answering research as
well as (to better capture the diverse nature of web
queries) several more general response types such
as Advice and General Information. Additionally,
we asked judges to anticipate what the best result
length would be for the query, as shown in Table 2.
Each of the 12,790 queries received three assess-
ments by MTurk judges. For answer types, the
number of times all three judges agreed was 4537
(35.4%); two agreed 6030 times (47.1%), and none
agreed 2223 times (17.4%). Not surprisingly, there
was significant overlap between the label General-
Info and the other categories. For answer length
estimations, all three judges agreed in 2361 cases
(18.5%), two agreed in 7210 cases (56.4%) and none
3219 times (25.2%).
Figure 1 summarizes expected length judgments
by estimated answer category. Distribution of the
length categories differs a great deal across the in-
dividual expected response categories. In general,
the results are intuitive: judges preferred short re-
sponses for “precise queries” (e.g., those asking for
numbers) and they preferred longer responses for
queries in broad categories like Advice or Gener-
alInfo. But some results are less intuitive: for ex-
ample, judges preferred different response lengths
for queries categorized as Person and Organization
– in fact for the latter the largest single selection
made was List. Reviewing the queries for these
two categories, we note that most queries about or-
ganizations in our collection asked for companies
</bodyText>
<page confidence="0.997694">
705
</page>
<table confidence="0.93479">
length type average std dev
Word or Phrase 38.1 25.8
Sentence 148.1 71.4
Paragraph 490.5 303.1
Section 1574.2 1251.1
</table>
<tableCaption confidence="0.930371">
Table 3: Average number of characters for each answer
length type for the stimuli used in the second experiment.
</tableCaption>
<bodyText confidence="0.999915888888889">
(e.g. “around the world travel agency”) and for
these there usually is more than one correct answer,
whereas the queries about persons (“CEO of mi-
crosoft” ) typically only had one relevant answer.
The results of this table show that there are some
trends but not definitive relationships between query
type (as classified in this study) and expected answer
length. More detailed classifications might help re-
solve some of the conflicts.
</bodyText>
<subsectionHeader confidence="0.994105">
3.3 Result Length Study
</subsectionHeader>
<bodyText confidence="0.999976875">
The purpose of the second study was twofold: first,
to see if doing a larger study confirms what is hinted
at in the literature: that search result lengths longer
than the standard snippet may be desirable for at
least a subset of queries. Second, we wanted to
see if judges’ predictions of desirable results lengths
would be confirmed by other judges’ responses to
search results of different lengths.
</bodyText>
<subsectionHeader confidence="0.974844">
3.3.1 Method
</subsectionHeader>
<bodyText confidence="0.999992811320755">
It has been found that obtaining judges’ agree-
ment on intent of a query from a log can be difficult
(Rose and Levinson, 2004; Kellar et al., 2007). In
order to make the task of judging query relevance
easier, for the next phase of the study we focused
on only those queries for which all three assessors
in the first experiment agreed both on the category
label and on the estimated ideal length. There were
1099 such high-confidence queries, whose average
number of words was 6.3 (2.9) and average number
of characters was 34.5 (14.3).
We randomly selected a subset of the high-
agreement queries from the first experiment and
manually excluded queries for which it seemed ob-
vious that no responses could be found in Wikipedia.
These included queries about song lyrics, since in-
tellectual property restrictions prevent these being
posted, and crossword puzzle questions such as “a
four letter word for water.”
The remaining set contained 170 queries. MTurk
annotators were asked to find one good text passage
(in English) for each query from the Wikipedia on-
line encyclopedia. They were also asked to subdi-
vide the text of this answer into each of the following
lengths: a word or phrase, a sentence, a paragraph,
a section or an entire article.2 Thus, the shorter an-
swer passages are subsumed by the longer ones.
Table 3 shows the average lengths and standard
deviations of each result length type. Table 4 con-
tains sample answers for the shorter length formats
for one query. For 24 of the 170 queries the annota-
tors could not find a suitable response in Wikipedia,
e.g., “How many miles between NY and Milwau-
kee?” We collected two to five results for each of the
remaining 146 queries and manually chose the best
of these answer passages. Note that, by design, all
responses were factually correct; they only differed
in their length.
Ten MTurk judges saw each query/answer length
pair, and for each of these, were told: “Below you
see a search engine query and a possible response.
We would like you to give us your opinion about the
response. We are especially interested in the length
of the response. Is it suitable for the query? Is there
too much or not enough information? Please rate the
response on a scale from 0 (very bad response) to 10
(very good response).” There were 124 judges in to-
tal; of these, 16 did more than 146 HITs, meaning
they saw the same query more than one time (but
with different lengths). Upon examination of the re-
sults, we determined that two of these high-volume
judges were not trying to do the task properly, and so
we dropped their judgments from the final analysis.
</bodyText>
<sectionHeader confidence="0.70249" genericHeader="evaluation">
3.3.2 Results
</sectionHeader>
<bodyText confidence="0.9999955">
Our results show that judges prefer results of dif-
ferent lengths, depending on the query. The re-
sults also suggest that judges’ estimates of a pre-
ferred result length in the first experiment are ac-
curate predictors when there is strong agreement
among them. Figure 2 shows in four diagrams
</bodyText>
<footnote confidence="0.959100714285714">
2Note the slight difference between the length categories in
the first and second experiment: The List and Other options
were dropped for the second experiment because we wanted to
concentrate on textual length. Additionally, to provide more
than one option between Sentence and Article, the category
One or more paragraphs was split up into two: (One) Para-
graph and (One) Section.
</footnote>
<page confidence="0.996006">
706
</page>
<bodyText confidence="0.989271666666667">
query Who was the first person to scale K2?
Paragraph An Italian expedition finally succeeded in ascending to the summit of K2 on July 31, 1954.
The expedition was led by Ardito Desio, although the two climbers who actually reached
the top were Lino Lacedelli and Achille Compagnoni. The team included a Pakistani mem-
ber, Colonel Muhammad Ata-ullah. He had been a part of an earlier 1953 American expe-
dition which failed to make the summit because of a storm which killed a key climber, Art
Gilkey. On the expedition also was the famous Italian climber Walter Bonatti. He proved
vital to the expeditions success in that he carried vital oxygen to 26,600ft for Lacedelli
and Compagnoni. His dramatic bivouac, at that altitude with the equipment, wrote another
chapter in the saga of Himalayan climbing.
Sentence The expedition was led by Ardito Desio, although the two climbers who actually reached
the top were Lino Lacedelli and Achille Compagnoni.
</bodyText>
<subsectionHeader confidence="0.581898">
Phrase Lino Lacedelli and Achille Compagnoni
</subsectionHeader>
<tableCaption confidence="0.583293714285714">
Table 4: Sample answers of differing lengths used as input for the second study. Note that the shorter answers are
contained in the longer ones. For the full article case, judges were asked to follow a hyperlink to an article.
Figure 2: Results of the second experiment, where each query/answer-length pair was assessed by 8–10 judges using
a scale of 0 (‘very bad’) to 10 (‘very good’). Marks indicate means and standard errors. The top left graph shows
responses of different lengths for queries that were classified as best answered with a phrase in the first experiment.
The upper right shows responses for queries predicted to be best answered with a sentence, lower left for best answered
with one or more paragraphs and lower right for best answered with an article.
</tableCaption>
<page confidence="0.954775">
707
</page>
<table confidence="0.9082108">
Slope Std. Error p-value
Phrase -0.850 0.044 &lt; 0.0001
Sentence -0.550 0.050 &lt; 0.0001
Paragraph 0.328 0.049 &lt; 0.0001
Article 0.856 0.053 &lt; 0.0001
</table>
<tableCaption confidence="0.997002333333333">
Table 5: Results of unweighted linear regression on the
data for the second experiment, which was separated into
four groups based on the predicted preferred length.
</tableCaption>
<bodyText confidence="0.999963977777778">
how queries assigned by judges to one of the four
length categories from the first experiment were
judged when presented with responses of the five
answer lengths from the second experiment. The
graphs show the means and standard error of the
judges’ scores across all queries for each predicted-
length/presented-length combination.
In order to test whether these results are signifi-
cant we performed four separate linear regressions;
one for each of the predicted preferred length cat-
egories. The snippet length, the independent vari-
able, was coded as 1-5, shortest to longest. The
score for each query-snippet pair is the dependent
variable. Table 5 shows that for each group there is
evidence to reject the null hypothesis that the slope
is equal to 0 at the 99% confidence level. High
scores are associated with shorter snippet lengths
for queries with predicted preferred length phrase
or sentence and also with longer snippet lengths for
queries with predicted preferred length paragraphs
or article. These associations are strongest for the
queries with the most extreme predicted preferred
lengths (phrase and article).
Our results also suggest the intuition that the best
answer lengths do not form strictly distinct classes,
but rather lie on a continuum. If the ideal response is
from a certain category (e.g., a sentence), returning a
result from an adjacent category (a phrase or a para-
graph) is not strongly penalized by judges, whereas
retuning a result from a category further up or down
the scale (an article) is.
One potential drawback of this study format is
that we do not show judges a list of results for
queries, as is standard in search engines, and so they
do not experience the tradeoff effect of longer results
requiring more scrolling if the desired answer is not
shown first. However, the earlier results of Cutrell &amp;
Guan (2007) and Paek et al. (2004) suggest that the
preference for longer results occurs even in contexts
that require looking through multiple results. An-
other potential drawback of the study is that judges
only view one relevant result; the effects of showing
a list of long non-relevant results may be more neg-
ative than that of showing short non-relevant results;
this study would not capture that effect.
</bodyText>
<sectionHeader confidence="0.998662" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999823815789473">
Our studies suggest that different queries are best
served with different response lengths (Experi-
ment 1), and that for a subset of especially clear
queries, human judges can predict the preferred re-
sult lengths (Experiment 2). The results furthermore
support the contention that standard results listings
are too short in many cases, at least assuming that
the summary shows information that is relevant for
the query. These findings have important implica-
tions for the design of search results presentations,
suggesting that as user queries become more expres-
sive, search engine results should become more re-
sponsive to the type of answer desired. This may
mean showing more context in the results listing, or
perhaps using more dynamic tools such as expand-
on-mouseover to help answer the query in place.
The obvious next step is to determine how to au-
tomatically classify queries according to their pre-
dicted result length and type. For classifying ac-
cording to expected length, we have run some initial
experiments based on unigram word counts which
correctly classified 78% of 286 test queries (on 805
training queries) into one of three length bins. We
plan to pursue this further in future work. For classi-
fying according to type, as discussed above, most
automated query classification for web logs have
been based on the topic of the query rather than on
the intended result type, but the question answering
literature has intensively investigated how to pre-
dict appropriate answer types. It is likely that the
techniques from these two fields can be productively
combined to address this challenge.
Acknowledgments. This work was supported in
part by Powerset, Inc., and in part by Microsoft Re-
search through the MSR European PhD Scholarship
Programme. We would like to thank Susan Gruber
and Bonnie Webber for their helpful comments and
suggestions.
</bodyText>
<page confidence="0.996842">
708
</page>
<sectionHeader confidence="0.995878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999811342105263">
A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josi-
fovski, and T. Zhang. 2007. Robust classification of
rare queries using web knowledge. Proceedings of SI-
GIR 2007.
A. Broder. 2002. A taxonomy of web search. ACM
SIGIR Forum, 36(2):3–10.
E. Cutrell and Z. Guan. 2007. What Are You Looking
For? An Eye-tracking Study of Information Usage in
Web Search. Proceedings ofACM SIGCHI 2007.
D.E. Egan, J.R. Remde, L.M. Gomez, T.K. Landauer,
J. Eberhardt, and C.C. Lochbaum. 1989. Formative
design evaluation of Superbook. ACM Transactions
on Information Systems (TOIS), 7(1):30–57.
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003.
Open-domain textual question answering techniques.
Natural Language Engineering, 9(03):231–267.
E. Hovy, U. Hermjakob, and D. Ravichandran. 2002.
A question/answer typology with surface text patterns.
Proceedings of the second international conference on
Human Language Technology Research, pages 247–
251.
B.J. Jansen and Spink. 2006. How are we searching
the World Wide Web? A comparison of nine search
engine transaction logs. Information Processing and
Management, 42(1):248–263.
B.J. Jansen, A. Spink, and S. Koshman. 2007. Web
searcher interaction with the Dogpile.com metasearch
engine. Journal of the American Society for Informa-
tion Science and Technology, 58(5):744–755.
M. Kellar, C. Watters, and M. Shepherd. 2007. A Goal-
based Classification of Web Information Tasks. JA-
SIST, 43(1).
J. Lin, D. Quan, V. Sinha, K. Bakshi, D. Huynh, B. Katz,
and D.R. Karger. 2003. What Makes a Good Answer?
The Role of Context in Question Answering. Human-
Computer Interaction (INTERACT 2003).
G. Marchionini, R.W. White, and Marchionini. 2008.
Find What You Need, Understand What You Find.
Journal ofHuman-Computer Interaction (to appear).
T. Paek, S.T. Dumais, and R. Logan. 2004. WaveLens:
A new view onto internet search results. Proceedings
on the ACM SIGCHI Conference on Human Factors in
Computing Systems, pages 727–734.
J. Pedersen, D. Cutting, and J. Tukey. 1991. Snippet
search: A single phrase approach to text access. Pro-
ceedings of the 1991 Joint Statistical Meetings.
G. Ramakrishnan and D. Paranjpe. 2004. Is question an-
swering an acquired skill? Proceedings of the 13th
international conference on World Wide Web, pages
111–120.
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. Proceedings of the 13th interna-
tional conference on World Wide Web, pages 13–19.
D.E. Rose, D. Orr, and R.G.P. Kantamneni. 2007. Sum-
mary attributes and perceived search quality. Pro-
ceedings of the 16th international conference on World
Wide Web, pages 1201–1202.
D. Shen, R. Pan, J.T. Sun, J.J. Pan, K. Wu, J. Yin,
and Q. Yang. 2005. Q2C@UST: our winning solu-
tion to query classification in KDDCUP 2005. ACM
SIGKDD Explorations Newsletter, 7(2):100–110.
Q. Su, D. Pavlov, J. Chow, and W. Baker. 2007. Internet-
Scale Collection of Human-Reviewed Data. Proceed-
ings of WWW 2007.
A. Tombros and M. Sanderson. 1998. Advantages of
query biased summaries in information retrieval. Pro-
ceedings of the 21st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 2–10.
E.M. Voorhees. 2003. Overview of the TREC 2003
Question Answering Track. Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
R.W. White, J. Jose, and I. Ruthven. 2003. A task-
oriented study on the influencing effects of query-
biased summarisation in web searching. Information
Processing and Management, 39(5):707–733.
</reference>
<page confidence="0.998668">
709
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672857">
<title confidence="0.999713">Improving Search Results Quality by Customizing Summary Lengths</title>
<author confidence="0.999986">Michael Kaisser</author>
<affiliation confidence="0.9076895">University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.869769">Edinburgh EH8 9LW</address>
<email confidence="0.97355">m.kaisser@sms.ed.ac.uk</email>
<author confidence="0.999803">Marti A Hearst</author>
<affiliation confidence="0.999549">UC Berkeley</affiliation>
<address confidence="0.9992285">102 South Hall Berkeley, CA 94705</address>
<email confidence="0.998277">hearst@ischool.berkeley.edu</email>
<author confidence="0.991499">John B Lowe</author>
<affiliation confidence="0.999566">Powerset, Inc.</affiliation>
<address confidence="0.999734">475 Brannan St. San Francisco, CA 94107</address>
<email confidence="0.999667">johnblowe@gmail.com</email>
<abstract confidence="0.998212142857143">Web search engines today typically show results as a list of titles and short snippets that summarize how the retrieved documents are related to the query. However, recent research suggests that longer summaries can be preferable for certain types of queries. This paper presents empirical evidence that judges can predict appropriate search result summary lengths, and that perceptions of search result quality can be affected by varying these result lengths. These findings have important implications for search results presentation, especially for natural language queries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Broder</author>
<author>M Fontoura</author>
<author>E Gabrilovich</author>
<author>A Joshi</author>
<author>V Josifovski</author>
<author>T Zhang</author>
</authors>
<title>Robust classification of rare queries using web knowledge.</title>
<date>2007</date>
<booktitle>Proceedings of SIGIR</booktitle>
<contexts>
<context position="12421" citStr="Broder et al. (2007)" startWordPosition="1995" endWordPosition="1998">of information need. For example, a search involving the topic of weather can consist of the simple information need of looking at today’s forecast, or the rich and complex information need of studying meteorology. Over many years, Spink &amp; Jansen et al. (2006; 2007) have manually analyzed samples of query logs to track a number of different trends. One of the most notable is the change in topic mix. As an alternative to man703 ual classification of query topics, Shen et al. (2005) described an algorithm for automatically classifying web queries into a set of pre-defined topics. More recently, Broder et al. (2007) presented a highly accurate method (around .7 F-score) for classifying short, rare queries into a taxonomy of 6,000 categories. 3 Study Goals Related work suggests that longer results are preferable, but not for all query types. The goal of our efforts was to determine preferred result length for search results, depending on type of query. To do this, we performed two studies: 1. We asked a set of judges to categorize a large set of web queries according to their expected preferred response type and expected preferred response length. 2. We then developed high-quality answer passages of diffe</context>
</contexts>
<marker>Broder, Fontoura, Gabrilovich, Joshi, Josifovski, Zhang, 2007</marker>
<rawString>A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josifovski, and T. Zhang. 2007. Robust classification of rare queries using web knowledge. Proceedings of SIGIR 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Broder</author>
</authors>
<title>A taxonomy of web search.</title>
<date>2002</date>
<journal>ACM SIGIR Forum,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="10603" citStr="Broder (2002)" startWordPosition="1691" endWordPosition="1692">es. Ramakrishnan et al. (2004) show an automated method for determining expected answer types using syntactic information and mapping query terms to WordNet. 2.4 Categorizing Web Queries A different line of research is the query log categorization problem. In query logs, the queries are often much more terse and ill-defined than in the TREC QA track, and, accordingly, the taxonomies used to classify what is called the query intent have been much more general. In an attempt to demonstrate how information needs for web search differ from the assumptions of pre-web information retrieval systems, Broder (2002) created a taxonomy of web search goals, and then estimated frequency of such goals by a combination of an online survey (3,200 responses, 10% response rate) and a manual analysis of 1,000 query from the AltaVista query logs. This taxonomy has been heavily influential in discussions of query types on the Web. Rose &amp; Levinson (2004) followed up on Broder’s work, again using web query logs, but developing a taxonomy that differed somewhat from Broder’s. They manually classified a set of 1,500 AltaVista search engine log queries. For two sets of 500 queries, the labeler saw just the query and the</context>
</contexts>
<marker>Broder, 2002</marker>
<rawString>A. Broder. 2002. A taxonomy of web search. ACM SIGIR Forum, 36(2):3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cutrell</author>
<author>Z Guan</author>
</authors>
<title>What Are You Looking For? An Eye-tracking Study of Information Usage in Web Search.</title>
<date>2007</date>
<booktitle>Proceedings ofACM SIGCHI</booktitle>
<contexts>
<context position="7976" citStr="Cutrell &amp; Guan (2007)" startWordPosition="1261" endWordPosition="1264">expanded the summary with a few words at a time. Eleven out of 18 participants preferred instant view over the other two views, and on average all participants produced faster and more accurate results with this view. Seven participants preferred dynamic view over the others, but many others found this view disruptive. The dynamic view suffered from the problem that, as the text expanded, the mouse no longer covered the selected results, and 702 so an unintended, different search result sometimes started to expand. Notably, none of the participants preferred the standard results listing view. Cutrell &amp; Guan (2007), compared search summaries of varying length: short (1 line of text), medium (2-3 lines) and long (6-7 lines) using search engine-produced snippets (it is unclear if the summary text was contiguous or included ellipses). They also compared 6 navigational queries (where the goal is to find a website’s homepage), with 6 informational queries (e.g., “find when the Titanic set sail for its only voyage and what port it left from,” “find out how long the Las Vegas monorail is”). In a study with 22 participants, they found that participants were 24 seconds faster on average with the long view than w</context>
<context position="26891" citStr="Cutrell &amp; Guan (2007)" startWordPosition="4391" endWordPosition="4394">t rather lie on a continuum. If the ideal response is from a certain category (e.g., a sentence), returning a result from an adjacent category (a phrase or a paragraph) is not strongly penalized by judges, whereas retuning a result from a category further up or down the scale (an article) is. One potential drawback of this study format is that we do not show judges a list of results for queries, as is standard in search engines, and so they do not experience the tradeoff effect of longer results requiring more scrolling if the desired answer is not shown first. However, the earlier results of Cutrell &amp; Guan (2007) and Paek et al. (2004) suggest that the preference for longer results occurs even in contexts that require looking through multiple results. Another potential drawback of the study is that judges only view one relevant result; the effects of showing a list of long non-relevant results may be more negative than that of showing short non-relevant results; this study would not capture that effect. 4 Conclusions and Future Work Our studies suggest that different queries are best served with different response lengths (Experiment 1), and that for a subset of especially clear queries, human judges </context>
</contexts>
<marker>Cutrell, Guan, 2007</marker>
<rawString>E. Cutrell and Z. Guan. 2007. What Are You Looking For? An Eye-tracking Study of Information Usage in Web Search. Proceedings ofACM SIGCHI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Egan</author>
<author>J R Remde</author>
<author>L M Gomez</author>
<author>T K Landauer</author>
<author>J Eberhardt</author>
<author>C C Lochbaum</author>
</authors>
<title>Formative design evaluation of Superbook.</title>
<date>1989</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="3838" citStr="Egan et al., 1989" startWordPosition="605" endWordPosition="608"> Eventually, search engines started showing what are known variously as query-biased summaries, keyword-in701 Proceedings ofACL-08: HLT, pages 701–709, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics context (KWIC) extractions, and user-directed summaries (Tombros and Sanderson, 1998). In these summaries, sentence fragments, full sentences, or groups of sentences that contain query terms are extracted from the full text. Early versions of this idea were developed in the Snippet Search tool (Pedersen et al., 1991) and the Superbook tool’s Table-ofContents view (Egan et al., 1989). A query-biased summary shows sentences that summarize the ways the query terms are used within the document. In addition to showing which subsets of query terms occur in a retrieved document, this display also exposes the context in which the query terms appear with respect to one another. Research suggests that query-biased summaries are superior to showing the first few sentences from documents. Tombros &amp; Sanderson (1998), in a study with 20 participants using TREC ad hoc data, found higher precision and recall and higher subjective preferences for query-biased summaries over summaries sho</context>
</contexts>
<marker>Egan, Remde, Gomez, Landauer, Eberhardt, Lochbaum, 1989</marker>
<rawString>D.E. Egan, J.R. Remde, L.M. Gomez, T.K. Landauer, J. Eberhardt, and C.C. Lochbaum. 1989. Formative design evaluation of Superbook. ACM Transactions on Information Systems (TOIS), 7(1):30–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>S J Maiorano</author>
<author>M A Pasca</author>
</authors>
<title>Open-domain textual question answering techniques.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>03</issue>
<contexts>
<context position="9772" citStr="Harabagiu et al., 2003" startWordPosition="1559" endWordPosition="1562">ing summary length, but they only compared relatively similar summary lengths (2 vs. 3 vs. 4 lines long). 2.3 Categorizing Questions by Expected Answer Types In the field of automated question-answering, much effort has been expended on automatically determining the kind of answer that is expected for a given question. The candidate answer types are often drawn from the types of questions that have appeared in the TREC Question Answering track (Voorhees, 2003). For example, the Webclopedia project created a taxonomy of 180 types of question targets (Hovy et al., 2002), and the FALCON project (Harabagiu et al., 2003) developed an answer taxonomy with 33 top level categories (such as PERSON, TIME, REASON, PRODUCT, LOCATION, NUMERICAL VALUE, QUOTATION), and these were further refined into an unspecified number of additional categories. Ramakrishnan et al. (2004) show an automated method for determining expected answer types using syntactic information and mapping query terms to WordNet. 2.4 Categorizing Web Queries A different line of research is the query log categorization problem. In query logs, the queries are often much more terse and ill-defined than in the TREC QA track, and, accordingly, the taxonom</context>
</contexts>
<marker>Harabagiu, Maiorano, Pasca, 2003</marker>
<rawString>S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003. Open-domain textual question answering techniques. Natural Language Engineering, 9(03):231–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>U Hermjakob</author>
<author>D Ravichandran</author>
</authors>
<title>A question/answer typology with surface text patterns.</title>
<date>2002</date>
<booktitle>Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>247--251</pages>
<contexts>
<context position="9723" citStr="Hovy et al., 2002" startWordPosition="1551" endWordPosition="1554"> effects. They did not find effects for varying summary length, but they only compared relatively similar summary lengths (2 vs. 3 vs. 4 lines long). 2.3 Categorizing Questions by Expected Answer Types In the field of automated question-answering, much effort has been expended on automatically determining the kind of answer that is expected for a given question. The candidate answer types are often drawn from the types of questions that have appeared in the TREC Question Answering track (Voorhees, 2003). For example, the Webclopedia project created a taxonomy of 180 types of question targets (Hovy et al., 2002), and the FALCON project (Harabagiu et al., 2003) developed an answer taxonomy with 33 top level categories (such as PERSON, TIME, REASON, PRODUCT, LOCATION, NUMERICAL VALUE, QUOTATION), and these were further refined into an unspecified number of additional categories. Ramakrishnan et al. (2004) show an automated method for determining expected answer types using syntactic information and mapping query terms to WordNet. 2.4 Categorizing Web Queries A different line of research is the query log categorization problem. In query logs, the queries are often much more terse and ill-defined than in</context>
</contexts>
<marker>Hovy, Hermjakob, Ravichandran, 2002</marker>
<rawString>E. Hovy, U. Hermjakob, and D. Ravichandran. 2002. A question/answer typology with surface text patterns. Proceedings of the second international conference on Human Language Technology Research, pages 247– 251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>Spink</author>
</authors>
<title>How are we searching the World Wide Web? A comparison of nine search engine transaction logs.</title>
<date>2006</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>42--1</pages>
<marker>Jansen, Spink, 2006</marker>
<rawString>B.J. Jansen and Spink. 2006. How are we searching the World Wide Web? A comparison of nine search engine transaction logs. Information Processing and Management, 42(1):248–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>A Spink</author>
<author>S Koshman</author>
</authors>
<title>Web searcher interaction with the Dogpile.com metasearch engine.</title>
<date>2007</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>58</volume>
<issue>5</issue>
<contexts>
<context position="16809" citStr="Jansen et al., 2007" startWordPosition="2708" endWordPosition="2711">answer, in the same order as listed in Table 1; the x-axis shows the preferred length as listed in Table 2. Three bars with length greater than 1,500 are trimmed to the maximum size to improve readability (GeneralInfo/Paragraphs, GeneralInfo/Article, and Number/Phrase). terest of our research. The average number of words per query (as determined by white space separation) was 5.8 (sd. 2.9) and the average number of characters (including punctuation and white space) was 32.3 (14.9). This is substantially longer than the current average for web search query, which was approximately 2.8 in 2005 (Jansen et al., 2007); this is due to the existence of natural language queries. Judges were asked to classify each query according to its expected response type into one of 17 categories (see Table 1). These categories include answer types used in question answering research as well as (to better capture the diverse nature of web queries) several more general response types such as Advice and General Information. Additionally, we asked judges to anticipate what the best result length would be for the query, as shown in Table 2. Each of the 12,790 queries received three assessments by MTurk judges. For answer type</context>
</contexts>
<marker>Jansen, Spink, Koshman, 2007</marker>
<rawString>B.J. Jansen, A. Spink, and S. Koshman. 2007. Web searcher interaction with the Dogpile.com metasearch engine. Journal of the American Society for Information Science and Technology, 58(5):744–755.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kellar</author>
<author>C Watters</author>
<author>M Shepherd</author>
</authors>
<title>A Goalbased Classification of Web Information Tasks.</title>
<date>2007</date>
<journal>JASIST,</journal>
<volume>43</volume>
<issue>1</issue>
<contexts>
<context position="19798" citStr="Kellar et al., 2007" startWordPosition="3198" endWordPosition="3201">lp resolve some of the conflicts. 3.3 Result Length Study The purpose of the second study was twofold: first, to see if doing a larger study confirms what is hinted at in the literature: that search result lengths longer than the standard snippet may be desirable for at least a subset of queries. Second, we wanted to see if judges’ predictions of desirable results lengths would be confirmed by other judges’ responses to search results of different lengths. 3.3.1 Method It has been found that obtaining judges’ agreement on intent of a query from a log can be difficult (Rose and Levinson, 2004; Kellar et al., 2007). In order to make the task of judging query relevance easier, for the next phase of the study we focused on only those queries for which all three assessors in the first experiment agreed both on the category label and on the estimated ideal length. There were 1099 such high-confidence queries, whose average number of words was 6.3 (2.9) and average number of characters was 34.5 (14.3). We randomly selected a subset of the highagreement queries from the first experiment and manually excluded queries for which it seemed obvious that no responses could be found in Wikipedia. These included quer</context>
</contexts>
<marker>Kellar, Watters, Shepherd, 2007</marker>
<rawString>M. Kellar, C. Watters, and M. Shepherd. 2007. A Goalbased Classification of Web Information Tasks. JASIST, 43(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Quan</author>
<author>V Sinha</author>
<author>K Bakshi</author>
<author>D Huynh</author>
<author>B Katz</author>
<author>D R Karger</author>
</authors>
<title>What Makes a Good Answer? The Role of Context in Question Answering. HumanComputer Interaction (INTERACT</title>
<date>2003</date>
<contexts>
<context position="5663" citStr="Lin et al. (2003)" startWordPosition="893" endWordPosition="896">pace required by each search hit. There is also a tension between showing short snippets that contain all or most of the query terms and showing coherent stretches of text. If the query terms do not co-occur near one another, then the extract has to become very long if full sentences and all query terms are to be shown. Many web search engine snippets compromise by showing fragments instead of sentences. 2.2 Studies Comparing Results Lengths Recently, a few studies have analyzed the results of varying search summary length. In the question-answering context (as opposed to general web search), Lin et al. (2003) conducted a usability study with 32 computer science students comparing four types of answer context: exact answer, answer-in-sentence, answer-in-paragraph, and answer-in-document. To remove effects of incorrect answers, they used a system that produced only correct answers, drawn from an online encyclopedia. Participants viewed answers for 8 question scenarios. Lin et al. (2003) found no significant differences in task completion times, but they did find differences in subjective responses. Most participants (53%) preferred paragraph-sized chunks, noting that a sentence wasn’t much more info</context>
</contexts>
<marker>Lin, Quan, Sinha, Bakshi, Huynh, Katz, Karger, 2003</marker>
<rawString>J. Lin, D. Quan, V. Sinha, K. Bakshi, D. Huynh, B. Katz, and D.R. Karger. 2003. What Makes a Good Answer? The Role of Context in Question Answering. HumanComputer Interaction (INTERACT 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Marchionini</author>
<author>R W White</author>
<author>Marchionini</author>
</authors>
<title>Find What You Need, Understand What You Find.</title>
<date>2008</date>
<journal>Journal ofHuman-Computer Interaction</journal>
<note>(to appear).</note>
<contexts>
<context position="1165" citStr="Marchionini et al., 2008" startWordPosition="167" endWordPosition="171">that longer summaries can be preferable for certain types of queries. This paper presents empirical evidence that judges can predict appropriate search result summary lengths, and that perceptions of search result quality can be affected by varying these result lengths. These findings have important implications for search results presentation, especially for natural language queries. 1 Introduction Search results listings on the web have become standardized as a list of information summarizing the retrieved documents. This summary information is often referred to as the document’s surrogate (Marchionini et al., 2008). In older search systems, such as those used in news and legal search, the document surrogate typically consisted of the title and important metadata, such as date, author, source, and length of the article, as well as the document’s manually written abstract. In most cases, the full text content of the document was not available to the search engine and so no extracts could be made. In web search, document surrogates typically show the web page’s title, a URL, and information extracted from the full text contents of the document. This latter part is referred to by several different names, in</context>
</contexts>
<marker>Marchionini, White, Marchionini, 2008</marker>
<rawString>G. Marchionini, R.W. White, and Marchionini. 2008. Find What You Need, Understand What You Find. Journal ofHuman-Computer Interaction (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>S T Dumais</author>
<author>R Logan</author>
</authors>
<title>WaveLens: A new view onto internet search results.</title>
<date>2004</date>
<booktitle>Proceedings on the ACM SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>727--734</pages>
<contexts>
<context position="6534" citStr="Paek et al. (2004)" startWordPosition="1021" endWordPosition="1024">d only correct answers, drawn from an online encyclopedia. Participants viewed answers for 8 question scenarios. Lin et al. (2003) found no significant differences in task completion times, but they did find differences in subjective responses. Most participants (53%) preferred paragraph-sized chunks, noting that a sentence wasn’t much more information beyond the exact answer, and a full document was oftentimes too long. That said, 23% preferred full documents, 20% preferred sentences, and one participant preferred exact answer, thus suggesting that there is considerable individual variation. Paek et al. (2004) experimented with showing differing amounts of summary information in results listings, controlling the study design so that only one result in each list of 10 was relevant. For half the test questions, the target information was visible in the original snippet, and for the other half, the participant needed to use their mouse to view more information from the relevant search result. They compared three interface conditions: (i) a standard search results listing, in which a mouse click on the title brings up the full text of the web page, (ii) “instant” view, for which a mouseclick expanded t</context>
<context position="26914" citStr="Paek et al. (2004)" startWordPosition="4396" endWordPosition="4399">m. If the ideal response is from a certain category (e.g., a sentence), returning a result from an adjacent category (a phrase or a paragraph) is not strongly penalized by judges, whereas retuning a result from a category further up or down the scale (an article) is. One potential drawback of this study format is that we do not show judges a list of results for queries, as is standard in search engines, and so they do not experience the tradeoff effect of longer results requiring more scrolling if the desired answer is not shown first. However, the earlier results of Cutrell &amp; Guan (2007) and Paek et al. (2004) suggest that the preference for longer results occurs even in contexts that require looking through multiple results. Another potential drawback of the study is that judges only view one relevant result; the effects of showing a list of long non-relevant results may be more negative than that of showing short non-relevant results; this study would not capture that effect. 4 Conclusions and Future Work Our studies suggest that different queries are best served with different response lengths (Experiment 1), and that for a subset of especially clear queries, human judges can predict the preferr</context>
</contexts>
<marker>Paek, Dumais, Logan, 2004</marker>
<rawString>T. Paek, S.T. Dumais, and R. Logan. 2004. WaveLens: A new view onto internet search results. Proceedings on the ACM SIGCHI Conference on Human Factors in Computing Systems, pages 727–734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pedersen</author>
<author>D Cutting</author>
<author>J Tukey</author>
</authors>
<title>Snippet search: A single phrase approach to text access.</title>
<date>1991</date>
<booktitle>Proceedings of the 1991 Joint Statistical Meetings.</booktitle>
<contexts>
<context position="3771" citStr="Pedersen et al., 1991" startWordPosition="593" endWordPosition="597">rt of the full text that the search engines retained from their crawls. Eventually, search engines started showing what are known variously as query-biased summaries, keyword-in701 Proceedings ofACL-08: HLT, pages 701–709, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics context (KWIC) extractions, and user-directed summaries (Tombros and Sanderson, 1998). In these summaries, sentence fragments, full sentences, or groups of sentences that contain query terms are extracted from the full text. Early versions of this idea were developed in the Snippet Search tool (Pedersen et al., 1991) and the Superbook tool’s Table-ofContents view (Egan et al., 1989). A query-biased summary shows sentences that summarize the ways the query terms are used within the document. In addition to showing which subsets of query terms occur in a retrieved document, this display also exposes the context in which the query terms appear with respect to one another. Research suggests that query-biased summaries are superior to showing the first few sentences from documents. Tombros &amp; Sanderson (1998), in a study with 20 participants using TREC ad hoc data, found higher precision and recall and higher s</context>
</contexts>
<marker>Pedersen, Cutting, Tukey, 1991</marker>
<rawString>J. Pedersen, D. Cutting, and J. Tukey. 1991. Snippet search: A single phrase approach to text access. Proceedings of the 1991 Joint Statistical Meetings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ramakrishnan</author>
<author>D Paranjpe</author>
</authors>
<title>Is question answering an acquired skill?</title>
<date>2004</date>
<booktitle>Proceedings of the 13th international conference on World Wide Web,</booktitle>
<pages>111--120</pages>
<marker>Ramakrishnan, Paranjpe, 2004</marker>
<rawString>G. Ramakrishnan and D. Paranjpe. 2004. Is question answering an acquired skill? Proceedings of the 13th international conference on World Wide Web, pages 111–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rose</author>
<author>D Levinson</author>
</authors>
<title>Understanding user goals in web search.</title>
<date>2004</date>
<booktitle>Proceedings of the 13th international conference on World Wide Web,</booktitle>
<pages>13--19</pages>
<contexts>
<context position="19776" citStr="Rose and Levinson, 2004" startWordPosition="3194" endWordPosition="3197"> classifications might help resolve some of the conflicts. 3.3 Result Length Study The purpose of the second study was twofold: first, to see if doing a larger study confirms what is hinted at in the literature: that search result lengths longer than the standard snippet may be desirable for at least a subset of queries. Second, we wanted to see if judges’ predictions of desirable results lengths would be confirmed by other judges’ responses to search results of different lengths. 3.3.1 Method It has been found that obtaining judges’ agreement on intent of a query from a log can be difficult (Rose and Levinson, 2004; Kellar et al., 2007). In order to make the task of judging query relevance easier, for the next phase of the study we focused on only those queries for which all three assessors in the first experiment agreed both on the category label and on the estimated ideal length. There were 1099 such high-confidence queries, whose average number of words was 6.3 (2.9) and average number of characters was 34.5 (14.3). We randomly selected a subset of the highagreement queries from the first experiment and manually excluded queries for which it seemed obvious that no responses could be found in Wikipedi</context>
<context position="10936" citStr="Rose &amp; Levinson (2004)" startWordPosition="1746" endWordPosition="1749">d than in the TREC QA track, and, accordingly, the taxonomies used to classify what is called the query intent have been much more general. In an attempt to demonstrate how information needs for web search differ from the assumptions of pre-web information retrieval systems, Broder (2002) created a taxonomy of web search goals, and then estimated frequency of such goals by a combination of an online survey (3,200 responses, 10% response rate) and a manual analysis of 1,000 query from the AltaVista query logs. This taxonomy has been heavily influential in discussions of query types on the Web. Rose &amp; Levinson (2004) followed up on Broder’s work, again using web query logs, but developing a taxonomy that differed somewhat from Broder’s. They manually classified a set of 1,500 AltaVista search engine log queries. For two sets of 500 queries, the labeler saw just the query and the retrieved documents; for the third set the labeler also saw information about which item(s) the searcher clicked on. They found that the classifications that used the extra information about clickthrough did not change the proportions of assignments to each category. Because they did not directly compare judgments with and without</context>
</contexts>
<marker>Rose, Levinson, 2004</marker>
<rawString>D.E. Rose and D. Levinson. 2004. Understanding user goals in web search. Proceedings of the 13th international conference on World Wide Web, pages 13–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rose</author>
<author>D Orr</author>
<author>R G P Kantamneni</author>
</authors>
<title>Summary attributes and perceived search quality.</title>
<date>2007</date>
<booktitle>Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>1201--1202</pages>
<contexts>
<context position="8944" citStr="Rose et al. (2007)" startWordPosition="1428" endWordPosition="1431">“find when the Titanic set sail for its only voyage and what port it left from,” “find out how long the Las Vegas monorail is”). In a study with 22 participants, they found that participants were 24 seconds faster on average with the long view than with the short and medium view. The also found that participants were 10 seconds slower on average with the long view for the navigational tasks. They present eye tracking evidence which suggests that on the navigational task, the extra text distracts the eye from the URL. They did not report on subjective responses to the different answer lengths. Rose et al. (2007) varied search results summaries along several dimensions, finding that text choppiness and sentence truncation had negative effects, and genre cues had positive effects. They did not find effects for varying summary length, but they only compared relatively similar summary lengths (2 vs. 3 vs. 4 lines long). 2.3 Categorizing Questions by Expected Answer Types In the field of automated question-answering, much effort has been expended on automatically determining the kind of answer that is expected for a given question. The candidate answer types are often drawn from the types of questions tha</context>
</contexts>
<marker>Rose, Orr, Kantamneni, 2007</marker>
<rawString>D.E. Rose, D. Orr, and R.G.P. Kantamneni. 2007. Summary attributes and perceived search quality. Proceedings of the 16th international conference on World Wide Web, pages 1201–1202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>R Pan</author>
<author>J T Sun</author>
<author>J J Pan</author>
<author>K Wu</author>
<author>J Yin</author>
<author>Q Yang</author>
</authors>
<title>Q2C@UST: our winning solution to query classification in KDDCUP</title>
<date>2005</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="12286" citStr="Shen et al. (2005)" startWordPosition="1974" endWordPosition="1977">ry intent. Alternatively, queries from web query logs can be classified according to the topic of the query, independent of the type of information need. For example, a search involving the topic of weather can consist of the simple information need of looking at today’s forecast, or the rich and complex information need of studying meteorology. Over many years, Spink &amp; Jansen et al. (2006; 2007) have manually analyzed samples of query logs to track a number of different trends. One of the most notable is the change in topic mix. As an alternative to man703 ual classification of query topics, Shen et al. (2005) described an algorithm for automatically classifying web queries into a set of pre-defined topics. More recently, Broder et al. (2007) presented a highly accurate method (around .7 F-score) for classifying short, rare queries into a taxonomy of 6,000 categories. 3 Study Goals Related work suggests that longer results are preferable, but not for all query types. The goal of our efforts was to determine preferred result length for search results, depending on type of query. To do this, we performed two studies: 1. We asked a set of judges to categorize a large set of web queries according to th</context>
</contexts>
<marker>Shen, Pan, Sun, Pan, Wu, Yin, Yang, 2005</marker>
<rawString>D. Shen, R. Pan, J.T. Sun, J.J. Pan, K. Wu, J. Yin, and Q. Yang. 2005. Q2C@UST: our winning solution to query classification in KDDCUP 2005. ACM SIGKDD Explorations Newsletter, 7(2):100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Su</author>
<author>D Pavlov</author>
<author>J Chow</author>
<author>W Baker</author>
</authors>
<title>InternetScale Collection of Human-Reviewed Data.</title>
<date>2007</date>
<booktitle>Proceedings of WWW</booktitle>
<contexts>
<context position="15440" citStr="Su et al., 2007" startWordPosition="2488" endWordPosition="2491">xplanation 15. Yes/No, with or without explanation or evidence 16. Other 17. Unjudgable Table 1: Allowable responses to the question: “What sort of result or results does the query ask for?” in the first experiment. 1. A word or short phrase 2. A sentence 3. One or more paragraphs (i.e. at least several sentences) 4. An article or full document 5. A list 6. Other, or some combination of the above Table 2: Allowable responses to the question: “How long is the best result for this query?” in the first experiment. annotation using this service and have found that the results are of high quality (Su et al., 2007). 3.2 Estimating Ideal Answer Length and Type We developed a set of 12,790 queries, drawn from Powerset’s in house query database which contains representative subsets of queries from different search engines’ query logs, as well as hand-edited query sets used for regression testing. There are a disproportionally large number of natural language queries in this set compared with query sets from typical keyword engines. Such queries are often complete questions and are sometimes grammatical fragments (e.g., “date of next US election”) and so are likely to be amenable to interesting natural lang</context>
</contexts>
<marker>Su, Pavlov, Chow, Baker, 2007</marker>
<rawString>Q. Su, D. Pavlov, J. Chow, and W. Baker. 2007. InternetScale Collection of Human-Reviewed Data. Proceedings of WWW 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tombros</author>
<author>M Sanderson</author>
</authors>
<title>Advantages of query biased summaries in information retrieval.</title>
<date>1998</date>
<booktitle>Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>2--10</pages>
<contexts>
<context position="3538" citStr="Tombros and Sanderson, 1998" startWordPosition="555" endWordPosition="558">ifferent lengths. 2 Related Work 2.1 Query-biased Summaries In the early days of the web, the result summary consisted of the first few lines of text, due both to concerns about intellectual property, and because often that was the only part of the full text that the search engines retained from their crawls. Eventually, search engines started showing what are known variously as query-biased summaries, keyword-in701 Proceedings ofACL-08: HLT, pages 701–709, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics context (KWIC) extractions, and user-directed summaries (Tombros and Sanderson, 1998). In these summaries, sentence fragments, full sentences, or groups of sentences that contain query terms are extracted from the full text. Early versions of this idea were developed in the Snippet Search tool (Pedersen et al., 1991) and the Superbook tool’s Table-ofContents view (Egan et al., 1989). A query-biased summary shows sentences that summarize the ways the query terms are used within the document. In addition to showing which subsets of query terms occur in a retrieved document, this display also exposes the context in which the query terms appear with respect to one another. Researc</context>
<context position="4267" citStr="Tombros &amp; Sanderson (1998)" startWordPosition="672" endWordPosition="675">ms are extracted from the full text. Early versions of this idea were developed in the Snippet Search tool (Pedersen et al., 1991) and the Superbook tool’s Table-ofContents view (Egan et al., 1989). A query-biased summary shows sentences that summarize the ways the query terms are used within the document. In addition to showing which subsets of query terms occur in a retrieved document, this display also exposes the context in which the query terms appear with respect to one another. Research suggests that query-biased summaries are superior to showing the first few sentences from documents. Tombros &amp; Sanderson (1998), in a study with 20 participants using TREC ad hoc data, found higher precision and recall and higher subjective preferences for query-biased summaries over summaries showing the first few sentences. Similar results for timing and subjective measurements were found by White et al. (2003) in a study with 24 participants. White et al. (2003) also describe experiments with different sentence selection mechanisms, including giving more weight to sentences that contained query words along with text formatting. There are significant design questions surrounding how best to formulate and display que</context>
</contexts>
<marker>Tombros, Sanderson, 1998</marker>
<rawString>A. Tombros and M. Sanderson. 1998. Advantages of query biased summaries in information retrieval. Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 2–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>Proceedings of the Twelfth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="9613" citStr="Voorhees, 2003" startWordPosition="1534" endWordPosition="1535">ons, finding that text choppiness and sentence truncation had negative effects, and genre cues had positive effects. They did not find effects for varying summary length, but they only compared relatively similar summary lengths (2 vs. 3 vs. 4 lines long). 2.3 Categorizing Questions by Expected Answer Types In the field of automated question-answering, much effort has been expended on automatically determining the kind of answer that is expected for a given question. The candidate answer types are often drawn from the types of questions that have appeared in the TREC Question Answering track (Voorhees, 2003). For example, the Webclopedia project created a taxonomy of 180 types of question targets (Hovy et al., 2002), and the FALCON project (Harabagiu et al., 2003) developed an answer taxonomy with 33 top level categories (such as PERSON, TIME, REASON, PRODUCT, LOCATION, NUMERICAL VALUE, QUOTATION), and these were further refined into an unspecified number of additional categories. Ramakrishnan et al. (2004) show an automated method for determining expected answer types using syntactic information and mapping query terms to WordNet. 2.4 Categorizing Web Queries A different line of research is the </context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>E.M. Voorhees. 2003. Overview of the TREC 2003 Question Answering Track. Proceedings of the Twelfth Text REtrieval Conference (TREC 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W White</author>
<author>J Jose</author>
<author>I Ruthven</author>
</authors>
<title>A taskoriented study on the influencing effects of querybiased summarisation in web searching.</title>
<date>2003</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>39</volume>
<issue>5</issue>
<contexts>
<context position="4556" citStr="White et al. (2003)" startWordPosition="718" endWordPosition="721">e document. In addition to showing which subsets of query terms occur in a retrieved document, this display also exposes the context in which the query terms appear with respect to one another. Research suggests that query-biased summaries are superior to showing the first few sentences from documents. Tombros &amp; Sanderson (1998), in a study with 20 participants using TREC ad hoc data, found higher precision and recall and higher subjective preferences for query-biased summaries over summaries showing the first few sentences. Similar results for timing and subjective measurements were found by White et al. (2003) in a study with 24 participants. White et al. (2003) also describe experiments with different sentence selection mechanisms, including giving more weight to sentences that contained query words along with text formatting. There are significant design questions surrounding how best to formulate and display query-biased summaries. As with standard document summarization and extraction, there is an inherent trade-off between showing long, informative summaries and minimizing the screen space required by each search hit. There is also a tension between showing short snippets that contain all or m</context>
</contexts>
<marker>White, Jose, Ruthven, 2003</marker>
<rawString>R.W. White, J. Jose, and I. Ruthven. 2003. A taskoriented study on the influencing effects of querybiased summarisation in web searching. Information Processing and Management, 39(5):707–733.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>