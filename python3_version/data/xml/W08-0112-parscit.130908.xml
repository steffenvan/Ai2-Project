<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005165">
<title confidence="0.9981925">
What Are Meeting Summaries? An Analysis of Human Extractive
Summaries in Meeting Corpus
</title>
<author confidence="0.99947">
Fei Liu, Yang Liu
</author>
<affiliation confidence="0.9989125">
Erik Jonsson School of Engineering and Computer Science
The University of Texas at Dallas
</affiliation>
<address confidence="0.921339">
Richardson, TX, USA
</address>
<email confidence="0.999787">
{feiliu,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.998582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994456428571429">
Significant research efforts have been devoted to
speech summarization, including automatic ap-
proaches and evaluation metrics. However, a fun-
damental problem about what summaries are for the
speech data and whether humans agree with each
other remains unclear. This paper performs an anal-
ysis of human annotated extractive summaries us-
ing the ICSI meeting corpus with an aim to examine
their consistency and the factors impacting human
agreement. In addition to using Kappa statistics and
ROUGE scores, we also proposed a sentence dis-
tance score and divergence distance as a quantitative
measure. This study is expected to help better define
the speech summarization problem.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999804666666667">
With the fast development of recording and storage tech-
niques in recent years, speech summarization has re-
ceived more attention. A variety of approaches have
been investigated for speech summarization, for exam-
ple, maximum entropy, conditional random fields, latent
semantic analysis, support vector machines, maximum
marginal relevance (Maskey and Hirschberg, 2003; Hori
et al., 2003; Buist et al., 2005; Galley, 2006; Murray et
al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These
studies used different domains, such as broadcast news,
lectures, and meetings. In these approaches, different in-
formation sources have been examined from both text and
speech related features (e.g., prosody, speaker activity,
turn-taking, discourse).
How to evaluate speech summaries has also been stud-
ied recently, but so far there is no consensus on eval-
uation yet. Often the goal in evaluation is to develop
an automatic metric to have a high correlation with hu-
man evaluation scores. Different methods have been used
in the above summarization research to compare system
generated summaries with human annotation, such as F-
measure, ROUGE, Pyramid, sumACCY (Lin and Hovy,
2003; Nenkova and Passonneau, 2004; Hori et al., 2003).
Typically multiple reference human summaries are used
</bodyText>
<page confidence="0.973001">
80
</page>
<bodyText confidence="0.999884347826087">
in evaluation in order to account for the inconsistency
among human annotations.
While there have been efforts on speech summariza-
tion approaches and evaluation, some fundamental prob-
lems are still unclear. For example, what are speech sum-
maries? Do humans agree with each other on summary
extraction? In this paper, we focus on the meeting do-
main, one of the most challenging speech genre, to an-
alyze human summary annotation. Meetings often have
several participants. Its speech is spontaneous, contains
disfluencies, and lacks structure. These all post new chal-
lenges to the consensus of human extracted summaries.
Our goal in this study is to investigate the variation of
human extractive summaries, and help to better under-
stand the gold standard reference summaries for meet-
ing summarization. This paper aims to answer two key
questions: (1) How much variation is there in human ex-
tractive meeting summaries? (2) What are the factors
that may impact interannotator agreement? We use three
different metrics to evaluate the variation among human
summaries, including Kappa statistic, ROUGE score, and
a new proposed divergence distance score to reflect the
coherence and quality of an annotation.
</bodyText>
<sectionHeader confidence="0.962641" genericHeader="method">
2 Corpus Description
</sectionHeader>
<bodyText confidence="0.988639333333333">
We use the ICSI meeting corpus (Janin et al., 2003) which
contains 75 naturally-occurred meetings, each about an
hour long. All of them have been transcribed and anno-
tated with dialog acts (DA) (Shriberg et al., 2004), top-
ics, and abstractive and extractive summaries in the AMI
project (Murray et al., 2005).
We selected 27 meetings from this corpus. Three anno-
tators (undergraduate students) were recruited to extract
summary sentences on a topic basis using the topic seg-
ments from the AMI annotation. Each sentence corre-
sponds to one DA annotated in the corpus. The annota-
tors were told to use their own judgment to pick summary
sentences that are informative and can preserve discus-
sion flow. The recommended percentages for the selected
summary sentences and words were set to 8.0% and
16.0% respectively. Human subjects were provided with
both the meeting audio files and an annotation Graphi-
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 80–83,
Columbus, June 2008. c�2008 Association for Computational Linguistics
cal User Interface, from which they can browse the man-
ual transcripts and see the percentage of the currently se-
lected summary sentences and words.
We refer to the above 27 meetings Data set I in this
paper. In addition, some of our studies are performed
based on the 6 meeting used in (Murray et al., 2005),
for which we have human annotated summaries using 3
different guidelines:
</bodyText>
<listItem confidence="0.998694666666667">
• Data set II: summary annotated on a topic basis. This is
a subset of the 27 annotated meetings above.
• Data set III: annotation is done for the entire meeting
without topic segments.
• Data set IV: the extractive summaries are from the AMI
annotation (Murray et al., 2005).
</listItem>
<sectionHeader confidence="0.781138" genericHeader="method">
3 Analysis Results
</sectionHeader>
<subsectionHeader confidence="0.999014">
3.1 Kappa Statistic
</subsectionHeader>
<bodyText confidence="0.999973222222222">
Kappa coefficient (Carletta, 1996) is commonly used
as a standard to reflect inter-annotator agreement. Ta-
ble 1 shows the average Kappa results, calculated for
each meeting using the data sets described in Section 2.
Compared to Kappa score on text summarization, which
is reported to be 0.38 by (Mani et al., 2002) on a set
of TREC documents, the inter-annotator agreement on
meeting corpus is lower. This is likely due to the dif-
ference between the meeting style and written text.
</bodyText>
<table confidence="0.265086">
Data Set I II III IV
</table>
<tableCaption confidence="0.7633515">
Avg-Kappa 0.261 0.245 0.335 0.290
Table 1: Average Kappa scores on different data sets.
</tableCaption>
<bodyText confidence="0.999899">
There are several other observations from Table 1.
First, comparing the results for Data Set (II) and (III),
both containing six meetings, the agreement is higher
for Data Set (III). Originally, we expected that by di-
viding the transcript into several topics, human subjects
can focus better on each topic discussed during the meet-
ing. However, the result does not support this hypoth-
esis. Moreover, the Kappa result of Data Set (III) also
outperforms that of Data Set (IV). The latter data set is
from the AMI annotation, where they utilized a different
annotation scheme: the annotators were asked to extract
dialog acts that are highly relevant to the given abstrac-
tive meeting summary. Contrary to our expectation, the
Kappa score in this data set is still lower than that of Data
Set (III), which used a direct sentence extraction scheme
on the whole transcript. This suggests that even using
the abstracts as a guidance, people still have a high varia-
tion in extracting summary sentences. We also calculated
the pairwise Kappa score between annotations in differ-
ent data sets. The inter-group Kappa score is much lower
than those of the intragroup agreement, most likely due
to the different annotation specifications used in the two
different data sets.
</bodyText>
<subsectionHeader confidence="0.999664">
3.2 Impacting Factors
</subsectionHeader>
<bodyText confidence="0.999638692307692">
We further analyze inter-annotator agreement with re-
spect to two factors: topic length and meeting partic-
ipants. All of the following experiments are based on
Data Set (I) with 27 meetings.
We computed Kappa statistic for each topic instead of
the entire meeting. The distribution of Kappa score with
respect to the topic length (measured using the number of
DAs) is shown in Figure 1. When the topic length is less
than 100, Kappa scores vary greatly, from -0.065 to 1.
Among the entire range of different topic lengths, there
seems no obvious relationship between the Kappa score
and the topic length (a regression from the data points
does not suggest a fit with an interpretable trend).
</bodyText>
<figureCaption confidence="0.992285">
Figure 1: Relationship between Kappa score and topic length.
</figureCaption>
<bodyText confidence="0.999597222222222">
Using the same Kappa score for each topic, we also in-
vestigated its relationship with the number of speakers in
that topic. Here we focused on the topic segments longer
than a threshold (with more than 60 DAs) as there seems
to be a wide range of Kappa results when the topic is
short (in Figure 1). Table 2 shows the average Kappa
score for these long topics, using the number of speak-
ers in the topic as the variable. We notice that when the
speaker number varies from 4 to 7, kappa scores grad-
ually decrease with the increasing of speaker numbers.
This phenomenon is consistent with our intuition. Gener-
ally the more participants are involved in a conversation,
the more discussions can take place. Human annotators
feel more ambiguity in selecting summary sentences for
the discussion part. The pattern does not hold for other
speaker numbers, namely, 2, 3, and 8. This might be due
to a lack of enough data points, and we will further ana-
lyze this in the future research.
</bodyText>
<table confidence="0.994534875">
# of speakers # of topics Avg Kappa score
2 2 0.204
3 6 0.182
4 26 0.29
5 26 0.249
6 33 0.226
7 19 0.221
8 7 0.3
</table>
<tableCaption confidence="0.9981415">
Table 2: Average Kappa score with respect to the number of
speakers after removing short topics.
</tableCaption>
<subsectionHeader confidence="0.991316">
3.3 ROUGE Score
</subsectionHeader>
<bodyText confidence="0.999884571428572">
ROUGE (Lin and Hovy, 2003) has been adopted as
a standard evaluation metric in various summarization
tasks. It is computed based on the n-gram overlap be-
tween a summary and a set of reference summaries.
Though the Kappa statistics can measure human agree-
ment on sentence selection, it does not account for the
fact that different annotators choose different sentences
</bodyText>
<figure confidence="0.998338333333333">
Kappa score
-0.
0.8
0.6
0.4
0.2
1.2
0
2
1
0 200 400 600 800 1000 1200 1400
Topic length
</figure>
<page confidence="0.989785">
81
</page>
<bodyText confidence="0.99957375">
that are similar in content. ROUGE measures the word
match and thus can compensate this problem of Kappa.
Table 3 shows the ROUGE-2 and ROUGE-SU4 F-
measure results. For each annotator, we computed
ROUGE scores using other annotators’ summaries as ref-
erences. For Data Set (I), we present results for each an-
notator, since one of our goals is to evaluate the qual-
ity of different annotator’s summary annotation. The low
ROUGE scores suggest the large variation among human
annotations. We can see from the table that annotator
1 has the lowest ROUGE score and thus lowest agree-
ment with the other two annotators in Data Set (I). The
ROUGE score for Data Set (III) is higher than the others.
This is consistent with the result using Kappa statistic:
the more sentences two summaries have in common, the
more overlapped n-grams they tend to share.
</bodyText>
<table confidence="0.998657333333333">
ROUGE-2 ROUGE-SU4
data (I) Annotator 1 0.407 0.457
Annotator 2 0.421 0.471
Annotator 3 0.433 0.483
data (III) 2 annotators 0.532 0.564
data (IV) 3 annotators 0.447 0.484
</table>
<tableCaption confidence="0.998853">
Table 3: ROUGE F-measure scores for different data sets.
</tableCaption>
<subsectionHeader confidence="0.996625">
3.4 Sentence Distance and Divergence Scores
</subsectionHeader>
<bodyText confidence="0.99992078125">
From the annotation, we notice that the summary sen-
tences are not uniformly distributed in the transcript, but
rather with a clustering or coherence property. However,
neither Kappa coefficient nor ROUGE score can rep-
resent such clustering tendency of meeting summaries.
This paper attempts to develop an evaluation metric to
measure this property among different human annotators.
For a sentence i selected by one annotator, we define a
distance score di to measure its minimal distance to sum-
mary sentences selected by other annotators (distance be-
tween two sentences is represented using the difference
of their sentence indexes). di is 0 if more than one anno-
tator have extracted the same sentence as summary sen-
tence. Using the annotated summaries for the 27 meet-
ings in Data Set (I), we computed the sentence distance
scores for each annotator. Figure 2 shows the distribution
of the distance score for the 3 annotators. We can see
that the distance score distributions for the three annota-
tors differ. Intuitively, small distance scores mean better
coherence and more consistency with other annotators’
results. We thus propose a mechanism to quantify each
annotator’s summary annotation by using a random vari-
able (RV) to represent an annotator’s sentence distance
scores.
When all the annotators agree with each other, the RV
d will take a value of 0 with probability 1. In general,
when the annotators select sentences close to each other,
the RV d will have small values with high probabilities.
Therefore we create a probability distribution Q for the
ideal situation where the annotators have high agreement,
and use this to quantify the quality of each annotation. Q
is defined as:
</bodyText>
<figureCaption confidence="0.970169">
Figure 2: Percentage distribution of the summary sentence dis-
tance scores for the 3 annotators in Data Set (I).
</figureCaption>
<equation confidence="0.9863628">
I (dmax − i + 1) × q i =60
1 − Edmax
i�1 Q(i)
= 1 − dmaxx(dmax+1) × q i = 0
2
</equation>
<bodyText confidence="0.9420227">
where dmax denotes the maximum distance score based
on the selected summary sentences from all the annota-
tors. We assign linearly decreasing probabilities Q(i) for
different distance values i (i &gt; 0) in order to give more
credit to sentences with small distance scores. The rest
of the probability mass is given to Q(0). The parame-
ter q is small, such that the probability distribution Q can
approximate the ideal situation.
For each annotator, the probability distribution P is de-
fined as:
</bodyText>
<equation confidence="0.948037666666667">
� wi x fi f, ∈ Dp
P(i) = Ei wixf:
0 otherwise
</equation>
<bodyText confidence="0.999831333333333">
where Dp is the set of the possible distance values for this
annotator, fi is the frequency for a distance score i, and
wi is the weight assigned to that distance (wi is i when
i =6 0; w0 is p). We use parameter p to vary the weighting
scale for the distance scores in order to penalize more for
the large distance values.
Using the distribution P for each annotator and the
ideal distribution Q, we compute their KL-divergence,
called the Divergence Distance score (DD-score):
</bodyText>
<equation confidence="0.999729">
P (i)
P(i) log Q(i)
</equation>
<bodyText confidence="0.999933055555556">
We expect that the smaller the score is, the better the sum-
mary is. In the extreme case, if an annotator’s DD-score
is equal to 0, it means that all of this annotator’s extracted
sentences are selected by other annotators.
Figure 3 shows the DD-score for each annotator cal-
culated using Data Set (I), with varying q parameters.
Our experiments showed that the scale parameter p in the
annotator’s probability distribution only affects the abso-
lute value of the DD-score for the annotators, but does
not change the ranking of each annotator. Therefore we
simply set p = 10 when reporting DD-scores. Figure 3
shows that different weight scale q does not impact the
ranking of the annotators either. We observe in Figure 3,
annotator 1 has the highest DD score to the desirable dis-
tribution. We found this is consistent with the cumulative
distance score obtained from the distance score distribu-
tion, where annotator 1 has the least cumulative frequen-
cies for all the distance values greater than 0. This is
</bodyText>
<figure confidence="0.997656642857143">
Percentage
0.45
0.35
0.25
0.15
0.05
0.5
0.4
0.3
0.2
0.1
0
0 1 2 3 4 5 6 7 8 9 10 &gt;10
Distance Score
</figure>
<bodyText confidence="0.508588">
Annotator 1 Annotator 2 Annotator 3
</bodyText>
<equation confidence="0.740694666666667">
Q(i) =
DD =�
i
</equation>
<page confidence="0.978947">
82
</page>
<bodyText confidence="0.97099175">
also consistent with the ROUGE scores, where annotator
1 has the lowest ROUGE score. These suggest that the
DD-score can be used to quantify the consistency of an
annotator with others.
</bodyText>
<figureCaption confidence="0.988862">
Figure 3: Divergence distance score when varying parameter q
in the ideal distribution Q.
</figureCaption>
<bodyText confidence="0.999971722222222">
We also investigated using the sentence distance scores
to improve the human annotation quality. Our hypothe-
sis is that those selected summary sentences with high
distance scores do not contain crucial information of
the meeting content and thus can be removed from the
reference summary. To verify this, for each annota-
tor, we removed the summary sentences with distance
scores greater than some threshold, and then computed
the ROUGE score for the newly generated summary by
comparing to other two summary annotations that are
kept unchanged. The ROUGE-2 scores when varying the
threshold is shown in Figure 4. No threshold in the X-
axis means that no sentence is taken out from the human
summary. We can see from the figure that the removal
of sentences with high distance scores can result in even
better F-measure scores. This suggests that we can delete
the incoherent human selected sentences while maintain-
ing the content information in the summary.
</bodyText>
<figureCaption confidence="0.952958">
Figure 4: ROUGE-2 score after removing summary sentences
with a distance score greater than a threshold.
</figureCaption>
<sectionHeader confidence="0.999204" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9993855">
In this paper we conducted an analysis about human an-
notated extractive summaries using a subset of the ICSI
meeting corpus. Different measurements have been used
to examine interannotator agreement, including Kappa
coefficient, which requires exact same sentence selection;
ROUGE, which measures the content similarity using n-
gram match; and our proposed sentence distance scores
and divergence, which evaluate the annotation consis-
tency based on the sentence position. We find that the
topic length does not have an impact on the human agree-
ment using Kappa, but the number of speakers seems to
be correlated with the agreement. The ROUGE score and
the divergence distance scores show some consistency
in terms of evaluating human annotation agreement. In
addition, using the sentence distance score, we demon-
strated that we can remove some poorly chosen sentences
from the summary to improve human annotation agree-
ment and preserve the information in the summary. In
our future work, we will explore other factors, such as
summary length, and the speaker information for the se-
lect summaries. We will also use a bigger data set for a
more reliable conclusion.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99983325">
The authors thank University of Edinburgh for sharing the an-
notation on the ICSI meeting corpus. This research is supported
by NSF award IIS-0714132. The views in this paper are those
of the authors and do not represent the funding agencies.
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999629925">
A. H. Buist, W. Kraaij, and S. Raaijmakers. 2005. Automatic
summarization of meeting data: A feasibility study. In Proc.
of the 15th CLIN conference.
J. Carletta. 1996. Assessing agreement on classification tasks:
the kappa statistic. Computational Linguistics, 22(2):249–
254.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Proc. of
EMNLP, pages 364–372.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation methods for
automatic speech summarization. In Proc. of Eurospeech,
pages 2825–2828.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan,
B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.
2003. The ICSI meeting corpus. In Proc. ofICASSP.
C. Y. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Proc. of
HLT–NAACL.
I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim. 2002. Summac: a text summarization eval-
uation. Natural Language Engineering, 8:43–68.
S. Maskey and J. Hirschberg. 2003. Automatic summariza-
tion of broadcast news using structural features. In Proc. of
EUROSPEECH, pages 1173–1176.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Evalu-
ating automatic summaries of meeting recordings. In Proc.
of the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation.
A. Nenkova and R. Passonneau. 2004. Evaluating content se-
lection in summarization: The pyramid method. In Proc. of
HLT-NAACL.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The ICSI meeting recorder dialog act (MRDA) corpus. In
Proc. of 5th SIGDial Workshop, pages 97–100.
S. Xie and Y. Liu. 2008. Using corpus and knowledge-based
similarity measure in maximum marginal relevance for meet-
ing summarization. In Proc. ofICASSP.
J. Zhang, H. Chan, P. Fung, and L. Cao. 2007. A compara-
tive study on speech summarization of broadcast news and
lecture speech. In Proc. ofInterspeech.
</reference>
<figure confidence="0.971068625">
Divergence
Distance Score
ce sta
co
25
20
sta
15
ce
10
5
0
7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
-log(q)
Annotator 1
Annotator 2
Annotator 1
Annotator 3
Annotator 2
F-score
0.46
0.45
0.44
0.43
0.42
0.41
0.39
0.38
0.4
3 4 5 6 7 8 no threshold
Threshold of Distance Score
Annotator 1 Annotator 2 Annotator 3
</figure>
<page confidence="0.987163">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.350152">
<title confidence="0.9909035">What Are Meeting Summaries? An Analysis of Human Summaries in Meeting Corpus</title>
<author confidence="0.9117575">Fei Liu</author>
<author confidence="0.9117575">Yang Erik Jonsson School of Engineering</author>
<author confidence="0.9117575">Computer</author>
<affiliation confidence="0.7007055">The University of Texas at Richardson, TX,</affiliation>
<abstract confidence="0.998930666666667">Significant research efforts have been devoted to speech summarization, including automatic approaches and evaluation metrics. However, a fundamental problem about what summaries are for the speech data and whether humans agree with each other remains unclear. This paper performs an analysis of human annotated extractive summaries using the ICSI meeting corpus with an aim to examine their consistency and the factors impacting human agreement. In addition to using Kappa statistics and ROUGE scores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A H Buist</author>
<author>W Kraaij</author>
<author>S Raaijmakers</author>
</authors>
<title>Automatic summarization of meeting data: A feasibility study.</title>
<date>2005</date>
<booktitle>In Proc. of the 15th CLIN conference.</booktitle>
<contexts>
<context position="1352" citStr="Buist et al., 2005" startWordPosition="197" endWordPosition="200">tistics and ROUGE scores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been </context>
</contexts>
<marker>Buist, Kraaij, Raaijmakers, 2005</marker>
<rawString>A. H. Buist, W. Kraaij, and S. Raaijmakers. 2005. Automatic summarization of meeting data: A feasibility study. In Proc. of the 15th CLIN conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>254</pages>
<contexts>
<context position="5229" citStr="Carletta, 1996" startWordPosition="824" endWordPosition="825"> summary sentences and words. We refer to the above 27 meetings Data set I in this paper. In addition, some of our studies are performed based on the 6 meeting used in (Murray et al., 2005), for which we have human annotated summaries using 3 different guidelines: • Data set II: summary annotated on a topic basis. This is a subset of the 27 annotated meetings above. • Data set III: annotation is done for the entire meeting without topic segments. • Data set IV: the extractive summaries are from the AMI annotation (Murray et al., 2005). 3 Analysis Results 3.1 Kappa Statistic Kappa coefficient (Carletta, 1996) is commonly used as a standard to reflect inter-annotator agreement. Table 1 shows the average Kappa results, calculated for each meeting using the data sets described in Section 2. Compared to Kappa score on text summarization, which is reported to be 0.38 by (Mani et al., 2002) on a set of TREC documents, the inter-annotator agreement on meeting corpus is lower. This is likely due to the difference between the meeting style and written text. Data Set I II III IV Avg-Kappa 0.261 0.245 0.335 0.290 Table 1: Average Kappa scores on different data sets. There are several other observations from </context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22(2):249– 254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>364--372</pages>
<contexts>
<context position="1366" citStr="Galley, 2006" startWordPosition="201" endWordPosition="202">ores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the ab</context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>M. Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proc. of EMNLP, pages 364–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hori</author>
<author>T Hori</author>
<author>S Furui</author>
</authors>
<title>Evaluation methods for automatic speech summarization.</title>
<date>2003</date>
<booktitle>In Proc. of Eurospeech,</booktitle>
<pages>2825--2828</pages>
<contexts>
<context position="1332" citStr="Hori et al., 2003" startWordPosition="193" endWordPosition="196"> to using Kappa statistics and ROUGE scores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Differen</context>
</contexts>
<marker>Hori, Hori, Furui, 2003</marker>
<rawString>C. Hori, T. Hori, and S. Furui. 2003. Evaluation methods for automatic speech summarization. In Proc. of Eurospeech, pages 2825–2828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proc. ofICASSP.</booktitle>
<contexts>
<context position="3498" citStr="Janin et al., 2003" startWordPosition="536" endWordPosition="539">ate the variation of human extractive summaries, and help to better understand the gold standard reference summaries for meeting summarization. This paper aims to answer two key questions: (1) How much variation is there in human extractive meeting summaries? (2) What are the factors that may impact interannotator agreement? We use three different metrics to evaluate the variation among human summaries, including Kappa statistic, ROUGE score, and a new proposed divergence distance score to reflect the coherence and quality of an annotation. 2 Corpus Description We use the ICSI meeting corpus (Janin et al., 2003) which contains 75 naturally-occurred meetings, each about an hour long. All of them have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and abstractive and extractive summaries in the AMI project (Murray et al., 2005). We selected 27 meetings from this corpus. Three annotators (undergraduate students) were recruited to extract summary sentences on a topic basis using the topic segments from the AMI annotation. Each sentence corresponds to one DA annotated in the corpus. The annotators were told to use their own judgment to pick summary sentences that are</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In Proc. ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proc. of HLT–NAACL.</booktitle>
<contexts>
<context position="2115" citStr="Lin and Hovy, 2003" startWordPosition="318" endWordPosition="321">es, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization research to compare system generated summaries with human annotation, such as Fmeasure, ROUGE, Pyramid, sumACCY (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hori et al., 2003). Typically multiple reference human summaries are used 80 in evaluation in order to account for the inconsistency among human annotations. While there have been efforts on speech summarization approaches and evaluation, some fundamental problems are still unclear. For example, what are speech summaries? Do humans agree with each other on summary extraction? In this paper, we focus on the meeting domain, one of the most challenging speech genre, to analyze human summary annotation. Meetings often have several participants. Its speech is spontan</context>
<context position="9051" citStr="Lin and Hovy, 2003" startWordPosition="1485" endWordPosition="1488"> the more participants are involved in a conversation, the more discussions can take place. Human annotators feel more ambiguity in selecting summary sentences for the discussion part. The pattern does not hold for other speaker numbers, namely, 2, 3, and 8. This might be due to a lack of enough data points, and we will further analyze this in the future research. # of speakers # of topics Avg Kappa score 2 2 0.204 3 6 0.182 4 26 0.29 5 26 0.249 6 33 0.226 7 19 0.221 8 7 0.3 Table 2: Average Kappa score with respect to the number of speakers after removing short topics. 3.3 ROUGE Score ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. It is computed based on the n-gram overlap between a summary and a set of reference summaries. Though the Kappa statistics can measure human agreement on sentence selection, it does not account for the fact that different annotators choose different sentences Kappa score -0. 0.8 0.6 0.4 0.2 1.2 0 2 1 0 200 400 600 800 1000 1200 1400 Topic length 81 that are similar in content. ROUGE measures the word match and thus can compensate this problem of Kappa. Table 3 shows the ROUGE-2 and ROUGE-SU4 Fmeasure results. For</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C. Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proc. of HLT–NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>G Klein</author>
<author>D House</author>
<author>L Hirschman</author>
<author>T Firmin</author>
<author>B Sundheim</author>
</authors>
<title>Summac: a text summarization evaluation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<pages>8--43</pages>
<contexts>
<context position="5510" citStr="Mani et al., 2002" startWordPosition="870" endWordPosition="873">: summary annotated on a topic basis. This is a subset of the 27 annotated meetings above. • Data set III: annotation is done for the entire meeting without topic segments. • Data set IV: the extractive summaries are from the AMI annotation (Murray et al., 2005). 3 Analysis Results 3.1 Kappa Statistic Kappa coefficient (Carletta, 1996) is commonly used as a standard to reflect inter-annotator agreement. Table 1 shows the average Kappa results, calculated for each meeting using the data sets described in Section 2. Compared to Kappa score on text summarization, which is reported to be 0.38 by (Mani et al., 2002) on a set of TREC documents, the inter-annotator agreement on meeting corpus is lower. This is likely due to the difference between the meeting style and written text. Data Set I II III IV Avg-Kappa 0.261 0.245 0.335 0.290 Table 1: Average Kappa scores on different data sets. There are several other observations from Table 1. First, comparing the results for Data Set (II) and (III), both containing six meetings, the agreement is higher for Data Set (III). Originally, we expected that by dividing the transcript into several topics, human subjects can focus better on each topic discussed during </context>
</contexts>
<marker>Mani, Klein, House, Hirschman, Firmin, Sundheim, 2002</marker>
<rawString>I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and B. Sundheim. 2002. Summac: a text summarization evaluation. Natural Language Engineering, 8:43–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maskey</author>
<author>J Hirschberg</author>
</authors>
<title>Automatic summarization of broadcast news using structural features.</title>
<date>2003</date>
<booktitle>In Proc. of EUROSPEECH,</booktitle>
<pages>1173--1176</pages>
<contexts>
<context position="1313" citStr="Maskey and Hirschberg, 2003" startWordPosition="189" endWordPosition="192"> human agreement. In addition to using Kappa statistics and ROUGE scores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluati</context>
</contexts>
<marker>Maskey, Hirschberg, 2003</marker>
<rawString>S. Maskey and J. Hirschberg. 2003. Automatic summarization of broadcast news using structural features. In Proc. of EUROSPEECH, pages 1173–1176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
<author>J Moore</author>
</authors>
<title>Evaluating automatic summaries of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation.</booktitle>
<contexts>
<context position="1387" citStr="Murray et al., 2005" startWordPosition="203" endWordPosition="206">proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization res</context>
<context position="3755" citStr="Murray et al., 2005" startWordPosition="578" endWordPosition="581">es? (2) What are the factors that may impact interannotator agreement? We use three different metrics to evaluate the variation among human summaries, including Kappa statistic, ROUGE score, and a new proposed divergence distance score to reflect the coherence and quality of an annotation. 2 Corpus Description We use the ICSI meeting corpus (Janin et al., 2003) which contains 75 naturally-occurred meetings, each about an hour long. All of them have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and abstractive and extractive summaries in the AMI project (Murray et al., 2005). We selected 27 meetings from this corpus. Three annotators (undergraduate students) were recruited to extract summary sentences on a topic basis using the topic segments from the AMI annotation. Each sentence corresponds to one DA annotated in the corpus. The annotators were told to use their own judgment to pick summary sentences that are informative and can preserve discussion flow. The recommended percentages for the selected summary sentences and words were set to 8.0% and 16.0% respectively. Human subjects were provided with both the meeting audio files and an annotation GraphiProceedin</context>
<context position="5154" citStr="Murray et al., 2005" startWordPosition="812" endWordPosition="815">n browse the manual transcripts and see the percentage of the currently selected summary sentences and words. We refer to the above 27 meetings Data set I in this paper. In addition, some of our studies are performed based on the 6 meeting used in (Murray et al., 2005), for which we have human annotated summaries using 3 different guidelines: • Data set II: summary annotated on a topic basis. This is a subset of the 27 annotated meetings above. • Data set III: annotation is done for the entire meeting without topic segments. • Data set IV: the extractive summaries are from the AMI annotation (Murray et al., 2005). 3 Analysis Results 3.1 Kappa Statistic Kappa coefficient (Carletta, 1996) is commonly used as a standard to reflect inter-annotator agreement. Table 1 shows the average Kappa results, calculated for each meeting using the data sets described in Section 2. Compared to Kappa score on text summarization, which is reported to be 0.38 by (Mani et al., 2002) on a set of TREC documents, the inter-annotator agreement on meeting corpus is lower. This is likely due to the difference between the meeting style and written text. Data Set I II III IV Avg-Kappa 0.261 0.245 0.335 0.290 Table 1: Average Kapp</context>
</contexts>
<marker>Murray, Renals, Carletta, Moore, 2005</marker>
<rawString>G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Evaluating automatic summaries of meeting recordings. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="2145" citStr="Nenkova and Passonneau, 2004" startWordPosition="322" endWordPosition="325"> these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization research to compare system generated summaries with human annotation, such as Fmeasure, ROUGE, Pyramid, sumACCY (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hori et al., 2003). Typically multiple reference human summaries are used 80 in evaluation in order to account for the inconsistency among human annotations. While there have been efforts on speech summarization approaches and evaluation, some fundamental problems are still unclear. For example, what are speech summaries? Do humans agree with each other on summary extraction? In this paper, we focus on the meeting domain, one of the most challenging speech genre, to analyze human summary annotation. Meetings often have several participants. Its speech is spontaneous, contains disfluencies, a</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>J Ang</author>
<author>H Carvey</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus.</title>
<date>2004</date>
<booktitle>In Proc. of 5th SIGDial Workshop,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="3664" citStr="Shriberg et al., 2004" startWordPosition="563" endWordPosition="566">answer two key questions: (1) How much variation is there in human extractive meeting summaries? (2) What are the factors that may impact interannotator agreement? We use three different metrics to evaluate the variation among human summaries, including Kappa statistic, ROUGE score, and a new proposed divergence distance score to reflect the coherence and quality of an annotation. 2 Corpus Description We use the ICSI meeting corpus (Janin et al., 2003) which contains 75 naturally-occurred meetings, each about an hour long. All of them have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and abstractive and extractive summaries in the AMI project (Murray et al., 2005). We selected 27 meetings from this corpus. Three annotators (undergraduate students) were recruited to extract summary sentences on a topic basis using the topic segments from the AMI annotation. Each sentence corresponds to one DA annotated in the corpus. The annotators were told to use their own judgment to pick summary sentences that are informative and can preserve discussion flow. The recommended percentages for the selected summary sentences and words were set to 8.0% and 16.0% respectively. Human</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In Proc. of 5th SIGDial Workshop, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Xie</author>
<author>Y Liu</author>
</authors>
<title>Using corpus and knowledge-based similarity measure in maximum marginal relevance for meeting summarization. In</title>
<date>2008</date>
<booktitle>Proc. ofICASSP.</booktitle>
<contexts>
<context position="1427" citStr="Xie and Liu, 2008" startWordPosition="211" endWordPosition="214">vergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization research to compare system generated summar</context>
</contexts>
<marker>Xie, Liu, 2008</marker>
<rawString>S. Xie and Y. Liu. 2008. Using corpus and knowledge-based similarity measure in maximum marginal relevance for meeting summarization. In Proc. ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhang</author>
<author>H Chan</author>
<author>P Fung</author>
<author>L Cao</author>
</authors>
<title>A comparative study on speech summarization of broadcast news and lecture speech.</title>
<date>2007</date>
<booktitle>In Proc. ofInterspeech.</booktitle>
<contexts>
<context position="1407" citStr="Zhang et al., 2007" startWordPosition="207" endWordPosition="210">istance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization research to compare sys</context>
</contexts>
<marker>Zhang, Chan, Fung, Cao, 2007</marker>
<rawString>J. Zhang, H. Chan, P. Fung, and L. Cao. 2007. A comparative study on speech summarization of broadcast news and lecture speech. In Proc. ofInterspeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>