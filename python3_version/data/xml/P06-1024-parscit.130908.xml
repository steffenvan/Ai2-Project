<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004584">
<title confidence="0.9979485">
Learning More Effective Dialogue Strategies Using Limited Dialogue
Move Features
</title>
<author confidence="0.972014">
Matthew Frampton and Oliver Lemon
</author>
<affiliation confidence="0.9673885">
HCRC, School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.984094">
Edinburgh, EH8 9LW, UK
</address>
<email confidence="0.998471">
M.J.E.Frampton@sms.ed.ac.uk, olemon@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999773125">
We explore the use of restricted dialogue
contexts in reinforcement learning (RL)
of effective dialogue strategies for infor-
mation seeking spoken dialogue systems
(e.g. COMMUNICATOR (Walker et al.,
2001)). The contexts we use are richer
than previous research in this area, e.g.
(Levin and Pieraccini, 1997; Scheffler and
Young, 2001; Singh et al., 2002; Pietquin,
2004), which use only slot-based infor-
mation, but are much less complex than
the full dialogue “Information States” ex-
plored in (Henderson et al., 2005), for
which tractabe learning is an issue. We
explore how incrementally adding richer
features allows learning of more effective
dialogue strategies. We use 2 user simu-
lations learned from COMMUNICATOR
data (Walker et al., 2001; Georgila et al.,
2005b) to explore the effects of differ-
ent features on learned dialogue strategies.
Our results show that adding the dialogue
moves of the last system and user turns
increases the average reward of the auto-
matically learned strategies by 65.9% over
the original (hand-coded) COMMUNI-
CATOR systems, and by 7.8% over a base-
line RL policy that uses only slot-status
features. We show that the learned strate-
gies exhibit an emergent “focus switch-
ing” strategy and effective use of the ‘give
help’ action.
</bodyText>
<sectionHeader confidence="0.999632" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995958125">
Reinforcement Learning (RL) applied to the prob-
lem of dialogue management attempts to find op-
timal mappings from dialogue contexts to sys-
tem actions. The idea of using Markov Deci-
sion Processes (MDPs) and reinforcement learn-
ing to design dialogue strategies for dialogue sys-
tems was first proposed by (Levin and Pierac-
cini, 1997). There, and in subsequent work such
as (Singh et al., 2002; Pietquin, 2004; Scheffler
and Young, 2001), only very limited state infor-
mation was used in strategy learning, based al-
ways on the number and status of filled informa-
tion slots in the application (e.g. departure-city is
filled, destination-city is unfilled). This we refer to
as low-level contextual information. Much prior
work (Singh et al., 2002) concentrated only on
specific strategy decisions (e.g. confirmation and
initiative strategies), rather than the full problem
of what system dialogue move to take next.
The simple strategies learned for low-level def-
initions of state cannot be sensitive to (sometimes
critical) aspects of the dialogue context, such as
the user’s last dialogue move (DM) (e.g. request-
help) unless that move directly affects the status of
an information slot (e.g. provide-info(destination-
city)). We refer to additional contextual infor-
mation such as the system and user’s last di-
alogue moves as high-level contextual informa-
tion. (Frampton and Lemon, 2005) learned full
strategies with limited ‘high-level’ information
(i.e. the dialogue move(s) of the last user utter-
ance) and only used a stochastic user simulation
whose probabilities were supplied via common-
sense and intuition, rather than learned from data.
This paper uses data-driven n-gram user simula-
tions (Georgila et al., 2005a) and a richer dialogue
context.
On the other hand, increasing the size of the
state space for RL has the danger of making
the learning problem intractable, and at the very
least means that data is more sparse and state ap-
proximation methods may need to be used (Hen-
derson et al., 2005). To date, the use of very
large state spaces relies on a “hybrid” super-
vised/reinforcement learning technique, where the
reinforcement learning element has not yet been
shown to significantly improve policies over the
purely supervised case (Henderson et al., 2005).
</bodyText>
<page confidence="0.982665">
185
</page>
<note confidence="0.5432505">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185–192,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.997046666666667">
The extended state spaces that we propose are
based on theories of dialogue such as (Clark, 1996;
Searle, 1969; Austin, 1962; Larsson and Traum,
2000), where which actions a dialogue participant
can or should take next are not based solely on
the task-state (i.e. in our domain, which slots are
filled), but also on wider contextual factors such
as a user’s dialogue moves or speech acts. In
future work we also intend to use feature selec-
tion techniques (e.g. correlation-based feature sub-
set (CFS) evaluation (Rieser and Lemon, 2006))
on the COMMUNICATOR data (Georgila et al.,
2005a; Walker et al., 2001) in order to identify ad-
ditional context features that it may be effective to
represent in the state.
</bodyText>
<subsectionHeader confidence="0.99246">
1.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999882275">
To explore these issues we have developed a Re-
inforcement Learning (RL) program to learn di-
alogue strategies while accurate simulated users
(Georgila et al., 2005a) converse with a dialogue
manager. See (Singh et al., 2002; Scheffler and
Young, 2001) and (Sutton and Barto, 1998) for a
detailed description of Markov Decision Processes
and the relevant RL algorithms.
In dialogue management we are faced with the
problem of deciding which dialogue actions it is
best to perform in different states. We use (RL) be-
cause it is a method of learning by delayed reward
using trial-and-error search. These two proper-
ties appear to make RL techniques a good fit with
the problem of automatically optimising dialogue
strategies, because in task-oriented dialogue of-
ten the “reward” of the dialogue (e.g. successfully
booking a flight) is not obtainable immediately,
and the large space of possible dialogues for any
task makes some degree of trial-and-error explo-
ration necessary.
We use both 4-gram and 5-gram user sim-
ulations for testing and for training (i.e. train
with 4-gram, test with 5-gram, and vice-versa).
These simulations also simulate ASR errors since
the probabilities are learned from recognition hy-
potheses and system behaviour logged in the
COMMUNICATOR data (Walker et al., 2001) fur-
ther annotated with speech acts and contexts by
(Georgila et al., 2005b). Here the task domain is
flight-booking, and the aim for the dialogue man-
ager is to obtain values for the user’s flight infor-
mation “slots” i.e. departure city, destination city,
departure date and departure time, before making
a database query. We add the dialogue moves of
the last user and system turns as context features
and use these in strategy learning. We compare
the learned strategies to 2 baselines: the original
COMMUNICATOR systems and an RL strategy
which uses only slot status features.
</bodyText>
<subsectionHeader confidence="0.796779">
1.2 Outline
</subsectionHeader>
<bodyText confidence="0.999977666666667">
Section 2 contains a description of our basic ex-
perimental framework, and a detailed description
of the reinforcement learning component and user
simulations. Sections 3 and 4 describe the experi-
ments and analyse our results, and in section 5 we
conclude and suggest future work.
</bodyText>
<sectionHeader confidence="0.952001" genericHeader="method">
2 The Experimental Framework
</sectionHeader>
<bodyText confidence="0.998822333333333">
Each experiment is executed using the DIPPER
Information State Update dialogue manager (Bos
et al., 2003) (which here is used to track and up-
date dialogue context rather than deciding which
actions to take), a Reinforcement Learning pro-
gram (which determines the next dialogue action
to take), and various user simulations. In sections
2.3 and 2.4 we give more details about the rein-
forcement learner and user simulations.
</bodyText>
<subsectionHeader confidence="0.996031">
2.1 The action set for the learner
</subsectionHeader>
<bodyText confidence="0.999283333333333">
Below is a list of all the different actions that the
RL dialogue manager can take and must learn to
choose between based on the context:
</bodyText>
<listItem confidence="0.9993089">
1. An open question e.g. ‘How may I help you?’
2. Ask the value for any one of slots 1...n.
3. Explicitly confirm any one of slots 1...n.
4. Ask for the nth slot whilst implicitly confirm-
ing&apos; either slot value n − 1 e.g. ‘So you want
to fly from London to where?’, or slot value
n + 1
5. Give help.
6. Pass to human operator.
7. Database Query.
</listItem>
<bodyText confidence="0.9998134">
There are a couple of restrictions regarding
which actions can be taken in which states: an
open question is only available at the start of the
dialogue, and the dialogue manager can only try
to confirm non-empty slots.
</bodyText>
<subsectionHeader confidence="0.999757">
2.2 The Reward Function
</subsectionHeader>
<bodyText confidence="0.9996555">
We employ an “all-or-nothing” reward function
which is as follows:
</bodyText>
<listItem confidence="0.8469555">
1. Database query, all slots confirmed: +100
2. Any other database query: −75
</listItem>
<bodyText confidence="0.998029833333333">
&apos;Where n = 1 we implicitly confirm the final slot and
where n = 4 we implicitly confirm the first slot. This action
set does not include actions that ask the nth slot whilst im-
plicitly confirming slot value n − 2. These will be added in
future experiments as we continue to increase the action and
state space.
</bodyText>
<page confidence="0.996109">
186
</page>
<listItem confidence="0.993161666666667">
3. User simulation hangs-up: −100
4. DIPPER passes to a human operator: −50
5. Each system turn: −5
</listItem>
<bodyText confidence="0.999912785714286">
To maximise the chances of a slot value be-
ing correct, it must be confirmed rather than just
filled. The reward function reflects the fact that
a successful dialogue manager must maximise its
chances of getting the slots correct i.e. they must
all be confirmed. (Walker et al., 2000) showed
with the PARADISE evaluation that confirming
slots increases user satisfaction.
The maximum reward that can be obtained for
a single dialogue is 85, (the dialogue manager
prompts the user, the user replies by filling all four
of the slots in a single utterance, and the dialogue
manager then confirms all four slots and submits a
database query).
</bodyText>
<subsectionHeader confidence="0.998879">
2.3 The Reinforcement Learner’s Parameters
</subsectionHeader>
<bodyText confidence="0.975946666666667">
When the reinforcement learner agent is initial-
ized, it is given a parameter string which includes
the following:
</bodyText>
<listItem confidence="0.9986643">
1. Step Parameter: a = decreasing
2. Discount Factor: -y = 1
3. Action Selection Type = softmax (alternative
is E-greedy)
4. Action Selection Parameter: temperature =
15
5. Eligibility Trace Parameter: A = 0.9
6. Eligibility Trace = replacing (alternative is
accumulating)
7. Initial Q-values = 25
</listItem>
<bodyText confidence="0.999117928571429">
The reinforcement learner updates its Q-values
using the Sarsa(A) algorithm (see (Sutton and
Barto, 1998)). The first parameter is the step-
parameter a which may be a value between 0 and
1, or specified as decreasing. If it is decreasing, as
it is in our experiments, then for any given Q-value
update a is k where k is the number of times that
the state-action pair for which the update is be-
ing performed has been visited. This kind of step
parameter will ensure that given a sufficient num-
ber of training dialogues, each of the Q-values will
eventually converge. The second parameter (dis-
count factor) -y may take a value between 0 and 1.
For the dialogue management problem we set it to
1 so that future rewards are taken into account as
strongly as possible.
Apart from updating Q-values, the reinforce-
ment learner must also choose the next action
for the dialogue manager and the third parameter
specifies whether it does this by E-greedy or soft-
max action selection (here we have used softmax).
The fifth parameter, the eligibility trace param-
eter A, may take a value between 0 and 1, and the
sixth parameter specifies whether the eligibility
traces are replacing or accumulating. We used re-
placing traces because they produced faster learn-
ing for the slot-filling task. The seventh parameter
is for supplying the initial Q-values.
</bodyText>
<subsectionHeader confidence="0.999538">
2.4 N-Gram User Simulations
</subsectionHeader>
<bodyText confidence="0.999992395348837">
Here user simulations, rather than real users, inter-
act with the dialogue system during learning. This
is because thousands of dialogues may be neces-
sary to train even a simple system (here we train
on up to 50000 dialogues), and for a proper explo-
ration of the state-action space the system should
sometimes take actions that are not optimal for the
current situation, making it a sadistic and time-
consuming procedure for any human training the
system. (Eckert et al., 1997) were the first to
use a user simulation for this purpose, but it was
not goal-directed and so could produce inconsis-
tent utterances. The later simulations of (Pietquin,
2004) and (Scheffler and Young, 2001) were to
some extent “goal-directed” and also incorporated
an ASR error simulation. The user simulations in-
teract with the system via intentions. Intentions
are preferred because they are easier to generate
than word sequences and because they allow er-
ror modelling of all parts of the system, for exam-
ple ASR error modelling and semantic errors. The
user and ASR simulations must be realistic if the
learned strategy is to be directly applicable in a
real system.
The n-gram user simulations used here (see
(Georgila et al., 2005a) for details and evaluation
results) treat a dialogue as a sequence of pairs of
speech acts and tasks. They take as input the n −1
most recent speech act-task pairs in the dialogue
history, and based on n-gram probabilities learned
from the COMMUNICATOR data (automatically
annotated with speech acts and Information States
(Georgila et al., 2005b)), they then output a user
utterance as a further speech-act task pair. These
user simulations incorporate the effects of ASR er-
rors since they are built from the user utterances
as they were recognized by the ASR components
of the original COMMUNICATOR systems. Note
that the user simulations do not provide instanti-
ated slot values e.g. a response to provide a des-
tination city is the speech-act task pair “[provide
info] [dest city]”. We cannot assume that two such
responses in the same dialogue refer to the same
</bodyText>
<page confidence="0.989909">
187
</page>
<bodyText confidence="0.943844882352941">
destination cities. Hence in the dialogue man-
ager’s Information State where we record whether
a slot is empty, filled, or confirmed, we only up-
date from filled to confirmed when the slot value
is implicitly or explicitly confirmed. An additional
function maps the user speech-act task pairs to a
form that can be interpreted by the dialogue man-
ager. Post-mapping user responses are made up of
one or more of the following types of utterance:
(1) Stay quiet, (2) Provide 1 or more slot values,
(3) Yes, (4) No, (5) Ask for help, (6) Hang-up, (7)
Null (out-of-domain or no ASR hypothesis).
The quality of the 4 and 5-gram user sim-
ulations has been established through a variety
of metrics and against the behaviour of the ac-
tual users of the COMMUNICATOR systems, see
(Georgila et al., 2005a).
</bodyText>
<subsectionHeader confidence="0.732749">
2.4.1 Limitations of the user simulations
</subsectionHeader>
<bodyText confidence="0.999983642857143">
The user and ASR simulations are a fundamen-
tally important factor in determining the nature of
the learned strategies. For this reason we should
note the limitations of the n-gram simulations used
here. A first limitation is that we cannot be sure
that the COMMUNICATOR training data is suffi-
ciently complete, and a second is that the n-gram
simulations only use a window of n moves in
the dialogue history. This second limitation be-
comes a problem when the user simulation’s cur-
rent move ought to take into account something
that occurred at an earlier stage in the dialogue.
This might result in the user simulation repeating a
slot value unnecessarily, or the chance of an ASR
error for a particular word being independent of
whether the same word was previously recognised
correctly. The latter case means we cannot sim-
ulate for example, a particular slot value always
being liable to misrecognition. These limitations
will affect the nature of the learned strategies. Dif-
ferent state features may assume more or less im-
portance than they would if the simulations were
more realistic. This is a point that we will return to
in the analysis of the experimental results. In fu-
ture work we will use the more accurate user sim-
ulations recently developed following (Georgila et
al., 2005a) and we expect that these will improve
our results still further.
</bodyText>
<sectionHeader confidence="0.999862" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999058142857143">
First we learned strategies with the 4-gram user
simulation and tested with the 5-gram simula-
tion, and then did the reverse. We experimented
with different feature sets, exploring whether bet-
ter strategies could be learned by adding limited
context features. We used two baselines for com-
parison:
</bodyText>
<listItem confidence="0.963151">
• The performance of the original COMMUNI-
CATOR systems in the data set (Walker et al.,
2001).
• An RL baseline dialogue manager learned
</listItem>
<bodyText confidence="0.72510875">
using only slot-status features i.e. for each
of slots 1 − 4, is the slot empty, filled or con-
firmed?
We then learned two further strategies:
</bodyText>
<listItem confidence="0.8942164">
• Strategy 2 (UDM) was learned by adding the
user’s last dialogue move to the state.
• Strategy 3 (USDM) was learned by adding
both the user and system’s last dialogue
moves to the state.
</listItem>
<bodyText confidence="0.9999155">
The possible system and user dialogue moves
were those given in sections 2.1 and 2.4 respec-
tively, and the reward function was that described
in section 2.2.
</bodyText>
<subsectionHeader confidence="0.996688">
3.1 The COMMUNICATOR data baseline
</subsectionHeader>
<bodyText confidence="0.9999853">
We computed the scores for the original hand-
coded COMMUNICATOR systems as was done
by (Henderson et al., 2005), and we call this the
“HLG05” score. This scoring function is based
on task completion and dialogue length rewards as
determined by the PARADISE evaluation (Walker
et al., 2000). This function gives 25 points for
each slot which is filled, another 25 for each that
is confirmed, and deducts 1 point for each sys-
tem action. In this case the maximum possible
score is 197 i.e. 200 minus 3 actions, (the sys-
tem prompts the user, the user replies by filling all
four of the slots in one turn, and the system then
confirms all four slots and offers the flight). The
average score for the 1242 dialogues in the COM-
MUNICATOR dataset where the aim was to fill
and confirm only the same four slots as we have
used here was 115.26. The other COMMUNICA-
TOR dialogues involved different slots relating to
return flights, hotel-bookings and car-rentals.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.998469083333334">
Figure 1 tracks the improvement of the 3 learned
strategies for 50000 training dialogues with the 4-
gram user simulation, and figure 2 for 50000 train-
ing dialogues with the 5-gram simulation. They
show the average reward (according to the func-
tion of section 2.2) obtained by each strategy over
intervals of 1000 training dialogues.
Table 1 shows the results for testing the strate-
gies learned after 50000 training dialogues (the
baseline RL strategy, strategy 2 (UDM) and strat-
egy 3 (USDM)). The ‘a’ strategies were trained
with the 4-gram user simulation and tested with
</bodyText>
<page confidence="0.995502">
188
</page>
<table confidence="0.999690714285714">
Features Av. Score HLG05 Filled Slots Conf. Slots Length
4 → 5 gram = (a)
RL Baseline (a) Slots status 51.67 190.32 100 100 −9.68
RL Strat 2, UDM (a) + Last User DM 53.65** 190.67 100 100 −9.33
RL Strat 3, USDM (a) + Last System DM 54.9** 190.98 100 100 −9.02
5 → 4 gram = (b)
RL Baseline (b) Slots status 51.4 190.28 100 100 −9.72
RL Strat 2, UDM (b) + Last User DM 54.46* 190.83 100 100 −9.17
RL Strat 3, USDM (b) + Last System DM 56.24** 191.25 100 100 −8.75
RL Baseline (av) Slots status 51.54 190.3 100 100 −9.7
RL Strat 2, UDM (av) + Last User DM 54.06** 190.75 100 100 −9.25
RL Strat 3, USDM (av) + Last System DM 55.57** 191.16 100 100 −8.84
COMM Systems 115.26 84.6 63.7 −33.1
Hybrid RL *** Information States 142.6 88.1 70.9 −16.4
</table>
<tableCaption confidence="0.8316624">
Table 1: Testing the learned strategies after 50000 training dialogues, average reward achieved per dia-
logue over 1000 test dialogues. (a) = strategy trained using 4-gram and tested with 5-gram; (b) = strategy
trained with 5-gram and tested with 4-gram; (av) = average; * significance level p &lt; 0.025; ** signifi-
cance level p &lt; 0.005; *** Note: The Hybrid RL scores (here updated from (Henderson et al., 2005))
are not directly comparable since that system has a larger action set and fewer policy constraints.
</tableCaption>
<bodyText confidence="0.999987014705883">
the 5-gram, while the ‘b’ strategies were trained
with the 5-gram user simulation and tested with
the 4-gram. The table also shows average scores
for the strategies. Column 2 contains the average
reward obtained per dialogue by each strategy over
1000 test dialogues (computed using the function
of section 2.2).
The 1000 test dialogues for each strategy were
divided into 10 sets of 100. We carried out t-tests
and found that in both the ‘a’ and ‘b’ cases, strat-
egy 2 (UDM) performs significantly better than
the RL baseline (significance levels p &lt; 0.005
and p &lt; 0.025), and strategy 3 (USDM) performs
significantly better than strategy 2 (UDM) (signif-
icance level p &lt; 0.005). With respect to average
performance, strategy 2 (UDM) improves over the
RL baseline by 4.9%, and strategy 3 (USDM) im-
proves by 7.8%. Although there seem to be only
negligible qualitative differences between strate-
gies 2(b) and 3(b) and their ‘a’ equivalents, the
former perform slightly better in testing. This sug-
gests that the 4-gram simulation used for testing
the ‘b’ strategies is a little more reliable in filling
and confirming slot values than the 5-gram.
The 3rd column “HLG05” shows the average
scores for the dialogues as computed by the re-
ward function of (Henderson et al., 2005). This is
done for comparison with that work but also with
the COMMUNICATOR data baseline. Using the
HLG05 reward function, strategy 3 (USDM) im-
proves over the original COMMUNICATOR sys-
tems baseline by 65.9%. The components making
up the reward are shown in the final 3 columns
of table 1. Here we see that all of the RL strate-
gies are able to fill and confirm all of the 4 slots
when conversing with the simulated COMMUNI-
CATOR users. The only variation is in the aver-
age length of dialogue required to confirm all four
slots. The COMMUNICATOR systems were of-
ten unable to confirm or fill all of the user slots,
and the dialogues were quite long on average. As
stated in section 2.4.1, the n-gram simulations do
not simulate the case of a particular user goal ut-
terance being unrecognisable for the system. This
was a problem that could be encountered by the
real COMMUNICATOR systems.
Nevertheless, the performance of all the learned
strategies compares very well to the COMMUNI-
CATOR data baseline. For example, in an average
dialogue, the RL strategies filled and confirmed all
four slots with around 9 actions not including of-
fering the flight, but the COMMUNICATOR sys-
tems took an average of around 33 actions per di-
alogue, and often failed to complete the task.
With respect to the hybrid RL result of (Hen-
derson et al., 2005), shown in the final row of the
table, Strategy 3 (USDM) shows a 34% improve-
ment, though these results are not directly compa-
rable because that system uses a larger action set
and has fewer constraints (e.g. it can ask “how may
I help you?” at any time, not just at the start of a
dialogue).
Finally, let us note that the performance of the
RL strategies is close to optimal, but that there is
some room for improvement. With respect to the
HLG05 metric, the optimal system score would be
197, but this would only be available in rare cases
where the simulated user supplies all 4 slots in the
</bodyText>
<page confidence="0.991963">
189
</page>
<figure confidence="0.99753">
Training With 4-gram
0 5 10 15 20 25 30 35 40 45 50
Number of Dialogues (Thousands)
</figure>
<figureCaption confidence="0.965053">
Figure 1: Training the dialogue strategies with the
4-gram user simulation
</figureCaption>
<figure confidence="0.997385333333333">
Training With 5-gram
0 5 10 15 20 25 30 35 40 45 50
Number of Dialogues (Thousands)
</figure>
<figureCaption confidence="0.9107295">
Figure 2: Training the dialogue strategies with the
5-gram user simulation
</figureCaption>
<figure confidence="0.998755192307692">
Average Reward
-100
-120
-20
-40
-60
-80
40
20
0
Baseline
Strategy 2
Strategy 3
Average Reward
-100
-120
-20
-40
-60
-80
40
20
0
Baseline
Strategy 2
Strategy 3
</figure>
<bodyText confidence="0.998352">
first utterance. With respect to the metric we have
used here (with a −5 per system turn penalty), the
optimal score is 85 (and we currently score an av-
erage of 55.57). Thus we expect that there are
still further improvments that can be made to more
fully exploit the dialogue context (see section 4.3).
</bodyText>
<subsectionHeader confidence="0.999483">
4.1 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.903189">
Below are a list of general characteristics of the
learned strategies:
1. The reinforcement learner learns to query the
database only in states where all four slots
have been confirmed.
</bodyText>
<listItem confidence="0.967428222222222">
2. With sufficient exploration, the reinforce-
ment learner learns not to pass the call to a
human operator in any state.
3. The learned strategies employ implicit confir-
mations wherever possible. This allows them
to fill and confirm the slots in fewer turns than
if they simply asked the slot values and then
used explicit confirmation.
4. As a result of characteristic 3, which slots
</listItem>
<bodyText confidence="0.980186272727273">
can be asked and implicitly confirmed at the
same time influences the order in which the
learned strategies attempt to fill and confirm
each slot, e.g. if the status of the third slot is
‘filled’ and the others are ‘empty’, the learner
learns to ask for the second or fourth slot
rather than the first, since it can implicitly
confirm the third while it asks for the second
or fourth slots, but it cannot implicitly con-
firm the third while it asks for the first slot.
This action is not available (see section 2.1).
</bodyText>
<subsectionHeader confidence="0.999284">
4.2 Emergent behaviour
</subsectionHeader>
<bodyText confidence="0.999990666666667">
In testing the UDM strategy (2) filled and con-
firmed all of the slots in fewer turns on aver-
age than the RL baseline, and strategy 3 (USDM)
did this in fewer turns than strategy 2 (UDM).
What then were the qualitative differences be-
tween the three strategies? The behaviour of the
three strategies only seems to really deviate when
a user response fails to fill or confirm one or more
slots. Then the baseline strategy’s state has not
changed and so it will repeat its last dialogue
move, whereas the state for strategies 2 (UDM)
and 3 (USDM) has changed and as a result, these
may now try different actions. It is in such circum-
stances that the UDM strategy seems to be more
effective than the baseline, and strategy 3 (USDM)
more effective than the UDM strategy. In figure 3
we show illustrative state and learned action pairs
for the different strategies. They relate to a sit-
uation where the first user response(s) in the di-
alogue has/have failed to fill a single slot value.
NB: here ‘emp’ stands for ‘empty’ and ‘fill’ for
‘filled’ and they appear in the first four state vari-
ables, which stand for slot states. For strategy 2
(UDM), the fifth variable represents the user’s last
</bodyText>
<page confidence="0.993556">
190
</page>
<bodyText confidence="0.999684333333333">
dialogue move, and the for strategy 3 (USDM), the
fifth variable represents the system’s last dialogue
move, and the sixth, the user’s last dialogue move.
</bodyText>
<figure confidence="0.993625166666667">
BASELINE STRATEGY
State:
[emp,emp,emp,emp]
Action: askSlot2
STRATEGY 2 (UDM)
State:
[emp,emp,emp,emp,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,user(null)]
Action: askSlot1
STRATEGY 3 (USDM)
State:
[emp,emp,emp,emp,askSlot3,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,askSlot3,user(null)]
Action: giveHelp
State:
[emp,emp,emp,emp,giveHelp,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,giveHelp,user(null)]
Action: askSlot3
</figure>
<figureCaption confidence="0.683331">
Figure 3: Examples of the different learned strate-
gies and emergent behaviours: focus switching
(for UDM) and giving help (for USDM)
</figureCaption>
<bodyText confidence="0.9987156">
Here we can see that should the user responses
continue to fail to provide a slot value, the base-
line’s state will be unchanged and so the strategy
will simply ask for slot 2 again. The state for strat-
egy 2 (UDM) does change however. This strategy
switches focus between slots 3 and 1 depending on
whether the user’s last dialogue move was ‘null’ or
‘quiet’ NB. As stated in section 2.4, ‘null’ means
out-of-domain or that there was no ASR hypothe-
sis. Strategy 3 (USDM) is different again. Knowl-
edge of the system’s last dialogue move as well
as the user’s last move has enabled the learner to
make effective use of the ‘give help’ action, rather
than to rely on switching focus. When the user’s
last dialogue move is ‘null’ in response to the sys-
tem move ‘askSlot3’, then the strategy uses the
‘give help’ action before returning to ask for slot 3
again. The example described here is not the only
example of strategy 2 (UDM) employing focus
switching while strategy 3 (USDM) prefers to use
the ‘give help’ action when a user response fails
to fill or confirm a slot. This kind of behaviour in
strategies 2 and 3 is emergent dialogue behaviour
that has been learned by the system rather than ex-
plicitly programmed.
</bodyText>
<subsectionHeader confidence="0.9813475">
4.3 Further possibilities for improvement
over the RL baseline
</subsectionHeader>
<bodyText confidence="0.999990142857143">
Further improvements over the RL baseline might
be possible with a wider set of system actions.
Strategies 2 and 3 may learn to make more ef-
fective use of additional actions than the baseline
e.g. additional actions that implicitly confirm one
slot whilst asking another may allow more of the
switching focus described in section 4.1. Other
possible additional actions include actions that ask
for or confirm two or more slots simultaneously.
In section 2.4.1, we highlighted the fact that the
n-gram user simulations are not completely real-
istic and that this will make certain state features
more or less important in learning a strategy. Thus
had we been able to use even more realistic user
simulations, including certain additional context
features in the state might have enabled a greater
improvement over the baseline. Dialogue length
is an example of a feature that could have made
a difference had the simulations been able to sim-
ulate the case of a particular goal utterance being
unrecognisable for the system. The reinforcement
learner may then be able to use the dialogue length
feature to learn when to give up asking for a par-
ticular slot value and make a partially complete
database query. This would of course require a
reward function that gave some reward to partially
complete database queries rather than the all-or-
nothing reward function used here.
</bodyText>
<sectionHeader confidence="0.997485" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999983318181818">
We have used user simulations that are n-gram
models learned from COMMUNICATOR data to
explore reinforcement learning of full dialogue
strategies with some “high-level” context infor-
mation (the user and and system’s last dialogue
moves). Almost all previous work (e.g. (Singh
et al., 2002; Pietquin, 2004; Scheffler and Young,
2001)) has included only low-level information
in state representations. In contrast, the explo-
ration of very large state spaces to date relies on a
“hybrid” supervised/reinforcement learning tech-
nique, where the reinforcement learning element
has not been shown to significantly improve poli-
cies over the purely supervised case (Henderson et
al., 2005).
We presented our experimental environment,
the reinforcement learner, the simulated users,
and our methodology. In testing with the sim-
ulated COMMUNICATOR users, the new strate-
gies learned with higher-level (i.e. dialogue move)
information in the state outperformed the low-
level RL baseline (only slot status information)
</bodyText>
<page confidence="0.995596">
191
</page>
<bodyText confidence="0.99998896875">
by 7.8% and the original COMMUNICATOR sys-
tems by 65.9%. These strategies obtained more
reward than the RL baseline by filling and con-
firming all of the slots with fewer system turns on
average. Moreover, the learned strategies show
interesting emergent dialogue behaviour such as
making effective use of the ‘give help’ action and
switching focus to different subtasks when the cur-
rent subtask is proving problematic.
In future work, we plan to use even more realis-
tic user simulations, for example those developed
following (Georgila et al., 2005a), which incorpo-
rate elements of goal-directed user behaviour. We
will continue to investigate whether we can main-
tain tractability and learn superior strategies as we
add incrementally more high-level contextual in-
formation to the state. At some stage this may
necessitate using a generalisation method such as
linear function approximation (Henderson et al.,
2005). We also intend to use feature selection
techniques (e.g. CFS subset evaluation (Rieser and
Lemon, 2006)) on in order to determine which
contextual features this suggests are important.
We will also carry out a more direct comparison
with the hybrid strategies learned by (Henderson
et al., 2005). In the slightly longer term, we will
test our learned strategies on humans using a full
spoken dialogue system. We hypothesize that the
strategies which perform the best in terms of task
completion and user satisfaction scores (Walker et
al., 2000) will be those learned with high-level di-
alogue context information in the state.
</bodyText>
<sectionHeader confidence="0.996883" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.946195">
This work is supported by the ESRC and the TALK
project, www.talk-project.org.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871111111111">
John L. Austin. 1962. How To Do Things With Words.
Oxford University Press.
Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi
Oka. 2003. Dipper: Description and formalisation
of an information-state update dialogue system ar-
chitecture. In 4th SIGdial Workshop on Discourse
and Dialogue, Sapporo.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Weiland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system
evaluation. In IEEE Workshop on Automatic Speech
Recognition and Understanding.
Matthew Frampton and Oliver Lemon. 2005. Rein-
forcement Learning Of Dialogue Strategies Using
The User’s Last Dialogue Act. In IJCAI workshop
on Knowledge and Reasoning in Practical Dialogue
Systems.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005a. Learning User Simulations for In-
formation State Update Dialogue Systems. In In-
terspeech/Eurospeech: the 9th biennial conference
of the International Speech Communication Associ-
ation.
Kallirroi Georgila, Oliver Lemon, and James Hender-
son. 2005b. Automatic annotation of COMMUNI-
CATOR dialogue data for learning dialogue strate-
gies and user simulations. In Ninth Workshop on the
Semantics and Pragmatics of Dialogue (SEMDIAL:
DIALOR).
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2005. Hybrid Reinforcement/Supervised
Learning for Dialogue Policies from COMMUNI-
CATOR data. In IJCAI workshop on Knowledge and
Reasoning in Practical Dialogue Systems,.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323–340.
Esther Levin and Roberto Pieraccini. 1997. A
stochastic model of computer-human interaction
for learning dialogue strategies. In Eurospeech,
Rhodes,Greece.
Olivier Pietquin. 2004. A Framework for Unsuper-
vised Learning ofDialogue Strategies. Presses Uni-
versitaires de Louvain, SIMILAR Collection.
Verena Rieser and Oliver Lemon. 2006. Using ma-
chine learning to explore human multimodal clarifi-
cation strategies. In Proc. ACL.
Konrad Scheffler and Steve Young. 2001. Corpus-
based dialogue simulation for automatic strategy
learning and evaluation. In NAACL-2001 Work-
shop on Adaptation in Dialogue Systems, Pittsburgh,
USA.
John R. Searle. 1969. Speech Acts. Cambridge Uni-
versity Press.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal ofArtificial Intelli-
gence Research (JAIR).
Richard Sutton and Andrew Barto. 1998. Reinforce-
ment Learning. MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
Marilyn A. Walker, Rebecca J. Passonneau, and
Julie E. Boland. 2001. Quantitative and Qualita-
tive Evaluation of Darpa Communicator Spoken Di-
alogue Systems. In Meeting of the Association for
Computational Linguistics, pages 515–522.
</reference>
<page confidence="0.998193">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.825108">
<title confidence="0.9989285">Learning More Effective Dialogue Strategies Using Limited Dialogue Move Features</title>
<author confidence="0.995831">Frampton Lemon</author>
<affiliation confidence="0.9997075">HCRC, School of Informatics University of Edinburgh</affiliation>
<address confidence="0.999437">Edinburgh, EH8 9LW, UK</address>
<email confidence="0.996691">M.J.E.Frampton@sms.ed.ac.uk,olemon@inf.ed.ac.uk</email>
<abstract confidence="0.990074636363636">We explore the use of restricted dialogue contexts in reinforcement learning (RL) of effective dialogue strategies for information seeking spoken dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than previous research in this area, e.g. (Levin and Pieraccini, 1997; Scheffler and Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based information, but are much less complex than the full dialogue “Information States” explored in (Henderson et al., 2005), for which tractabe learning is an issue. We explore how incrementally adding richer features allows learning of more effective dialogue strategies. We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of different features on learned dialogue strategies. Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the autolearned strategies by the original (hand-coded) COMMUNIsystems, and by a baseline RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John L Austin</author>
</authors>
<title>How To Do Things With Words.</title>
<date>1962</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="4115" citStr="Austin, 1962" startWordPosition="637" endWordPosition="638">erson et al., 2005). To date, the use of very large state spaces relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). 185 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185–192, Sydney, July 2006. c�2006 Association for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>John L. Austin. 1962. How To Do Things With Words. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Ewan Klein</author>
<author>Oliver Lemon</author>
<author>Tetsushi Oka</author>
</authors>
<title>Dipper: Description and formalisation of an information-state update dialogue system architecture.</title>
<date>2003</date>
<booktitle>In 4th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Sapporo.</location>
<contexts>
<context position="7012" citStr="Bos et al., 2003" startWordPosition="1103" endWordPosition="1106">text features and use these in strategy learning. We compare the learned strategies to 2 baselines: the original COMMUNICATOR systems and an RL strategy which uses only slot status features. 1.2 Outline Section 2 contains a description of our basic experimental framework, and a detailed description of the reinforcement learning component and user simulations. Sections 3 and 4 describe the experiments and analyse our results, and in section 5 we conclude and suggest future work. 2 The Experimental Framework Each experiment is executed using the DIPPER Information State Update dialogue manager (Bos et al., 2003) (which here is used to track and update dialogue context rather than deciding which actions to take), a Reinforcement Learning program (which determines the next dialogue action to take), and various user simulations. In sections 2.3 and 2.4 we give more details about the reinforcement learner and user simulations. 2.1 The action set for the learner Below is a list of all the different actions that the RL dialogue manager can take and must learn to choose between based on the context: 1. An open question e.g. ‘How may I help you?’ 2. Ask the value for any one of slots 1...n. 3. Explicitly con</context>
</contexts>
<marker>Bos, Klein, Lemon, Oka, 2003</marker>
<rawString>Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi Oka. 2003. Dipper: Description and formalisation of an information-state update dialogue system architecture. In 4th SIGdial Workshop on Discourse and Dialogue, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4087" citStr="Clark, 1996" startWordPosition="633" endWordPosition="634">s may need to be used (Henderson et al., 2005). To date, the use of very large state spaces relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). 185 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185–192, Sydney, July 2006. c�2006 Association for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent i</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiland Eckert</author>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>User modeling for spoken dialogue system evaluation.</title>
<date>1997</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="11586" citStr="Eckert et al., 1997" startWordPosition="1895" endWordPosition="1898">hey produced faster learning for the slot-filling task. The seventh parameter is for supplying the initial Q-values. 2.4 N-Gram User Simulations Here user simulations, rather than real users, interact with the dialogue system during learning. This is because thousands of dialogues may be necessary to train even a simple system (here we train on up to 50000 dialogues), and for a proper exploration of the state-action space the system should sometimes take actions that are not optimal for the current situation, making it a sadistic and timeconsuming procedure for any human training the system. (Eckert et al., 1997) were the first to use a user simulation for this purpose, but it was not goal-directed and so could produce inconsistent utterances. The later simulations of (Pietquin, 2004) and (Scheffler and Young, 2001) were to some extent “goal-directed” and also incorporated an ASR error simulation. The user simulations interact with the system via intentions. Intentions are preferred because they are easier to generate than word sequences and because they allow error modelling of all parts of the system, for example ASR error modelling and semantic errors. The user and ASR simulations must be realistic</context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>Weiland Eckert, Esther Levin, and Roberto Pieraccini. 1997. User modeling for spoken dialogue system evaluation. In IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Frampton</author>
<author>Oliver Lemon</author>
</authors>
<title>Reinforcement Learning Of Dialogue Strategies Using The User’s Last Dialogue Act.</title>
<date>2005</date>
<booktitle>In IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems.</booktitle>
<contexts>
<context position="2904" citStr="Frampton and Lemon, 2005" startWordPosition="444" endWordPosition="447">only on specific strategy decisions (e.g. confirmation and initiative strategies), rather than the full problem of what system dialogue move to take next. The simple strategies learned for low-level definitions of state cannot be sensitive to (sometimes critical) aspects of the dialogue context, such as the user’s last dialogue move (DM) (e.g. requesthelp) unless that move directly affects the status of an information slot (e.g. provide-info(destinationcity)). We refer to additional contextual information such as the system and user’s last dialogue moves as high-level contextual information. (Frampton and Lemon, 2005) learned full strategies with limited ‘high-level’ information (i.e. the dialogue move(s) of the last user utterance) and only used a stochastic user simulation whose probabilities were supplied via commonsense and intuition, rather than learned from data. This paper uses data-driven n-gram user simulations (Georgila et al., 2005a) and a richer dialogue context. On the other hand, increasing the size of the state space for RL has the danger of making the learning problem intractable, and at the very least means that data is more sparse and state approximation methods may need to be used (Hende</context>
</contexts>
<marker>Frampton, Lemon, 2005</marker>
<rawString>Matthew Frampton and Oliver Lemon. 2005. Reinforcement Learning Of Dialogue Strategies Using The User’s Last Dialogue Act. In IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning User Simulations for Information State Update Dialogue Systems.</title>
<date>2005</date>
<booktitle>In Interspeech/Eurospeech: the 9th biennial conference of the International Speech Communication Association.</booktitle>
<contexts>
<context position="1015" citStr="Georgila et al., 2005" startWordPosition="144" endWordPosition="147">dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than previous research in this area, e.g. (Levin and Pieraccini, 1997; Scheffler and Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based information, but are much less complex than the full dialogue “Information States” explored in (Henderson et al., 2005), for which tractabe learning is an issue. We explore how incrementally adding richer features allows learning of more effective dialogue strategies. We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of different features on learned dialogue strategies. Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9% over the original (hand-coded) COMMUNICATOR systems, and by 7.8% over a baseline RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action. 1 Introduction Reinforcement Learning (RL) applied to the problem of dialogue management attempts to fin</context>
<context position="3235" citStr="Georgila et al., 2005" startWordPosition="494" endWordPosition="497">(DM) (e.g. requesthelp) unless that move directly affects the status of an information slot (e.g. provide-info(destinationcity)). We refer to additional contextual information such as the system and user’s last dialogue moves as high-level contextual information. (Frampton and Lemon, 2005) learned full strategies with limited ‘high-level’ information (i.e. the dialogue move(s) of the last user utterance) and only used a stochastic user simulation whose probabilities were supplied via commonsense and intuition, rather than learned from data. This paper uses data-driven n-gram user simulations (Georgila et al., 2005a) and a richer dialogue context. On the other hand, increasing the size of the state space for RL has the danger of making the learning problem intractable, and at the very least means that data is more sparse and state approximation methods may need to be used (Henderson et al., 2005). To date, the use of very large state spaces relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). 185 Proceedings of the 21st International Con</context>
<context position="4575" citStr="Georgila et al., 2005" startWordPosition="711" endWordPosition="714">iation for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology To explore these issues we have developed a Reinforcement Learning (RL) program to learn dialogue strategies while accurate simulated users (Georgila et al., 2005a) converse with a dialogue manager. See (Singh et al., 2002; Scheffler and Young, 2001) and (Sutton and Barto, 1998) for a detailed description of Markov Decision Processes and the relevant RL algorithms. In dialogue management we are faced with the problem of deciding which dialogue actions it </context>
<context position="6080" citStr="Georgila et al., 2005" startWordPosition="953" endWordPosition="956">ed dialogue often the “reward” of the dialogue (e.g. successfully booking a flight) is not obtainable immediately, and the large space of possible dialogues for any task makes some degree of trial-and-error exploration necessary. We use both 4-gram and 5-gram user simulations for testing and for training (i.e. train with 4-gram, test with 5-gram, and vice-versa). These simulations also simulate ASR errors since the probabilities are learned from recognition hypotheses and system behaviour logged in the COMMUNICATOR data (Walker et al., 2001) further annotated with speech acts and contexts by (Georgila et al., 2005b). Here the task domain is flight-booking, and the aim for the dialogue manager is to obtain values for the user’s flight information “slots” i.e. departure city, destination city, departure date and departure time, before making a database query. We add the dialogue moves of the last user and system turns as context features and use these in strategy learning. We compare the learned strategies to 2 baselines: the original COMMUNICATOR systems and an RL strategy which uses only slot status features. 1.2 Outline Section 2 contains a description of our basic experimental framework, and a detail</context>
<context position="12323" citStr="Georgila et al., 2005" startWordPosition="2017" endWordPosition="2020">stent utterances. The later simulations of (Pietquin, 2004) and (Scheffler and Young, 2001) were to some extent “goal-directed” and also incorporated an ASR error simulation. The user simulations interact with the system via intentions. Intentions are preferred because they are easier to generate than word sequences and because they allow error modelling of all parts of the system, for example ASR error modelling and semantic errors. The user and ASR simulations must be realistic if the learned strategy is to be directly applicable in a real system. The n-gram user simulations used here (see (Georgila et al., 2005a) for details and evaluation results) treat a dialogue as a sequence of pairs of speech acts and tasks. They take as input the n −1 most recent speech act-task pairs in the dialogue history, and based on n-gram probabilities learned from the COMMUNICATOR data (automatically annotated with speech acts and Information States (Georgila et al., 2005b)), they then output a user utterance as a further speech-act task pair. These user simulations incorporate the effects of ASR errors since they are built from the user utterances as they were recognized by the ASR components of the original COMMUNICA</context>
<context position="13983" citStr="Georgila et al., 2005" startWordPosition="2300" endWordPosition="2303">irmed when the slot value is implicitly or explicitly confirmed. An additional function maps the user speech-act task pairs to a form that can be interpreted by the dialogue manager. Post-mapping user responses are made up of one or more of the following types of utterance: (1) Stay quiet, (2) Provide 1 or more slot values, (3) Yes, (4) No, (5) Ask for help, (6) Hang-up, (7) Null (out-of-domain or no ASR hypothesis). The quality of the 4 and 5-gram user simulations has been established through a variety of metrics and against the behaviour of the actual users of the COMMUNICATOR systems, see (Georgila et al., 2005a). 2.4.1 Limitations of the user simulations The user and ASR simulations are a fundamentally important factor in determining the nature of the learned strategies. For this reason we should note the limitations of the n-gram simulations used here. A first limitation is that we cannot be sure that the COMMUNICATOR training data is sufficiently complete, and a second is that the n-gram simulations only use a window of n moves in the dialogue history. This second limitation becomes a problem when the user simulation’s current move ought to take into account something that occurred at an earlier </context>
<context position="15315" citStr="Georgila et al., 2005" startWordPosition="2524" endWordPosition="2527">ance of an ASR error for a particular word being independent of whether the same word was previously recognised correctly. The latter case means we cannot simulate for example, a particular slot value always being liable to misrecognition. These limitations will affect the nature of the learned strategies. Different state features may assume more or less importance than they would if the simulations were more realistic. This is a point that we will return to in the analysis of the experimental results. In future work we will use the more accurate user simulations recently developed following (Georgila et al., 2005a) and we expect that these will improve our results still further. 3 Experiments First we learned strategies with the 4-gram user simulation and tested with the 5-gram simulation, and then did the reverse. We experimented with different feature sets, exploring whether better strategies could be learned by adding limited context features. We used two baselines for comparison: • The performance of the original COMMUNICATOR systems in the data set (Walker et al., 2001). • An RL baseline dialogue manager learned using only slot-status features i.e. for each of slots 1 − 4, is the slot empty, fill</context>
<context position="30425" citStr="Georgila et al., 2005" startWordPosition="5095" endWordPosition="5098"> outperformed the lowlevel RL baseline (only slot status information) 191 by 7.8% and the original COMMUNICATOR systems by 65.9%. These strategies obtained more reward than the RL baseline by filling and confirming all of the slots with fewer system turns on average. Moreover, the learned strategies show interesting emergent dialogue behaviour such as making effective use of the ‘give help’ action and switching focus to different subtasks when the current subtask is proving problematic. In future work, we plan to use even more realistic user simulations, for example those developed following (Georgila et al., 2005a), which incorporate elements of goal-directed user behaviour. We will continue to investigate whether we can maintain tractability and learn superior strategies as we add incrementally more high-level contextual information to the state. At some stage this may necessitate using a generalisation method such as linear function approximation (Henderson et al., 2005). We also intend to use feature selection techniques (e.g. CFS subset evaluation (Rieser and Lemon, 2006)) on in order to determine which contextual features this suggests are important. We will also carry out a more direct compariso</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2005</marker>
<rawString>Kallirroi Georgila, James Henderson, and Oliver Lemon. 2005a. Learning User Simulations for Information State Update Dialogue Systems. In Interspeech/Eurospeech: the 9th biennial conference of the International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>Oliver Lemon</author>
<author>James Henderson</author>
</authors>
<title>Automatic annotation of COMMUNICATOR dialogue data for learning dialogue strategies and user simulations.</title>
<date>2005</date>
<booktitle>In Ninth Workshop on the Semantics and Pragmatics of Dialogue (SEMDIAL:</booktitle>
<publisher>DIALOR).</publisher>
<contexts>
<context position="1015" citStr="Georgila et al., 2005" startWordPosition="144" endWordPosition="147">dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than previous research in this area, e.g. (Levin and Pieraccini, 1997; Scheffler and Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based information, but are much less complex than the full dialogue “Information States” explored in (Henderson et al., 2005), for which tractabe learning is an issue. We explore how incrementally adding richer features allows learning of more effective dialogue strategies. We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of different features on learned dialogue strategies. Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9% over the original (hand-coded) COMMUNICATOR systems, and by 7.8% over a baseline RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action. 1 Introduction Reinforcement Learning (RL) applied to the problem of dialogue management attempts to fin</context>
<context position="3235" citStr="Georgila et al., 2005" startWordPosition="494" endWordPosition="497">(DM) (e.g. requesthelp) unless that move directly affects the status of an information slot (e.g. provide-info(destinationcity)). We refer to additional contextual information such as the system and user’s last dialogue moves as high-level contextual information. (Frampton and Lemon, 2005) learned full strategies with limited ‘high-level’ information (i.e. the dialogue move(s) of the last user utterance) and only used a stochastic user simulation whose probabilities were supplied via commonsense and intuition, rather than learned from data. This paper uses data-driven n-gram user simulations (Georgila et al., 2005a) and a richer dialogue context. On the other hand, increasing the size of the state space for RL has the danger of making the learning problem intractable, and at the very least means that data is more sparse and state approximation methods may need to be used (Henderson et al., 2005). To date, the use of very large state spaces relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). 185 Proceedings of the 21st International Con</context>
<context position="4575" citStr="Georgila et al., 2005" startWordPosition="711" endWordPosition="714">iation for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology To explore these issues we have developed a Reinforcement Learning (RL) program to learn dialogue strategies while accurate simulated users (Georgila et al., 2005a) converse with a dialogue manager. See (Singh et al., 2002; Scheffler and Young, 2001) and (Sutton and Barto, 1998) for a detailed description of Markov Decision Processes and the relevant RL algorithms. In dialogue management we are faced with the problem of deciding which dialogue actions it </context>
<context position="6080" citStr="Georgila et al., 2005" startWordPosition="953" endWordPosition="956">ed dialogue often the “reward” of the dialogue (e.g. successfully booking a flight) is not obtainable immediately, and the large space of possible dialogues for any task makes some degree of trial-and-error exploration necessary. We use both 4-gram and 5-gram user simulations for testing and for training (i.e. train with 4-gram, test with 5-gram, and vice-versa). These simulations also simulate ASR errors since the probabilities are learned from recognition hypotheses and system behaviour logged in the COMMUNICATOR data (Walker et al., 2001) further annotated with speech acts and contexts by (Georgila et al., 2005b). Here the task domain is flight-booking, and the aim for the dialogue manager is to obtain values for the user’s flight information “slots” i.e. departure city, destination city, departure date and departure time, before making a database query. We add the dialogue moves of the last user and system turns as context features and use these in strategy learning. We compare the learned strategies to 2 baselines: the original COMMUNICATOR systems and an RL strategy which uses only slot status features. 1.2 Outline Section 2 contains a description of our basic experimental framework, and a detail</context>
<context position="12323" citStr="Georgila et al., 2005" startWordPosition="2017" endWordPosition="2020">stent utterances. The later simulations of (Pietquin, 2004) and (Scheffler and Young, 2001) were to some extent “goal-directed” and also incorporated an ASR error simulation. The user simulations interact with the system via intentions. Intentions are preferred because they are easier to generate than word sequences and because they allow error modelling of all parts of the system, for example ASR error modelling and semantic errors. The user and ASR simulations must be realistic if the learned strategy is to be directly applicable in a real system. The n-gram user simulations used here (see (Georgila et al., 2005a) for details and evaluation results) treat a dialogue as a sequence of pairs of speech acts and tasks. They take as input the n −1 most recent speech act-task pairs in the dialogue history, and based on n-gram probabilities learned from the COMMUNICATOR data (automatically annotated with speech acts and Information States (Georgila et al., 2005b)), they then output a user utterance as a further speech-act task pair. These user simulations incorporate the effects of ASR errors since they are built from the user utterances as they were recognized by the ASR components of the original COMMUNICA</context>
<context position="13983" citStr="Georgila et al., 2005" startWordPosition="2300" endWordPosition="2303">irmed when the slot value is implicitly or explicitly confirmed. An additional function maps the user speech-act task pairs to a form that can be interpreted by the dialogue manager. Post-mapping user responses are made up of one or more of the following types of utterance: (1) Stay quiet, (2) Provide 1 or more slot values, (3) Yes, (4) No, (5) Ask for help, (6) Hang-up, (7) Null (out-of-domain or no ASR hypothesis). The quality of the 4 and 5-gram user simulations has been established through a variety of metrics and against the behaviour of the actual users of the COMMUNICATOR systems, see (Georgila et al., 2005a). 2.4.1 Limitations of the user simulations The user and ASR simulations are a fundamentally important factor in determining the nature of the learned strategies. For this reason we should note the limitations of the n-gram simulations used here. A first limitation is that we cannot be sure that the COMMUNICATOR training data is sufficiently complete, and a second is that the n-gram simulations only use a window of n moves in the dialogue history. This second limitation becomes a problem when the user simulation’s current move ought to take into account something that occurred at an earlier </context>
<context position="15315" citStr="Georgila et al., 2005" startWordPosition="2524" endWordPosition="2527">ance of an ASR error for a particular word being independent of whether the same word was previously recognised correctly. The latter case means we cannot simulate for example, a particular slot value always being liable to misrecognition. These limitations will affect the nature of the learned strategies. Different state features may assume more or less importance than they would if the simulations were more realistic. This is a point that we will return to in the analysis of the experimental results. In future work we will use the more accurate user simulations recently developed following (Georgila et al., 2005a) and we expect that these will improve our results still further. 3 Experiments First we learned strategies with the 4-gram user simulation and tested with the 5-gram simulation, and then did the reverse. We experimented with different feature sets, exploring whether better strategies could be learned by adding limited context features. We used two baselines for comparison: • The performance of the original COMMUNICATOR systems in the data set (Walker et al., 2001). • An RL baseline dialogue manager learned using only slot-status features i.e. for each of slots 1 − 4, is the slot empty, fill</context>
<context position="30425" citStr="Georgila et al., 2005" startWordPosition="5095" endWordPosition="5098"> outperformed the lowlevel RL baseline (only slot status information) 191 by 7.8% and the original COMMUNICATOR systems by 65.9%. These strategies obtained more reward than the RL baseline by filling and confirming all of the slots with fewer system turns on average. Moreover, the learned strategies show interesting emergent dialogue behaviour such as making effective use of the ‘give help’ action and switching focus to different subtasks when the current subtask is proving problematic. In future work, we plan to use even more realistic user simulations, for example those developed following (Georgila et al., 2005a), which incorporate elements of goal-directed user behaviour. We will continue to investigate whether we can maintain tractability and learn superior strategies as we add incrementally more high-level contextual information to the state. At some stage this may necessitate using a generalisation method such as linear function approximation (Henderson et al., 2005). We also intend to use feature selection techniques (e.g. CFS subset evaluation (Rieser and Lemon, 2006)) on in order to determine which contextual features this suggests are important. We will also carry out a more direct compariso</context>
</contexts>
<marker>Georgila, Lemon, Henderson, 2005</marker>
<rawString>Kallirroi Georgila, Oliver Lemon, and James Henderson. 2005b. Automatic annotation of COMMUNICATOR dialogue data for learning dialogue strategies and user simulations. In Ninth Workshop on the Semantics and Pragmatics of Dialogue (SEMDIAL: DIALOR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
</authors>
<title>Hybrid Reinforcement/Supervised Learning for Dialogue Policies from COMMUNICATOR data.</title>
<date>2005</date>
<booktitle>In IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems,.</booktitle>
<contexts>
<context position="765" citStr="Henderson et al., 2005" startWordPosition="105" endWordPosition="108"> University of Edinburgh Edinburgh, EH8 9LW, UK M.J.E.Frampton@sms.ed.ac.uk, olemon@inf.ed.ac.uk Abstract We explore the use of restricted dialogue contexts in reinforcement learning (RL) of effective dialogue strategies for information seeking spoken dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than previous research in this area, e.g. (Levin and Pieraccini, 1997; Scheffler and Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based information, but are much less complex than the full dialogue “Information States” explored in (Henderson et al., 2005), for which tractabe learning is an issue. We explore how incrementally adding richer features allows learning of more effective dialogue strategies. We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of different features on learned dialogue strategies. Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9% over the original (hand-coded) COMMUNICATOR systems, and by 7.8% over a baseline RL policy that uses only slot</context>
<context position="3522" citStr="Henderson et al., 2005" startWordPosition="546" endWordPosition="550">2005) learned full strategies with limited ‘high-level’ information (i.e. the dialogue move(s) of the last user utterance) and only used a stochastic user simulation whose probabilities were supplied via commonsense and intuition, rather than learned from data. This paper uses data-driven n-gram user simulations (Georgila et al., 2005a) and a richer dialogue context. On the other hand, increasing the size of the state space for RL has the danger of making the learning problem intractable, and at the very least means that data is more sparse and state approximation methods may need to be used (Henderson et al., 2005). To date, the use of very large state spaces relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). 185 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185–192, Sydney, July 2006. c�2006 Association for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larss</context>
<context position="16463" citStr="Henderson et al., 2005" startWordPosition="2720" endWordPosition="2723"> slot-status features i.e. for each of slots 1 − 4, is the slot empty, filled or confirmed? We then learned two further strategies: • Strategy 2 (UDM) was learned by adding the user’s last dialogue move to the state. • Strategy 3 (USDM) was learned by adding both the user and system’s last dialogue moves to the state. The possible system and user dialogue moves were those given in sections 2.1 and 2.4 respectively, and the reward function was that described in section 2.2. 3.1 The COMMUNICATOR data baseline We computed the scores for the original handcoded COMMUNICATOR systems as was done by (Henderson et al., 2005), and we call this the “HLG05” score. This scoring function is based on task completion and dialogue length rewards as determined by the PARADISE evaluation (Walker et al., 2000). This function gives 25 points for each slot which is filled, another 25 for each that is confirmed, and deducts 1 point for each system action. In this case the maximum possible score is 197 i.e. 200 minus 3 actions, (the system prompts the user, the user replies by filling all four of the slots in one turn, and the system then confirms all four slots and offers the flight). The average score for the 1242 dialogues i</context>
<context position="19042" citStr="Henderson et al., 2005" startWordPosition="3185" endWordPosition="3188">, UDM (av) + Last User DM 54.06** 190.75 100 100 −9.25 RL Strat 3, USDM (av) + Last System DM 55.57** 191.16 100 100 −8.84 COMM Systems 115.26 84.6 63.7 −33.1 Hybrid RL *** Information States 142.6 88.1 70.9 −16.4 Table 1: Testing the learned strategies after 50000 training dialogues, average reward achieved per dialogue over 1000 test dialogues. (a) = strategy trained using 4-gram and tested with 5-gram; (b) = strategy trained with 5-gram and tested with 4-gram; (av) = average; * significance level p &lt; 0.025; ** significance level p &lt; 0.005; *** Note: The Hybrid RL scores (here updated from (Henderson et al., 2005)) are not directly comparable since that system has a larger action set and fewer policy constraints. the 5-gram, while the ‘b’ strategies were trained with the 5-gram user simulation and tested with the 4-gram. The table also shows average scores for the strategies. Column 2 contains the average reward obtained per dialogue by each strategy over 1000 test dialogues (computed using the function of section 2.2). The 1000 test dialogues for each strategy were divided into 10 sets of 100. We carried out t-tests and found that in both the ‘a’ and ‘b’ cases, strategy 2 (UDM) performs significantly </context>
<context position="20415" citStr="Henderson et al., 2005" startWordPosition="3415" endWordPosition="3418">significance level p &lt; 0.005). With respect to average performance, strategy 2 (UDM) improves over the RL baseline by 4.9%, and strategy 3 (USDM) improves by 7.8%. Although there seem to be only negligible qualitative differences between strategies 2(b) and 3(b) and their ‘a’ equivalents, the former perform slightly better in testing. This suggests that the 4-gram simulation used for testing the ‘b’ strategies is a little more reliable in filling and confirming slot values than the 5-gram. The 3rd column “HLG05” shows the average scores for the dialogues as computed by the reward function of (Henderson et al., 2005). This is done for comparison with that work but also with the COMMUNICATOR data baseline. Using the HLG05 reward function, strategy 3 (USDM) improves over the original COMMUNICATOR systems baseline by 65.9%. The components making up the reward are shown in the final 3 columns of table 1. Here we see that all of the RL strategies are able to fill and confirm all of the 4 slots when conversing with the simulated COMMUNICATOR users. The only variation is in the average length of dialogue required to confirm all four slots. The COMMUNICATOR systems were often unable to confirm or fill all of the </context>
<context position="21746" citStr="Henderson et al., 2005" startWordPosition="3649" endWordPosition="3653"> do not simulate the case of a particular user goal utterance being unrecognisable for the system. This was a problem that could be encountered by the real COMMUNICATOR systems. Nevertheless, the performance of all the learned strategies compares very well to the COMMUNICATOR data baseline. For example, in an average dialogue, the RL strategies filled and confirmed all four slots with around 9 actions not including offering the flight, but the COMMUNICATOR systems took an average of around 33 actions per dialogue, and often failed to complete the task. With respect to the hybrid RL result of (Henderson et al., 2005), shown in the final row of the table, Strategy 3 (USDM) shows a 34% improvement, though these results are not directly comparable because that system uses a larger action set and has fewer constraints (e.g. it can ask “how may I help you?” at any time, not just at the start of a dialogue). Finally, let us note that the performance of the RL strategies is close to optimal, but that there is some room for improvement. With respect to the HLG05 metric, the optimal system score would be 197, but this would only be available in rare cases where the simulated user supplies all 4 slots in the 189 Tr</context>
<context position="29550" citStr="Henderson et al., 2005" startWordPosition="4959" endWordPosition="4962">ned from COMMUNICATOR data to explore reinforcement learning of full dialogue strategies with some “high-level” context information (the user and and system’s last dialogue moves). Almost all previous work (e.g. (Singh et al., 2002; Pietquin, 2004; Scheffler and Young, 2001)) has included only low-level information in state representations. In contrast, the exploration of very large state spaces to date relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). We presented our experimental environment, the reinforcement learner, the simulated users, and our methodology. In testing with the simulated COMMUNICATOR users, the new strategies learned with higher-level (i.e. dialogue move) information in the state outperformed the lowlevel RL baseline (only slot status information) 191 by 7.8% and the original COMMUNICATOR systems by 65.9%. These strategies obtained more reward than the RL baseline by filling and confirming all of the slots with fewer system turns on average. Moreover, the learned strategies show interesting emergent dialogue behaviour </context>
<context position="30792" citStr="Henderson et al., 2005" startWordPosition="5149" endWordPosition="5152">ctive use of the ‘give help’ action and switching focus to different subtasks when the current subtask is proving problematic. In future work, we plan to use even more realistic user simulations, for example those developed following (Georgila et al., 2005a), which incorporate elements of goal-directed user behaviour. We will continue to investigate whether we can maintain tractability and learn superior strategies as we add incrementally more high-level contextual information to the state. At some stage this may necessitate using a generalisation method such as linear function approximation (Henderson et al., 2005). We also intend to use feature selection techniques (e.g. CFS subset evaluation (Rieser and Lemon, 2006)) on in order to determine which contextual features this suggests are important. We will also carry out a more direct comparison with the hybrid strategies learned by (Henderson et al., 2005). In the slightly longer term, we will test our learned strategies on humans using a full spoken dialogue system. We hypothesize that the strategies which perform the best in terms of task completion and user satisfaction scores (Walker et al., 2000) will be those learned with high-level dialogue conte</context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2005</marker>
<rawString>James Henderson, Oliver Lemon, and Kallirroi Georgila. 2005. Hybrid Reinforcement/Supervised Learning for Dialogue Policies from COMMUNICATOR data. In IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
<author>David Traum</author>
</authors>
<title>Information state and dialogue management in the TRINDI Dialogue Move Engine Toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<pages>6--3</pages>
<contexts>
<context position="4141" citStr="Larsson and Traum, 2000" startWordPosition="639" endWordPosition="642">2005). To date, the use of very large state spaces relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). 185 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185–192, Sydney, July 2006. c�2006 Association for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology To explore these issues w</context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>Staffan Larsson and David Traum. 2000. Information state and dialogue management in the TRINDI Dialogue Move Engine Toolkit. Natural Language Engineering, 6(3-4):323–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>A stochastic model of computer-human interaction for learning dialogue strategies.</title>
<date>1997</date>
<booktitle>In Eurospeech,</booktitle>
<location>Rhodes,Greece.</location>
<contexts>
<context position="1855" citStr="Levin and Pieraccini, 1997" startWordPosition="281" endWordPosition="285">earned strategies by 65.9% over the original (hand-coded) COMMUNICATOR systems, and by 7.8% over a baseline RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action. 1 Introduction Reinforcement Learning (RL) applied to the problem of dialogue management attempts to find optimal mappings from dialogue contexts to system actions. The idea of using Markov Decision Processes (MDPs) and reinforcement learning to design dialogue strategies for dialogue systems was first proposed by (Levin and Pieraccini, 1997). There, and in subsequent work such as (Singh et al., 2002; Pietquin, 2004; Scheffler and Young, 2001), only very limited state information was used in strategy learning, based always on the number and status of filled information slots in the application (e.g. departure-city is filled, destination-city is unfilled). This we refer to as low-level contextual information. Much prior work (Singh et al., 2002) concentrated only on specific strategy decisions (e.g. confirmation and initiative strategies), rather than the full problem of what system dialogue move to take next. The simple strategies</context>
</contexts>
<marker>Levin, Pieraccini, 1997</marker>
<rawString>Esther Levin and Roberto Pieraccini. 1997. A stochastic model of computer-human interaction for learning dialogue strategies. In Eurospeech, Rhodes,Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
</authors>
<title>A Framework for Unsupervised Learning ofDialogue Strategies. Presses Universitaires de Louvain, SIMILAR Collection.</title>
<date>2004</date>
<contexts>
<context position="618" citStr="Pietquin, 2004" startWordPosition="83" endWordPosition="84">rning More Effective Dialogue Strategies Using Limited Dialogue Move Features Matthew Frampton and Oliver Lemon HCRC, School of Informatics University of Edinburgh Edinburgh, EH8 9LW, UK M.J.E.Frampton@sms.ed.ac.uk, olemon@inf.ed.ac.uk Abstract We explore the use of restricted dialogue contexts in reinforcement learning (RL) of effective dialogue strategies for information seeking spoken dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than previous research in this area, e.g. (Levin and Pieraccini, 1997; Scheffler and Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based information, but are much less complex than the full dialogue “Information States” explored in (Henderson et al., 2005), for which tractabe learning is an issue. We explore how incrementally adding richer features allows learning of more effective dialogue strategies. We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of different features on learned dialogue strategies. Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the autom</context>
<context position="1930" citStr="Pietquin, 2004" startWordPosition="297" endWordPosition="298">.8% over a baseline RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action. 1 Introduction Reinforcement Learning (RL) applied to the problem of dialogue management attempts to find optimal mappings from dialogue contexts to system actions. The idea of using Markov Decision Processes (MDPs) and reinforcement learning to design dialogue strategies for dialogue systems was first proposed by (Levin and Pieraccini, 1997). There, and in subsequent work such as (Singh et al., 2002; Pietquin, 2004; Scheffler and Young, 2001), only very limited state information was used in strategy learning, based always on the number and status of filled information slots in the application (e.g. departure-city is filled, destination-city is unfilled). This we refer to as low-level contextual information. Much prior work (Singh et al., 2002) concentrated only on specific strategy decisions (e.g. confirmation and initiative strategies), rather than the full problem of what system dialogue move to take next. The simple strategies learned for low-level definitions of state cannot be sensitive to (sometim</context>
<context position="11761" citStr="Pietquin, 2004" startWordPosition="1926" endWordPosition="1927">real users, interact with the dialogue system during learning. This is because thousands of dialogues may be necessary to train even a simple system (here we train on up to 50000 dialogues), and for a proper exploration of the state-action space the system should sometimes take actions that are not optimal for the current situation, making it a sadistic and timeconsuming procedure for any human training the system. (Eckert et al., 1997) were the first to use a user simulation for this purpose, but it was not goal-directed and so could produce inconsistent utterances. The later simulations of (Pietquin, 2004) and (Scheffler and Young, 2001) were to some extent “goal-directed” and also incorporated an ASR error simulation. The user simulations interact with the system via intentions. Intentions are preferred because they are easier to generate than word sequences and because they allow error modelling of all parts of the system, for example ASR error modelling and semantic errors. The user and ASR simulations must be realistic if the learned strategy is to be directly applicable in a real system. The n-gram user simulations used here (see (Georgila et al., 2005a) for details and evaluation results)</context>
<context position="29174" citStr="Pietquin, 2004" startWordPosition="4906" endWordPosition="4907">h feature to learn when to give up asking for a particular slot value and make a partially complete database query. This would of course require a reward function that gave some reward to partially complete database queries rather than the all-ornothing reward function used here. 5 Conclusion and Future Work We have used user simulations that are n-gram models learned from COMMUNICATOR data to explore reinforcement learning of full dialogue strategies with some “high-level” context information (the user and and system’s last dialogue moves). Almost all previous work (e.g. (Singh et al., 2002; Pietquin, 2004; Scheffler and Young, 2001)) has included only low-level information in state representations. In contrast, the exploration of very large state spaces to date relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). We presented our experimental environment, the reinforcement learner, the simulated users, and our methodology. In testing with the simulated COMMUNICATOR users, the new strategies learned with higher-level (i.e. dialogue </context>
</contexts>
<marker>Pietquin, 2004</marker>
<rawString>Olivier Pietquin. 2004. A Framework for Unsupervised Learning ofDialogue Strategies. Presses Universitaires de Louvain, SIMILAR Collection.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Using machine learning to explore human multimodal clarification strategies.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="4526" citStr="Rieser and Lemon, 2006" startWordPosition="703" endWordPosition="706">ACL, pages 185–192, Sydney, July 2006. c�2006 Association for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology To explore these issues we have developed a Reinforcement Learning (RL) program to learn dialogue strategies while accurate simulated users (Georgila et al., 2005a) converse with a dialogue manager. See (Singh et al., 2002; Scheffler and Young, 2001) and (Sutton and Barto, 1998) for a detailed description of Markov Decision Processes and the relevant RL algorithms. In dialogue management we are faced with t</context>
<context position="30897" citStr="Rieser and Lemon, 2006" startWordPosition="5165" endWordPosition="5168"> proving problematic. In future work, we plan to use even more realistic user simulations, for example those developed following (Georgila et al., 2005a), which incorporate elements of goal-directed user behaviour. We will continue to investigate whether we can maintain tractability and learn superior strategies as we add incrementally more high-level contextual information to the state. At some stage this may necessitate using a generalisation method such as linear function approximation (Henderson et al., 2005). We also intend to use feature selection techniques (e.g. CFS subset evaluation (Rieser and Lemon, 2006)) on in order to determine which contextual features this suggests are important. We will also carry out a more direct comparison with the hybrid strategies learned by (Henderson et al., 2005). In the slightly longer term, we will test our learned strategies on humans using a full spoken dialogue system. We hypothesize that the strategies which perform the best in terms of task completion and user satisfaction scores (Walker et al., 2000) will be those learned with high-level dialogue context information in the state. Acknowledgements This work is supported by the ESRC and the TALK project, ww</context>
</contexts>
<marker>Rieser, Lemon, 2006</marker>
<rawString>Verena Rieser and Oliver Lemon. 2006. Using machine learning to explore human multimodal clarification strategies. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konrad Scheffler</author>
<author>Steve Young</author>
</authors>
<title>Corpusbased dialogue simulation for automatic strategy learning and evaluation.</title>
<date>2001</date>
<booktitle>In NAACL-2001 Workshop on Adaptation in Dialogue Systems,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="1958" citStr="Scheffler and Young, 2001" startWordPosition="299" endWordPosition="302">ine RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action. 1 Introduction Reinforcement Learning (RL) applied to the problem of dialogue management attempts to find optimal mappings from dialogue contexts to system actions. The idea of using Markov Decision Processes (MDPs) and reinforcement learning to design dialogue strategies for dialogue systems was first proposed by (Levin and Pieraccini, 1997). There, and in subsequent work such as (Singh et al., 2002; Pietquin, 2004; Scheffler and Young, 2001), only very limited state information was used in strategy learning, based always on the number and status of filled information slots in the application (e.g. departure-city is filled, destination-city is unfilled). This we refer to as low-level contextual information. Much prior work (Singh et al., 2002) concentrated only on specific strategy decisions (e.g. confirmation and initiative strategies), rather than the full problem of what system dialogue move to take next. The simple strategies learned for low-level definitions of state cannot be sensitive to (sometimes critical) aspects of the </context>
<context position="4966" citStr="Scheffler and Young, 2001" startWordPosition="775" endWordPosition="778">a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology To explore these issues we have developed a Reinforcement Learning (RL) program to learn dialogue strategies while accurate simulated users (Georgila et al., 2005a) converse with a dialogue manager. See (Singh et al., 2002; Scheffler and Young, 2001) and (Sutton and Barto, 1998) for a detailed description of Markov Decision Processes and the relevant RL algorithms. In dialogue management we are faced with the problem of deciding which dialogue actions it is best to perform in different states. We use (RL) because it is a method of learning by delayed reward using trial-and-error search. These two properties appear to make RL techniques a good fit with the problem of automatically optimising dialogue strategies, because in task-oriented dialogue often the “reward” of the dialogue (e.g. successfully booking a flight) is not obtainable immed</context>
<context position="11793" citStr="Scheffler and Young, 2001" startWordPosition="1929" endWordPosition="1932">with the dialogue system during learning. This is because thousands of dialogues may be necessary to train even a simple system (here we train on up to 50000 dialogues), and for a proper exploration of the state-action space the system should sometimes take actions that are not optimal for the current situation, making it a sadistic and timeconsuming procedure for any human training the system. (Eckert et al., 1997) were the first to use a user simulation for this purpose, but it was not goal-directed and so could produce inconsistent utterances. The later simulations of (Pietquin, 2004) and (Scheffler and Young, 2001) were to some extent “goal-directed” and also incorporated an ASR error simulation. The user simulations interact with the system via intentions. Intentions are preferred because they are easier to generate than word sequences and because they allow error modelling of all parts of the system, for example ASR error modelling and semantic errors. The user and ASR simulations must be realistic if the learned strategy is to be directly applicable in a real system. The n-gram user simulations used here (see (Georgila et al., 2005a) for details and evaluation results) treat a dialogue as a sequence </context>
<context position="29202" citStr="Scheffler and Young, 2001" startWordPosition="4908" endWordPosition="4911">rn when to give up asking for a particular slot value and make a partially complete database query. This would of course require a reward function that gave some reward to partially complete database queries rather than the all-ornothing reward function used here. 5 Conclusion and Future Work We have used user simulations that are n-gram models learned from COMMUNICATOR data to explore reinforcement learning of full dialogue strategies with some “high-level” context information (the user and and system’s last dialogue moves). Almost all previous work (e.g. (Singh et al., 2002; Pietquin, 2004; Scheffler and Young, 2001)) has included only low-level information in state representations. In contrast, the exploration of very large state spaces to date relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). We presented our experimental environment, the reinforcement learner, the simulated users, and our methodology. In testing with the simulated COMMUNICATOR users, the new strategies learned with higher-level (i.e. dialogue move) information in the sta</context>
</contexts>
<marker>Scheffler, Young, 2001</marker>
<rawString>Konrad Scheffler and Steve Young. 2001. Corpusbased dialogue simulation for automatic strategy learning and evaluation. In NAACL-2001 Workshop on Adaptation in Dialogue Systems, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Speech Acts.</title>
<date>1969</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4101" citStr="Searle, 1969" startWordPosition="635" endWordPosition="636"> be used (Henderson et al., 2005). To date, the use of very large state spaces relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not yet been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). 185 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185–192, Sydney, July 2006. c�2006 Association for Computational Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>John R. Searle. 1969. Speech Acts. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder Singh</author>
<author>Diane Litman</author>
<author>Michael Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system.</title>
<date>2002</date>
<journal>Journal ofArtificial Intelligence Research (JAIR).</journal>
<contexts>
<context position="1914" citStr="Singh et al., 2002" startWordPosition="293" endWordPosition="296">OR systems, and by 7.8% over a baseline RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action. 1 Introduction Reinforcement Learning (RL) applied to the problem of dialogue management attempts to find optimal mappings from dialogue contexts to system actions. The idea of using Markov Decision Processes (MDPs) and reinforcement learning to design dialogue strategies for dialogue systems was first proposed by (Levin and Pieraccini, 1997). There, and in subsequent work such as (Singh et al., 2002; Pietquin, 2004; Scheffler and Young, 2001), only very limited state information was used in strategy learning, based always on the number and status of filled information slots in the application (e.g. departure-city is filled, destination-city is unfilled). This we refer to as low-level contextual information. Much prior work (Singh et al., 2002) concentrated only on specific strategy decisions (e.g. confirmation and initiative strategies), rather than the full problem of what system dialogue move to take next. The simple strategies learned for low-level definitions of state cannot be sensi</context>
<context position="4938" citStr="Singh et al., 2002" startWordPosition="771" endWordPosition="774">ual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology To explore these issues we have developed a Reinforcement Learning (RL) program to learn dialogue strategies while accurate simulated users (Georgila et al., 2005a) converse with a dialogue manager. See (Singh et al., 2002; Scheffler and Young, 2001) and (Sutton and Barto, 1998) for a detailed description of Markov Decision Processes and the relevant RL algorithms. In dialogue management we are faced with the problem of deciding which dialogue actions it is best to perform in different states. We use (RL) because it is a method of learning by delayed reward using trial-and-error search. These two properties appear to make RL techniques a good fit with the problem of automatically optimising dialogue strategies, because in task-oriented dialogue often the “reward” of the dialogue (e.g. successfully booking a fli</context>
<context position="29158" citStr="Singh et al., 2002" startWordPosition="4902" endWordPosition="4905">e the dialogue length feature to learn when to give up asking for a particular slot value and make a partially complete database query. This would of course require a reward function that gave some reward to partially complete database queries rather than the all-ornothing reward function used here. 5 Conclusion and Future Work We have used user simulations that are n-gram models learned from COMMUNICATOR data to explore reinforcement learning of full dialogue strategies with some “high-level” context information (the user and and system’s last dialogue moves). Almost all previous work (e.g. (Singh et al., 2002; Pietquin, 2004; Scheffler and Young, 2001)) has included only low-level information in state representations. In contrast, the exploration of very large state spaces to date relies on a “hybrid” supervised/reinforcement learning technique, where the reinforcement learning element has not been shown to significantly improve policies over the purely supervised case (Henderson et al., 2005). We presented our experimental environment, the reinforcement learner, the simulated users, and our methodology. In testing with the simulated COMMUNICATOR users, the new strategies learned with higher-level</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. Journal ofArtificial Intelligence Research (JAIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sutton</author>
<author>Andrew Barto</author>
</authors>
<title>Reinforcement Learning.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4995" citStr="Sutton and Barto, 1998" startWordPosition="780" endWordPosition="783">h acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology To explore these issues we have developed a Reinforcement Learning (RL) program to learn dialogue strategies while accurate simulated users (Georgila et al., 2005a) converse with a dialogue manager. See (Singh et al., 2002; Scheffler and Young, 2001) and (Sutton and Barto, 1998) for a detailed description of Markov Decision Processes and the relevant RL algorithms. In dialogue management we are faced with the problem of deciding which dialogue actions it is best to perform in different states. We use (RL) because it is a method of learning by delayed reward using trial-and-error search. These two properties appear to make RL techniques a good fit with the problem of automatically optimising dialogue strategies, because in task-oriented dialogue often the “reward” of the dialogue (e.g. successfully booking a flight) is not obtainable immediately, and the large space o</context>
<context position="9850" citStr="Sutton and Barto, 1998" startWordPosition="1595" endWordPosition="1598">er then confirms all four slots and submits a database query). 2.3 The Reinforcement Learner’s Parameters When the reinforcement learner agent is initialized, it is given a parameter string which includes the following: 1. Step Parameter: a = decreasing 2. Discount Factor: -y = 1 3. Action Selection Type = softmax (alternative is E-greedy) 4. Action Selection Parameter: temperature = 15 5. Eligibility Trace Parameter: A = 0.9 6. Eligibility Trace = replacing (alternative is accumulating) 7. Initial Q-values = 25 The reinforcement learner updates its Q-values using the Sarsa(A) algorithm (see (Sutton and Barto, 1998)). The first parameter is the stepparameter a which may be a value between 0 and 1, or specified as decreasing. If it is decreasing, as it is in our experiments, then for any given Q-value update a is k where k is the number of times that the state-action pair for which the update is being performed has been visited. This kind of step parameter will ensure that given a sufficient number of training dialogues, each of the Q-values will eventually converge. The second parameter (discount factor) -y may take a value between 0 and 1. For the dialogue management problem we set it to 1 so that futur</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard Sutton and Andrew Barto. 1998. Reinforcement Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards Developing General Models of Usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="8935" citStr="Walker et al., 2000" startWordPosition="1451" endWordPosition="1454">y confirm the first slot. This action set does not include actions that ask the nth slot whilst implicitly confirming slot value n − 2. These will be added in future experiments as we continue to increase the action and state space. 186 3. User simulation hangs-up: −100 4. DIPPER passes to a human operator: −50 5. Each system turn: −5 To maximise the chances of a slot value being correct, it must be confirmed rather than just filled. The reward function reflects the fact that a successful dialogue manager must maximise its chances of getting the slots correct i.e. they must all be confirmed. (Walker et al., 2000) showed with the PARADISE evaluation that confirming slots increases user satisfaction. The maximum reward that can be obtained for a single dialogue is 85, (the dialogue manager prompts the user, the user replies by filling all four of the slots in a single utterance, and the dialogue manager then confirms all four slots and submits a database query). 2.3 The Reinforcement Learner’s Parameters When the reinforcement learner agent is initialized, it is given a parameter string which includes the following: 1. Step Parameter: a = decreasing 2. Discount Factor: -y = 1 3. Action Selection Type = </context>
<context position="16641" citStr="Walker et al., 2000" startWordPosition="2749" endWordPosition="2752">’s last dialogue move to the state. • Strategy 3 (USDM) was learned by adding both the user and system’s last dialogue moves to the state. The possible system and user dialogue moves were those given in sections 2.1 and 2.4 respectively, and the reward function was that described in section 2.2. 3.1 The COMMUNICATOR data baseline We computed the scores for the original handcoded COMMUNICATOR systems as was done by (Henderson et al., 2005), and we call this the “HLG05” score. This scoring function is based on task completion and dialogue length rewards as determined by the PARADISE evaluation (Walker et al., 2000). This function gives 25 points for each slot which is filled, another 25 for each that is confirmed, and deducts 1 point for each system action. In this case the maximum possible score is 197 i.e. 200 minus 3 actions, (the system prompts the user, the user replies by filling all four of the slots in one turn, and the system then confirms all four slots and offers the flight). The average score for the 1242 dialogues in the COMMUNICATOR dataset where the aim was to fill and confirm only the same four slots as we have used here was 115.26. The other COMMUNICATOR dialogues involved different slo</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn A. Walker, Candace A. Kamm, and Diane J. Litman. 2000. Towards Developing General Models of Usability with PARADISE. Natural Language Engineering, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Rebecca J Passonneau</author>
<author>Julie E Boland</author>
</authors>
<title>Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems.</title>
<date>2001</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>515--522</pages>
<contexts>
<context position="992" citStr="Walker et al., 2001" startWordPosition="140" endWordPosition="143">ation seeking spoken dialogue systems (e.g. COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than previous research in this area, e.g. (Levin and Pieraccini, 1997; Scheffler and Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based information, but are much less complex than the full dialogue “Information States” explored in (Henderson et al., 2005), for which tractabe learning is an issue. We explore how incrementally adding richer features allows learning of more effective dialogue strategies. We use 2 user simulations learned from COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the effects of different features on learned dialogue strategies. Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9% over the original (hand-coded) COMMUNICATOR systems, and by 7.8% over a baseline RL policy that uses only slot-status features. We show that the learned strategies exhibit an emergent “focus switching” strategy and effective use of the ‘give help’ action. 1 Introduction Reinforcement Learning (RL) applied to the problem of dialogue man</context>
<context position="4598" citStr="Walker et al., 2001" startWordPosition="715" endWordPosition="718"> Linguistics The extended state spaces that we propose are based on theories of dialogue such as (Clark, 1996; Searle, 1969; Austin, 1962; Larsson and Traum, 2000), where which actions a dialogue participant can or should take next are not based solely on the task-state (i.e. in our domain, which slots are filled), but also on wider contextual factors such as a user’s dialogue moves or speech acts. In future work we also intend to use feature selection techniques (e.g. correlation-based feature subset (CFS) evaluation (Rieser and Lemon, 2006)) on the COMMUNICATOR data (Georgila et al., 2005a; Walker et al., 2001) in order to identify additional context features that it may be effective to represent in the state. 1.1 Methodology To explore these issues we have developed a Reinforcement Learning (RL) program to learn dialogue strategies while accurate simulated users (Georgila et al., 2005a) converse with a dialogue manager. See (Singh et al., 2002; Scheffler and Young, 2001) and (Sutton and Barto, 1998) for a detailed description of Markov Decision Processes and the relevant RL algorithms. In dialogue management we are faced with the problem of deciding which dialogue actions it is best to perform in d</context>
<context position="6006" citStr="Walker et al., 2001" startWordPosition="940" endWordPosition="943">m of automatically optimising dialogue strategies, because in task-oriented dialogue often the “reward” of the dialogue (e.g. successfully booking a flight) is not obtainable immediately, and the large space of possible dialogues for any task makes some degree of trial-and-error exploration necessary. We use both 4-gram and 5-gram user simulations for testing and for training (i.e. train with 4-gram, test with 5-gram, and vice-versa). These simulations also simulate ASR errors since the probabilities are learned from recognition hypotheses and system behaviour logged in the COMMUNICATOR data (Walker et al., 2001) further annotated with speech acts and contexts by (Georgila et al., 2005b). Here the task domain is flight-booking, and the aim for the dialogue manager is to obtain values for the user’s flight information “slots” i.e. departure city, destination city, departure date and departure time, before making a database query. We add the dialogue moves of the last user and system turns as context features and use these in strategy learning. We compare the learned strategies to 2 baselines: the original COMMUNICATOR systems and an RL strategy which uses only slot status features. 1.2 Outline Section </context>
<context position="15786" citStr="Walker et al., 2001" startWordPosition="2601" endWordPosition="2604">alysis of the experimental results. In future work we will use the more accurate user simulations recently developed following (Georgila et al., 2005a) and we expect that these will improve our results still further. 3 Experiments First we learned strategies with the 4-gram user simulation and tested with the 5-gram simulation, and then did the reverse. We experimented with different feature sets, exploring whether better strategies could be learned by adding limited context features. We used two baselines for comparison: • The performance of the original COMMUNICATOR systems in the data set (Walker et al., 2001). • An RL baseline dialogue manager learned using only slot-status features i.e. for each of slots 1 − 4, is the slot empty, filled or confirmed? We then learned two further strategies: • Strategy 2 (UDM) was learned by adding the user’s last dialogue move to the state. • Strategy 3 (USDM) was learned by adding both the user and system’s last dialogue moves to the state. The possible system and user dialogue moves were those given in sections 2.1 and 2.4 respectively, and the reward function was that described in section 2.2. 3.1 The COMMUNICATOR data baseline We computed the scores for the or</context>
</contexts>
<marker>Walker, Passonneau, Boland, 2001</marker>
<rawString>Marilyn A. Walker, Rebecca J. Passonneau, and Julie E. Boland. 2001. Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems. In Meeting of the Association for Computational Linguistics, pages 515–522.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>