<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000159">
<title confidence="0.98487">
How well does active learning actually work? Time-based evaluation of
cost-reduction strategies for language documentation.
</title>
<author confidence="0.998738">
Jason Baldridge
</author>
<affiliation confidence="0.9965165">
Department of Linguistics
The University of Texas at Austin
</affiliation>
<email confidence="0.998491">
jbaldrid@mail.utexas.edu
</email>
<sectionHeader confidence="0.993896" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959388888889">
Machine involvement has the potential to
speed up language documentation. We as-
sess this potential with timed annotation
experiments that consider annotator exper-
tise, example selection methods, and sug-
gestions from a machine classifier. We
find that better example selection and la-
bel suggestions improve efficiency, but ef-
fectiveness depends strongly on annota-
tor expertise. Our expert performed best
with uncertainty selection, but gained lit-
tle from suggestions. Our non-expert per-
formed best with random selection and
suggestions. The results underscore the
importance both of measuring annotation
cost reductions with respect to time and of
the need for cost-sensitive learning meth-
ods that adapt to annotators.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999107">
Data annotated with linguistically interesting la-
bels is used in a wide variety of contexts. Com-
putational linguists generally use annotated data
as training and evaluation material for natural lan-
guage processing systems; corpus linguists use it
to test hypotheses about language; documentary
linguists create interlinear glossed texts to pre-
serve examples of endangered languages and hy-
potheses about the grammars of those languages.
Regardless of the context, creating annotated data
is costly in terms of time and/or money. Since both
time and money are undeniably in limited supply,
there is a widely shared desire to reduce this cost.
Reducing cost involves strategies that do more
with fewer human-annotated labels and/or reduce
the per-label cost. An example of the former is ac-
tive learning, which focuses annotation effort on
data points selected by the learner(s) for their ex-
pected utility in developing a more accurate model
</bodyText>
<note confidence="0.313282">
Alexis Palmer
Computational Linguistics
Saarland University
</note>
<email confidence="0.79208">
apalmer@coli.uni-sb.de
</email>
<bodyText confidence="0.999858512195122">
(Settles, 2009). Examples of the latter include
providing suggestions from a machine labeler and
using extremely cheap human labelers, e.g. with
the Amazon Mechanical Turk (Snow et al., 2008).
Different techniques may be more or less appli-
cable depending on the language being annotated,
the kind of labels which are desired (tags, syntac-
tic structures, etc.), and the desired use of the an-
notated data (e.g., for training models, testing lin-
guistic hypotheses, or preserving a language).
This paper discusses experiments that measure
the effectiveness of machine-aided annotation for
language documentation using both active learn-
ing simulation experiments and annotation ex-
periments which involve actual documentary lin-
guists interacting with machine example selec-
tion and label suggestion. Specifically, we deal
with the task of labeling morphemes of the Mayan
language Uspanteko with fine-grained parts-of-
speech. We also run active learning simulation
experiments for part-of-speech tagging for Dan-
ish, Dutch, English, Swedish, and Uspanteko to
show the validity of our models and methods in a
standard setting. For Uspanteko, we provide re-
sults from annotation experiments in which anno-
tation cost is measured in terms of the actual an-
notation time required while varying three factors:
(1) example selection, (2) machine label sugges-
tions, and (3) annotator expertise.
Our findings indicate that there is consider-
able promise for reducing the cost of produc-
ing IGT, but they also demonstrate considerable
variation due to the interaction of these factors.
This suggests different prescriptions for appropri-
ate strategies in different contexts. Most clearly,
the worst performing strategy—by far—is that
used in nearly all documentary work: sequential
annotation without automation. Also, our expert
annotator did best with examples picked by un-
certainty selection, while our non-expert did best
with random selection aided by machine label sug-
</bodyText>
<page confidence="0.97499">
296
</page>
<note confidence="0.997127">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 296–305,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<table confidence="0.799135166666667">
Language #words-tr #words-dev #tags #sents-tr #sents-dev Avg.sent Avg.tr.sent Avg.dev.sent
Danish 62825 31561 10 3570 1618 18.18 17.60 19.50
Dutch 129586 65483 13 9365 3982 14.61 13.84 16.44
English 167593 131768 45 6945 5527 24.00 24.13 23.84
Swedish 127684 63783 41 7326 3714 17.34 17.43 17.17
Uspanteko 43473 19906 69 7423 3288 5.92 5.86 6.05
</table>
<tableCaption confidence="0.999523">
Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length.
</tableCaption>
<bodyText confidence="0.9998175">
gestions. This difference confirms the importance
of cost-sensitive active learning strategies that are
not just learner-guided, but also take into account
modeling of the annotators (Settles et al., 2008;
Haertel et al., 2008; Vijayanarasimhan and Grau-
man, 2008). Finally, we confirm the importance
of using actual annotation time to measure annota-
tion cost: a unit-cost assumption—even at a fine-
grained level—can dramatically misrepresent the
actual effectiveness of different strategies.
</bodyText>
<sectionHeader confidence="0.891885" genericHeader="introduction">
2 Task and data
</sectionHeader>
<bodyText confidence="0.9979998125">
Annotation task: language documentation
The amount of money spent on obtaining human
annotations is an extremely important concern in
much language annotation. However, there is a
further urgency for annotation in the case of lan-
guage documentation: languages are dying at the
rate of two each month. By the end of this cen-
tury, half of the approximately 6000 extant spoken
languages will cease to be transmitted effectively
from one generation of speakers to the next (Crys-
tal, 2000). Recorded and transcribed texts anno-
tated with detailed linguistic information create an
important multi-faceted record of these languages,
but there are few trained linguists with adequate
time and appropriate levels of funding relative to
the size of the problem. Annotation cost—in both
time and money—is thus keenly felt in the work
of documenting and describing endangered lan-
guages. Active learning and automated label sug-
gestions could help deal with this language docu-
mentation bottleneck.
We focus on one stage of language documen-
tation, the production of interlinear glossed text
(IGT), a standard form of annotation that in-
volves both morphological and grammatical anal-
ysis. IGT is generally created following transcrip-
tion and translation of recorded speech, with the
annotations often being provided by trained anno-
tators with varying levels of expertise. The result
is generally a small amount of IGT annotated data
and a greater amount of unannotated data.
Data We use a collection of 32 interlinear
glossed texts (IGT) in the Mayan language Uspan-
teko. This corpus was cleaned up and adapted by
Palmer et al. (2009) from an original collection of
67 texts that were collected, transcribed, translated
and annotated by the OKMA language documen-
tation project (Pixabaj et al., 2007).
Two core tasks in creating IGT are morpholog-
ical analysis and tagging morphemes with their
glosses (labels indicating part-of-speech and/or
grammatical function). We deal with the latter task
and assume texts are morphologically segmented.
Standard four-line IGT has morphemes on one line
and their glosses on the next. The gloss line in-
cludes labels for grammatical morphemes (e.g. PL
or COM) and translations of stems (e.g. hablar or
idioma). The following is an Uspanteko example:
</bodyText>
<listItem confidence="0.951787">
(1) TEXT: Kita’ tinch’ab’ej laj inyolj iin
MORPH: kita’ t-in-ch’abe-j laj in-yol-j iin
GLOSS: NEG INC-E1S-hablar-SC PREP A1S-idioma-SC yo
POS: PART TAM-PERS-VT-SUF PREP PERS-S-SUF PRON
TRANS: ‘No le hablo en mi idioma.’
</listItem>
<bodyText confidence="0.998634631578947">
We use a single layer that is a combination of the
GLOSS and POS layers (Palmer et al., 2009). For
(1), the morphemes and labels for our task are:
We also consider POS-tagging for Danish,
Dutch, English, and Swedish; the English is from
sections 00-05 (as training set) and 19-21 (as de-
velopment set) of the Penn Treebank (Marcus et
al., 1993), and the other languages are from the
CoNLL-X dependency parsing shared task (Buch-
holz and Marsi, 2006).1 We split the original train-
ing data into training and development sets. Ta-
ble 1 shows the number of words and sentences
in each split of each dataset, as well as the num-
ber of possible labels and the average sentence
length. The Uspanteko data is counted in mor-
phemes rather than words; also, the Uspanteko
texts are divided at the clause rather than sentence
level. This gives the corpus a much lower average
clause length than the other languages (Table 1).
</bodyText>
<footnote confidence="0.9873425">
1The subset of the Penn Treebank was chosen to be of
comparable size to the CoNLL datasets.
</footnote>
<figure confidence="0.995209473684211">
-j
SC
(2) kita’
laj
PREP
in-
A1S
t-
NEG INC
iin
PRON
ch’abe
VT
in-
E1S
yol
S
-j
SC
</figure>
<page confidence="0.991555">
297
</page>
<sectionHeader confidence="0.985756" genericHeader="method">
3 Model and methods
</sectionHeader>
<bodyText confidence="0.999364095238095">
Classification model. We use a standard maxi-
mum entropy classifier for tagging Danish, Dutch,
English, and Swedish words with POS-tags and
tagging Uspanteko morphemes with Gloss/POS
tags. The label for a word/morpheme is pre-
dicted based on the word/morpheme itself plus
a window of two units before and after. Stan-
dard part-of-speech tagging features (Ratnaparkhi,
1998; Curran and Clark, 2003) are extracted from
the morpheme to help with predicting labels for
previously unseen morphemes. This is a strong
but standard model; better, more complex models
could be used, but the gains are likely to be small.
Thus, we opted for simplicity in our model so as to
focus more on the interaction between the annota-
tor and different levels of machine involvement.
The accuracy of the tagger on the datasets when
trained on all available training material is given
in the following table, along with accuracy of a
unigram model (learned from the training set and
constrained by a tag dictionary for known words).
</bodyText>
<table confidence="0.993651666666667">
Unigram Model
Danish 91.62% 95.58%
Dutch 90.92% 93.57%
English 87.87% 93.25%
Swedish 84.91% 87.74%
Uspanteko 77.84% 79.39%
</table>
<bodyText confidence="0.999389082191781">
Sample selection. We consider three sample
selection methods: sequential, random, and
uncertainty. Sequential selection is important
to consider as it is the default in documentary
projects. It is sub-optimal for corpora with con-
tiguous sub-domains, since it necessitates working
through many similar examples before getting to
possibly more informative examples. Random se-
lection is a model-free method that avoids the sub-
domain trap by sampling freely from the entire
corpus. It generally works better than sequential
selection and provides a strong baseline against
which to compare learner-guided selection.
Uncertainty selection (Cohn et al., 1995) iden-
tifies examples the model is least confident about.
We measure uncertainty as the entropy of the la-
bel distribution predicted by the maximum en-
tropy model for each example. Uncertainty for
a clause is calculated as the average entropy per
morpheme; clauses with the highest average en-
tropy are selected for labeling.
A recent development in active learning is cost-
sensitive selection that is guided not only by the
learner but also by the expected cost of labeling an
example based on its likely complexity and/or the
reliability of the annotator. Settles et al. (2008)
provide empirical validation for cost-related in-
tuitions; for example, that cost of annotation is
static neither per example nor per annotator. Also,
they show that taking annotation cost into account
can improve active learning effectiveness, but that
learning to predict annotation cost is not yet well-
understood. A cost-sensitive Return on Investment
heuristic is developed in Haertel et al. (2008) and
tested in a simulated POS-tagging context. Our
experiments do not employ cost-sensitive selec-
tion, but our results—from live (non-simulated)
active learning experiments of real-world scale—
empirically support the need to consider cost-
sensitive selection if better cost reductions are to
be achieved.
Annotation setup. We compare results from
two annotators with different levels of exposure to
Uspanteko. Both are documentary linguists with
extensive field experience. Our expert annota-
tor is a native speaker of K’ichee’, a closely re-
lated Mayan language, and has worked extensively
on Uspanteko. Our non-expert annotator had no
prior experience with Uspanteko and only limited
exposure to Mayan languages. During annotation,
he used an Uspanteko-Spanish dictionary.
For each selection method, we consider two
conditions for providing classifier labels: a do-
suggest (ds) condition where the labels predicted
by the machine learner are shown to the annotator,
and a no-suggest (ns) condition where the annota-
tor does not see the predictions. With ds, the anno-
tator is shown the most probable label and a ranked
list of all labels assigned a probability greater than
half that of the best label. For ns, the annotator
sees a frequency-ranked list of labels previously
seen in training data for the given morpheme.
Annotators improve as they see more examples.
To minimize the impact of this learning process,
annotation is done in rounds. Each round con-
sists of sixty clauses—six batches of ten each for
the six experimental cases. The annotator is free
to break between batches. Following annotation,
the newly-labeled clauses are added to the train-
ing data, and a new model is trained and evaluated.
Both annotators completed fifty-six rounds of an-
notation. See Palmer et al. (2009) for more details
on the annotation setup.
</bodyText>
<page confidence="0.985543">
298
</page>
<bodyText confidence="0.999984276595745">
Measuring annotation cost. Active learning
studies usually simulate annotation and use a unit
cost assumption that each word, sentence, con-
stituent, document, etc. takes the same time to an-
notate. This is often the only option since corpora
typically do not retain annotation time, but it is
likely to exaggerate the annotation cost reductions
achieved. This is exacerbated with active learn-
ing: the informative examples it seeks to find are
typically harder to annotate (Hachey et al., 2005).
Baldridge and Osborne (2008) correlate a unit
cost in terms of discriminants (decisions made
by annotators about valid parses) to annotation
time. This is a better approximation than unit costs
where such a relationship cannot be established.
However, it is based on a static measurement of
annotation time, and clearly the time taken to an-
notate an example is not a function of the example
alone. Annotation time is actually dynamic in that
it is dependent on how many and what kinds of
examples have already been annotated. An “infor-
mative” example is likely to take longer to anno-
tate if selected early than it would after the anno-
tator has seen many other examples.
Thus, it is important to measure annotation time
embedded in the context of a particular annota-
tion experiment with the sample selection/labeling
strategies of interest. In our annotation experi-
ments, we measure the exact time taken to anno-
tate each example by each annotator and use this
as the cost metric, inspired by Ngai and Yarowsky
(2000). In the simulation studies, as we are un-
able to measure time, we measure cost by sen-
tence/clause and word/morpheme.
Learning curve comparison. We are interested
in comparative evaluation of many different exper-
imental settings, across which we vary selection
methods, use of label suggestions, and annotators.
To achieve this, it is useful to have a summary
value for comparing the results from two individ-
ual experiments. One such measure is the percent-
age error reduction (PER), measured over a dis-
crete set of points on the first 20% of the points on
the learning curve (Melville and Mooney, 2004).2
We use a new related measure, which we call
the overall percentage error reduction (OPER),
that uses the entire area under the curves given by
</bodyText>
<footnote confidence="0.7192225">
2This is justified in standard conditions, sampling from a
finite corpus: active learning runs out of interesting examples
after considering a fraction of the data, so the curve is artifi-
cially pulled down by the remaining, boring examples.
</footnote>
<bodyText confidence="0.998272333333333">
fitted nonlinear regression models rather than av-
eraging over a subset of data points. Specifically,
we fit a modified Michaelis-Menton model:
</bodyText>
<equation confidence="0.999842">
Um(A + cost)
f(cost, (K, Um, A)) =
K + cost
</equation>
<bodyText confidence="0.999211214285714">
The (original) parameters Um and K respectively
correspond to the horizontal asymptote and the
cost where accuracy is halfway between 0 and Um.
The additional parameter A allows for a better fit
to our data by allowing for less sharp elbows and
letting cost be zero. Model parameters were de-
termined with nls in R (Ritz and Streibig, 2008).
With the fitted regression models, it is straight-
forward to calculate the area under the curve be-
tween a start cost ci and end cost cj by taking the
integral from ci to cj. The overall accuracy for
the experiment is given by dividing that area by
100 × (cj − ci). Call this the overall curve accu-
racy (OCA). Then, for experiment A compared to
</bodyText>
<equation confidence="0.990447">
enment B, OPER A,B oCAA—CCAB
ex
p ( ) - 100−OCAB . For
</equation>
<bodyText confidence="0.999783666666667">
the simulation experiments we calculate OPER for
only the first 20% of cost units, like Melville and
Mooney. For the annotation experiments, we cal-
culate it for the minimum amount of time spent on
any of the experiments (which ended up using less
than 10% of all available morphemes).
</bodyText>
<sectionHeader confidence="0.987526" genericHeader="method">
4 Simulation experiments
</sectionHeader>
<bodyText confidence="0.999698521739131">
We verify that our tagger and dataset behave as
expected in standard active learning experiments
by running simulations on the Uspanteko data set,
and on POS-tagging for Danish, Dutch, English,
and Swedish. Here, we vary only the selection
method: sequential, random, or uncertainty.
For each language, we randomly select a seed
set of 10 labeled sentences. The number of exam-
ples selected to be labeled in each round begins
at 10 and doubles after every 20 rounds. For rand
and unc, each batch of examples is selected from a
pool (size of 1000) that is itself randomly selected
from the entire set of remaining unlabeled exam-
ples. rand and unc experiments for each language
are replicated 5 times; splines and regressions are
computed over all runs for each condition.
Figure 1 gives learning curves for the Uspan-
teko simulations, with cost measured in terms of
(a) clauses and (b) morphemes. Both graphs show
the usual behavior found in active learning exper-
iments. rand and unc both rise more quickly than
seq, and unc is well above rand. The relation-
ship between the methods is the same regardless
</bodyText>
<page confidence="0.985294">
299
</page>
<table confidence="0.792085">
50 55 60 65 70 75 80
Accuracy on all tokens
Accuracy on all tokens
50 55 60 65 70 75 80
Uncertainty
Random
Sequential
</table>
<figure confidence="0.998531875">
0 2000 4000 6000
Number of clauses selected
0 10000 20000 30000 40000
Number of morphemes selected
Uncertainty
Random
Sequential
(a) (b)
</figure>
<figureCaption confidence="0.998908">
Figure 1: Learning curves for simulations; (a) clause cost and (b) morphemes cost. The dashed vertical
lines indicate (a) #clauses=1485 and (b) #morphemes=8695 (to compare OPER values).
</figureCaption>
<table confidence="0.99701775">
rand unc unc
seq seq rand
Uspanteko-Clauses 5.86 13.27 7.86
Uspanteko-Morphs 7.47 11.68 4.55
</table>
<tableCaption confidence="0.984789">
Table 2: OPER values for Uspanteko simulations,
</tableCaption>
<bodyText confidence="0.987637916666667">
comparing clause and morpheme cost. B indi-
cates we compute OPER(A,B).
of the cost metric, but the relative differences in
cost-savings are not, which we see when we look
at OPER values.
The dashed vertical lines in the two graphs cor-
respond to the 20% mark used to calculate OPER
values, which are given in Table 2. Most impor-
tantly, note the much larger OPER for unc over
rand with clause cost (7.86 vs 4.55). Also note
that OPER(rand,seq) is lower with clause cost—
this indicates that the beginning portions of the
corpus contain longer sentences with more mor-
phemes, an accident which overstates how well
seq would likely work in general.
Since rand is unbiased with respect to pick-
ing longer sentences, the large increase of
OPER(unc,rand) from 4.55 to 7.86 is a clear in-
dication of the well-known—but not always at-
tended to—tendency of uncertainty sampling to
select longer sentences. Consequently, one should
at least use sub-sentence cost in order not to over-
state the gains from active learning. The annota-
tion experiments in the next section take this word
</bodyText>
<table confidence="0.998996714285714">
rand unc unc
seq seq rand
Danish 4.58 6.95 2.48
Dutch 21.95 23.68 2.20
English 6.55 8.00 1.56
Swedish 9.56 9.29 -0.30
Uspanteko 7.47 11.68 4.55
</table>
<tableCaption confidence="0.8476835">
Table 3: OPER values for morpheme cost for sim-
ulations. B indicates we compute OPER(A,B).
</tableCaption>
<bodyText confidence="0.999718578947369">
of caution one step further: even sub-sentence cost
(morpheme cost, in our setting) can overestimate
gains since the morphemes selected are actually
harder to annotate and thus take more time.
Table 3 gives overall percentage error reduc-
tions (OPER) between different selection methods
based on word/morpheme cost, for each language.
For all languages, rand and unc are better than
seq. Only in the case of Swedish is there no ben-
efit from unc over rand. For Dutch, the large
gains over seq for both rand and unc accurately
reflect the heterogeneity of the underlying Alpino
corpus.3 Most importantly, for Uspanteko, there
are large reductions from unc to rand to seq, mir-
roring the clear trends in Figure 1b.
These simulations have an unrealistic “perfect”
annotator, the corpus. Next, we discuss results
with real annotators—who may be fallible or may
(reasonably) beg to differ with the corpus analysis.
</bodyText>
<footnote confidence="0.943438">
3http://www.let.rug.nl/vannoord/trees/
</footnote>
<page confidence="0.982882">
300
</page>
<figure confidence="0.95894775">
20 30 40 50 60 70
Accuracy on all tokens
20 30 40 50 60 70
Accuracy on all tokens
0 1000 2000 3000 4000
Morphemes annotated
0 5000 10000 15000 20000 25000
Cumulative annotation time
</figure>
<table confidence="0.971434125">
Non−Expert, No Suggest, Sequential
Non−Expert, Suggest, Random
Expert, No Suggest, Sequential
Expert, No Suggest, Uncertainty
Non−Expert, No Suggest, Sequential
Non−Expert, Suggest, Random
Expert, No Suggest, Sequential
Expert, No Suggest, Uncertainty
</table>
<figure confidence="0.988435">
(a) (b)
</figure>
<figureCaption confidence="0.999088">
Figure 2: A sample of the learning curves with (a) morpheme cost and (b) time cost. Morpheme cost
</figureCaption>
<bodyText confidence="0.745001">
ranks strategies for a given annotator similarly to time cost, but it gives dramatically different results
from time cost when used to compare different annotators.
</bodyText>
<sectionHeader confidence="0.970399" genericHeader="method">
5 Annotation experiments
</sectionHeader>
<bodyText confidence="0.999987807692308">
With two annotators (expert, non-expert), three
selection methods (seq, rand, unc), and two ma-
chine labeling settings (ns, ds), we obtain 12 dif-
ferent experiments. Each experiment measures ac-
curacy in terms of all words and unknown words
and cost in terms of clauses, morphemes and time;
this produces six views on every experiment. In
this paper we focus on one view: accuracy over all
words with time-based evaluation of cost.
As with the simulations, clause cost in the an-
notation experiments overestimates the cost reduc-
tions. For morpheme cost, the annotation experi-
ments show that (a) it also overstates cost reduc-
tions compared to time, and (b) it can mis-state
relative effectiveness when comparing annotators.
The big picture. Figure 2 shows curves for four
experiments: seq-ns for both annotators4 and the
most effective overall condition for each annota-
tor. Figure 2a uses morpheme cost evaluation; on
that metric, both annotators appear to be about
equally effective with seq-ns and much more ef-
fective with machine involvement (unc or ds) than
without. Additionally, the non-expert’s rand-ds
appears to beat the expert’s unc-ns. However, the
time cost evaluation in Figure 2b tells a dramat-
ically different story. Each annotator’s machine-
</bodyText>
<footnote confidence="0.6406805">
4Recall that sequential annotation is the default mode for
producing IGT, so this strategy is of particular interest.
</footnote>
<bodyText confidence="0.999874541666667">
involved experiment is much better than their seq-
ns, but now the expert’s best is clearly better than
the non-expert’s. We see this as clear evidence for
the need for cost-sensitive learning over vanilla ac-
tive learning (as we do here).5
The non-expert with rand-ds caught up to and
surpassed the unaided expert in about six hours
total annotation time, and he caught up to her
unc-ns curve after 35 hours. This is encourag-
ing since often language documentation projects
have participants with a wide range of expertise
levels, and these results suggest that assistance
from machine learning, if done properly, may in-
crease the effectiveness of participants with less
language-specific expertise. We are also encour-
aged, with respect to the effectiveness of active
learning, that the expert’s best performance is ob-
tained with uncertainty-based selection.
Within annotator comparisons. Figure 3
shows both actual measurements and the fitted
nonlinear regression curves used to compute
OPER. Figure 3a, the expert without suggestions,
exhibits typical active learning behavior similar to
that seen in the simulation experiments. Figure 3b,
</bodyText>
<footnote confidence="0.962793">
5It is also clear to see that, unsurprisingly, the expert spent
much less time to complete the 56 rounds than the non-expert.
In general, the expert annotator was much quicker, particu-
larly in early rounds, averaging 4.1 seconds per morpheme
annotated against the non-expert’s 8.0 second average. See
Palmer et al. (2009) for more details.
</footnote>
<page confidence="0.996141">
301
</page>
<figure confidence="0.972872096774193">
30 40 50 60 70
Accuracy on all tokens
30 40 50 60 70
Accuracy on all tokens
0 5000 10000 15000 20000 25000 30000
Cumulative annotation time
0 5000 10000 15000 20000 25000 30000
Cumulative annotation time
●
●
●
●
●●●●
●●
●●●●●●●●●●●●●●●●●●●●●●●●●
●● ●●●●●●
●●
● Non−expert, Suggest, Uncertainty
Non−expert, Suggest, Random
Non−expert, Suggest, Sequential
Non−expert, No Suggest, Sequential
●
●
●
●
●
●
● Expert, No Suggest, Uncertainty
Expert, No Suggest, Random
Expert, No Suggest, Sequential
(a) (b)
</figure>
<figureCaption confidence="0.947297666666667">
Figure 3: Sample measurements and fitted nonlinear regression curves for (a) the expert and (b) the
non-expert. Note that the scale is consistent for comparability. The dashed vertical lines indicate 12,500
seconds (about 35 hours), which is the upper limit used in computing OPER values for Table 4.
</figureCaption>
<bodyText confidence="0.998130923076923">
the non-expert with suggestions, shows that in the
ds conditions the non-expert was less effective
with unc. This is not unexpected: uncertainty
selects harder examples that will either take
longer to annotate or are easier to get wrong,
especially if the annotator trusts the classifier and
especially on examples the classifier is uncertain
about. Nonetheless, in all ds cases, the non-expert
performs better than with seq-ns.
OPER. Table 4 provides OPER values from
time 0 to 12,500 seconds (about 35 hours), the
minimum amount of annotation time logged in any
one of the twelve experiments.6 The table mixes
three types of comparison: (1) the boxed values
on the diagonal give OPER for the expert versus
the non-expert given the same selection and sug-
gestion conditions; (2) the upper (right) triangle
gives OPER for the expert versus herself for dif-
ferent conditions; and (3) the lower (left) trian-
gle is the non-expert versus himself. For exam-
ple: (1) the expert obtained an 11.52 OPER versus
the non-expert when both used rand-ns; (2) the
expert obtained a 10.52 OPER by using rand-ds
rather than seq-ns; and (3) the non-expert obtained
a 5.93 OPER over rand-ns by using rand-ds.
A number of patterns emerge. Quite unsurpris-
</bodyText>
<footnote confidence="0.979011">
6Stopping at 12,500 seconds ensures a fair comparison,
for example, between the expert and the non-expert because
it requires no extrapolation of the expert’s performance.
</footnote>
<figure confidence="0.9712625">
❳❳❳❳❳❳
exp
seq-ns
ran
d-ns
unc-ns
seq-ds
ran
d-ds
unc-ds
</figure>
<bodyText confidence="0.924005666666667">
ation of table in the OPER subsection.
plan
ingly, the values on the diagonal show that the ex-
pert is more effective than the non-expert in all
conditions. Also, every other condition is more ef-
fective than seq-ns for both annotators (first row
for the expert, first column for the non-expert).
unc-ns and rand-ds are particularly effective for
the non-expert, giving OPERs of 19.20 and 18.59
over seq-ns, respectively. These reductions, big-
ger than the
reductions of 14.17 and 10.52
for the same conditions, considerably reduce the
large gap in seq-ns effectiveness between the two
annotators (see Figure 2b).
The expert actually gains very little from ds for
both rand and unc: adding suggestions gave OP-
ERs of just 1.83 and .39, respectively. In con-
trast, the non-expert obtains an
Table 4: Overall percentage error reduction
(OPER) comparisons, with timing cost. See ex-
expert’s
improvement of
5.93 OPER when suggestions are used with rand,
</bodyText>
<figure confidence="0.557212833333333">
non-exp
11.19 -2.62 -9.91 1.06 -9.09 19.13
d-ds unc-ds
seq-ns rand-ns unc-ns seq-ds ran 8.85 14.17 6.34 10.52 14.50
15.99
13.46 11.52 5.83 -2.76 1.83 6.20
19.20 6.63 10.76 -9.12 -4.25 0.39
10.24 -3.72 -11.09 12.34
18.59 5.93 -0.76 9.30
4.46 8.72
4.45
7.67
</figure>
<page confidence="0.997113">
302
</page>
<bodyText confidence="0.99991252">
but performs worse when used with unc (-9.91
OPER). Even more striking: the non-expert’s
unc-ds is worse than rand-ns (-2.62 OPER), a
completely model-free setting. These variations
demonstrate the importance of modeling annotator
fallibility and sensitivity to cost, as well as char-
acteristics of the annotation task itself, if learner-
guided selection and suggestion are to be used
(Donmez and Carbonell, 2008; Arora et al., 2009).
Annotator accuracy. Another factor which
must be considered when annotation is done by
human annotators (rather than being simulated)
is the accuracy of the humans’ labels. Table 5
shows the overall accuracy of the annotators’ la-
bels for each condition (after 56 rounds) as mea-
sured against the original OKMA annotations.
Unsurprisingly, unc selection picks examples that
are more difficult to annotate: accuracy for both
annotators suffers in both unc-ns and unc-ds.
It may seem surprising that the non-expert’s ac-
curacies are generally higher than the expert’s.
The main reason for this is that the non-expert
took nearly twice as long to annotate his examples,
so each one was done with more care. However,
this difference also highlights challenges that arise
when we bring active learning into non-simulated
annotation contexts. The typical assumption is
that gold standard labeled data represents a true,
fixed target, against which annotator or machine-
predicted labels should be measured. In language
documentation, though, the analysis of the lan-
guage is continually evolving, and analysis and
annotation each inform the other. In fact, the ex-
pert recognized (in the morphological segmenta-
tion) several linguistic phenomena for which the
analysis has changed since the original OKMA an-
notations were done. As she changed her analy-
ses, her labels diverged from those of the original
corpus—another reason for her “lower” accuracy.
This is to say that the ground truth of the current
OKMA annotations we had to work with can be
viewed as one (valid) stage in the iterative reanal-
ysis process that language documentation is.
Error analysis. Preliminary analysis of ‘errors’
made by the annotators supports the idea that
the results seen in Table 5 are heavily influenced
by changes in the expert’s analysis of the lan-
guage. Some duplicate clause annotation oc-
curred for each annotator, because each of the
twelve annotator-selection-suggestion conditions
</bodyText>
<table confidence="0.997794428571429">
expert non-expert
seq-ns 73.17% 75.09%
rand-ns 69.90% 74.37%
unc-ns 61.23% 60.04%
seq-ds 67.48% 73.13%
rand-ds 68.34% 73.03%
unc-ds 59.79% 60.27%
</table>
<tableCaption confidence="0.9671825">
Table 5: Overall accuracy of annotators’ labels,
measured against OKMA annotations.
</tableCaption>
<bodyText confidence="0.9999464">
drew from the same global set of unlabeled ex-
amples. This duplication allows us to measure
the consistency of each annotator on labeling such
duplicate clauses. Table 6 shows the percentage
of morphemes labeled consistently by each anno-
tator. Numbers for the expert appear in the top
(right) triangle, and for the non-expert in the bot-
tom (left) triangle. Overall intra-annotator consis-
tency is much higher for the expert (88.38%) than
for the non-expert (81.64%), suggesting that the
expert maintained a more consistent mental model
of the language, but one which disagrees in some
areas with the original annotations.
Another key error source comes from differ-
ences in use of one individual label: the annota-
tors could assign a label that does not appear in
the original corpus. This is yet another issue that
does not—in fact, cannot—arise in simulated ac-
tive learning. The label ESP was introduced for la-
beling Spanish loans or insertions (such as the dis-
course marker entonces) which do not have a clear
function in Uspanteko grammar. Such tokens are
inconsistently labeled in the original corpus, usu-
ally with catch-all categories like particle or ad-
verb. The annotators felt that the best analysis was
to mark the tokens as of Spanish origin. The expert
annotator used the ESP label for 2086 of 24129 to-
kens (8.65%) versus 221 of 22819 tokens (0.97%)
for the non-expert. Any such token labeled with
ESP is scored as incorrect when compared to the
OKMA standard, so this label alone accounts for
more than 7% of the expert annotator’s total error.
Finally, Table 7 presents inter-annotator agree-
ment measured as percent agreement on mor-
phemes in clauses labeled by both annotators.
Note that in general agreement seems to be low-
est for clauses duplicated in unc conditions, sup-
porting the expected result that uncertainty-based
selection does indeed select clauses that are more
difficult for human annotators to label.
</bodyText>
<page confidence="0.997147">
303
</page>
<table confidence="0.999798888888889">
exp seq-ns rand-ns unc-ns seq-ds rand-ds unc-ds
PPPPPP
non
seq-ns — 95.00% (41) 87.10% (56) 92.39% (60) 91.02% (28) 88.83% (51)
rand-ns 90.11% (49) — 90.91% (57) 87.57% (35) 90.94% (50) 89.53% (57)
unc-ns 80.80% (44) 81.68% (54) — 81.35% (41) 89.10% (40) 87.82% (332)
seq-ds 90.00% (54) 87.94% 77.97% (48) — 86.13% (42) 82.14% (42)
rand-ds 90.15% (52) 86.64% 79.46% (62) 81.43% (44) — 87.06% (49)
unc-ds 84.15% (47) 78.55% (52) 77.68% (328) 78.81% (35) 77.95% (60) —
</table>
<tableCaption confidence="0.993457">
Table 6: Annotation consistency, expert and non-expert, (number of duplicate clauses, of 560 possible)
</tableCaption>
<table confidence="0.999795555555556">
exp seq-ns rand-ns unc-ns seq-ds rand-ds unc-ds
PPPPPP
non
seq-ns 69.91% (523) 70.82% (42) 62.42% (48) 72.35% (54) 74.25% (28) 67.82% (47)
rand-ns 71.32% (48) 83.94% (39) 66.56% (47) 66.15% (43) 73.75% (42) 67.55% (52)
unc-ns 66.31% (48) 67.87% (53) 62.31% (301) 58.87% (51) 73.31% (40) 61.10% (298)
seq-ds 73.35% (60) 75.56% (34) 56.39% (37) 60.02% (540) 66.00% (44) 61.01% (36)
rand-ds 68.67% (50) 76.40% (63) 66.67% (58) 65.88% (47) 76.33% (42) 66.99% (64)
unc-ds 65.41% (50) 67.98% (55) 60.43% (263) 58.13% (38) 70.74% (57) 60.40% (275)
</table>
<tableCaption confidence="0.9546145">
Table 7: IAA: expert v. non-expert, percentage of morphemes in agreement, (number of duplicate
clauses, of 560 possible)
</tableCaption>
<sectionHeader confidence="0.998061" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994962264151">
Through actual annotation experiments that con-
trol for several factors, we have evaluated the po-
tential of incorporating active learning and label
suggestions to speed up morpheme glossing in a
realistic language documentation context. Some
configurations of learner-guided example selec-
tion and machine label suggestions perform far
better than the standard strategy of sequential se-
lection without suggestions. However, the effec-
tiveness of any given strategy depends on annota-
tor expertise. The impact of differences between
annotators directly bears on the point made by
Donmez and Carbonell (2008) that if cost reduc-
tions are to be reliably obtained with active learn-
ing techniques, annotators’ fallibility, unreliabil-
ity, and sensitivity to cost must be modeled.
Our results suggest some possible prescriptions
for tuning techniques according to annotator ex-
pertise. However, even if we can estimate a rela-
tive level of expertise, following such broad pre-
scriptions is unlikely to be more robust than an ap-
proach which adapts selection and suggestion to
the individual annotator, perhaps working within
an annotation group. Indeed, it seems that dealing
with variation in annotators/oracles may be more
important than devising better selection strategies.
The difference in performance due to expertise
suggests that using multiple annotators to check
relative annotation rate and accuracy of different
annotators could be a key ingredient in any actu-
ally deployed active learning system. This could
provide for better modeling of individual anno-
tators as part of an annotation group they can be
compared against, allowing the system, for exam-
ple, to throttle active selection if an annotator ap-
pears to be too slow or inaccurate.
Another major issue we highlight is the uncer-
tainty around the question of whether active learn-
ing works in practical applications. Respondents
to the survey of Tomanek and Olsson (2009) in-
dicated that this uncertainty—will active learn-
ing work? what methods or techniques will work
best?—is one of the reasons active learning is not
widely used in actual annotation. In addition, cre-
ating the necessary software infrastructure to build
an active learning enabled annotation system—
a system which must interface robustly between
data, annotator, and machine classifier, yet still
be easy to use—is a substantial hurdle. It seems
unlikely that there will be much uptake until a)
consistent, large cost reductions can be shown in
actual annotation studies, and b) appropriate, tun-
able, widely-available software exists.
</bodyText>
<sectionHeader confidence="0.999316" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.542557833333333">
This work is funded by NSF grant BCS 06651988
“Reducing Annotation Effort in the Documenta-
tion of Languages using Machine Learning and
Active Learning.” Thanks to Eric Campbell, Ka-
trin Erk, Michel Jacobson, Taesun Moon, Telma
Kaan Pixabaj, and Elias Ponvert.
</bodyText>
<page confidence="0.999279">
304
</page>
<sectionHeader confidence="0.995865" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901688888889">
Shilpa Arora, Eric Nyberg, and Carolyn P. Ros´e. 2009.
Estimating annotation cost for active learning in a
multi-annotator environment. In Proceedings of the
NAACL HLT Workshop on Active Learning for Nat-
ural Language Processing, pages 18–26, Boulder,
CO.
Jason Baldridge and Miles Osborne. 2008. Active
learning and logarithmic opinion pools for HPSG
parse selection. Natural Language Engineering,
14(2):199–222.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149–164, New York City, June. Association
for Computational Linguistics.
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1995. Active learning with statistical mod-
els. In G. Tesauro, D. Touretzky, and T. Leen, ed-
itors, Advances in Neural Information Processing
Systems, volume 7, pages 705–712. The MIT Press.
David Crystal. 2000. Language Death. Cambridge
University Press, Cambridge.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proceedings of the 10th Conference of the
European Association for Computational Linguis-
tics, pages 91–98.
Pinar Donmez and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of
CIKM08, Napa Valley, CA.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the 9th Confer-
ence on Computational Natural Language Learning,
Ann Arbor, MI.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger,
and James L. Carroll. 2008. Return on invest-
ment for active learning. In Proceedings of the NIPS
Workshop on Cost-Sensitive Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional linguistics, 19:313–330.
Prem Melville and Raymond J. Mooney. 2004. Di-
verse ensembles for active learning. In Proceed-
ings of the 21st International Conference on Ma-
chine Learning, pages 584–591, Banff, Canada.
Grace Ngai and David Yarowsky. 2000. Rule writ-
ing or annotation: cost-efficient resource usage for
base noun phrase chunking. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 117–125, Hong Kong.
Alexis Palmer, Taesun Moon, and Jason Baldridge.
2009. Evaluating automation strategies in language
documentation. In Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 36–44, Boulder, CO.
Telma Can Pixabaj, Miguel Angel Vicente M´endez,
Mar´ıa Vicente M´endez, and Oswaldo Ajcot Dami´an.
2007. Text Collections in Four Mayan Languages.
Archived in The Archive of the Indigenous Lan-
guages of Latin America.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA.
Christian Ritz and Jens Carl Streibig. 2008. Nonlinear
Regression with R. Springer.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Pro-
ceedings of the NIPS Workshop on Cost-Sensitive
Learning.
Burr Settles. 2009. Active learning literature survey.
Technical Report Computer Sciences Technical Re-
port 1648, University of Wisconsin-Madison.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP 2008,
pages 254–263.
Katrin Tomanek and Fredrik Olsson. 2009. A Web
Survey on the Use of Active learning to support an-
notation of text data. In Proceedings of the NAACL
HLT Workshop on Active Learning for Natural Lan-
guage Processing, pages 45–48, Boulder, CO.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2008. Multi-level active prediction of useful im-
age annotations for recognition. In Proceedings of
NIPS08, Vancouver, Canada.
</reference>
<page confidence="0.999238">
305
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.737900">
<title confidence="0.821109">well does active learning Time-based evaluation cost-reduction strategies for language documentation.</title>
<author confidence="0.989703">Jason</author>
<affiliation confidence="0.9943265">Department of The University of Texas at</affiliation>
<email confidence="0.997981">jbaldrid@mail.utexas.edu</email>
<abstract confidence="0.998664789473684">Machine involvement has the potential to speed up language documentation. We assess this potential with timed annotation experiments that consider annotator expertise, example selection methods, and suggestions from a machine classifier. We find that better example selection and label suggestions improve efficiency, but effectiveness depends strongly on annotator expertise. Our expert performed best with uncertainty selection, but gained little from suggestions. Our non-expert performed best with random selection and suggestions. The results underscore the importance both of measuring annotation cost reductions with respect to time and of the need for cost-sensitive learning methods that adapt to annotators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shilpa Arora</author>
<author>Eric Nyberg</author>
<author>Carolyn P Ros´e</author>
</authors>
<title>Estimating annotation cost for active learning in a multi-annotator environment.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing,</booktitle>
<pages>18--26</pages>
<location>Boulder, CO.</location>
<marker>Arora, Nyberg, Ros´e, 2009</marker>
<rawString>Shilpa Arora, Eric Nyberg, and Carolyn P. Ros´e. 2009. Estimating annotation cost for active learning in a multi-annotator environment. In Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 18–26, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning and logarithmic opinion pools for HPSG parse selection.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="13754" citStr="Baldridge and Osborne (2008)" startWordPosition="2137" endWordPosition="2140">ed fifty-six rounds of annotation. See Palmer et al. (2009) for more details on the annotation setup. 298 Measuring annotation cost. Active learning studies usually simulate annotation and use a unit cost assumption that each word, sentence, constituent, document, etc. takes the same time to annotate. This is often the only option since corpora typically do not retain annotation time, but it is likely to exaggerate the annotation cost reductions achieved. This is exacerbated with active learning: the informative examples it seeks to find are typically harder to annotate (Hachey et al., 2005). Baldridge and Osborne (2008) correlate a unit cost in terms of discriminants (decisions made by annotators about valid parses) to annotation time. This is a better approximation than unit costs where such a relationship cannot be established. However, it is based on a static measurement of annotation time, and clearly the time taken to annotate an example is not a function of the example alone. Annotation time is actually dynamic in that it is dependent on how many and what kinds of examples have already been annotated. An “informative” example is likely to take longer to annotate if selected early than it would after th</context>
</contexts>
<marker>Baldridge, Osborne, 2008</marker>
<rawString>Jason Baldridge and Miles Osborne. 2008. Active learning and logarithmic opinion pools for HPSG parse selection. Natural Language Engineering, 14(2):199–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="7982" citStr="Buchholz and Marsi, 2006" startWordPosition="1210" endWordPosition="1214"> MORPH: kita’ t-in-ch’abe-j laj in-yol-j iin GLOSS: NEG INC-E1S-hablar-SC PREP A1S-idioma-SC yo POS: PART TAM-PERS-VT-SUF PREP PERS-S-SUF PRON TRANS: ‘No le hablo en mi idioma.’ We use a single layer that is a combination of the GLOSS and POS layers (Palmer et al., 2009). For (1), the morphemes and labels for our task are: We also consider POS-tagging for Danish, Dutch, English, and Swedish; the English is from sections 00-05 (as training set) and 19-21 (as development set) of the Penn Treebank (Marcus et al., 1993), and the other languages are from the CoNLL-X dependency parsing shared task (Buchholz and Marsi, 2006).1 We split the original training data into training and development sets. Table 1 shows the number of words and sentences in each split of each dataset, as well as the number of possible labels and the average sentence length. The Uspanteko data is counted in morphemes rather than words; also, the Uspanteko texts are divided at the clause rather than sentence level. This gives the corpus a much lower average clause length than the other languages (Table 1). 1The subset of the Penn Treebank was chosen to be of comparable size to the CoNLL datasets. -j SC (2) kita’ laj PREP inA1S tNEG INC iin P</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cohn</author>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Active learning with statistical models.</title>
<date>1995</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>7</volume>
<pages>705--712</pages>
<editor>In G. Tesauro, D. Touretzky, and T. Leen, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="10420" citStr="Cohn et al., 1995" startWordPosition="1611" endWordPosition="1614"> sample selection methods: sequential, random, and uncertainty. Sequential selection is important to consider as it is the default in documentary projects. It is sub-optimal for corpora with contiguous sub-domains, since it necessitates working through many similar examples before getting to possibly more informative examples. Random selection is a model-free method that avoids the subdomain trap by sampling freely from the entire corpus. It generally works better than sequential selection and provides a strong baseline against which to compare learner-guided selection. Uncertainty selection (Cohn et al., 1995) identifies examples the model is least confident about. We measure uncertainty as the entropy of the label distribution predicted by the maximum entropy model for each example. Uncertainty for a clause is calculated as the average entropy per morpheme; clauses with the highest average entropy are selected for labeling. A recent development in active learning is costsensitive selection that is guided not only by the learner but also by the expected cost of labeling an example based on its likely complexity and/or the reliability of the annotator. Settles et al. (2008) provide empirical validat</context>
</contexts>
<marker>Cohn, Ghahramani, Jordan, 1995</marker>
<rawString>David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. 1995. Active learning with statistical models. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information Processing Systems, volume 7, pages 705–712. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Crystal</author>
</authors>
<title>Language Death.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="5533" citStr="Crystal, 2000" startWordPosition="823" endWordPosition="825">ption—even at a finegrained level—can dramatically misrepresent the actual effectiveness of different strategies. 2 Task and data Annotation task: language documentation The amount of money spent on obtaining human annotations is an extremely important concern in much language annotation. However, there is a further urgency for annotation in the case of language documentation: languages are dying at the rate of two each month. By the end of this century, half of the approximately 6000 extant spoken languages will cease to be transmitted effectively from one generation of speakers to the next (Crystal, 2000). Recorded and transcribed texts annotated with detailed linguistic information create an important multi-faceted record of these languages, but there are few trained linguists with adequate time and appropriate levels of funding relative to the size of the problem. Annotation cost—in both time and money—is thus keenly felt in the work of documenting and describing endangered languages. Active learning and automated label suggestions could help deal with this language documentation bottleneck. We focus on one stage of language documentation, the production of interlinear glossed text (IGT), a </context>
</contexts>
<marker>Crystal, 2000</marker>
<rawString>David Crystal. 2000. Language Death. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="9032" citStr="Curran and Clark, 2003" startWordPosition="1396" endWordPosition="1399">than the other languages (Table 1). 1The subset of the Penn Treebank was chosen to be of comparable size to the CoNLL datasets. -j SC (2) kita’ laj PREP inA1S tNEG INC iin PRON ch’abe VT inE1S yol S -j SC 297 3 Model and methods Classification model. We use a standard maximum entropy classifier for tagging Danish, Dutch, English, and Swedish words with POS-tags and tagging Uspanteko morphemes with Gloss/POS tags. The label for a word/morpheme is predicted based on the word/morpheme itself plus a window of two units before and after. Standard part-of-speech tagging features (Ratnaparkhi, 1998; Curran and Clark, 2003) are extracted from the morpheme to help with predicting labels for previously unseen morphemes. This is a strong but standard model; better, more complex models could be used, but the gains are likely to be small. Thus, we opted for simplicity in our model so as to focus more on the interaction between the annotator and different levels of machine involvement. The accuracy of the tagger on the datasets when trained on all available training material is given in the following table, along with accuracy of a unigram model (learned from the training set and constrained by a tag dictionary for kn</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the 10th Conference of the European Association for Computational Linguistics, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinar Donmez</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Proactive learning: Cost-sensitive active learning with multiple imperfect oracles.</title>
<date>2008</date>
<booktitle>In Proceedings of CIKM08,</booktitle>
<location>Napa Valley, CA.</location>
<contexts>
<context position="28362" citStr="Donmez and Carbonell, 2008" startWordPosition="4538" endWordPosition="4541">-ds unc-ds seq-ns rand-ns unc-ns seq-ds ran 8.85 14.17 6.34 10.52 14.50 15.99 13.46 11.52 5.83 -2.76 1.83 6.20 19.20 6.63 10.76 -9.12 -4.25 0.39 10.24 -3.72 -11.09 12.34 18.59 5.93 -0.76 9.30 4.46 8.72 4.45 7.67 302 but performs worse when used with unc (-9.91 OPER). Even more striking: the non-expert’s unc-ds is worse than rand-ns (-2.62 OPER), a completely model-free setting. These variations demonstrate the importance of modeling annotator fallibility and sensitivity to cost, as well as characteristics of the annotation task itself, if learnerguided selection and suggestion are to be used (Donmez and Carbonell, 2008; Arora et al., 2009). Annotator accuracy. Another factor which must be considered when annotation is done by human annotators (rather than being simulated) is the accuracy of the humans’ labels. Table 5 shows the overall accuracy of the annotators’ labels for each condition (after 56 rounds) as measured against the original OKMA annotations. Unsurprisingly, unc selection picks examples that are more difficult to annotate: accuracy for both annotators suffers in both unc-ns and unc-ds. It may seem surprising that the non-expert’s accuracies are generally higher than the expert’s. The main reas</context>
<context position="34352" citStr="Donmez and Carbonell (2008)" startWordPosition="5487" endWordPosition="5490">0 possible) 6 Conclusion Through actual annotation experiments that control for several factors, we have evaluated the potential of incorporating active learning and label suggestions to speed up morpheme glossing in a realistic language documentation context. Some configurations of learner-guided example selection and machine label suggestions perform far better than the standard strategy of sequential selection without suggestions. However, the effectiveness of any given strategy depends on annotator expertise. The impact of differences between annotators directly bears on the point made by Donmez and Carbonell (2008) that if cost reductions are to be reliably obtained with active learning techniques, annotators’ fallibility, unreliability, and sensitivity to cost must be modeled. Our results suggest some possible prescriptions for tuning techniques according to annotator expertise. However, even if we can estimate a relative level of expertise, following such broad prescriptions is unlikely to be more robust than an approach which adapts selection and suggestion to the individual annotator, perhaps working within an annotation group. Indeed, it seems that dealing with variation in annotators/oracles may b</context>
</contexts>
<marker>Donmez, Carbonell, 2008</marker>
<rawString>Pinar Donmez and Jaime G. Carbonell. 2008. Proactive learning: Cost-sensitive active learning with multiple imperfect oracles. In Proceedings of CIKM08, Napa Valley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Beatrice Alex</author>
<author>Markus Becker</author>
</authors>
<title>Investigating the effects of selective sampling on the annotation task.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Computational Natural Language Learning,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="13724" citStr="Hachey et al., 2005" startWordPosition="2133" endWordPosition="2136">oth annotators completed fifty-six rounds of annotation. See Palmer et al. (2009) for more details on the annotation setup. 298 Measuring annotation cost. Active learning studies usually simulate annotation and use a unit cost assumption that each word, sentence, constituent, document, etc. takes the same time to annotate. This is often the only option since corpora typically do not retain annotation time, but it is likely to exaggerate the annotation cost reductions achieved. This is exacerbated with active learning: the informative examples it seeks to find are typically harder to annotate (Hachey et al., 2005). Baldridge and Osborne (2008) correlate a unit cost in terms of discriminants (decisions made by annotators about valid parses) to annotation time. This is a better approximation than unit costs where such a relationship cannot be established. However, it is based on a static measurement of annotation time, and clearly the time taken to annotate an example is not a function of the example alone. Annotation time is actually dynamic in that it is dependent on how many and what kinds of examples have already been annotated. An “informative” example is likely to take longer to annotate if selecte</context>
</contexts>
<marker>Hachey, Alex, Becker, 2005</marker>
<rawString>Ben Hachey, Beatrice Alex, and Markus Becker. 2005. Investigating the effects of selective sampling on the annotation task. In Proceedings of the 9th Conference on Computational Natural Language Learning, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbie A Haertel</author>
<author>Kevin D Seppi</author>
<author>Eric K Ringger</author>
<author>James L Carroll</author>
</authors>
<title>Return on investment for active learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIPS Workshop on Cost-Sensitive Learning.</booktitle>
<contexts>
<context position="4768" citStr="Haertel et al., 2008" startWordPosition="703" endWordPosition="706">v Avg.sent Avg.tr.sent Avg.dev.sent Danish 62825 31561 10 3570 1618 18.18 17.60 19.50 Dutch 129586 65483 13 9365 3982 14.61 13.84 16.44 English 167593 131768 45 6945 5527 24.00 24.13 23.84 Swedish 127684 63783 41 7326 3714 17.34 17.43 17.17 Uspanteko 43473 19906 69 7423 3288 5.92 5.86 6.05 Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length. gestions. This difference confirms the importance of cost-sensitive active learning strategies that are not just learner-guided, but also take into account modeling of the annotators (Settles et al., 2008; Haertel et al., 2008; Vijayanarasimhan and Grauman, 2008). Finally, we confirm the importance of using actual annotation time to measure annotation cost: a unit-cost assumption—even at a finegrained level—can dramatically misrepresent the actual effectiveness of different strategies. 2 Task and data Annotation task: language documentation The amount of money spent on obtaining human annotations is an extremely important concern in much language annotation. However, there is a further urgency for annotation in the case of language documentation: languages are dying at the rate of two each month. By the end of this</context>
<context position="11396" citStr="Haertel et al. (2008)" startWordPosition="1767" endWordPosition="1770">arning is costsensitive selection that is guided not only by the learner but also by the expected cost of labeling an example based on its likely complexity and/or the reliability of the annotator. Settles et al. (2008) provide empirical validation for cost-related intuitions; for example, that cost of annotation is static neither per example nor per annotator. Also, they show that taking annotation cost into account can improve active learning effectiveness, but that learning to predict annotation cost is not yet wellunderstood. A cost-sensitive Return on Investment heuristic is developed in Haertel et al. (2008) and tested in a simulated POS-tagging context. Our experiments do not employ cost-sensitive selection, but our results—from live (non-simulated) active learning experiments of real-world scale— empirically support the need to consider costsensitive selection if better cost reductions are to be achieved. Annotation setup. We compare results from two annotators with different levels of exposure to Uspanteko. Both are documentary linguists with extensive field experience. Our expert annotator is a native speaker of K’ichee’, a closely related Mayan language, and has worked extensively on Uspante</context>
</contexts>
<marker>Haertel, Seppi, Ringger, Carroll, 2008</marker>
<rawString>Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger, and James L. Carroll. 2008. Return on investment for active learning. In Proceedings of the NIPS Workshop on Cost-Sensitive Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<pages>19--313</pages>
<contexts>
<context position="7878" citStr="Marcus et al., 1993" startWordPosition="1194" endWordPosition="1197">ablar or idioma). The following is an Uspanteko example: (1) TEXT: Kita’ tinch’ab’ej laj inyolj iin MORPH: kita’ t-in-ch’abe-j laj in-yol-j iin GLOSS: NEG INC-E1S-hablar-SC PREP A1S-idioma-SC yo POS: PART TAM-PERS-VT-SUF PREP PERS-S-SUF PRON TRANS: ‘No le hablo en mi idioma.’ We use a single layer that is a combination of the GLOSS and POS layers (Palmer et al., 2009). For (1), the morphemes and labels for our task are: We also consider POS-tagging for Danish, Dutch, English, and Swedish; the English is from sections 00-05 (as training set) and 19-21 (as development set) of the Penn Treebank (Marcus et al., 1993), and the other languages are from the CoNLL-X dependency parsing shared task (Buchholz and Marsi, 2006).1 We split the original training data into training and development sets. Table 1 shows the number of words and sentences in each split of each dataset, as well as the number of possible labels and the average sentence length. The Uspanteko data is counted in morphemes rather than words; also, the Uspanteko texts are divided at the clause rather than sentence level. This gives the corpus a much lower average clause length than the other languages (Table 1). 1The subset of the Penn Treebank </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prem Melville</author>
<author>Raymond J Mooney</author>
</authors>
<title>Diverse ensembles for active learning.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning,</booktitle>
<pages>584--591</pages>
<location>Banff, Canada.</location>
<contexts>
<context position="15342" citStr="Melville and Mooney, 2004" startWordPosition="2405" endWordPosition="2408"> Ngai and Yarowsky (2000). In the simulation studies, as we are unable to measure time, we measure cost by sentence/clause and word/morpheme. Learning curve comparison. We are interested in comparative evaluation of many different experimental settings, across which we vary selection methods, use of label suggestions, and annotators. To achieve this, it is useful to have a summary value for comparing the results from two individual experiments. One such measure is the percentage error reduction (PER), measured over a discrete set of points on the first 20% of the points on the learning curve (Melville and Mooney, 2004).2 We use a new related measure, which we call the overall percentage error reduction (OPER), that uses the entire area under the curves given by 2This is justified in standard conditions, sampling from a finite corpus: active learning runs out of interesting examples after considering a fraction of the data, so the curve is artificially pulled down by the remaining, boring examples. fitted nonlinear regression models rather than averaging over a subset of data points. Specifically, we fit a modified Michaelis-Menton model: Um(A + cost) f(cost, (K, Um, A)) = K + cost The (original) parameters </context>
</contexts>
<marker>Melville, Mooney, 2004</marker>
<rawString>Prem Melville and Raymond J. Mooney. 2004. Diverse ensembles for active learning. In Proceedings of the 21st International Conference on Machine Learning, pages 584–591, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>117--125</pages>
<location>Hong Kong.</location>
<contexts>
<context position="14741" citStr="Ngai and Yarowsky (2000)" startWordPosition="2305" endWordPosition="2308">alone. Annotation time is actually dynamic in that it is dependent on how many and what kinds of examples have already been annotated. An “informative” example is likely to take longer to annotate if selected early than it would after the annotator has seen many other examples. Thus, it is important to measure annotation time embedded in the context of a particular annotation experiment with the sample selection/labeling strategies of interest. In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). In the simulation studies, as we are unable to measure time, we measure cost by sentence/clause and word/morpheme. Learning curve comparison. We are interested in comparative evaluation of many different experimental settings, across which we vary selection methods, use of label suggestions, and annotators. To achieve this, it is useful to have a summary value for comparing the results from two individual experiments. One such measure is the percentage error reduction (PER), measured over a discrete set of points on the first 20% of the points on the learning curve (Melville and Mooney, 2004</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: cost-efficient resource usage for base noun phrase chunking. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 117–125, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Palmer</author>
<author>Taesun Moon</author>
<author>Jason Baldridge</author>
</authors>
<title>Evaluating automation strategies in language documentation.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing,</booktitle>
<pages>36--44</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="6664" citStr="Palmer et al. (2009)" startWordPosition="1000" endWordPosition="1003">one stage of language documentation, the production of interlinear glossed text (IGT), a standard form of annotation that involves both morphological and grammatical analysis. IGT is generally created following transcription and translation of recorded speech, with the annotations often being provided by trained annotators with varying levels of expertise. The result is generally a small amount of IGT annotated data and a greater amount of unannotated data. Data We use a collection of 32 interlinear glossed texts (IGT) in the Mayan language Uspanteko. This corpus was cleaned up and adapted by Palmer et al. (2009) from an original collection of 67 texts that were collected, transcribed, translated and annotated by the OKMA language documentation project (Pixabaj et al., 2007). Two core tasks in creating IGT are morphological analysis and tagging morphemes with their glosses (labels indicating part-of-speech and/or grammatical function). We deal with the latter task and assume texts are morphologically segmented. Standard four-line IGT has morphemes on one line and their glosses on the next. The gloss line includes labels for grammatical morphemes (e.g. PL or COM) and translations of stems (e.g. hablar </context>
<context position="13185" citStr="Palmer et al. (2009)" startWordPosition="2047" endWordPosition="2050">an half that of the best label. For ns, the annotator sees a frequency-ranked list of labels previously seen in training data for the given morpheme. Annotators improve as they see more examples. To minimize the impact of this learning process, annotation is done in rounds. Each round consists of sixty clauses—six batches of ten each for the six experimental cases. The annotator is free to break between batches. Following annotation, the newly-labeled clauses are added to the training data, and a new model is trained and evaluated. Both annotators completed fifty-six rounds of annotation. See Palmer et al. (2009) for more details on the annotation setup. 298 Measuring annotation cost. Active learning studies usually simulate annotation and use a unit cost assumption that each word, sentence, constituent, document, etc. takes the same time to annotate. This is often the only option since corpora typically do not retain annotation time, but it is likely to exaggerate the annotation cost reductions achieved. This is exacerbated with active learning: the informative examples it seeks to find are typically harder to annotate (Hachey et al., 2005). Baldridge and Osborne (2008) correlate a unit cost in terms</context>
<context position="24463" citStr="Palmer et al. (2009)" startWordPosition="3904" endWordPosition="3907">ased selection. Within annotator comparisons. Figure 3 shows both actual measurements and the fitted nonlinear regression curves used to compute OPER. Figure 3a, the expert without suggestions, exhibits typical active learning behavior similar to that seen in the simulation experiments. Figure 3b, 5It is also clear to see that, unsurprisingly, the expert spent much less time to complete the 56 rounds than the non-expert. In general, the expert annotator was much quicker, particularly in early rounds, averaging 4.1 seconds per morpheme annotated against the non-expert’s 8.0 second average. See Palmer et al. (2009) for more details. 301 30 40 50 60 70 Accuracy on all tokens 30 40 50 60 70 Accuracy on all tokens 0 5000 10000 15000 20000 25000 30000 Cumulative annotation time 0 5000 10000 15000 20000 25000 30000 Cumulative annotation time ● ● ● ● ●●●● ●● ●●●●●●●●●●●●●●●●●●●●●●●●● ●● ●●●●●● ●● ● Non−expert, Suggest, Uncertainty Non−expert, Suggest, Random Non−expert, Suggest, Sequential Non−expert, No Suggest, Sequential ● ● ● ● ● ● ● Expert, No Suggest, Uncertainty Expert, No Suggest, Random Expert, No Suggest, Sequential (a) (b) Figure 3: Sample measurements and fitted nonlinear regression curves for (a)</context>
</contexts>
<marker>Palmer, Moon, Baldridge, 2009</marker>
<rawString>Alexis Palmer, Taesun Moon, and Jason Baldridge. 2009. Evaluating automation strategies in language documentation. In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, pages 36–44, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Telma Can Pixabaj</author>
</authors>
<title>Miguel Angel Vicente M´endez, Mar´ıa Vicente M´endez, and Oswaldo Ajcot Dami´an.</title>
<date>2007</date>
<booktitle>Text Collections in Four Mayan Languages. Archived in The Archive of the Indigenous Languages of Latin</booktitle>
<publisher>America.</publisher>
<marker>Pixabaj, 2007</marker>
<rawString>Telma Can Pixabaj, Miguel Angel Vicente M´endez, Mar´ıa Vicente M´endez, and Oswaldo Ajcot Dami´an. 2007. Text Collections in Four Mayan Languages. Archived in The Archive of the Indigenous Languages of Latin America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="9007" citStr="Ratnaparkhi, 1998" startWordPosition="1394" endWordPosition="1395">rage clause length than the other languages (Table 1). 1The subset of the Penn Treebank was chosen to be of comparable size to the CoNLL datasets. -j SC (2) kita’ laj PREP inA1S tNEG INC iin PRON ch’abe VT inE1S yol S -j SC 297 3 Model and methods Classification model. We use a standard maximum entropy classifier for tagging Danish, Dutch, English, and Swedish words with POS-tags and tagging Uspanteko morphemes with Gloss/POS tags. The label for a word/morpheme is predicted based on the word/morpheme itself plus a window of two units before and after. Standard part-of-speech tagging features (Ratnaparkhi, 1998; Curran and Clark, 2003) are extracted from the morpheme to help with predicting labels for previously unseen morphemes. This is a strong but standard model; better, more complex models could be used, but the gains are likely to be small. Thus, we opted for simplicity in our model so as to focus more on the interaction between the annotator and different levels of machine involvement. The accuracy of the tagger on the datasets when trained on all available training material is given in the following table, along with accuracy of a unigram model (learned from the training set and constrained b</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Ritz</author>
<author>Jens Carl Streibig</author>
</authors>
<title>Nonlinear Regression with R.</title>
<date>2008</date>
<publisher>Springer.</publisher>
<contexts>
<context position="16255" citStr="Ritz and Streibig, 2008" startWordPosition="2559" endWordPosition="2562">fraction of the data, so the curve is artificially pulled down by the remaining, boring examples. fitted nonlinear regression models rather than averaging over a subset of data points. Specifically, we fit a modified Michaelis-Menton model: Um(A + cost) f(cost, (K, Um, A)) = K + cost The (original) parameters Um and K respectively correspond to the horizontal asymptote and the cost where accuracy is halfway between 0 and Um. The additional parameter A allows for a better fit to our data by allowing for less sharp elbows and letting cost be zero. Model parameters were determined with nls in R (Ritz and Streibig, 2008). With the fitted regression models, it is straightforward to calculate the area under the curve between a start cost ci and end cost cj by taking the integral from ci to cj. The overall accuracy for the experiment is given by dividing that area by 100 × (cj − ci). Call this the overall curve accuracy (OCA). Then, for experiment A compared to enment B, OPER A,B oCAA—CCAB ex p ( ) - 100−OCAB . For the simulation experiments we calculate OPER for only the first 20% of cost units, like Melville and Mooney. For the annotation experiments, we calculate it for the minimum amount of time spent on any</context>
</contexts>
<marker>Ritz, Streibig, 2008</marker>
<rawString>Christian Ritz and Jens Carl Streibig. 2008. Nonlinear Regression with R. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
<author>Lewis Friedland</author>
</authors>
<title>Active learning with real annotation costs.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIPS Workshop on Cost-Sensitive Learning.</booktitle>
<contexts>
<context position="4746" citStr="Settles et al., 2008" startWordPosition="699" endWordPosition="702">gs #sents-tr #sents-dev Avg.sent Avg.tr.sent Avg.dev.sent Danish 62825 31561 10 3570 1618 18.18 17.60 19.50 Dutch 129586 65483 13 9365 3982 14.61 13.84 16.44 English 167593 131768 45 6945 5527 24.00 24.13 23.84 Swedish 127684 63783 41 7326 3714 17.34 17.43 17.17 Uspanteko 43473 19906 69 7423 3288 5.92 5.86 6.05 Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length. gestions. This difference confirms the importance of cost-sensitive active learning strategies that are not just learner-guided, but also take into account modeling of the annotators (Settles et al., 2008; Haertel et al., 2008; Vijayanarasimhan and Grauman, 2008). Finally, we confirm the importance of using actual annotation time to measure annotation cost: a unit-cost assumption—even at a finegrained level—can dramatically misrepresent the actual effectiveness of different strategies. 2 Task and data Annotation task: language documentation The amount of money spent on obtaining human annotations is an extremely important concern in much language annotation. However, there is a further urgency for annotation in the case of language documentation: languages are dying at the rate of two each mon</context>
<context position="10994" citStr="Settles et al. (2008)" startWordPosition="1707" endWordPosition="1710">tion. Uncertainty selection (Cohn et al., 1995) identifies examples the model is least confident about. We measure uncertainty as the entropy of the label distribution predicted by the maximum entropy model for each example. Uncertainty for a clause is calculated as the average entropy per morpheme; clauses with the highest average entropy are selected for labeling. A recent development in active learning is costsensitive selection that is guided not only by the learner but also by the expected cost of labeling an example based on its likely complexity and/or the reliability of the annotator. Settles et al. (2008) provide empirical validation for cost-related intuitions; for example, that cost of annotation is static neither per example nor per annotator. Also, they show that taking annotation cost into account can improve active learning effectiveness, but that learning to predict annotation cost is not yet wellunderstood. A cost-sensitive Return on Investment heuristic is developed in Haertel et al. (2008) and tested in a simulated POS-tagging context. Our experiments do not employ cost-sensitive selection, but our results—from live (non-simulated) active learning experiments of real-world scale— emp</context>
</contexts>
<marker>Settles, Craven, Friedland, 2008</marker>
<rawString>Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proceedings of the NIPS Workshop on Cost-Sensitive Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2009</date>
<tech>Technical Report Computer Sciences Technical Report 1648,</tech>
<institution>University of Wisconsin-Madison.</institution>
<contexts>
<context position="2003" citStr="Settles, 2009" startWordPosition="289" endWordPosition="290">se languages. Regardless of the context, creating annotated data is costly in terms of time and/or money. Since both time and money are undeniably in limited supply, there is a widely shared desire to reduce this cost. Reducing cost involves strategies that do more with fewer human-annotated labels and/or reduce the per-label cost. An example of the former is active learning, which focuses annotation effort on data points selected by the learner(s) for their expected utility in developing a more accurate model Alexis Palmer Computational Linguistics Saarland University apalmer@coli.uni-sb.de (Settles, 2009). Examples of the latter include providing suggestions from a machine labeler and using extremely cheap human labelers, e.g. with the Amazon Mechanical Turk (Snow et al., 2008). Different techniques may be more or less applicable depending on the language being annotated, the kind of labels which are desired (tags, syntactic structures, etc.), and the desired use of the annotated data (e.g., for training models, testing linguistic hypotheses, or preserving a language). This paper discusses experiments that measure the effectiveness of machine-aided annotation for language documentation using b</context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>Burr Settles. 2009. Active learning literature survey. Technical Report Computer Sciences Technical Report 1648, University of Wisconsin-Madison.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP 2008, pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Fredrik Olsson</author>
</authors>
<title>A Web Survey on the Use of Active learning to support annotation of text data.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing,</booktitle>
<pages>45--48</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="35673" citStr="Tomanek and Olsson (2009)" startWordPosition="5694" endWordPosition="5697"> expertise suggests that using multiple annotators to check relative annotation rate and accuracy of different annotators could be a key ingredient in any actually deployed active learning system. This could provide for better modeling of individual annotators as part of an annotation group they can be compared against, allowing the system, for example, to throttle active selection if an annotator appears to be too slow or inaccurate. Another major issue we highlight is the uncertainty around the question of whether active learning works in practical applications. Respondents to the survey of Tomanek and Olsson (2009) indicated that this uncertainty—will active learning work? what methods or techniques will work best?—is one of the reasons active learning is not widely used in actual annotation. In addition, creating the necessary software infrastructure to build an active learning enabled annotation system— a system which must interface robustly between data, annotator, and machine classifier, yet still be easy to use—is a substantial hurdle. It seems unlikely that there will be much uptake until a) consistent, large cost reductions can be shown in actual annotation studies, and b) appropriate, tunable, w</context>
</contexts>
<marker>Tomanek, Olsson, 2009</marker>
<rawString>Katrin Tomanek and Fredrik Olsson. 2009. A Web Survey on the Use of Active learning to support annotation of text data. In Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 45–48, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudheendra Vijayanarasimhan</author>
<author>Kristen Grauman</author>
</authors>
<title>Multi-level active prediction of useful image annotations for recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS08,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4805" citStr="Vijayanarasimhan and Grauman, 2008" startWordPosition="707" endWordPosition="711"> Avg.dev.sent Danish 62825 31561 10 3570 1618 18.18 17.60 19.50 Dutch 129586 65483 13 9365 3982 14.61 13.84 16.44 English 167593 131768 45 6945 5527 24.00 24.13 23.84 Swedish 127684 63783 41 7326 3714 17.34 17.43 17.17 Uspanteko 43473 19906 69 7423 3288 5.92 5.86 6.05 Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length. gestions. This difference confirms the importance of cost-sensitive active learning strategies that are not just learner-guided, but also take into account modeling of the annotators (Settles et al., 2008; Haertel et al., 2008; Vijayanarasimhan and Grauman, 2008). Finally, we confirm the importance of using actual annotation time to measure annotation cost: a unit-cost assumption—even at a finegrained level—can dramatically misrepresent the actual effectiveness of different strategies. 2 Task and data Annotation task: language documentation The amount of money spent on obtaining human annotations is an extremely important concern in much language annotation. However, there is a further urgency for annotation in the case of language documentation: languages are dying at the rate of two each month. By the end of this century, half of the approximately 6</context>
</contexts>
<marker>Vijayanarasimhan, Grauman, 2008</marker>
<rawString>Sudheendra Vijayanarasimhan and Kristen Grauman. 2008. Multi-level active prediction of useful image annotations for recognition. In Proceedings of NIPS08, Vancouver, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>