<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.072680">
<title confidence="0.998932">
A Machine Learning Approach to German Pronoun Resolution
</title>
<author confidence="0.993476">
Beata Kouchnir
</author>
<affiliation confidence="0.995525">
Department of Computational Linguistics
T¨ubingen University
</affiliation>
<address confidence="0.698264">
72074 T¨ubingen, Germany
</address>
<email confidence="0.997245">
kouchnir@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948055555556">
This paper presents a novel ensemble
learning approach to resolving German
pronouns. Boosting, the method in
question, combines the moderately ac-
curate hypotheses of several classifiers
to form a highly accurate one. Exper-
iments show that this approach is su-
perior to a single decision-tree classi-
fier. Furthermore, we present a stan-
dalone system that resolves pronouns in
unannotated text by using a fully auto-
matic sequence of preprocessing mod-
ules that mimics the manual annotation
process. Although the system performs
well within a limited textual domain,
further research is needed to make it
effective for open-domain question an-
swering and text summarisation.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938933333333">
Automatic coreference resolution, pronominal and
otherwise, has been a popular research area in
Natural Language Processing for more than two
decades, with extensive documentation of both
the rule-based and the machine learning approach.
For the latter, good results have been achieved
with large feature sets (including syntactic, se-
mantic, grammatical and morphological informa-
tion) derived from handannotated corpora. How-
ever, for applications that work with plain text (e.g.
question answering, text summarisation), this ap-
proach is not practical.
The system presented in this paper resolves
German pronouns in free text by imitating the
manual annotation process with off-the-shelf lan-
guage sofware. As the avalability and reliability of
such software is limited, the system can use only
a small number of features. The fact that most
German pronouns are morphologically ambiguous
proves an additional challenge.
The choice of boosting as the underlying ma-
chine learning algorithm is motivated both by its
theoretical concept as well as its performance for
other NLP tasks. The fact that boosting uses the
method of ensemble learning, i.e. combining the
decisions of several classifiers, suggests that the
combined hypothesis will be more accurate than
one learned by a single classifier. On the practical
side, boosting has distinguished itself by achieving
good results with small feature sets.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999783619047619">
Although extensive research has been conducted
on statistical anaphora resolution, the bulk of
the work has concentrated on the English lan-
guage. Nevertheless, comparing different strate-
gies helped shape the system described in this pa-
per.
(McCarthy and Lehnert, 1995) were among
the first to use machine learning for coreference
resolution. RESOLVE was trained on data from
MUC-5 English Joint Venture (EJV) corpus and
used the C4.5 decision tree algorithm (Quinlan,
1993) with eight features, most of which were tai-
lored to the joint venturte domain. The system
achieved an F-measure of 86.5 for full coreference
resolution (no values were given for pronouns).
Although a number this high must be attributed to
the specific textual domain, RESOLVE also out-
performed the authors’ rule-based algorithm by
7.6 percentage points, which encouraged further
reseach in this direction.
Unlike the other systems presented in this sec-
tion, (Morton, 2000) does not use a decision tree
algorithm but opts instead for a maximum entropy
model. The model is trained on a subset of the
Wall Street Journal, comprising 21 million tokens.
The reported F-measure for pronoun resolution is
81.5. However, (Morton, 2000) only attempts to
resolve singular pronouns, and there is no mention
of what percentage of total pronouns are covered
by this restriction.
(Soon et al., 2001) use the C4.5 algorithm with
a set of 12 domain-independent features, ten syn-
tactic and two semantic. Their system was trained
on both the MUC-6 and the MUC-7 datasets, for
which it achieved F-scores of 62.6 and 60.4, re-
spectively. Although these results are far worse
than the ones reported in (McCarthy and Lehnert,
1995), they are comparable to the best-performing
rule-based systems in the respective competitions.
As (McCarthy and Lehnert, 1995), (Soon et al.,
2001) do not report separate results for pronouns.
(Ng and Cardie, 2002) expanded on the work
of (Soon et al., 2001) by adding 41 lexical, se-
mantic and grammatical features. However, since
using this many features proved to be detrimen-
tal to performance, all features that induced low
precision rules were discarded, leaving only 19.
The final system outperformed that of (Soon et al.,
2001), with F-scores of 69.1 and 63.4 for MUC-6
and MUC-7, respectively. For pronouns, the re-
ported results are 74.6 and 57.8, respectively.
The experiment presented in (Strube et al.,
2002) is one of the few dealing with the applica-
tion of machine learning to German coreference
resolution covering definite noun phrases, proper
names and personal, possessive and demonstrative
pronouns. The research is based on the Heidelberg
Text Corpus (see Section 4), which makes it ideal
for comparison with our system. (Strube et al.,
2002) used 15 features modeled after those used
by state-of-the-art resolution systems for English.
The results for personal and possessive pronouns
are 82.79 and 84.94, respectively.
</bodyText>
<sectionHeader confidence="0.976841" genericHeader="method">
3 Boosting
</sectionHeader>
<bodyText confidence="0.99998371875">
All of the systems described in the previous sec-
tion use a single classifier to resolve coreference.
Our intuition, however, is that a combination of
classifiers is better suited for this task. The con-
cept of ensemble learning (Dietterich, 2000) is
based on the assumption that combining the hy-
potheses of several classifiers yields a hypothesis
that is much more accurate than that of an individ-
ual classifier.
One of the most popular ensemble learning
methods is boosting (Schapire, 2002). It is based
on the observation that finding many weak hy-
potheses is easier than finding one strong hypothe-
sis. This is achieved by running a base learning al-
gorithm over several iterations. Initially, an impor-
tance weight is distributed uniformly among the
training examples. After each iteration, the weight
is redistributed, so that misclassified examples get
higher weights. The base learner is thus forced to
concentrate on difficult examples.
Although boosting has not yet been applied
to coreference resolution, it has outperformed
stateof-the-art systems for NLP tasks such as part-
ofspeech tagging and prepositional phrase attach-
ment (Abney et al., 1999), word sense disam-
biguation (Escudero et al., 2000), and named en-
tity recognition (Carreras et al., 2002).
The implementation used for this project is
BoosTexter (Schapire and Singer, 2000), a toolkit
freely available for research purposes. In addition
to labels, BoosTexter assigns confidence weights
that reflect the reliability of the decisions.
</bodyText>
<sectionHeader confidence="0.992515" genericHeader="method">
4 System Description
</sectionHeader>
<bodyText confidence="0.9993986">
Our system resolves pronouns in three stages:
preprocessing, classification, and postprocessing.
Figure 1 gives an overview of the system archi-
tecture, while this section provides details of each
component.
</bodyText>
<subsectionHeader confidence="0.988379">
4.1 Training and Test Data
</subsectionHeader>
<bodyText confidence="0.996516">
The system was trained with data from the Heidel-
berg Text Corpus (HTC), provided by the Euro-
pean Media Laboratory in Heidelberg, Germany.
</bodyText>
<figureCaption confidence="0.999574">
Figure 1: System Architecture
</figureCaption>
<bodyText confidence="0.99983475">
The HTC is a collection of 250 short texts (30-700
tokens) describing architecture, historical events
and people associated with the city of Heidelberg.
To examine its domain (in)dependence, the system
was tested on 40 unseen HTC texts as well as on
25 articles from the Spiegel magazine, the topics
of which include current events, science, arts and
entertainment, and travel.
</bodyText>
<subsectionHeader confidence="0.98887">
4.2 The MMAX Annotation Tool
</subsectionHeader>
<bodyText confidence="0.998291">
The manual annotation of the training data was
done with the MMAX (Multi-Modal Annotation
in XML) annotation tool (M¨uller and Strube,
2001). The fist step of coreference annotation is to
identify the markables, i.e. noun phrases that refer
to real-word entities. Each markable is annotated
with the following attributes:
np form: proper noun, definite NP, indefinite
NP, personal pronoun, possessive pronoun, or
demonstrative pronoun.
person, number and gender. The possible val-
ues are 1s, 1p, 2s, 2p, 3m, 3f, 3n, 3p.
semantic class: human, physical object (in-
cludes animals), or abstract. When the se-
mantic class is ambiguous, the ”abstract” op-
tion is chosen.
type: if the entity that the markable refers to
is new to the discourse, the value is ”none”. If
the markable refers to an already mentioned
entity, the value is ”anaphoric”. An anaphoric
markable has another attribute for its rela-
tion to the antecedent. The values for this at-
tribute are ”direct”, ”pronominal”, and ”ISA”
(hyponym-hyperonym).
To mark coreference, MMAX uses coreference
sets, such that every new reference to an already
mentioned entity is added to the set of that entity.
Implicitly, there is a set for every entity in the dis-
course - if an entity occurs only once, its set con-
tains one markable.
</bodyText>
<table confidence="0.981834115384615">
grammatical role: subject, object (direct or 4.3 Feature Vector
indirect), or other.
agreement: this attribute is a combination of The features used by our system are summarised
in Table 4.3. The individual features for anaphor
Feature Description
pron the pronoun
ana npform NP form of the anaphor
ana gramrole grammatical role of the
anaphor
ana agr agreement of the anaphor
ana semclass* semantic class of the anaphor
ante npform NP form of the antecedent
ante gramrole grammatical role of the an-
tecedent
ante agr agreement of the antecedent
ante semclass* semantic class of the an-
tecedent
dist distance in markables
between anaphor and an-
tecedent (1 .. 20)
same agr same agreement of anaphor
and antecedent?
same gramrole same grammatical role of
anaphor and antecedent?
same semclass* same semantic class of
anaphor and antecedent?
</table>
<tableCaption confidence="0.683142666666667">
Table 1: Features used by our system. *-ed fea-
tures were only used for 10-fold cross-validation
on the manually annotated data
</tableCaption>
<bodyText confidence="0.983737975">
and antecedent - NP form, grammatical role, se-
mantic class - are extracted directly from the an-
notation. The relational features are generated by
comparing the individual ones. The binary tar-
get function - coreferent, non-coreferent - is de-
termined by comparing the values of the member
attribute. If both markables are members of the
same set, they are coreferent, otherwise they are
not.
Due to lack of resources, the semantic class at-
tribute cannot be annotated automatically, and is
therefore used only for comparison with (Strube
et al., 2002).
4.4 Noun Phrase Chunking, NER and
POS-Tagging
To identify markables automatically, the sys-
tem uses the noun phrase chunker described in
(Schmid and Schulte im Walde, 2000), which
displays case information along with the chunks.
The chunker is based on a head-lexicalised prob-
abilistic context free grammar (H-L PCFG) and
achieves an F-measure of 92 for range only and
83 for range and label, whereby a range of a noun
chunk is defined as ”all words from the beginning
of the noun phrase to the head noun”. This is dif-
ferent from manually annotated markables, which
can be complex noun phrases.
Despite good overall performance, the chunker
fails on multi-word proper names in which case it
marks each word as an individual chunk.&apos; Since
many pronouns refer to named entities, the chun-
ker needs to be supplemented by a named entity
recogniser. Although, to our knowledge, there cur-
rently does not exist an off-the-shelf named entity
recogniser for German, we were able to obtain the
system submitted by (Curran and Clark, 2003) to
the 2003 CoNLL competition. In order to run the
recogniser, the data needs to be tokenised, tagged
and lemmatised, all of which is done by the Tree-
Tagger (Schmid, 1995).
</bodyText>
<subsectionHeader confidence="0.98445">
4.5 Markable Creation
</subsectionHeader>
<bodyText confidence="0.992229566666667">
After the markables are identified, they are auto-
matically annotated with the attributes described
in Section 4.4. The NP form can be reliably deter-
mined by examining the output of the noun chun-
ker and the named entity recogniser. Pronouns and
named entities are already labeled during chunk-
ing. The remaining markables are labelled as def-
inite NPs if their first words are definite articles
or possessive determiners, and as indefinite NPs
otherwise. Grammatical role is determined by the
case assigned to the markable - subject if nomi-
native, object if accusative. Although datives and
genitives can also be objects, they are more likely
to be adjuncts and are therefore assigned the value
”other”.
For non-pronominal markables, agreement is
determined by lexicon lookup of the head nouns.
Number ambiguities are resolved with the help of
the case information. Most proper names, except
for a few common ones, do not appear in the lexi-
con and have to remain ambiguous. Although it is
impossible to fully resolve the agreement ambigu-
ities of pronominal markables, they can be classi-
&apos;An example is [Verteidigunsminister Donald]
[Rumsfeld] ([Minister of Defense Donald] [Rumsfeld]).
fied as either feminine/plural or masculine/neuter.
Therefore we added two underspecified values to
the agreement attribute: 3f 3p and 3m 3n. Each
of these values was made to agree with both of its
subvalues.
</bodyText>
<subsectionHeader confidence="0.998856">
4.6 Antecedent Selection
</subsectionHeader>
<bodyText confidence="0.999991466666667">
After classification, one non-pronominal an-
tecedent has to be found for each pronoun. As
BoosTexter assigns confidence weights to its pre-
dictions, we have a choice between selecting the
antecedent closest to the anaphor (closest-first)
and the one with the highest weight (best-first).
Furthermore, we have a choice between ignoring
pronominal antecedents (and risking to discard all
the correct antecedents within the window) and re-
solving them (and risking multiplication of errors).
In case all of the instances within the window have
been classified as non-coreferent, we choose the
negative instance with the lowest weight as the an-
tecedent. The following section presents the re-
sults for each of the selection strategies.
</bodyText>
<sectionHeader confidence="0.994927" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99987675">
Before evaluating the actual system, we compared
the performance of boosting to that of C4.5, as re-
ported in (Strube et al., 2002). Trained on the same
corpus and evaluated with the 10-fold crossvali-
dation method, boosting significantly outperforms
C4.5 on both personal and possessive pronouns
(see Table 2). These results support the intuition
that ensemble methods are superior to single clas-
sifiers.
To put the performance of our system into per-
spective, we established a baseline and an upper
bound for the task. The baseline chooses as the an-
tecedent the closest non-pronominal markable that
agrees in number and gender with the pronoun.
The upper bound is the system’s performance on
the manually annotated (gold standard) data with-
out the semantic features.
For the baseline, accuracy is significantly higher
for the gold standard data than for the two test
sets (see Table 3). This shows that agreement is
the most important feature, which, if annotated
correctly, resolves almost half of the pronouns.
The classification results of the gold standard data,
which are much lower than the ones in Table 2 also
</bodyText>
<table confidence="0.980282333333333">
PPER PPOS
(Strube et al., 2002) 82.8 84.9
our system 87.4 86.9
</table>
<tableCaption confidence="0.9002485">
Table 2: Comparison of classification perfor-
mance (F ) with (Strube et al., 2002)
</tableCaption>
<bodyText confidence="0.999943333333333">
demonstrate the importance of the semantic fea-
tures. As for the test sets, while the classifier sig-
nificantly outperformed the baseline for the HTC
set, it did nothing for the Spiegel set. This shows
the limitations of an algorithm trained on overly
restricted data.
Among the selection heuristics, the approach of
resolving pronominal antecedents proved consis-
tently more effective than ignoring them, while
the results for the closest-first and best-first strate-
gies were mixed. They imply, however, that the
bestfirst approach should be chosen if the classifier
performed above a certain threshold; otherwise the
closest-first approach is safer.
Overall, the fact that 67.2 of the pronouns were
correctly resolved in the automatically annotated
HTC test set, while the upper bound is 82.0, vali-
dates the approach taken for this system.
</bodyText>
<sectionHeader confidence="0.996435" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9994931">
The pronoun resolution system presented in this
paper performs well for unannotated text of a lim-
ited domain. While the results are encouraging
considering the knowledge-poor approach, exper-
iments with a more complex textual domain show
that the system is unsuitable for wide-coverage
tasks such as question answering and summarisa-
tion.
To examine whether the system would yield
comparable results in unrestricted text, it needs to
be trained on a more diverse and possibly larger
corpus. For this purpose, T¨uba-D/Z, a treebank
consisting of German newswire text, is presently
being annotated with coreference information. As
the syntactic annotation of the treebank is richer
than that of the HTC corpus, additional features
may be derived from it. Experiments with T¨uba-
D/Z will show whether the performance achieved
for the HTC test set is scalable.
For future versions of the system, it might also
</bodyText>
<table confidence="0.998405714285714">
HTC-Gold HTC-Test Spiegel
Baseline accuracy 46.7% 30.9% 31.1%
Classification F score 77.9 62.8 30.4
Best-first, ignoring pronominal ant. 82.0% 67.2% 28.3%
Best-first, resolving pronominal ant. 72.2% 49.1% 21.7%
Closest-first, ignoring pronominal ant. 82.0% 57.3% 34.4%
Closest-first, resolving pronominal ant. 72.2% 49.1% 22.8%
</table>
<tableCaption confidence="0.9506045">
Table 3: Accuracy of the different selection heuristics compared with baseline accuracy and classification
F-score. HTC-Gold and HTC-Test stand for manually and automatically annotated test sets, respectively.
</tableCaption>
<bodyText confidence="0.993842333333333">
be beneficial to use full parses instead of chunks.
As most German verbs are morphologically un-
ambiguous, an analysis of them could help disam-
biguate pronouns. However, due to the relatively
free word order of the German language, this ap-
proach requires extensive reseach.
</bodyText>
<sectionHeader confidence="0.998178" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880279411764">
Steven Abney, Robert E. Schapire, and Yoram Singer.
1999. Boosting applied to tagging and PP attach-
ment. In Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Xavier Carreras, Lluis M`arquez, and Lluis Padr´o.
2002. Named entity extraction using AdaBoost.
In Proceedings of CoNLL-2002, pages 167–170,
Taipei, Taiwan.
James R. Curran and Stephen Clark. 2003. Language-
independent NER using a maximum entropy tagger.
In Proceedings of CoNLL-2003, pages 164–167, Ed-
monton, Canada.
Thomas G. Dietterich. 2000. Ensemble methods in
machine learning. In First International Workshop
on Multiple Classifier Systems, Lecture Notes in
Computer Science, pages 1–15. Springer, New York.
Gerard Escudero, Lluis M`arquez, and German Rigau.
2000. Boosting applied to word sense disambigua-
tion. In Proceedings of the 12th European Confer-
ence on Machine Learning, pages 129–141.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI’95), pages 1050–
1055, Montreal, Canada.
Thomas S. Morton. 2000. Coreference for nlp appli-
cations. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics
(ACL’00), Hong Kong.
Christoph M¨uller and Michael Strube. 2001. Annotat-
ing anaphoric and bridging relations with MMAX.
In Proceedings of the 2nd SIGdial Workshop on Dis-
course and Dialogue, pages 90–95, Aalborg, Den-
mark.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings ofthe 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL’02),
pages 104–111, Philadelphia, PA, USA.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufman, San Mateo, CA.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135–168.
Robert E. Schapire. 2002. The boosting approach to
machine learning: an overview. In Proceedings of
the MSRI Workshop on Nonlinear Estimation and
Classification.
Helmut Schmid and Sabine Schulte im Walde. 2000.
Robust German noun chunking with a probabilis-
tic context-free grammar. In Proceedings of
the 18th International Conference on Computa-
tional Linguistics (COLING-00), pages 726–732,
Saarbr¨ucken, Germany.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT-Workshop.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
Michael Strube, Stefan Rapp, and Christoph M¨uller.
2002. The influence of minimum edit distance
on reference resolution. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP’02), pages 312–319,
Philadelphia, PA, USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942724">
<title confidence="0.999915">A Machine Learning Approach to German Pronoun Resolution</title>
<author confidence="0.998113">Beata Kouchnir</author>
<affiliation confidence="0.999888">Department of Computational Linguistics T¨ubingen University</affiliation>
<address confidence="0.999783">72074 T¨ubingen, Germany</address>
<email confidence="0.999479">kouchnir@sfs.uni-tuebingen.de</email>
<abstract confidence="0.997114842105263">This paper presents a novel ensemble learning approach to resolving German pronouns. Boosting, the method in question, combines the moderately accurate hypotheses of several classifiers to form a highly accurate one. Experiments show that this approach is superior to a single decision-tree classifier. Furthermore, we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process. Although the system performs well within a limited textual domain, further research is needed to make it effective for open-domain question answering and text summarisation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boosting applied to tagging and PP attachment.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="6383" citStr="Abney et al., 1999" startWordPosition="982" endWordPosition="985">g many weak hypotheses is easier than finding one strong hypothesis. This is achieved by running a base learning algorithm over several iterations. Initially, an importance weight is distributed uniformly among the training examples. After each iteration, the weight is redistributed, so that misclassified examples get higher weights. The base learner is thus forced to concentrate on difficult examples. Although boosting has not yet been applied to coreference resolution, it has outperformed stateof-the-art systems for NLP tasks such as partofspeech tagging and prepositional phrase attachment (Abney et al., 1999), word sense disambiguation (Escudero et al., 2000), and named entity recognition (Carreras et al., 2002). The implementation used for this project is BoosTexter (Schapire and Singer, 2000), a toolkit freely available for research purposes. In addition to labels, BoosTexter assigns confidence weights that reflect the reliability of the decisions. 4 System Description Our system resolves pronouns in three stages: preprocessing, classification, and postprocessing. Figure 1 gives an overview of the system architecture, while this section provides details of each component. 4.1 Training and Test D</context>
</contexts>
<marker>Abney, Schapire, Singer, 1999</marker>
<rawString>Steven Abney, Robert E. Schapire, and Yoram Singer. 1999. Boosting applied to tagging and PP attachment. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
<author>Lluis Padr´o</author>
</authors>
<title>Named entity extraction using AdaBoost.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>167--170</pages>
<location>Taipei, Taiwan.</location>
<marker>Carreras, M`arquez, Padr´o, 2002</marker>
<rawString>Xavier Carreras, Lluis M`arquez, and Lluis Padr´o. 2002. Named entity extraction using AdaBoost. In Proceedings of CoNLL-2002, pages 167–170, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Languageindependent NER using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>164--167</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="11379" citStr="Curran and Clark, 2003" startWordPosition="1789" endWordPosition="1792">ange of a noun chunk is defined as ”all words from the beginning of the noun phrase to the head noun”. This is different from manually annotated markables, which can be complex noun phrases. Despite good overall performance, the chunker fails on multi-word proper names in which case it marks each word as an individual chunk.&apos; Since many pronouns refer to named entities, the chunker needs to be supplemented by a named entity recogniser. Although, to our knowledge, there currently does not exist an off-the-shelf named entity recogniser for German, we were able to obtain the system submitted by (Curran and Clark, 2003) to the 2003 CoNLL competition. In order to run the recogniser, the data needs to be tokenised, tagged and lemmatised, all of which is done by the TreeTagger (Schmid, 1995). 4.5 Markable Creation After the markables are identified, they are automatically annotated with the attributes described in Section 4.4. The NP form can be reliably determined by examining the output of the noun chunker and the named entity recogniser. Pronouns and named entities are already labeled during chunking. The remaining markables are labelled as definite NPs if their first words are definite articles or possessiv</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Languageindependent NER using a maximum entropy tagger. In Proceedings of CoNLL-2003, pages 164–167, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Ensemble methods in machine learning.</title>
<date>2000</date>
<booktitle>In First International Workshop on Multiple Classifier Systems, Lecture Notes in Computer Science,</booktitle>
<pages>1--15</pages>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="5475" citStr="Dietterich, 2000" startWordPosition="842" endWordPosition="843">possessive and demonstrative pronouns. The research is based on the Heidelberg Text Corpus (see Section 4), which makes it ideal for comparison with our system. (Strube et al., 2002) used 15 features modeled after those used by state-of-the-art resolution systems for English. The results for personal and possessive pronouns are 82.79 and 84.94, respectively. 3 Boosting All of the systems described in the previous section use a single classifier to resolve coreference. Our intuition, however, is that a combination of classifiers is better suited for this task. The concept of ensemble learning (Dietterich, 2000) is based on the assumption that combining the hypotheses of several classifiers yields a hypothesis that is much more accurate than that of an individual classifier. One of the most popular ensemble learning methods is boosting (Schapire, 2002). It is based on the observation that finding many weak hypotheses is easier than finding one strong hypothesis. This is achieved by running a base learning algorithm over several iterations. Initially, an importance weight is distributed uniformly among the training examples. After each iteration, the weight is redistributed, so that misclassified exam</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Thomas G. Dietterich. 2000. Ensemble methods in machine learning. In First International Workshop on Multiple Classifier Systems, Lecture Notes in Computer Science, pages 1–15. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Escudero</author>
<author>Lluis M`arquez</author>
<author>German Rigau</author>
</authors>
<title>Boosting applied to word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>129--141</pages>
<marker>Escudero, M`arquez, Rigau, 2000</marker>
<rawString>Gerard Escudero, Lluis M`arquez, and German Rigau. 2000. Boosting applied to word sense disambiguation. In Proceedings of the 12th European Conference on Machine Learning, pages 129–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph F McCarthy</author>
<author>Wendy G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI’95),</booktitle>
<pages>1050--1055</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2565" citStr="McCarthy and Lehnert, 1995" startWordPosition="375" endWordPosition="378">for other NLP tasks. The fact that boosting uses the method of ensemble learning, i.e. combining the decisions of several classifiers, suggests that the combined hypothesis will be more accurate than one learned by a single classifier. On the practical side, boosting has distinguished itself by achieving good results with small feature sets. 2 Related Work Although extensive research has been conducted on statistical anaphora resolution, the bulk of the work has concentrated on the English language. Nevertheless, comparing different strategies helped shape the system described in this paper. (McCarthy and Lehnert, 1995) were among the first to use machine learning for coreference resolution. RESOLVE was trained on data from MUC-5 English Joint Venture (EJV) corpus and used the C4.5 decision tree algorithm (Quinlan, 1993) with eight features, most of which were tailored to the joint venturte domain. The system achieved an F-measure of 86.5 for full coreference resolution (no values were given for pronouns). Although a number this high must be attributed to the specific textual domain, RESOLVE also outperformed the authors’ rule-based algorithm by 7.6 percentage points, which encouraged further reseach in this</context>
<context position="3979" citStr="McCarthy and Lehnert, 1995" startWordPosition="604" endWordPosition="607"> a subset of the Wall Street Journal, comprising 21 million tokens. The reported F-measure for pronoun resolution is 81.5. However, (Morton, 2000) only attempts to resolve singular pronouns, and there is no mention of what percentage of total pronouns are covered by this restriction. (Soon et al., 2001) use the C4.5 algorithm with a set of 12 domain-independent features, ten syntactic and two semantic. Their system was trained on both the MUC-6 and the MUC-7 datasets, for which it achieved F-scores of 62.6 and 60.4, respectively. Although these results are far worse than the ones reported in (McCarthy and Lehnert, 1995), they are comparable to the best-performing rule-based systems in the respective competitions. As (McCarthy and Lehnert, 1995), (Soon et al., 2001) do not report separate results for pronouns. (Ng and Cardie, 2002) expanded on the work of (Soon et al., 2001) by adding 41 lexical, semantic and grammatical features. However, since using this many features proved to be detrimental to performance, all features that induced low precision rules were discarded, leaving only 19. The final system outperformed that of (Soon et al., 2001), with F-scores of 69.1 and 63.4 for MUC-6 and MUC-7, respectively</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>Joseph F. McCarthy and Wendy G. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI’95), pages 1050– 1055, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Morton</author>
</authors>
<title>Coreference for nlp applications.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL’00),</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="3243" citStr="Morton, 2000" startWordPosition="483" endWordPosition="484">olution. RESOLVE was trained on data from MUC-5 English Joint Venture (EJV) corpus and used the C4.5 decision tree algorithm (Quinlan, 1993) with eight features, most of which were tailored to the joint venturte domain. The system achieved an F-measure of 86.5 for full coreference resolution (no values were given for pronouns). Although a number this high must be attributed to the specific textual domain, RESOLVE also outperformed the authors’ rule-based algorithm by 7.6 percentage points, which encouraged further reseach in this direction. Unlike the other systems presented in this section, (Morton, 2000) does not use a decision tree algorithm but opts instead for a maximum entropy model. The model is trained on a subset of the Wall Street Journal, comprising 21 million tokens. The reported F-measure for pronoun resolution is 81.5. However, (Morton, 2000) only attempts to resolve singular pronouns, and there is no mention of what percentage of total pronouns are covered by this restriction. (Soon et al., 2001) use the C4.5 algorithm with a set of 12 domain-independent features, ten syntactic and two semantic. Their system was trained on both the MUC-6 and the MUC-7 datasets, for which it achie</context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>Thomas S. Morton. 2000. Coreference for nlp applications. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL’00), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Michael Strube</author>
</authors>
<title>Annotating anaphoric and bridging relations with MMAX.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>90--95</pages>
<location>Aalborg, Denmark.</location>
<marker>M¨uller, Strube, 2001</marker>
<rawString>Christoph M¨uller and Michael Strube. 2001. Annotating anaphoric and bridging relations with MMAX. In Proceedings of the 2nd SIGdial Workshop on Discourse and Dialogue, pages 90–95, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings ofthe 40th Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<pages>104--111</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="4194" citStr="Ng and Cardie, 2002" startWordPosition="636" endWordPosition="639"> percentage of total pronouns are covered by this restriction. (Soon et al., 2001) use the C4.5 algorithm with a set of 12 domain-independent features, ten syntactic and two semantic. Their system was trained on both the MUC-6 and the MUC-7 datasets, for which it achieved F-scores of 62.6 and 60.4, respectively. Although these results are far worse than the ones reported in (McCarthy and Lehnert, 1995), they are comparable to the best-performing rule-based systems in the respective competitions. As (McCarthy and Lehnert, 1995), (Soon et al., 2001) do not report separate results for pronouns. (Ng and Cardie, 2002) expanded on the work of (Soon et al., 2001) by adding 41 lexical, semantic and grammatical features. However, since using this many features proved to be detrimental to performance, all features that induced low precision rules were discarded, leaving only 19. The final system outperformed that of (Soon et al., 2001), with F-scores of 69.1 and 63.4 for MUC-6 and MUC-7, respectively. For pronouns, the reported results are 74.6 and 57.8, respectively. The experiment presented in (Strube et al., 2002) is one of the few dealing with the application of machine learning to German coreference resolu</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings ofthe 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pages 104–111, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan</publisher>
<location>Kaufman, San Mateo, CA.</location>
<contexts>
<context position="2770" citStr="Quinlan, 1993" startWordPosition="409" endWordPosition="410">ingle classifier. On the practical side, boosting has distinguished itself by achieving good results with small feature sets. 2 Related Work Although extensive research has been conducted on statistical anaphora resolution, the bulk of the work has concentrated on the English language. Nevertheless, comparing different strategies helped shape the system described in this paper. (McCarthy and Lehnert, 1995) were among the first to use machine learning for coreference resolution. RESOLVE was trained on data from MUC-5 English Joint Venture (EJV) corpus and used the C4.5 decision tree algorithm (Quinlan, 1993) with eight features, most of which were tailored to the joint venturte domain. The system achieved an F-measure of 86.5 for full coreference resolution (no values were given for pronouns). Although a number this high must be attributed to the specific textual domain, RESOLVE also outperformed the authors’ rule-based algorithm by 7.6 percentage points, which encouraged further reseach in this direction. Unlike the other systems presented in this section, (Morton, 2000) does not use a decision tree algorithm but opts instead for a maximum entropy model. The model is trained on a subset of the W</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufman, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="6572" citStr="Schapire and Singer, 2000" startWordPosition="1011" endWordPosition="1014"> distributed uniformly among the training examples. After each iteration, the weight is redistributed, so that misclassified examples get higher weights. The base learner is thus forced to concentrate on difficult examples. Although boosting has not yet been applied to coreference resolution, it has outperformed stateof-the-art systems for NLP tasks such as partofspeech tagging and prepositional phrase attachment (Abney et al., 1999), word sense disambiguation (Escudero et al., 2000), and named entity recognition (Carreras et al., 2002). The implementation used for this project is BoosTexter (Schapire and Singer, 2000), a toolkit freely available for research purposes. In addition to labels, BoosTexter assigns confidence weights that reflect the reliability of the decisions. 4 System Description Our system resolves pronouns in three stages: preprocessing, classification, and postprocessing. Figure 1 gives an overview of the system architecture, while this section provides details of each component. 4.1 Training and Test Data The system was trained with data from the Heidelberg Text Corpus (HTC), provided by the European Media Laboratory in Heidelberg, Germany. Figure 1: System Architecture The HTC is a coll</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
</authors>
<title>The boosting approach to machine learning: an overview.</title>
<date>2002</date>
<booktitle>In Proceedings of the MSRI Workshop on Nonlinear Estimation and Classification.</booktitle>
<contexts>
<context position="5720" citStr="Schapire, 2002" startWordPosition="882" endWordPosition="883">solution systems for English. The results for personal and possessive pronouns are 82.79 and 84.94, respectively. 3 Boosting All of the systems described in the previous section use a single classifier to resolve coreference. Our intuition, however, is that a combination of classifiers is better suited for this task. The concept of ensemble learning (Dietterich, 2000) is based on the assumption that combining the hypotheses of several classifiers yields a hypothesis that is much more accurate than that of an individual classifier. One of the most popular ensemble learning methods is boosting (Schapire, 2002). It is based on the observation that finding many weak hypotheses is easier than finding one strong hypothesis. This is achieved by running a base learning algorithm over several iterations. Initially, an importance weight is distributed uniformly among the training examples. After each iteration, the weight is redistributed, so that misclassified examples get higher weights. The base learner is thus forced to concentrate on difficult examples. Although boosting has not yet been applied to coreference resolution, it has outperformed stateof-the-art systems for NLP tasks such as partofspeech t</context>
</contexts>
<marker>Schapire, 2002</marker>
<rawString>Robert E. Schapire. 2002. The boosting approach to machine learning: an overview. In Proceedings of the MSRI Workshop on Nonlinear Estimation and Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Robust German noun chunking with a probabilistic context-free grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING-00),</booktitle>
<pages>726--732</pages>
<location>Saarbr¨ucken, Germany.</location>
<marker>Schmid, Walde, 2000</marker>
<rawString>Helmut Schmid and Sabine Schulte im Walde. 2000. Robust German noun chunking with a probabilistic context-free grammar. In Proceedings of the 18th International Conference on Computational Linguistics (COLING-00), pages 726–732, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements in part-ofspeech tagging with an application to German.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL SIGDAT-Workshop.</booktitle>
<contexts>
<context position="11551" citStr="Schmid, 1995" startWordPosition="1822" endWordPosition="1823">phrases. Despite good overall performance, the chunker fails on multi-word proper names in which case it marks each word as an individual chunk.&apos; Since many pronouns refer to named entities, the chunker needs to be supplemented by a named entity recogniser. Although, to our knowledge, there currently does not exist an off-the-shelf named entity recogniser for German, we were able to obtain the system submitted by (Curran and Clark, 2003) to the 2003 CoNLL competition. In order to run the recogniser, the data needs to be tokenised, tagged and lemmatised, all of which is done by the TreeTagger (Schmid, 1995). 4.5 Markable Creation After the markables are identified, they are automatically annotated with the attributes described in Section 4.4. The NP form can be reliably determined by examining the output of the noun chunker and the named entity recogniser. Pronouns and named entities are already labeled during chunking. The remaining markables are labelled as definite NPs if their first words are definite articles or possessive determiners, and as indefinite NPs otherwise. Grammatical role is determined by the case assigned to the markable - subject if nominative, object if accusative. Although </context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Improvements in part-ofspeech tagging with an application to German. In Proceedings of the ACL SIGDAT-Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="3656" citStr="Soon et al., 2001" startWordPosition="549" endWordPosition="552"> RESOLVE also outperformed the authors’ rule-based algorithm by 7.6 percentage points, which encouraged further reseach in this direction. Unlike the other systems presented in this section, (Morton, 2000) does not use a decision tree algorithm but opts instead for a maximum entropy model. The model is trained on a subset of the Wall Street Journal, comprising 21 million tokens. The reported F-measure for pronoun resolution is 81.5. However, (Morton, 2000) only attempts to resolve singular pronouns, and there is no mention of what percentage of total pronouns are covered by this restriction. (Soon et al., 2001) use the C4.5 algorithm with a set of 12 domain-independent features, ten syntactic and two semantic. Their system was trained on both the MUC-6 and the MUC-7 datasets, for which it achieved F-scores of 62.6 and 60.4, respectively. Although these results are far worse than the ones reported in (McCarthy and Lehnert, 1995), they are comparable to the best-performing rule-based systems in the respective competitions. As (McCarthy and Lehnert, 1995), (Soon et al., 2001) do not report separate results for pronouns. (Ng and Cardie, 2002) expanded on the work of (Soon et al., 2001) by adding 41 lexi</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Stefan Rapp</author>
<author>Christoph M¨uller</author>
</authors>
<title>The influence of minimum edit distance on reference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP’02),</booktitle>
<pages>312--319</pages>
<location>Philadelphia, PA, USA.</location>
<marker>Strube, Rapp, M¨uller, 2002</marker>
<rawString>Michael Strube, Stefan Rapp, and Christoph M¨uller. 2002. The influence of minimum edit distance on reference resolution. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP’02), pages 312–319, Philadelphia, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>