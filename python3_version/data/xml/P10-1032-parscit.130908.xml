<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9982545">
Exploring Syntactic Structural Features for Sub-Tree Alignment
using Bilingual Tree Kernels
</title>
<author confidence="0.991281">
Jun Sun1,2 Min Zhang1 Chew Lim Tan2
</author>
<affiliation confidence="0.950939">
1 Institute for Infocomm Research 2School of Computing, National University of Singapore
</affiliation>
<email confidence="0.990895">
sunjun@comp.nus.edu.sg mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.997281" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999714888888889">
We propose Bilingual Tree Kernels (BTKs) to
capture the structural similarities across a pair of
syntactic translational equivalences and apply
BTKs to sub-tree alignment along with some
plain features. Our study reveals that the struc-
tural features embedded in a bilingual parse tree
pair are very effective for sub-tree alignment
and the bilingual tree kernels can well capture
such features. The experimental results show
that our approach achieves a significant im-
provement on both gold standard tree bank and
automatically parsed tree pairs against a heuris-
tic similarity based method. We further apply
the sub-tree alignment in machine translation
with two methods. It is suggested that the sub-
tree alignment benefits both phrase and syntax
based systems by relaxing the constraint of the
word alignment.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.972013631578947">
Syntax based Statistical Machine Translation
(SMT) systems allow the translation process to be
more grammatically performed, which provides
decent reordering capability. However, most of the
syntax based systems construct the syntactic trans-
lation rules based on word alignment, which not
only suffers from the pipeline errors, but also fails
to effectively utilize the syntactic structural fea-
tures. To address those deficiencies, Tinsley et al.
(2007) attempt to directly capture the syntactic
translational equivalences by automatically con-
ducting sub-tree alignment, which can be defined
as follows:
A sub-tree alignment process pairs up sub-tree
pairs across bilingual parse trees whose contexts
are semantically translational equivalent. Accord-
ing to Tinsley et al. (2007), a sub-tree aligned
parse tree pair follows the following criteria:
(i) a node can only be linked once;
</bodyText>
<figureCaption confidence="0.9445975">
Figure 1: Sub-tree alignment as referred to
Node alignment
</figureCaption>
<bodyText confidence="0.938388476190476">
(ii) descendants of a source linked node may
only link to descendants of its target
linked counterpart;
(iii) ancestors of a source linked node may on-
ly link to ancestors of its target linked
counterpart.
By sub-tree alignment, translational equivalent
sub-tree pairs are coupled as aligned counterparts.
Each pair consists of both the lexical constituents
and their maximum tree structures generated over
the lexical sequences in the original parse trees.
Due to the 1-to-1 mapping between sub-trees and
tree nodes, sub-tree alignment can also be consi-
dered as node alignment by conducting multiple
links across the internal nodes as shown in Fig. 1.
Previous studies conduct sub-tree alignments by
either using a rule based method or conducting
some similarity measurement only based on lexi-
cal features. Groves et al. (2004) conduct sub-tree
alignment by using some heuristic rules, lack of
extensibility and generality. Tinsley et al. (2007)
</bodyText>
<page confidence="0.98429">
306
</page>
<note confidence="0.9435385">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306–315,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999973962962963">
and Imamura (2001) propose some score functions
based on the lexical similarity and co-occurrence.
These works fail to utilize the structural features,
rendering the syntactic rich task of sub-tree align-
ment less convincing and attractive. This may be
due to the fact that the syntactic structures in a
parse tree pair are hard to describe using plain fea-
tures. In addition, explicitly utilizing syntactic tree
fragments results in exponentially high dimen-
sional feature vectors, which is hard to compute.
Alternatively, convolution parse tree kernels (Col-
lins and Duffy, 2001), which implicitly explore
the tree structure information, have been success-
fully applied in many NLP tasks, such as Semantic
parsing (Moschitti, 2004) and Relation Extraction
(Zhang et al. 2006). However, all those studies are
carried out in monolingual tasks. In multilingual
tasks such as machine translation, tree kernels are
seldom applied.
In this paper, we propose Bilingual Tree Ker-
nels (BTKs) to model the bilingual translational
equivalences, in our case, to conduct sub-tree
alignment. This is motivated by the decent effec-
tiveness of tree kernels in expressing the similarity
between tree structures. We propose two kinds of
BTKs named dependent Bilingual Tree Kernel
(dBTK), which takes the sub-tree pair as a whole
and independent Bilingual Tree Kernel (iBTK),
which individually models the source and the tar-
get sub-trees. Both kernels can be utilized within
different feature spaces using various representa-
tions of the sub-structures.
Along with BTKs, various lexical and syntactic
structural features are proposed to capture the cor-
respondence between bilingual sub-trees using a
polynomial kernel. We then attempt to combine
the polynomial kernel and BTKs to construct a
composite kernel. The sub-tree alignment task is
considered as a binary classification problem. We
employ a kernel based classifier with the compo-
site kernel to classify each candidate of sub-tree
pair as aligned or unaligned. Then a greedy search
algorithm is performed according to the three cri-
teria of sub-tree alignment within the space of
candidates classified as aligned.
We evaluate the sub-tree alignment on both the
gold standard tree bank and an automatically
parsed corpus. Experimental results show that the
proposed BTKs benefit sub-tree alignment on both
corpora, along with the lexical features and the
plain structural features. Further experiments in
machine translation also suggest that the obtained
sub-tree alignment can improve the performance
of both phrase and syntax based SMT systems.
</bodyText>
<sectionHeader confidence="0.892201" genericHeader="introduction">
2 Bilingual Tree Kernels
</sectionHeader>
<bodyText confidence="0.997883857142857">
In this section, we propose the two BTKs and
study their capability and complexity in modeling
the bilingual structural similarity. Before elaborat-
ing the concepts of BTKs, we first illustrate some
notations to facilitate further understanding.
Each sub-tree pair ሺܵ · ܶሻ can be explicitly de-
composed into multiple sub-structures which be-
long to the given sub-structure spaces. ࣭ூ ൌ
ሼݏଵ, ..., ݏ௜, ..., ݏூሽ refers to the source tree sub-
structure space; while ࣮௃ ൌ ൛ݐଵ, ..., ݐ௝, ..., ݐ௃ൟ refers
to the target sub-structure space. A sub-structure
pair ሺݏ௜, ݐ௝ሻ refers to an element in the set of the
Cartesian product of the two sub-structure spaces:
ሺݏ௜, ݐ௝ሻ א ࣭ூ ൈ ࣮௃.
</bodyText>
<sectionHeader confidence="0.743735" genericHeader="method">
2.1 Independent Bilingual Tree Kernel
(iBTK)
</sectionHeader>
<bodyText confidence="0.999551">
Given the sub-structure spaces ࣭ூ and ࣮௃, we con-
struct two vectors using the integer counts of the
source and target sub-structures:
</bodyText>
<equation confidence="0.9271435">
߶ሺܵሻ ൌ ൫#ሺݏଵሻ, ..., #ሺݏ௞ሻ, ..., #ሺݏ|࣭಺|ሻ൯
߶ሺܶሻ ൌ ቀ#ሺݐଵሻ, ..., #ሺݐ௞ሻ, ..., #ሺݐห࣮಻หሻቁ
</equation>
<bodyText confidence="0.999974666666667">
where #ሺݏ௞ሻ and #ሺݐ௞ሻ are the numbers of oc-
currences of the sub-structures ݏ௞ and ݐ௞. In order
to compute the dot product of the feature vectors
in the exponentially high dimensional feature
space, we introduce the tree kernel functions as
follows:
</bodyText>
<equation confidence="0.900276">
ࣥ௜஻்௄ሺܵ · ܶ, ܵᇱ · ܶᇱሻ ൌ ࣥሺܵ, ܵᇱሻ ൅ ࣥሺܶ, ܶᇱሻ
</equation>
<bodyText confidence="0.999159571428571">
The iBTK is defined as a composite kernel con-
sisting of a source tree kernel and a target tree
kernel which measures the source and the target
structural similarity respectively. Therefore, the
composite kernel can be computed using the ordi-
nary monolingual tree kernels (Collins and Duffy,
2001).
</bodyText>
<equation confidence="0.964167333333333">
ࣥሺܵ, ܵᇱሻ ൌ൏ ߶ሺܵሻ, ߶ሺܵᇱሻ ൐
ൌ∑ ቀ∑ ௡ೞאேೄ ܫ௜ሺ݊௦ሻ · ∑௡ೞᇲאேೄ ܫ௜ሺ݊௦ᇱሻ
|࣭಺ |ᇲ ቁ
௜ୀଵ
ൌ ∑∑ ௡ೞᇲאேೄ ∆ሺ݊௦, ݊௦ᇱሻ
௡ೞאேೄ ᇲ
</equation>
<bodyText confidence="0.991444111111111">
where ܰௌ and ܰௌᇱ refer to the node sets of the
source sub-tree ܵ and ܵᇱ respectvely. ܫ௜ሺ݊௦ሻ is an
indicator function which equals to 1 iff the sub-
structure ݏ௜ is rooted at the node ݊௦ and 0 other-
wise.∆ሺ݊௦, ݊௦ᇱሻ ൌ ∑ ൫ܫ௜ሺ݊௦ሻ · ܫ௜ሺ݊௦ᇱሻ൯
|࣭಺ |is the num-
௜ୀଵ
ber of identical sub-structures rooted at ݊௦ and ݊௦ᇱ.
Then we compute the ∆ሺ݊௦, ݊௦ᇱሻ function as follows:
</bodyText>
<page confidence="0.98763">
307
</page>
<listItem confidence="0.716915">
(1) If the production rule at ݊௦ and ݊௦ᇱ are different,
</listItem>
<equation confidence="0.9850122">
∆ሺ݊௦, ݊௦ᇱሻ ൌ 0;
(2)else if both ݊௦and ݊௦ᇱ are POS tags, ∆ሺ݊௦, ݊௦ᇱሻ ൌ ߣ;
(3)else, ∆ሺ݊௦, ݊௦ᇱሻ ൌ ߣ ∏ ቀ1 ൅ ∆൫ܿሺ݊௦, ݈ሻ, ܿሺ݊௦ᇱ, ݈ሻ൯ቁ
௡௖ሺ௡ೞሻ .
௟ୀଵ
</equation>
<bodyText confidence="0.9996602">
where ݊ܿሺ݊௦ሻ is the child number of ݊௦, ܿሺ݊௦, ݈ሻ
is the a child of ݊௦, ߣ is the decay factor used to
make the kernel value less variable with respect to
the number of sub-structures.
Similarly, we can decompose the target kernel
</bodyText>
<equation confidence="0.718304">
as ࣥሺܶ, ܶᇱሻ ൌ ∑ ∑௡೟ᇲאே೅ ∆ሺ݊௧, ݊௧ᇱሻ and run the
௡೟אே೅ ᇲ
algorithm above as well.
</equation>
<bodyText confidence="0.9994805">
The disadvantage of the iBTK is that it fails to
capture the correspondence across the sub-
structure pairs. However, the composite style of
constructing the iBTK helps keep the computa-
tional complexity comparable to the monolingual
tree kernel, which is ܱሺ|ܰௌ |· |ܰௌᇱ |൅ |்ܰ |· |்ܰᇱ |ሻ.
</bodyText>
<subsectionHeader confidence="0.998549">
2.2 Dependent Bilingual Tree Kernel (dBTK)
</subsectionHeader>
<bodyText confidence="0.999995470588235">
The iBTK explores the structural similarity of the
source and the target sub-trees respectively. As an
alternative, we further define a kernel to capture
the relationship across the counterparts without
increasing the computational complexity. As a
result, we propose the dependent Bilingual Tree
kernel (dBTK) to jointly evaluate the similarity
across sub-tree pairs by enlarging the feature
space to the Cartesian product of the two sub-
structure sets.
A dBTK takes the source and the target sub-
structure pair as a whole and recursively calculate
over the joint sub-structures of the given sub-tree
pair. We define the dBTK as follows:
Given the sub-structure space ࣭ூ ൈ ࣮௃, we con-
struct a vector using the integer counts of the sub-
structure pairs to represent a sub-tree pair:
</bodyText>
<equation confidence="0.984174533333333">
#ሺݏଵ, ݐଵሻ, ... , #ሺݏଵ, ݐห࣮಻หሻ, #ሺݏଶ, ݐଵሻ,
߶ሺܵ · ܶሻ ൌ ቆ ... , #ሺݏ|࣭಺|, ݐଵሻ, ... , #ሺݏ|࣭಺|, ݐห࣮಻หሻ
∑ ∑௡೟ᇲאே೅ ܫ௞ሺ݊௦ᇱ, ݊௧ᇱሻ
௞ୀଵ ௡ೞᇲאேೄ
ൌ ∑ ∑ ∑ ∑ ∆ ൬ሺ݊௦, ݊௧ሻ,
௡ೞᇲאேೄ ௡೟ᇲאே೅ ሺ݊௦ᇱ, ݊௧ᇱሻ൰
ᇲ (1)
௡ೞאேೄ ௡೟אே೅ ᇲൌ ∑ ∑ ∑ ∑ ൬∆ሺ݊௦, ݊௦ᇱሻ ·
௡ೞᇲאேೄ ௡೟ᇲאே೅ ∆ሺ݊௧, ݊௧ᇱሻ ൰
ᇲ (2)
௡ೞאேೄ ௡೟אே೅ ᇲ ൌ∑ ∑௡ೞᇲאேೄ ∆ሺ݊௦, ݊௦ᇱሻ
ᇲ ∑ ∑௡೟ᇲאே೅ ∆ሺ݊௧, ݊௧ᇱሻ
ᇲ
௡ೞאேೄ ௡೟אே೅
ൌ ࣥሺܵ, ܵᇱሻ · ࣥሺܶ, ܶᇱሻ
</equation>
<bodyText confidence="0.99928235">
It is infeasible to explicitly compute the kernel
function by expressing the sub-trees as feature
vectors. In order to achieve convenient computa-
tion, we deduce the kernel function as the above.
The deduction from (1) to (2) is derived accord-
ing to the fact that the number of identical sub-
structure pairs rooted in the node pairs ሺ݊௦, ݊௧ሻ and
ሺ݊௦ᇱ, ݊௧ᇱሻ equals to the product of the respective
counts. As a result, the dBTK can be evaluated as
a product of two monolingual tree kernels. Here
we verify the correctness of the kernel by directly
constructing the feature space for the inner prod-
uct. Alternatively, Cristianini and Shawe-Taylor
(2000) prove the positive semi-definite characte-
ristic of the tensor product of two kernels. The
decomposition benefits the efficient computation
to use the algorithm for the monolingual tree ker-
nel in Section 2.1.
The computational complexity of the dBTK is
still ܱሺ|ܰௌ|·|ܰௌᇱ |൅ |்ܰ |·|்ܰᇱ|ሻ.
</bodyText>
<sectionHeader confidence="0.971815" genericHeader="method">
3 Sub-structure Spaces for BTKs
</sectionHeader>
<bodyText confidence="0.9999742">
The syntactic translational equivalences under
BTKs are evaluated with respective to the sub-
structures factorized from the candidate sub-tree
pairs. In this section, we propose different sub-
structures to facilitate the measurement of syntac-
tic similarity for sub-tree alignment. Since the
proposed BTKs can be computed by individually
evaluating the source and target monolingual tree
kernels, the definition of the sub-structure can be
simplified to base only on monolingual sub-trees.
</bodyText>
<subsectionHeader confidence="0.999542">
3.1 Subset Tree
</subsectionHeader>
<bodyText confidence="0.99989825">
Motivated from Collins and Duffy (2002) in mo-
nolingual tree kernels, the Subset Tree (SST) can
be employed as sub-structures. An SST is any sub-
graph, which includes more than one non-terminal
node, with the constraint that the entire rule pro-
ductions are included. Fig. 2 shows an example of
the SSTs decomposed from the source sub-tree
rooted at VP*.
</bodyText>
<subsectionHeader confidence="0.999814">
3.2 Root directed Subset Tree
</subsectionHeader>
<bodyText confidence="0.999603916666667">
Monolingual Tree kernels achieve decent perfor-
mance using the SSTs due to the rich exploration
of syntactic information. However, the sub-tree
alignment task requires strong capability of dis-
criminating the sub-trees with their roots across
adjacent generations, because those candidates
share many identical SSTs. As illustrated in Fig 2,
the source sub-tree rooted at VP*, which should
be aligned to the target sub-tree rooted at NP*,
may be likely aligned to the sub-tree rooted at PP*,
where #൫ݏ௜, ݐ௝൯ is the number of occurrences of
the sub-structure pair ൫ݏ௜, ݐ௝൯.
</bodyText>
<equation confidence="0.998588">
ࣥௗ஻்௄ሺܵ · ܶ, ܵᇱ · ܶᇱሻ
ൌ൏ ߶ሺܵ · ܶሻ, ߶ሺܵᇱ · ܶᇱሻ ൐
∑௡ೞאேೄ∑௡೟אே೅ܫ௞ሺ݊௦,݊௧ሻ ·
ൌ ∑ ቆ
|࣭಺ൈ࣮಻|
ᇲ ቇ
</equation>
<page confidence="0.99614">
308
</page>
<figureCaption confidence="0.999856">
Figure 2: Illustration of SST, RdSST and RgSST
</figureCaption>
<bodyText confidence="0.999958857142857">
which shares quite a similar context with NP*. It
is also easy to show that the latter shares all the
SSTs that the former obtains. In consequence, the
values of the SST based kernel function are quite
similar between the candidate sub-tree pair rooted
at (VP*,NP*) and (VP*,PP*).
In order to effectively differentiate the candi-
dates like the above, we propose the Root directed
Subset Tree (RdSST) by encapsulating each SST
with the root of the given sub-tree. As shown in
Fig 2, a sub-structure is considered identical to the
given examples, when the SST is identical and the
root tag of the given sub-tree is NP. As a result,
the kernel function in Section 2.1 is re-defined as:
</bodyText>
<equation confidence="0.9855734">
ࣥሺܵ, ܵᇱሻ ൌ ∑ ∑௡ೞᇲאேೄ ∆ሺ݊௦, ݊௦ᇱሻܫሺݎ௦, ݎ௦ᇱሻ
ᇲ
௡ೞאேೄ
ൌ ܫሺݎ௦, ݎ௦ᇱሻ ∑ ∑ ௡ೞᇲאேೄ ∆ሺ݊௦, ݊௦ᇱሻ
௡ೞאேೄ ᇲ
</equation>
<bodyText confidence="0.999986857142857">
where ݎ௦ and ݎ௦ᇱ are the root nodes of the sub-
tree ܵ and ܵᇱrespectively. The indicator function
ܫሺݎ௦, ݎ௦ᇱሻ equals to 1 if ݎ௦ and ݎ௦ᇱ are identical, and 0
otherwise. Although defined for individual SST,
the indicator function can be evaluated outside the
summation, without increasing the computational
complexity of the kernel function.
</bodyText>
<subsectionHeader confidence="0.999563">
3.3 Root generated Subset Tree
</subsectionHeader>
<bodyText confidence="0.9925112">
Some grammatical tags (NP/VP) may have iden-
tical tags as their parents or children which may
make RdSST less effective. Consequently, we step
further to propose the sub-structure of Root gener-
ated Subset Tree (RgSST). An RgSST requires the
root node of the given sub-tree to be part of the
sub-structure. In other words, all sub-structures
should be generated from the root of the given
sub-tree as presented in Fig. 2. Therefore the ker-
nel function can be simplified to only capture the
sub-structure rooted at the root of the sub-tree.
ࣥሺܵ, ܵᇱሻ ൌ ∆ሺݎ௦, ݎ௦ᇱሻ
where ݎ௦ and ݎ௦ᇱ are the root nodes of the sub-
tree ܵ and ܵᇱ respectively. The time complexity is
reduced to ܱሺ|ܰௌ |൅ |ܰௌᇱ |൅ |்ܰ |൅ |்ܰᇱ |ሻ.
</bodyText>
<subsectionHeader confidence="0.997226">
3.4 Root only
</subsectionHeader>
<bodyText confidence="0.9819575">
More aggressively, we can simplify the kernel to
only measure the common root node without con-
sidering the complex tree structures. Therefore the
kernel function is simplified to be a binary func-
tion with time complexity ܱሺ1ሻ.
ࣥሺܵ,ܵᇱሻ ൌ ܫሺݎ௦,ݎ௦ᇱሻ
</bodyText>
<sectionHeader confidence="0.990908" genericHeader="method">
4 Plain features
</sectionHeader>
<bodyText confidence="0.999958875">
Besides BTKs, we introduce various plain lexical
features and structural features which can be ex-
pressed as feature functions. The lexical features
with directions are defined as conditional feature
functions based on the conditional lexical transla-
tion probabilities. The plain syntactic structural
features can deal with the structural divergence of
bilingual parse trees in a more general perspective.
</bodyText>
<subsectionHeader confidence="0.997583">
4.1 Lexical and Word Alignment Features
</subsectionHeader>
<bodyText confidence="0.996223">
In this section, we define seven lexical features to
measure semantic similarity of a given sub-tree
pair.
Internal Lexical Features: We define two lex-
ical features with respective to the internal span of
the sub-tree pair.
</bodyText>
<equation confidence="0.548237333333333">
߶ଵሺܵ|ܶሻ ൌ ൫∏௩א௜௡ሺ்ሻ∑௨א௜௡ሺௌሻܲሺݑ|ݒሻ൯
భ
|೔೙ሺ೅ሻ|
309
భ
߶ଶሺܶ|ܵሻ ൌ ൫∏௨א௜௡ሺௌሻ ∑௩א௜௡ሺ்ሻ ܲሺݒ|ݑሻ൯
</equation>
<bodyText confidence="0.99952175">
where ܲሺݒ|ݑሻ refers to the lexical translation
probability from the source word ݑ to the target
word ݒ within the sub-tree spans, while ܲሺݑ|ݒሻ
refers to that from target to source; ݅݊ሺܵሻ refers to
the word set for the internal span of the source
sub-tree ܵ, while ݅݊ሺܶሻ refers to that of the target
sub-tree ܶ.
Internal-External Lexical Features: These
features are motivated by the fact that lexical
translation probabilities within the translational
equivalence tend to be high, and that of the non-
equivalent counterparts tend to be low.
</bodyText>
<table confidence="0.439167">
߶ଷሺܵ|ܶሻ ൌ ൫∏௩א௜௡ሺ்ሻ∑௨א௢௨௧ሺௌሻܲሺݑ|ݒሻ൯ భ
߶ସሺܶ|ܵሻ ൌ ൫∏௨א௜௡ሺௌሻ ∑௩א௢௨௧ሺ்ሻ ܲሺݒ|ݑሻ൯
|೔೙ሺ೅ሻ|
భ
|೔೙ሺೄሻ|
</table>
<bodyText confidence="0.999912285714286">
where ݋ݑݐሺܵሻ refers to the word set for the ex-
ternal span of the source sub-tree ܵ, while ݋ݑݐሺܶሻ
refers to that of the target sub-tree ܶ.
Internal Word Alignment Features: The word
alignment links account much for the co-
occurrence of the aligned terms. We define the
internal word alignment features as follows:
</bodyText>
<equation confidence="0.9959056">
ଵ
∑ ∑ ߜሺݑ, ݒሻ · ൫ܲሺݑ|ݒሻ · ܲሺݒ|ݑሻ൯ଶ
௩א௜௡ሺ்ሻ ௨א௜௡ሺௌሻ
߶ହሺܵ, ܶሻ ൌ
ሺ|݅݊ሺܵሻ |· |݅݊ሺܶሻ|ሻ
</equation>
<bodyText confidence="0.685872">
where
</bodyText>
<equation confidence="0.9219">
ߜሺݑ, ݒሻ ൌ ቄ1 if ሺݑ, ݒሻ is aligned
0 otherwise
</equation>
<bodyText confidence="0.992439375">
The binary function ߜሺݑ, ݒሻ is employed to trig-
ger the computation only when a word aligned
link exists for the two words ሺݑ, ݒሻ within the sub-
tree span.
Internal-External Word Alignment Features:
Similar to the lexical features, we also introduce
the internal-external word alignment features as
follows:
</bodyText>
<table confidence="0.991768">
ሺܵܶሻ ൌ ∑௩א௜௡ሺ்ሻ∑ ௨א௢௨௧ሺௌሻߜሺݑ,ݒሻ · ൫ܲሺݑ|ݒሻ · ଵ
߶, ൌ ܲሺݒ|ݑሻ൯ଶ
߶଻ሺܵ, ܶሻ
∑ ௩א௢௨௧ሺ்ሻ ሺ|݋ݑݐሺܵሻ |ଵ ଵ
∑ ߜሺݑ, · |݅݊ሺܶሻ|ሻ ଶ ଶ
௨א௜௡ሺௌሻ ݒሻ · ൫ܲሺݑ|ݒሻ · ܲሺݒ|ݑሻ൯
ሺ|݅݊ሺܵሻ |· ଵ
|݋ݑݐሺܶሻ|ሻ ଶ
</table>
<bodyText confidence="0.684559">
where
</bodyText>
<equation confidence="0.3043765">
ߜሺݑ, ݒሻ ൌ ቄ1 if ሺݑ, ݒሻ is aligned
0 otherwise
</equation>
<subsectionHeader confidence="0.952407">
4.2 Online Structural Features
</subsectionHeader>
<bodyText confidence="0.999657428571428">
In addition to the lexical correspondence, we also
capture the structural divergence by introducing
the following tree structural features.
Span difference: Translational equivalent sub-
tree pairs tend to share similar length of spans.
Thus the model will penalize the candidate sub-
tree pairs with largely different length of spans.
</bodyText>
<equation confidence="0.9936095">
߮ଵሺܵ,ܶሻ ൌ ቚ|௜௡ሺௌሻ |െ |௜௡ሺ்ሻ|
 |௜௡ሺ܁ሻ  ||௜௡ሺ܂ሻ |ቚ
</equation>
<bodyText confidence="0.964943428571429">
܁ and ܂ refer to the entire source and target parse
trees respectively. Therefore, |݅݊ሺ܁ሻ |and |݅݊ሺ܂ሻ |are
the respective span length of the parse tree used for
normalization.
Number of Descendants: Similarly, the num-
ber of the root’s descendants of the aligned sub-
trees should also correspond.
</bodyText>
<equation confidence="0.991871">
߮ଶሺܵ,ܶሻ ൌ ቚ|ோሺௌሻ |െ |ோሺ்ሻ|ቚ
|ோሺ܁ሻ ||ோሺ܂ሻ|
</equation>
<bodyText confidence="0.999959625">
where ܴሺ.ሻ refers to the descendant set of the
root to a sub-tree.
Tree Depth difference: Intuitively, translation-
al equivalent sub-tree pairs tend to have similar
depth from the root of the parse tree. We allow the
model to penalize the candidate sub-tree pairs with
quite different distance of path from the root of the
parse tree to the root of the sub-tree.
</bodyText>
<equation confidence="0.980735">
߮ଷሺܵ, ܶሻ ൌ ቚ ஽௘௣௧௛ሺௌሻ െ ஽௘௣௧௛ሺ்ሻ
ு௘௜௚௛௧ሺ܁ሻ ு௘௜௚௛௧ሺ܂ሻቚ
</equation>
<sectionHeader confidence="0.998608" genericHeader="method">
5 Alignment Model
</sectionHeader>
<bodyText confidence="0.999472625">
Given feature spaces defined in the last two sec-
tions, we propose a 2-phase sub-tree alignment
model as follows:
In the 1st phase, a kernel based classifier, SVM
in our study, is employed to classify each candi-
date sub-tree pair as aligned or unaligned. The
feature vector of the classifier is computed using a
composite kernel:
</bodyText>
<equation confidence="0.997497">
ࣥሺܵ · ܶ, ܵᇱ · ܶᇱሻ ൌ
ߠ଴ࣥ෡௣ሺܵ · ܶ, ܵᇱ · ܶᇱሻ ൅ ∑ ߠ୧ࣥ෡஻்௄
K ୧ୀଵ ௜ ሺܵ · ܶ, ܵᇱ · ܶᇱሻ
</equation>
<bodyText confidence="0.940263818181818">
ࣥ෡௣ሺ·,·ሻ is the normalized form of the polynomi-
al kennel ࣥ௣ሺ·,·ሻ, which is a polynomial kernel
with the degree of 2, utilizing the plain features.
ࣥ෡஻்௄
௜ ሺ·,·ሻ is the normalized form of the BTK
ࣥ஻்௄
௜ ሺ·,·ሻ , exploring the corresponding sub-
structure space. The composite kernel can be con-
structed using the polynomial kernel for plain fea-
tures and various BTKs for tree structure by linear
combination with coefficient ߠ୧, where ∑ ߠ୧
</bodyText>
<equation confidence="0.427188">
୧ୀ଴ ൌ 1.
K
</equation>
<bodyText confidence="0.999670714285714">
In the 2nd phase, we adopt a greedy search with
respect to the alignment probabilities. Since SVM
is a large margin based discriminative classifier
rather than a probabilistic model, we introduce a
sigmoid function to convert the distance against
the hyperplane to a posterior alignment probability
as follows:
</bodyText>
<figure confidence="0.706045333333333">
|೔೙ሺೄሻ|
ଵ
ଶ
310
P(a+IS, T) _ 1
Chinese English
1+e-D+
1
1 + e-D-
</figure>
<bodyText confidence="0.999936125">
where D+ is the distance for the instances classi-
fied as aligned and D_ is that for the unaligned.
We use P(a+IS, T) as the confidence to conduct
the sure links for those classified as aligned. On
this perspective, the alignment probability is suit-
able as a searching metric. The search space is
reduced to that of the candidates classified as
aligned after the 1st phase.
</bodyText>
<sectionHeader confidence="0.994063" genericHeader="method">
6 Experiments on Sub-Tree Alignments
</sectionHeader>
<bodyText confidence="0.9981152">
In order to evaluate the effectiveness of the align-
ment model and its capability in the applications
requiring syntactic translational equivalences, we
employ two corpora to carry out the sub-tree
alignment evaluation. The first is HIT gold stan-
dard English Chinese parallel tree bank referred as
HIT corpus1. The other is the automatically parsed
bilingual tree pairs selected from FBIS corpus (al-
lowing minor parsing errors) with human anno-
tated sub-tree alignment.
</bodyText>
<subsectionHeader confidence="0.997926">
6.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.99966328">
HIT corpus, which is collected from English learn-
ing text books in China as well as example sen-
tences in dictionaries, is used for the gold standard
corpus evaluation. The word segmentation, toke-
nization and parse-tree in the corpus are manually
constructed or checked. The corpus is constructed
with manually annotated sub-tree alignment. The
annotation strictly reserves the semantic equiva-
lence of the aligned sub-tree pair. Only sure links
are conducted in the internal node level, without
considering possible links adopted in word align-
ment. A different annotation criterion of the Chi-
nese parse tree, designed by the annotator, is em-
ployed. Compared with the widely used Penn
TreeBank annotation, the new criterion utilizes
some different grammar tags and is able to effec-
tively describe some rare language phenomena in
Chinese. The annotator still uses Penn TreeBank
annotation on the English side. The statistics of
HIT corpus used in our experiment is shown in
Table 1. We use 5000 sentences for experiment
and divide them into three parts, with 3k for train-
ing, 1k for testing and 1k for tuning the parameters
of kernels and thresholds of pruning the negative
instances.
</bodyText>
<footnote confidence="0.991798">
1HIT corpus is designed and constructed by HIT-MITLAB.
http://mitlab.hit.edu.cn/index.php/resources.html .
</footnote>
<table confidence="0.975285">
# of Sentence pair 5000
Avg. Sentence Length 12.93 12.92
Avg. # of sub-tree 21.40 23.58
Avg. # of alignment 11.60
</table>
<tableCaption confidence="0.999836">
Table 1. Corpus Statistics for HIT corpus
</tableCaption>
<bodyText confidence="0.999820518518518">
Most linguistically motivated syntax based
SMT systems require an automatic parser to per-
form the rule induction. Thus, it is important to
evaluate the sub-tree alignment on the automati-
cally parsed corpus with parsing errors. In addition,
HIT corpus is not applicable for MT experiment
due to the problems of domain divergence, annota-
tion discrepancy (Chinese parse tree employs a
different grammar from Penn Treebank annota-
tions) and degree of tolerance for parsing errors.
Due to the above issues, we annotate a new data
set to apply the sub-tree alignment in machine
translation. We randomly select 300 bilingual sen-
tence pairs from the Chinese-English FBIS corpus
with the length &lt;_ 30 in both the source and target
sides. The selected plain sentence pairs are further
parsed by Stanford parser (Klein and Manning,
2003) on both the English and Chinese sides. We
manually annotate the sub-tree alignment for the
automatically parsed tree pairs according to the
definition in Section 1. To be fully consistent with
the definition, we strictly reserve the semantic
equivalence for the aligned sub-trees to keep a
high precision. In other words, we do not conduct
any doubtful links. The corpus is further divided
into 200 aligned tree pairs for training and 100 for
testing as shown in Table 2.
</bodyText>
<tableCaption confidence="0.818371">
Chinese
# of Sentence pair
Avg. Sentence Length
Avg. # of sub-tree
Avg. # of alignment
Table 2. Statistics of FBIS selected Corpus
</tableCaption>
<subsectionHeader confidence="0.999303">
6.2 Baseline approach
</subsectionHeader>
<bodyText confidence="0.9855917">
We implement the work in Tinsley et al. (2007) as
our baseline methodology.
Given a tree pair &lt; S, T &gt; , the baseline ap-
proach first takes all the links between the sub-tree
pairs as alignment hypotheses, i.e., the Cartesian
product of the two sub-tree sets:
[Sl,..., Si, ... , SI} x IT,, ... , Ti, ... , Til
By using the lexical translation probabilities,
each hypothesis is assigned an alignment score.
All hypotheses with zero score are pruned out.
</bodyText>
<figure confidence="0.7126195">
P(a_IS, T) _
300
16.94
28.97
17.07
English
20.81
34.39
</figure>
<page confidence="0.994373">
311
</page>
<table confidence="0.999883071428571">
Feature Space P R F
Lex 61.62 58.33 59.93
Lex +Online Str 70.08 69.02 69.54
Plain +dBTK-STT 80.36 78.08 79.20
Plain +dBTK-RdSTT 87.52 74.13 80.27
Plain +dBTK-RgSTT 88.54 70.18 78.30
Plain +dBTK-Root 81.05 84.38 82.68
Plain +iBTK-STT 81.57 73.51 77.33
Plain +iBTK-RdSTT 82.27 77.85 80.00
Plain +iBTK-RgSTT 82.92 78.77 80.80
Plain +iBTK-Root 76.37 76.81 76.59
Plain +dBTK-Root 85.53 85.12 85.32
+iBTK-RgSTT
Baseline 64.14 66.99 65.53
</table>
<tableCaption confidence="0.6819245">
Table 3. Structure feature contribution for HIT test set
*Plain= Lex +Online Str
</tableCaption>
<bodyText confidence="0.999965823529412">
Then the algorithm iteratively selects the link of
the sub-tree pairs with the maximum score as a
sure link, and blocks all hypotheses that contradict
with this link and itself, until no non-blocked hy-
potheses remain.
The baseline system uses many heuristics in
searching the optimal solutions with alternative
score functions. Heuristic skip1 skips the tied hy-
potheses with the same score, until it finds the
highest-scoring hypothesis with no competitors of
the same score. Heuristic skip2 deals with the
same problem. Initially, it skips over the tied hy-
potheses. When a hypothesis sub-tree pair (Si,Tj)
without any competitor of the same score is found,
where neither Si nor Tj has been skipped over, the
hypothesis is chosen as a sure link. Heuristic
span1 postpones the selection of the hypotheses
on the POS level. Since the highest-scoring hypo-
theses tend to appear on the leaf nodes, it may in-
troduce ambiguity when conducting the alignment
for a POS node whose child word appears twice in
a sentence.
The baseline method proposes two score func-
tions based on the lexical translation probability.
They also compute the score function by splitting
the tree into the internal and external components.
Tinsley et al. (2007) adopt the lexical transla-
tion probabilities dumped by GIZA++ (Och and
Ney, 2003) to compute the span based scores for
each pair of sub-trees. Although all of their heuris-
tics combinations are re-implemented in our study,
we only present the best result among them with
the highest Recall and F-value as our baseline,
denoted as skip2_s1_span12.
</bodyText>
<footnote confidence="0.991559333333333">
2 s1 denotes score function 1 in Tinsley et al. (2007),
skip2_s1_span1 denotes the utilization of heuristics skip2 and
span1 while using score function 1
</footnote>
<table confidence="0.999814785714286">
Feature Space P R F
Lex 73.48 71.66 72.56
Lex +Online Str 77.02 73.63 75.28
Plain +dBTK-STT 81.44 74.42 77.77
Plain +dBTK-RdSTT 81.40 69.29 74.86
Plain +dBTK-RgSTT 81.90 67.32 73.90
Plain +dBTK-Root 78.60 80.90 79.73
Plain +iBTK-STT 82.94 79.44 81.15
Plain +iBTK-RdSTT 83.14 80 81.54
Plain +iBTK-RgSTT 83.09 79.72 81.37
Plain +iBTK-Root 78.61 79.49 79.05
Plain +dBTK-Root 82.70 82.70 82.70
+iBTK-RdSTT
Baseline 70.48 78.70 74.36
</table>
<tableCaption confidence="0.999818">
Table 4. Structure feature contribution for FBIS test set
</tableCaption>
<subsectionHeader confidence="0.995137">
6.3 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999913238095238">
We use SVM with binary classes as the classifier.
In case of the implementation, we modify the Tree
Kernel tool (Moschitti, 2004) and SVMLight
(Joachims, 1999). The coefficient Bi for the com-
posite kernel are tuned with respect to F-measure
(F) on the development set of HIT corpus. We
empirically set C=2.4 for SVM and use a = 0.23,
the default parameter A = 0.4 for BTKs.
Since the negative training instances largely
overwhelm the positive instances, we prune the
negative instances using the thresholds according
to the lexical feature functions (01, 02, 03, 04) and
online structural feature functions (V1, V2, V3 ).
Those thresholds are also tuned on the develop-
ment set of HIT corpus with respect to F-measure.
To learn the lexical and word alignment fea-
tures for both the proposed model and the baseline
method, we train GIZA++ on the entire FBIS bi-
lingual corpus (240k). The evaluation is conducted
by means of Precision (P), Recall (R) and F-
measure (F).
</bodyText>
<subsectionHeader confidence="0.996295">
6.4 Experimental results
</subsectionHeader>
<bodyText confidence="0.9971907">
In Tables 3 and 4, we incrementally enlarge the
feature spaces in certain order for both corpora
and examine the feature contribution to the align-
ment results. In detail, the iBTKs and dBTKs are
firstly combined with the polynomial kernel for
plain features individually, then the best iBTK and
dBTK are chosen to construct a more complex
composite kernel along with the polynomial kernel
for both corpora. The experimental results show
that:
</bodyText>
<listItem confidence="0.892526333333333">
• All the settings with structural features of the
proposed approach achieve better performance
than the baseline method. This is because the
</listItem>
<page confidence="0.997475">
312
</page>
<bodyText confidence="0.998938">
baseline only assesses semantic similarity using
the lexical features. The improvement suggests
that the proposed framework with syntactic
structural features is more effective in modeling
the bilingual syntactic correspondence.
</bodyText>
<listItem confidence="0.582127944444444">
• By introducing BTKs to construct a composite
kernel, the performance in both corpora is sig-
nificantly improved against only using the poly-
nomial kernel for plain features. This suggests
that the structural features captured by BTKs are
quite useful for the sub-tree alignment task. We
also try to use BTKs alone without the poly-
nomial kernel for plain features; however, the
performance is rather low. This suggests that the
structure correspondence cannot be used to
measure the semantically equivalent tree struc-
tures alone, since the same syntactic structure
tends to be reused in the same parse tree and
lose the ability of disambiguation to some extent.
In other words, to capture the semantic similari-
ty, structure features requires lexical features to
cooperate.
• After comparing iBTKs with the corresponding
</listItem>
<bodyText confidence="0.953971898305085">
dBTKs, we find that for FBIS corpus, iBTK
greatly outperforms dBTK in any feature space
except the Root space. However, when it comes
the HIT corpus, the gaps between the corres-
ponding iBTKs and dBTKs are much closer,
while on the Root space, dBTK outperforms
iBTK to a large amount. This finding can be ex-
plained by the relationship between the amount
of training data and the high dimensional feature
space. Since dBTKs are constructed in a joint
manner which obtains a much larger high di-
mensional feature space than those of iBTKs,
dBTKs require more training data to excel its
capability, otherwise it will suffer from the data
sparseness problem. The reason that dBTK out-
performs iBTK in the feature space of Root in
FBIS corpus is that although it is a joint feature
space, the Root node pairs can be constructed
from a close set of grammar tags and to form a
relatively low dimensional space.
As a result, when applying to FBIS corpus,
which only contains limited amount of training
data, dBTKs will suffer more from the data
sparseness problem, and therefore, a relatively
low performance. When enlarging the amount of
training corpus to the HIT corpus, the ability of
dBTKs excels and the benefit from data increas-
ing of dBTKs is more significant than iBTKs.
• We also find that the introduction of BTKs gains
more improvement in HIT gold standard corpus
than in FBIS corpus. Other than the factor of
the amount of training data, this is also because
the plain features in Table 3 are not as effective
as those in Table 4, since they are trained on
FBIS corpus which facilitates Table 4 more with
respect to the domains. On the other hand, the
grammatical tags and syntactic tree structures
are more accurate in HIT corpus, which facili-
tates the performance of BTKs in Table 3.
• On the comparison across the different feature
spaces of BTKs, we find that STT, RdSTT and
TgSTT are rather selective, since Recalls of
those feature spaces are relatively low, exp. for
HIT corpus. However, the Root sub-structure
obtains a satisfactory Recall for both corpora.
That’s why we attempt to construct a more
complex composite kernel in adoption of the
kernel of dBTK-Root as below.
• To gain an extra performance boosting, we fur-
ther construct a composite kernel which includes
the best iBTK and the best dBTK for each cor-
pus along with the polynomial kernel for plain
features. In the HIT corpus, we use dBTK in the
Root space and iBTK in the RgSST space; while
for FBIS corpus, we use dBTK in the Root
space and iBTK in the RdSST space. The expe-
rimental results suggest that by combining iBTK
and dBTK together, we can achieve more im-
provement.
</bodyText>
<sectionHeader confidence="0.994535" genericHeader="method">
7 Experiments on Machine Translation
</sectionHeader>
<bodyText confidence="0.9997088">
In addition to the intrinsic alignment evaluation,
we further conduct the extrinsic MT evaluation.
We explore the effectiveness of sub-tree alignment
for both phrase based and linguistically motivated
syntax based SMT systems.
</bodyText>
<subsectionHeader confidence="0.9736">
7.1 Experimental configuration
</subsectionHeader>
<bodyText confidence="0.999976142857143">
In the experiments, we train the translation model
on FBIS corpus (7.2M (Chinese) + 9.2M (English)
words in 240,000 sentence pairs) and train a 4-
gram language model on the Xinhua portion of the
English Gigaword corpus (181M words) using the
SRILM Toolkits (Stolcke, 2002). We use these
sentences with less than 50 characters from the
NIST MT-2002 test set as the development set (to
speed up tuning for syntax based system) and the
NIST MT-2005 test set as our test set. We use the
Stanford parser (Klein and Manning, 2003) to
parse bilingual sentences on the training set and
Chinese sentences on the development and test set.
The evaluation metric is case-sensitive BLEU-4.
</bodyText>
<page confidence="0.998285">
313
</page>
<table confidence="0.834799833333333">
System Model BLEU
Moses BP* 23.86
EWoS 25.38
Syntax STSSG 25.92
STSSG DirC 25.95
EWoS 26.45
</table>
<tableCaption confidence="0.973764">
Table 5. MT evaluation on various systems
*BP denotes bilingual phrases
</tableCaption>
<bodyText confidence="0.999146303030303">
For the phrase based system, we use Moses
(Koehn et al., 2007) with its default settings. For
the syntax based system, since sub-tree alignment
can directly benefit Tree-2-Tree based systems,
we apply the sub-tree alignment in a syntax sys-
tem based on Synchronous Tree Substitution
Grammar (STSG) (Zhang et al., 2007). The STSG
based decoder uses a pair of elementary tree3 as a
basic translation unit. Recent research on tree
based systems shows that relaxing the restriction
from tree structure to tree sequence structure
(Synchronous Tree Sequence Substitution Gram-
mar: STSSG) significantly improves the transla-
tion performance (Zhang et al., 2008). We imple-
ment the STSG/STSSG based model in the Pisces
decoder with the identical features and settings in
Sun et al. (2009). In the Pisces decoder, the
STSSG based decoder translates each span itera-
tively in a bottom up manner which guarantees
that when translating a source span, any of its sub-
spans is already translated. The STSG based de-
coding can be easily performed with the STSSG
decoder by restricting the translation rule set to be
elementary tree pairs only.
As for the alignment setting, we use the word
alignment trained on the entire FBIS (240k) cor-
pus by GIZA++ with heuristic grow-diag-final for
both Moses and the syntax system. For sub-tree-
alignment, we use the above word alignment to
learn lexical/word alignment feature, and train
with the FBIS training corpus (200) using the
composite kernel of Plain+dBTK-Root+iBTK-
RdSTT.
</bodyText>
<subsectionHeader confidence="0.976962">
7.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999042">
Compared with the adoption of word alignment,
translational equivalences generated from struc-
tural alignment tend to be more grammatically
</bodyText>
<footnote confidence="0.8390355">
3 An elementary tree is a fragment whose leaf nodes can be
either non-terminal symbols or terminal symbols.
</footnote>
<bodyText confidence="0.960703366666667">
aware and syntactically meaningful. However,
utilizing syntactic translational equivalences alone
for machine translation loses the capability of
modeling non-syntactic phrases (Koehn et al.,
2003). Consequently, instead of using phrases
constraint by sub-tree alignment alone, we attempt
to combine word alignment and sub-tree align-
ment and deploy the capability of both with two
methods.
• Directly Concatenate (DirC) is operated by di-
rectly concatenating the rule set genereted from
sub-tree alignment and the original rule set gen-
erated from word alignment (Tinsley et al.,
2009). As shown in Table 5, we gain minor im-
provement in the Bleu score for all configura-
tions.
• Alternatively, we proposed a new approach to
generate the rule set from the scratch. We con-
strain the bilingual phrases to be consistent with
Either Word alignment or Sub-tree alignment
(EWoS) instead of being originally consistent
with the word alignment only. The method helps
tailoring the rule set decently without redundant
counts for syntactic rules. The performance is
further improved compared to DirC in all sys-
tems.
The findings suggest that with the modeling of
non-syntactic phrases maintained, more emphasis
on syntactic phrases can benefit both the phrase
and syntax based SMT systems.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999982428571428">
In this paper, we explore syntactic structure fea-
tures by means of Bilingual Tree Kernels and ap-
ply them to bilingual sub-tree alignment along
with various lexical and plain structural features.
We use both gold standard tree bank and the au-
tomatically parsed corpus for the sub-tree align-
ment evaluation. Experimental results show that
our model significantly outperforms the baseline
method and the proposed Bilingual Tree Kernels
are very effective in capturing the cross-lingual
structural similarity. Further experiment shows
that the obtained sub-tree alignment benefits both
phrase and syntax based MT systems by deliver-
ing more weight on syntactic phrases.
</bodyText>
<sectionHeader confidence="0.998665" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999605333333333">
We thank MITLAB4 in Harbin Institute of Tech-
nology for licensing us their sub-tree alignment
corpus for our research.
</bodyText>
<figure confidence="0.957857428571429">
4 http://mitlab.hit.edu.cn/ .
DirC 23.98
EWoS 24.48
Syntax
STSG
STSG 24.71
DirC 25.16
</figure>
<page confidence="0.996788">
314
</page>
<sectionHeader confidence="0.998174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913268656716">
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP-08. 877-886.
Nello Cristianini and John Shawe-Taylor. 2000. An
introduction to support vector machines and other
kernelbased learning methods. Cambridge: Cam-
bridge University Press.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS-01.
Declan Groves, Mary Hearne and Andy Way. 2004.
Robust sub-sentential alignment of phrase-structure
trees. In Proceedings of COLING-04, pages 1072-
1078.
Kenji Imamura. 2001. Hierarchical Phrase Alignment
Harmonized with Parsing. In Proceedings of
NLPRS-01, Tokyo. 377-384.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. SchÄolkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, MIT press.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL-
03. 423-430.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL-03. 48-54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of ACL-07. 177-180.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19-51, March.
Alessandro Moschitti. 2004. A Study on Convolution
Kernels for Shallow Semantic Parsing. In Proceed-
ings of ACL-04.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP-02.
901-904.
Jun Sun, Min Zhang and Chew Lim Tan. 2009. A non-
contiguous Tree Sequence Alignment-based Model
for Statistical Machine Translation. In Proceedings
of ACL-IJCNLP-09. 914-922.
John Tinsley, Ventsislav Zhechev, Mary Hearne and
Andy Way. 2007. Robust language pair-independent
sub-tree alignment. In Proceedings of MT Summit
XI -07.
John Tinsley, Mary Hearne and Andy Way. 2009. Pa-
rallel treebanks in phrase-based statistical machine
translation. In Proceedings of CICLING-09.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of ACL-COLING-06. 825-
832.
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng
Li and Chew Lim Tan. 2007. A tree-to-tree align-
ment-based model for statistical machine transla-
tion. In Proceedings of MT Summit XI -07. 535-542.
Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li,
Chew Lim Tan and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-08. 559-567.
</reference>
<page confidence="0.999307">
315
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556403">
<title confidence="0.999191">Exploring Syntactic Structural Features for Sub-Tree Alignment using Bilingual Tree Kernels</title>
<author confidence="0.980762">Min Chew Lim</author>
<affiliation confidence="0.736709">for Infocomm Research of Computing, National University of Singapore</affiliation>
<email confidence="0.728489">sunjun@comp.nus.edu.sgmzhang@i2r.a-star.edu.sgtancl@comp.nus.edu.sg</email>
<abstract confidence="0.993967684210526">We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-08.</booktitle>
<pages>877--886</pages>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of EMNLP-08. 877-886.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An introduction to support vector machines and other kernelbased learning methods. Cambridge:</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10409" citStr="Cristianini and Shawe-Taylor (2000)" startWordPosition="1686" endWordPosition="1689">le to explicitly compute the kernel function by expressing the sub-trees as feature vectors. In order to achieve convenient computation, we deduce the kernel function as the above. The deduction from (1) to (2) is derived according to the fact that the number of identical substructure pairs rooted in the node pairs ሺ݊௦, ݊௧ሻ and ሺ݊௦ᇱ, ݊௧ᇱሻ equals to the product of the respective counts. As a result, the dBTK can be evaluated as a product of two monolingual tree kernels. Here we verify the correctness of the kernel by directly constructing the feature space for the inner product. Alternatively, Cristianini and Shawe-Taylor (2000) prove the positive semi-definite characteristic of the tensor product of two kernels. The decomposition benefits the efficient computation to use the algorithm for the monolingual tree kernel in Section 2.1. The computational complexity of the dBTK is still ܱሺ|ܰௌ|·|ܰௌᇱ | |்ܰ |·|்ܰᇱ|ሻ. 3 Sub-structure Spaces for BTKs The syntactic translational equivalences under BTKs are evaluated with respective to the substructures factorized from the candidate sub-tree pairs. In this section, we propose different substructures to facilitate the measurement of syntactic similarity for sub-tree alignment. S</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An introduction to support vector machines and other kernelbased learning methods. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS-01.</booktitle>
<contexts>
<context position="3778" citStr="Collins and Duffy, 2001" startWordPosition="554" endWordPosition="558">2010 Association for Computational Linguistics and Imamura (2001) propose some score functions based on the lexical similarity and co-occurrence. These works fail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute. Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing (Moschitti, 2004) and Relation Extraction (Zhang et al. 2006). However, all those studies are carried out in monolingual tasks. In multilingual tasks such as machine translation, tree kernels are seldom applied. In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment. This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree</context>
<context position="7324" citStr="Collins and Duffy, 2001" startWordPosition="1120" endWordPosition="1123">ቀ#ሺݐଵሻ, ..., #ሺݐሻ, ..., #ሺݐหหሻቁ where #ሺݏሻ and #ሺݐሻ are the numbers of occurrences of the sub-structures ݏ and ݐ. In order to compute the dot product of the feature vectors in the exponentially high dimensional feature space, we introduce the tree kernel functions as follows: ்ሺܵ · ܶ, ܵᇱ · ܶᇱሻ ൌ ሺܵ, ܵᇱሻ  ሺܶ, ܶᇱሻ The iBTK is defined as a composite kernel consisting of a source tree kernel and a target tree kernel which measures the source and the target structural similarity respectively. Therefore, the composite kernel can be computed using the ordinary monolingual tree kernels (Collins and Duffy, 2001). ሺܵ, ܵᇱሻ ൌ ߶ሺܵሻ, ߶ሺܵᇱሻ  ൌ∑ ቀ∑ ೞאேೄ ܫሺ݊௦ሻ · ∑ೞᇲאேೄ ܫሺ݊௦ᇱሻ | |ᇲ ቁ ୀଵ ൌ ∑∑ ೞᇲאேೄ ∆ሺ݊௦, ݊௦ᇱሻ ೞאேೄ ᇲ where ܰௌ and ܰௌᇱ refer to the node sets of the source sub-tree ܵ and ܵᇱ respectvely. ܫሺ݊௦ሻ is an indicator function which equals to 1 iff the substructure ݏ is rooted at the node ݊௦ and 0 otherwise.∆ሺ݊௦, ݊௦ᇱሻ ൌ ∑ ൫ܫሺ݊௦ሻ · ܫሺ݊௦ᇱሻ൯ | |is the numୀଵ ber of identical sub-structures rooted at ݊௦ and ݊௦ᇱ. Then we compute the ∆ሺ݊௦, ݊௦ᇱሻ function as follows: 307 (1) If the production rule at ݊௦ and ݊௦ᇱ are different, ∆ሺ݊௦, ݊௦ᇱሻ ൌ 0; (2)else if both ݊௦and ݊௦ᇱ are POS tags, ∆ሺ݊௦, ݊௦ᇱሻ ൌ ߣ; </context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Proceedings of NIPS-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Declan Groves</author>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Robust sub-sentential alignment of phrase-structure trees.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>1072--1078</pages>
<contexts>
<context position="2891" citStr="Groves et al. (2004)" startWordPosition="424" endWordPosition="427"> sub-tree alignment, translational equivalent sub-tree pairs are coupled as aligned counterparts. Each pair consists of both the lexical constituents and their maximum tree structures generated over the lexical sequences in the original parse trees. Due to the 1-to-1 mapping between sub-trees and tree nodes, sub-tree alignment can also be considered as node alignment by conducting multiple links across the internal nodes as shown in Fig. 1. Previous studies conduct sub-tree alignments by either using a rule based method or conducting some similarity measurement only based on lexical features. Groves et al. (2004) conduct sub-tree alignment by using some heuristic rules, lack of extensibility and generality. Tinsley et al. (2007) 306 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306–315, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and Imamura (2001) propose some score functions based on the lexical similarity and co-occurrence. These works fail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic str</context>
</contexts>
<marker>Groves, Hearne, Way, 2004</marker>
<rawString>Declan Groves, Mary Hearne and Andy Way. 2004. Robust sub-sentential alignment of phrase-structure trees. In Proceedings of COLING-04, pages 1072-1078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
</authors>
<title>Hierarchical Phrase Alignment Harmonized with Parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of NLPRS-01,</booktitle>
<pages>377--384</pages>
<location>Tokyo.</location>
<contexts>
<context position="3219" citStr="Imamura (2001)" startWordPosition="471" endWordPosition="472">lso be considered as node alignment by conducting multiple links across the internal nodes as shown in Fig. 1. Previous studies conduct sub-tree alignments by either using a rule based method or conducting some similarity measurement only based on lexical features. Groves et al. (2004) conduct sub-tree alignment by using some heuristic rules, lack of extensibility and generality. Tinsley et al. (2007) 306 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306–315, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and Imamura (2001) propose some score functions based on the lexical similarity and co-occurrence. These works fail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute. Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree struc</context>
</contexts>
<marker>Imamura, 2001</marker>
<rawString>Kenji Imamura. 2001. Hierarchical Phrase Alignment Harmonized with Parsing. In Proceedings of NLPRS-01, Tokyo. 377-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods -Support Vector Learning,</booktitle>
<editor>In B. SchÄolkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT press.</publisher>
<contexts>
<context position="26444" citStr="Joachims, 1999" startWordPosition="4341" endWordPosition="4342"> 77.02 73.63 75.28 Plain +dBTK-STT 81.44 74.42 77.77 Plain +dBTK-RdSTT 81.40 69.29 74.86 Plain +dBTK-RgSTT 81.90 67.32 73.90 Plain +dBTK-Root 78.60 80.90 79.73 Plain +iBTK-STT 82.94 79.44 81.15 Plain +iBTK-RdSTT 83.14 80 81.54 Plain +iBTK-RgSTT 83.09 79.72 81.37 Plain +iBTK-Root 78.61 79.49 79.05 Plain +dBTK-Root 82.70 82.70 82.70 +iBTK-RdSTT Baseline 70.48 78.70 74.36 Table 4. Structure feature contribution for FBIS test set 6.3 Experimental settings We use SVM with binary classes as the classifier. In case of the implementation, we modify the Tree Kernel tool (Moschitti, 2004) and SVMLight (Joachims, 1999). The coefficient Bi for the composite kernel are tuned with respect to F-measure (F) on the development set of HIT corpus. We empirically set C=2.4 for SVM and use a = 0.23, the default parameter A = 0.4 for BTKs. Since the negative training instances largely overwhelm the positive instances, we prune the negative instances using the thresholds according to the lexical feature functions (01, 02, 03, 04) and online structural feature functions (V1, V2, V3 ). Those thresholds are also tuned on the development set of HIT corpus with respect to F-measure. To learn the lexical and word alignment f</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. SchÄolkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods -Support Vector Learning, MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL03.</booktitle>
<pages>423--430</pages>
<contexts>
<context position="22393" citStr="Klein and Manning, 2003" startWordPosition="3679" endWordPosition="3682">rpus with parsing errors. In addition, HIT corpus is not applicable for MT experiment due to the problems of domain divergence, annotation discrepancy (Chinese parse tree employs a different grammar from Penn Treebank annotations) and degree of tolerance for parsing errors. Due to the above issues, we annotate a new data set to apply the sub-tree alignment in machine translation. We randomly select 300 bilingual sentence pairs from the Chinese-English FBIS corpus with the length &lt;_ 30 in both the source and target sides. The selected plain sentence pairs are further parsed by Stanford parser (Klein and Manning, 2003) on both the English and Chinese sides. We manually annotate the sub-tree alignment for the automatically parsed tree pairs according to the definition in Section 1. To be fully consistent with the definition, we strictly reserve the semantic equivalence for the aligned sub-trees to keep a high precision. In other words, we do not conduct any doubtful links. The corpus is further divided into 200 aligned tree pairs for training and 100 for testing as shown in Table 2. Chinese # of Sentence pair Avg. Sentence Length Avg. # of sub-tree Avg. # of alignment Table 2. Statistics of FBIS selected Cor</context>
<context position="32380" citStr="Klein and Manning, 2003" startWordPosition="5331" endWordPosition="5334">phrase based and linguistically motivated syntax based SMT systems. 7.1 Experimental configuration In the experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words in 240,000 sentence pairs) and train a 4- gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002). We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set (to speed up tuning for syntax based system) and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is case-sensitive BLEU-4. 313 System Model BLEU Moses BP* 23.86 EWoS 25.38 Syntax STSSG 25.92 STSSG DirC 25.95 EWoS 26.45 Table 5. MT evaluation on various systems *BP denotes bilingual phrases For the phrase based system, we use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substituti</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of ACL03. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL-03.</booktitle>
<pages>48--54</pages>
<contexts>
<context position="34667" citStr="Koehn et al., 2003" startWordPosition="5687" endWordPosition="5690">nment to learn lexical/word alignment feature, and train with the FBIS training corpus (200) using the composite kernel of Plain+dBTK-Root+iBTKRdSTT. 7.2 Experimental results Compared with the adoption of word alignment, translational equivalences generated from structural alignment tend to be more grammatically 3 An elementary tree is a fragment whose leaf nodes can be either non-terminal symbols or terminal symbols. aware and syntactically meaningful. However, utilizing syntactic translational equivalences alone for machine translation loses the capability of modeling non-syntactic phrases (Koehn et al., 2003). Consequently, instead of using phrases constraint by sub-tree alignment alone, we attempt to combine word alignment and sub-tree alignment and deploy the capability of both with two methods. • Directly Concatenate (DirC) is operated by directly concatenating the rule set genereted from sub-tree alignment and the original rule set generated from word alignment (Tinsley et al., 2009). As shown in Table 5, we gain minor improvement in the Bleu score for all configurations. • Alternatively, we proposed a new approach to generate the rule set from the scratch. We constrain the bilingual phrases t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL-03. 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07.</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin</location>
<contexts>
<context position="32763" citStr="Koehn et al., 2007" startWordPosition="5394" endWordPosition="5397">entences with less than 50 characters from the NIST MT-2002 test set as the development set (to speed up tuning for syntax based system) and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is case-sensitive BLEU-4. 313 System Model BLEU Moses BP* 23.86 EWoS 25.38 Syntax STSSG 25.92 STSSG DirC 25.95 EWoS 26.45 Table 5. MT evaluation on various systems *BP denotes bilingual phrases For the phrase based system, we use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implem</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of ACL-07. 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="25353" citStr="Och and Ney, 2003" startWordPosition="4168" endWordPosition="4171">kipped over, the hypothesis is chosen as a sure link. Heuristic span1 postpones the selection of the hypotheses on the POS level. Since the highest-scoring hypotheses tend to appear on the leaf nodes, it may introduce ambiguity when conducting the alignment for a POS node whose child word appears twice in a sentence. The baseline method proposes two score functions based on the lexical translation probability. They also compute the score function by splitting the tree into the internal and external components. Tinsley et al. (2007) adopt the lexical translation probabilities dumped by GIZA++ (Och and Ney, 2003) to compute the span based scores for each pair of sub-trees. Although all of their heuristics combinations are re-implemented in our study, we only present the best result among them with the highest Recall and F-value as our baseline, denoted as skip2_s1_span12. 2 s1 denotes score function 1 in Tinsley et al. (2007), skip2_s1_span1 denotes the utilization of heuristics skip2 and span1 while using score function 1 Feature Space P R F Lex 73.48 71.66 72.56 Lex +Online Str 77.02 73.63 75.28 Plain +dBTK-STT 81.44 74.42 77.77 Plain +dBTK-RdSTT 81.40 69.29 74.86 Plain +dBTK-RgSTT 81.90 67.32 73.90</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19-51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A Study on Convolution Kernels for Shallow Semantic Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04.</booktitle>
<contexts>
<context position="3929" citStr="Moschitti, 2004" startWordPosition="579" endWordPosition="580">ail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute. Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing (Moschitti, 2004) and Relation Extraction (Zhang et al. 2006). However, all those studies are carried out in monolingual tasks. In multilingual tasks such as machine translation, tree kernels are seldom applied. In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment. This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree structures. We propose two kinds of BTKs named dependent Bilingual Tree Kernel (dBTK), which takes the sub-tree pair as a whole and independent Biling</context>
<context position="26414" citStr="Moschitti, 2004" startWordPosition="4337" endWordPosition="4338">.48 71.66 72.56 Lex +Online Str 77.02 73.63 75.28 Plain +dBTK-STT 81.44 74.42 77.77 Plain +dBTK-RdSTT 81.40 69.29 74.86 Plain +dBTK-RgSTT 81.90 67.32 73.90 Plain +dBTK-Root 78.60 80.90 79.73 Plain +iBTK-STT 82.94 79.44 81.15 Plain +iBTK-RdSTT 83.14 80 81.54 Plain +iBTK-RgSTT 83.09 79.72 81.37 Plain +iBTK-Root 78.61 79.49 79.05 Plain +dBTK-Root 82.70 82.70 82.70 +iBTK-RdSTT Baseline 70.48 78.70 74.36 Table 4. Structure feature contribution for FBIS test set 6.3 Experimental settings We use SVM with binary classes as the classifier. In case of the implementation, we modify the Tree Kernel tool (Moschitti, 2004) and SVMLight (Joachims, 1999). The coefficient Bi for the composite kernel are tuned with respect to F-measure (F) on the development set of HIT corpus. We empirically set C=2.4 for SVM and use a = 0.23, the default parameter A = 0.4 for BTKs. Since the negative training instances largely overwhelm the positive instances, we prune the negative instances using the thresholds according to the lexical feature functions (01, 02, 03, 04) and online structural feature functions (V1, V2, V3 ). Those thresholds are also tuned on the development set of HIT corpus with respect to F-measure. To learn th</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. In Proceedings of ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP-02.</booktitle>
<pages>901--904</pages>
<contexts>
<context position="32128" citStr="Stolcke, 2002" startWordPosition="5287" endWordPosition="5288">TK together, we can achieve more improvement. 7 Experiments on Machine Translation In addition to the intrinsic alignment evaluation, we further conduct the extrinsic MT evaluation. We explore the effectiveness of sub-tree alignment for both phrase based and linguistically motivated syntax based SMT systems. 7.1 Experimental configuration In the experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words in 240,000 sentence pairs) and train a 4- gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002). We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set (to speed up tuning for syntax based system) and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is case-sensitive BLEU-4. 313 System Model BLEU Moses BP* 23.86 EWoS 25.38 Syntax STSSG 25.92 STSSG DirC 25.95 EWoS 26.45 Table 5. MT evaluation on various systems *BP denotes bilingual phrases For the phrase based system</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of ICSLP-02. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Sun</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>A noncontiguous Tree Sequence Alignment-based Model for Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP-09.</booktitle>
<pages>914--922</pages>
<contexts>
<context position="33477" citStr="Sun et al. (2009)" startWordPosition="5505" endWordPosition="5508">nefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its subspans is already translated. The STSG based decoding can be easily performed with the STSSG decoder by restricting the translation rule set to be elementary tree pairs only. As for the alignment setting, we use the word alignment trained on the entire FBIS (240k) corpus by GIZA++ with heuristic grow-diag-final for both Moses and the syntax system. For sub-treealignment, we use the above word alignment to learn lexical/word a</context>
</contexts>
<marker>Sun, Zhang, Tan, 2009</marker>
<rawString>Jun Sun, Min Zhang and Chew Lim Tan. 2009. A noncontiguous Tree Sequence Alignment-based Model for Statistical Machine Translation. In Proceedings of ACL-IJCNLP-09. 914-922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Tinsley</author>
<author>Ventsislav Zhechev</author>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Robust language pair-independent sub-tree alignment.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit XI -07.</booktitle>
<contexts>
<context position="1575" citStr="Tinsley et al. (2007)" startWordPosition="222" endWordPosition="225"> two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1 Introduction Syntax based Statistical Machine Translation (SMT) systems allow the translation process to be more grammatically performed, which provides decent reordering capability. However, most of the syntax based systems construct the syntactic translation rules based on word alignment, which not only suffers from the pipeline errors, but also fails to effectively utilize the syntactic structural features. To address those deficiencies, Tinsley et al. (2007) attempt to directly capture the syntactic translational equivalences by automatically conducting sub-tree alignment, which can be defined as follows: A sub-tree alignment process pairs up sub-tree pairs across bilingual parse trees whose contexts are semantically translational equivalent. According to Tinsley et al. (2007), a sub-tree aligned parse tree pair follows the following criteria: (i) a node can only be linked once; Figure 1: Sub-tree alignment as referred to Node alignment (ii) descendants of a source linked node may only link to descendants of its target linked counterpart; (iii) a</context>
<context position="3009" citStr="Tinsley et al. (2007)" startWordPosition="441" endWordPosition="444">of both the lexical constituents and their maximum tree structures generated over the lexical sequences in the original parse trees. Due to the 1-to-1 mapping between sub-trees and tree nodes, sub-tree alignment can also be considered as node alignment by conducting multiple links across the internal nodes as shown in Fig. 1. Previous studies conduct sub-tree alignments by either using a rule based method or conducting some similarity measurement only based on lexical features. Groves et al. (2004) conduct sub-tree alignment by using some heuristic rules, lack of extensibility and generality. Tinsley et al. (2007) 306 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 306–315, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and Imamura (2001) propose some score functions based on the lexical similarity and co-occurrence. These works fail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tr</context>
<context position="23065" citStr="Tinsley et al. (2007)" startWordPosition="3793" endWordPosition="3796"> annotate the sub-tree alignment for the automatically parsed tree pairs according to the definition in Section 1. To be fully consistent with the definition, we strictly reserve the semantic equivalence for the aligned sub-trees to keep a high precision. In other words, we do not conduct any doubtful links. The corpus is further divided into 200 aligned tree pairs for training and 100 for testing as shown in Table 2. Chinese # of Sentence pair Avg. Sentence Length Avg. # of sub-tree Avg. # of alignment Table 2. Statistics of FBIS selected Corpus 6.2 Baseline approach We implement the work in Tinsley et al. (2007) as our baseline methodology. Given a tree pair &lt; S, T &gt; , the baseline approach first takes all the links between the sub-tree pairs as alignment hypotheses, i.e., the Cartesian product of the two sub-tree sets: [Sl,..., Si, ... , SI} x IT,, ... , Ti, ... , Til By using the lexical translation probabilities, each hypothesis is assigned an alignment score. All hypotheses with zero score are pruned out. P(a_IS, T) _ 300 16.94 28.97 17.07 English 20.81 34.39 311 Feature Space P R F Lex 61.62 58.33 59.93 Lex +Online Str 70.08 69.02 69.54 Plain +dBTK-STT 80.36 78.08 79.20 Plain +dBTK-RdSTT 87.52 7</context>
<context position="25272" citStr="Tinsley et al. (2007)" startWordPosition="4155" endWordPosition="4158">ithout any competitor of the same score is found, where neither Si nor Tj has been skipped over, the hypothesis is chosen as a sure link. Heuristic span1 postpones the selection of the hypotheses on the POS level. Since the highest-scoring hypotheses tend to appear on the leaf nodes, it may introduce ambiguity when conducting the alignment for a POS node whose child word appears twice in a sentence. The baseline method proposes two score functions based on the lexical translation probability. They also compute the score function by splitting the tree into the internal and external components. Tinsley et al. (2007) adopt the lexical translation probabilities dumped by GIZA++ (Och and Ney, 2003) to compute the span based scores for each pair of sub-trees. Although all of their heuristics combinations are re-implemented in our study, we only present the best result among them with the highest Recall and F-value as our baseline, denoted as skip2_s1_span12. 2 s1 denotes score function 1 in Tinsley et al. (2007), skip2_s1_span1 denotes the utilization of heuristics skip2 and span1 while using score function 1 Feature Space P R F Lex 73.48 71.66 72.56 Lex +Online Str 77.02 73.63 75.28 Plain +dBTK-STT 81.44 74</context>
</contexts>
<marker>Tinsley, Zhechev, Hearne, Way, 2007</marker>
<rawString>John Tinsley, Ventsislav Zhechev, Mary Hearne and Andy Way. 2007. Robust language pair-independent sub-tree alignment. In Proceedings of MT Summit XI -07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Tinsley</author>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Parallel treebanks in phrase-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of CICLING-09.</booktitle>
<contexts>
<context position="35053" citStr="Tinsley et al., 2009" startWordPosition="5747" endWordPosition="5750">terminal symbols or terminal symbols. aware and syntactically meaningful. However, utilizing syntactic translational equivalences alone for machine translation loses the capability of modeling non-syntactic phrases (Koehn et al., 2003). Consequently, instead of using phrases constraint by sub-tree alignment alone, we attempt to combine word alignment and sub-tree alignment and deploy the capability of both with two methods. • Directly Concatenate (DirC) is operated by directly concatenating the rule set genereted from sub-tree alignment and the original rule set generated from word alignment (Tinsley et al., 2009). As shown in Table 5, we gain minor improvement in the Bleu score for all configurations. • Alternatively, we proposed a new approach to generate the rule set from the scratch. We constrain the bilingual phrases to be consistent with Either Word alignment or Sub-tree alignment (EWoS) instead of being originally consistent with the word alignment only. The method helps tailoring the rule set decently without redundant counts for syntactic rules. The performance is further improved compared to DirC in all systems. The findings suggest that with the modeling of non-syntactic phrases maintained, </context>
</contexts>
<marker>Tinsley, Hearne, Way, 2009</marker>
<rawString>John Tinsley, Mary Hearne and Andy Way. 2009. Parallel treebanks in phrase-based statistical machine translation. In Proceedings of CICLING-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING-06.</booktitle>
<pages>825--832</pages>
<contexts>
<context position="3973" citStr="Zhang et al. 2006" startWordPosition="584" endWordPosition="587">ndering the syntactic rich task of sub-tree alignment less convincing and attractive. This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. In addition, explicitly utilizing syntactic tree fragments results in exponentially high dimensional feature vectors, which is hard to compute. Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing (Moschitti, 2004) and Relation Extraction (Zhang et al. 2006). However, all those studies are carried out in monolingual tasks. In multilingual tasks such as machine translation, tree kernels are seldom applied. In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment. This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree structures. We propose two kinds of BTKs named dependent Bilingual Tree Kernel (dBTK), which takes the sub-tree pair as a whole and independent Bilingual Tree Kernel (iBTK), which individually m</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features. In Proceedings of ACL-COLING-06. 825-832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>AiTi Aw</author>
<author>Jun Sun</author>
<author>Sheng Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>A tree-to-tree alignment-based model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit XI</booktitle>
<volume>07</volume>
<pages>535--542</pages>
<contexts>
<context position="33018" citStr="Zhang et al., 2007" startWordPosition="5433" endWordPosition="5436"> sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is case-sensitive BLEU-4. 313 System Model BLEU Moses BP* 23.86 EWoS 25.38 Syntax STSSG 25.92 STSSG DirC 25.95 EWoS 26.45 Table 5. MT evaluation on various systems *BP denotes bilingual phrases For the phrase based system, we use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translatin</context>
</contexts>
<marker>Zhang, Jiang, Aw, Sun, Li, Tan, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li and Chew Lim Tan. 2007. A tree-to-tree alignment-based model for statistical machine translation. In Proceedings of MT Summit XI -07. 535-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>AiTi Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08.</booktitle>
<pages>559--567</pages>
<contexts>
<context position="33352" citStr="Zhang et al., 2008" startWordPosition="5483" endWordPosition="5486">use Moses (Koehn et al., 2007) with its default settings. For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al., 2007). The STSG based decoder uses a pair of elementary tree3 as a basic translation unit. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al., 2008). We implement the STSG/STSSG based model in the Pisces decoder with the identical features and settings in Sun et al. (2009). In the Pisces decoder, the STSSG based decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its subspans is already translated. The STSG based decoding can be easily performed with the STSSG decoder by restricting the translation rule set to be elementary tree pairs only. As for the alignment setting, we use the word alignment trained on the entire FBIS (240k) corpus by GIZA++ with heuristic grow-di</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li, Chew Lim Tan and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proceedings of ACL-08. 559-567.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>