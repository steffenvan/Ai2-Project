<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004422">
<title confidence="0.963256">
The CUED HiFST System for the WMT10 Translation Shared Task
</title>
<author confidence="0.981304">
Juan Pino Gonzalo Iglesias$&apos; Adri`a de Gispert
Graeme Blackwood Jamie Brunning William Byrne
</author>
<affiliation confidence="0.999747">
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
</affiliation>
<email confidence="0.912562">
{jmp84,gi212,ad465,gwb24,jjjb2,wjb31}@eng.cam.ac.uk
</email>
<affiliation confidence="0.457995">
t Department of Signal Processing and Communications, University of Vigo, Vigo, Spain
</affiliation>
<sectionHeader confidence="0.968326" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999834875">
This paper describes the Cambridge Uni-
versity Engineering Department submis-
sion to the Fifth Workshop on Statistical
Machine Translation. We report results for
the French-English and Spanish-English
shared translation tasks in both directions.
The CUED system is based on HiFST, a
hierarchical phrase-based decoder imple-
mented using weighted finite-state trans-
ducers. In the French-English task, we
investigate the use of context-dependent
alignment models. We also show that
lattice minimum Bayes-risk decoding is
an effective framework for multi-source
translation, leading to large gains in BLEU
score.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997433210526316">
This paper describes the Cambridge University
Engineering Department (CUED) system submis-
sion to the ACL 2010 Fifth Workshop on Statis-
tical Machine Translation (WMT10). Our trans-
lation system is HiFST (Iglesias et al., 2009a), a
hierarchical phrase-based decoder that generates
translation lattices directly. Decoding is guided
by a CYK parser based on a synchronous context-
free grammar induced from automatic word align-
ments (Chiang, 2007). The decoder is imple-
mented with Weighted Finite State Transducers
(WFSTs) using standard operations available in
the OpenFst libraries (Allauzen et al., 2007). The
use of WFSTs allows fast and efficient exploration
of a vast translation search space, avoiding search
errors in decoding. It also allows better integration
with other steps in our translation pipeline such as
5-gram language model (LM) rescoring and lattice
minimum Bayes-risk (LMBR) decoding.
</bodyText>
<affiliation confidence="0.935418">
&apos;Now a member of the Department of Engineering, Uni-
versity of Cambridge, Cambridge, CB2 1PZ, U.K.
</affiliation>
<table confidence="0.9996833">
# Sentences # Tokens # Types
(A)Europarl+News-Commentary
FR 1.7 M 52.4M 139.7k
EN 47.6M 121.6k
(B)Europarl+News-Commentary+UN
FR 8.7 M 277.9M 421.0k
EN 241.4M 482.1k
(C)Europarl+News-Commentary+UN+Giga
FR 30.2 M 962.4M 2.4M
EN 815.3M 2.7M
</table>
<tableCaption confidence="0.874224">
Table 1: Parallel data sets used for French-to-
English experiments.
</tableCaption>
<bodyText confidence="0.999904583333333">
We participated in the French-English and
Spanish-English translation shared tasks in each
translation direction. This paper describes the de-
velopment of these systems. Additionally, we re-
port multi-source translation experiments that lead
to very large gains in BLEU score.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing. Section 3 presents and discusses re-
sults and Section 4 describes an additional experi-
ment on multi-source translation.
</bodyText>
<sectionHeader confidence="0.976206" genericHeader="method">
2 System Development
</sectionHeader>
<bodyText confidence="0.999995">
We built three French-English and two Spanish-
English systems, trained on different portions of
the parallel data sets available for this shared task.
Statistics for the different parallel sets are sum-
marised in Tables 1 and 2. No additional parallel
data was used. As will be shown, the largest paral-
lel corpus gave the best results in French, but this
was not the case in Spanish.
</bodyText>
<subsectionHeader confidence="0.997586">
2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.997862">
The data was minimally cleaned by replacing
HTML-related metatags by their corresponding
</bodyText>
<page confidence="0.986902">
155
</page>
<note confidence="0.693962">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 155–160,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.999846857142857">
# Sentences # Tokens # Types
(A) Europarl + News-Commentary
SP 1.7M 49.4M 167.2k
EN 47.0M 122.7k
(B) Europarl + News-Commentary + UN
SP 6.5M 205.6M 420.8k
EN 192.0M 402.8k
</table>
<tableCaption confidence="0.676293666666667">
Table 2: Parallel data sets used for Spanish-to-
English experiments.
UTF8 token (e.g., replacing “&amp;” by “&amp;”) as
this interacts with tokenization. Data was then to-
kenized and lowercased, so mixed case is added as
post-processing.
</tableCaption>
<subsectionHeader confidence="0.992675">
2.2 Alignments
</subsectionHeader>
<bodyText confidence="0.999966761904762">
Parallel data was aligned using the MTTK toolkit
(Deng and Byrne, 2005). In the English-to-French
and English-to-Spanish directions, we trained
a word-to-phrase HMM model with maximum
phrase length of 2. In the French to English and
Spanish to English directions, we trained a word-
to-phrase HMM Model with a bigram translation
table and maximum phrase length of 4.
We also trained context-dependent alignment
models (Brunning et al., 2009) for the French-
English medium-size (B) dataset. The context of
a word is based on its part-of-speech and the part-
of-speech tags of the surrounding words. These
tags were obtained by applying the TnT Tagger
(Brants, 2000) for English and the TreeTagger
(Schmid, 1994) for French. Decision tree clus-
tering based on optimisation of the EM auxiliary
function was used to group contexts that trans-
late similarly. Unfortunately, time constraints pre-
vented us from training context-dependent models
for the larger (C) dataset.
</bodyText>
<subsectionHeader confidence="0.992517">
2.3 Language Model
</subsectionHeader>
<bodyText confidence="0.999938727272727">
For each target language, we used the SRILM
Toolkit (Stolcke, 2002) to estimate separate 4-
gram LMs with Kneser-Ney smoothing (Kneser
and Ney, 1995), for each of the corpora listed in
Tables 3, 4 and 5. The LM vocabulary was ad-
justed to the parallel data set used. The compo-
nent models of each language pair were then in-
terpolated to form a single LM for use in first-pass
translation decoding. For French-to-English trans-
lation, the interpolation weights were optimised
for perplexity on a development set.
</bodyText>
<table confidence="0.998388636363636">
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 246.4M
CNA 1.3M 34.8M
LTW 12.9M 298.7M
XIN 16.0M 352.5M
AFP 30.4M 710.6M
APW 62.1M 1268.6M
NYT 73.6M 1622.5M
Giga 21.4M 573.8M
News 48.7M 1128.4M
Total 275.4M 6236.4M
</table>
<tableCaption confidence="0.997188">
Table 3: English monolingual training corpora.
</tableCaption>
<table confidence="0.999529857142857">
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 282.8
AFP 25.2M 696.0M
APW 12.7M 300.6M
News 15.2M 373.5M
Giga 21.4M 684.4M
Total 83.5 M 2337.3M
</table>
<tableCaption confidence="0.988177">
Table 4: French monolingual training corpora.
</tableCaption>
<table confidence="0.99799025">
Corpus # Sentences # Tokens
NC + News 4.0M 110.8M
EU + Gigaword (5g) 249.4M 1351.5M
Total 253.4 M 1462.3M
</table>
<tableCaption confidence="0.999564">
Table 5: Spanish monolingual training corpora.
</tableCaption>
<bodyText confidence="0.9999">
The Spanish-English first pass LM was trained
directly on the NC+News portion of monolingual
data, as we did not find improvements by using
Europarl. The second pass rescoring LM used all
available data.
</bodyText>
<subsectionHeader confidence="0.999461">
2.4 Grammar Extraction and Decoding
</subsectionHeader>
<bodyText confidence="0.999816076923077">
After unioning the Viterbi alignments, phrase-
based rules of up to five source words in length
were extracted, hierarchical rules with up to two
non-contiguous non-terminals in the source side
were then extracted applying the restrictions de-
scribed in (Chiang, 2007). For Spanish-English
and French-English tasks, we used a shallow-1
grammar where hierarchical rules are allowed to
be applied only once on top of phrase-based rules.
This has been shown to perform as well as a
fully hierarchical grammar for a Europarl Spanish-
English task (Iglesias et al., 2009b).
For translation, we used the HiFST de-
</bodyText>
<page confidence="0.993967">
156
</page>
<bodyText confidence="0.998080166666667">
coder (Iglesias et al., 2009a). HiFST is a hierarchi-
cal decoder that builds target word lattices guided
by a probabilistic synchronous context-free gram-
mar. Assuming N to be the set of non-terminals
and T the set of terminals or words, then we can
define the grammar as a set R = IRr} of rules
</bodyText>
<equation confidence="0.4985015">
Rr : N —* (&apos;yr,αr) / pr, where N E N; and
&apos;y, α E IN U T}+.
</equation>
<bodyText confidence="0.998254303030303">
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N —* &apos;y. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N, x, y),
spanning sx+y−1
x on the source sentence s1...sJ.
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N, x, y) of the CYK grid, we build a
target language word lattice L(N, x, y) containing
every translation of sx+y−1
x from every derivation
headed by N. For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al., 2007) based on the combined trans-
lation and LM scores.
As explained before, we are using shallow-1 hi-
erarchical grammars (de Gispert et al., 2010) in
our experiments for WMT2010. One very inter-
esting aspect is that HiFST is able to build ex-
act search spaces with this model, i.e. there is no
pruning in search that may lead to spurious under-
generation errors.
</bodyText>
<subsectionHeader confidence="0.963166">
2.5 Parameter Optimisation
</subsectionHeader>
<bodyText confidence="0.999973416666667">
Minimum error rate training (MERT) (Och, 2003)
under the BLEU score (Papineni et al., 2001) opti-
mises the weights of the following decoder fea-
tures with respect to the newstest2008 develop-
ment set: target LM, number of usages of the
glue rule, word and rule insertion penalties, word
deletion scale factor, source-to-target and target-
to-source translation models, source-to-target and
target-to-source lexical models, and three binary
rule count features inspired by Bender et al. (2007)
indicating whether a rule occurs once, twice, or
more than twice in the parallel training data.
</bodyText>
<subsectionHeader confidence="0.996183">
2.6 Lattice Rescoring
</subsectionHeader>
<bodyText confidence="0.9998216">
One of the advantages of HiFST is direct gener-
ation of large translation lattices encoding many
alternative translation hypotheses. These first-pass
lattices are rescored with second-pass higher-order
LMs prior to LMBR.
</bodyText>
<subsectionHeader confidence="0.760902">
2.6.1 5-gram LM Lattice Rescoring
</subsectionHeader>
<bodyText confidence="0.999938923076923">
We build sentence-specific, zero-cutoff stupid-
backoff (Brants et al., 2007) 5-gram LMs esti-
mated over approximately 6.2 billion words for
English, 2.3 billion words for French, and 1.4 bil-
lion words for Spanish. For the English-French
task, the second-pass LM training data is the same
monolingual data used for the first-pass LMs (as
summarised in Tables 3, 4). The Spanish second-
pass 5-gram LM includes an additional 1.4 billion
words of monolingual data from the Spanish Giga-
Word Second Edition (Mendonca et al., 2009) and
Europarl, which were not included in the first-pass
LM (see Table 5).
</bodyText>
<subsectionHeader confidence="0.579063">
2.6.2 LMBR Decoding
</subsectionHeader>
<bodyText confidence="0.9996394">
Minimum Bayes-risk (MBR) decoding (Kumar
and Byrne, 2004) over the full evidence space
of the 5-gram rescored lattices was applied to
select the translation hypothesis that maximises
the conditional expected gain under the linearised
sentence-level BLEU score (Tromble et al., 2008;
Blackwood and Byrne, 2010). The unigram preci-
sion p and average recall ratio r were set as de-
scribed in Tromble et al. (2008) using the new-
stest2008 development set.
</bodyText>
<subsectionHeader confidence="0.993129">
2.7 Hypothesis Combination
</subsectionHeader>
<bodyText confidence="0.999825">
Linearised lattice minimum Bayes-risk decoding
(Tromble et al., 2008) can also be used as an ef-
fective framework for multiple lattice combination
(de Gispert et al., 2010). For the French-English
language pair, we used LMBR to combine transla-
tion lattices produced by systems trained on alter-
native data sets.
</bodyText>
<subsectionHeader confidence="0.998547">
2.8 Post-processing
</subsectionHeader>
<bodyText confidence="0.999223">
For both Spanish-English and French-English sys-
tems, the recasing procedure was performed with
the SRILM toolkit. For the Spanish-English sys-
tem, we created models from the GigaWord set
corresponding to each system output language.
</bodyText>
<page confidence="0.994397">
157
</page>
<table confidence="0.9997584">
Task Configuration newstest2008 newstest2009 newstest2010
HiFST (A) 23.4 26.4 –
HiFST (B) 24.0 27.3 –
HiFST (B)CD 24.2 27.6 28.0
FR → EN +5g+LMBR 24.6 28.4 28.9
HiFST (C) 24.7 28.4 28.5
+5g+LMBR 25.3 29.1 29.3
LMBR (B)CD+(C) 25.6 29.3 29.6
HiFST (A) 22.5 24.2 –
HiFST (B) 23.4 24.8 –
HiFST (B)CD 23.3 24.8 26.7
EN → FR +5g+LMBR 23.7 25.3 27.1
HiFST (C) 23.6 25.6 27.4
+5g+LMBR 23.9 25.8 27.8
LMBR (B)CD+(C) 24.2 26.1 28.2
</table>
<tableCaption confidence="0.876092">
Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference
lowercase IBM BLEU. Bold results correspond to submitted systems.
</tableCaption>
<bodyText confidence="0.994873375">
For the French-English system, the English model
was trained using the monolingual News corpus
and the target side of the News-Commentary cor-
pus, whereas the French model was trained using
all available constrained French data.
English, Spanish and French outputs were also
detokenized before submission. In French, words
separated by apostrophes were joined.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="method">
3 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.877436">
French–English Language Pair
</subsectionHeader>
<bodyText confidence="0.999863714285714">
Results are reported in Table 6. We can see
that using more parallel data consistently improves
performance. In the French-to-English direction,
the system HiFST (B) improves over HiFST (A)
by +0.9 BLEU and HiFST (C) improves over
HiFST (B) by +1.1 BLEU on the newstest2009
development set prior to any rescoring. The
same trend can be observed in the English-to-
French direction (+0.6 BLEU and +0.8 BLEU im-
provement). The use of context dependent align-
ment models gives a small improvement in the
French-to-English direction: system (B)CD im-
proves by +0.3 BLEU over system (B) on new-
stest2009. In the English-to-French direction,
there is no improvement nor degradation in per-
formance. 5-gram and LMBR rescoring also give
consistent improvement throughout the datasets.
Finally, combination between the medium-size
system (B)CD and the full-size system (C) gives
further small gains in BLEU over LMBR on each
individual system.
</bodyText>
<subsectionHeader confidence="0.96846">
Spanish–English Language Pair
</subsectionHeader>
<bodyText confidence="0.999851368421053">
Results are reported in Table 7. We report experi-
mental results on two systems. The HiFST(A) sys-
tem is built on the Europarl + News-Commentary
training set. Systems HiFST (B),(B2) and (B3)
use UN data in different ways. System (B) simply
uses all the data for the standard rule extraction
procedure. System HiFST (B2) includes UN data
to build alignment models and therefore reinforce
alignments obtained from smaller dataset (A), but
extracts rules only from dataset (A). HiFST (B3)
combines hierarchical phrases extracted for sys-
tem (A) with phrases extracted from system (B).
Unfortunately, these three larger data strategies
lead to degradation over using only the smaller
dataset (A). For this reason, our best systems only
use the Euparl + News-Commentary parallel data.
This is surprising given that additional data was
helpful for the French-English task. Solving this
issue is left for future work.
</bodyText>
<sectionHeader confidence="0.94725" genericHeader="method">
4 Multi-Source Translation Experiments
</sectionHeader>
<bodyText confidence="0.9967498">
Multi-source translation (Och and Ney, 2001;
Schroeder et al., 2009) is possible whenever mul-
tiple translations of the source language input sen-
tence are available. The motivation for multi-
source translation is that some of the ambiguity
that must be resolved in translating between one
pair of languages may not be present in a differ-
ent pair. In the following experiments, multiple
LMBR is applied for the first time to the task of
multi-source translation.
</bodyText>
<page confidence="0.993547">
158
</page>
<table confidence="0.999754625">
Task Configuration newstest2008 newstest2009 newstest2010
HiFST (A) 24.6 26.0 29.1
+5g+LMBR 25.4 27.0 30.5
SP → EN HiFST (B) 23.7 25.4 –
HiFST (B2) 24.3 25.7 –
HiFST (B3) 24.2 25.6 –
EN → SP HiFST (A) 23.9 24.5 28.0
+5g+LMBR 24.7 25.5 29.1
</table>
<tableCaption confidence="0.989526">
Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBM
BLEU. Bold results correspond to submitted systems.
</tableCaption>
<table confidence="0.9999435">
Configuration newstest2008 newstest2009 newstest2010
FR→EN HiFST+5g 24.8 28.5 28.8
+LMBR 25.3 29.0 29.2
ES→EN HiFST+5g 25.2 26.8 30.1
+LMBR 25.4 26.9 30.3
FR→EN + ES→EN LMBR 27.2 30.4 32.0
</table>
<tableCaption confidence="0.8164745">
Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translation
of French (FR) and Spanish (ES) into English (EN).
</tableCaption>
<bodyText confidence="0.99930325">
Separate second-pass 5-gram rescored lattices
EFR and EES are generated for each test set sen-
tence using the French-to-English and Spanish-to-
English HiFST translation systems. The MBR hy-
pothesis space is formed as the union of these lat-
tices. In a similar manner to MBR decoding over
multiple k-best lists in de Gispert et al. (2009),
the path posterior probability of each n-gram u re-
quired for linearised LMBR is computed as a lin-
ear interpolation of the posterior probabilities ac-
cording to each individual lattice so that p(u|E) =
AFR p(u|EFR) + AES p(u|EES), where p(u|E) is the
sum of the posterior probabilities of all paths con-
taining the n-gram u. The interpolation weights
AFR + AES = 1 are optimised for BLEU score on
the development set newstest2008.
The results of single-system and multi-source
LMBR decoding are shown in Table 8. The opti-
mised interpolation weights were AFR = 0.55 and
AES = 0.45. Single-system LMBR gives relatively
small gains on these test sets. Much larger gains
are obtained through multi-source MBR combina-
tion. Compared to the best of the single-system 5-
gram rescored lattices, the BLEU score improves
by +2.0 for newstest2008, +1.9 for newstest2009,
and +1.9 for newstest2010. For scoring with re-
spect to a single reference, these are very large
gains indeed.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.99999">
We have described the CUED submission to
WMT10 using HiFST, a hierarchical phrase-based
translation system. Results are very competitive in
terms of automatic metric for both English-French
and English-Spanish tasks in both directions. In
the French-English task, we have seen that the UN
and Giga additional parallel data are helpful. It
is surprising that UN data did not help for the
Spanish-English language pair.
Future work includes investigating this issue,
developing detokenization tailored to each output
language and applying context dependent align-
ment models to larger parallel datasets.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982431">
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022. Gonzalo Iglesias was sup-
ported by the Spanish Government research grant
BES-2007-15956 (projects TEC2006-13694-C03-
03 and TEC2009-14094-C04-04).
</bodyText>
<page confidence="0.998412">
159
</page>
<sectionHeader confidence="0.990057" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917563218391">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11–23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396–401.
Graeme Blackwood and William Byrne. 2010. Ef-
ficient Path Counting Transducers for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices (to appear). In Proceedings of the
ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858–867.
Thorsten Brants. 2000. Tnt – a statistical part-of-
speech tagger. In Proceedings ofANLP, pages 224–
231, April.
Jamie Brunning, Adri`a de Gispert, and William Byrne.
2009. Context-dependent alignment models for
statistical machine translation. In Proceedings of
HLT/NAACL, pages 110–118.
Jean-C´edric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133–137.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings ofHLT/NAACL, Companion Volume: ShortPa-
pers, pages 73–76.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars (to
appear). In Computational Linguistics.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proceedings of HLT/EMNLP, pages
169–176.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings ofNAACL, pages 433–441.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. The HiFST System for
the EuroParl Spanish-to-English Task. In Proceed-
ings of SEPLN, pages 207–214.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings ofICASSP, volume 1, pages 181–184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169–
176.
Angelo Mendonca, David Graff, and Denise DiPersio.
2009. Spanish Gigaword Second Edition, Linguistic
Data Consortium.
Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Machine Translation
Summit 2001, pages 253–258.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
ofACL, pages 311–318.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word Lattices for Multi-Source Translation.
In Proceedings of EACL, pages 719–727.
Andreas Stolcke. 2002. SRILM–An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901–904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620–629.
</reference>
<page confidence="0.997484">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.421042">
<title confidence="0.998624">The CUED HiFST System for the WMT10 Translation Shared Task</title>
<author confidence="0.981906">Pino Gonzalo de_Graeme Blackwood Jamie Brunning William</author>
<affiliation confidence="0.961625">Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ,</affiliation>
<address confidence="0.458698">of Signal Processing and Communications, University of Vigo, Vigo, Spain</address>
<abstract confidence="0.997211">This paper describes the Cambridge University Engineering Department submission to the Fifth Workshop on Statistical Machine Translation. We report results for the French-English and Spanish-English shared translation tasks in both directions. The CUED system is based on HiFST, a hierarchical phrase-based decoder implemented using weighted finite-state transducers. In the French-English task, we investigate the use of context-dependent alignment models. We also show that lattice minimum Bayes-risk decoding is an effective framework for multi-source translation, leading to large gains in BLEU score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of CIAA,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="1598" citStr="Allauzen et al., 2007" startWordPosition="218" endWordPosition="221">score. 1 Introduction This paper describes the Cambridge University Engineering Department (CUED) system submission to the ACL 2010 Fifth Workshop on Statistical Machine Translation (WMT10). Our translation system is HiFST (Iglesias et al., 2009a), a hierarchical phrase-based decoder that generates translation lattices directly. Decoding is guided by a CYK parser based on a synchronous contextfree grammar induced from automatic word alignments (Chiang, 2007). The decoder is implemented with Weighted Finite State Transducers (WFSTs) using standard operations available in the OpenFst libraries (Allauzen et al., 2007). The use of WFSTs allows fast and efficient exploration of a vast translation search space, avoiding search errors in decoding. It also allows better integration with other steps in our translation pipeline such as 5-gram language model (LM) rescoring and lattice minimum Bayes-risk (LMBR) decoding. &apos;Now a member of the Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K. # Sentences # Tokens # Types (A)Europarl+News-Commentary FR 1.7 M 52.4M 139.7k EN 47.6M 121.6k (B)Europarl+News-Commentary+UN FR 8.7 M 277.9M 421.0k EN 241.4M 482.1k (C)Europarl+News-Commentary+UN+Giga</context>
<context position="8345" citStr="Allauzen et al., 2007" startWordPosition="1312" endWordPosition="1316">rsive algorithm to construct word lattices with all possible translations produced by the hierarchical rules. Construction proceeds by traversing the CYK grid along the back-pointers established in parsing. In each cell (N, x, y) of the CYK grid, we build a target language word lattice L(N, x, y) containing every translation of sx+y−1 x from every derivation headed by N. For efficiency, this lattice can use pointers to lattices on other cells of the grid. In the third step, we apply the word-based LM via standard WFST composition with failure transitions, and perform likelihood-based pruning (Allauzen et al., 2007) based on the combined translation and LM scores. As explained before, we are using shallow-1 hierarchical grammars (de Gispert et al., 2010) in our experiments for WMT2010. One very interesting aspect is that HiFST is able to build exact search spaces with this model, i.e. there is no pruning in search that may lead to spurious undergeneration errors. 2.5 Parameter Optimisation Minimum error rate training (MERT) (Och, 2003) under the BLEU score (Papineni et al., 2001) optimises the weights of the following decoder features with respect to the newstest2008 development set: target LM, number of</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of CIAA, pages 11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Evgeny Matusov</author>
<author>Stefan Hahn</author>
<author>Sasa Hasan</author>
<author>Shahram Khadivi</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH Arabic-to-English spoken language translation system.</title>
<date>2007</date>
<booktitle>In Proceedings of ASRU,</booktitle>
<pages>396--401</pages>
<contexts>
<context position="9214" citStr="Bender et al. (2007)" startWordPosition="1453" endWordPosition="1456">ces with this model, i.e. there is no pruning in search that may lead to spurious undergeneration errors. 2.5 Parameter Optimisation Minimum error rate training (MERT) (Och, 2003) under the BLEU score (Papineni et al., 2001) optimises the weights of the following decoder features with respect to the newstest2008 development set: target LM, number of usages of the glue rule, word and rule insertion penalties, word deletion scale factor, source-to-target and targetto-source translation models, source-to-target and target-to-source lexical models, and three binary rule count features inspired by Bender et al. (2007) indicating whether a rule occurs once, twice, or more than twice in the parallel training data. 2.6 Lattice Rescoring One of the advantages of HiFST is direct generation of large translation lattices encoding many alternative translation hypotheses. These first-pass lattices are rescored with second-pass higher-order LMs prior to LMBR. 2.6.1 5-gram LM Lattice Rescoring We build sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over approximately 6.2 billion words for English, 2.3 billion words for French, and 1.4 billion words for Spanish. For the English</context>
</contexts>
<marker>Bender, Matusov, Hahn, Hasan, Khadivi, Ney, 2007</marker>
<rawString>Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa Hasan, Shahram Khadivi, and Hermann Ney. 2007. The RWTH Arabic-to-English spoken language translation system. In Proceedings of ASRU, pages 396–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>William Byrne</author>
</authors>
<title>Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices (to appear).</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="10512" citStr="Blackwood and Byrne, 2010" startWordPosition="1653" endWordPosition="1656">ata used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) over the full evidence space of the 5-gram rescored lattices was applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Bayes-risk decoding (Tromble et al., 2008) can also be used as an effective framework for multiple lattice combination (de Gispert et al., 2010). For the French-English language pair, we used LMBR to combine translation lattices produced by systems trained on alternative data sets. 2.8 Post-processing For both Spanish-English and French-English systems, the recasing procedure was performed with the SRIL</context>
</contexts>
<marker>Blackwood, Byrne, 2010</marker>
<rawString>Graeme Blackwood and William Byrne. 2010. Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices (to appear). In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="9662" citStr="Brants et al., 2007" startWordPosition="1518" endWordPosition="1521">o-target and targetto-source translation models, source-to-target and target-to-source lexical models, and three binary rule count features inspired by Bender et al. (2007) indicating whether a rule occurs once, twice, or more than twice in the parallel training data. 2.6 Lattice Rescoring One of the advantages of HiFST is direct generation of large translation lattices encoding many alternative translation hypotheses. These first-pass lattices are rescored with second-pass higher-order LMs prior to LMBR. 2.6.1 5-gram LM Lattice Rescoring We build sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over approximately 6.2 billion words for English, 2.3 billion words for French, and 1.4 billion words for Spanish. For the English-French task, the second-pass LM training data is the same monolingual data used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) o</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLP-ACL, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Tnt – a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings ofANLP,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="4627" citStr="Brants, 2000" startWordPosition="687" endWordPosition="688">it (Deng and Byrne, 2005). In the English-to-French and English-to-Spanish directions, we trained a word-to-phrase HMM model with maximum phrase length of 2. In the French to English and Spanish to English directions, we trained a wordto-phrase HMM Model with a bigram translation table and maximum phrase length of 4. We also trained context-dependent alignment models (Brunning et al., 2009) for the FrenchEnglish medium-size (B) dataset. The context of a word is based on its part-of-speech and the partof-speech tags of the surrounding words. These tags were obtained by applying the TnT Tagger (Brants, 2000) for English and the TreeTagger (Schmid, 1994) for French. Decision tree clustering based on optimisation of the EM auxiliary function was used to group contexts that translate similarly. Unfortunately, time constraints prevented us from training context-dependent models for the larger (C) dataset. 2.3 Language Model For each target language, we used the SRILM Toolkit (Stolcke, 2002) to estimate separate 4- gram LMs with Kneser-Ney smoothing (Kneser and Ney, 1995), for each of the corpora listed in Tables 3, 4 and 5. The LM vocabulary was adjusted to the parallel data set used. The component m</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. Tnt – a statistical part-ofspeech tagger. In Proceedings ofANLP, pages 224– 231, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Brunning</author>
<author>Adri`a de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Context-dependent alignment models for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>110--118</pages>
<marker>Brunning, de Gispert, Byrne, 2009</marker>
<rawString>Jamie Brunning, Adri`a de Gispert, and William Byrne. 2009. Context-dependent alignment models for statistical machine translation. In Proceedings of HLT/NAACL, pages 110–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A generalized CYK algorithm for parsing stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proceedings of TAPD,</booktitle>
<pages>133--137</pages>
<contexts>
<context position="7367" citStr="Chappelier and Rajman, 1998" startWordPosition="1150" endWordPosition="1153">een shown to perform as well as a fully hierarchical grammar for a Europarl SpanishEnglish task (Iglesias et al., 2009b). For translation, we used the HiFST de156 coder (Iglesias et al., 2009a). HiFST is a hierarchical decoder that builds target word lattices guided by a probabilistic synchronous context-free grammar. Assuming N to be the set of non-terminals and T the set of terminals or words, then we can define the grammar as a set R = IRr} of rules Rr : N —* (&apos;yr,αr) / pr, where N E N; and &apos;y, α E IN U T}+. HiFST translates in three steps. The first step is a variant of the CYK algorithm (Chappelier and Rajman, 1998), in which we apply hypothesis recombination without pruning. Only the source language sentence is parsed using the corresponding source-side context-free grammar with rules N —* &apos;y. Each cell in the CYK grid is specified by a non-terminal symbol and position: (N, x, y), spanning sx+y−1 x on the source sentence s1...sJ. For the second step, we use a recursive algorithm to construct word lattices with all possible translations produced by the hierarchical rules. Construction proceeds by traversing the CYK grid along the back-pointers established in parsing. In each cell (N, x, y) of the CYK gri</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-C´edric Chappelier and Martin Rajman. 1998. A generalized CYK algorithm for parsing stochastic CFG. In Proceedings of TAPD, pages 133–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1438" citStr="Chiang, 2007" startWordPosition="197" endWordPosition="198">t models. We also show that lattice minimum Bayes-risk decoding is an effective framework for multi-source translation, leading to large gains in BLEU score. 1 Introduction This paper describes the Cambridge University Engineering Department (CUED) system submission to the ACL 2010 Fifth Workshop on Statistical Machine Translation (WMT10). Our translation system is HiFST (Iglesias et al., 2009a), a hierarchical phrase-based decoder that generates translation lattices directly. Decoding is guided by a CYK parser based on a synchronous contextfree grammar induced from automatic word alignments (Chiang, 2007). The decoder is implemented with Weighted Finite State Transducers (WFSTs) using standard operations available in the OpenFst libraries (Allauzen et al., 2007). The use of WFSTs allows fast and efficient exploration of a vast translation search space, avoiding search errors in decoding. It also allows better integration with other steps in our translation pipeline such as 5-gram language model (LM) rescoring and lattice minimum Bayes-risk (LMBR) decoding. &apos;Now a member of the Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K. # Sentences # Tokens # Types (A)Europarl+</context>
<context position="6562" citStr="Chiang, 2007" startWordPosition="1005" endWordPosition="1006">0.8M EU + Gigaword (5g) 249.4M 1351.5M Total 253.4 M 1462.3M Table 5: Spanish monolingual training corpora. The Spanish-English first pass LM was trained directly on the NC+News portion of monolingual data, as we did not find improvements by using Europarl. The second pass rescoring LM used all available data. 2.4 Grammar Extraction and Decoding After unioning the Viterbi alignments, phrasebased rules of up to five source words in length were extracted, hierarchical rules with up to two non-contiguous non-terminals in the source side were then extracted applying the restrictions described in (Chiang, 2007). For Spanish-English and French-English tasks, we used a shallow-1 grammar where hierarchical rules are allowed to be applied only once on top of phrase-based rules. This has been shown to perform as well as a fully hierarchical grammar for a Europarl SpanishEnglish task (Iglesias et al., 2009b). For translation, we used the HiFST de156 coder (Iglesias et al., 2009a). HiFST is a hierarchical decoder that builds target word lattices guided by a probabilistic synchronous context-free grammar. Assuming N to be the set of non-terminals and T the set of terminals or words, then we can define the g</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Sami Virpioja</author>
<author>Mikko Kurimo</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes Risk Combination of Translation Hypotheses from Alternative Morphological Decompositions.</title>
<date>2009</date>
<booktitle>In Proceedings ofHLT/NAACL, Companion Volume: ShortPapers,</booktitle>
<pages>73--76</pages>
<marker>de Gispert, Virpioja, Kurimo, Byrne, 2009</marker>
<rawString>Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, and William Byrne. 2009. Minimum Bayes Risk Combination of Translation Hypotheses from Alternative Morphological Decompositions. In Proceedings ofHLT/NAACL, Companion Volume: ShortPapers, pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Gonzalo Iglesias</author>
<author>Graeme Blackwood</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrase-based translation with weighted finite state transducers and shallow-n grammars (to appear). In Computational Linguistics.</title>
<date>2010</date>
<marker>de Gispert, Iglesias, Blackwood, Banga, Byrne, 2010</marker>
<rawString>Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchical phrase-based translation with weighted finite state transducers and shallow-n grammars (to appear). In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM Word and Phrase Alignment for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="4039" citStr="Deng and Byrne, 2005" startWordPosition="592" endWordPosition="595">Machine Translation and MetricsMATR, pages 155–160, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics # Sentences # Tokens # Types (A) Europarl + News-Commentary SP 1.7M 49.4M 167.2k EN 47.0M 122.7k (B) Europarl + News-Commentary + UN SP 6.5M 205.6M 420.8k EN 192.0M 402.8k Table 2: Parallel data sets used for Spanish-toEnglish experiments. UTF8 token (e.g., replacing “&amp;” by “&amp;”) as this interacts with tokenization. Data was then tokenized and lowercased, so mixed case is added as post-processing. 2.2 Alignments Parallel data was aligned using the MTTK toolkit (Deng and Byrne, 2005). In the English-to-French and English-to-Spanish directions, we trained a word-to-phrase HMM model with maximum phrase length of 2. In the French to English and Spanish to English directions, we trained a wordto-phrase HMM Model with a bigram translation table and maximum phrase length of 4. We also trained context-dependent alignment models (Brunning et al., 2009) for the FrenchEnglish medium-size (B) dataset. The context of a word is based on its part-of-speech and the partof-speech tags of the surrounding words. These tags were obtained by applying the TnT Tagger (Brants, 2000) for English</context>
</contexts>
<marker>Deng, Byrne, 2005</marker>
<rawString>Yonggang Deng and William Byrne. 2005. HMM Word and Phrase Alignment for Statistical Machine Translation. In Proceedings of HLT/EMNLP, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrasebased translation with weighted finite state transducers.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>433--441</pages>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009a. Hierarchical phrasebased translation with weighted finite state transducers. In Proceedings ofNAACL, pages 433–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>The HiFST System for the EuroParl Spanish-to-English Task.</title>
<date>2009</date>
<booktitle>In Proceedings of SEPLN,</booktitle>
<pages>207--214</pages>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009b. The HiFST System for the EuroParl Spanish-to-English Task. In Proceedings of SEPLN, pages 207–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings ofICASSP,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="5095" citStr="Kneser and Ney, 1995" startWordPosition="757" endWordPosition="760">rd is based on its part-of-speech and the partof-speech tags of the surrounding words. These tags were obtained by applying the TnT Tagger (Brants, 2000) for English and the TreeTagger (Schmid, 1994) for French. Decision tree clustering based on optimisation of the EM auxiliary function was used to group contexts that translate similarly. Unfortunately, time constraints prevented us from training context-dependent models for the larger (C) dataset. 2.3 Language Model For each target language, we used the SRILM Toolkit (Stolcke, 2002) to estimate separate 4- gram LMs with Kneser-Ney smoothing (Kneser and Ney, 1995), for each of the corpora listed in Tables 3, 4 and 5. The LM vocabulary was adjusted to the parallel data set used. The component models of each language pair were then interpolated to form a single LM for use in first-pass translation decoding. For French-to-English translation, the interpolation weights were optimised for perplexity on a development set. Corpus # Sentences # Tokens EU + NC + UN 9.0M 246.4M CNA 1.3M 34.8M LTW 12.9M 298.7M XIN 16.0M 352.5M AFP 30.4M 710.6M APW 62.1M 1268.6M NYT 73.6M 1622.5M Giga 21.4M 573.8M News 48.7M 1128.4M Total 275.4M 6236.4M Table 3: English monolingua</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings ofICASSP, volume 1, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="10260" citStr="Kumar and Byrne, 2004" startWordPosition="1616" endWordPosition="1619">ff (Brants et al., 2007) 5-gram LMs estimated over approximately 6.2 billion words for English, 2.3 billion words for French, and 1.4 billion words for Spanish. For the English-French task, the second-pass LM training data is the same monolingual data used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) over the full evidence space of the 5-gram rescored lattices was applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Bayes-risk decoding (Tromble et al., 2008) can also be used as an effective framework for multiple lattice combination (de Gispert et al., 2010). For the </context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of HLT-NAACL, pages 169– 176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelo Mendonca</author>
<author>David Graff</author>
<author>Denise DiPersio</author>
</authors>
<date>2009</date>
<booktitle>Spanish Gigaword Second Edition, Linguistic Data Consortium.</booktitle>
<contexts>
<context position="10108" citStr="Mendonca et al., 2009" startWordPosition="1592" endWordPosition="1595">ices are rescored with second-pass higher-order LMs prior to LMBR. 2.6.1 5-gram LM Lattice Rescoring We build sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over approximately 6.2 billion words for English, 2.3 billion words for French, and 1.4 billion words for Spanish. For the English-French task, the second-pass LM training data is the same monolingual data used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) over the full evidence space of the 5-gram rescored lattices was applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Ba</context>
</contexts>
<marker>Mendonca, Graff, DiPersio, 2009</marker>
<rawString>Angelo Mendonca, David Graff, and Denise DiPersio. 2009. Spanish Gigaword Second Edition, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical multi-source translation.</title>
<date>2001</date>
<booktitle>In Machine Translation Summit</booktitle>
<pages>253--258</pages>
<contexts>
<context position="14196" citStr="Och and Ney, 2001" startWordPosition="2230" endWordPosition="2233">e reinforce alignments obtained from smaller dataset (A), but extracts rules only from dataset (A). HiFST (B3) combines hierarchical phrases extracted for system (A) with phrases extracted from system (B). Unfortunately, these three larger data strategies lead to degradation over using only the smaller dataset (A). For this reason, our best systems only use the Euparl + News-Commentary parallel data. This is surprising given that additional data was helpful for the French-English task. Solving this issue is left for future work. 4 Multi-Source Translation Experiments Multi-source translation (Och and Ney, 2001; Schroeder et al., 2009) is possible whenever multiple translations of the source language input sentence are available. The motivation for multisource translation is that some of the ambiguity that must be resolved in translating between one pair of languages may not be present in a different pair. In the following experiments, multiple LMBR is applied for the first time to the task of multi-source translation. 158 Task Configuration newstest2008 newstest2009 newstest2010 HiFST (A) 24.6 26.0 29.1 +5g+LMBR 25.4 27.0 30.5 SP → EN HiFST (B) 23.7 25.4 – HiFST (B2) 24.3 25.7 – HiFST (B3) 24.2 25.</context>
</contexts>
<marker>Och, Ney, 2001</marker>
<rawString>Franz Josef Och and Hermann Ney. 2001. Statistical multi-source translation. In Machine Translation Summit 2001, pages 253–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="8773" citStr="Och, 2003" startWordPosition="1388" endWordPosition="1389"> cells of the grid. In the third step, we apply the word-based LM via standard WFST composition with failure transitions, and perform likelihood-based pruning (Allauzen et al., 2007) based on the combined translation and LM scores. As explained before, we are using shallow-1 hierarchical grammars (de Gispert et al., 2010) in our experiments for WMT2010. One very interesting aspect is that HiFST is able to build exact search spaces with this model, i.e. there is no pruning in search that may lead to spurious undergeneration errors. 2.5 Parameter Optimisation Minimum error rate training (MERT) (Och, 2003) under the BLEU score (Papineni et al., 2001) optimises the weights of the following decoder features with respect to the newstest2008 development set: target LM, number of usages of the glue rule, word and rule insertion penalties, word deletion scale factor, source-to-target and targetto-source translation models, source-to-target and target-to-source lexical models, and three binary rule count features inspired by Bender et al. (2007) indicating whether a rule occurs once, twice, or more than twice in the parallel training data. 2.6 Lattice Rescoring One of the advantages of HiFST is direct</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="8818" citStr="Papineni et al., 2001" startWordPosition="1394" endWordPosition="1397">step, we apply the word-based LM via standard WFST composition with failure transitions, and perform likelihood-based pruning (Allauzen et al., 2007) based on the combined translation and LM scores. As explained before, we are using shallow-1 hierarchical grammars (de Gispert et al., 2010) in our experiments for WMT2010. One very interesting aspect is that HiFST is able to build exact search spaces with this model, i.e. there is no pruning in search that may lead to spurious undergeneration errors. 2.5 Parameter Optimisation Minimum error rate training (MERT) (Och, 2003) under the BLEU score (Papineni et al., 2001) optimises the weights of the following decoder features with respect to the newstest2008 development set: target LM, number of usages of the glue rule, word and rule insertion penalties, word deletion scale factor, source-to-target and targetto-source translation models, source-to-target and target-to-source lexical models, and three binary rule count features inspired by Bender et al. (2007) indicating whether a rule occurs once, twice, or more than twice in the parallel training data. 2.6 Lattice Rescoring One of the advantages of HiFST is direct generation of large translation lattices enc</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<volume>12</volume>
<location>Manchester, UK.</location>
<contexts>
<context position="4673" citStr="Schmid, 1994" startWordPosition="694" endWordPosition="695">rench and English-to-Spanish directions, we trained a word-to-phrase HMM model with maximum phrase length of 2. In the French to English and Spanish to English directions, we trained a wordto-phrase HMM Model with a bigram translation table and maximum phrase length of 4. We also trained context-dependent alignment models (Brunning et al., 2009) for the FrenchEnglish medium-size (B) dataset. The context of a word is based on its part-of-speech and the partof-speech tags of the surrounding words. These tags were obtained by applying the TnT Tagger (Brants, 2000) for English and the TreeTagger (Schmid, 1994) for French. Decision tree clustering based on optimisation of the EM auxiliary function was used to group contexts that translate similarly. Unfortunately, time constraints prevented us from training context-dependent models for the larger (C) dataset. 2.3 Language Model For each target language, we used the SRILM Toolkit (Stolcke, 2002) to estimate separate 4- gram LMs with Kneser-Ney smoothing (Kneser and Ney, 1995), for each of the corpora listed in Tables 3, 4 and 5. The LM vocabulary was adjusted to the parallel data set used. The component models of each language pair were then interpol</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, volume 12. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josh Schroeder</author>
<author>Trevor Cohn</author>
<author>Philipp Koehn</author>
</authors>
<title>Word Lattices for Multi-Source Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>719--727</pages>
<contexts>
<context position="14221" citStr="Schroeder et al., 2009" startWordPosition="2234" endWordPosition="2237">nts obtained from smaller dataset (A), but extracts rules only from dataset (A). HiFST (B3) combines hierarchical phrases extracted for system (A) with phrases extracted from system (B). Unfortunately, these three larger data strategies lead to degradation over using only the smaller dataset (A). For this reason, our best systems only use the Euparl + News-Commentary parallel data. This is surprising given that additional data was helpful for the French-English task. Solving this issue is left for future work. 4 Multi-Source Translation Experiments Multi-source translation (Och and Ney, 2001; Schroeder et al., 2009) is possible whenever multiple translations of the source language input sentence are available. The motivation for multisource translation is that some of the ambiguity that must be resolved in translating between one pair of languages may not be present in a different pair. In the following experiments, multiple LMBR is applied for the first time to the task of multi-source translation. 158 Task Configuration newstest2008 newstest2009 newstest2010 HiFST (A) 24.6 26.0 29.1 +5g+LMBR 25.4 27.0 30.5 SP → EN HiFST (B) 23.7 25.4 – HiFST (B2) 24.3 25.7 – HiFST (B3) 24.2 25.6 – EN → SP HiFST (A) 23.</context>
</contexts>
<marker>Schroeder, Cohn, Koehn, 2009</marker>
<rawString>Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009. Word Lattices for Multi-Source Translation. In Proceedings of EACL, pages 719–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM–An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>3</volume>
<pages>901--904</pages>
<contexts>
<context position="5013" citStr="Stolcke, 2002" startWordPosition="746" endWordPosition="747">., 2009) for the FrenchEnglish medium-size (B) dataset. The context of a word is based on its part-of-speech and the partof-speech tags of the surrounding words. These tags were obtained by applying the TnT Tagger (Brants, 2000) for English and the TreeTagger (Schmid, 1994) for French. Decision tree clustering based on optimisation of the EM auxiliary function was used to group contexts that translate similarly. Unfortunately, time constraints prevented us from training context-dependent models for the larger (C) dataset. 2.3 Language Model For each target language, we used the SRILM Toolkit (Stolcke, 2002) to estimate separate 4- gram LMs with Kneser-Ney smoothing (Kneser and Ney, 1995), for each of the corpora listed in Tables 3, 4 and 5. The LM vocabulary was adjusted to the parallel data set used. The component models of each language pair were then interpolated to form a single LM for use in first-pass translation decoding. For French-to-English translation, the interpolation weights were optimised for perplexity on a development set. Corpus # Sentences # Tokens EU + NC + UN 9.0M 246.4M CNA 1.3M 34.8M LTW 12.9M 298.7M XIN 16.0M 352.5M AFP 30.4M 710.6M APW 62.1M 1268.6M NYT 73.6M 1622.5M Gig</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM–An Extensible Language Modeling Toolkit. In Proceedings of ICSLP, volume 3, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy W Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>620--629</pages>
<contexts>
<context position="10484" citStr="Tromble et al., 2008" startWordPosition="1649" endWordPosition="1652">the same monolingual data used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) over the full evidence space of the 5-gram rescored lattices was applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Bayes-risk decoding (Tromble et al., 2008) can also be used as an effective framework for multiple lattice combination (de Gispert et al., 2010). For the French-English language pair, we used LMBR to combine translation lattices produced by systems trained on alternative data sets. 2.8 Post-processing For both Spanish-English and French-English systems, the recasing procedure</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy W. Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Proceedings of EMNLP, pages 620–629.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>