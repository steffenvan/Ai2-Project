<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009543">
<note confidence="0.959363333333333">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 24-31
Edmonton, May-June 2003
</note>
<title confidence="0.986112">
In Question Answering, Two Heads Are Better Than One
</title>
<author confidence="0.965587">
Jennifer Chu-Carroll Krzysztof Czuba John Prager Abraham Ittycheriah
</author>
<affiliation confidence="0.793591">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.7909595">
P.O. Box 704
Yorktown Heights, NY 10598, U.S.A.
</address>
<email confidence="0.997912">
jencc,kczuba,jprager,abei@us.ibm.com
</email>
<sectionHeader confidence="0.995615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950789473684">
Motivated by the success of ensemble methods
in machine learning and other areas of natu-
ral language processing, we developed a multi-
strategy and multi-source approach to question
answering which is based on combining the re-
sults from different answering agents searching
for answers in multiple corpora. The answer-
ing agents adopt fundamentally different strate-
gies, one utilizing primarily knowledge-based
mechanisms and the other adopting statistical
techniques. We present our multi-level answer
resolution algorithm that combines results from
the answering agents at the question, passage,
and/or answer levels. Experiments evaluating
the effectiveness of our answer resolution algo-
rithm show a 35.0% relative improvement over
our baseline system in the number of questions
correctly answered, and a 32.8% improvement
according to the average precision metric.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957290909091">
Traditional question answering (QA) systems typically
employ a pipeline approach, consisting roughly of ques-
tion analysis, document/passage retrieval, and answer se-
lection (see e.g., (Prager et al., 2000; Moldovan et al.,
2000; Hovy et al., 2001; Clarke et al., 2001)). Although a
typical QA system classifies questions based on expected
answer types, it adopts the same strategy for locating po-
tential answers from the same corpus regardless of the
question classification. In our own earlier work, we de-
veloped a specialized mechanism called Virtual Annota-
tion for handling definition questions (e.g., “Who was
Galileo?” and “What are antibiotics?”) that consults,
in addition to the standard reference corpus, a structured
knowledge source (WordNet) for answering such ques-
tions (Prager et al., 2001). We have shown that better
performance is achieved by applying Virtual Annotation
and our general purpose QA strategy in parallel. In this
paper, we investigate the impact of adopting such a multi-
strategy and multi-source approach to QA in a more gen-
eral fashion.
Our approach to question answering is additionally
motivated by the success of ensemble methods in ma-
chine learning, where multiple classifiers are employed
and their results are combined to produce the final output
of the ensemble (for an overview, see (Dietterich, 1997)).
Such ensemble methods have recently been adopted in
question answering (Chu-Carroll et al., 2003b; Burger
et al., 2003). In our question answering system, PI-
QUANT, we utilize in parallel multiple answering agents
that adopt different processing strategies and consult dif-
ferent knowledge sources in identifying answers to given
questions, and we employ resolution mechanisms to com-
bine the results produced by the individual answering
agents.
We call our approach multi-strategy since we com-
bine the results from a number of independent agents im-
plementing different answer finding strategies. We also
call it multi-source since the different agents can search
for answers in multiple knowledge sources. In this pa-
per, we focus on two answering agents that adopt fun-
damentally different strategies: one agent uses predomi-
nantly knowledge-based mechanisms, whereas the other
agent is based on statistical methods. Our multi-level
resolution algorithm enables combination of results from
each answering agent at the question, passage, and/or an-
swer levels. Our experiments show that in most cases
our multi-level resolution algorithm outperforms its com-
ponents, supporting a tightly-coupled design for multi-
agent QA systems. Experimental results show signifi-
cant performance improvement over our single-strategy,
single-source baselines, with the best performing multi-
level resolution algorithm achieving a 35.0% relative im-
provement in the number of correct answers and a 32.8%
improvement in average precision, on a previously un-
seen test set.
</bodyText>
<figure confidence="0.999326933333333">
Question
Answer
Question
Analysis
Answer
Resolution
QFrame
Answering Agents
Knowledge-Based
Answering Agent
Statistical
Answering Agent
Definition Q
Answering Agent
KSP-Based
Answering Agent
QGoals
Knowledge Sources
WordNet
Keyword
Search
Semantic
Search
KSP
Cyc
Aquaint
corpus
TREC
corpus
EB
</figure>
<figureCaption confidence="0.999918">
Figure 1: PIQUANT’s Architecture
</figureCaption>
<sectionHeader confidence="0.930131" genericHeader="method">
2 A Multi-Agent QA Architecture
</sectionHeader>
<bodyText confidence="0.999918255319149">
In order to enable a multi-source and multi-strategy ap-
proach to question answering, we developed a modu-
lar and extensible QA architecture as shown in Figure 1
(Chu-Carroll et al., 2003a; Chu-Carroll et al., 2003b).
With a consistent interface defined for each component,
this architecture allows for easy plug-and-play of individ-
ual components for experimental purposes.
In our architecture, a question is first processed by the
question analysis component. The analysis results are
represented as a QFrame, which minimally includes a set
of question features that help activate one or more an-
swering agents. Each answering agent takes the QFrame
and generates its own set of requests to a variety of
knowledge sources. This may include performing search
against a text corpus and extracting answers from the re-
sulting passages, or performing a query against a struc-
tured knowledge source, such as WordNet (Miller, 1995)
or Cyc (Lenat, 1995). The (intermediate) results from
the individual answering agents are then passed on to the
answer resolution component, which combines and re-
solves the set of results, and either produces the system’s
final answers or feeds the intermediate results back to the
answering agents for further processing.
We have developed multiple answering agents, some
general purpose and others tailored for specific ques-
tion types. Figure 1 shows the answering agents cur-
rently available in PIQUANT. The knowledge-based and
statistical answering agents are general-purpose agents
that adopt different processing strategies and consult a
number of different text resources. The definition-Q
agent targets definition questions (e.g., “What is peni-
cillin?” and “Who is Picasso?”) with a technique called
Virtual Annotation using the external knowledge source
WordNet (Prager et al., 2001). The KSP-based answer-
ing agent focuses on a subset of factoid questions with
specific logical forms, such as capital(?COUNTRY) and
state tree(?STATE). The answering agent sends requests
to the KSP (Knowledge Sources Portal), which returns, if
possible, an answer from a structured knowledge source
(Chu-Carroll et al., 2003a).
In the rest of this paper, we briefly describe our two
general-purpose answering agents. We then focus on a
multi-level answer resolution algorithm, applicable at dif-
ferent points in the QA process of these two answering
agents. Finally, we discuss experiments conducted to dis-
cover effective methods for combining results from mul-
tiple answering agents.
</bodyText>
<sectionHeader confidence="0.948463" genericHeader="method">
3 Component Answering Agents
</sectionHeader>
<bodyText confidence="0.999704090909091">
We focus on two end-to-end answering agents designed
to answer short, fact-seeking questions from a collection
of text documents, as motivated by the requirements of
the TREC QA track (Voorhees, 2003). Both answer-
ing agents adopt the classic pipeline architecture, con-
sisting roughly of question analysis, passage retrieval,
and answer selection components. Although the answer-
ing agents adopt fundamentally different strategies in
their individual components, they have performed quite
comparably in past TREC QA tracks (Voorhees, 2001;
Voorhees, 2002).
</bodyText>
<subsectionHeader confidence="0.998285">
3.1 Knowledge-Based Answering Agent
</subsectionHeader>
<bodyText confidence="0.999978125">
Our first answering agent utilizes a primarily knowledge-
driven approach, based on Predictive Annotation (Prager
et al., 2000). A key characteristic of this approach is that
potential answers, such as person names, locations, and
dates, in the corpus are predictively annotated. In other
words, the corpus is indexed not only with keywords, as
is typical for most search engines, but also with the se-
mantic classes of these pre-identified potential answers.
During the question analysis phase, a rule-based mech-
anism is employed to select one or more expected an-
swer types, from a set of about 80 classes used in the
predictive annotation process, along with a set of ques-
tion keywords. A weighted search engine query is then
constructed from the keywords, their morphological vari-
ations, synonyms, and the answer type(s). The search en-
gine returns a hit list of typically 10 passages, each con-
sisting of 1-3 sentences. The candidate answers in these
passages are identified and ranked based on three criteria:
1) match in semantic type between candidate answer and
expected answer, 2) match in weighted grammatical rela-
tionships between question and answer passages, and 3)
frequency of answer in candidate passages (redundancy).
The answering agent returns the top n ranked candidate
answers along with a confidence score for each answer.
</bodyText>
<subsectionHeader confidence="0.999745">
3.2 Statistical Answering Agent
</subsectionHeader>
<bodyText confidence="0.972050205882353">
The second answering agent takes a statistical approach
to question answering (Ittycheriah, 2001; Ittycheriah et
al., 2001). It models the distribution p(c|q, a), which
measures the “correctness” (c) of an answer (a) to a ques-
tion (q), by introducing a hidden variable representing the
answer type (e) as follows:
p(c|q, a) = Ee p(c, e |q, a)
= Ee p(c|e, q, a)p(e|q, a)
p(e|q, a) is the answer type model which predicts, from
the question and a proposed answer, the answer type they
both satisfy. p(c|e, q, a) is the answer selection model.
Given a question, an answer, and the predicted answer
type, it seeks to model the correctness of this configura-
tion. These distributions are modeled using a maximum
entropy formulation (Berger et al., 1996), using training
data which consists of human judgments of question an-
swer pairs. For the answer type model, 13K questions
were annotated with 31 categories. For the answer selec-
tion model, 892 questions from the TREC 8 and TREC 9
QA tracks were used, along with 4K trivia questions.
During runtime, the question is first analyzed by the
answer type model, which selects one out of a set of 31
types for use by the answer selection model. Simultane-
ously, the question is expanded using local context anal-
ysis (Xu and Croft, 1996) with an encyclopedia, and the
top 1000 documents are retrieved by the search engine.
From these documents, the top 100 passages are chosen
that 1) maximize the question word match, 2) have the
desired answer type, 3) minimize the dispersion of ques-
tion words, and 4) have similar syntactic structures as the
question. From these passages, candidate answers are ex-
tracted and ranked using the answer selection model. The
top n candidate answers are then returned, each with an
associated confidence score.
</bodyText>
<sectionHeader confidence="0.98325" genericHeader="method">
4 Answer Resolution
</sectionHeader>
<bodyText confidence="0.997623833333333">
Given two answering agents with the same pipeline archi-
tecture, there are multiple points in the process at which
(intermediate) results can be combined, as illustrated in
Figure 2. More specifically, it is possible for one answer-
ing agent to provide input to the other after the question
analysis, passage retrieval, and answer selection phases.
In PIQUANT, the knowledge based agent may accept in-
put from the statistical agent after each of these three
phases.1 The contributions from the statistical agent are
taken into consideration by the knowledge based answer-
ing agent in a phase-dependent fashion. The rest of this
section details our combination strategies for each phase.
</bodyText>
<subsectionHeader confidence="0.981882">
4.1 Question-Level Combination
</subsectionHeader>
<bodyText confidence="0.969965606060606">
One of the key tasks of the question analysis component
is to determine the expected answer type, such as PERSON
for “Who discovered America?” and DATE for “When
did World War II end?” This information is taken into ac-
count by most existing QA systems when ranking candi-
date answers, and can also be used in the passage retrieval
process to increase the precision of candidate passages.
We seek to improve the knowledge-based agent’s
performance in passage retrieval and answer selection
through better answer type identification by consulting
the statistical agent’s expected answer type. This task,
however, is complicated by the fact that QA systems em-
ploy different sets of answer types, often with different
granularities and/or with overlapping types. For instance,
while one system may generate ROYALTY for the ques-
tion “Who was the King of France in 1702?”, another
system may produce PERSON as the most specific an-
swer type in its repertoire. This is quite a serious problem
for us as the knowledge based agent uses over 80 answer
types while the statistical agent adopts only 31 categories.
In order to distinguish actual answer type discrepan-
cies from those due to granularity differences, we first
manually created a mapping between the two sets of an-
swer types. This mapping specifies, for each answer type
used by the statistical agent, a set ofpossible correspond-
ing types used by the knowledge-based agent. For exam-
ple, the GEOLOGICALOBJ class is mapped to a set of finer
grained classes: RIVER, MOUNTAIN, LAKE, and OCEAN.
At processing time, the statistical agent’s answer type
is mapped to the knowledge-based agent’s classes (SA-
1Although it is possible for the statistical agent to receive
input from the knowledge based agent as well, we have not pur-
sued that option because of implementation issues.
</bodyText>
<figureCaption confidence="0.984903">
Figure 2: Answer Resolution Strategies
</figureCaption>
<figure confidence="0.996290388888889">
Agent 1 (Knowledge-Based)
Passage
Retrieval 1
Question
Analysis 1
question
type
Agent 2 (Statistical)
Passage
Retrieval 2
Question
Analysis 2
QFrame
Answer
Selection 1
passages answers
Answer
Selection 2
</figure>
<bodyText confidence="0.874879666666667">
types), which are then merged with the answer type(s) se-
lected by the knowledge-based agent itself (KBA-types)
as follows:
</bodyText>
<listItem confidence="0.9832325">
1. If the intersection of KBA-types and SA-types is
non-null, i.e., the two agents produced consistent an-
swer types, then the merged set is KBA-types.
2. Otherwise, the two sets of answer types are truly
in disagreement, and the merged set is the union of
KBA-types and SA-types.
</listItem>
<bodyText confidence="0.9880595">
The merged answer types are then used by the
knowledge-based agent in further processing.
</bodyText>
<subsectionHeader confidence="0.86997">
4.2 Passage-Level Combination
</subsectionHeader>
<bodyText confidence="0.999131375">
The passage retrieval component selects, from a large text
corpus, a small number of short passages from which an-
swers are identified. Oftentimes, multiple passages that
answer a question are retrieved. Some of these passages
may be better suited than others for the answer selection
algorithm employed downstream. For example, consider
“When was Benjamin Disraeli prime minister?”, whose
answer can be found in both passages below:
</bodyText>
<listItem confidence="0.987699375">
1. Benjamin Disraeli, who had become prime minister
in 1868, was born into Judaism but was baptized a
Christian at the age of 12.
2. France had a Jewish prime minister in 1936, Eng-
land in 1868, and Spain, of all countries, in 1835,
but none of them, Leon Blum, Benjamin Disraeli or
Juan Alvarez Mendizabel, were devoutly observant,
as Lieberman is.
</listItem>
<bodyText confidence="0.999809615384615">
Although the correct answer, 1868, is present in both
passages, it is substantially easier to identify the answer
from the first passage, where it is directly stated, than
from the second passage, where recognition of parallel
constructs is needed to identify the correct answer.
Because of strategic differences in question analysis
and passage retrieval, our two answering agents often re-
trieve different passages for the same question. Thus, we
perform passage-level combination to make a wider va-
riety of passages available to the answer selection com-
ponent, as shown in Figure 2. The potential advantages
are threefold. First, passages from agent 2 may contain
answers absent in passages retrieved by agent 1. Sec-
ond, agent 2 may have retrieved passages better suited for
the downstream answer selection algorithm than those re-
trieved by agent 1. Third, passages from agent 2 may con-
tain additional occurrences of the correct answer, which
boosts the system’s confidence in the answer through the
redundancy measure.2
Our passage-level combination algorithm adds to the
passages extracted by the knowledge-based agent the top-
ranked passages from the statistical agent that contain
candidate answers of the right type. More specifically,
the statistical agent’s passages are semantically annotated
and the top 10 passages containing at least one candidate
of the expected answer type(s) are selected.3
</bodyText>
<subsectionHeader confidence="0.989714">
4.3 Answer-Level Combination
</subsectionHeader>
<bodyText confidence="0.999943125">
The answer selection component identifies, from a set
of passages, the top n answers for the given question,
with their associated confidence scores. An answer-level
combination algorithm takes the top answer(s) from the
individual answering agents and determines the overall
best answer(s). Of our three combination algorithms, this
most closely resembles traditional ensemble methods, as
voting takes place among the end results of individual an-
</bodyText>
<footnote confidence="0.780776">
2On the other hand, such redundancy may result in error
compounding, as discussed in Section 5.3.
3We selected the top 10 passages so that the same number
of passages are considered from both answering agents.
</footnote>
<bodyText confidence="0.999805782608696">
swering agents to determine the final output of the ensem-
ble.
We developed two answer-level combination algo-
rithms, both utilizing a simple confidence-based voting
mechanism, based on the premise that answers selected
by both agents with high confidence are more likely to
be correct than those identified by only one agent.4 In
both algorithms, named entity normalization is first per-
formed on all candidate answers considered. In the first
algorithm, only the top answer from each agent is taken
into account. If the two top answers are equivalent, the
answer is selected with the combined confidence from
both agents; otherwise, the more confident answer is se-
lected.5 In the second algorithm, the top 5 answers from
each agent are allowed to participate in the voting pro-
cess. Each instance of an answer votes with a weight
equal to its confidence value and the weights of equiv-
alent answers are again summed. The answer with the
highest weight, or confidence value, is selected as the
system’s final answer. Since in our evaluation, the second
algorithm uniformly outperforms the first, it is adopted as
our answer-level combination algorithm in the rest of the
paper.
</bodyText>
<sectionHeader confidence="0.988704" genericHeader="method">
5 Performance Evaluation
</sectionHeader>
<subsectionHeader confidence="0.898541">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999989736842105">
To assess the effectiveness of our multi-level answer res-
olution algorithm, we devised experiments to evaluate the
impact of the question, passage, and answer-level combi-
nation algorithms described in the previous section.
The baseline systems are the knowledge-based and sta-
tistical agents performing individually against a single
reference corpus. In addition, our earlier experiments
showed that when employing a single answer finding
strategy, consulting multiple text corpora yielded better
performance than using a single corpus. We thus con-
figured a version of our knowledge-based agent to make
use of three available text corpora,6 the AQUAINT cor-
pus (news articles from 1998-2000), the TREC corpus
(news articles from 1988-1994),7 and a subset of the En-
cyclopedia Britannica. This multi-source version of the
knowledge-based agent will be used in all answer resolu-
tion experiments in conjunction with the statistical agent.
We configured multiple versions of PIQUANT to eval-
uate our question, passage, and answer-level combination
</bodyText>
<footnote confidence="0.999707375">
4In future work we will be investigating weighted voting
schemes based on question features.
5The confidence values from both answering agents are nor-
malized to be between 0 and 1.
6The statistical agent is currently unable to consult multiple
corpora.
7Both the AQUAINT and TREC corpora are available from
the Linguistics Data Consortium, http://www.ldc.org.
</footnote>
<bodyText confidence="0.999822692307692">
algorithms individually and cumulatively. For cumula-
tive effects, we 1) combined the algorithms pair-wise,
and 2) employed all three algorithms together. The two
test sets were selected from the TREC 10 and 11 QA
track questions (Voorhees, 2002; Voorhees, 2003). For
both test sets, we eliminated those questions that did not
have known answers in the reference corpus. Further-
more, from the TREC 10 test set, we discarded all defini-
tion questions,8 since the knowledge-based agent adopts
a specialized strategy for handling definition questions
which greatly reduces potential contributions from other
answering agents. This results in a TREC 10 test set of
313 questions and a TREC 11 test set of 453 questions.
</bodyText>
<subsectionHeader confidence="0.992726">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999895571428571">
We ran each of the baseline and combined systems on the
two test sets. For each run, the system outputs its top
answer and its confidence score for each question. All
answers for a run are then sorted in descending order of
the confidence scores. Two established TREC QA eval-
uation metrics are adopted to assess the results for each
run as follows:
</bodyText>
<listItem confidence="0.945297166666667">
1. % Correct: Percentage of correct answers.
2. Average Precision: A confidence-weighted score
that rewards systems with high confidence in cor-
rect answers as follows, where N is the number of
questions:
# correct up to question i/i
</listItem>
<bodyText confidence="0.999456105263158">
Table 1 shows our experimental results. The top sec-
tion shows the comparable baseline results from the sta-
tistical agent (SA-SS) and the single-source knowledge-
based agent (KBA-SS). It also includes results for the
multi-source knowledge-based agent (KBA-MS), which
improve upon those for its single-source counterpart
(KBA-SS).
The middle section of the table shows the answer
resolution results, including applying the question, pas-
sage, and answer-level combination algorithms individu-
ally (Q, P, and A, respectively), applying them pair-wise
(Q+P, P+A, and Q+A), and employing all three algo-
rithms (Q+P+A). Finally, the last row of the table shows
the relative improvement by comparing the best perform-
ing system configuration (highlighted in boldface) with
the better performing single-source, single-strategy base-
line system (SA-SS or KBA-SS, in italics).
Overall, PIQUANT’s multi-strategy and multi-source
approach achieved a 35.0% relative improvement in the
</bodyText>
<footnote confidence="0.8514935">
8Definition questions were intentionally excluded by the
track coordinator in the TREC 11 test set.
</footnote>
<equation confidence="0.777842333333333">
1 N
N
i=1
</equation>
<table confidence="0.997339461538462">
TREC 10 (313) TREC 11 (453)
% Corr Avg Prec % Corr Avg Prec
SA-SS 36.7% 0.569 32.9% 0.534
KBA-SS 39.6% 0.595 32.5% 0.531
KBA-MS 43.8% 0.641 38.2% 0.622
Q 44.7% 0.647 38.9% 0.632
P 49.5% 0.661 40.0% 0.627
A 49.5% 0.712 43.5% 0.704
Q+P 48.9% 0.656 41.1% 0.640
P+A 51.1% 0.711 44.2% 0.686
Q+A 49.8% 0.716 43.9% 0.709
Q+P+A 50.8% 0.706 44.4% 0.690
rel. improv. 29.0% 20.3% 35.0% 32.8%
</table>
<tableCaption confidence="0.999868">
Table 1: Experimental Results
</tableCaption>
<bodyText confidence="0.999974">
number of correct answers and a 32.8% improvement in
average precision on the TREC 11 data set. Of the com-
bined improvement, approximately half was achieved by
the multi-source aspect of PIQUANT, while the other half
was obtained by PIQUANT’s multi-strategy feature. Al-
though the absolute average precision values are com-
parable on both test sets and the absolute percentage of
correct answers is lower on the TREC 11 data, the im-
provement is greater on TREC 11 in both cases. This
is because the TREC 10 questions were taken into ac-
count for manual rule refinement in the knowledge-based
agent, resulting in higher baselines on the TREC 10 test
set. We believe that the larger improvement on the previ-
ously unseen TREC 11 data is a more reliable estimate of
PIQUANT’s performance on future test sets.
We applied an earlier version of our combination algo-
rithms, which performed between our current P and P+A
algorithms, in our submission to the TREC 11 QA track.
Using the average precision metric, that version of PI-
QUANT was among the top 5 best performing systems
out of 67 runs submitted by 34 groups.
</bodyText>
<subsectionHeader confidence="0.996836">
5.3 Discussion and Analysis
</subsectionHeader>
<bodyText confidence="0.993020125">
A cursory examination of the results in Table 1 allows
us to draw two general conclusions about PIQUANT’s
performance. First, all three combination algorithms ap-
plied individually improved upon the baseline using both
evaluation metrics on both test sets. In addition, overall
performance is generally better the later in the process
the combination occurs, i.e., the answer-level combina-
tion algorithm outperformed the passage-level combina-
tion algorithm, which in turn outperformed the question-
level combination algorithm. Second, the cumulative im-
provement from multiple combination algorithms is in
general greater than that from the components. For in-
stance, the Q+A algorithm uniformly outperformed the Q
and A algorithms alone. Note, however, that the Q+P+A
algorithm achieved the highest performance only on the
TREC 11 test set using the % correct metric. We believe
</bodyText>
<table confidence="0.9962">
KBA
TREC 10 (313) TREC 11 (453)
+ - + -
SA + 185 43 254 58
- 24 61 41 100
</table>
<tableCaption confidence="0.998603">
Table 2: Passage Retrieval Analysis
</tableCaption>
<bodyText confidence="0.999933043478261">
that this is because of compounding errors that occurred
during the multiple combination process.
In ensemble methods, the individual components must
make different mistakes in order for the combined sys-
tem to potentially perform better than the component sys-
tems (Dietterich, 1997). We examined the differences
in results between the two answering agents from their
question analysis, passage retrieval, and answer selection
components. We focused our analysis on the potential
gain/loss from incorporating contributions from the sta-
tistical agent, and how the potential was realized as actual
performance gain/loss in our end-to-end system.
At the question level, we examined those questions
for which the two agents proposed incompatible answer
types. On the TREC 10 test set, the statistical agent in-
troduced correct answer types in 6 cases and incorrect
answer types in 9 cases. As a result, in some cases the
question-level combination algorithm improved system
performance (comparing A and Q+A) and in others it
degraded performance (comparing P and Q+P). On the
other hand, on the TREC 11 test set, the statistical agent
introduced correct and incorrect answer types in 15 and
6 cases, respectively. As a result, in most cases perfor-
mance improved when the question-level combination al-
gorithm was invoked. The difference in question analysis
performance again reflects the fact that TREC 10 ques-
tions were used in question analysis rule refinement in
the knowledge-based agent.
At the passage level, we examined, for each ques-
tion, whether the candidate passages contained the cor-
rect answer. Table 2 shows the distribution of ques-
tions for which correct answers were (+) and were not
(-) present in the passages for both agents. The bold-
faced cells represent questions for which the statistical
agent retrieved passages with correct answers while the
knowledge-based agent did not. There were 43 and 58
such questions in the TREC 10 and TREC 11 test sets, re-
spectively, and employing the passage-level combination
algorithm resulted only in an additional 18 and 8 correct
answers on each test set. This is because the statistical
agent’s proposes in its 10 passages, on average, 29 candi-
date answers, most of which are incorrect, of the proper
semantic type per question. As the downstream answer
selection component takes redundancy into account in an-
swer ranking, incorrect answers may reinforce one an-
other and become top ranked answers. This suggests that
</bodyText>
<table confidence="0.996722333333333">
KBA
TREC 10 (313) TREC 11 (453)
1st 2-5th none 1st 2-5th none
SA 1st 66 22 26 93 21 35
2-5th 26 9 13 29 19 22
none 45 14 92 51 21 162
</table>
<tableCaption confidence="0.999392">
Table 3: Answer Voting Analysis
</tableCaption>
<bodyText confidence="0.999646888888889">
the relative contributions of our answer selection features
may not be optimally tuned for our multi-agent approach
to QA. We plan to investigate this issue in future work.
At the answer level, we analyzed each agent’s top 5
answers, used in the combination algorithm’s voting pro-
cess. Table 3 shows the distribution of questions for
which an answer was found in 1st place, in 2nd-5th place,
and not found in top 5. Since we employ a linear vot-
ing strategy based on confidence scores, we classify the
cells in Table 3 as follows based on the perceived likeli-
hood that the correct answers for questions in each cell
wins in the voting process. The boldfaced and underlined
cells contain highly likely candidates, since a correct an-
swer was found in 1st place by both agents.9 The bold-
faced cells consist of likely candidates, since a 1st place
correct answer was supported by a 2nd-5th place answer.
The italicized and underlined cells contain possible can-
didates, while the rest of the cells cannot produce correct
1st place answers using our current voting algorithm. On
TREC 10 data, 194 questions fall into the highly likely,
likely, and possible categories, out of which the voting al-
gorithm successfully selected 155 correct answers in 1st
place. On TREC 11 data, 197 correct answers were se-
lected out of 248 questions that fall into these categories.
These results represent success rates of 79.9% and 79.4%
for our answer-level combination algorithm on the two
test sets.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.990302657142857">
There has been much work in employing ensemble meth-
ods to increase system performance in machine learning.
In NLP, such methods have been applied to tasks such
as POS tagging (Brill and Wu, 1998), word sense dis-
ambiguation (Pedersen, 2000), parsing (Henderson and
Brill, 1999), and machine translation (Frederking and
Nirenburg, 1994).
In question answering, a number of researchers have
investigated federated systems for identifying answers to
questions. For example, (Clarke et al., 2003) and (Lin et
al., 2003) employ techniques for utilizing both unstruc-
9These cells are not marked as definite because in a small
number of cases, the two answers are not equivalent. For exam-
ple, for the TREC 9 question, “Who is the emperor ofJapan?”,
Hirohito, Akihito, and Taisho are all considered correct answers
based on the reference corpus.
tured text and structured databases for question answer-
ing. However, the approaches taken by both these sys-
tems differ from ours in that they enforce an order be-
tween the two strategies by attempting to locate answers
in structured databases first for select question types and
falling back to unstructured text when the former fails,
while we explore both options in parallel and combine
the results from multiple answering agents.
The multi-agent approach to question answering most
similar to ours is that by Burger et al. (2003). They
applied ensemble methods to combine the 67 runs sub-
mitted to the TREC 11 QA track, using an unweighted
centroid method for selecting among the 67 proposed an-
swers for each question. However, their combined sys-
tem did not outperform the top scoring system(s). Fur-
thermore, their approach differs from ours in that they fo-
cused on combining the end results of a large number of
systems, while we investigated a tightly-coupled design
for combining two answering agents.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999944052631579">
In this paper, we introduced a multi-strategy and multi-
source approach to question answering that enables com-
bination of answering agents adopting different strategies
and consulting multiple knowledge sources. In partic-
ular, we focused on two answering agents, one adopt-
ing a knowledge-based approach and one using statistical
methods. We discussed our answer resolution component
which employs a multi-level combination algorithm that
allows for resolution at the question, passage, and answer
levels. Best performance using the % correct metric was
achieved by the three-level algorithm that combines af-
ter each stage, while highest average precision was ob-
tained by a two-level algorithm merging at the question
and answer levels, supporting a tightly-coupled design
for multi-agent question answering. Our experiments
showed that our best performing algorithms achieved a
35.0% relative improvement in the number of correct an-
swers and a 32.8% improvement in average precision on
a previously unseen test set.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999439125">
We would like to thank Dave Ferrucci, Chris Welty, and
Salim Roukos for helpful discussions, Diane Litman and
the anonymous reviewers for their comments on an ear-
lier draft of this paper. This work was supported in
part by the Advanced Research and Development Ac-
tivity (ARDA)’s Advanced Question Answering for In-
telligence (AQUAINT) Program under contract number
MDA904-01-C-0988.
</bodyText>
<sectionHeader confidence="0.996316" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998810344086021">
Adam L. Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–71.
Eric Brill and Jun Wu. 1998. Classifier combination for
improved lexical disambiguation. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics, pages 191–195.
John D. Burger, Lisa Ferro, Warren Greiff, John Hender-
son, Marc Light, and Scott Mardis. 2003. MITRE’s
Qanda at TREC-11. In Proceedings of the Eleventh
Text Retrieval Conference. To appear.
Jennifer Chu-Carroll, David Ferrucci, John Prager, and
Christopher Welty. 2003a. Hybridization in ques-
tion answering systems. In Working Notes of the AAAI
Spring Symposium on New Directions in Question An-
swering, pages 116–121.
Jennifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003b. A
multi-strategy and multi-source approach to question
answering. In Proceedings of the Eleventh Text Re-
trieval Conference. To appear.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358–365.
C.L.A. Clarke, G.V. Cormack, G. Kemkes, M. Laszlo,
T.R. Lynam, E.L. Terra, and P.L. Tilker. 2003. Statis-
tical selection of exact answers. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Thomas G. Dietterich. 1997. Machine learning research:
Four current directions. AI Magazine, 18(4):97–136.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Fourth
Conference on Applied Natural Language Processing.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the 4th Conference on Em-
pirical Methods in Natural Language Processing.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2001. Question answering
in Webclopedia. In Proceedings of the Ninth Text RE-
trieval Conference, pages 655–664.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. Question answering using
maximum entropy components. In Proceedings of the
2nd Conference of the North American Chapter of the
Association for Computational Linguistics, pages 33–
39.
Abraham Ittycheriah. 2001. Trainable Question Answer-
ing Systems. Ph.D. thesis, Rutgers - The State Univer-
sity of New Jersey.
Douglas B. Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11).
Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory Mar-
ton, and Stefanie Tellex. 2003. Extracting an-
swers from the web using knowledge annotation and
knowledge mining techniques. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
George Miller. 1995. Wordnet: A lexical database for
English. Communications of the ACM, 38(11).
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 563–570.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive Bayesian classifiers for word sense
disambiguation. In Proceedings of the 1st Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 63–69.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive anno-
tation. In Proceedings of the 23rd SIGIR Conference,
pages 184–191.
John Prager, Dragomir Radev, and Krzysztof Czuba.
2001. Answering what-is questions by virtual anno-
tation. In Proceedings of Human Language Technolo-
gies Conference, pages 26–30.
Ellen M. Voorhees. 2001. Overview of the TREC-9
question answering track. In Proceedings of the 9th
Text Retrieval Conference, pages 71–80.
Ellen M. Voorhees. 2002. Overview of the TREC 2001
question answering track. In Proceedings of the 10th
Text Retrieval Conference, pages 42–51.
Ellen M. Voorhees. 2003. Overview of the TREC
2002 question answering track. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of the 19th SIGIR Conference, pages 4–11.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.223704">
<note confidence="0.922023333333333">Proceedings of HLT-NAACL 2003 Main Papers , pp. 24-31 Edmonton, May-June 2003</note>
<title confidence="0.847376">In Question Answering, Two Heads Are Better Than One</title>
<author confidence="0.99565">Jennifer Chu-Carroll Krzysztof Czuba John Prager Abraham Ittycheriah</author>
<affiliation confidence="0.726442">IBM T.J. Watson Research</affiliation>
<author confidence="0.671191">P O Box</author>
<affiliation confidence="0.646061">Yorktown Heights, NY 10598,</affiliation>
<email confidence="0.999559">jencc,kczuba,jprager,abei@us.ibm.com</email>
<abstract confidence="0.9988787">Motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multistrategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora. The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques. We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels. Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent Della Pietra</author>
<author>Stephen Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="9687" citStr="Berger et al., 1996" startWordPosition="1456" endWordPosition="1459"> models the distribution p(c|q, a), which measures the “correctness” (c) of an answer (a) to a question (q), by introducing a hidden variable representing the answer type (e) as follows: p(c|q, a) = Ee p(c, e |q, a) = Ee p(c|e, q, a)p(e|q, a) p(e|q, a) is the answer type model which predicts, from the question and a proposed answer, the answer type they both satisfy. p(c|e, q, a) is the answer selection model. Given a question, an answer, and the predicted answer type, it seeks to model the correctness of this configuration. These distributions are modeled using a maximum entropy formulation (Berger et al., 1996), using training data which consists of human judgments of question answer pairs. For the answer type model, 13K questions were annotated with 31 categories. For the answer selection model, 892 questions from the TREC 8 and TREC 9 QA tracks were used, along with 4K trivia questions. During runtime, the question is first analyzed by the answer type model, which selects one out of a set of 31 types for use by the answer selection model. Simultaneously, the question is expanded using local context analysis (Xu and Croft, 1996) with an encyclopedia, and the top 1000 documents are retrieved by the </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent Della Pietra, and Stephen Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Jun Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>191--195</pages>
<contexts>
<context position="28794" citStr="Brill and Wu, 1998" startWordPosition="4532" endWordPosition="4535">n TREC 10 data, 194 questions fall into the highly likely, likely, and possible categories, out of which the voting algorithm successfully selected 155 correct answers in 1st place. On TREC 11 data, 197 correct answers were selected out of 248 questions that fall into these categories. These results represent success rates of 79.9% and 79.4% for our answer-level combination algorithm on the two test sets. 6 Related Work There has been much work in employing ensemble methods to increase system performance in machine learning. In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). In question answering, a number of researchers have investigated federated systems for identifying answers to questions. For example, (Clarke et al., 2003) and (Lin et al., 2003) employ techniques for utilizing both unstruc9These cells are not marked as definite because in a small number of cases, the two answers are not equivalent. For example, for the TREC 9 question, “Who is the emperor ofJapan?”, Hirohito, Akihito, and Taisho are all considered correc</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>Eric Brill and Jun Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 191–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>Lisa Ferro</author>
<author>Warren Greiff</author>
<author>John Henderson</author>
<author>Marc Light</author>
<author>Scott Mardis</author>
</authors>
<title>MITRE’s Qanda at TREC-11.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eleventh Text Retrieval Conference.</booktitle>
<note>To appear.</note>
<contexts>
<context position="2669" citStr="Burger et al., 2003" startWordPosition="391" endWordPosition="394"> is achieved by applying Virtual Annotation and our general purpose QA strategy in parallel. In this paper, we investigate the impact of adopting such a multistrategy and multi-source approach to QA in a more general fashion. Our approach to question answering is additionally motivated by the success of ensemble methods in machine learning, where multiple classifiers are employed and their results are combined to produce the final output of the ensemble (for an overview, see (Dietterich, 1997)). Such ensemble methods have recently been adopted in question answering (Chu-Carroll et al., 2003b; Burger et al., 2003). In our question answering system, PIQUANT, we utilize in parallel multiple answering agents that adopt different processing strategies and consult different knowledge sources in identifying answers to given questions, and we employ resolution mechanisms to combine the results produced by the individual answering agents. We call our approach multi-strategy since we combine the results from a number of independent agents implementing different answer finding strategies. We also call it multi-source since the different agents can search for answers in multiple knowledge sources. In this paper, </context>
<context position="29967" citStr="Burger et al. (2003)" startWordPosition="4717" endWordPosition="4720">, Akihito, and Taisho are all considered correct answers based on the reference corpus. tured text and structured databases for question answering. However, the approaches taken by both these systems differ from ours in that they enforce an order between the two strategies by attempting to locate answers in structured databases first for select question types and falling back to unstructured text when the former fails, while we explore both options in parallel and combine the results from multiple answering agents. The multi-agent approach to question answering most similar to ours is that by Burger et al. (2003). They applied ensemble methods to combine the 67 runs submitted to the TREC 11 QA track, using an unweighted centroid method for selecting among the 67 proposed answers for each question. However, their combined system did not outperform the top scoring system(s). Furthermore, their approach differs from ours in that they focused on combining the end results of a large number of systems, while we investigated a tightly-coupled design for combining two answering agents. 7 Conclusions In this paper, we introduced a multi-strategy and multisource approach to question answering that enables combi</context>
</contexts>
<marker>Burger, Ferro, Greiff, Henderson, Light, Mardis, 2003</marker>
<rawString>John D. Burger, Lisa Ferro, Warren Greiff, John Henderson, Marc Light, and Scott Mardis. 2003. MITRE’s Qanda at TREC-11. In Proceedings of the Eleventh Text Retrieval Conference. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>David Ferrucci</author>
<author>John Prager</author>
<author>Christopher Welty</author>
</authors>
<title>Hybridization in question answering systems.</title>
<date>2003</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on New Directions in Question Answering,</booktitle>
<pages>116--121</pages>
<contexts>
<context position="2646" citStr="Chu-Carroll et al., 2003" startWordPosition="387" endWordPosition="390">own that better performance is achieved by applying Virtual Annotation and our general purpose QA strategy in parallel. In this paper, we investigate the impact of adopting such a multistrategy and multi-source approach to QA in a more general fashion. Our approach to question answering is additionally motivated by the success of ensemble methods in machine learning, where multiple classifiers are employed and their results are combined to produce the final output of the ensemble (for an overview, see (Dietterich, 1997)). Such ensemble methods have recently been adopted in question answering (Chu-Carroll et al., 2003b; Burger et al., 2003). In our question answering system, PIQUANT, we utilize in parallel multiple answering agents that adopt different processing strategies and consult different knowledge sources in identifying answers to given questions, and we employ resolution mechanisms to combine the results produced by the individual answering agents. We call our approach multi-strategy since we combine the results from a number of independent agents implementing different answer finding strategies. We also call it multi-source since the different agents can search for answers in multiple knowledge s</context>
<context position="4637" citStr="Chu-Carroll et al., 2003" startWordPosition="677" endWordPosition="680">s and a 32.8% improvement in average precision, on a previously unseen test set. Question Answer Question Analysis Answer Resolution QFrame Answering Agents Knowledge-Based Answering Agent Statistical Answering Agent Definition Q Answering Agent KSP-Based Answering Agent QGoals Knowledge Sources WordNet Keyword Search Semantic Search KSP Cyc Aquaint corpus TREC corpus EB Figure 1: PIQUANT’s Architecture 2 A Multi-Agent QA Architecture In order to enable a multi-source and multi-strategy approach to question answering, we developed a modular and extensible QA architecture as shown in Figure 1 (Chu-Carroll et al., 2003a; Chu-Carroll et al., 2003b). With a consistent interface defined for each component, this architecture allows for easy plug-and-play of individual components for experimental purposes. In our architecture, a question is first processed by the question analysis component. The analysis results are represented as a QFrame, which minimally includes a set of question features that help activate one or more answering agents. Each answering agent takes the QFrame and generates its own set of requests to a variety of knowledge sources. This may include performing search against a text corpus and ext</context>
<context position="6589" citStr="Chu-Carroll et al., 2003" startWordPosition="970" endWordPosition="973">adopt different processing strategies and consult a number of different text resources. The definition-Q agent targets definition questions (e.g., “What is penicillin?” and “Who is Picasso?”) with a technique called Virtual Annotation using the external knowledge source WordNet (Prager et al., 2001). The KSP-based answering agent focuses on a subset of factoid questions with specific logical forms, such as capital(?COUNTRY) and state tree(?STATE). The answering agent sends requests to the KSP (Knowledge Sources Portal), which returns, if possible, an answer from a structured knowledge source (Chu-Carroll et al., 2003a). In the rest of this paper, we briefly describe our two general-purpose answering agents. We then focus on a multi-level answer resolution algorithm, applicable at different points in the QA process of these two answering agents. Finally, we discuss experiments conducted to discover effective methods for combining results from multiple answering agents. 3 Component Answering Agents We focus on two end-to-end answering agents designed to answer short, fact-seeking questions from a collection of text documents, as motivated by the requirements of the TREC QA track (Voorhees, 2003). Both answe</context>
</contexts>
<marker>Chu-Carroll, Ferrucci, Prager, Welty, 2003</marker>
<rawString>Jennifer Chu-Carroll, David Ferrucci, John Prager, and Christopher Welty. 2003a. Hybridization in question answering systems. In Working Notes of the AAAI Spring Symposium on New Directions in Question Answering, pages 116–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>John Prager</author>
<author>Christopher Welty</author>
<author>Krzysztof Czuba</author>
<author>David Ferrucci</author>
</authors>
<title>A multi-strategy and multi-source approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eleventh Text Retrieval Conference.</booktitle>
<note>To appear.</note>
<contexts>
<context position="2646" citStr="Chu-Carroll et al., 2003" startWordPosition="387" endWordPosition="390">own that better performance is achieved by applying Virtual Annotation and our general purpose QA strategy in parallel. In this paper, we investigate the impact of adopting such a multistrategy and multi-source approach to QA in a more general fashion. Our approach to question answering is additionally motivated by the success of ensemble methods in machine learning, where multiple classifiers are employed and their results are combined to produce the final output of the ensemble (for an overview, see (Dietterich, 1997)). Such ensemble methods have recently been adopted in question answering (Chu-Carroll et al., 2003b; Burger et al., 2003). In our question answering system, PIQUANT, we utilize in parallel multiple answering agents that adopt different processing strategies and consult different knowledge sources in identifying answers to given questions, and we employ resolution mechanisms to combine the results produced by the individual answering agents. We call our approach multi-strategy since we combine the results from a number of independent agents implementing different answer finding strategies. We also call it multi-source since the different agents can search for answers in multiple knowledge s</context>
<context position="4637" citStr="Chu-Carroll et al., 2003" startWordPosition="677" endWordPosition="680">s and a 32.8% improvement in average precision, on a previously unseen test set. Question Answer Question Analysis Answer Resolution QFrame Answering Agents Knowledge-Based Answering Agent Statistical Answering Agent Definition Q Answering Agent KSP-Based Answering Agent QGoals Knowledge Sources WordNet Keyword Search Semantic Search KSP Cyc Aquaint corpus TREC corpus EB Figure 1: PIQUANT’s Architecture 2 A Multi-Agent QA Architecture In order to enable a multi-source and multi-strategy approach to question answering, we developed a modular and extensible QA architecture as shown in Figure 1 (Chu-Carroll et al., 2003a; Chu-Carroll et al., 2003b). With a consistent interface defined for each component, this architecture allows for easy plug-and-play of individual components for experimental purposes. In our architecture, a question is first processed by the question analysis component. The analysis results are represented as a QFrame, which minimally includes a set of question features that help activate one or more answering agents. Each answering agent takes the QFrame and generates its own set of requests to a variety of knowledge sources. This may include performing search against a text corpus and ext</context>
<context position="6589" citStr="Chu-Carroll et al., 2003" startWordPosition="970" endWordPosition="973">adopt different processing strategies and consult a number of different text resources. The definition-Q agent targets definition questions (e.g., “What is penicillin?” and “Who is Picasso?”) with a technique called Virtual Annotation using the external knowledge source WordNet (Prager et al., 2001). The KSP-based answering agent focuses on a subset of factoid questions with specific logical forms, such as capital(?COUNTRY) and state tree(?STATE). The answering agent sends requests to the KSP (Knowledge Sources Portal), which returns, if possible, an answer from a structured knowledge source (Chu-Carroll et al., 2003a). In the rest of this paper, we briefly describe our two general-purpose answering agents. We then focus on a multi-level answer resolution algorithm, applicable at different points in the QA process of these two answering agents. Finally, we discuss experiments conducted to discover effective methods for combining results from multiple answering agents. 3 Component Answering Agents We focus on two end-to-end answering agents designed to answer short, fact-seeking questions from a collection of text documents, as motivated by the requirements of the TREC QA track (Voorhees, 2003). Both answe</context>
</contexts>
<marker>Chu-Carroll, Prager, Welty, Czuba, Ferrucci, 2003</marker>
<rawString>Jennifer Chu-Carroll, John Prager, Christopher Welty, Krzysztof Czuba, and David Ferrucci. 2003b. A multi-strategy and multi-source approach to question answering. In Proceedings of the Eleventh Text Retrieval Conference. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Clarke</author>
<author>Gordon Cormack</author>
<author>Thomas Lynam</author>
</authors>
<title>Exploiting redundancy in question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th SIGIR Conference,</booktitle>
<pages>358--365</pages>
<contexts>
<context position="1474" citStr="Clarke et al., 2001" startWordPosition="206" endWordPosition="209">mbines results from the answering agents at the question, passage, and/or answer levels. Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric. 1 Introduction Traditional question answering (QA) systems typically employ a pipeline approach, consisting roughly of question analysis, document/passage retrieval, and answer selection (see e.g., (Prager et al., 2000; Moldovan et al., 2000; Hovy et al., 2001; Clarke et al., 2001)). Although a typical QA system classifies questions based on expected answer types, it adopts the same strategy for locating potential answers from the same corpus regardless of the question classification. In our own earlier work, we developed a specialized mechanism called Virtual Annotation for handling definition questions (e.g., “Who was Galileo?” and “What are antibiotics?”) that consults, in addition to the standard reference corpus, a structured knowledge source (WordNet) for answering such questions (Prager et al., 2001). We have shown that better performance is achieved by applying </context>
</contexts>
<marker>Clarke, Cormack, Lynam, 2001</marker>
<rawString>Charles Clarke, Gordon Cormack, and Thomas Lynam. 2001. Exploiting redundancy in question answering. In Proceedings of the 24th SIGIR Conference, pages 358–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L A Clarke</author>
<author>G V Cormack</author>
<author>G Kemkes</author>
<author>M Laszlo</author>
<author>T R Lynam</author>
<author>E L Terra</author>
<author>P L Tilker</author>
</authors>
<title>Statistical selection of exact answers.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eleventh Text Retrieval Conference.</booktitle>
<note>To appear.</note>
<contexts>
<context position="29090" citStr="Clarke et al., 2003" startWordPosition="4572" endWordPosition="4575">sults represent success rates of 79.9% and 79.4% for our answer-level combination algorithm on the two test sets. 6 Related Work There has been much work in employing ensemble methods to increase system performance in machine learning. In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). In question answering, a number of researchers have investigated federated systems for identifying answers to questions. For example, (Clarke et al., 2003) and (Lin et al., 2003) employ techniques for utilizing both unstruc9These cells are not marked as definite because in a small number of cases, the two answers are not equivalent. For example, for the TREC 9 question, “Who is the emperor ofJapan?”, Hirohito, Akihito, and Taisho are all considered correct answers based on the reference corpus. tured text and structured databases for question answering. However, the approaches taken by both these systems differ from ours in that they enforce an order between the two strategies by attempting to locate answers in structured databases first for sel</context>
</contexts>
<marker>Clarke, Cormack, Kemkes, Laszlo, Lynam, Terra, Tilker, 2003</marker>
<rawString>C.L.A. Clarke, G.V. Cormack, G. Kemkes, M. Laszlo, T.R. Lynam, E.L. Terra, and P.L. Tilker. 2003. Statistical selection of exact answers. In Proceedings of the Eleventh Text Retrieval Conference. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Machine learning research: Four current directions.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="2547" citStr="Dietterich, 1997" startWordPosition="375" endWordPosition="376">ed knowledge source (WordNet) for answering such questions (Prager et al., 2001). We have shown that better performance is achieved by applying Virtual Annotation and our general purpose QA strategy in parallel. In this paper, we investigate the impact of adopting such a multistrategy and multi-source approach to QA in a more general fashion. Our approach to question answering is additionally motivated by the success of ensemble methods in machine learning, where multiple classifiers are employed and their results are combined to produce the final output of the ensemble (for an overview, see (Dietterich, 1997)). Such ensemble methods have recently been adopted in question answering (Chu-Carroll et al., 2003b; Burger et al., 2003). In our question answering system, PIQUANT, we utilize in parallel multiple answering agents that adopt different processing strategies and consult different knowledge sources in identifying answers to given questions, and we employ resolution mechanisms to combine the results produced by the individual answering agents. We call our approach multi-strategy since we combine the results from a number of independent agents implementing different answer finding strategies. We </context>
<context position="24751" citStr="Dietterich, 1997" startWordPosition="3858" endWordPosition="3859">or instance, the Q+A algorithm uniformly outperformed the Q and A algorithms alone. Note, however, that the Q+P+A algorithm achieved the highest performance only on the TREC 11 test set using the % correct metric. We believe KBA TREC 10 (313) TREC 11 (453) + - + - SA + 185 43 254 58 - 24 61 41 100 Table 2: Passage Retrieval Analysis that this is because of compounding errors that occurred during the multiple combination process. In ensemble methods, the individual components must make different mistakes in order for the combined system to potentially perform better than the component systems (Dietterich, 1997). We examined the differences in results between the two answering agents from their question analysis, passage retrieval, and answer selection components. We focused our analysis on the potential gain/loss from incorporating contributions from the statistical agent, and how the potential was realized as actual performance gain/loss in our end-to-end system. At the question level, we examined those questions for which the two agents proposed incompatible answer types. On the TREC 10 test set, the statistical agent introduced correct answer types in 6 cases and incorrect answer types in 9 cases</context>
</contexts>
<marker>Dietterich, 1997</marker>
<rawString>Thomas G. Dietterich. 1997. Machine learning research: Four current directions. AI Magazine, 18(4):97–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fourth Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="28933" citStr="Frederking and Nirenburg, 1994" startWordPosition="4550" endWordPosition="4553">uccessfully selected 155 correct answers in 1st place. On TREC 11 data, 197 correct answers were selected out of 248 questions that fall into these categories. These results represent success rates of 79.9% and 79.4% for our answer-level combination algorithm on the two test sets. 6 Related Work There has been much work in employing ensemble methods to increase system performance in machine learning. In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). In question answering, a number of researchers have investigated federated systems for identifying answers to questions. For example, (Clarke et al., 2003) and (Lin et al., 2003) employ techniques for utilizing both unstruc9These cells are not marked as definite because in a small number of cases, the two answers are not equivalent. For example, for the TREC 9 question, “Who is the emperor ofJapan?”, Hirohito, Akihito, and Taisho are all considered correct answers based on the reference corpus. tured text and structured databases for question answering. However, the approaches taken by both </context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In Proceedings of the Fourth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Henderson</author>
<author>Eric Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4th Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="28875" citStr="Henderson and Brill, 1999" startWordPosition="4543" endWordPosition="4546">sible categories, out of which the voting algorithm successfully selected 155 correct answers in 1st place. On TREC 11 data, 197 correct answers were selected out of 248 questions that fall into these categories. These results represent success rates of 79.9% and 79.4% for our answer-level combination algorithm on the two test sets. 6 Related Work There has been much work in employing ensemble methods to increase system performance in machine learning. In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). In question answering, a number of researchers have investigated federated systems for identifying answers to questions. For example, (Clarke et al., 2003) and (Lin et al., 2003) employ techniques for utilizing both unstruc9These cells are not marked as definite because in a small number of cases, the two answers are not equivalent. For example, for the TREC 9 question, “Who is the emperor ofJapan?”, Hirohito, Akihito, and Taisho are all considered correct answers based on the reference corpus. tured text and structured databases for </context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>John C. Henderson and Eric Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings of the 4th Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Laurie Gerber</author>
<author>Ulf Hermjakob</author>
<author>Michael Junk</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Question answering in Webclopedia.</title>
<date>2001</date>
<booktitle>In Proceedings of the Ninth Text REtrieval Conference,</booktitle>
<pages>655--664</pages>
<contexts>
<context position="1452" citStr="Hovy et al., 2001" startWordPosition="202" endWordPosition="205">n algorithm that combines results from the answering agents at the question, passage, and/or answer levels. Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric. 1 Introduction Traditional question answering (QA) systems typically employ a pipeline approach, consisting roughly of question analysis, document/passage retrieval, and answer selection (see e.g., (Prager et al., 2000; Moldovan et al., 2000; Hovy et al., 2001; Clarke et al., 2001)). Although a typical QA system classifies questions based on expected answer types, it adopts the same strategy for locating potential answers from the same corpus regardless of the question classification. In our own earlier work, we developed a specialized mechanism called Virtual Annotation for handling definition questions (e.g., “Who was Galileo?” and “What are antibiotics?”) that consults, in addition to the standard reference corpus, a structured knowledge source (WordNet) for answering such questions (Prager et al., 2001). We have shown that better performance is</context>
</contexts>
<marker>Hovy, Gerber, Hermjakob, Junk, Lin, 2001</marker>
<rawString>Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael Junk, and Chin-Yew Lin. 2001. Question answering in Webclopedia. In Proceedings of the Ninth Text REtrieval Conference, pages 655–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Martin Franz</author>
<author>Wei-Jing Zhu</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Question answering using maximum entropy components.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--39</pages>
<contexts>
<context position="9063" citStr="Ittycheriah et al., 2001" startWordPosition="1347" endWordPosition="1350">y 10 passages, each consisting of 1-3 sentences. The candidate answers in these passages are identified and ranked based on three criteria: 1) match in semantic type between candidate answer and expected answer, 2) match in weighted grammatical relationships between question and answer passages, and 3) frequency of answer in candidate passages (redundancy). The answering agent returns the top n ranked candidate answers along with a confidence score for each answer. 3.2 Statistical Answering Agent The second answering agent takes a statistical approach to question answering (Ittycheriah, 2001; Ittycheriah et al., 2001). It models the distribution p(c|q, a), which measures the “correctness” (c) of an answer (a) to a question (q), by introducing a hidden variable representing the answer type (e) as follows: p(c|q, a) = Ee p(c, e |q, a) = Ee p(c|e, q, a)p(e|q, a) p(e|q, a) is the answer type model which predicts, from the question and a proposed answer, the answer type they both satisfy. p(c|e, q, a) is the answer selection model. Given a question, an answer, and the predicted answer type, it seeks to model the correctness of this configuration. These distributions are modeled using a maximum entropy formulati</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparkhi, 2001</marker>
<rawString>Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and Adwait Ratnaparkhi. 2001. Question answering using maximum entropy components. In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics, pages 33– 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
</authors>
<title>Trainable Question Answering Systems.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Rutgers - The State University of New Jersey.</institution>
<contexts>
<context position="9036" citStr="Ittycheriah, 2001" startWordPosition="1345" endWordPosition="1346">it list of typically 10 passages, each consisting of 1-3 sentences. The candidate answers in these passages are identified and ranked based on three criteria: 1) match in semantic type between candidate answer and expected answer, 2) match in weighted grammatical relationships between question and answer passages, and 3) frequency of answer in candidate passages (redundancy). The answering agent returns the top n ranked candidate answers along with a confidence score for each answer. 3.2 Statistical Answering Agent The second answering agent takes a statistical approach to question answering (Ittycheriah, 2001; Ittycheriah et al., 2001). It models the distribution p(c|q, a), which measures the “correctness” (c) of an answer (a) to a question (q), by introducing a hidden variable representing the answer type (e) as follows: p(c|q, a) = Ee p(c, e |q, a) = Ee p(c|e, q, a)p(e|q, a) p(e|q, a) is the answer type model which predicts, from the question and a proposed answer, the answer type they both satisfy. p(c|e, q, a) is the answer selection model. Given a question, an answer, and the predicted answer type, it seeks to model the correctness of this configuration. These distributions are modeled using </context>
</contexts>
<marker>Ittycheriah, 2001</marker>
<rawString>Abraham Ittycheriah. 2001. Trainable Question Answering Systems. Ph.D. thesis, Rutgers - The State University of New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Lenat</author>
</authors>
<title>Cyc: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="5394" citStr="Lenat, 1995" startWordPosition="797" endWordPosition="798">ual components for experimental purposes. In our architecture, a question is first processed by the question analysis component. The analysis results are represented as a QFrame, which minimally includes a set of question features that help activate one or more answering agents. Each answering agent takes the QFrame and generates its own set of requests to a variety of knowledge sources. This may include performing search against a text corpus and extracting answers from the resulting passages, or performing a query against a structured knowledge source, such as WordNet (Miller, 1995) or Cyc (Lenat, 1995). The (intermediate) results from the individual answering agents are then passed on to the answer resolution component, which combines and resolves the set of results, and either produces the system’s final answers or feeds the intermediate results back to the answering agents for further processing. We have developed multiple answering agents, some general purpose and others tailored for specific question types. Figure 1 shows the answering agents currently available in PIQUANT. The knowledge-based and statistical answering agents are general-purpose agents that adopt different processing st</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Douglas B. Lenat. 1995. Cyc: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Aaron Fernandes</author>
<author>Boris Katz</author>
<author>Gregory Marton</author>
<author>Stefanie Tellex</author>
</authors>
<title>Extracting answers from the web using knowledge annotation and knowledge mining techniques.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eleventh Text Retrieval Conference.</booktitle>
<note>To appear.</note>
<contexts>
<context position="29113" citStr="Lin et al., 2003" startWordPosition="4577" endWordPosition="4580">tes of 79.9% and 79.4% for our answer-level combination algorithm on the two test sets. 6 Related Work There has been much work in employing ensemble methods to increase system performance in machine learning. In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). In question answering, a number of researchers have investigated federated systems for identifying answers to questions. For example, (Clarke et al., 2003) and (Lin et al., 2003) employ techniques for utilizing both unstruc9These cells are not marked as definite because in a small number of cases, the two answers are not equivalent. For example, for the TREC 9 question, “Who is the emperor ofJapan?”, Hirohito, Akihito, and Taisho are all considered correct answers based on the reference corpus. tured text and structured databases for question answering. However, the approaches taken by both these systems differ from ours in that they enforce an order between the two strategies by attempting to locate answers in structured databases first for select question types and </context>
</contexts>
<marker>Lin, Fernandes, Katz, Marton, Tellex, 2003</marker>
<rawString>Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory Marton, and Stefanie Tellex. 2003. Extracting answers from the web using knowledge annotation and knowledge mining techniques. In Proceedings of the Eleventh Text Retrieval Conference. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>Wordnet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="5373" citStr="Miller, 1995" startWordPosition="793" endWordPosition="794">ug-and-play of individual components for experimental purposes. In our architecture, a question is first processed by the question analysis component. The analysis results are represented as a QFrame, which minimally includes a set of question features that help activate one or more answering agents. Each answering agent takes the QFrame and generates its own set of requests to a variety of knowledge sources. This may include performing search against a text corpus and extracting answers from the resulting passages, or performing a query against a structured knowledge source, such as WordNet (Miller, 1995) or Cyc (Lenat, 1995). The (intermediate) results from the individual answering agents are then passed on to the answer resolution component, which combines and resolves the set of results, and either produces the system’s final answers or feeds the intermediate results back to the answering agents for further processing. We have developed multiple answering agents, some general purpose and others tailored for specific question types. Figure 1 shows the answering agents currently available in PIQUANT. The knowledge-based and statistical answering agents are general-purpose agents that adopt di</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George Miller. 1995. Wordnet: A lexical database for English. Communications of the ACM, 38(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Roxana Girju</author>
<author>Richard Goodrum</author>
<author>Vasile Rus</author>
</authors>
<title>The structure and performance of an opendomain question answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>563--570</pages>
<contexts>
<context position="1433" citStr="Moldovan et al., 2000" startWordPosition="198" endWordPosition="201">-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels. Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric. 1 Introduction Traditional question answering (QA) systems typically employ a pipeline approach, consisting roughly of question analysis, document/passage retrieval, and answer selection (see e.g., (Prager et al., 2000; Moldovan et al., 2000; Hovy et al., 2001; Clarke et al., 2001)). Although a typical QA system classifies questions based on expected answer types, it adopts the same strategy for locating potential answers from the same corpus regardless of the question classification. In our own earlier work, we developed a specialized mechanism called Virtual Annotation for handling definition questions (e.g., “Who was Galileo?” and “What are antibiotics?”) that consults, in addition to the standard reference corpus, a structured knowledge source (WordNet) for answering such questions (Prager et al., 2001). We have shown that be</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Girju, Goodrum, Rus, 2000</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Roxana Girju, Richard Goodrum, and Vasile Rus. 2000. The structure and performance of an opendomain question answering system. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 563–570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive Bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="28838" citStr="Pedersen, 2000" startWordPosition="4540" endWordPosition="4541">ly likely, likely, and possible categories, out of which the voting algorithm successfully selected 155 correct answers in 1st place. On TREC 11 data, 197 correct answers were selected out of 248 questions that fall into these categories. These results represent success rates of 79.9% and 79.4% for our answer-level combination algorithm on the two test sets. 6 Related Work There has been much work in employing ensemble methods to increase system performance in machine learning. In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). In question answering, a number of researchers have investigated federated systems for identifying answers to questions. For example, (Clarke et al., 2003) and (Lin et al., 2003) employ techniques for utilizing both unstruc9These cells are not marked as definite because in a small number of cases, the two answers are not equivalent. For example, for the TREC 9 question, “Who is the emperor ofJapan?”, Hirohito, Akihito, and Taisho are all considered correct answers based on the reference corpus. tur</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Ted Pedersen. 2000. A simple approach to building ensembles of naive Bayesian classifiers for word sense disambiguation. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Eric Brown</author>
<author>Anni Coden</author>
<author>Dragomir Radev</author>
</authors>
<title>Question-answering by predictive annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd SIGIR Conference,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="1410" citStr="Prager et al., 2000" startWordPosition="194" endWordPosition="197"> We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels. Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric. 1 Introduction Traditional question answering (QA) systems typically employ a pipeline approach, consisting roughly of question analysis, document/passage retrieval, and answer selection (see e.g., (Prager et al., 2000; Moldovan et al., 2000; Hovy et al., 2001; Clarke et al., 2001)). Although a typical QA system classifies questions based on expected answer types, it adopts the same strategy for locating potential answers from the same corpus regardless of the question classification. In our own earlier work, we developed a specialized mechanism called Virtual Annotation for handling definition questions (e.g., “Who was Galileo?” and “What are antibiotics?”) that consults, in addition to the standard reference corpus, a structured knowledge source (WordNet) for answering such questions (Prager et al., 2001)</context>
<context position="7693" citStr="Prager et al., 2000" startWordPosition="1130" endWordPosition="1133">m a collection of text documents, as motivated by the requirements of the TREC QA track (Voorhees, 2003). Both answering agents adopt the classic pipeline architecture, consisting roughly of question analysis, passage retrieval, and answer selection components. Although the answering agents adopt fundamentally different strategies in their individual components, they have performed quite comparably in past TREC QA tracks (Voorhees, 2001; Voorhees, 2002). 3.1 Knowledge-Based Answering Agent Our first answering agent utilizes a primarily knowledgedriven approach, based on Predictive Annotation (Prager et al., 2000). A key characteristic of this approach is that potential answers, such as person names, locations, and dates, in the corpus are predictively annotated. In other words, the corpus is indexed not only with keywords, as is typical for most search engines, but also with the semantic classes of these pre-identified potential answers. During the question analysis phase, a rule-based mechanism is employed to select one or more expected answer types, from a set of about 80 classes used in the predictive annotation process, along with a set of question keywords. A weighted search engine query is then </context>
</contexts>
<marker>Prager, Brown, Coden, Radev, 2000</marker>
<rawString>John Prager, Eric Brown, Anni Coden, and Dragomir Radev. 2000. Question-answering by predictive annotation. In Proceedings of the 23rd SIGIR Conference, pages 184–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Dragomir Radev</author>
<author>Krzysztof Czuba</author>
</authors>
<title>Answering what-is questions by virtual annotation.</title>
<date>2001</date>
<booktitle>In Proceedings of Human Language Technologies Conference,</booktitle>
<pages>26--30</pages>
<contexts>
<context position="2010" citStr="Prager et al., 2001" startWordPosition="287" endWordPosition="290">(Prager et al., 2000; Moldovan et al., 2000; Hovy et al., 2001; Clarke et al., 2001)). Although a typical QA system classifies questions based on expected answer types, it adopts the same strategy for locating potential answers from the same corpus regardless of the question classification. In our own earlier work, we developed a specialized mechanism called Virtual Annotation for handling definition questions (e.g., “Who was Galileo?” and “What are antibiotics?”) that consults, in addition to the standard reference corpus, a structured knowledge source (WordNet) for answering such questions (Prager et al., 2001). We have shown that better performance is achieved by applying Virtual Annotation and our general purpose QA strategy in parallel. In this paper, we investigate the impact of adopting such a multistrategy and multi-source approach to QA in a more general fashion. Our approach to question answering is additionally motivated by the success of ensemble methods in machine learning, where multiple classifiers are employed and their results are combined to produce the final output of the ensemble (for an overview, see (Dietterich, 1997)). Such ensemble methods have recently been adopted in question</context>
<context position="6265" citStr="Prager et al., 2001" startWordPosition="922" endWordPosition="925">ack to the answering agents for further processing. We have developed multiple answering agents, some general purpose and others tailored for specific question types. Figure 1 shows the answering agents currently available in PIQUANT. The knowledge-based and statistical answering agents are general-purpose agents that adopt different processing strategies and consult a number of different text resources. The definition-Q agent targets definition questions (e.g., “What is penicillin?” and “Who is Picasso?”) with a technique called Virtual Annotation using the external knowledge source WordNet (Prager et al., 2001). The KSP-based answering agent focuses on a subset of factoid questions with specific logical forms, such as capital(?COUNTRY) and state tree(?STATE). The answering agent sends requests to the KSP (Knowledge Sources Portal), which returns, if possible, an answer from a structured knowledge source (Chu-Carroll et al., 2003a). In the rest of this paper, we briefly describe our two general-purpose answering agents. We then focus on a multi-level answer resolution algorithm, applicable at different points in the QA process of these two answering agents. Finally, we discuss experiments conducted t</context>
</contexts>
<marker>Prager, Radev, Czuba, 2001</marker>
<rawString>John Prager, Dragomir Radev, and Krzysztof Czuba. 2001. Answering what-is questions by virtual annotation. In Proceedings of Human Language Technologies Conference, pages 26–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC-9 question answering track.</title>
<date>2001</date>
<booktitle>In Proceedings of the 9th Text Retrieval Conference,</booktitle>
<pages>71--80</pages>
<contexts>
<context position="7513" citStr="Voorhees, 2001" startWordPosition="1108" endWordPosition="1109">ombining results from multiple answering agents. 3 Component Answering Agents We focus on two end-to-end answering agents designed to answer short, fact-seeking questions from a collection of text documents, as motivated by the requirements of the TREC QA track (Voorhees, 2003). Both answering agents adopt the classic pipeline architecture, consisting roughly of question analysis, passage retrieval, and answer selection components. Although the answering agents adopt fundamentally different strategies in their individual components, they have performed quite comparably in past TREC QA tracks (Voorhees, 2001; Voorhees, 2002). 3.1 Knowledge-Based Answering Agent Our first answering agent utilizes a primarily knowledgedriven approach, based on Predictive Annotation (Prager et al., 2000). A key characteristic of this approach is that potential answers, such as person names, locations, and dates, in the corpus are predictively annotated. In other words, the corpus is indexed not only with keywords, as is typical for most search engines, but also with the semantic classes of these pre-identified potential answers. During the question analysis phase, a rule-based mechanism is employed to select one or </context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Ellen M. Voorhees. 2001. Overview of the TREC-9 question answering track. In Proceedings of the 9th Text Retrieval Conference, pages 71–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>In Proceedings of the 10th Text Retrieval Conference,</booktitle>
<pages>42--51</pages>
<contexts>
<context position="7530" citStr="Voorhees, 2002" startWordPosition="1110" endWordPosition="1111"> from multiple answering agents. 3 Component Answering Agents We focus on two end-to-end answering agents designed to answer short, fact-seeking questions from a collection of text documents, as motivated by the requirements of the TREC QA track (Voorhees, 2003). Both answering agents adopt the classic pipeline architecture, consisting roughly of question analysis, passage retrieval, and answer selection components. Although the answering agents adopt fundamentally different strategies in their individual components, they have performed quite comparably in past TREC QA tracks (Voorhees, 2001; Voorhees, 2002). 3.1 Knowledge-Based Answering Agent Our first answering agent utilizes a primarily knowledgedriven approach, based on Predictive Annotation (Prager et al., 2000). A key characteristic of this approach is that potential answers, such as person names, locations, and dates, in the corpus are predictively annotated. In other words, the corpus is indexed not only with keywords, as is typical for most search engines, but also with the semantic classes of these pre-identified potential answers. During the question analysis phase, a rule-based mechanism is employed to select one or more expected ans</context>
<context position="19783" citStr="Voorhees, 2002" startWordPosition="3053" endWordPosition="3054">mbination 4In future work we will be investigating weighted voting schemes based on question features. 5The confidence values from both answering agents are normalized to be between 0 and 1. 6The statistical agent is currently unable to consult multiple corpora. 7Both the AQUAINT and TREC corpora are available from the Linguistics Data Consortium, http://www.ldc.org. algorithms individually and cumulatively. For cumulative effects, we 1) combined the algorithms pair-wise, and 2) employed all three algorithms together. The two test sets were selected from the TREC 10 and 11 QA track questions (Voorhees, 2002; Voorhees, 2003). For both test sets, we eliminated those questions that did not have known answers in the reference corpus. Furthermore, from the TREC 10 test set, we discarded all definition questions,8 since the knowledge-based agent adopts a specialized strategy for handling definition questions which greatly reduces potential contributions from other answering agents. This results in a TREC 10 test set of 313 questions and a TREC 11 test set of 453 questions. 5.2 Experimental Results We ran each of the baseline and combined systems on the two test sets. For each run, the system outputs i</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>Ellen M. Voorhees. 2002. Overview of the TREC 2001 question answering track. In Proceedings of the 10th Text Retrieval Conference, pages 42–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In Proceedings of the Eleventh Text Retrieval Conference.</booktitle>
<note>To appear.</note>
<contexts>
<context position="7177" citStr="Voorhees, 2003" startWordPosition="1062" endWordPosition="1063">e (Chu-Carroll et al., 2003a). In the rest of this paper, we briefly describe our two general-purpose answering agents. We then focus on a multi-level answer resolution algorithm, applicable at different points in the QA process of these two answering agents. Finally, we discuss experiments conducted to discover effective methods for combining results from multiple answering agents. 3 Component Answering Agents We focus on two end-to-end answering agents designed to answer short, fact-seeking questions from a collection of text documents, as motivated by the requirements of the TREC QA track (Voorhees, 2003). Both answering agents adopt the classic pipeline architecture, consisting roughly of question analysis, passage retrieval, and answer selection components. Although the answering agents adopt fundamentally different strategies in their individual components, they have performed quite comparably in past TREC QA tracks (Voorhees, 2001; Voorhees, 2002). 3.1 Knowledge-Based Answering Agent Our first answering agent utilizes a primarily knowledgedriven approach, based on Predictive Annotation (Prager et al., 2000). A key characteristic of this approach is that potential answers, such as person na</context>
<context position="19800" citStr="Voorhees, 2003" startWordPosition="3055" endWordPosition="3056">ture work we will be investigating weighted voting schemes based on question features. 5The confidence values from both answering agents are normalized to be between 0 and 1. 6The statistical agent is currently unable to consult multiple corpora. 7Both the AQUAINT and TREC corpora are available from the Linguistics Data Consortium, http://www.ldc.org. algorithms individually and cumulatively. For cumulative effects, we 1) combined the algorithms pair-wise, and 2) employed all three algorithms together. The two test sets were selected from the TREC 10 and 11 QA track questions (Voorhees, 2002; Voorhees, 2003). For both test sets, we eliminated those questions that did not have known answers in the reference corpus. Furthermore, from the TREC 10 test set, we discarded all definition questions,8 since the knowledge-based agent adopts a specialized strategy for handling definition questions which greatly reduces potential contributions from other answering agents. This results in a TREC 10 test set of 313 questions and a TREC 11 test set of 453 questions. 5.2 Experimental Results We ran each of the baseline and combined systems on the two test sets. For each run, the system outputs its top answer and</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Ellen M. Voorhees. 2003. Overview of the TREC 2002 question answering track. In Proceedings of the Eleventh Text Retrieval Conference. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the 19th SIGIR Conference,</booktitle>
<pages>4--11</pages>
<contexts>
<context position="10216" citStr="Xu and Croft, 1996" startWordPosition="1549" endWordPosition="1552"> These distributions are modeled using a maximum entropy formulation (Berger et al., 1996), using training data which consists of human judgments of question answer pairs. For the answer type model, 13K questions were annotated with 31 categories. For the answer selection model, 892 questions from the TREC 8 and TREC 9 QA tracks were used, along with 4K trivia questions. During runtime, the question is first analyzed by the answer type model, which selects one out of a set of 31 types for use by the answer selection model. Simultaneously, the question is expanded using local context analysis (Xu and Croft, 1996) with an encyclopedia, and the top 1000 documents are retrieved by the search engine. From these documents, the top 100 passages are chosen that 1) maximize the question word match, 2) have the desired answer type, 3) minimize the dispersion of question words, and 4) have similar syntactic structures as the question. From these passages, candidate answers are extracted and ranked using the answer selection model. The top n candidate answers are then returned, each with an associated confidence score. 4 Answer Resolution Given two answering agents with the same pipeline architecture, there are </context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 1996. Query expansion using local and global document analysis. In Proceedings of the 19th SIGIR Conference, pages 4–11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>