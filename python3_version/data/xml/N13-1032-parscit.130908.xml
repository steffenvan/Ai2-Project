<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9869325">
Improving reordering performance using higher order and structural
features
</title>
<author confidence="0.880582">
Mitesh M. Khapra Ananthakrishnan Ramanathan Karthik Visweswariah
</author>
<affiliation confidence="0.846287">
IBM Research India IBM Research India IBM Research India
</affiliation>
<email confidence="0.997383">
mikhapra@in.ibm.com anandr42@gmail.com v-karthik@in.ibm.com
</email>
<sectionHeader confidence="0.993862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99959356">
Recent work has shown that word aligned data
can be used to learn a model for reordering
source sentences to match the target order.
This model learns the cost of putting a word
immediately before another word and finds the
best reordering by solving an instance of the
Traveling Salesman Problem (TSP). However,
for efficiently solving the TSP, the model is
restricted to pairwise features which examine
only a pair of words and their neighborhood.
In this work, we go beyond these pairwise fea-
tures and learn a model to rerank the n-best
reorderings produced by the TSP model us-
ing higher order and structural features which
help in capturing longer range dependencies.
In addition to using a more informative set
of source side features, we also capture target
side features indirectly by using the transla-
tion score assigned to a reordering. Our exper-
iments, involving Urdu-English, show that the
proposed approach outperforms a state-of-the-
art PBSMT system which uses the TSP model
for reordering by 1.3 BLEU points, and a pub-
licly available state-of-the-art MT system, Hi-
ero, by 3 BLEU points.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986377777778">
Handling the differences in word orders between
pairs of languages is crucial in producing good ma-
chine translation. This is especially true for lan-
guage pairs such as Urdu-English which have sig-
nificantly different sentence structures. For exam-
ple, the typical word order in Urdu is Subject Object
Verb whereas the typical word order in English is
Subject Verb Object. Phrase based systems (Koehn
et al., 2003) rely on a lexicalized distortion model
(Al-Onaizan and Papineni, 2006; Tillman, 2004)
and the target language model to produce output
words in the correct order. This is known to be in-
adequate when the languages are very different in
terms of word order (refer to Table 3 in Section 3).
Pre-ordering source sentences while training and
testing has become a popular approach in overcom-
ing the word ordering challenge. Most techniques
for pre-ordering (Collins et al., 2005; Wang et al.,
2007; Ramanathan et al., 2009) depend on a high
quality source language parser, which means these
methods work only if the source language has a
parser (this rules out many languages). Recent work
(Visweswariah et al., 2011) has shown that it is pos-
sible to learn a reordering model from a relatively
small number of hand aligned sentences. This elim-
inates the need of a source or target parser.
In this work, we build upon the work of
Visweswariah et al. (2011) which solves the reorder-
ing problem by treating it as an instance of the
Traveling Salesman Problem (TSP). They learn a
model which assigns costs to all pairs of words in
a sentence, where the cost represents the penalty of
putting a word immediately preceding another word.
The best permutation is found via the chained Lin-
Kernighan heuristic for solving a TSP. Since this
model relies on solving a TSP efficiently, it cannot
capture features other than pairwise features that ex-
amine the words and neighborhood for each pair of
words in the source sentence. In the remainder of
this paper we refer to this model as the TSP model.
Our aim is to go beyond this limitation of the TSP
model and use a richer set of features instead of us-
ing pairwise features only. In particular, we are in-
terested in features that allow us to examine triples
of words/POS tags in the candidate reordering per-
</bodyText>
<page confidence="0.989693">
315
</page>
<note confidence="0.4707035">
Proceedings of NAACL-HLT 2013, pages 315–324,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.978601634615384">
mutation (this is akin to going from bigram to tri-
gram language models), and also structural features
that allow us to examine the properties of the seg-
mentation induced by the candidate permutation. To
go beyond the set of features incorporated by the
TSP model, we do not solve the search problem
which would be NP-hard. Instead, we restrict our-
selves to an n-best list produced by the base TSP
model and then search in that list. Using a richer
set of features, we learn a model to rerank these n-
best reorderings. The parameters of the model are
learned using the averaged perceptron algorithm. In
addition to using a richer set of source side features
we also indirectly capture target side features by in-
terpolating the score assigned by our model with the
score assigned by the decoder of a MT system.
To justify the use of these informative features,
we point to the example in Table 1. Here, the head
(driver) of the underlined English Noun Phrase (The
driver of the car) appears to the left of the Noun
Phrase whereas the head (chaalak {driver}) of the
corresponding Urdu Noun Phrase (gaadi {car} ka
{of} chaalak {driver}) appears to the right of the
Noun Phrase. To produce the correct reordering of
the source Urdu sentence the model has to make an
unusual choice of putting gaadi {car} before bola
{said}. We say this is an unusual choice because the
model examines only pairwise features and it is un-
likely that it would have seen sentences having the
bigram “car said”. If the exact segmentation of the
source sentence was known, then the model could
have used the information that the word gaadi {car}
appears in a segment whose head is the noun chaalak
{driver} and hence its not unusual to put gaadi {car}
before bola {said} (because the construct “NP said”
is not unusual). However, since the segmentation
of the source sentence is not known in advance, we
use a heuristic (explained later) to find the segmen-
tation induced by a reordering. We then extract
features (such as first word current segment,
end word current segment) to approximate these
long range dependencies.
Using this richer set of features with Urdu-
English as the source language pair, our approach
outperforms the following state of the art systems:
(i) a PBSMT system which uses TSP model for re-
ordering (by 1.3 BLEU points), (ii) a hierarchical
PBSMT system (by 3 BLEU points). The overall
Input Urdu: fir gaadi ka chaalak kuch bola
Gloss: then car of driver said something
English: Then the driver of the car said something.
Ref. reordering: fir chaalak ka gaadi bola kuch
</bodyText>
<tableCaption confidence="0.998834">
Table 1: Example motivating the use of structural features
</tableCaption>
<bodyText confidence="0.998940588235294">
gain is 6.3 BLEU points when compared to a stan-
dard PBSMT system which uses a lexicalized distor-
tion model (Al-Onaizan and Papineni, 2006).
The rest of this paper is organized as follows. In
Section 2 we discuss our approach of re-ranking the
n-best reorderings produced by the TSP model. This
includes a discussion of the model used, the features
used and the algorithm used for learning the parame-
ters of the model. It also includes a discussion on the
modification to the Chained Lin-Kernighan heuris-
tic to produce n-best reorderings. Next, in Section
3 we describe our experimental setup and report the
results of our experiments. In Section 4 we present
some discussions based on our study. In section 5 we
briefly describe some prior related work. Finally, in
Section 6, we present some concluding remarks and
highlight possible directions for future work.
</bodyText>
<sectionHeader confidence="0.7295255" genericHeader="method">
2 Re-ranking using higher order and
structural features
</sectionHeader>
<bodyText confidence="0.9999476">
As mentioned earlier, the TSP model (Visweswariah
et al., 2011) looks only at local features for a word
pair (wi, wj). We believe that for better reorder-
ing it is essential to look at higher order and struc-
tural features (i.e., features which look at the overall
structure of a sentence). The primary reason why
Visweswariah et al. (2011) consider only pairwise
bigram features is that with higher order features the
reordering problem can no longer be cast as a TSP
and hence cannot be solved using existing efficient
heuristic solvers. However, we do not have to deal
with an NP-Hard search problem because instead of
considering all possible reorderings we restrict our
search space to only the n-best reorderings produced
by the base TSP model. Formally, given a set of
reorderings, H = [7r1, 7r2, 7r3, ...., 7rn], for a source
sentence s, we are interesting in assigning a score,
score(7r), to each of these reorderings and pick the
reordering which has the highest score. In this paper,
we parametrize this score as:
</bodyText>
<equation confidence="0.99719">
score(7r) = OT 0(7r) (1)
</equation>
<page confidence="0.989447">
316
</page>
<bodyText confidence="0.999795">
where, 0 is the weight vector and 0(7r) is a vector
of features extracted from the reordering 7r. The aim
then is to find,
</bodyText>
<equation confidence="0.988439">
7r* = arg max score(7r) (2)
7rEil
</equation>
<bodyText confidence="0.999943153846154">
In the following sub-sections, we first briefly
describe our overall approach towards finding 7r*.
Next, we describe our modification to the Lin-
Kernighan heuristic for producing n-best outputs
for TSP instead of the 1-best output used by
(Visweswariah et al., 2011). We then discuss the fea-
tures used for re-ranking these n-best outputs, fol-
lowed by a discussion on the learning algorithm used
for estimating the parameters of the model. Finally,
we describe how we interpolate the score assigned
by our model with the score assigned by the decoder
of a SMT engine to indirectly capture target side fea-
tures.
</bodyText>
<subsectionHeader confidence="0.997997">
2.1 Overall approach
</subsectionHeader>
<bodyText confidence="0.999929764705882">
The training stage of our approach involves two
phases : (i) Training a TSP model which will be
used to generate n-best reorderings and (ii) Training
a re-ranking model using these n-best reorderings.
For training both the models we need a collection
of sentences where the desired reordering 7r*(x) for
each input sentence x is known. These reference or-
derings are derived from word aligned source-target
sentence pairs (see first 4 rows of Figure 1). We first
divide this word aligned data into N parts and use
A−Z to denote the alignments leaving out the i-th
part. We then train a TSP model M−Z using refer-
ence reorderings derived from A−Z as described in
(Visweswariah et al., 2011). Next, we produce n-
best reorderings for the source sentences using the
algorithm getNBestReorderings(sentence) de-
scribed later. Dividing the data into N parts is nec-
essary to ensure that the re-ranking model is trained
using a realistic n-best list rather than a very opti-
mistic n-best list (which would be the case if part i
is reordered using a model which has already seen
part i during training).
Each of the n-best reorderings is then repre-
sented as a feature vector comprising of higher
order and structural features. The weights
of these features are then estimated using the
averaged perceptron method. At test time,
getNBestReorderings(sentence) is used to gen-
erate the n-best reorderings for the test sentence us-
ing the trained TSP model. These reorderings are
then represented using higher order and structural
features and re-ranked using the weights learned ear-
lier. We now describe the different stages of our al-
gorithm.
</bodyText>
<subsectionHeader confidence="0.709652">
2.2 Generating n-best reorderings for the TSP
model
</subsectionHeader>
<bodyText confidence="0.981502">
The first stage of our approach is to train a TSP
model and generate n-best reorderings using it. The
decoder used by Visweswariah et al. (2011) relies
on the Chained Lin-Kernighan heuristic (Lin and
Kernighan, 1973) to produce the 1-best permutation
for the TSP problem. Since our algorithm aims at
re-ranking an n-best list of permutations (reorder-
ings), we made a modification to the Chained Lin-
Kernighan heuristic to produce this n-best list as
shown in Algorithm 1 .
Algorithm 1 getNBestReorderings(sentence)
In Algorithm 1 perturb() is a four-edge pertur-
bation described in (Applegate et al., 2003), and
linkernighan() is the Lin-Kernighan heuristic that
applies a sequence of flips that potentially returns
a lower cost permutation as described in (Lin and
Kernighan, 1973). The cost C(7r) is calculated us-
ing a trained TSP model.
</bodyText>
<subsectionHeader confidence="0.958974">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.999957333333333">
We represent each of the n-best reorderings obtained
above as a vector of features which can be divided
into two sets : (i) higher order features and (ii) struc-
</bodyText>
<equation confidence="0.985593">
NbestSet = 0
7r* = Identity permutation
7r* = linkernighan(7r*)
insert(NbestSet, 7r*)
for i = 1 → nIter do
0
7r
0
7r= linkernighan(7r0)
if C(7r0) &lt; maxwENbestSetC(7r) then
InsertOrReplace(NbestSet,7r0)
end if
if C(7r0) &lt; C(7r*) then
7r* = 7r0
end if
end for
= perturb(7r*)
</equation>
<page confidence="0.994118">
317
</page>
<table confidence="0.594130333333333">
Segmentation Based Features Features fired for the seg-
(extracted for every segment in ment [mere(PRP) ghar(NN)]
the induced segmentation) in Figure1
</table>
<tableCaption confidence="0.658873777777778">
end lex current segment ghar
end lex prev segment Shyam
end pos current segment NN
end pos prev segment NN
length of current segment 2
first lex current segment mere
first lex next segment aaye
first pos current segment PRP
first pos next segment VRB
</tableCaption>
<figure confidence="0.96104">
Higher order features Features fired for the triplet
Shyam(NN) the(Vaux)
aaye(VRB) in Figure1
lex triplet jumps lex triplet = “Shyam the
aaye” &amp;&amp; jumps = [4, −1]
pos triplet jumps pos triplet = “NN Vaux
VRB” &amp;&amp; jumps = [4, −1]
</figure>
<tableCaption confidence="0.980706">
Table 2: Features used in our model.
</tableCaption>
<bodyText confidence="0.99892775">
tural features. The higher order features are es-
sentially trigram lexical and pos features whereas
the structural features are derived from the sentence
structure induced by a reordering (explained later).
</bodyText>
<subsectionHeader confidence="0.956717">
2.3.1 Higher Order Features
</subsectionHeader>
<bodyText confidence="0.998984">
Since deriving a good reordering would essen-
tially require analyzing the syntactic structure of the
source sentence, the tasks of reordering and parsing
are often considered to be related. The main motiva-
tion for using higher order features thus comes from
a related work on parsing (Koo and Collins, 2010)
where the performance of a state of the art parser
was improved by considering higher order depen-
dencies. In our model we use trigram features (see
Table 2) of the following form:
0(rui, rui+1, rui+2, J(rui, rui+1), J(rui+1, rui+2))
where rui =word at position i in the reordered
source sentence and J(x, y) = difference between
the positions of x and y in the original source
sentence.
Figure 1 shows an example of jumps between dif-
ferent word pairs in an Urdu sentence. Since such
higher order features will typically be sparse, we
also use some back-off features. For example, in-
stead of using the absolute values of jumps we di-
vide the jumps into 3 buckets, viz., high, low and
medium and use these buckets in conjunction with
the triplets as back-off features.
</bodyText>
<figureCaption confidence="0.979378">
Figure 1: Segmentation induced on the Urdu sentence
</figureCaption>
<bodyText confidence="0.7537058">
when it is reordered according to its English translation.
Note that the words Shyam and mere are adjacent to each
other in the original Urdu sentence but not in the re-
ordered Urdu sentence. Hence, the word mere marks the
beginning of a new segment.
</bodyText>
<subsectionHeader confidence="0.890297">
2.3.2 Structural Features
</subsectionHeader>
<bodyText confidence="0.999997846153846">
The second set of features is based on the hy-
pothesis that any reordering of the source sentence
induces a segmentation on the sentence. This seg-
mentation is based on the following heuristic: if wi
and wi+1 appear next to each other in the original
sentence but do not appear next to each other in the
reordered sentence then wi marks the end of a seg-
ment and wi+1 marks the beginning of the next seg-
ment. To understand this better please refer to Fig-
ure 1 which shows the correct reordering of an Urdu
sentence based on its English translation and the cor-
responding segmentation induced on the Urdu sen-
tence. If the correct segmentation of a sentence is
known in advance then one could use a hierarchical
model where the goal would be to reorder segments
instead of reordering words individually (basically,
instead of words, treat segments as units of reorder-
ing. In principle, this is similar to what is done by
parser based reordering methods). Since the TSP
model does not explicitly use segmentation based
features it often produces wrong reorderings (refer
to the motivating example in Section 1).
Reordering such sentences correctly requires
some knowledge about the hierarchical structure of
the sentence. To capture such hierarchical informa-
tion, we use features which look at the elements
</bodyText>
<page confidence="0.995601">
318
</page>
<bodyText confidence="0.999976818181818">
(words, pos tags) of a segment and its neighboring
segments. These features along with examples are
listed in Table 2. These features should help us in
selecting a reordering which induces a segmentation
which is closest to the correct segmentation induced
by the reference reordering. Note that every feature
listed in Table 2 is a binary feature which takes on
the value 1 if it fires for the given reordering and
value 0 if it does not fire for the given reordering. In
addition to the features listed in Table 2 we also use
the score assigned by the TSP model as a feature.
</bodyText>
<subsectionHeader confidence="0.984528">
2.4 Estimating model parameters
</subsectionHeader>
<bodyText confidence="0.999987555555556">
We use perceptron as the learning algorithm for es-
timating the parameters of our model described in
Equation 1. To begin with, all parameters are ini-
tialized to 0 and the learning algorithm is run for N
iterations. During each iteration the parameters are
updated after every training instance is seen. For ex-
ample, during the i-th iteration, after seeing the j-th
training sentence, we update the k-th parameter 0k
using the following update rule:
</bodyText>
<equation confidence="0.8939914">
0(i,j) k= 0(i,j−1)+ Ok(7rgold
j ) − Ok(7r∗j) (3)
k
where, 0(i,j) k= value of the k-th parameter after
seeing sentence j in iteration i
Ok = k-th feature
7rj = gold reordering for the j-th sentence
gold
7r∗j = arg max
π∈IIj
</equation>
<bodyText confidence="0.999364714285714">
where IIj is the set of n-best reorderings for the j-
th sentence. 7r∗j is thus the highest-scoring reorder-
ing for the j-th sentence under the current parame-
ter vector. Since the averaged perceptron method is
known to perform better than the perceptron method,
we used the averaged values of the parameters at the
end of N iterations, calculated as:
</bodyText>
<equation confidence="0.935123666666667">
N
0avg = 1
k N · t
i=1
where, N = Number of iterations
t = Number of training instances
</equation>
<bodyText confidence="0.8945506">
We observed that in most cases the reference re-
ordering in not a part of the n-best list produced
by the TSP model. In such cases instead of using
Ok(7rgold
j ) for updating the weights in Equation 3 we
use Ok(7rclosest to gold) as this is known to be a better
j
strategy for learning a re-ranking model (Arun and
Koehn, 2007). 7rclosest to gold is given by:
j
</bodyText>
<figure confidence="0.583579">
# of common bigram pairs in 7rij and 7rgold
j
len(7rgold
j )
where, IIj = set of n-best reorderings for jth sentence
�
7r
</figure>
<bodyText confidence="0.93143875">
closest to gold is thus the reordering which has the
maximum overlap with 7rgold
j in terms of the number
of word pairs (wm, wn) where wn is put next to wm.
</bodyText>
<subsectionHeader confidence="0.991652">
2.5 Interpolating with MT score
</subsectionHeader>
<bodyText confidence="0.999935230769231">
The approach described above aims at producing a
better reordering by extracting richer features from
the source sentence. Since the final aim is to im-
prove the performance of an MT system, it would
potentially be beneficial to interpolate the scores as-
signed by Equation 1 to a given reordering with the
score assigned by the decoder of an MT system to
the translation of the source sentence under this re-
ordering. Intuitively, the MT score would allow us
to capture features from the target sentence which
are obviously not available to our model. With this
motivation, we use the following interpolated score
(scoreI) to select the best translation.
</bodyText>
<equation confidence="0.782964">
scoreI(ti) = A·scoreθ(7ri) + (1 − A) · scoreMT(ti)
</equation>
<bodyText confidence="0.9878038125">
where, ti =translation produced under the i-th
reordering of the source sentence
scoreθ(7ri) =score assigned by our model to the
i-th reordering
scoreMT(ti) =score assigned by the MT system to ti
The weight A is used to ensure that scoreθ(7ri) and
scoreMT(7ri) are in the same range (it just serves as
a normalization constant). We acknowledge that the
above process is expensive because it requires the
MT system to decode n reorderings for every source
sentence. However, the aim of this work is to show
that interpolating with the MT score which implic-
itly captures features from the target sentence helps
in improving the performance. Ideally, this interpo-
lation should (and can) be done at decode time with-
out having to decode n reorderings for every source
</bodyText>
<equation confidence="0.853556125">
0(i,j−1)T O(7r)
0(i,j) (4)
k
t
L,
j=1
arg max
π�j∈IIj
</equation>
<page confidence="0.99631">
319
</page>
<bodyText confidence="0.9997735">
sentence (for example by expressing the n reorder-
ings as a lattice), but, we leave this as future work.
</bodyText>
<sectionHeader confidence="0.988611" genericHeader="method">
3 Empirical evaluation
</sectionHeader>
<bodyText confidence="0.999971926829268">
We evaluated our reordering approach on Urdu-
English. We use two types of evaluation, one in-
trinsic and one extrinsic. For intrinsic evaluation,
we compare the reordered source sentence in Urdu
with a reference reordering obtained from the hand
alignments using BLEU (referred to as monolingual
BLEU or mBLEU by (Visweswariah et al., 2011) ).
Additionally, we evaluate the effect of reordering on
MT performance using BLEU (extrinsic evaluation).
As mentioned earlier, our training process in-
volves two phases : (i) Generating n-best reorder-
ings for the training data and (ii) using these n-best
reorderings to train a perceptron model. We use the
same data for training the reordering model as well
as our perceptron model. This data contains 180K
words of manual alignments (part of the NIST MT-
08 training data) and 3.9M words of automatically
generated machine alignments (1.7M words from
the NIST MT-08 training data1 and 2.2M words ex-
tracted from sources on the web2). The machine
alignments were generated using a supervised maxi-
mum entropy model (Ittycheriah and Roukos, 2005)
and then corrected using an improved correction
model (McCarley et al., 2011). We first divide the
training data into 10 folds. The n-best reorder-
ings for each fold are then generated using a model
trained on the remaining 9 folds. This division into
10 folds is done for reasons explained earlier in Sec-
tion 2.1. These n-best reorderings are then used to
train the perceptron model as described in Section
2.4. Note that Visweswariah et al. (2011) used only
manually aligned data for training the TSP model.
However, we use machine aligned data in addition
to manually aligned data for training the TSP model
as it leads to better performance. We used this im-
provised TSP model as the state of the art baseline
(rows 2 and 3 in Tables 3 and 4 respectively) for
comparing with our approach.
We observed that the perceptron algorithm con-
verges after 5 iterations beyond which there is very
little (&lt;1%) improvement in the bigram precision on
</bodyText>
<footnote confidence="0.9999335">
1http://www.ldc.upenn.edu
2http://centralasiaonline.com
</footnote>
<bodyText confidence="0.999940472222222">
the training data itself (bigram precision is the frac-
tion of word pairs which are correctly put next to
each other). Hence, for all the numbers reported in
this paper, we used 5 iterations of perceptron train-
ing. Similarly, while generating the n-best reorder-
ings, we experimented with following values of n :
10, 25, 50, 100 and 200. We observed that, by re-
stricting the search space to the top-50 reorderings
we get the best reordering performance (mBLEU)
on a development set. Hence, we used n=50 for our
MT experiments.
For intrinsic evaluation we use a development set
of 8017 Urdu tokens reordered manually. Table 3
compares the performance of the top-1 reordering
output by our algorithm with the top-1 reordering
generated by the improved TSP model in terms of
mBLEU. We see a gain of 1.8 mBLEU points with
our approach.
Next, we see the impact of the better reorderings
produced by our system on the performance of
a state-of-the-art MT system. For this, we used
a standard phrase based system (Al-Onaizan and
Papineni, 2006) with a lexicalized distortion model
with a window size of +/-4 words (Tillmann and
Ney, 2003). As mentioned earlier, our training data
consisted of 3.9M words including the NIST MT-08
training data. We use HMM alignments along with
higher quality alignments from a supervised aligner
(McCarley et al., 2011). The Gigaword English
corpus was used for building the English language
model. We report results on the NIST MT-08
evaluation set, averaging BLEU scores from the
News and Web conditions to provide a single BLEU
score. Table 4 compares the MT performance
obtained by reordering the training and test data
using the following approaches:
</bodyText>
<listItem confidence="0.9857055">
1. No pre-ordering: A baseline system which
does not use any source side reordering as a pre-
processing step
2. HIERO : A state of the art hierarchical phrase
based translation system (Chiang, 2007)
3. TSP: A system which uses the 1-best reordering
produced by the TSP model
4. Higher order &amp; structural features: A system
</listItem>
<page confidence="0.993755">
320
</page>
<table confidence="0.879351">
Approach mBLEU
Unreordered 31.2
TSP 56.6
Higher order &amp; structural features 58.4
</table>
<tableCaption confidence="0.9435955">
Table 3: mBLEU scores for Urdu to English reordering
using different models.
</tableCaption>
<table confidence="0.999779">
Approach BLEU
No pre-ordering 21.9
HIERO 25.2
TSP 26.9
Higher order &amp; structural features 27.5
Interpolating with MT score 28.2
</table>
<tableCaption confidence="0.9580245">
Table 4: MT performance for Urdu to English without re-
ordering and with reordering using different approaches.
</tableCaption>
<bodyText confidence="0.9995350625">
which reranks n-best reorderings produced by TSP
using higher order and structural features
5. Interpolating with MT score : A system which
interpolates the score assigned to a reordering by
our model with the score assigned by a MT system
We used Joshua 4.0 (Ganitkevitch et al., 2012)
which provides an open source implementation of
HIERO. For training, tuning and testing HIERO
we used the same experimental setup as described
above. As seen in Table 4, we get an overall gain of
6.2 BLEU points with our approach as compared to
a baseline system which does not use any reordering.
More importantly, we outperform (i) a PBSMT sys-
tem which uses the TSP model by 1.3 BLEU points
and (ii) a state of the art hierarchical phrase based
translation system by 3 points.
</bodyText>
<sectionHeader confidence="0.99568" genericHeader="method">
4 Discussions
</sectionHeader>
<bodyText confidence="0.9995365">
We now discuss some error corrections and ablation
tests.
</bodyText>
<subsectionHeader confidence="0.999478">
4.1 Example of error correction
</subsectionHeader>
<bodyText confidence="0.999955142857143">
We first give an example where the proposed ap-
proach performed better than the TSP model. In the
example below, I = input sentence, E= gold English
translation, T = incorrect reordering produced by
TSP and O = correct reordering produced by our
approach. Note that the words roman catholic aur
protestant in the input sentence get translated as
</bodyText>
<table confidence="0.998869714285714">
Sentence length mBLEU
Unreordered TSP Our
approach
1-14 words (small) 29.7 58.7 57.8
15-22 words (med.) 28.2 56.8 59.2
23+ words (long) 33.4 55.8 58.2
All 31.2 56.6 58.4
</table>
<tableCaption confidence="0.975739">
Table 5: mBLEU improvements on sentences of different
lengths
</tableCaption>
<bodyText confidence="0.997800071428571">
a continuous phrase in English (Roman Catholic
and Protestant) and hence should be treated as a
single unit by the reordering model. The TSP model
fails to keep this segment intact whereas our model
(which uses segmentation based features) does so
and matches the reference reordering.
I: ab roman catholic aur protestant ke darmiyaan
ikhtilafat khatam ho chuke hai
E: The differences between Roman Catholics and
Protestants have now ended
T: ab roman ikhtilafat ke darmiyaan catholic aur
protestant hai khatam ho chuke
O: ab ikhtilafat ke darmiyaan roman catholic aur
protestant hai khatam ho chuke
</bodyText>
<subsectionHeader confidence="0.994923">
4.2 Performance based on sentence length
</subsectionHeader>
<bodyText confidence="0.999951133333333">
We split the test data into roughly three equal parts
based on length, and calculated the mBLEU im-
provements on each of these parts as reported in
Table 5. These results show that the model works
much better for medium-to-long sentences. In fact,
we see a drop in performance for small sentences. A
possible reason for this could be that the structural
features that we use are derived through a heuristic
that is error-prone, and in shorter sentences, where
there would be fewer reordering problems, these er-
rors hurt more than they help. While this needs to be
analyzed further, we could meanwhile combine the
two models fruitfully by using the base TSP model
for small sentences and the new model for longer
sentences.
</bodyText>
<page confidence="0.99666">
321
</page>
<table confidence="0.988823692307692">
Disabled feature mBLEU
end lex current segment 57.6
end lex prev segment 57.6
end pos current segment 57.8
end pos prev segment 57.4
length 57.6
lex triplet jumps 58.0
pos triplet jumps 56.1
first lex current segment 58.2
first lex next segment 58.2
first pos current segment 57.6
first pos next segment 57.6
NONE 58.4
</table>
<tableCaption confidence="0.8895155">
Table 6: Ablation test indicating the contribution of each
feature to the reordering performance.
</tableCaption>
<subsectionHeader confidence="0.999398">
4.3 Ablation test
</subsectionHeader>
<bodyText confidence="0.999984933333333">
To study the contribution of each feature to the
reordering performance, we did an ablation test
wherein we disabled one feature at a time and mea-
sured the change in the mBLEU scores. Table 6
summarizes the results of our ablation test. The
maximum drop in performance is obtained when the
pos triplet jumps feature is disabled. This obser-
vation supports our claim that higher order features
(more than bigrams) are essential for better reorder-
ing. The lex triplet jumps feature has the least
impact on the performance mainly because it is a
lexicalized feature and hence very sparse. Also note
that there is a high correlation between the perfor-
mances obtained by dropping one feature from each
of the following pairs :
</bodyText>
<listItem confidence="0.826018125">
i) first lex current segment, first lex next segment
ii) first pos current segment, first pos next segment
iii) end lex current segment, end lex next segment.
This is because these pairs of features are
highly dependent features. Note that similar to
the pos triplet jumps feature we also tried a
pos quadruplet jumps feature but it did not help
(mainly due to overfitting and sparsity).
</listItem>
<sectionHeader confidence="0.999656" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999983577777778">
There are several studies which have shown that re-
ordering the source side sentence to match the target
side order leads to improvements in Machine Trans-
lation. These approaches can be broadly classified
into three types. First, approaches which reorder
source sentences by applying rules to the source side
parse; the rules are either hand-written (Collins et
al., 2005; Wang et al., 2007; Ramanathan et al.,
2009) or learned from data (Xia and McCord, 2004;
Genzel, 2010; Visweswariah et al., 2010). These
approaches require a source side parser which is
not available for many languages. The second type
of approaches treat machine translation decoding
as a parsing problem by using source and/or tar-
get side syntax in a Context Free Grammar frame-
work. These include Hierarchical models (Chi-
ang, 2007) and syntax based models (Yamada and
Knight, 2002; Galley et al., 2006; Liu et al., 2006;
Zollmann and Venugopal, 2006). The third type of
approaches, avoid the use of a parser (as required
by syntax based models) and instead train a reorder-
ing model using reference reorderings derived from
aligned data. These approaches (Tromble and Eis-
ner, 2009; Visweswariah et al., 2011; DeNero and
Uszkoreit, 2011; Neubig et al., 2012) have a low de-
code time complexity as reordering is done as a pre-
processing step and not integrated with the decoder.
Our work falls under the third category, as it im-
proves upon the work of (Visweswariah et al., 2011)
which is closely related to the work of (Tromble
and Eisner, 2009) but performs better. The focus
of our work is to use higher order and structural
features (based on segmentation of the source sen-
tence) which are not captured by their model. Some
other works have used collocation based segmenta-
tion (Henr´ıquez Q. et al., 2010) and Multiword Ex-
pressions as segments (Bouamor et al., 2012) to im-
prove the performance of SMT but without much
success. The idea of improving performance by re-
ranking a n-best list of outputs has been used re-
cently for the related task of parsing (Katz-Brown et
al., 2011) using targeted self-training for improving
the performance of reordering. However, in contrast,
in our work we directly aim at improving the perfor-
mance of a reordering model.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999940833333333">
In this work, we proposed a model for re-ranking
the n-best reorderings produced by a state of the
art reordering model (TSP model) which is limited
to pair wise features. Our model uses a more in-
formative set of features consisting of higher order
features, structural features and target side features
</bodyText>
<page confidence="0.994172">
322
</page>
<bodyText confidence="0.99997165">
(captured indirectly using translation scores). The
problem of intractability is solved by restricting the
search space to the n-best reorderings produced by
the TSP model. A detailed ablation test shows that
of all the features used, the pos triplet features are
most informative for reordering. A gain of 1.3 and 3
BLEU points over a state of the art phrase based and
hierarchical machine translation system respectively
provides good extrinsic validation of our claim that
such long range features are useful.
As future work, we would like to evaluate our al-
gorithm on other language pairs. We also plan to
integrate the score assigned by our model into the
decoder to avoid having to do n decodings for ev-
ery source sentence. Also, it would be interesting
to model the segmentation explicitly, where the aim
would be to first segment the sentence and then use
a two level hierarchical reordering model which first
reorders these segments and then reorders the words
within the segment.
</bodyText>
<sectionHeader confidence="0.998022" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999381654320987">
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529–536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David Applegate, William Cook, and Andre Rohe. 2003.
Chained lin-kernighan for large traveling salesman
problems. In INFORMS Journal On Computing.
Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In In Proceed-
ings of MT Summit.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2012. Identifying bilingual multi-
word expressions for statistical machine translation.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet Uur Doan, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201–228, June.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531–540,
Morristown, NJ, USA. Association for Computational
Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 193–203, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 961–968, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Pack-
ing, pro, and paraphrases. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 283–291, Montr´eal, Canada, June. Association
for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ’10,
pages 376–384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Carlos Henr´ıquez Q., R. Marta Costa-juss`a, Vidas
Daudaravicius, E. Rafael Banchs, and B. Jos´e Mari˜no.
2010. Using collocation segmentation to augment the
phrase table. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, WMT ’10, pages 98–102, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ’05, pages 89–96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a parser for
machine translation reordering. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 183–192,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
</reference>
<page confidence="0.990901">
323
</page>
<reference confidence="0.999273661764706">
Annual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1–11, Stroudsburg, PA,
USA. Association for Computational Linguistics.
S. Lin and B. W. Kernighan. 1973. An effective heuristic
algorithm for the travelling-salesman problem. Oper-
ations Research, pages 498–516.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 609–616, Stroudsburg,
PA, USA. Association for Computational Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos,
Bing Xiang, and Jian-ming Xu. 2011. A correc-
tion model for word alignments. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 889–898,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 843–853, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97–133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 486–496,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
</reference>
<page confidence="0.998993">
324
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372289">
<title confidence="0.958674">Improving reordering performance using higher order and structural features</title>
<author confidence="0.999777">Mitesh M Khapra Ananthakrishnan Ramanathan Karthik Visweswariah</author>
<affiliation confidence="0.99963">IBM Research India IBM Research India IBM Research</affiliation>
<email confidence="0.972058">mikhapra@in.ibm.comanandr42@gmail.comv-karthik@in.ibm.com</email>
<abstract confidence="0.976357153846154">Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order. This model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the Traveling Salesman Problem (TSP). However, for efficiently solving the TSP, the model is restricted to pairwise features which examine only a pair of words and their neighborhood. In this work, we go beyond these pairwise feaand learn a model to rerank the reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies. In addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering. Our experiments, involving Urdu-English, show that the proposed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL, ACL-44,</booktitle>
<pages>529--536</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1863" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="287" endWordPosition="290"> uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points. 1 Introduction Handling the differences in word orders between pairs of languages is crucial in producing good machine translation. This is especially true for language pairs such as Urdu-English which have significantly different sentence structures. For example, the typical word order in Urdu is Subject Object Verb whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages)</context>
<context position="6527" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="1093" endWordPosition="1096"> UrduEnglish as the source language pair, our approach outperforms the following state of the art systems: (i) a PBSMT system which uses TSP model for reordering (by 1.3 BLEU points), (ii) a hierarchical PBSMT system (by 3 BLEU points). The overall Input Urdu: fir gaadi ka chaalak kuch bola Gloss: then car of driver said something English: Then the driver of the car said something. Ref. reordering: fir chaalak ka gaadi bola kuch Table 1: Example motivating the use of structural features gain is 6.3 BLEU points when compared to a standard PBSMT system which uses a lexicalized distortion model (Al-Onaizan and Papineni, 2006). The rest of this paper is organized as follows. In Section 2 we discuss our approach of re-ranking the n-best reorderings produced by the TSP model. This includes a discussion of the model used, the features used and the algorithm used for learning the parameters of the model. It also includes a discussion on the modification to the Chained Lin-Kernighan heuristic to produce n-best reorderings. Next, in Section 3 we describe our experimental setup and report the results of our experiments. In Section 4 we present some discussions based on our study. In section 5 we briefly describe some prio</context>
<context position="22870" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="3868" endWordPosition="3871">erings we get the best reordering performance (mBLEU) on a development set. Hence, we used n=50 for our MT experiments. For intrinsic evaluation we use a development set of 8017 Urdu tokens reordered manually. Table 3 compares the performance of the top-1 reordering output by our algorithm with the top-1 reordering generated by the improved TSP model in terms of mBLEU. We see a gain of 1.8 mBLEU points with our approach. Next, we see the impact of the better reorderings produced by our system on the performance of a state-of-the-art MT system. For this, we used a standard phrase based system (Al-Onaizan and Papineni, 2006) with a lexicalized distortion model with a window size of +/-4 words (Tillmann and Ney, 2003). As mentioned earlier, our training data consisted of 3.9M words including the NIST MT-08 training data. We use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al., 2011). The Gigaword English corpus was used for building the English language model. We report results on the NIST MT-08 evaluation set, averaging BLEU scores from the News and Web conditions to provide a single BLEU score. Table 4 compares the MT performance obtained by reordering the training a</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of ACL, ACL-44, pages 529–536, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Applegate</author>
<author>William Cook</author>
<author>Andre Rohe</author>
</authors>
<title>Chained lin-kernighan for large traveling salesman problems.</title>
<date>2003</date>
<journal>In INFORMS Journal On Computing.</journal>
<contexts>
<context position="11426" citStr="Applegate et al., 2003" startWordPosition="1910" endWordPosition="1913"> for the TSP model The first stage of our approach is to train a TSP model and generate n-best reorderings using it. The decoder used by Visweswariah et al. (2011) relies on the Chained Lin-Kernighan heuristic (Lin and Kernighan, 1973) to produce the 1-best permutation for the TSP problem. Since our algorithm aims at re-ranking an n-best list of permutations (reorderings), we made a modification to the Chained LinKernighan heuristic to produce this n-best list as shown in Algorithm 1 . Algorithm 1 getNBestReorderings(sentence) In Algorithm 1 perturb() is a four-edge perturbation described in (Applegate et al., 2003), and linkernighan() is the Lin-Kernighan heuristic that applies a sequence of flips that potentially returns a lower cost permutation as described in (Lin and Kernighan, 1973). The cost C(7r) is calculated using a trained TSP model. 2.3 Features We represent each of the n-best reorderings obtained above as a vector of features which can be divided into two sets : (i) higher order features and (ii) strucNbestSet = 0 7r* = Identity permutation 7r* = linkernighan(7r*) insert(NbestSet, 7r*) for i = 1 → nIter do 0 7r 0 7r= linkernighan(7r0) if C(7r0) &lt; maxwENbestSetC(7r) then InsertOrReplace(Nbest</context>
</contexts>
<marker>Applegate, Cook, Rohe, 2003</marker>
<rawString>David Applegate, William Cook, and Andre Rohe. 2003. Chained lin-kernighan for large traveling salesman problems. In INFORMS Journal On Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Philipp Koehn</author>
</authors>
<title>Online learning methods for discriminative training of phrase based statistical machine translation. In</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="17757" citStr="Arun and Koehn, 2007" startWordPosition="3004" endWordPosition="3007">current parameter vector. Since the averaged perceptron method is known to perform better than the perceptron method, we used the averaged values of the parameters at the end of N iterations, calculated as: N 0avg = 1 k N · t i=1 where, N = Number of iterations t = Number of training instances We observed that in most cases the reference reordering in not a part of the n-best list produced by the TSP model. In such cases instead of using Ok(7rgold j ) for updating the weights in Equation 3 we use Ok(7rclosest to gold) as this is known to be a better j strategy for learning a re-ranking model (Arun and Koehn, 2007). 7rclosest to gold is given by: j # of common bigram pairs in 7rij and 7rgold j len(7rgold j ) where, IIj = set of n-best reorderings for jth sentence � 7r closest to gold is thus the reordering which has the maximum overlap with 7rgold j in terms of the number of word pairs (wm, wn) where wn is put next to wm. 2.5 Interpolating with MT score The approach described above aims at producing a better reordering by extracting richer features from the source sentence. Since the final aim is to improve the performance of an MT system, it would potentially be beneficial to interpolate the scores ass</context>
</contexts>
<marker>Arun, Koehn, 2007</marker>
<rawString>Abhishek Arun and Philipp Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dhouha Bouamor</author>
<author>Nasredine Semmar</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Identifying bilingual multiword expressions for statistical machine translation.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="30449" citStr="Bouamor et al., 2012" startWordPosition="5134" endWordPosition="5137">t al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) and Multiword Expressions as segments (Bouamor et al., 2012) to improve the performance of SMT but without much success. The idea of improving performance by reranking a n-best list of outputs has been used recently for the related task of parsing (Katz-Brown et al., 2011) using targeted self-training for improving the performance of reordering. However, in contrast, in our work we directly aim at improving the performance of a reordering model. 6 Conclusion In this work, we proposed a model for re-ranking the n-best reorderings produced by a state of the art reordering model (TSP model) which is limited to pair wise features. Our model uses a more inf</context>
</contexts>
<marker>Bouamor, Semmar, Zweigenbaum, 2012</marker>
<rawString>Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2012. Identifying bilingual multiword expressions for statistical machine translation. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="23712" citStr="Chiang, 2007" startWordPosition="4007" endWordPosition="4008">ith higher quality alignments from a supervised aligner (McCarley et al., 2011). The Gigaword English corpus was used for building the English language model. We report results on the NIST MT-08 evaluation set, averaging BLEU scores from the News and Web conditions to provide a single BLEU score. Table 4 compares the MT performance obtained by reordering the training and test data using the following approaches: 1. No pre-ordering: A baseline system which does not use any source side reordering as a preprocessing step 2. HIERO : A state of the art hierarchical phrase based translation system (Chiang, 2007) 3. TSP: A system which uses the 1-best reordering produced by the TSP model 4. Higher order &amp; structural features: A system 320 Approach mBLEU Unreordered 31.2 TSP 56.6 Higher order &amp; structural features 58.4 Table 3: mBLEU scores for Urdu to English reordering using different models. Approach BLEU No pre-ordering 21.9 HIERO 25.2 TSP 26.9 Higher order &amp; structural features 27.5 Interpolating with MT score 28.2 Table 4: MT performance for Urdu to English without reordering and with reordering using different approaches. which reranks n-best reorderings produced by TSP using higher order and st</context>
<context position="29415" citStr="Chiang, 2007" startWordPosition="4957" endWordPosition="4959">y classified into three types. First, approaches which reorder source sentences by applying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves u</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL, pages 531–540,</rawString>
</citation>
<citation valid="false">
<authors>
<author>NJ Morristown</author>
</authors>
<institution>USA. Association for Computational Linguistics.</institution>
<marker>Morristown, </marker>
<rawString>Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing sentence structure from parallel corpora for reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>193--203</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29818" citStr="DeNero and Uszkoreit, 2011" startWordPosition="5022" endWordPosition="5025">anguages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) and Multiword Expressions as </context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reordering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 193–203, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29485" citStr="Galley et al., 2006" startWordPosition="4968" endWordPosition="4971">ource sentences by applying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related t</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 961–968, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Yuan Cao</author>
<author>Jonathan Weese</author>
<author>Matt Post</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Joshua 4.0: Packing, pro, and paraphrases.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>283--291</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="24524" citStr="Ganitkevitch et al., 2012" startWordPosition="4137" endWordPosition="4140">tructural features 58.4 Table 3: mBLEU scores for Urdu to English reordering using different models. Approach BLEU No pre-ordering 21.9 HIERO 25.2 TSP 26.9 Higher order &amp; structural features 27.5 Interpolating with MT score 28.2 Table 4: MT performance for Urdu to English without reordering and with reordering using different approaches. which reranks n-best reorderings produced by TSP using higher order and structural features 5. Interpolating with MT score : A system which interpolates the score assigned to a reordering by our model with the score assigned by a MT system We used Joshua 4.0 (Ganitkevitch et al., 2012) which provides an open source implementation of HIERO. For training, tuning and testing HIERO we used the same experimental setup as described above. As seen in Table 4, we get an overall gain of 6.2 BLEU points with our approach as compared to a baseline system which does not use any reordering. More importantly, we outperform (i) a PBSMT system which uses the TSP model by 1.3 BLEU points and (ii) a state of the art hierarchical phrase based translation system by 3 points. 4 Discussions We now discuss some error corrections and ablation tests. 4.1 Example of error correction We first give an</context>
</contexts>
<marker>Ganitkevitch, Cao, Weese, Post, Callison-Burch, 2012</marker>
<rawString>Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and Chris Callison-Burch. 2012. Joshua 4.0: Packing, pro, and paraphrases. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 283–291, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning sourceside reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>376--384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29083" citStr="Genzel, 2010" startWordPosition="4905" endWordPosition="4906">let jumps feature we also tried a pos quadruplet jumps feature but it did not help (mainly due to overfitting and sparsity). 5 Related Work There are several studies which have shown that reordering the source side sentence to match the target side order leads to improvements in Machine Translation. These approaches can be broadly classified into three types. First, approaches which reorder source sentences by applying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference r</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning sourceside reordering rules for large scale machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 376–384, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlos Henr´ıquez Q</author>
<author>R Marta Costa-juss`a</author>
<author>Vidas Daudaravicius</author>
<author>E Rafael Banchs</author>
<author>B Jos´e Mari˜no</author>
</authors>
<title>Using collocation segmentation to augment the phrase table.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>98--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Q, Costa-juss`a, Daudaravicius, Banchs, Mari˜no, 2010</marker>
<rawString>A. Carlos Henr´ıquez Q., R. Marta Costa-juss`a, Vidas Daudaravicius, E. Rafael Banchs, and B. Jos´e Mari˜no. 2010. Using collocation segmentation to augment the phrase table. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 98–102, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for Arabic-English machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP, HLT ’05,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="20837" citStr="Ittycheriah and Roukos, 2005" startWordPosition="3526" endWordPosition="3529">ioned earlier, our training process involves two phases : (i) Generating n-best reorderings for the training data and (ii) using these n-best reorderings to train a perceptron model. We use the same data for training the reordering model as well as our perceptron model. This data contains 180K words of manual alignments (part of the NIST MT08 training data) and 3.9M words of automatically generated machine alignments (1.7M words from the NIST MT-08 training data1 and 2.2M words extracted from sources on the web2). The machine alignments were generated using a supervised maximum entropy model (Ittycheriah and Roukos, 2005) and then corrected using an improved correction model (McCarley et al., 2011). We first divide the training data into 10 folds. The n-best reorderings for each fold are then generated using a model trained on the remaining 9 folds. This division into 10 folds is done for reasons explained earlier in Section 2.1. These n-best reorderings are then used to train the perceptron model as described in Section 2.4. Note that Visweswariah et al. (2011) used only manually aligned data for training the TSP model. However, we use machine aligned data in addition to manually aligned data for training the</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for Arabic-English machine translation. In Proceedings of HLT/EMNLP, HLT ’05, pages 89–96, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Katz-Brown</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Franz Och</author>
<author>David Talbot</author>
<author>Hiroshi Ichikawa</author>
<author>Masakazu Seno</author>
<author>Hideto Kazawa</author>
</authors>
<title>Training a parser for machine translation reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>183--192</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="30662" citStr="Katz-Brown et al., 2011" startWordPosition="5173" endWordPosition="5176">wariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) and Multiword Expressions as segments (Bouamor et al., 2012) to improve the performance of SMT but without much success. The idea of improving performance by reranking a n-best list of outputs has been used recently for the related task of parsing (Katz-Brown et al., 2011) using targeted self-training for improving the performance of reordering. However, in contrast, in our work we directly aim at improving the performance of a reordering model. 6 Conclusion In this work, we proposed a model for re-ranking the n-best reorderings produced by a state of the art reordering model (TSP model) which is limited to pair wise features. Our model uses a more informative set of features consisting of higher order features, structural features and target side features 322 (captured indirectly using translation scores). The problem of intractability is solved by restricting</context>
</contexts>
<marker>Katz-Brown, Petrov, McDonald, Och, Talbot, Ichikawa, Seno, Kazawa, 2011</marker>
<rawString>Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno, and Hideto Kazawa. 2011. Training a parser for machine translation reordering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 183–192, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1793" citStr="Koehn et al., 2003" startWordPosition="277" endWordPosition="280">ed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points. 1 Introduction Handling the differences in word orders between pairs of languages is crucial in producing good machine translation. This is especially true for language pairs such as Urdu-English which have significantly different sentence structures. For example, the typical word order in Urdu is Subject Object Verb whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work on</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13314" citStr="Koo and Collins, 2010" startWordPosition="2223" endWordPosition="2226">s triplet = “NN Vaux VRB” &amp;&amp; jumps = [4, −1] Table 2: Features used in our model. tural features. The higher order features are essentially trigram lexical and pos features whereas the structural features are derived from the sentence structure induced by a reordering (explained later). 2.3.1 Higher Order Features Since deriving a good reordering would essentially require analyzing the syntactic structure of the source sentence, the tasks of reordering and parsing are often considered to be related. The main motivation for using higher order features thus comes from a related work on parsing (Koo and Collins, 2010) where the performance of a state of the art parser was improved by considering higher order dependencies. In our model we use trigram features (see Table 2) of the following form: 0(rui, rui+1, rui+2, J(rui, rui+1), J(rui+1, rui+2)) where rui =word at position i in the reordered source sentence and J(x, y) = difference between the positions of x and y in the original source sentence. Figure 1 shows an example of jumps between different word pairs in an Urdu sentence. Since such higher order features will typically be sparse, we also use some back-off features. For example, instead of using th</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1–11, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lin</author>
<author>B W Kernighan</author>
</authors>
<title>An effective heuristic algorithm for the travelling-salesman problem.</title>
<date>1973</date>
<journal>Operations Research,</journal>
<pages>498--516</pages>
<contexts>
<context position="11038" citStr="Lin and Kernighan, 1973" startWordPosition="1849" endWordPosition="1852">the averaged perceptron method. At test time, getNBestReorderings(sentence) is used to generate the n-best reorderings for the test sentence using the trained TSP model. These reorderings are then represented using higher order and structural features and re-ranked using the weights learned earlier. We now describe the different stages of our algorithm. 2.2 Generating n-best reorderings for the TSP model The first stage of our approach is to train a TSP model and generate n-best reorderings using it. The decoder used by Visweswariah et al. (2011) relies on the Chained Lin-Kernighan heuristic (Lin and Kernighan, 1973) to produce the 1-best permutation for the TSP problem. Since our algorithm aims at re-ranking an n-best list of permutations (reorderings), we made a modification to the Chained LinKernighan heuristic to produce this n-best list as shown in Algorithm 1 . Algorithm 1 getNBestReorderings(sentence) In Algorithm 1 perturb() is a four-edge perturbation described in (Applegate et al., 2003), and linkernighan() is the Lin-Kernighan heuristic that applies a sequence of flips that potentially returns a lower cost permutation as described in (Lin and Kernighan, 1973). The cost C(7r) is calculated using</context>
</contexts>
<marker>Lin, Kernighan, 1973</marker>
<rawString>S. Lin and B. W. Kernighan. 1973. An effective heuristic algorithm for the travelling-salesman problem. Operations Research, pages 498–516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>609--616</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29503" citStr="Liu et al., 2006" startWordPosition="4972" endWordPosition="4975">plying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tro</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 609–616, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Scott McCarley</author>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
<author>Bing Xiang</author>
<author>Jian-ming Xu</author>
</authors>
<title>A correction model for word alignments.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>889--898</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="20915" citStr="McCarley et al., 2011" startWordPosition="3538" endWordPosition="3541">erings for the training data and (ii) using these n-best reorderings to train a perceptron model. We use the same data for training the reordering model as well as our perceptron model. This data contains 180K words of manual alignments (part of the NIST MT08 training data) and 3.9M words of automatically generated machine alignments (1.7M words from the NIST MT-08 training data1 and 2.2M words extracted from sources on the web2). The machine alignments were generated using a supervised maximum entropy model (Ittycheriah and Roukos, 2005) and then corrected using an improved correction model (McCarley et al., 2011). We first divide the training data into 10 folds. The n-best reorderings for each fold are then generated using a model trained on the remaining 9 folds. This division into 10 folds is done for reasons explained earlier in Section 2.1. These n-best reorderings are then used to train the perceptron model as described in Section 2.4. Note that Visweswariah et al. (2011) used only manually aligned data for training the TSP model. However, we use machine aligned data in addition to manually aligned data for training the TSP model as it leads to better performance. We used this improvised TSP mode</context>
<context position="23178" citStr="McCarley et al., 2011" startWordPosition="3917" endWordPosition="3920"> generated by the improved TSP model in terms of mBLEU. We see a gain of 1.8 mBLEU points with our approach. Next, we see the impact of the better reorderings produced by our system on the performance of a state-of-the-art MT system. For this, we used a standard phrase based system (Al-Onaizan and Papineni, 2006) with a lexicalized distortion model with a window size of +/-4 words (Tillmann and Ney, 2003). As mentioned earlier, our training data consisted of 3.9M words including the NIST MT-08 training data. We use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al., 2011). The Gigaword English corpus was used for building the English language model. We report results on the NIST MT-08 evaluation set, averaging BLEU scores from the News and Web conditions to provide a single BLEU score. Table 4 compares the MT performance obtained by reordering the training and test data using the following approaches: 1. No pre-ordering: A baseline system which does not use any source side reordering as a preprocessing step 2. HIERO : A state of the art hierarchical phrase based translation system (Chiang, 2007) 3. TSP: A system which uses the 1-best reordering produced by the</context>
</contexts>
<marker>McCarley, Ittycheriah, Roukos, Xiang, Xu, 2011</marker>
<rawString>J. Scott McCarley, Abraham Ittycheriah, Salim Roukos, Bing Xiang, and Jian-ming Xu. 2011. A correction model for word alignments. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 889–898, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
</authors>
<title>Inducing a discriminative parser to optimize machine translation reordering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>843--853</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="29840" citStr="Neubig et al., 2012" startWordPosition="5026" endWordPosition="5029"> approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) and Multiword Expressions as segments (Bouamor et a</context>
</contexts>
<marker>Neubig, Watanabe, Mori, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, and Shinsuke Mori. 2012. Inducing a discriminative parser to optimize machine translation reordering. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 843–853, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Hansraj Choudhary</author>
<author>Avishek Ghosh</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Case markers and morphology: addressing the crux of the fluency problem in English-Hindi smt.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="2310" citStr="Ramanathan et al., 2009" startWordPosition="362" endWordPosition="365">whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages). Recent work (Visweswariah et al., 2011) has shown that it is possible to learn a reordering model from a relatively small number of hand aligned sentences. This eliminates the need of a source or target parser. In this work, we build upon the work of Visweswariah et al. (2011) which solves the reordering problem by treating it as an instance of the Traveling Salesman Problem (TSP). They learn a model which assigns costs to all pairs of words</context>
<context position="29026" citStr="Ramanathan et al., 2009" startWordPosition="4893" endWordPosition="4896">ures are highly dependent features. Note that similar to the pos triplet jumps feature we also tried a pos quadruplet jumps feature but it did not help (mainly due to overfitting and sparsity). 5 Related Work There are several studies which have shown that reordering the source side sentence to match the target side order leads to improvements in Machine Translation. These approaches can be broadly classified into three types. First, approaches which reorder source sentences by applying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based model</context>
</contexts>
<marker>Ramanathan, Choudhary, Ghosh, Bhattacharyya, 2009</marker>
<rawString>Ananthakrishnan Ramanathan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and morphology: addressing the crux of the fluency problem in English-Hindi smt. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1879" citStr="Tillman, 2004" startWordPosition="291" endWordPosition="292">ing by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points. 1 Introduction Handling the differences in word orders between pairs of languages is crucial in producing good machine translation. This is especially true for language pairs such as Urdu-English which have significantly different sentence structures. For example, the typical word order in Urdu is Subject Object Verb whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages). Recent work (V</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word reordering and a dynamic programming beam search algorithm for statistical machine translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="22964" citStr="Tillmann and Ney, 2003" startWordPosition="3884" endWordPosition="3887">ur MT experiments. For intrinsic evaluation we use a development set of 8017 Urdu tokens reordered manually. Table 3 compares the performance of the top-1 reordering output by our algorithm with the top-1 reordering generated by the improved TSP model in terms of mBLEU. We see a gain of 1.8 mBLEU points with our approach. Next, we see the impact of the better reorderings produced by our system on the performance of a state-of-the-art MT system. For this, we used a standard phrase based system (Al-Onaizan and Papineni, 2006) with a lexicalized distortion model with a window size of +/-4 words (Tillmann and Ney, 2003). As mentioned earlier, our training data consisted of 3.9M words including the NIST MT-08 training data. We use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al., 2011). The Gigaword English corpus was used for building the English language model. We report results on the NIST MT-08 evaluation set, averaging BLEU scores from the News and Web conditions to provide a single BLEU score. Table 4 compares the MT performance obtained by reordering the training and test data using the following approaches: 1. No pre-ordering: A baseline system which does </context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2003. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational Linguistics, 29(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="29763" citStr="Tromble and Eisner, 2009" startWordPosition="5013" endWordPosition="5017"> source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (He</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Jiri Navratil</author>
<author>Jeffrey Sorensen</author>
<author>Vijil Chenthamarakshan</author>
<author>Nandakishore Kambhatla</author>
</authors>
<title>Syntax based reordering with automatically derived rules for improved statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="29111" citStr="Visweswariah et al., 2010" startWordPosition="4907" endWordPosition="4910">ure we also tried a pos quadruplet jumps feature but it did not help (mainly due to overfitting and sparsity). 5 Related Work There are several studies which have shown that reordering the source side sentence to match the target side order leads to improvements in Machine Translation. These approaches can be broadly classified into three types. First, approaches which reorder source sentences by applying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from alig</context>
</contexts>
<marker>Visweswariah, Navratil, Sorensen, Chenthamarakshan, Kambhatla, 2010</marker>
<rawString>Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kambhatla. 2010. Syntax based reordering with automatically derived rules for improved statistical machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Rajakrishnan Rajkumar</author>
<author>Ankur Gandhe</author>
<author>Ananthakrishnan Ramanathan</author>
<author>Jiri Navratil</author>
</authors>
<title>A word reordering model for improved machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>486--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2504" citStr="Visweswariah et al., 2011" startWordPosition="394" endWordPosition="397">) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages). Recent work (Visweswariah et al., 2011) has shown that it is possible to learn a reordering model from a relatively small number of hand aligned sentences. This eliminates the need of a source or target parser. In this work, we build upon the work of Visweswariah et al. (2011) which solves the reordering problem by treating it as an instance of the Traveling Salesman Problem (TSP). They learn a model which assigns costs to all pairs of words in a sentence, where the cost represents the penalty of putting a word immediately preceding another word. The best permutation is found via the chained LinKernighan heuristic for solving a TSP</context>
<context position="7371" citStr="Visweswariah et al., 2011" startWordPosition="1231" endWordPosition="1234"> the algorithm used for learning the parameters of the model. It also includes a discussion on the modification to the Chained Lin-Kernighan heuristic to produce n-best reorderings. Next, in Section 3 we describe our experimental setup and report the results of our experiments. In Section 4 we present some discussions based on our study. In section 5 we briefly describe some prior related work. Finally, in Section 6, we present some concluding remarks and highlight possible directions for future work. 2 Re-ranking using higher order and structural features As mentioned earlier, the TSP model (Visweswariah et al., 2011) looks only at local features for a word pair (wi, wj). We believe that for better reordering it is essential to look at higher order and structural features (i.e., features which look at the overall structure of a sentence). The primary reason why Visweswariah et al. (2011) consider only pairwise bigram features is that with higher order features the reordering problem can no longer be cast as a TSP and hence cannot be solved using existing efficient heuristic solvers. However, we do not have to deal with an NP-Hard search problem because instead of considering all possible reorderings we res</context>
<context position="8782" citStr="Visweswariah et al., 2011" startWordPosition="1473" endWordPosition="1476">re interesting in assigning a score, score(7r), to each of these reorderings and pick the reordering which has the highest score. In this paper, we parametrize this score as: score(7r) = OT 0(7r) (1) 316 where, 0 is the weight vector and 0(7r) is a vector of features extracted from the reordering 7r. The aim then is to find, 7r* = arg max score(7r) (2) 7rEil In the following sub-sections, we first briefly describe our overall approach towards finding 7r*. Next, we describe our modification to the LinKernighan heuristic for producing n-best outputs for TSP instead of the 1-best output used by (Visweswariah et al., 2011). We then discuss the features used for re-ranking these n-best outputs, followed by a discussion on the learning algorithm used for estimating the parameters of the model. Finally, we describe how we interpolate the score assigned by our model with the score assigned by the decoder of a SMT engine to indirectly capture target side features. 2.1 Overall approach The training stage of our approach involves two phases : (i) Training a TSP model which will be used to generate n-best reorderings and (ii) Training a re-ranking model using these n-best reorderings. For training both the models we ne</context>
<context position="10966" citStr="Visweswariah et al. (2011)" startWordPosition="1839" endWordPosition="1842">ructural features. The weights of these features are then estimated using the averaged perceptron method. At test time, getNBestReorderings(sentence) is used to generate the n-best reorderings for the test sentence using the trained TSP model. These reorderings are then represented using higher order and structural features and re-ranked using the weights learned earlier. We now describe the different stages of our algorithm. 2.2 Generating n-best reorderings for the TSP model The first stage of our approach is to train a TSP model and generate n-best reorderings using it. The decoder used by Visweswariah et al. (2011) relies on the Chained Lin-Kernighan heuristic (Lin and Kernighan, 1973) to produce the 1-best permutation for the TSP problem. Since our algorithm aims at re-ranking an n-best list of permutations (reorderings), we made a modification to the Chained LinKernighan heuristic to produce this n-best list as shown in Algorithm 1 . Algorithm 1 getNBestReorderings(sentence) In Algorithm 1 perturb() is a four-edge perturbation described in (Applegate et al., 2003), and linkernighan() is the Lin-Kernighan heuristic that applies a sequence of flips that potentially returns a lower cost permutation as de</context>
<context position="20093" citStr="Visweswariah et al., 2011" startWordPosition="3406" endWordPosition="3409">lation should (and can) be done at decode time without having to decode n reorderings for every source 0(i,j−1)T O(7r) 0(i,j) (4) k t L, j=1 arg max π�j∈IIj 319 sentence (for example by expressing the n reorderings as a lattice), but, we leave this as future work. 3 Empirical evaluation We evaluated our reordering approach on UrduEnglish. We use two types of evaluation, one intrinsic and one extrinsic. For intrinsic evaluation, we compare the reordered source sentence in Urdu with a reference reordering obtained from the hand alignments using BLEU (referred to as monolingual BLEU or mBLEU by (Visweswariah et al., 2011) ). Additionally, we evaluate the effect of reordering on MT performance using BLEU (extrinsic evaluation). As mentioned earlier, our training process involves two phases : (i) Generating n-best reorderings for the training data and (ii) using these n-best reorderings to train a perceptron model. We use the same data for training the reordering model as well as our perceptron model. This data contains 180K words of manual alignments (part of the NIST MT08 training data) and 3.9M words of automatically generated machine alignments (1.7M words from the NIST MT-08 training data1 and 2.2M words ex</context>
<context position="29790" citStr="Visweswariah et al., 2011" startWordPosition="5018" endWordPosition="5021">is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) a</context>
</contexts>
<marker>Visweswariah, Rajkumar, Gandhe, Ramanathan, Navratil, 2011</marker>
<rawString>Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A word reordering model for improved machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 486–496, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2284" citStr="Wang et al., 2007" startWordPosition="358" endWordPosition="361">ubject Object Verb whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages). Recent work (Visweswariah et al., 2011) has shown that it is possible to learn a reordering model from a relatively small number of hand aligned sentences. This eliminates the need of a source or target parser. In this work, we build upon the work of Visweswariah et al. (2011) which solves the reordering problem by treating it as an instance of the Traveling Salesman Problem (TSP). They learn a model which assigns c</context>
<context position="29000" citStr="Wang et al., 2007" startWordPosition="4889" endWordPosition="4892">these pairs of features are highly dependent features. Note that similar to the pos triplet jumps feature we also tried a pos quadruplet jumps feature but it did not help (mainly due to overfitting and sparsity). 5 Related Work There are several studies which have shown that reordering the source side sentence to match the target side order leads to improvements in Machine Translation. These approaches can be broadly classified into three types. First, approaches which reorder source sentences by applying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as requ</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="29069" citStr="Xia and McCord, 2004" startWordPosition="4901" endWordPosition="4904">imilar to the pos triplet jumps feature we also tried a pos quadruplet jumps feature but it did not help (mainly due to overfitting and sparsity). 5 Related Work There are several studies which have shown that reordering the source side sentence to match the target side order leads to improvements in Machine Translation. These approaches can be broadly classified into three types. First, approaches which reorder source sentences by applying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model usi</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A decoder for syntax-based statistical mt.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="29464" citStr="Yamada and Knight, 2002" startWordPosition="4964" endWordPosition="4967">pproaches which reorder source sentences by applying rules to the source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical mt. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="29534" citStr="Zollmann and Venugopal, 2006" startWordPosition="4976" endWordPosition="4979">e source side parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but perf</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>