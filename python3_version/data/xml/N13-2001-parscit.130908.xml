<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.985187">
Critical Reflections on Evaluation Practices in Coreference Resolution
</title>
<author confidence="0.98843">
Gordana Ili´c Holen
</author>
<affiliation confidence="0.9981525">
Department of Informatics
University of Oslo
</affiliation>
<address confidence="0.644124">
Norway
</address>
<email confidence="0.994274">
gordanil@ifi.uio.no
</email>
<sectionHeader confidence="0.998636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995830176470588">
In this paper we revisit the task of quantitative
evaluation of coreference resolution systems.
We review the most commonly used metrics
(MUC, B3, CEAF and BLANC) on the basis of
their evaluation of coreference resolution in
five texts from the OntoNotes corpus. We ex-
amine both the correlation between the met-
rics and the degree to which our human judge-
ment of coreference resolution agrees with the
metrics. In conclusion we claim that loss of
information value is an essential factor, insuf-
ficiently adressed in current metrics, in human
perception of the degree of success or failure
of coreference resolution. We thus conjec-
ture that including a layer of mention infor-
mation weight could improve both the coref-
erence resolution and its evaluation.
</bodyText>
<sectionHeader confidence="0.995501" genericHeader="categories and subject descriptors">
1 Introduction and motivation
</sectionHeader>
<bodyText confidence="0.999869428571429">
Coreference resolution (CR) is the task of link-
ing together multiple expressions of a given entity
(Yang et al., 2003). The field has experienced a
surge of interest with several shared tasks in re-
cent years: SemEval 2010 (Recasens et al., 2010),
CoNLL 2011 (Pradhan et al., 2011) and CoNLL
2012 (Pradhan et al., 2012). However the field has
from the very start been riddled with problems re-
lated to the scoring and comparison of CR systems.
Currently there are five metrics in wider use: MUC
(Vilain et al., 1995), B3 (Bagga and Baldwin, 1998),
the two CEAF metrics (Luo, 2005) and BLANC (Re-
casens and Hovy, 2011). As there is no global agree-
ment on which metrics are the most appropriate, the
</bodyText>
<page confidence="0.845732">
1
</page>
<bodyText confidence="0.99977975">
above-mentioned shared tasks have used a combi-
nation of several metrics to evaluate the contenders.
Although coreference resolution is a subproblem of
natural language understanding, coreference resolu-
tion evaluation metrics have predominately been dis-
cussed in terms of abstract entities and hypothetical
system errors. In our view, it is of utmost importance
to observe actual texts and actual system errors.
</bodyText>
<sectionHeader confidence="0.988174" genericHeader="method">
2 Background: The metrics
</sectionHeader>
<bodyText confidence="0.99996975">
In this section, we will present the five metrics in the
usual terms of precision, recall and F-score. We fol-
low the predominant practice and use the term men-
tion for individual referring expressions, and entity
for sets of mentions that refer to the same object
(Luo et al., 2004). We use the term key entity (K)
for gold entities, and response entity (R) for entities
which were produced by the CR system.
</bodyText>
<subsectionHeader confidence="0.986738">
2.1 Link-based: MUC and BLANC
</subsectionHeader>
<bodyText confidence="0.9999664">
The MUC metric (Vilain et al., 1995) is based on
comparing the number of links in the key entity
(|K |− 1) to the number of links missing from the
response entity, routinely calculated as the number
of partitions of the key entity |p(K) |minus one, so
</bodyText>
<equation confidence="0.993356">
Recall = (|K|−1)−(|P(K)|−1) = |K|−|P(K) |For
|K|−1 |K|−1
</equation>
<bodyText confidence="0.795377">
the whole document, recalls for entities are simply
</bodyText>
<equation confidence="0.905875333333333">
� |Ki|−|P(Ki)|
added: Recall = In calculating pre-
E(|Ki|−1)
</equation>
<bodyText confidence="0.977195">
cision, the case is inverted: The base entity is now
the response, and the question posed is how many
missing links have to be added to the key partitions
to form the response entity.
BLANC (Recasens and Hovy, 2011) is a variant of
the Rand index (Rand, 1971) adapted for the task
</bodyText>
<note confidence="0.860597">
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 1–7,
Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.971108266666667">
of coreference resolution. The BLANC metric makes
use of both coreferent and non-coreferent links, cor-
rect and incorrect. The final precision, recall and
F-score are the average of the P, R and F-score of
corresponding coreferential and non-referential val-
ues. However, since this is an analysis of isolated
entities, there are no non-coreferential links. For
that reason, in this paper we only present corefer-
ential precision, recall and F-score for this metric:
rc rc 2PcRc
Pc = rc+wc , Rc = rc+wn and Fc = Pc+Rc , where
rc is the number of correct coreferential links, wc
the number of incorrect coreferential links, and wn
is the number of non-coreferential links incorrectly
marked as coreferent by the system.
</bodyText>
<subsectionHeader confidence="0.909577">
2.2 Entity and mention-based: B3 and CEAF
</subsectionHeader>
<bodyText confidence="0.96483384">
B3 (Bagga and Baldwin, 1998) calculates precision
and recall for every mention in the document, and
then combines them to an overall precision and re-
call. Precision of a single mention mi is the number
of correct mentions in the response entity Ri that
contains mi divided by the total number of mentions
in Ri. Recall of mi is again the number of correct
mentions in Ri, this time divided by the number of
mentions in the key entity Ki that contains mention
mi. The precision and recall for the entire docu-
ment can be calculated as weighted sums of preci-
sion and recall of the individual mentions. The de-
fault weight, also used in this experiment is 1n, where
n is the number of mentions in the document.
CEAF (Luo, 2005) is based on the best alignment
of subsets of key and response entities. For any
mapping g E Gm the total similarity Φ(g) is the
sum of all similarities. The best alignment g* is
found by maximizing the sum of similarities Φ(g)
between the key and response entities, while the
maximum total similarity is the sum of the best sim-
ilarities. Precision and recall are defined in terms
of the similarity measure Φ(g*): PI DW)
DW )
Φ(g∗)
</bodyText>
<equation confidence="0.988219">
R = E.i φ(Ki,Ki).
</equation>
<bodyText confidence="0.937503571428572">
There are two versions of CEAF with different
similarity measures, 0m(K, R) = |K n R |and
0e(K, R) = 2JKnRJ
JKJ+JRJ. 0e is the basis for CEAFe
which shows a measure of correct entities while
CEAFm, based on 0m, shows the percentage of cor-
rectly resolved mentions.
</bodyText>
<table confidence="0.999695571428571">
MUC B3 CEAFe CEAFm BLANC MELA
MUC – 0.46 0.22 0.47 0.35 0.63
B3 0.59 – 0.47 0.56 0.42 0.61
CEAFe 0.46 0.59 – 0.51 0.26 0.38
CEAFm 0.57 0.70 0.62 – 0.46 0.60
BLANC 0.57 0.70 0.57 0.68 – 0.35
MELA 0.59 0.73 0.59 0.70 0.70 –
</table>
<tableCaption confidence="0.996521">
Table 1: Kendall T rank correlation coefficient for teams
participating in CoNLL shared tasks, with CoNLL 2011
in the upper right, CoNLL 2012 in the lower left corner.
</tableCaption>
<sectionHeader confidence="0.967822" genericHeader="method">
3 Correlating CoNLL shared tasks results
</sectionHeader>
<bodyText confidence="0.999875935483871">
To illustrate the complexity of the present evaluation
best practices, we have applied the Kendall T rank
correlation coefficient to the ratings the metrics gave
coreference resolution systems that competed in the
two recent CoNLL shared tasks. The official metrics
of the CoNLL shared tasks was MELA (Denis and
Baldridge, 2009), a weighted average of MUC, B3
and CEAFe.
The results for CoNLL 2011 (Table 1) show
a rather weak correlation among the metrics go-
ing down to as low as 0.22 between CEAFe and
MUC. Somewhat surprisingly, the two link-based
metrics, MUC and BLANC, also show a low degree of
agreement (0.35), while the mention-based metrics,
CEAFm and B3, show the highest agreement of all
non-composite metrics. However, this agreement is
not particularly high either as the two metrics agree
on just above the half of all the cases (0.56).
The results for CoNLL 2012 show much higher
correlation among the metrics ranging from 0.46 to
0.70. Again CEAFm and B3 show the highest corre-
lation, but unlike in 2011 BLANC “joins” this clus-
ter. CEAFe and MUC are again least correlated, while
CEAFe and BLANC, in 2011 almost independent,
show average correlation (0.57) in 2012.
In our view, comparatively low correlations as
well as surprising variation from year to year sug-
gests a certain degree of ’fuzziness’ in quantitative
coreference resolution evaluation. We leave the in-
vestigation of variation between the two years for
future work.
</bodyText>
<sectionHeader confidence="0.995794" genericHeader="method">
4 Error analysis
</sectionHeader>
<bodyText confidence="0.998306333333333">
To better understand the functioning of the met-
rics we have conducted an error analysis on the
key/response entity pairs from five short texts from
</bodyText>
<page confidence="0.962603">
2
</page>
<bodyText confidence="0.99992075">
the development corpus of the CoNLL 2011 Shared
Task (Pradhan et al., 2011), one text from each of
the five represented genres: Broadcast Conversa-
tions (BC), Broadcast News (BN), Magazine (MZ),
News Wire (NW) and Web Blogs and News Groups
(WB). The texts were chosen as randomly as pos-
sible, the only constraint being length1. The gold
standard texts are originally from OntoNotes 4.0,
and contain 64 mentions distributed among 21 key
entities. The response texts are the output of Stan-
ford’s Multi-Pass Sieve Coreference Resolution Sys-
tem (Lee et al., 2011).
</bodyText>
<subsectionHeader confidence="0.993879">
4.1 Categorization
</subsectionHeader>
<bodyText confidence="0.999816428571428">
Instead of classifying entities according to their
score by some of the metrics, or a combination of
several of them, as done by the CoNLL shared tasks,
we have based the classification on a notion of lin-
guistic common sense – our subjective idea of how
humans evaluate the success or failure of CR. We
divide key/response entity pairs into four categories:
</bodyText>
<listItem confidence="0.99995425">
• Category 1: Perfect match
• Category 2: Partial match
• Category 3: Merged entities
• Category 4: Failed coreference resolution
</listItem>
<bodyText confidence="0.9999705">
We will concentrate on the amount of informational
value from the key entity that has been preserved in
the response entity. In the course of these experi-
ments, our aim is to see if that rather informal idea
can be operationalized in a way amenable to future
use in automated CR and/or quantitative evaluation.
</bodyText>
<subsectionHeader confidence="0.888877">
4.1.1 Category 1: Perfect match
</subsectionHeader>
<bodyText confidence="0.840901583333333">
This class consists of four key/response entity
pairs with complete string match. The key and
response entities being identical, all metrics show
unanimously precision and recall of 100%. The
informational value is, of course, completely pre-
served. Unfortunately, those examples are few and
simple: They constitute only 19% of the entities and
14% of mentions in this sample, and all seem to be
achieved by the simplest form of string matching.
Key Response MUC B3 CEAFQ CEAF_ BLANC
entities entities
BC45
</bodyText>
<listItem confidence="0.990618833333333">
• The KMT P 100.00 100.00 90.90 100.00 100.00
vice chairman R 80.00 83.33 90.90 83.33 66.67
• Wang • Wang F 88.89 90.91 90.90 90.91 80.00
Jin-pyng Jin-pyng
• his • his
• his • his
• He • He
• he • he
BC22
• KMT • KMT P 100.00 100.00 80.00 100.00 100.00
Chairman Chairman R 50.00 66.67 80.00 66.67 33.33
Lien Chan Lien Chan F 66.67 80.00 80.00 80.00 50.00
• Chairman
Lien Chan
• Lien Chan • Lien Chan
BN1
• Bill • Bill P 100.00 100.00 76.92 100.00 68.42
Clinton Clinton R 85.71 62.50 76.92 62.50 46.43
• The President F 92.31 76.92 76.92 76.92 55.32
• he
• his
• Mr.Clinton • Mr.Clinton
• his • his
• He • He
• he • he
NW2
• New • New P 100.00 100.00 88.89 100.00 100.00
Zealand Zealand R 75.00 80.00 88.89 80.00 60.00
• New • New F 85.71 88.89 88.89 88.89 75.00
Zealand Zealand
• New • New
Zealand Zealand
• New • New
Zealand’s Zealand’s
• New
• Zealand
</listItem>
<tableCaption confidence="0.993924">
Table 2: Category 2a: Partial match (partial entities)
</tableCaption>
<subsubsectionHeader confidence="0.303381">
4.1.2 Category 2: Partial match
</subsubsectionHeader>
<bodyText confidence="0.999885578947369">
The partial response entities can be divided in two
subcategories: 2a) The cases where the response en-
tities are partial, i.e.they form a proper subset of the
key entity mentions (Table 2) and 2b) The cases
where the response mentions are partial, i.e. sub-
strings of the corresponding key mentions (Table 3).
The scoring of the examples has followed CoNLL
shared tasks’ strict mention detection requirements2
with the consequence that Category 2b entities have
received considerably lower scores than the Cate-
gory 2a entities even in cases where the loss of
informational value has been comparable. For in-
stance, the response entity NW1 (Table 3) has re-
ceived an average F-score of 56.67%, but its loss
of informational value is comparable to that in enti-
ties BC45 and BN1 (Table 2). The BC45’s response
entity has lost the information that Jiyun Tian
is a vice-chief, while entities BC45 and BN1
have lost the information that the person referred to
</bodyText>
<footnote confidence="0.99834475">
1The texts longer than five sentences were discarded, to
make the analysis tractable.
2Only response mentions with boundaries identical to the
gold mentions are recognized as correct (Pradhan et al., 2011)
</footnote>
<page confidence="0.99659">
3
</page>
<bodyText confidence="0.992262833333333">
is The KMT vice chairman (BC45) and The
President (BN1). However, the latter mentions
have received a considerably higher average F-score
of 88.32% and 75.68% respectively. This indicates
that stricter mention detection requirements do not
necessarily improve the quality of CR evaluation.
</bodyText>
<table confidence="0.998890230769231">
Key Response MUC B3 CEAFQ CEAF_ BLANC
entities entities
MZ22
• a school in • a school P 0.00 50.00 50.00 50.00 0.00
Shenzhen for in Shenzhen R 0.00 50.00 50.00 50.00 0.00
the children of F 0.00 50.00 50.00 50.00 0.00
Hong Kong
expats
• the school • the school
in Shenzhen in Shenzhen
NW0
• China’s • People’s P 0.00 0.00 0.00 0.00 0.00
People’s Congress R 0.00 0.00 0.00 0.00 0.00
Congress F 0.00 0.00 0.00 0.00 0.00
• China’s • People’s
People’s Congress
Congress
NW1
• vice-chief • committee P 50.00 66.67 66.67 66.67 33.33
committee member R 50.00 66.67 66.67 66.67 33.33
member Jiyun Tian F 50.00 66.67 66.67 66.67 33.33
Jiyun Tian
• Jiyun Tian • Jiyun Tian
• He • He
NW5
• China’s • China’s P 0.00 50.00 50.00 50.00 0.00
People’s People’s R 0.00 50.00 50.00 50.00 0.00
Congress Congress F 0.00 50.00 50.00 50.00 0.00
delegation delegation
led by
vice-chief
committee
member Jiyun
Tian
• the • the
delegation delegation
from China’s from China’s
People’s People’s
Congress Congress
</table>
<tableCaption confidence="0.860086">
Table 3: Category 2b: Partial match (partial mentions).
4.1.3 Category 3: Merged entities
</tableCaption>
<bodyText confidence="0.9993434">
This category consists of response entities that
contain mentions from two or more key entities (Ta-
ble 4). Our sample contains only four examples in
this category, but it is still possible to discern two
subcategories:
</bodyText>
<sectionHeader confidence="0.514819" genericHeader="method">
1. The new information is incorrect
</sectionHeader>
<bodyText confidence="0.991851181818182">
In the key entity MZ40, the sex of the gender-neutral
her ten-year-old child has been given by
the mention him. Replacing it with the mention
she in the response entity gives the wrong informa-
tion about the child’s sex. Entities BN2 and MZ17
also belong to this subcategory, but here the men-
tions in the response entity are morphologically in-
consistent, thus making the mistake easier to detect.
2. The new information is correct or neutral
In entity pair MZ19 the key mention the latter
group was replaced with response mention them,
</bodyText>
<table confidence="0.595629888888889">
Key Response MUC B3 CEAF. CEAF_ blanc
entities entities
BN2
• The P 66.67 25.00 33.33 25.00 0.00
President R 0.00 50.00 33.33 50.00 0.00
• he and his • he and his F 0.00 33.33 33.33 33.33 0.00
wife, now a wife, now a
New York New York
senator senator
</table>
<listItem confidence="0.892529761904762">
• their • he
• his
MZ19
• the more • the more P 50.00 66.67 66.67 66.67 33.33
affluent affluent R 50.00 66.67 66.67 66.67 33.33
Taiwanese Taiwanese F 50.00 66.67 66.67 66.67 33.33
• their • their
• the latter • them
group
MZ17
• her elder • her elder P 0.00 33.33 40.00 33.33 0.00
son and son and R 0.00 50.00 40.00 50.00 0.00
daughter daughter F 0.00 40.00 40.00 40.00 0.00
• them • him
• him
MZ40
• Her • Her P 50.00 66.67 57.14 66.67 33.33
ten-year-old ten-year-old R 33.33 50.00 57.14 50.00 20.00
child child F 40.00 57.14 57.14 57.14 25.00
• him • she
• The child • The child
</listItem>
<tableCaption confidence="0.998641">
Table 4: Category 3: Merged entities
</tableCaption>
<bodyText confidence="0.961057741935484">
2b.
into two subcategori
es:
rics agree that the CR precision, recall an
d F-score
equal zero.
4.1.4 Category 4: Unsuccessful coreference
resolution
The entities in this category (Table 5) are divided
the omitted and replacement mentions having very
similar informational content.
As expected, the scores in Category 3 are lower
then those in Category 2 (as a whole), but they are
still consistently better than the scores of the Cate-
gory
No response entity has been given Two of the
key entities (MZ38 and NW4) were not aligned with
any response entities, and not surprisingly all met-
The response entities do not contain a single
mention that is correct Although the re-
sponse entities in the remaining entity pairs are non-
empty, an intuitive CR evaluation says there is not
much sense in aligning near-vacuous mentions if
the entity is otherwise wrong or empty. Already
in the two rather simple cases of
and
the metrics show large discrepancies: While link-
based
and BLANC correctly give an F-score of
0.00 as there are no correct links in the entity, the
mention-based B3 and CEAF
</bodyText>
<equation confidence="0.65012825">
“heavy”
WB0
WB1
MUC
</equation>
<bodyText confidence="0.372114">
measures award them
</bodyText>
<listItem confidence="0.396782">
• him
</listItem>
<page confidence="0.960138">
4
</page>
<table confidence="0.994723764705882">
Key Response MUC B3 CEAFe CEAF_ BLANC
entities entities
WB0
• the beauty • the one P 0.00 50.00 50.00 50.00 0.00
industry hand R 0.00 50.00 50.00 50.00 0.00
• it • it F 0.00 50.00 50.00 50.00 0.00
WB1
• the consumer • clinical P 0.00 50.00 50.00 50.00 0.00
dermatologists R 0.00 50.00 50.00 50.00 0.00
• they • they F 0.00 50.00 50.00 50.00 0.00
MZ33
• Chang, P 100.00 100.00 75.00 100.00 100.00
Mei-liang, R 50.00 60.00 75.00 60.00 30.00
chairperson F 66.67 75.00 75.00 75.00 46.15
of the TBAD
Women’s
Division, • her
</table>
<listItem confidence="0.990972">
• her
• she • she
• Her • Her
• she
</listItem>
<tableCaption confidence="0.999289">
Table 5: Category 4: Unsuccessful coreference resolution
</tableCaption>
<bodyText confidence="0.999595666666667">
with a rather high F-score of 50.00.
Entity MZ33 has been awarded high F-scores by
all metrics, averaging 67.56%. However, almost all
information from the key entity in MZ33 has been
lost in the response entity: The key entity contains
information on a person, a female, a Taiwanese na-
tional, her name (Chang Mei-lian) and the ad-
ditional information that she is a chairperson
of the TBAD, Women’s Division. The
response entity contains the information that its
mentions refer to a female, which is most probably
a person, but might be a ship, or a well loved pet.
None of the metrics indicate that such a substantial
loss of information renders the coreference resolu-
tion of MZ33 practically useless for a human user.
</bodyText>
<sectionHeader confidence="0.979502" genericHeader="method">
5 Entity ranking
</sectionHeader>
<bodyText confidence="0.9998700625">
As some of the metrics yield consistently lower
F-score levels, it is more appropriate to compare
rankings of entities than the actual F-scores (Table
6). We have also – to infuse an iota of old-school
armchair linguistics – added a sixth rating column,
showing intuitive rankings, based on informational
value retained. The lowest rankings for any metric
are marked in bold.
The entities showing broad agreement among the
metrics are only the best (Category 1) and the worst
ones (MZ38 and NW4, Category 4).
The metrics disagreement surfaces with entities
WB0 and WB1 of Category 4. The link-based met-
rics, MUC and BLANC, rank them last (13th), while
they are ranked much higher (13th out of 19) by
the mention-based and entity-based metrics (B3 and
</bodyText>
<table confidence="0.999776181818182">
Entity MUC B3 CEAFe CEAF_ BLANC Human
BC45 6 5 5 5 5 7
BC22 8 7 7 7 8 8
BC51 1 1 1 1 1 1
BN0 1 1 1 1 1 1
BN1 5 8 8 8 7 13
BN2 13 18 18 18 13 15
MZ19 10 10 10 10 10 14
MZ33 8 9 9 9 9 17
MZ17 13 17 17 17 13 15
MZ40 12 12 12 12 12 17
MZ22 13 13 13 13 13 10
MZ24 1 1 1 1 1 1
MZ38 – 19 19 19 13 19
NW0 13 19 19 19 13 10
NW1 10 10 10 10 10 8
NW2 7 6 6 6 6 5
NW3 1 1 1 1 1 1
NW4 – 19 19 19 13 19
NW5 13 13 13 13 13 10
WB0 13 13 13 13 13 19
WB1 13 13 13 13 13 19
</table>
<tableCaption confidence="0.999653">
Table 6: Ranking of our example entities.
</tableCaption>
<bodyText confidence="0.998239791666667">
CEAF). In this case the human evaluator agrees with
the link-based metrics: If there is not a single cor-
rect link within an entity, our intuition says that no
useful CR has taken place.
However, the presence of a single correct coref-
erent link is not sufficient for our intuition of suc-
cessful resolution. Consider entities MZ22 and
NW5 (Table 3): They also consist of two en-
tities where only one is correct, and have re-
ceived the same ratings as WB0 and WB1, but
in this case we judge CR as much more success-
ful. There are two main differences between this
and the previous case. Firstly, the correct men-
tion is in the previous case a meaning-lean pro-
noun (it and they) while the correct mention
in this case is a ’full-bodied’ NP (the school
in Shenzhen and the delegation from
China’s People’s Congress). In addition,
in both of the Category 2 entity pairs, the incorrect
mention holds an informational value very close or
identical to that of the correct mention. This exam-
ple illustrates the importance of informational value
content of the mentions for the human evaluation of
the resolution.
</bodyText>
<sectionHeader confidence="0.995932" genericHeader="method">
6 Formalizing the intuition
</sectionHeader>
<bodyText confidence="0.9813895">
We have earlier (§4.1) introduced a classificaion
based on an informal notion of (human) intuitive
coreference resolution evaluation. In this section we
will try to formalize the classification.
Category 1 The key entities and response entities
are identical:
</bodyText>
<page confidence="0.89303">
5
</page>
<equation confidence="0.565253">
∀x(x ∈ K ↔ x ∈ R) (1)
</equation>
<bodyText confidence="0.9590375">
Category 2 The response entity is a proper subset
of the key entity:
</bodyText>
<equation confidence="0.939123">
∀x(x ∈ R → x ∈ K)∧
(2a)
∃y(y ∈ K ∧ y ∈/ R)
</equation>
<bodyText confidence="0.999597857142857">
This is the only condition for Category 2a. Cat-
egory 2b shares the condition (2a), but to formalize
it, we have to add overlap(x,y) relation. We can de-
fine it as a common substring for x and y of a certain
length, possibly including at least one major syntac-
tic category, or even the lexical ’head’ if some way
of operationalizing that notion is available.
</bodyText>
<equation confidence="0.9011345">
∃x(x ∈ K ∧ x ∈ R)∧ (2b)
∃y∃z(y ∈ K ∧ z ∈ R ∧ overlap(y, z))
</equation>
<bodyText confidence="0.998484857142857">
We need at least two correct mentions in the re-
sponse entity, and at least one that overlaps, as re-
sponse entities containing only one correct mention
do not have any correct links.
Category 3 Response entity contains a subset of
the key entity mentions as well as additional men-
tion(s) belonging to some other entity (E):
</bodyText>
<equation confidence="0.934426">
∃x(x ∈ K ∧ x ∈ R)∧
∃y(y ∈/ K ∧ y ∈ R ∧ y ∈ E) (3)
</equation>
<bodyText confidence="0.99675875">
Category 4 The entities belonging to this category
have a twofold definition: The response entity is ei-
ther empty or if it contains one correct mention, it
cannot contain an overlapping mention.
</bodyText>
<equation confidence="0.991723">
∀x(x ∈ K → x ∈/ R)∨
∃x(x ∈ K ∧ x ∈ R) → (4)
∀y∀z((y ∈ K ∧ z ∈ R) → ¬overlap(y, z))
</equation>
<bodyText confidence="0.9999608">
The classification that has been introduced as a in-
formal one in §4.1 is thus computable given an op-
erational definition of overlap. In future work we
will investigate the distribution of the four error cat-
egories on a larger sample.
</bodyText>
<sectionHeader confidence="0.993015" genericHeader="conclusions">
7 Conclusion and outlook
</sectionHeader>
<bodyText confidence="0.999768083333334">
In this paper we have compared metrics on the ba-
sis of their evaluation of coreference resolution per-
formed on real-life texts, and contrasted their eval-
uation to an intuitive human evaluation of corefer-
ence resolution. We conjecture that humans require
both correct coreferent links and correct (whole or
partial) mentions of a certain information weight to
consider a resolution successful.
This approach has some shortcomings. Firstly,
the manual nature of the analysis has imposed a
limit on the number of the examples, so our data
may not be representative. Secondly, there is un-
certainity connected to how well the coreference
resolution evaluation metrics are suited to be used
in this way. The latter drawback is the more seri-
ous one: the metrics were not designed to evaluate
single key/response pairs, but whole texts. How-
ever, we would argue that if we want to discover
new insights into the evaluation process, some level
of approximation is necessary. There are at least
two arguments in favor of this particular approxi-
mation: Firstly, all metrics are based on evaluating
key/response pairs. Analyzing their performance at
this level can be a reasonable indicator of their per-
formance on the text level. Secondly, even if metrics
are treated “unfairly”, they are all treated equally.
We thus believe that this work can be seen as an
illustration of remaining evaluation challenges in the
field of coreference resolution.
A natural extension of this work would be in-
cluding more humans in evaluating coreference res-
olution systems, to provide a more representative
human judgement. This evaluation should then be
extended from evaluating coreference resolution of
single key/response entity pairs, to assessing the
quality of coreference resolution on a text as a
whole.
And, finally: Every mention carries an in-
formation value, and this weight varies from
quite heavy (as in vice-chief committee
member Jiyun Tian), to somewhat lighter
(Jiyun Tian) to virtually weightless (He). In-
formation weights are not distributed randomly, but
conform to discourse structure. It would be inter-
esting to map the pattern of their distribution, and
see if incorporating this information could improve
both coreference resolution and its quantitative eval-
uation.
</bodyText>
<sectionHeader confidence="0.999308" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9847815">
We would like to thank the anonymous reviewers for
their useful comments.
</bodyText>
<page confidence="0.998843">
6
</page>
<sectionHeader confidence="0.998302" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999562476190476">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In The First Interna-
tional Conference on Language Resources and Evalu-
ation Workshop on Linguistics Coreference, pages 563
– 566, Granada, Spain.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classication. Procesamiento del Lenguaje Natural,
42:87–96.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of the 15th Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 28–34, Portland,
Oregon.
Xiaoqiang Luo, Abe Ittycheriah Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 136–143, Barcelona, Spain.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 25–32, Vancouver, Canada.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–27, Portland,
Oregon.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the Joint
Conference on EMNLP and CoNLL: Shared Task,
pages 1–40, Jeju Island, Korea.
W. M. Rand. 1971. Objective criteria for evaluation of
clustering methods. Journal of American Statistical
Association, 66(336):846–850.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(04):485–510.
Marta Recasens, Lluis M`arquez, Emili Sapena,
M. Ant`onia Marti, Mariona Taul´e, V´eronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, ACL 2010, pages
1–8, Uppsala, Sweden.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message understanding Conference
(MUC-6), pages 45–52, San Francisco, CA.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competition
learning approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 176–183, Sapporo, Japan.
</reference>
<page confidence="0.999515">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882000">
<title confidence="0.999045">Critical Reflections on Evaluation Practices in Coreference Resolution</title>
<author confidence="0.999061">Gordana Ili´c</author>
<affiliation confidence="0.9989335">Department of University of</affiliation>
<email confidence="0.916724">gordanil@ifi.uio.no</email>
<abstract confidence="0.998038666666667">In this paper we revisit the task of quantitative evaluation of coreference resolution systems. We review the most commonly used metrics on the basis of their evaluation of coreference resolution in five texts from the OntoNotes corpus. We examine both the correlation between the metrics and the degree to which our human judgement of coreference resolution agrees with the metrics. In conclusion we claim that loss of information value is an essential factor, insufficiently adressed in current metrics, in human perception of the degree of success or failure of coreference resolution. We thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<location>Granada,</location>
<contexts>
<context position="1499" citStr="Bagga and Baldwin, 1998" startWordPosition="238" endWordPosition="241">th the coreference resolution and its evaluation. 1 Introduction and motivation Coreference resolution (CR) is the task of linking together multiple expressions of a given entity (Yang et al., 2003). The field has experienced a surge of interest with several shared tasks in recent years: SemEval 2010 (Recasens et al., 2010), CoNLL 2011 (Pradhan et al., 2011) and CoNLL 2012 (Pradhan et al., 2012). However the field has from the very start been riddled with problems related to the scoring and comparison of CR systems. Currently there are five metrics in wider use: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two CEAF metrics (Luo, 2005) and BLANC (Recasens and Hovy, 2011). As there is no global agreement on which metrics are the most appropriate, the 1 above-mentioned shared tasks have used a combination of several metrics to evaluate the contenders. Although coreference resolution is a subproblem of natural language understanding, coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors. In our view, it is of utmost importance to observe actual texts and actual system errors. 2 Background: The metrics In this se</context>
<context position="4167" citStr="Bagga and Baldwin, 1998" startWordPosition="684" endWordPosition="687">are the average of the P, R and F-score of corresponding coreferential and non-referential values. However, since this is an analysis of isolated entities, there are no non-coreferential links. For that reason, in this paper we only present coreferential precision, recall and F-score for this metric: rc rc 2PcRc Pc = rc+wc , Rc = rc+wn and Fc = Pc+Rc , where rc is the number of correct coreferential links, wc the number of incorrect coreferential links, and wn is the number of non-coreferential links incorrectly marked as coreferent by the system. 2.2 Entity and mention-based: B3 and CEAF B3 (Bagga and Baldwin, 1998) calculates precision and recall for every mention in the document, and then combines them to an overall precision and recall. Precision of a single mention mi is the number of correct mentions in the response entity Ri that contains mi divided by the total number of mentions in Ri. Recall of mi is again the number of correct mentions in Ri, this time divided by the number of mentions in the key entity Ki that contains mention mi. The precision and recall for the entire document can be calculated as weighted sums of precision and recall of the individual mentions. The default weight, also used</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563 – 566, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Global joint models for coreference resolution and named entity classication.</title>
<date>2009</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>42--87</pages>
<contexts>
<context position="6334" citStr="Denis and Baldridge, 2009" startWordPosition="1074" endWordPosition="1077">7 0.70 0.62 – 0.46 0.60 BLANC 0.57 0.70 0.57 0.68 – 0.35 MELA 0.59 0.73 0.59 0.70 0.70 – Table 1: Kendall T rank correlation coefficient for teams participating in CoNLL shared tasks, with CoNLL 2011 in the upper right, CoNLL 2012 in the lower left corner. 3 Correlating CoNLL shared tasks results To illustrate the complexity of the present evaluation best practices, we have applied the Kendall T rank correlation coefficient to the ratings the metrics gave coreference resolution systems that competed in the two recent CoNLL shared tasks. The official metrics of the CoNLL shared tasks was MELA (Denis and Baldridge, 2009), a weighted average of MUC, B3 and CEAFe. The results for CoNLL 2011 (Table 1) show a rather weak correlation among the metrics going down to as low as 0.22 between CEAFe and MUC. Somewhat surprisingly, the two link-based metrics, MUC and BLANC, also show a low degree of agreement (0.35), while the mention-based metrics, CEAFm and B3, show the highest agreement of all non-composite metrics. However, this agreement is not particularly high either as the two metrics agree on just above the half of all the cases (0.56). The results for CoNLL 2012 show much higher correlation among the metrics ra</context>
</contexts>
<marker>Denis, Baldridge, 2009</marker>
<rawString>Pascal Denis and Jason Baldridge. 2009. Global joint models for coreference resolution and named entity classication. Procesamiento del Lenguaje Natural, 42:87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="8179" citStr="Lee et al., 2011" startWordPosition="1382" endWordPosition="1385">the key/response entity pairs from five short texts from 2 the development corpus of the CoNLL 2011 Shared Task (Pradhan et al., 2011), one text from each of the five represented genres: Broadcast Conversations (BC), Broadcast News (BN), Magazine (MZ), News Wire (NW) and Web Blogs and News Groups (WB). The texts were chosen as randomly as possible, the only constraint being length1. The gold standard texts are originally from OntoNotes 4.0, and contain 64 mentions distributed among 21 key entities. The response texts are the output of Stanford’s Multi-Pass Sieve Coreference Resolution System (Lee et al., 2011). 4.1 Categorization Instead of classifying entities according to their score by some of the metrics, or a combination of several of them, as done by the CoNLL shared tasks, we have based the classification on a notion of linguistic common sense – our subjective idea of how humans evaluate the success or failure of CR. We divide key/response entity pairs into four categories: • Category 1: Perfect match • Category 2: Partial match • Category 3: Merged entities • Category 4: Failed coreference resolution We will concentrate on the amount of informational value from the key entity that has been </context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 Shared Task. In Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 28–34, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>136--143</pages>
<location>Barcelona,</location>
<contexts>
<context position="2370" citStr="Luo et al., 2004" startWordPosition="381" endWordPosition="384">lthough coreference resolution is a subproblem of natural language understanding, coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors. In our view, it is of utmost importance to observe actual texts and actual system errors. 2 Background: The metrics In this section, we will present the five metrics in the usual terms of precision, recall and F-score. We follow the predominant practice and use the term mention for individual referring expressions, and entity for sets of mentions that refer to the same object (Luo et al., 2004). We use the term key entity (K) for gold entities, and response entity (R) for entities which were produced by the CR system. 2.1 Link-based: MUC and BLANC The MUC metric (Vilain et al., 1995) is based on comparing the number of links in the key entity (|K |− 1) to the number of links missing from the response entity, routinely calculated as the number of partitions of the key entity |p(K) |minus one, so Recall = (|K|−1)−(|P(K)|−1) = |K|−|P(K) |For |K|−1 |K|−1 the whole document, recalls for entities are simply � |Ki|−|P(Ki)| added: Recall = In calculating preE(|Ki|−1) cision, the case is inv</context>
</contexts>
<marker>Luo, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the Bell tree. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 136–143, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<pages>25--32</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1533" citStr="Luo, 2005" startWordPosition="246" endWordPosition="247">. 1 Introduction and motivation Coreference resolution (CR) is the task of linking together multiple expressions of a given entity (Yang et al., 2003). The field has experienced a surge of interest with several shared tasks in recent years: SemEval 2010 (Recasens et al., 2010), CoNLL 2011 (Pradhan et al., 2011) and CoNLL 2012 (Pradhan et al., 2012). However the field has from the very start been riddled with problems related to the scoring and comparison of CR systems. Currently there are five metrics in wider use: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two CEAF metrics (Luo, 2005) and BLANC (Recasens and Hovy, 2011). As there is no global agreement on which metrics are the most appropriate, the 1 above-mentioned shared tasks have used a combination of several metrics to evaluate the contenders. Although coreference resolution is a subproblem of natural language understanding, coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors. In our view, it is of utmost importance to observe actual texts and actual system errors. 2 Background: The metrics In this section, we will present the five me</context>
<context position="4861" citStr="Luo, 2005" startWordPosition="813" endWordPosition="814">s them to an overall precision and recall. Precision of a single mention mi is the number of correct mentions in the response entity Ri that contains mi divided by the total number of mentions in Ri. Recall of mi is again the number of correct mentions in Ri, this time divided by the number of mentions in the key entity Ki that contains mention mi. The precision and recall for the entire document can be calculated as weighted sums of precision and recall of the individual mentions. The default weight, also used in this experiment is 1n, where n is the number of mentions in the document. CEAF (Luo, 2005) is based on the best alignment of subsets of key and response entities. For any mapping g E Gm the total similarity Φ(g) is the sum of all similarities. The best alignment g* is found by maximizing the sum of similarities Φ(g) between the key and response entities, while the maximum total similarity is the sum of the best similarities. Precision and recall are defined in terms of the similarity measure Φ(g*): PI DW) DW ) Φ(g∗) R = E.i φ(Ki,Ki). There are two versions of CEAF with different similarity measures, 0m(K, R) = |K n R |and 0e(K, R) = 2JKnRJ JKJ+JRJ. 0e is the basis for CEAFe which s</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 25–32, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--27</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="1235" citStr="Pradhan et al., 2011" startWordPosition="191" endWordPosition="194"> of information value is an essential factor, insufficiently adressed in current metrics, in human perception of the degree of success or failure of coreference resolution. We thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation. 1 Introduction and motivation Coreference resolution (CR) is the task of linking together multiple expressions of a given entity (Yang et al., 2003). The field has experienced a surge of interest with several shared tasks in recent years: SemEval 2010 (Recasens et al., 2010), CoNLL 2011 (Pradhan et al., 2011) and CoNLL 2012 (Pradhan et al., 2012). However the field has from the very start been riddled with problems related to the scoring and comparison of CR systems. Currently there are five metrics in wider use: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two CEAF metrics (Luo, 2005) and BLANC (Recasens and Hovy, 2011). As there is no global agreement on which metrics are the most appropriate, the 1 above-mentioned shared tasks have used a combination of several metrics to evaluate the contenders. Although coreference resolution is a subproblem of natural language understanding, </context>
<context position="7696" citStr="Pradhan et al., 2011" startWordPosition="1303" endWordPosition="1306">ain least correlated, while CEAFe and BLANC, in 2011 almost independent, show average correlation (0.57) in 2012. In our view, comparatively low correlations as well as surprising variation from year to year suggests a certain degree of ’fuzziness’ in quantitative coreference resolution evaluation. We leave the investigation of variation between the two years for future work. 4 Error analysis To better understand the functioning of the metrics we have conducted an error analysis on the key/response entity pairs from five short texts from 2 the development corpus of the CoNLL 2011 Shared Task (Pradhan et al., 2011), one text from each of the five represented genres: Broadcast Conversations (BC), Broadcast News (BN), Magazine (MZ), News Wire (NW) and Web Blogs and News Groups (WB). The texts were chosen as randomly as possible, the only constraint being length1. The gold standard texts are originally from OntoNotes 4.0, and contain 64 mentions distributed among 21 key entities. The response texts are the output of Stanford’s Multi-Pass Sieve Coreference Resolution System (Lee et al., 2011). 4.1 Categorization Instead of classifying entities according to their score by some of the metrics, or a combinatio</context>
<context position="11634" citStr="Pradhan et al., 2011" startWordPosition="1989" endWordPosition="1992">ases where the loss of informational value has been comparable. For instance, the response entity NW1 (Table 3) has received an average F-score of 56.67%, but its loss of informational value is comparable to that in entities BC45 and BN1 (Table 2). The BC45’s response entity has lost the information that Jiyun Tian is a vice-chief, while entities BC45 and BN1 have lost the information that the person referred to 1The texts longer than five sentences were discarded, to make the analysis tractable. 2Only response mentions with boundaries identical to the gold mentions are recognized as correct (Pradhan et al., 2011) 3 is The KMT vice chairman (BC45) and The President (BN1). However, the latter mentions have received a considerably higher average F-score of 88.32% and 75.68% respectively. This indicates that stricter mention detection requirements do not necessarily improve the quality of CR evaluation. Key Response MUC B3 CEAFQ CEAF_ BLANC entities entities MZ22 • a school in • a school P 0.00 50.00 50.00 50.00 0.00 Shenzhen for in Shenzhen R 0.00 50.00 50.00 50.00 0.00 the children of F 0.00 50.00 50.00 50.00 0.00 Hong Kong expats • the school • the school in Shenzhen in Shenzhen NW0 • China’s • People’</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes. In Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1–27, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task,</booktitle>
<pages>1--40</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1273" citStr="Pradhan et al., 2012" startWordPosition="198" endWordPosition="201">factor, insufficiently adressed in current metrics, in human perception of the degree of success or failure of coreference resolution. We thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation. 1 Introduction and motivation Coreference resolution (CR) is the task of linking together multiple expressions of a given entity (Yang et al., 2003). The field has experienced a surge of interest with several shared tasks in recent years: SemEval 2010 (Recasens et al., 2010), CoNLL 2011 (Pradhan et al., 2011) and CoNLL 2012 (Pradhan et al., 2012). However the field has from the very start been riddled with problems related to the scoring and comparison of CR systems. Currently there are five metrics in wider use: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two CEAF metrics (Luo, 2005) and BLANC (Recasens and Hovy, 2011). As there is no global agreement on which metrics are the most appropriate, the 1 above-mentioned shared tasks have used a combination of several metrics to evaluate the contenders. Although coreference resolution is a subproblem of natural language understanding, coreference resolution evaluation metr</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Rand</author>
</authors>
<title>Objective criteria for evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of American Statistical Association,</journal>
<volume>66</volume>
<issue>336</issue>
<contexts>
<context position="3206" citStr="Rand, 1971" startWordPosition="531" endWordPosition="532">r of links in the key entity (|K |− 1) to the number of links missing from the response entity, routinely calculated as the number of partitions of the key entity |p(K) |minus one, so Recall = (|K|−1)−(|P(K)|−1) = |K|−|P(K) |For |K|−1 |K|−1 the whole document, recalls for entities are simply � |Ki|−|P(Ki)| added: Recall = In calculating preE(|Ki|−1) cision, the case is inverted: The base entity is now the response, and the question posed is how many missing links have to be added to the key partitions to form the response entity. BLANC (Recasens and Hovy, 2011) is a variant of the Rand index (Rand, 1971) adapted for the task Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 1–7, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics of coreference resolution. The BLANC metric makes use of both coreferent and non-coreferent links, correct and incorrect. The final precision, recall and F-score are the average of the P, R and F-score of corresponding coreferential and non-referential values. However, since this is an analysis of isolated entities, there are no non-coreferential links. For that reason, in this paper we only present coreferential precisio</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>W. M. Rand. 1971. Objective criteria for evaluation of clustering methods. Journal of American Statistical Association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>04</issue>
<contexts>
<context position="1569" citStr="Recasens and Hovy, 2011" startWordPosition="250" endWordPosition="254">otivation Coreference resolution (CR) is the task of linking together multiple expressions of a given entity (Yang et al., 2003). The field has experienced a surge of interest with several shared tasks in recent years: SemEval 2010 (Recasens et al., 2010), CoNLL 2011 (Pradhan et al., 2011) and CoNLL 2012 (Pradhan et al., 2012). However the field has from the very start been riddled with problems related to the scoring and comparison of CR systems. Currently there are five metrics in wider use: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two CEAF metrics (Luo, 2005) and BLANC (Recasens and Hovy, 2011). As there is no global agreement on which metrics are the most appropriate, the 1 above-mentioned shared tasks have used a combination of several metrics to evaluate the contenders. Although coreference resolution is a subproblem of natural language understanding, coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors. In our view, it is of utmost importance to observe actual texts and actual system errors. 2 Background: The metrics In this section, we will present the five metrics in the usual terms of precisio</context>
<context position="3162" citStr="Recasens and Hovy, 2011" startWordPosition="520" endWordPosition="523">ric (Vilain et al., 1995) is based on comparing the number of links in the key entity (|K |− 1) to the number of links missing from the response entity, routinely calculated as the number of partitions of the key entity |p(K) |minus one, so Recall = (|K|−1)−(|P(K)|−1) = |K|−|P(K) |For |K|−1 |K|−1 the whole document, recalls for entities are simply � |Ki|−|P(Ki)| added: Recall = In calculating preE(|Ki|−1) cision, the case is inverted: The base entity is now the response, and the question posed is how many missing links have to be added to the key partitions to form the response entity. BLANC (Recasens and Hovy, 2011) is a variant of the Rand index (Rand, 1971) adapted for the task Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 1–7, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics of coreference resolution. The BLANC metric makes use of both coreferent and non-coreferent links, correct and incorrect. The final precision, recall and F-score are the average of the P, R and F-score of corresponding coreferential and non-referential values. However, since this is an analysis of isolated entities, there are no non-coreferential links. For that reason, in this </context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Marta Recasens and Eduard Hovy. 2011. BLANC: Implementing the Rand index for coreference evaluation. Natural Language Engineering, 17(04):485–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Lluis M`arquez</author>
<author>Emili Sapena</author>
<author>M Ant`onia Marti</author>
<author>Mariona Taul´e</author>
<author>V´eronique Hoste</author>
<author>Massimo Poesio</author>
<author>Yannick Versley</author>
</authors>
<title>Semeval-2010 task 1: Coreference resolution in multiple languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010,</booktitle>
<pages>1--8</pages>
<location>Uppsala,</location>
<marker>Recasens, M`arquez, Sapena, Marti, Taul´e, Hoste, Poesio, Versley, 2010</marker>
<rawString>Marta Recasens, Lluis M`arquez, Emili Sapena, M. Ant`onia Marti, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley. 2010. Semeval-2010 task 1: Coreference resolution in multiple languages. In Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="1469" citStr="Vilain et al., 1995" startWordPosition="233" endWordPosition="236">on weight could improve both the coreference resolution and its evaluation. 1 Introduction and motivation Coreference resolution (CR) is the task of linking together multiple expressions of a given entity (Yang et al., 2003). The field has experienced a surge of interest with several shared tasks in recent years: SemEval 2010 (Recasens et al., 2010), CoNLL 2011 (Pradhan et al., 2011) and CoNLL 2012 (Pradhan et al., 2012). However the field has from the very start been riddled with problems related to the scoring and comparison of CR systems. Currently there are five metrics in wider use: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two CEAF metrics (Luo, 2005) and BLANC (Recasens and Hovy, 2011). As there is no global agreement on which metrics are the most appropriate, the 1 above-mentioned shared tasks have used a combination of several metrics to evaluate the contenders. Although coreference resolution is a subproblem of natural language understanding, coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors. In our view, it is of utmost importance to observe actual texts and actual system errors. 2 Back</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the Sixth Message understanding Conference (MUC-6), pages 45–52, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Coreference resolution using competition learning approach.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>176--183</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1073" citStr="Yang et al., 2003" startWordPosition="163" endWordPosition="166">correlation between the metrics and the degree to which our human judgement of coreference resolution agrees with the metrics. In conclusion we claim that loss of information value is an essential factor, insufficiently adressed in current metrics, in human perception of the degree of success or failure of coreference resolution. We thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation. 1 Introduction and motivation Coreference resolution (CR) is the task of linking together multiple expressions of a given entity (Yang et al., 2003). The field has experienced a surge of interest with several shared tasks in recent years: SemEval 2010 (Recasens et al., 2010), CoNLL 2011 (Pradhan et al., 2011) and CoNLL 2012 (Pradhan et al., 2012). However the field has from the very start been riddled with problems related to the scoring and comparison of CR systems. Currently there are five metrics in wider use: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two CEAF metrics (Luo, 2005) and BLANC (Recasens and Hovy, 2011). As there is no global agreement on which metrics are the most appropriate, the 1 above-mentioned share</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim Tan. 2003. Coreference resolution using competition learning approach. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 176–183, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>