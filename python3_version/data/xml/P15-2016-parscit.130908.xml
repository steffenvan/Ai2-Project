<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000761">
<title confidence="0.853945333333333">
I do not disagree: Leveraging monolingual alignment to detect
disagreement in dialogue
Ajda Gokcen and Marie-Catherine de Marneffe
</title>
<author confidence="0.56364">
Linguistics Department
</author>
<affiliation confidence="0.937895">
The Ohio State University
</affiliation>
<email confidence="0.998094">
gokcen.2@osu.edu, mcdm@ling.ohio-state.edu
</email>
<sectionHeader confidence="0.993868" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853681818182">
A wide array of natural dialogue dis-
course can be found on the internet.
Previous attempts to automatically de-
termine disagreement between interlocu-
tors in such dialogue have mostly re-
lied on n-gram and grammatical depen-
dency features taken from respondent text.
Agreement-disagreement classifiers built
upon these baseline features tend to do
poorly, yet have proven difficult to im-
prove upon. Using the Internet Argument
Corpus, which comprises quote and re-
sponse post pairs taken from an online de-
bate forum with human-annotated agree-
ment scoring, we introduce semantic en-
vironment features derived by comparing
quote and response sentences which align
well. We show that this method improves
classifier accuracy relative to the baseline
method namely in the retrieval of disagree-
ing pairs, which improves from 69% to
77%.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875571428571">
To achieve robust text understanding, natural lan-
guage processing systems need to automatically
extract information that is expressed indirectly.
Here we focus on identifying agreement and dis-
agreement in online debate posts. Previous work
on this task has used very shallow linguistic anal-
ysis: features are surface-level ones, such as n-
grams, post initial unigrams, bigrams and tri-
grams (which aim at learning the discourse func-
tions of discourse markers, e.g., well, really, you
know), repeated sequential use of punctuation
signs (e.g., !!, ?!). When automatically detecting
(dis)agreement, these features fall short, reaching
around 65% accuracy on a balanced dataset (Ab-
bott et al., 2011; Misra and Walker, 2013). Adding
extra-linguistic features, such as the structure of
the post threads and stance of the post’s author on
other subjects, boosts performance to 75% (Hasan
and Ng, 2013). In this work, we leverage richer
linguistic models to increase performance.
Agreement may be explicitly marked. In ex-
ample (1) in Table 2, the response-initial bigram
I agree is a strong cue of agreement that surface
features can learn, but there are more complex ex-
amples that surface features cannot capture. In ex-
ample (2), the response-initial word Yes is not indi-
cating agreement, despite being in general a good
cue for it. Instead it is necessary to capture the po-
larity mismatch between the first sentence in the
quote and the first sentence in the response (God
doesn’t take away sinful desires vs. Yes, God does
take away sinful desires) to infer that the response
disagrees with the quote. There may also be mis-
matches of modality, as demonstrated in the third
example (saw vs. may have believed). Here we
also see an example of an explicit agreement word
which is negated (that does not make it true) in a
way that most surface features fail to capture.
Some discourse-level parsing (Joty et al., 2013)
has been utilized in agreement detection, but most
previous work does not take discourse structure
into account: the response post is simply taken as
a whole as the reply to the quote. To overcome
this issue, we take advantage of the considerable
progress in monolingual alignment (e.g., Thadani
et al. 2012, Yao et al. 2013, Sultan et al. 2014)
which allows us to align sentences of the quote to
sentences in the response. This approach is rem-
iniscent of the one used for Recognizing Textual
Entailment (RTE, Dagan et al. 2006, Giampiccolo
et al. 2007) where, given two short passages, sys-
tems identify whether the second passage follows
from the first one according to the intuitions of an
intelligent human reader. One common approach
used in RTE was to align the two passages, and
reason based on the alignment obtained.
</bodyText>
<page confidence="0.980001">
94
</page>
<bodyText confidence="0.322205">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 94–99,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.607686">
Quote Response Score
</subsectionHeader>
<equation confidence="0.6531385">
1 CCW LAWS ARE FOR TRACKING GUN OWN- I agree. What is the point? Felons with firearms do 2.5
ERS WHO EXERCISE THIER RIGHTS!!! not bother with CCW licenses.
</equation>
<bodyText confidence="0.997442111111111">
2 God doesn’t take away sinful desires. You’ve never Yes, God does take away sinful desires. (If you ask -1.7
had sinful desires? I know I have. People assume Him.) I’m not saying that it doesn’t take any work
that when you become a Christian some manner of on your part, though. When you have a sinful de-
shield gets put up around you and shields you from sire, you allow a thought to become more than just
“worldly” things. I believe that’s wrong, I actually a stray idea. You foster and encourage the thought
believe that life as a Christian is very hard. We often and it becomes a desire. God takes away the de-
pawn it off as the end of our troubles to “convert” sires, helps you deal with your “stray thoughts”,
people. I don’t believe it. and shows you how to keep them from becoming
desires.
</bodyText>
<sectionHeader confidence="0.479179" genericHeader="method">
3 Your idea about science is a philosophy of science. Many people once believed that the earth is flat: -2
</sectionHeader>
<bodyText confidence="0.664458133333333">
[...] The Apostles saw Jesus walk on water. There perhaps some still do. [...] The apostles may have
was no ‘measure’ by your version of science, but believed that Jesus walked on water: that does NOT
what they saw remains true. make it true.
4 does life end here? end where? ambiguously phrased. if “here” = -1.4
“death”, then yes! by definition, yes!
5 Is even ‘channel’ sufficiently ateleological a verb? Yes. It describes an action without ascribing its 2.8
form to its end result, outcome, whatever but strictly
to a cause’s force’s in action. [...] But since it is un-
derstood that mechanical forces can also ‘channel’,
unintentional, out of simple mechanics, the word
channel cannot be called teleological. In the same
way, ‘sorting’ can be considered non-teleological,
hence mechanical, and thus suited to your glossary,
because things can be sorted by mechanical forces
alone.
</bodyText>
<tableCaption confidence="0.988679">
Table 1: QR pairs from the Internet Argument Corpus.
</tableCaption>
<bodyText confidence="0.999907291666667">
Here, similarly, once we have identified sen-
tences in the response which align well with sen-
tences in the quote, it becomes easier to extract
deep semantic features such as polarity and modal-
ity mismatch between sentences as well as em-
beddings under modality, negation, or attitude
verbs. For instance, in example (2) in Table 1, the
first sentence in the quote gets aligned with high
probability to the first sentence in the response,
which enables us to identify the polarity mismatch
(doesn’t vs. does). In example (3), the italicized
sentences are the most well-aligned, enabling us to
identify that the response’s author embeds under
modality the event of Jesus walking on water and
thus does not take it as a fact, whereas the quote’s
author does take it as a fact.
Our experiments demonstrate that our linguis-
tic model based on alignment significantly out-
performs a baseline bag-of-words model in the
recall of disagreeing quote-response (QR) pairs.
Such linguistic models will transfer more easily to
any debate dialogue, independent of the structural
information of post threads and author’s stance
which might not always be recoverable.
</bodyText>
<table confidence="0.9938454">
Full Data Set Balanced Training Set
Disagree 5741 779
Neutral 3125 0
Agree 1113 779
Total 9980 1158
</table>
<tableCaption confidence="0.999166">
Table 2: Category counts in the training set.
</tableCaption>
<sectionHeader confidence="0.98654" genericHeader="method">
2 Data
</sectionHeader>
<bodyText confidence="0.999963909090909">
We used the Internet Argument Corpus (IAC),
a corpus of quote-response pairs annotated for
agreement via Mechanical Turk (Walker et al.,
2012). Agreement scores span from -5 (strong dis-
agreement) and +5 (strong agreement). The distri-
bution is shown in Figure 2. Because the original
data skews toward disagreement, following Abbott
et al. (2011), we created a balanced set, discarding
“neutral” pairs between -1 and +1. We split the
data into training, development and test sets. 1 Ta-
ble 2 shows the category counts in the training set.
</bodyText>
<footnote confidence="0.8939145">
1We could not obtain the training-development-test
split from Abbott et al. (2011). Our split is avail-
able at www.ling.ohio-state.edu/˜mcdm/data/
2015/Balanced_IAC.zip.
</footnote>
<page confidence="0.991952">
95
</page>
<figure confidence="0.9992836875">
2000
1500
1000
500
0
(−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) (2,3) (3,4) (4,5)
Agreement Score Range
(a) Full dataset.
400
300
200
100
0
(−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) (2,3) (3,4) (4,5)
Agreement Score Range
(b) Balanced training set.
</figure>
<figureCaption confidence="0.989375666666667">
Figure 1: Agreement score distribution of the
dataset, before and after balancing. -5 is high dis-
agreement, +5 is high agreement.
</figureCaption>
<sectionHeader confidence="0.998254" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999533375">
In this section, we detail the features of our model.
We use the maximum entropy model as imple-
mented in the Stanford CoreNLP toolset (Man-
ning and Klein, 2003). Many of the features make
use of the typed dependencies from the CoreNLP
toolset (de Marneffe et al., 2006). For comparison,
the baseline features attempt to replicate Abbott et
al. (2011).
</bodyText>
<subsectionHeader confidence="0.999092">
3.1 Baseline Features from Abbott et al. 2011
</subsectionHeader>
<bodyText confidence="0.998837611111111">
N-Grams. All unigrams, bigrams, and trigrams
were taken from each response.
Discourse Markers. In lieu of tracking dis-
course markers such as oh and so really, Abbott
et al. (2011) tracked response-initial unigrams, bi-
grams, and trigrams.
Typed Dependencies and MPQA. In addi-
tion to all dependencies from the response be-
ing used as features, dependencies were supple-
mented with MPQA sentiment values (Wilson et
al., 2005). A dependency like (agree,I) would
also yield the sentiment-dependency feature (pos-
itive,I), whereas (wrong, you) would also yield
(negative,you).
Punctuation. The presence of special punctu-
ation such as repeated exclamation points (!!),
question marks (??), and interrobang strings (?!)
were tracked as binary features.
</bodyText>
<subsectionHeader confidence="0.99582">
3.2 Alignment+ Features
</subsectionHeader>
<bodyText confidence="0.999969">
Our features utilize focal sentences: not only well-
aligned sentences from the quote and response, but
also the first sentence of the response in general.
Tracking certain features in initial and aligned sen-
tences proved more informative than doing the
same without discerning location.
Alignment scoring comes from running the Ja-
cana aligner (Yao et al., 2013) pairwise on every
sentence from each QR pair. Pairs of quote and
response sentences with alignment scores above a
threshold tuned on the development set are then
analyzed for feature extraction. The sentence pair
with the maximum alignment score for each post
pair is also analyzed regardless of its meeting the
threshold.
Post Length. Following Misra and Walker
(2013), we track various length features such as
word count, sentence count, and average sentence
length, including differentials of these measures
between quote and response. Short responses (rel-
ative to both word-wise and sentence-wise counts)
tend to correlate with agreement, while longer re-
sponses tend to correlate with disagreement.
Emoticons. Emoticons are a popular way of
communicating sentiment in internet text. Many
emoticons in the corpus are in forum-specific
code, such as emoticon rolleyes. We also detect a
wider array of common emoticons as regular ex-
pressions beginning with colons, semicolons, or
equals signs, such as :-D, ;), and =).
Speech Acts. To account for phenomena such as
commands (e.g., please read carefully, try again,
and define evil) and the rhetorical use of multi-
ple questions in a row, we use punctuation, depen-
dencies, and phrase-level analysis to automatically
detect and count interrogative and imperative sen-
tences. A phrase-structure tree headed by SQ or a
sentence-final question mark means a sentence is
considered interrogative; if a sentence’s root is la-
beled VB and has no subject relation, it is deemed
an imperative. The features in the classifier are
counts of the instances of interrogatives and im-
peratives in the response.
</bodyText>
<figure confidence="0.9084215">
Number of OR Pairs
Number of OR Pairs
</figure>
<page confidence="0.971234">
96
</page>
<table confidence="0.97662975">
Accuracy Agreement Disagreement
P R F1 P R F1
Baseline 71.85 70.64 74.77 72.65 73.21 68.92 71.00
Alignment+ 75.45 76.04 74.32 75.17 74.89 76.58 75.73
</table>
<tableCaption confidence="0.9765565">
Table 3: Accuracy, precision (P), recall (R) and F1 scores for both categories (agreement and disagree-
ment) on the test set.
</tableCaption>
<bodyText confidence="0.9486651">
Personal Pronouns. The presence of first, sec-
ond, and third person pronouns in the response
are each tracked as binary features. The inclusion
of personal pronouns in a post tends to indicate a
more emotional or personal argument, especially
second person pronouns.
Explicit Truth Values. Rather than simply re-
lying on n-gram-based tracking of explicit state-
ments of agreement, we include as features polar
(positive or negative) and modal (modal or non-
modal) context of instances of the words agree,
disagree, true, false, right, and wrong found in
the response, parallel to the agreement and denial
tracking in Misra and Walker (2013). Polar con-
text is determined by the presence or absence of
negation modifiers (e.g., not, never) in the depen-
dencies; modal context is determined by the pres-
ence of modal auxiliaries (e.g., might, could) and
adverbs (e.g., possibly).
Sentiment Scoring. Expanding on the use of
MPQA sentiment values, we use the posi-
tive/negative/neutral and strong/weak classifica-
tions of the words in the MPQA lexicon to cal-
culate sentiment scores of the posts and focal sen-
tences (well-aligned sentences from the quote and
response as well as the first sentence of the re-
sponse). The scoring assigns a value to each
MPQA word in the quote or response: the posi-
tive/negative label of a word means a positive or
negative score and the strong/weak label deter-
mines the weight: whether the word is worth +/-2
or +/-1. The sum of these values is computed as
the sentiment score. A score is generated for both
the response and quote in their entireties as well as
for focal sentences.
Discourse Markers. Initial 1, 2 and 3-grams are
tracked relative to focal sentences. This picks up
on discourse markers (such as well and but) with-
out having to explicitly code for each marker we
want to track.
</bodyText>
<figure confidence="0.9274385">
0.0 0.2 0.4 0.6 0.8 1.0
1− specificity
</figure>
<figureCaption confidence="0.94531">
Figure 2: ROC curves. The gray dotted line repre-
sents the baseline feature set, while the solid black
line represents the alignment+ feature set.
</figureCaption>
<bodyText confidence="0.96252404">
Punctuation. As in the baseline, the presence of
special punctuation like !! and ?! are used as bi-
nary features.
Factuality Comparison. Given aligned words
from well-aligned sentences in the quote and re-
sponse (e.g., God doesn’t take away sinful de-
sires and Yes, God does take away sinful desires),
we analyze the polarity, modality, and any sub-
sequent contradiction of both the quote and re-
sponse instances. As with the analysis of explicit
truth value words, polarity and modality are de-
termined according to the presence or absence of
negation and modal modifiers (auxiliaries and ad-
verbs) in the dependencies. Contradictions are
tracked as phrases marked with known contradic-
tory adverbs and conjunctions (e.g., however...,
but...). An aligned word pair is analyzed if it in-
volves content words, or if the words serve as the
root of their sentence’s dependency structure re-
gardless of part of speech. The features generated
indicate the part of speech of the word in the quote
and whether there is (1) a polarity match/clash, (2)
a modality match/clash, or (3) any contradiction
phrases immediately following the word or sen-
tence in the quote or response.
</bodyText>
<figure confidence="0.798603">
0.0 0.2 0.4 0.6 0.8 1.0
sensitivity
</figure>
<page confidence="0.998145">
97
</page>
<sectionHeader confidence="0.99867" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999941461538461">
Table 3 compares the results obtained with the
baseline features and the alignment+ features. The
alignment+ features lead to an overall improve-
ment, but a statistically significant improvement
(p &lt; 0.05, McNemar’s test) is only achieved for
classifying disagreeing pairs. The baseline model
underclassifies for disagreement and overclassi-
fies for agreement, but the alignment+ model does
well on both. As most cases of high alignment do,
indeed, correspond with disagreement, these fea-
tures are better in picking up on disagreement in
general. The ROC curve in Figure 3 shows that
the alignment+ classifier consistently has a higher
sensitivity (true-positive) rate than the baseline.
Figure 4 shows for both feature sets (baseline
and alignment+) the correct (gray bar) and incor-
rect (black bar) classifications on the test set, by
agreement score. The agreement score is predic-
tive of the correctness of the system (confirmed
by a logistic regression predicting system accuracy
given strength of agreement score, p &lt; 0.001): the
stronger the (dis)agreement score, the more accu-
rate the system is. The alignment+ features help
classify accurately the less strong (dis)agreements.
Examples (4) and (5) in Table 1 are incorrectly
classified by the baseline but correctly by the
alignment+ classifier. In (4), the strongest feature
in the baseline is the unigram yes, but the align-
ment+ features compare does life end here? to end
where?, and the fact that the aligned sentence in
the response is a question suggests disagreement.
Example (5) shows that superficial features like a
response-initial yes are not always enough, even
when the pair is indeed in agreement. Here the
alignment+ model aligns the italic sentences (Is
even ‘channel’ sufficiently ateleological a verb?
and [...]the word channel cannot be called teleo-
logical), finding them to be in agreement and thus
getting the correct classification.
</bodyText>
<sectionHeader confidence="0.993867" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999718333333333">
The incorporation of alignment-based features
shows promise in improving agreement classifica-
tion. Further ablation testing is needed to deter-
mine the full extent to which alignment features
contribute, and not only better whole-post features
on their own. However, given that many pairs do
not have sentences which align at all, alignment
features cannot classify on their own without some
more basic features to fill in the gaps.
</bodyText>
<figure confidence="0.990283111111111">
125
100
75
50
25
0
(−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) (2,3) (3,4) (4,5)
Agreement Score Range
(a) Baseline feature set classifications.
125
100
75
50
25
0
(−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) (2,3) (3,4) (4,5)
Agreement Score Range
(b) Alignment+ feature set classifications.
</figure>
<figureCaption confidence="0.999593">
Figure 3: Correct and incorrect classifications on
</figureCaption>
<bodyText confidence="0.875807866666667">
the test set given the corpus agreement scores, for
both feature sets. The gray area represents correct
classifications, while the black area represents in-
correct classifications.
Following previous work, we focused on pairs
judged as being in strong (dis)agreement. How
do systems fare when uncertain cases are present
in the training data? This has not been investi-
gated. One aspect of language interpretation, how-
ever, is its inherent uncertainty. In future work,
we will use the full IAC corpus, and instead of
drawing a binary distinction between strong agree-
ments and disagreements, have a three-way classi-
fication where unclear instances are also catego-
rized.
</bodyText>
<sectionHeader confidence="0.997485" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999880818181818">
We thank Christopher Potts and the members of
the Clippers group at The Ohio State University
for productive discussions about this work, as well
as our anonymous reviewers for their helpful com-
ments. We also thank Xuchen Yao for his tremen-
dous help with the Jacana aligner. This research
is supported in part by the NSF under Grant No.
1464252. Any opinions, findings, and conclusions
or recommendations expressed in this paper are
those of the authors and do not necessarily reflect
the views of the NSF.
</bodyText>
<figure confidence="0.925846">
Number of OR Pairs
Number of OR Pairs
</figure>
<page confidence="0.995931">
98
</page>
<sectionHeader confidence="0.988942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999758173913044">
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.
Fox Tree, Robeson Bowmani, and Joseph King.
2011. How can you say such things?!?: Recogniz-
ing disagreement in informal political argument. In
Proceedings of the Workshop on Languages in So-
cial Media, pages 2–11.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. In J. Quinonero-Candela, I. Dagan,
B. Magnini, and F. d’Alch´e-Buc, editors, Machine
Learning Challenges, Lecture Notes in Computer
Science, volume 3944, pages 177–190. Springer-
Verlag.
Marie-Catherine de Marneffe, Bill McCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC.
Danilo Giampiccolo, Ido Dagan, Bernardo Magnini,
and Bill Dolan. 2007. The third PASCAL recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 1–9.
Kazi Saidul Hasan and Vincent Ng. 2013. Extra-
linguistic constraints on stance recognition in ide-
ological debates. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 816–821.
Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, and
Yashar Mehdad. 2013. Combining intra-and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In ACL (1), pages 486–496.
Christopher D. Manning and Dan Klein. 2003.
Optimization, maxent models, and condi-
tional estimation without magic. In Tu-
torial at HLT-NAACL 2003 and ACL 2003.
http://nlp.stanford.edu/software/classifier.shtml.
Amita Misra and Marilyn A. Walker. 2013. Topic in-
dependent identification of agreement and disagree-
ment in social media dialogue. In Conference of the
Special Interest Group on Discourse and Dialogue,
pages 41–50.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219–230.
Kapil Thadani, Scott Martin, and Michael White.
2012. A joint phrasal and dependency model for
paraphrase alignment. In Proceedings of COLING
2012, pages 1229–1238. The COLING 2012 Orga-
nizing Committee.
Marilyn A. Walker, Jean E. Fox Tree, Pranav Anand,
Rob Abbott, and Joseph King. 2012. A corpus for
research on deliberation and debate. In Proceedings
of the 8th Language Resources and Evaluation Con-
ference, pages 812–817.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005. OpinionFinder: A system for subjectiv-
ity analysis. In Demonstration Description in Con-
ference on Empirical Methods in Natural Language
Processing, pages 34–35.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013. A lightweight and
high performance monolingual word aligner. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 702–
707.
</reference>
<page confidence="0.998951">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.261195">
<title confidence="0.79753825">I do not disagree: Leveraging monolingual alignment to disagreement in dialogue Gokcen de Linguistics</title>
<author confidence="0.571617">The Ohio State</author>
<email confidence="0.998989">gokcen.2@osu.edu,mcdm@ling.ohio-state.edu</email>
<abstract confidence="0.977943782608696">A wide array of natural dialogue discourse can be found on the internet. Previous attempts to automatically determine disagreement between interlocutors in such dialogue have mostly relied on n-gram and grammatical dependency features taken from respondent text. Agreement-disagreement classifiers built upon these baseline features tend to do poorly, yet have proven difficult to improve upon. Using the Internet Argument Corpus, which comprises quote and response post pairs taken from an online debate forum with human-annotated agreement scoring, we introduce semantic environment features derived by comparing quote and response sentences which align well. We show that this method improves classifier accuracy relative to the baseline method namely in the retrieval of disagreeing pairs, which improves from 69% to 77%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rob Abbott</author>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Joseph King</author>
</authors>
<title>How can you say such things?!?: Recognizing disagreement in informal political argument.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>2--11</pages>
<contexts>
<context position="1766" citStr="Abbott et al., 2011" startWordPosition="258" endWordPosition="262">need to automatically extract information that is expressed indirectly. Here we focus on identifying agreement and disagreement in online debate posts. Previous work on this task has used very shallow linguistic analysis: features are surface-level ones, such as ngrams, post initial unigrams, bigrams and trigrams (which aim at learning the discourse functions of discourse markers, e.g., well, really, you know), repeated sequential use of punctuation signs (e.g., !!, ?!). When automatically detecting (dis)agreement, these features fall short, reaching around 65% accuracy on a balanced dataset (Abbott et al., 2011; Misra and Walker, 2013). Adding extra-linguistic features, such as the structure of the post threads and stance of the post’s author on other subjects, boosts performance to 75% (Hasan and Ng, 2013). In this work, we leverage richer linguistic models to increase performance. Agreement may be explicitly marked. In example (1) in Table 2, the response-initial bigram I agree is a strong cue of agreement that surface features can learn, but there are more complex examples that surface features cannot capture. In example (2), the response-initial word Yes is not indicating agreement, despite bein</context>
<context position="7749" citStr="Abbott et al. (2011)" startWordPosition="1269" endWordPosition="1272">gue, independent of the structural information of post threads and author’s stance which might not always be recoverable. Full Data Set Balanced Training Set Disagree 5741 779 Neutral 3125 0 Agree 1113 779 Total 9980 1158 Table 2: Category counts in the training set. 2 Data We used the Internet Argument Corpus (IAC), a corpus of quote-response pairs annotated for agreement via Mechanical Turk (Walker et al., 2012). Agreement scores span from -5 (strong disagreement) and +5 (strong agreement). The distribution is shown in Figure 2. Because the original data skews toward disagreement, following Abbott et al. (2011), we created a balanced set, discarding “neutral” pairs between -1 and +1. We split the data into training, development and test sets. 1 Table 2 shows the category counts in the training set. 1We could not obtain the training-development-test split from Abbott et al. (2011). Our split is available at www.ling.ohio-state.edu/˜mcdm/data/ 2015/Balanced_IAC.zip. 95 2000 1500 1000 500 0 (−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) (2,3) (3,4) (4,5) Agreement Score Range (a) Full dataset. 400 300 200 100 0 (−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) (2,3) (3,4) (4,5) Agreement Score Range (b)</context>
<context position="9090" citStr="Abbott et al. (2011)" startWordPosition="1482" endWordPosition="1485">agreement, +5 is high agreement. 3 Features In this section, we detail the features of our model. We use the maximum entropy model as implemented in the Stanford CoreNLP toolset (Manning and Klein, 2003). Many of the features make use of the typed dependencies from the CoreNLP toolset (de Marneffe et al., 2006). For comparison, the baseline features attempt to replicate Abbott et al. (2011). 3.1 Baseline Features from Abbott et al. 2011 N-Grams. All unigrams, bigrams, and trigrams were taken from each response. Discourse Markers. In lieu of tracking discourse markers such as oh and so really, Abbott et al. (2011) tracked response-initial unigrams, bigrams, and trigrams. Typed Dependencies and MPQA. In addition to all dependencies from the response being used as features, dependencies were supplemented with MPQA sentiment values (Wilson et al., 2005). A dependency like (agree,I) would also yield the sentiment-dependency feature (positive,I), whereas (wrong, you) would also yield (negative,you). Punctuation. The presence of special punctuation such as repeated exclamation points (!!), question marks (??), and interrobang strings (?!) were tracked as binary features. 3.2 Alignment+ Features Our features </context>
</contexts>
<marker>Abbott, Walker, Anand, Tree, Bowmani, King, 2011</marker>
<rawString>Rob Abbott, Marilyn Walker, Pranav Anand, Jean E. Fox Tree, Robeson Bowmani, and Joseph King. 2011. How can you say such things?!?: Recognizing disagreement in informal political argument. In Proceedings of the Workshop on Languages in Social Media, pages 2–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>Machine Learning Challenges, Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<editor>In J. Quinonero-Candela, I. Dagan, B. Magnini, and F. d’Alch´e-Buc, editors,</editor>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="3510" citStr="Dagan et al. 2006" startWordPosition="558" endWordPosition="561">t most surface features fail to capture. Some discourse-level parsing (Joty et al., 2013) has been utilized in agreement detection, but most previous work does not take discourse structure into account: the response post is simply taken as a whole as the reply to the quote. To overcome this issue, we take advantage of the considerable progress in monolingual alignment (e.g., Thadani et al. 2012, Yao et al. 2013, Sultan et al. 2014) which allows us to align sentences of the quote to sentences in the response. This approach is reminiscent of the one used for Recognizing Textual Entailment (RTE, Dagan et al. 2006, Giampiccolo et al. 2007) where, given two short passages, systems identify whether the second passage follows from the first one according to the intuitions of an intelligent human reader. One common approach used in RTE was to align the two passages, and reason based on the alignment obtained. 94 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 94–99, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Quote Response Score 1 CC</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In J. Quinonero-Candela, I. Dagan, B. Magnini, and F. d’Alch´e-Buc, editors, Machine Learning Challenges, Lecture Notes in Computer Science, volume 3944, pages 177–190. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill McCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>de Marneffe, McCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill McCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Ido Dagan</author>
<author>Bernardo Magnini</author>
<author>Bill Dolan</author>
</authors>
<title>The third PASCAL recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="3536" citStr="Giampiccolo et al. 2007" startWordPosition="562" endWordPosition="565">ures fail to capture. Some discourse-level parsing (Joty et al., 2013) has been utilized in agreement detection, but most previous work does not take discourse structure into account: the response post is simply taken as a whole as the reply to the quote. To overcome this issue, we take advantage of the considerable progress in monolingual alignment (e.g., Thadani et al. 2012, Yao et al. 2013, Sultan et al. 2014) which allows us to align sentences of the quote to sentences in the response. This approach is reminiscent of the one used for Recognizing Textual Entailment (RTE, Dagan et al. 2006, Giampiccolo et al. 2007) where, given two short passages, systems identify whether the second passage follows from the first one according to the intuitions of an intelligent human reader. One common approach used in RTE was to align the two passages, and reason based on the alignment obtained. 94 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 94–99, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Quote Response Score 1 CCW LAWS ARE FOR TRACKING GU</context>
</contexts>
<marker>Giampiccolo, Dagan, Magnini, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Ido Dagan, Bernardo Magnini, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Extralinguistic constraints on stance recognition in ideological debates.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>816--821</pages>
<contexts>
<context position="1966" citStr="Hasan and Ng, 2013" startWordPosition="291" endWordPosition="294"> linguistic analysis: features are surface-level ones, such as ngrams, post initial unigrams, bigrams and trigrams (which aim at learning the discourse functions of discourse markers, e.g., well, really, you know), repeated sequential use of punctuation signs (e.g., !!, ?!). When automatically detecting (dis)agreement, these features fall short, reaching around 65% accuracy on a balanced dataset (Abbott et al., 2011; Misra and Walker, 2013). Adding extra-linguistic features, such as the structure of the post threads and stance of the post’s author on other subjects, boosts performance to 75% (Hasan and Ng, 2013). In this work, we leverage richer linguistic models to increase performance. Agreement may be explicitly marked. In example (1) in Table 2, the response-initial bigram I agree is a strong cue of agreement that surface features can learn, but there are more complex examples that surface features cannot capture. In example (2), the response-initial word Yes is not indicating agreement, despite being in general a good cue for it. Instead it is necessary to capture the polarity mismatch between the first sentence in the quote and the first sentence in the response (God doesn’t take away sinful de</context>
</contexts>
<marker>Hasan, Ng, 2013</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2013. Extralinguistic constraints on stance recognition in ideological debates. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 816–821.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining intra-and multisentential rhetorical parsing for document-level discourse analysis.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>486--496</pages>
<contexts>
<context position="2982" citStr="Joty et al., 2013" startWordPosition="467" endWordPosition="470">ing in general a good cue for it. Instead it is necessary to capture the polarity mismatch between the first sentence in the quote and the first sentence in the response (God doesn’t take away sinful desires vs. Yes, God does take away sinful desires) to infer that the response disagrees with the quote. There may also be mismatches of modality, as demonstrated in the third example (saw vs. may have believed). Here we also see an example of an explicit agreement word which is negated (that does not make it true) in a way that most surface features fail to capture. Some discourse-level parsing (Joty et al., 2013) has been utilized in agreement detection, but most previous work does not take discourse structure into account: the response post is simply taken as a whole as the reply to the quote. To overcome this issue, we take advantage of the considerable progress in monolingual alignment (e.g., Thadani et al. 2012, Yao et al. 2013, Sultan et al. 2014) which allows us to align sentences of the quote to sentences in the response. This approach is reminiscent of the one used for Recognizing Textual Entailment (RTE, Dagan et al. 2006, Giampiccolo et al. 2007) where, given two short passages, systems iden</context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, and Yashar Mehdad. 2013. Combining intra-and multisentential rhetorical parsing for document-level discourse analysis. In ACL (1), pages 486–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Dan Klein</author>
</authors>
<title>Optimization, maxent models, and conditional estimation without magic.</title>
<date>2003</date>
<booktitle>In Tutorial at HLT-NAACL 2003 and ACL</booktitle>
<note>http://nlp.stanford.edu/software/classifier.shtml.</note>
<contexts>
<context position="8673" citStr="Manning and Klein, 2003" startWordPosition="1412" endWordPosition="1416">t www.ling.ohio-state.edu/˜mcdm/data/ 2015/Balanced_IAC.zip. 95 2000 1500 1000 500 0 (−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) (2,3) (3,4) (4,5) Agreement Score Range (a) Full dataset. 400 300 200 100 0 (−5,−4)(−4,−3)(−3,−2)(−2,−1) (−1,0) (0,1) (1,2) (2,3) (3,4) (4,5) Agreement Score Range (b) Balanced training set. Figure 1: Agreement score distribution of the dataset, before and after balancing. -5 is high disagreement, +5 is high agreement. 3 Features In this section, we detail the features of our model. We use the maximum entropy model as implemented in the Stanford CoreNLP toolset (Manning and Klein, 2003). Many of the features make use of the typed dependencies from the CoreNLP toolset (de Marneffe et al., 2006). For comparison, the baseline features attempt to replicate Abbott et al. (2011). 3.1 Baseline Features from Abbott et al. 2011 N-Grams. All unigrams, bigrams, and trigrams were taken from each response. Discourse Markers. In lieu of tracking discourse markers such as oh and so really, Abbott et al. (2011) tracked response-initial unigrams, bigrams, and trigrams. Typed Dependencies and MPQA. In addition to all dependencies from the response being used as features, dependencies were sup</context>
</contexts>
<marker>Manning, Klein, 2003</marker>
<rawString>Christopher D. Manning and Dan Klein. 2003. Optimization, maxent models, and conditional estimation without magic. In Tutorial at HLT-NAACL 2003 and ACL 2003. http://nlp.stanford.edu/software/classifier.shtml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amita Misra</author>
<author>Marilyn A Walker</author>
</authors>
<title>Topic independent identification of agreement and disagreement in social media dialogue.</title>
<date>2013</date>
<booktitle>In Conference of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>41--50</pages>
<contexts>
<context position="1791" citStr="Misra and Walker, 2013" startWordPosition="263" endWordPosition="266"> extract information that is expressed indirectly. Here we focus on identifying agreement and disagreement in online debate posts. Previous work on this task has used very shallow linguistic analysis: features are surface-level ones, such as ngrams, post initial unigrams, bigrams and trigrams (which aim at learning the discourse functions of discourse markers, e.g., well, really, you know), repeated sequential use of punctuation signs (e.g., !!, ?!). When automatically detecting (dis)agreement, these features fall short, reaching around 65% accuracy on a balanced dataset (Abbott et al., 2011; Misra and Walker, 2013). Adding extra-linguistic features, such as the structure of the post threads and stance of the post’s author on other subjects, boosts performance to 75% (Hasan and Ng, 2013). In this work, we leverage richer linguistic models to increase performance. Agreement may be explicitly marked. In example (1) in Table 2, the response-initial bigram I agree is a strong cue of agreement that surface features can learn, but there are more complex examples that surface features cannot capture. In example (2), the response-initial word Yes is not indicating agreement, despite being in general a good cue f</context>
<context position="10407" citStr="Misra and Walker (2013)" startWordPosition="1682" endWordPosition="1685">the first sentence of the response in general. Tracking certain features in initial and aligned sentences proved more informative than doing the same without discerning location. Alignment scoring comes from running the Jacana aligner (Yao et al., 2013) pairwise on every sentence from each QR pair. Pairs of quote and response sentences with alignment scores above a threshold tuned on the development set are then analyzed for feature extraction. The sentence pair with the maximum alignment score for each post pair is also analyzed regardless of its meeting the threshold. Post Length. Following Misra and Walker (2013), we track various length features such as word count, sentence count, and average sentence length, including differentials of these measures between quote and response. Short responses (relative to both word-wise and sentence-wise counts) tend to correlate with agreement, while longer responses tend to correlate with disagreement. Emoticons. Emoticons are a popular way of communicating sentiment in internet text. Many emoticons in the corpus are in forum-specific code, such as emoticon rolleyes. We also detect a wider array of common emoticons as regular expressions beginning with colons, sem</context>
<context position="12631" citStr="Misra and Walker (2013)" startWordPosition="2038" endWordPosition="2041">ouns. The presence of first, second, and third person pronouns in the response are each tracked as binary features. The inclusion of personal pronouns in a post tends to indicate a more emotional or personal argument, especially second person pronouns. Explicit Truth Values. Rather than simply relying on n-gram-based tracking of explicit statements of agreement, we include as features polar (positive or negative) and modal (modal or nonmodal) context of instances of the words agree, disagree, true, false, right, and wrong found in the response, parallel to the agreement and denial tracking in Misra and Walker (2013). Polar context is determined by the presence or absence of negation modifiers (e.g., not, never) in the dependencies; modal context is determined by the presence of modal auxiliaries (e.g., might, could) and adverbs (e.g., possibly). Sentiment Scoring. Expanding on the use of MPQA sentiment values, we use the positive/negative/neutral and strong/weak classifications of the words in the MPQA lexicon to calculate sentiment scores of the posts and focal sentences (well-aligned sentences from the quote and response as well as the first sentence of the response). The scoring assigns a value to eac</context>
</contexts>
<marker>Misra, Walker, 2013</marker>
<rawString>Amita Misra and Marilyn A. Walker. 2013. Topic independent identification of agreement and disagreement in social media dialogue. In Conference of the Special Interest Group on Discourse and Dialogue, pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--219</pages>
<contexts>
<context position="3328" citStr="Sultan et al. 2014" startWordPosition="526" endWordPosition="529">, as demonstrated in the third example (saw vs. may have believed). Here we also see an example of an explicit agreement word which is negated (that does not make it true) in a way that most surface features fail to capture. Some discourse-level parsing (Joty et al., 2013) has been utilized in agreement detection, but most previous work does not take discourse structure into account: the response post is simply taken as a whole as the reply to the quote. To overcome this issue, we take advantage of the considerable progress in monolingual alignment (e.g., Thadani et al. 2012, Yao et al. 2013, Sultan et al. 2014) which allows us to align sentences of the quote to sentences in the response. This approach is reminiscent of the one used for Recognizing Textual Entailment (RTE, Dagan et al. 2006, Giampiccolo et al. 2007) where, given two short passages, systems identify whether the second passage follows from the first one according to the intuitions of an intelligent human reader. One common approach used in RTE was to align the two passages, and reason based on the alignment obtained. 94 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joi</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014. Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence. Transactions of the Association for Computational Linguistics, 2:219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
<author>Scott Martin</author>
<author>Michael White</author>
</authors>
<title>A joint phrasal and dependency model for paraphrase alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>1229--1238</pages>
<contexts>
<context position="3290" citStr="Thadani et al. 2012" startWordPosition="518" endWordPosition="521">ere may also be mismatches of modality, as demonstrated in the third example (saw vs. may have believed). Here we also see an example of an explicit agreement word which is negated (that does not make it true) in a way that most surface features fail to capture. Some discourse-level parsing (Joty et al., 2013) has been utilized in agreement detection, but most previous work does not take discourse structure into account: the response post is simply taken as a whole as the reply to the quote. To overcome this issue, we take advantage of the considerable progress in monolingual alignment (e.g., Thadani et al. 2012, Yao et al. 2013, Sultan et al. 2014) which allows us to align sentences of the quote to sentences in the response. This approach is reminiscent of the one used for Recognizing Textual Entailment (RTE, Dagan et al. 2006, Giampiccolo et al. 2007) where, given two short passages, systems identify whether the second passage follows from the first one according to the intuitions of an intelligent human reader. One common approach used in RTE was to align the two passages, and reason based on the alignment obtained. 94 Proceedings of the 53rd Annual Meeting of the Association for Computational Lin</context>
</contexts>
<marker>Thadani, Martin, White, 2012</marker>
<rawString>Kapil Thadani, Scott Martin, and Michael White. 2012. A joint phrasal and dependency model for paraphrase alignment. In Proceedings of COLING 2012, pages 1229–1238. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Jean E Fox Tree</author>
<author>Pranav Anand</author>
<author>Rob Abbott</author>
<author>Joseph King</author>
</authors>
<title>A corpus for research on deliberation and debate.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th Language Resources and Evaluation Conference,</booktitle>
<pages>812--817</pages>
<contexts>
<context position="7546" citStr="Walker et al., 2012" startWordPosition="1237" endWordPosition="1240">del based on alignment significantly outperforms a baseline bag-of-words model in the recall of disagreeing quote-response (QR) pairs. Such linguistic models will transfer more easily to any debate dialogue, independent of the structural information of post threads and author’s stance which might not always be recoverable. Full Data Set Balanced Training Set Disagree 5741 779 Neutral 3125 0 Agree 1113 779 Total 9980 1158 Table 2: Category counts in the training set. 2 Data We used the Internet Argument Corpus (IAC), a corpus of quote-response pairs annotated for agreement via Mechanical Turk (Walker et al., 2012). Agreement scores span from -5 (strong disagreement) and +5 (strong agreement). The distribution is shown in Figure 2. Because the original data skews toward disagreement, following Abbott et al. (2011), we created a balanced set, discarding “neutral” pairs between -1 and +1. We split the data into training, development and test sets. 1 Table 2 shows the category counts in the training set. 1We could not obtain the training-development-test split from Abbott et al. (2011). Our split is available at www.ling.ohio-state.edu/˜mcdm/data/ 2015/Balanced_IAC.zip. 95 2000 1500 1000 500 0 (−5,−4)(−4,−</context>
</contexts>
<marker>Walker, Tree, Anand, Abbott, King, 2012</marker>
<rawString>Marilyn A. Walker, Jean E. Fox Tree, Pranav Anand, Rob Abbott, and Joseph King. 2012. A corpus for research on deliberation and debate. In Proceedings of the 8th Language Resources and Evaluation Conference, pages 812–817.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>OpinionFinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Demonstration Description in Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>34--35</pages>
<contexts>
<context position="9331" citStr="Wilson et al., 2005" startWordPosition="1519" endWordPosition="1522"> typed dependencies from the CoreNLP toolset (de Marneffe et al., 2006). For comparison, the baseline features attempt to replicate Abbott et al. (2011). 3.1 Baseline Features from Abbott et al. 2011 N-Grams. All unigrams, bigrams, and trigrams were taken from each response. Discourse Markers. In lieu of tracking discourse markers such as oh and so really, Abbott et al. (2011) tracked response-initial unigrams, bigrams, and trigrams. Typed Dependencies and MPQA. In addition to all dependencies from the response being used as features, dependencies were supplemented with MPQA sentiment values (Wilson et al., 2005). A dependency like (agree,I) would also yield the sentiment-dependency feature (positive,I), whereas (wrong, you) would also yield (negative,you). Punctuation. The presence of special punctuation such as repeated exclamation points (!!), question marks (??), and interrobang strings (?!) were tracked as binary features. 3.2 Alignment+ Features Our features utilize focal sentences: not only wellaligned sentences from the quote and response, but also the first sentence of the response in general. Tracking certain features in initial and aligned sentences proved more informative than doing the sa</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. OpinionFinder: A system for subjectivity analysis. In Demonstration Description in Conference on Empirical Methods in Natural Language Processing, pages 34–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Chris CallisonBurch</author>
<author>Peter Clark</author>
</authors>
<title>A lightweight and high performance monolingual word aligner.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>702--707</pages>
<marker>Yao, Van Durme, CallisonBurch, Clark, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013. A lightweight and high performance monolingual word aligner. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 702– 707.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>