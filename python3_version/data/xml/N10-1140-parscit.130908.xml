<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000052">
<title confidence="0.843518">
Accurate Non-Hierarchical Phrase-Based Translation
</title>
<author confidence="0.980179">
Michel Galley and Christopher D. Manning
</author>
<affiliation confidence="0.9855275">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.942645">
Stanford, CA 94305
</address>
<email confidence="0.99969">
{mgalley,manning}@cs.stanford.edu
</email>
<sectionHeader confidence="0.994806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997350826086956">
A principal weakness of conventional (i.e.,
non-hierarchical) phrase-based statistical machine
translation is that it can only exploit continuous
phrases. In this paper, we extend phrase-based
decoding to allow both source and target phrasal
discontinuities, which provide better generalization
on unseen data and yield significant improvements
to a standard phrase-based system (Moses).
More interestingly, our discontinuous phrase-
based system also outperforms a state-of-the-art
hierarchical system (Joshua) by a very significant
margin (+1.03 BLEU on average on five Chinese-
English NIST test sets), even though both Joshua
and our system support discontinuous phrases.
Since the key difference between these two systems
is that ours is not hierarchical—i.e., our system
uses a string-based decoder instead of CKY, and it
imposes no hard hierarchical reordering constraints
during training and decoding—this paper sets
out to challenge the commonly held belief that
the tree-based parameterization of systems such
as Hiero and Joshua is crucial to their good
performance against Moses.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99640092">
Phrase-based machine translation models (Och and
Ney, 2004) advanced the state of the art by extend-
ing the basic translation unit from words to phrases.
By conditioning translations on more than a sin-
gle word, a statistical machine translation (SMT)
system benefits from the larger context of a phrase
pair to properly handle multi-word units and lo-
cal reorderings. Experimentally, it was found that
longer phrases yield better MT output (Koehn et al.,
2003). However, while it is computationally feasi-
ble at training time to extract phrase pairs of nearly
unbounded size (Zhang and Vogel, 2005; Callison-
Burch et al., 2005), phrase pairs applicable at test
time tend to be fairly short. Indeed, data sparsity
often forces conventional phrase-based systems to
segment test sentences into small phrases, and there-
fore to translate dependent words (e.g., the French
ne ... pas) separately instead of jointly.
We present a solution to this sparsity problem by
going beyond using only continuous phrases, and
instead define our translation unit as any subset of
words of a sentence, i.e., a discontinuous phrase.
We generalize conventional multi-beam string-based
decoding (Koehn, 2004) to allow variable-size dis-
continuities in both source and target phrases. Since
each sentence pair can be more flexibly decomposed
into translation units, it is possible to exploit the rich
context of longer (possibly discontinuous) phrases
to improve translation quality. Our decoder provides
two extensions to Moses (Koehn et al., 2007): (a) to
cope with source gaps, we follow (Lopez, 2007) to
efficiently find all discontinuous phrases in the train-
ing data that also appear in the input sentence; (b) to
enable target discontinuities, we augment transla-
tion hypotheses to not only record the current par-
tial translation, but also a set of subphrases that may
be appended to the partial translation at some later
stages of decoding. With these enhancements, our
best discontinuous system outperforms Moses with
lexicalized reordering by 0.77 BLEU and 1.53 TER
points on average.
We also show that our approach compares favor-
ably to binary synchronous context-free grammar
(2-SCFG) systems such as Hiero (Chiang, 2007),
even though 2-SCFG systems also allow phrasal dis-
continuities. Part of this difference may be due to a
difference of expressiveness, since 2-SCFG models
impose hard hierarchical constraints that our mod-
els do not impose. Recent work (Wellington et
al., 2006; Søgaard and Kuhn, 2009; Søgaard and
</bodyText>
<page confidence="0.972157">
966
</page>
<note confidence="0.758801">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.9937791">
bm dn ap ct
target:
ak am
b� bn
ai bj ak
bi am bn
(i)
s�urce: a� bj ck di
(ii)
ai bj (iii)
</figure>
<figureCaption confidence="0.764984142857143">
Figure 1: 2-SCFG systems such as Hiero are unable to in-
dependently generate translation units a, b, c, and d with
the following types of alignments: (i) inside-out (Wu,
1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009);
(iii) “bonbon” (Simard et al., 2005). Standard phrase-
based decoders cope with (i), but not (ii) and (iii). Our
phrase-based decoder handles all three cases.
</figureCaption>
<bodyText confidence="0.999891833333333">
Wu, 2009) has questioned the empirical adequacy of
2-SCFG systems, which are unable to perform any
of the transformations shown in Fig. 1. For instance,
using manually-aligned bitexts for 12 European lan-
guages pairs, Søgaard and Kuhn found that inside-
out and cross-serial discontinuous translation units
(DTU) account for 1.6% (Danish-English) to 18.6%
(French-English) of all translation units. The em-
pirical adequacy of 2-SCFG models would presum-
ably be lower with automatically-aligned texts and if
the study also included non-European languages. In
contrast, phrase-based systems can properly handle
inside-out alignments when used with a reasonably
large distortion limit, and all configurations in Fig. 1
are accounted for in our system. In our experiments,
we show that our discontinuous phrase-based sys-
tem outperforms Joshua (Li et al., 2009), a reimple-
mentation of Hiero, by 1.03 BLEU points and 1.19
TER points on average. A final compelling advan-
tage of our decoder is that it preserves the compu-
tational efficiency of Moses (i.e., time complexity is
linear when a distortion limit is used), while SCFG
decoders have a running time that is at least cubic
(Huang et al., 2005).
</bodyText>
<sectionHeader confidence="0.969931" genericHeader="introduction">
2 Discontinuous Phrase Extraction
</sectionHeader>
<bodyText confidence="0.9855988">
In this section, we introduce the extraction of dis-
continuous phrases for phrase-based MT. We will
describe a decoder that can handle such phrases
in the next section. Formally, we define the dis-
continuous phrase-based translation problem as fol-
lows. We are given a source sentence f = fJ1 =
f1, ... , fj, ... , fJ, which is to be translated into a
target sentence e = e�1 = e1, ... , eZ, ... , f j. Un-
like (Och and Ney, 2004), in this work, a sentence
pair may be segmented into phrases that are not con-
</bodyText>
<figureCaption confidence="0.944174333333333">
Figure 2: Due to hierarchical constraints, Hiero only ex-
tracts two discontinuous phrases from the alignment on
the left, but our system extracts 11 (only 6 are shown).
</figureCaption>
<bodyText confidence="0.999105">
tinuous, so each phrase is characterized by a cover-
age set, i.e., a set of word indices. Assuming that
the sentence pair (f, e) is decomposed into K dis-
continuous phrases, we use s = (s1, ... , sK) and
t = (t1, ... , tK) to respectively represent the de-
composition of the source and target sentence into
K word subsets that are complementary and non-
overlapping. A pair of coverage sets (sk, tk) is said
to be consistent with the word alignment A if the
following condition holds:
</bodyText>
<equation confidence="0.949976">
b(i,j) E A : i E sk H j E tk (1)
</equation>
<bodyText confidence="0.999963875">
For continuous phrases, finding all phrase pairs
that satisfy this condition can be done in O(nm3)
time (Och and Ney, 2004), where n is the length of
the sentence and m is the maximum phrase length.
The set of discontinuous phrases is exponential in
the maximum span length, so phrase extraction must
be tailored to a specific text (e.g., a given test sen-
tence) for relatively large m values. Lopez (2007)
presents an efficient solution using suffix arrays for
finding all discontinuous phrases of the training data
that are relevant to a given test sentence or test set.
A complete overview of this technique is beyond
the scope of this paper, though we will mention
that it solves a phrase collocation problem by effi-
ciently identifying collocated continuous phrases of
the training data that also happen to be collocated in
the test sentence. While this technique was primar-
ily designed for extracting hierarchical phrases for
Hiero (Chiang, 2007), it can readily be applied to
the problem of finding all discontinuous phrases for
our phrase-based system. Indeed, the suffix-array
technique gives us for each input sentence a list of
relevant source coverage sets. For each such sk, we
can easily enumerate each tk satisfying Eq. 1. The
</bodyText>
<figure confidence="0.94296024137931">
veux ... jouer do ... want to p(ay
ne veux p(us do not want ... anymore
je ne veux p(us I do not want ... anymore
do not want X anymore
I do not want X anymore
do ... want
not ... anymore
I ... not ... anymore
I
do
not
want
to
p(ay
anymore
veux
ne ... p(us
je ne ... p(us
je
ne
veux
p(us
jouer
Hiero:
This work:
ne veux p(us X
je ne veux p(us X
967
source: fA 1A i-Er± t a 1 17 &apos;f/Fffi lc-�r
</figure>
<figureCaption confidence="0.9030345">
Figure 3: A particular decoder search path for the input shown at the top. Note that this example contains a cross-serial
DTU (which interleaves arrangements ... made with are ... for this), a structure Hiero can’t handle.
</figureCaption>
<figure confidence="0.9990229">
he
said
one
access
1 f0r this
00000-00
score = -7*2
translation
options
(subset):
he said
are
are ... for this
visit
arrangements
arrangements ... made
to
make
arrangements
made
00-00
score = -3*2
are
for this I made
00000-00
score = -4*8
1
made
for this
00000-00
score = -6*1
this
state
expansions:
he said
00
score = -1*3
visit
000000000
score = -8*5
</figure>
<bodyText confidence="0.999881714285714">
key difference between Hiero-style extraction and
our work is that Eq. 1 is the only constraint.1 Since
our decoder doesn’t impose hierarchical constraints,
we exploit all discontinuous phrase pairs consis-
tent with the word alignment, which often includes
sound translations not captured by Hiero (e.g., ne ...
plus translating to not ... anymore in Fig. 2).
</bodyText>
<sectionHeader confidence="0.998254" genericHeader="method">
3 Decoder
</sectionHeader>
<bodyText confidence="0.979813538461539">
The core engine of our phrase-based system, Phrasal
(Cer et al., 2010), is a multi-stack decoder similar to
Moses (Koehn, 2004), which we extended to sup-
port variable-size gaps in the source and the target.
In Moses, partial translation hypotheses are arranged
into different stacks according to the total number of
input words they cover. At every translation step,
stacks are pruned using partial translation cost and a
lower bound on the estimated future cost. Pruning
is implemented using both threshold and histogram
pruning, and Moses allows for hypothesis recombi-
nation between hypotheses that are indistinguishable
according to the underlying models.
The key difference between Moses and our sys-
tem is that, in order to account for target disconti-
nuities, phrases that contains gaps in the target are
appended to a partial translation hypothesis in mul-
tiple steps. Specifically, each translation hypothesis
in our decoder is not only represented as a transla-
tion prefix and a coverage set as in Moses, but it also
contains a set of isolated phrases (shown in italic in
Fig. 3) that must be added to the translation at some
later time. For instance, the figure shows how the
1In order to keep the number of phrases manageable, we
additionally require that each (maximal) contiguous substring
of sk and tk be connected with at least one word alignment.
</bodyText>
<figure confidence="0.960087588235294">
Beam search algorithm.
1 create initial hypothesis HO; add it to Sg0
2 forj=0toJ
3 if j &gt; 0 then
4 for n = 1 to N
5 for each Hnew in consolidate(Hjcn)
6 add Hnew to Sgj
7 if j &lt; J then
8 for n = 1 to N
9 Hold := Hgjn
10 u := first uncovered source word of Hold
11 for m = u to u + distortionLimit
12 for each (sk, tk) in translation options(m)
13 if source sk does not overlap Hold then
14 Hnew :=combine(Hold, sk, tk)
15 add Hnew to Scj+l, where l = �sk�
16 return arg max(Sig)
</figure>
<tableCaption confidence="0.971975">
Table 1: Discontinuous phrase-based MT.
</tableCaption>
<bodyText confidence="0.989459764705882">
phrase pair (T&gt;#��, arrangements ... made) is be-
ing added to a partial translation. The prefix (ar-
rangements) is immediately appended to form the
hypothesis (he said arrangements), and the isolated
phrase (made) is stored for later use.
A beam search algorithm for discontinuous
phrase-based MT is shown in Table 1. Pruning is
done implicitly in the table to avoid cluttering the
pseudo-code. The algorithm handles 2J + 1 stacks
Sg�, Sg1, ... , SgJ and Sc1, ... , ScJ, where each stack
may contain up to N hypotheses Hj1, ... , HjN.
The main loop of the algorithm alternates two
stages: grow (lines 7–15) and consolidate (lines 3–
6).2 The grow stage is similar to standard phrase-
2The distinction between S9 and Si stacks ensures that the
consolidate operation does not read and write hypotheses on the
same stack. While it may seem effective to store hypotheses in
</bodyText>
<page confidence="0.990232">
968
</page>
<bodyText confidence="0.9999808">
based MT: we take a hypothesis Hgjn from Sg jand
combine it with a translation option (sk, tk), which
yields a new hypothesis that is added to stack Scj+l
(where l = |sk|). The second stage, consolidate, lets
the decoder select any number of isolated phrases
(not necessarily all, and possibly zero) and append
them in any order at the end of the current trans-
lation.3 Consolidation operations are marked with
stars in the figure (for simplicity, the figure does
not display consolidations that keep hypotheses un-
changed). We limit the number of isolated phrases
to 4, which is generally enough to account for most
transformations seen in the data. Any hypothesis in
the last beam SgJ is automatically discarded if it con-
tains any isolated phrase.
One last difference with standard decoders is
that we also handle source discontinuities. This
problem is a known instance of MT by pattern
matching (Lopez, 2007), which we already men-
tioned in the previous section. The function transla-
tion options(m) of Table 1 returns the set of options
applicable at position m using this pattern match-
ing algorithm. Since this function is invoked a large
number of times, it is important to precompute its
return values for each m prior to decoding.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.984605625">
Our system incorporates the same eight baseline fea-
tures of Moses: two relative-frequency phrase trans-
lation probabilities p(e|f) and p(f|e), two lexically-
weighted phrase translation probabilities (Koehn et
al., 2003) lex(e|f) and lex(f|e), a language model
probability, word penalty, phrase penalty, and linear
distortion, and we optionally add 6 lexicalized re-
ordering features as computed in Moses.
Our computation of linear distortion is different
from the one in Moses, since we need to account
for discontinuous phrases. We found that it is
crucial to penalize discontinuous phrases that have
relatively long gaps. Hence, in our computation of
different stacks depending on the number of isolated phrases,
we have not found various implementations of this idea to work
better than the algorithm described here.
</bodyText>
<footnote confidence="0.9266625">
3We let isolated phrases be reordered freely, with only three
constraints: (1) the internal word order must be preserved, i.e., a
phrase may not be split or reordered. (2) isolated phrases drawn
from the same discontinuous phrase must appear in the specified
order (i.e., the phrase A ... B ... C may not yield the translation
A ... C ... B). (3) Empty gaps are forbidden.
</footnote>
<figureCaption confidence="0.9865505">
Figure 4: Linear distortion computed using both continu-
ous and discontinuous phrase.
</figureCaption>
<bodyText confidence="0.994115818181818">
linear distortion, we treat continuous subphrases
of each discontinuous phrase as if they were
continuous phrases on their own. Specifically,
let ¯s = (91, ... , sL) be the list of L (maximal)
continuous subphrases of the K source phrases
(L &gt; K) selected for a given translation hypothesis.
Subphrases in ¯s are enumerated according to their
order in the target language, which may be different
from the source-side ordering. We then compute
the linear distortion between pair of successive
elements (si, si+1) as follows:
</bodyText>
<equation confidence="0.999648">
d(¯s) = �sfirst
1 +
</equation>
<bodyText confidence="0.999973884615385">
where the superscripts first and last respectively
refer to source position of the first and last word
of a given subphrase. Fig. 4 shows an example of
how distortion is computed for phrases (s1, s2, s3),
including the discontinuous phrase s2 split into three
continuous subphrases. In practice, we compute
intra-phrase (shown with thin arrows in the figure)
and inter-phrase linear distortion separately in order
to produce two distinct features, since translation
tends to improves when the intra-phrase cost has a
lower feature weight.
Finally, we add two features that are not present
in Moses. First, we penalize target discontinuities
by including a feature that is the sum of the lengths
of all target gaps. The second feature is the count
of discontinuous phrases that are in configurations
(cross-serial DTU (Søgaard and Kuhn, 2009) and
“bonbon” (Simard et al., 2005)) that can’t be han-
dled by 2-SCFG systems. The advantage of such
features is two-fold. First, similarly to hierarchi-
cal systems, they prevent many distorted reorderings
that are unlikely to correspond to quality transla-
tions. Second, it imposes soft rather than hard con-
straints, which means that the decoder is entirely
free to violate hierarchical constraints when these
violations are supported by other features.
</bodyText>
<equation confidence="0.722276666666667">
last
first
si−1 + 1 − Si
L
i
i=2
</equation>
<page confidence="0.995266">
969
</page>
<sectionHeader confidence="0.99837" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999991243243244">
Three systems are evaluated in this paper: Moses
(Koehn et al., 2007), Joshua (Li et al., 2009) – a
reimplementation of Hiero, and our phrase-based
system. We made our best attempts to make our sys-
tem comparable to Moses. That is, when no discon-
tinuous phrases are provided to our system, it gener-
ates an output that is almost identical to Moses (only
about 1% of translations differ on average). In both
systems, we use the default settings of Moses, i.e.,
we set the beam size to 200, the distortion limit to 6,
we limit to 20 the number of target phrases that are
loaded for each source phrase, and we use the same
default eight features of Moses. We use version 1.3
of Joshua with its default settings. Both Moses and
our system are evaluated with and without lexical-
ized reordering (Tillmann, 2004).4 We believe it
to be fair to compare Joshua against phrase-based
systems that exploit lexicalized reordering, since Hi-
ero’s hierarchical rules are also lexically sensitive.5
The language pair for our experiments is Chinese-
to-English. The training data consists of about 28
million English words and 23.3 million Chinese
words drawn from various news parallel corpora dis-
tributed by the Linguistic Data Consortium (LDC).
In order to provide experiments comparable to previ-
ous work, we used the same corpora as (Wang et al.,
2007). We performed word alignment using a cross-
EM word aligner (Liang et al., 2006). For this, we
ran two iterations of IBM Model 1 and two HMM
iterations. Finally, we generated a symmetric word
alignment from cross-EM Viterbi alignment using
the Moses grow-diag heuristic in the case Moses and
our system. In the case of Joshua, we used the grow-
diag-final heuristic since this gave better results.
In order to train a competitive baseline given our
computational resources, we built a large 5-gram
language model using the Xinhua and AFP sections
</bodyText>
<footnote confidence="0.840855">
4We use Moses’ default orientations: monotone, swap, and
discontinuous. As far as this reordering model is concerned,
we treat discontinuous phrases as continuous, i.e., we simply
ignore what lies within gaps to determine phrase orientation.
5(Tillmann, 2004) learns for each phrase a tendency to ei-
ther remain monotone or to swap with other phrases. As noted
in (Lopez, 2008), Hiero can represent the same information
with hierarchical rules of the form uX, Xu, and XuX. Hi-
ero actually models lexicalized reordering patterns that (Till-
mann, 2004) does not account for, e.g., a transformation from
X1uX2v to X2u′v′X1.
</footnote>
<bodyText confidence="0.99975044">
of the Gigaword corpus (LDC2007T40) in addition
to the target side of the parallel data. This data rep-
resents a total of about 700 million words. We man-
ually removed documents of Gigaword that were re-
leased during periods that overlap with those of our
development and test sets. The language model was
smoothed with the modified Kneser-Ney algorithm
as implemented in SRILM (Stolcke, 2002), and we
only kept 4-grams and 5-grams that occurred at least
three times in the training data.
For tuning and testing, we use the official NIST
MT evaluation data for Chinese from 2003 to 2008
(MT03 to MT08), which all have four English ref-
erences for each input sentence. We used the 1664
sentences of MT06 for tuning and development and
all other sets for testing. Parameter tuning was
done with minimum error rate training (Och, 2003),
which was used to maximize IBM BLEU-4 (Pap-
ineni et al., 2001). Since MERT is prone to search
errors, especially with large numbers of parameters,
we ran each tuning experiment four times with dif-
ferent initial conditions. We used n-best lists of size
200. In the final evaluations, we report results using
both TER version 0.7.25 (Snover et al., 2006) and
BLEU-4 (both uncased).
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999987714285714">
We start by comparing some translations generated
by the best configurations of Joshua, Moses, and our
phrase-based decoder, systems we will empirically
evaluate later in this section. Fig. 5 shows trans-
lations of our development set MT06, which were
selected because our system makes a crucial use of
discontinuous phrases. In the first example, the Chi-
nese input contains&apos;�...Hj, which typically trans-
lates as when. Lacking an entry for the input phrase
Hj in its phrase table, Moses is
unable to translate this segment appropriately, and
must instead split this phrase to generate the trans-
lation when the right was deprived of, where Hj is
translated into of. This is evidently a poor transla-
tion. Conversely, our system uses a discontinuous
phrase to translate&apos;�...Hj, and translates the inter-
vening words separately.
The remaining three translations all contain cross-
serial DTUs (Søgaard and Kuhn, 2009) and thus
would be difficult to generate using 2-SCFG sys-
tems. The second example motivates the idea
</bodyText>
<page confidence="0.977551">
970
</page>
<figure confidence="0.979809652173913">
t
tap-V
aME
(AÍÅµ_F
1!1
,�
, it
right to life
,
Hf
, when
he was
deprived of the
can only
resort to violence
...
under such circumstances
��4j7
C,
�f_€&apos;z
...
MT06 — segment 1589
Reference: Under such cir-
</figure>
<bodyText confidence="0.9925322">
cumstances, when the right
of existence was deprived,
the only way remaining was
to overthrow the existing
dynasty by force and try to
replace it.
Joshua: Under such cir-
cumstances, when life be
deprived, can only resort to
violence to overthrow the
current dynasty, trying to re-
place,
Moses: Under such circum-
stances, when the right was
deprived of, can only adopt
the means of violence, in
an attempt to overthrow the
present dynasty replaced,
This work: Under such cir-
cumstances, when he was de-
prived of the right to life, it
can only resort to violence in
an attempt to overthrow the
current dynasty replaced,
in this kind case when life right was deprive when only can use violence of means
</bodyText>
<equation confidence="0.483991">
MT06 — segment 1044
</equation>
<figureCaption confidence="0.985999375">
Reference: CCP organi-
zation ministry demands
to further enlarge strength
of supervision of leading
cadres and cadre selection
and appointment
Joshua: Department de-
mands further intensify su-
pervision over the work
of selecting and appointing
leading cadres, and inten-
sify
Moses: The central organi-
zation department, called on
leading cadres, further in-
crease the intensity of super-
vision over work of selecting
and appointing cadres.
This work: The central orga-
nization department has called
for further increase the inten-
sity of supervision of leading
cadres and the work of select-
ing and appointing cadres.
</figureCaption>
<figure confidence="0.99872825">
CCP request further increase to leading cadres and cadre selection appointment work of supervision intensity
FP�n%
VSt—;7�
Vt
VI†�Tn%
jjt
fIQ
C,
Tn%At�IFNTT IF
...
has called for further
and ...
the central organization department
increase the intensity of
supervision of leading cadres
MT06 — segment 559
</figure>
<figureCaption confidence="0.885208529411765">
Reference: The government
will take all possible mea-
sures to prevent similar inci-
dents from happening in the
future.
Joshua: Government will
take all measures to prevent
the re-occurrence of similar
incidents in the future.
Moses: The government will
take all measures to prevent
the occurrence of similar inci-
dents in the future.
This work: The government
will take all measures to pre-
vent similar incidents from
happening again in the future.
</figureCaption>
<figure confidence="0.953409222222222">
government will take all measure to prevent future again happen similar of incidents
F T
��—_W
Mrtt
I3q±
1Æ
f�`Qt
C,
T�
in the future.
the government
will
take all
prevent similar
measures to
incidents from happening again
MT06 — segment 769
Reference: He also said that
</figure>
<bodyText confidence="0.910445363636364">
the arrangements are being
made now for the visits.
Joshua: He also said that
now is making arrange-
ments for this visit.
Moses: He also said that the
current visit is to make ar-
rangements.
This work: He also said that
the current arrangements are
made for the visit.
</bodyText>
<figure confidence="0.951464076923077">
he also said now are for this one visit make arrangements
f- ftL
he also
tNLr
IErtA
—ifiP-1
T IFHIt�
arrangements
are
made
for the
visit.
said that the current
</figure>
<figureCaption confidence="0.99449275">
Figure 5: Actual translations produced by Joshua, Moses, and our system. For our system, we also display phrase
alignments, including discontinuous phrase alignments. Results for these three systems here are displayed in rows 2,
4, and 8 of Table 2. The thick blue arrows represent alignments between discontinuous phrases, while red segmented
arrows align continuous phrases.
</figureCaption>
<page confidence="0.967652">
971
</page>
<table confidence="0.999826533333333">
System Gaps LexR MT06 (tune) BLEU MT03 BLEU MT04 BLEU MT05 BLEU MT08 BLEU ALL
BLEU TER TER TER TER TER TER
1 hierarchical src yes 33.55 58.04 33.25 59.73 36.03 58.92 32.03 61.11 26.30 61.30 31.70 58.21
2 (Joshua) src+tgt yes 33.84 58.11 33.47 59.85 36.10 58.82 32.17 61.20 26.61 61.21 31.90 58.22
3 phrase-based no no 33.17 59.24 32.60 60.80 35.38 59.55 31.15 62.43 25.56 61.98 31.08 59.14
4 (Moses) no yes 34.25 58.23 33.72 60.42 36.37 59.18 32.49 61.80 26.70 61.48 32.16 58.56
5 src no 33.77 58.56 33.20 60.42 36.17 59.13 31.75 61.62 25.99 61.47 31.68 58.60
6 7 tgt no 33.27 58.98 32.95 60.42 35.41 59.35 31.08 62.45 25.69 61.71 31.17 58.93
discontinuous src+tgt no 33.86 58.26 33.32 60.02 36.36 58.56 31.87 61.35 26.13 61.29 31.81 58.25
phrase-based
8 src+tgt yes 35.00 56.85 34.96 57.97 37.44 57.61 33.39 59.92 26.74 60.51 32.93 57.03
(this work)
Improvement over hierarchical +1.16 −1.26 +1.49 −1.88 +1.34 −1.21 +1.22 −1.28 +0.13 −0.70 +1.03 −1.19
Improvement over phrase-based +0.75 −1.38 +1.24 −2.45 +1.07 −1.57 +0.90 −1.88 +0.04 −0.97 +0.77 −1.53
Number of sentences 1664 919 1788 1082 1357 6810
</table>
<tableCaption confidence="0.95415225">
Table 2: Our system compared again conventional and hierarchical phrase-based MT (Moses and Joshua). using
uncased BLEUr4n4[%] and TER[%]. LexR indicates whether lexicalized reordering is enabled or not. We use ran-
domization tests (Riezler and Maxwell, 2005) to determine significance of our best results (row 8) against Joshua (row
2) and Moses (row 4): differences marked in bold are significant at the p &lt; .01 level.
</tableCaption>
<bodyText confidence="0.99993996969697">
that larger translation units, including discontinuous
phrases, lead to better translations. The reference in-
cludes the translation enlarge strength of supervision
of leading cadres, and our system is able to produce
a translation that is almost identical (increase the in-
tensity of supervision of leading cadres) using only
two phrases, pulling together input words that are
fairly far apart in the sentence. The third Chinese
sentence has a word order quite different from En-
glish, but our decoder flexibly reorders it in a manner
that can’t be handled with SCFG decoders to give
a word order (prevent similar events from happen-
ing) that matches the one in the reference. The last
Chinese sentence includes the topicalization word
�&apos;j(for), which indicates the input sentence has no
subject. One way to properly handle this translation
is to turn the sentence into a passive in English (as
in the reference), a transformation our system does,
thanks to its support for complex reorderings.
Our main results are displayed in Table 2. First,
Joshua systematically outperforms the Moses base-
line (+0.82 BLEU point and −0.92 TER point on
average), but performance of the two is about the
same when Moses incorporates lexicalized reorder-
ing. This finding is consistent with previous work
(Lopez, 2008). The results of our system displayed
in rows 5–8 demonstrate that our system consis-
tently outperforms Moses, whether they both use
lexicalized reordering or not. The performance of
our best system—i.e., with lexicalized reordering
and both source and target gaps—is significantly
better than the best Moses system (+0.77 BLEU
and −1.53 TER). While the performance of our sys-
tem without lexicalized reordering is close to that of
Joshua, our system with lexicalized reordering sig-
nificantly outperforms Joshua (p &lt; .01) in 9 out of
10 evaluations. The single experiment where our im-
provement over Hiero is insignificant (i.e., BLEU on
MT08) is mainly affected by a discrepancy of length
(our brevity penalty on MT08 is 0.92).
It is interesting to notice that our system allowing
phrasal discontinuities only on the source (row 5)
performs almost as well as the system that allows
them on both sides (row 7). For instance, while
source discontinuities improve performance by 0.7
BLEU point on MT06, further enabling target dis-
continuities only raises performance by a mere 0.09
BLEU point. This naturally raises the question of
whether our support for target gaps is ineffective,
or whether target-discontinuous phrases are some-
what superfluous to the MT task. While it is cer-
tainly difficult to either confirm or deny the latter
hypothesis, we can at least compare our handling of
target-discontinuous phrases with hierarchical sys-
tems. In one additional set of experiments, we re-
moved target-discontinuous phrases in Joshua prior
to MERT and test time. Specifically, we removed
all hierarchical phrases whose target side has the
form uXv, uXvX, XuXv, and uXvXw, and only
allowed rules whose target side has the form uX,
Xu, XuX, XXu, or uXX. After this filtering,
we found that target-discontinuous phrases in Joshua
are also not crucial to its performance, since their re-
moval only caused a drop of 0.2 BLEU point (row 1)
and almost no change in terms of TER. We speculate
that using target discontinuous phrases is more diffi-
</bodyText>
<page confidence="0.986872">
972
</page>
<figure confidence="0.451797">
# of English words per phrase
</figure>
<figureCaption confidence="0.998595">
Figure 6: Phrase length histogram for MT06.
</figureCaption>
<bodyText confidence="0.999746764705882">
cult, since it represents a generation rather than just
a matching problem.
In this paper, we have also argued that a main
benefit of discontinuous phrases—and particularly
source-discontinuous phrases—is that the decoder is
allowed to use larger translation units than when re-
stricted to continuous phrases. This claim is con-
firmed in Fig. 6. We find that our decoder makes
effective use of the extended set of translation op-
tions at its disposal: While the Moses baseline trans-
lates MT06 with an average 1.73 words per phrase,
adding support for discontinuities increases this av-
erage to 2.16, and reduces by 43% the use of sin-
gle word phrases. On MT06, 53% of the translated
sentences produced by our best system use at least
one source-discontinuous phrase, and 9% of them
exploit one or more target-discontinuous phrases.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999985709677419">
The main goal of this paper is to show that discontin-
uous phrases can greatly improve the performance
of phrase-based systems. While some of the most
recent phrase-based systems (Chiang, 2007; Watan-
abe et al., 2006) exploit context-free decoding algo-
rithms (CKY, Earley, etc.) to cope with discontinu-
ities, our system preserves the simplicity and speed
of conventional phrase-based decoders, and in par-
ticular does not build any intermediate tree structure,
does not impose any hard reordering constraints
other than the distortion limit, and still achieves
translation performance that is superior to that of a
state-of-the-art hierarchical system.
A few previous non-hierarchical systems have
also exploited phrasal discontinuities. The most no-
table previous attempt to incorporate gaps is de-
scribed in (Simard et al., 2005). Simard et al.
presents an extension to Moses that allows gaps in
both source and target phrases, though each of their
gap symbols must span exactly one word. This fact
makes decoding simpler, since the position of all tar-
get words in a translation hypothesis is known as
soon as the hypothesis is laid down, but fixed-size
discontinuous phrases are less general and increase
sparsity. By comparison, our gaps may span any
number of words, so we have an increased ability to
flexibly match the input sentence effectively. (Crego
and Yvon, 2009) also handles gaps, though this work
is applicable to an n-gram-based SMT framework
(Mari`oo et al., 2006), which is fairly different from
the phrase-based framework.
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99995195">
In this paper, we presented a generalization of con-
ventional phrase-based decoding to handle discon-
tinuities in both source and target phrases. Our
system significantly outperforms Moses and Joshua,
two standard implementations of conventional and
hierarchical phrase-based decoding. We found that
allowing discontinuities in the source is more use-
ful than target discontinuities in our system, though
we found that this turns out to also be the case with
the hierarchical phrases of Joshua. In future work,
we plan to extend the parameterization of phrase-
based lexicalized reordering models to be sensitive
to these discontinuities, and we will also consider
adding syntactic features to our models to penal-
ize discontinuities that are not syntactically moti-
vated (Marton and Resnik, 2008; Chiang et al.,
2009). The discontinuous phrase-based MT system
described in this work is part of Phrasal, an open-
source phrase-based system available for download
at http://nlp.stanford.edu/software/phrasal.
</bodyText>
<sectionHeader confidence="0.994208" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999850625">
The authors thank three anonymous reviewers, Dan
Jurafsky, Spence Green, Steven Bethard, Daniel Cer,
Chris Callison-Burch, and Pi-Chuan Chang for their
helpful comments. This paper is based on work
funded by the Defense Advanced Research Projects
Agency through IBM. The content does not neces-
sarily reflect the views of the U.S. Government, and
no official endorsement should be inferred.
</bodyText>
<figure confidence="0.997896375">
Moses
this work
1 2 3 4 5 6 7
15000
10000
5000
0
word mass
</figure>
<page confidence="0.996421">
973
</page>
<sectionHeader confidence="0.993506" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999609733333333">
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statistical
machine translation to larger corpora and longer
phrases. In Proc. of ACL, pages 255–262.
Daniel Cer, Michel Galley, Dan Jurafsky, and Christo-
pher Manning. 2010. Phrasal: A statistical machine
translation toolkit for exploring new model features.
In Proc. of NAACL-HLT, Demonstration Session.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. of NAACL, pages 218–226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Josep Crego and Frangois Yvon. 2009. Gappy transla-
tion units under left-to-right SMT decoding. In Proc.
of EAMT.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks. In
Proc. of the Ninth International Workshop on Parsing
Technology, pages 65–73.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer, Ondrej
Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. ofACL, Demonstration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. ofAMTA, pages 115–124.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based MT. In Proc. of WMT.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. of HLT-NAACL, pages
104–111.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proc. of EMNLP-CoNLL,
pages 976–985.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. of COLING.
Jos´e B. Mari`oo, Rafael E. Banchs, Josep M. Crego, Adri`a
de Gispert, Patrik Lambert, Jos´e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527–
549.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. of ACL, pages 1003–1011.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of ACL.
Stefan Riezler and John T. Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proc. of Workshop on Evaluation Mea-
sures, pages 57–64.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Ya-
mada, Philippe Langlais, and Arne Mauser. 2005.
Translating with non-contiguous phrases. In Proc. of
HLT-EMNLP, pages 755–762.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA, pages 223–231.
Anders Søgaard and Jonas Kuhn. 2009. Empirical lower
bounds on alignment error rates in syntax-based ma-
chine translation. In Proc. of the Third Workshop on
Syntax and Structure in Statistical Translation (SSST-
3) at NAACL HLT 2009, pages 19–27.
Anders Søgaard and Dekai Wu. 2009. Empirical lower
bounds on translation unit error rate for the full class
of inversion transduction grammars. In Proc. of IWPT,
pages 33–36.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proc. of ICSLP, pages 901–904.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
NAACL, pages 101–104.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proc. of EMNLP-CoNLL.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proc. of
COLING-ACL, pages 977–984.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Ying Zhang and Stephan Vogel. 2005. An efficient
phrase-to-phrase alignment model for arbitrarily long
phrase and large corpora. In Proc. of EAMT.
</reference>
<page confidence="0.99868">
974
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.644856">
<title confidence="0.999617">Accurate Non-Hierarchical Phrase-Based Translation</title>
<author confidence="0.996241">D Galley</author>
<affiliation confidence="0.835947">Computer Science Stanford</affiliation>
<address confidence="0.998783">Stanford, CA 94305</address>
<abstract confidence="0.99845">A principal weakness of conventional (i.e., non-hierarchical) phrase-based statistical machine translation is that it can only exploit continuous phrases. In this paper, we extend phrase-based decoding to allow both source and target phrasal discontinuities, which provide better generalization on unseen data and yield significant improvements to a standard phrase-based system (Moses). More interestingly, our discontinuous phrasebased system also outperforms a state-of-the-art hierarchical system (Joshua) by a very significant margin (+1.03 BLEU on average on five Chinese- English NIST test sets), even though both Joshua and our system support discontinuous phrases. Since the key difference between these two systems is that ours is not hierarchical—i.e., our system uses a string-based decoder instead of CKY, and it imposes no hard hierarchical reordering constraints during training and decoding—this paper sets out to challenge the commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>255--262</pages>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proc. of ACL, pages 255–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>Phrasal: A statistical machine translation toolkit for exploring new model features.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT, Demonstration Session.</booktitle>
<contexts>
<context position="9533" citStr="Cer et al., 2010" startWordPosition="1568" endWordPosition="1571">de 00-00 score = -3*2 are for this I made 00000-00 score = -4*8 1 made for this 00000-00 score = -6*1 this state expansions: he said 00 score = -1*3 visit 000000000 score = -8*5 key difference between Hiero-style extraction and our work is that Eq. 1 is the only constraint.1 Since our decoder doesn’t impose hierarchical constraints, we exploit all discontinuous phrase pairs consistent with the word alignment, which often includes sound translations not captured by Hiero (e.g., ne ... plus translating to not ... anymore in Fig. 2). 3 Decoder The core engine of our phrase-based system, Phrasal (Cer et al., 2010), is a multi-stack decoder similar to Moses (Koehn, 2004), which we extended to support variable-size gaps in the source and the target. In Moses, partial translation hypotheses are arranged into different stacks according to the total number of input words they cover. At every translation step, stacks are pruned using partial translation cost and a lower bound on the estimated future cost. Pruning is implemented using both threshold and histogram pruning, and Moses allows for hypothesis recombination between hypotheses that are indistinguishable according to the underlying models. The key dif</context>
</contexts>
<marker>Cer, Galley, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Michel Galley, Dan Jurafsky, and Christopher Manning. 2010. Phrasal: A statistical machine translation toolkit for exploring new model features. In Proc. of NAACL-HLT, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="32845" citStr="Chiang et al., 2009" startWordPosition="5424" endWordPosition="5427">Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that this turns out to also be the case with the hierarchical phrases of Joshua. In future work, we plan to extend the parameterization of phrasebased lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (Marton and Resnik, 2008; Chiang et al., 2009). The discontinuous phrase-based MT system described in this work is part of Phrasal, an opensource phrase-based system available for download at http://nlp.stanford.edu/software/phrasal. Acknowledgements The authors thank three anonymous reviewers, Dan Jurafsky, Spence Green, Steven Bethard, Daniel Cer, Chris Callison-Burch, and Pi-Chuan Chang for their helpful comments. This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. Moses</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proc. of NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3507" citStr="Chiang, 2007" startWordPosition="519" endWordPosition="520">y find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics bm dn ap ct target: ak am b� bn ai bj ak bi am bn (i) s�urce: a� bj ck di (ii) ai bj (iii) Figure 1: </context>
<context position="7836" citStr="Chiang, 2007" startWordPosition="1259" endWordPosition="1260">, a given test sentence) for relatively large m values. Lopez (2007) presents an efficient solution using suffix arrays for finding all discontinuous phrases of the training data that are relevant to a given test sentence or test set. A complete overview of this technique is beyond the scope of this paper, though we will mention that it solves a phrase collocation problem by efficiently identifying collocated continuous phrases of the training data that also happen to be collocated in the test sentence. While this technique was primarily designed for extracting hierarchical phrases for Hiero (Chiang, 2007), it can readily be applied to the problem of finding all discontinuous phrases for our phrase-based system. Indeed, the suffix-array technique gives us for each input sentence a list of relevant source coverage sets. For each such sk, we can easily enumerate each tk satisfying Eq. 1. The veux ... jouer do ... want to p(ay ne veux p(us do not want ... anymore je ne veux p(us I do not want ... anymore do not want X anymore I do not want X anymore do ... want not ... anymore I ... not ... anymore I do not want to p(ay anymore veux ne ... p(us je ne ... p(us je ne veux p(us jouer Hiero: This work</context>
<context position="30673" citStr="Chiang, 2007" startWordPosition="5089" endWordPosition="5090">on options at its disposal: While the Moses baseline translates MT06 with an average 1.73 words per phrase, adding support for discontinuities increases this average to 2.16, and reduces by 43% the use of single word phrases. On MT06, 53% of the translated sentences produced by our best system use at least one source-discontinuous phrase, and 9% of them exploit one or more target-discontinuous phrases. 7 Related Work The main goal of this paper is to show that discontinuous phrases can greatly improve the performance of phrase-based systems. While some of the most recent phrase-based systems (Chiang, 2007; Watanabe et al., 2006) exploit context-free decoding algorithms (CKY, Earley, etc.) to cope with discontinuities, our system preserves the simplicity and speed of conventional phrase-based decoders, and in particular does not build any intermediate tree structure, does not impose any hard reordering constraints other than the distortion limit, and still achieves translation performance that is superior to that of a state-of-the-art hierarchical system. A few previous non-hierarchical systems have also exploited phrasal discontinuities. The most notable previous attempt to incorporate gaps is</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Crego</author>
<author>Frangois Yvon</author>
</authors>
<title>Gappy translation units under left-to-right SMT decoding.</title>
<date>2009</date>
<booktitle>In Proc. of EAMT.</booktitle>
<contexts>
<context position="31853" citStr="Crego and Yvon, 2009" startWordPosition="5273" endWordPosition="5276">e previous attempt to incorporate gaps is described in (Simard et al., 2005). Simard et al. presents an extension to Moses that allows gaps in both source and target phrases, though each of their gap symbols must span exactly one word. This fact makes decoding simpler, since the position of all target words in a translation hypothesis is known as soon as the hypothesis is laid down, but fixed-size discontinuous phrases are less general and increase sparsity. By comparison, our gaps may span any number of words, so we have an increased ability to flexibly match the input sentence effectively. (Crego and Yvon, 2009) also handles gaps, though this work is applicable to an n-gram-based SMT framework (Mari`oo et al., 2006), which is fairly different from the phrase-based framework. 8 Conclusions In this paper, we presented a generalization of conventional phrase-based decoding to handle discontinuities in both source and target phrases. Our system significantly outperforms Moses and Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that thi</context>
</contexts>
<marker>Crego, Yvon, 2009</marker>
<rawString>Josep Crego and Frangois Yvon. 2009. Gappy translation units under left-to-right SMT decoding. In Proc. of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Machine translation as lexicalized parsing with hooks.</title>
<date>2005</date>
<booktitle>In Proc. of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>65--73</pages>
<contexts>
<context position="5665" citStr="Huang et al., 2005" startWordPosition="866" endWordPosition="869">e-based systems can properly handle inside-out alignments when used with a reasonably large distortion limit, and all configurations in Fig. 1 are accounted for in our system. In our experiments, we show that our discontinuous phrase-based system outperforms Joshua (Li et al., 2009), a reimplementation of Hiero, by 1.03 BLEU points and 1.19 TER points on average. A final compelling advantage of our decoder is that it preserves the computational efficiency of Moses (i.e., time complexity is linear when a distortion limit is used), while SCFG decoders have a running time that is at least cubic (Huang et al., 2005). 2 Discontinuous Phrase Extraction In this section, we introduce the extraction of discontinuous phrases for phrase-based MT. We will describe a decoder that can handle such phrases in the next section. Formally, we define the discontinuous phrase-based translation problem as follows. We are given a source sentence f = fJ1 = f1, ... , fj, ... , fJ, which is to be translated into a target sentence e = e�1 = e1, ... , eZ, ... , f j. Unlike (Och and Ney, 2004), in this work, a sentence pair may be segmented into phrases that are not conFigure 2: Due to hierarchical constraints, Hiero only extrac</context>
</contexts>
<marker>Huang, Zhang, Gildea, 2005</marker>
<rawString>Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Machine translation as lexicalized parsing with hooks. In Proc. of the Ninth International Workshop on Parsing Technology, pages 65–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1763" citStr="Koehn et al., 2003" startWordPosition="246" endWordPosition="249">commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne ... pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of w</context>
<context position="13675" citStr="Koehn et al., 2003" startWordPosition="2283" endWordPosition="2286">oblem is a known instance of MT by pattern matching (Lopez, 2007), which we already mentioned in the previous section. The function translation options(m) of Table 1 returns the set of options applicable at position m using this pattern matching algorithm. Since this function is invoked a large number of times, it is important to precompute its return values for each m prior to decoding. 4 Features Our system incorporates the same eight baseline features of Moses: two relative-frequency phrase translation probabilities p(e|f) and p(f|e), two lexicallyweighted phrase translation probabilities (Koehn et al., 2003) lex(e|f) and lex(f|e), a language model probability, word penalty, phrase penalty, and linear distortion, and we optionally add 6 lexicalized reordering features as computed in Moses. Our computation of linear distortion is different from the one in Moses, since we need to account for discontinuous phrases. We found that it is crucial to penalize discontinuous phrases that have relatively long gaps. Hence, in our computation of different stacks depending on the number of isolated phrases, we have not found various implementations of this idea to work better than the algorithm described here. </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch Mayne</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ofACL, Demonstration Session.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="2825" citStr="Koehn et al., 2007" startWordPosition="408" endWordPosition="411">tly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to bin</context>
<context position="16697" citStr="Koehn et al., 2007" startWordPosition="2774" endWordPosition="2777">ross-serial DTU (Søgaard and Kuhn, 2009) and “bonbon” (Simard et al., 2005)) that can’t be handled by 2-SCFG systems. The advantage of such features is two-fold. First, similarly to hierarchical systems, they prevent many distorted reorderings that are unlikely to correspond to quality translations. Second, it imposes soft rather than hard constraints, which means that the decoder is entirely free to violate hierarchical constraints when these violations are supported by other features. last first si−1 + 1 − Si L i i=2 969 5 Experimental Setup Three systems are evaluated in this paper: Moses (Koehn et al., 2007), Joshua (Li et al., 2009) – a reimplementation of Hiero, and our phrase-based system. We made our best attempts to make our system comparable to Moses. That is, when no discontinuous phrases are provided to our system, it generates an output that is almost identical to Moses (only about 1% of translations differ on average). In both systems, we use the default settings of Moses, i.e., we set the beam size to 200, the distortion limit to 6, we limit to 20 the number of target phrases that are loaded for each source phrase, and we use the same default eight features of Moses. We use version 1.3</context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ofACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. ofAMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="2486" citStr="Koehn, 2004" startWordPosition="359" endWordPosition="360">ze (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne ... pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the cur</context>
<context position="9590" citStr="Koehn, 2004" startWordPosition="1579" endWordPosition="1580">8 1 made for this 00000-00 score = -6*1 this state expansions: he said 00 score = -1*3 visit 000000000 score = -8*5 key difference between Hiero-style extraction and our work is that Eq. 1 is the only constraint.1 Since our decoder doesn’t impose hierarchical constraints, we exploit all discontinuous phrase pairs consistent with the word alignment, which often includes sound translations not captured by Hiero (e.g., ne ... plus translating to not ... anymore in Fig. 2). 3 Decoder The core engine of our phrase-based system, Phrasal (Cer et al., 2010), is a multi-stack decoder similar to Moses (Koehn, 2004), which we extended to support variable-size gaps in the source and the target. In Moses, partial translation hypotheses are arranged into different stacks according to the total number of input words they cover. At every translation step, stacks are pruned using partial translation cost and a lower bound on the estimated future cost. Pruning is implemented using both threshold and histogram pruning, and Moses allows for hypothesis recombination between hypotheses that are indistinguishable according to the underlying models. The key difference between Moses and our system is that, in order to</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proc. ofAMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: an open source toolkit for parsingbased MT.</title>
<date>2009</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="5329" citStr="Li et al., 2009" startWordPosition="806" endWordPosition="809">ound that insideout and cross-serial discontinuous translation units (DTU) account for 1.6% (Danish-English) to 18.6% (French-English) of all translation units. The empirical adequacy of 2-SCFG models would presumably be lower with automatically-aligned texts and if the study also included non-European languages. In contrast, phrase-based systems can properly handle inside-out alignments when used with a reasonably large distortion limit, and all configurations in Fig. 1 are accounted for in our system. In our experiments, we show that our discontinuous phrase-based system outperforms Joshua (Li et al., 2009), a reimplementation of Hiero, by 1.03 BLEU points and 1.19 TER points on average. A final compelling advantage of our decoder is that it preserves the computational efficiency of Moses (i.e., time complexity is linear when a distortion limit is used), while SCFG decoders have a running time that is at least cubic (Huang et al., 2005). 2 Discontinuous Phrase Extraction In this section, we introduce the extraction of discontinuous phrases for phrase-based MT. We will describe a decoder that can handle such phrases in the next section. Formally, we define the discontinuous phrase-based translati</context>
<context position="16723" citStr="Li et al., 2009" startWordPosition="2779" endWordPosition="2782">Kuhn, 2009) and “bonbon” (Simard et al., 2005)) that can’t be handled by 2-SCFG systems. The advantage of such features is two-fold. First, similarly to hierarchical systems, they prevent many distorted reorderings that are unlikely to correspond to quality translations. Second, it imposes soft rather than hard constraints, which means that the decoder is entirely free to violate hierarchical constraints when these violations are supported by other features. last first si−1 + 1 − Si L i i=2 969 5 Experimental Setup Three systems are evaluated in this paper: Moses (Koehn et al., 2007), Joshua (Li et al., 2009) – a reimplementation of Hiero, and our phrase-based system. We made our best attempts to make our system comparable to Moses. That is, when no discontinuous phrases are provided to our system, it generates an output that is almost identical to Moses (only about 1% of translations differ on average). In both systems, we use the default settings of Moses, i.e., we set the beam size to 200, the distortion limit to 6, we limit to 20 the number of target phrases that are loaded for each source phrase, and we use the same default eight features of Moses. We use version 1.3 of Joshua with its defaul</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an open source toolkit for parsingbased MT. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="18040" citStr="Liang et al., 2006" startWordPosition="3003" endWordPosition="3006">ann, 2004).4 We believe it to be fair to compare Joshua against phrase-based systems that exploit lexicalized reordering, since Hiero’s hierarchical rules are also lexically sensitive.5 The language pair for our experiments is Chineseto-English. The training data consists of about 28 million English words and 23.3 million Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC). In order to provide experiments comparable to previous work, we used the same corpora as (Wang et al., 2007). We performed word alignment using a crossEM word aligner (Liang et al., 2006). For this, we ran two iterations of IBM Model 1 and two HMM iterations. Finally, we generated a symmetric word alignment from cross-EM Viterbi alignment using the Moses grow-diag heuristic in the case Moses and our system. In the case of Joshua, we used the growdiag-final heuristic since this gave better results. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections 4We use Moses’ default orientations: monotone, swap, and discontinuous. As far as this reordering model is concerned, we treat disconti</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proc. of HLT-NAACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>976--985</pages>
<contexts>
<context position="2880" citStr="Lopez, 2007" startWordPosition="420" endWordPosition="421">eyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems s</context>
<context position="7291" citStr="Lopez (2007)" startWordPosition="1172" endWordPosition="1173">word subsets that are complementary and nonoverlapping. A pair of coverage sets (sk, tk) is said to be consistent with the word alignment A if the following condition holds: b(i,j) E A : i E sk H j E tk (1) For continuous phrases, finding all phrase pairs that satisfy this condition can be done in O(nm3) time (Och and Ney, 2004), where n is the length of the sentence and m is the maximum phrase length. The set of discontinuous phrases is exponential in the maximum span length, so phrase extraction must be tailored to a specific text (e.g., a given test sentence) for relatively large m values. Lopez (2007) presents an efficient solution using suffix arrays for finding all discontinuous phrases of the training data that are relevant to a given test sentence or test set. A complete overview of this technique is beyond the scope of this paper, though we will mention that it solves a phrase collocation problem by efficiently identifying collocated continuous phrases of the training data that also happen to be collocated in the test sentence. While this technique was primarily designed for extracting hierarchical phrases for Hiero (Chiang, 2007), it can readily be applied to the problem of finding a</context>
<context position="13121" citStr="Lopez, 2007" startWordPosition="2197" endWordPosition="2198">) and append them in any order at the end of the current translation.3 Consolidation operations are marked with stars in the figure (for simplicity, the figure does not display consolidations that keep hypotheses unchanged). We limit the number of isolated phrases to 4, which is generally enough to account for most transformations seen in the data. Any hypothesis in the last beam SgJ is automatically discarded if it contains any isolated phrase. One last difference with standard decoders is that we also handle source discontinuities. This problem is a known instance of MT by pattern matching (Lopez, 2007), which we already mentioned in the previous section. The function translation options(m) of Table 1 returns the set of options applicable at position m using this pattern matching algorithm. Since this function is invoked a large number of times, it is important to precompute its return values for each m prior to decoding. 4 Features Our system incorporates the same eight baseline features of Moses: two relative-frequency phrase translation probabilities p(e|f) and p(f|e), two lexicallyweighted phrase translation probabilities (Koehn et al., 2003) lex(e|f) and lex(f|e), a language model proba</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proc. of EMNLP-CoNLL, pages 976–985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="18881" citStr="Lopez, 2008" startWordPosition="3141" endWordPosition="3142">e case of Joshua, we used the growdiag-final heuristic since this gave better results. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections 4We use Moses’ default orientations: monotone, swap, and discontinuous. As far as this reordering model is concerned, we treat discontinuous phrases as continuous, i.e., we simply ignore what lies within gaps to determine phrase orientation. 5(Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. As noted in (Lopez, 2008), Hiero can represent the same information with hierarchical rules of the form uX, Xu, and XuX. Hiero actually models lexicalized reordering patterns that (Tillmann, 2004) does not account for, e.g., a transformation from X1uX2v to X2u′v′X1. of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as impl</context>
<context position="27534" citStr="Lopez, 2008" startWordPosition="4575" endWordPosition="4576">nese sentence includes the topicalization word �&apos;j(for), which indicates the input sentence has no subject. One way to properly handle this translation is to turn the sentence into a passive in English (as in the reference), a transformation our system does, thanks to its support for complex reorderings. Our main results are displayed in Table 2. First, Joshua systematically outperforms the Moses baseline (+0.82 BLEU point and −0.92 TER point on average), but performance of the two is about the same when Moses incorporates lexicalized reordering. This finding is consistent with previous work (Lopez, 2008). The results of our system displayed in rows 5–8 demonstrate that our system consistently outperforms Moses, whether they both use lexicalized reordering or not. The performance of our best system—i.e., with lexicalized reordering and both source and target gaps—is significantly better than the best Moses system (+0.77 BLEU and −1.53 TER). While the performance of our system without lexicalized reordering is close to that of Joshua, our system with lexicalized reordering significantly outperforms Joshua (p &lt; .01) in 9 out of 10 evaluations. The single experiment where our improvement over Hie</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Tera-scale translation models via pattern matching. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari`oo</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>N-gram-based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>549</pages>
<marker>Mari`oo, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>Jos´e B. Mari`oo, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos´e A. R. Fonollosa, and Marta R. Costa-juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527– 549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="32823" citStr="Marton and Resnik, 2008" startWordPosition="5420" endWordPosition="5423">ly outperforms Moses and Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that this turns out to also be the case with the hierarchical phrases of Joshua. In future work, we plan to extend the parameterization of phrasebased lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (Marton and Resnik, 2008; Chiang et al., 2009). The discontinuous phrase-based MT system described in this work is part of Phrasal, an opensource phrase-based system available for download at http://nlp.stanford.edu/software/phrasal. Acknowledgements The authors thank three anonymous reviewers, Dan Jurafsky, Spence Green, Steven Bethard, Daniel Cer, Chris Callison-Burch, and Pi-Chuan Chang for their helpful comments. This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement sho</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proc. of ACL, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1364" citStr="Och and Ney, 2004" startWordPosition="180" endWordPosition="183">3 BLEU on average on five ChineseEnglish NIST test sets), even though both Joshua and our system support discontinuous phrases. Since the key difference between these two systems is that ours is not hierarchical—i.e., our system uses a string-based decoder instead of CKY, and it imposes no hard hierarchical reordering constraints during training and decoding—this paper sets out to challenge the commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test ti</context>
<context position="6127" citStr="Och and Ney, 2004" startWordPosition="954" endWordPosition="957">Moses (i.e., time complexity is linear when a distortion limit is used), while SCFG decoders have a running time that is at least cubic (Huang et al., 2005). 2 Discontinuous Phrase Extraction In this section, we introduce the extraction of discontinuous phrases for phrase-based MT. We will describe a decoder that can handle such phrases in the next section. Formally, we define the discontinuous phrase-based translation problem as follows. We are given a source sentence f = fJ1 = f1, ... , fj, ... , fJ, which is to be translated into a target sentence e = e�1 = e1, ... , eZ, ... , f j. Unlike (Och and Ney, 2004), in this work, a sentence pair may be segmented into phrases that are not conFigure 2: Due to hierarchical constraints, Hiero only extracts two discontinuous phrases from the alignment on the left, but our system extracts 11 (only 6 are shown). tinuous, so each phrase is characterized by a coverage set, i.e., a set of word indices. Assuming that the sentence pair (f, e) is decomposed into K discontinuous phrases, we use s = (s1, ... , sK) and t = (t1, ... , tK) to respectively represent the decomposition of the source and target sentence into K word subsets that are complementary and nonoverl</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="19951" citStr="Och, 2003" startWordPosition="3321" endWordPosition="3322">ods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in SRILM (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data. For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translation</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="20014" citStr="Papineni et al., 2001" startWordPosition="3330" endWordPosition="3334">test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in SRILM (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data. For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translations of our development set MT06, which were selected because our </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proc. of Workshop on Evaluation Measures,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="26074" citStr="Riezler and Maxwell, 2005" startWordPosition="4336" endWordPosition="4339">.25 phrase-based 8 src+tgt yes 35.00 56.85 34.96 57.97 37.44 57.61 33.39 59.92 26.74 60.51 32.93 57.03 (this work) Improvement over hierarchical +1.16 −1.26 +1.49 −1.88 +1.34 −1.21 +1.22 −1.28 +0.13 −0.70 +1.03 −1.19 Improvement over phrase-based +0.75 −1.38 +1.24 −2.45 +1.07 −1.57 +0.90 −1.88 +0.04 −0.97 +0.77 −1.53 Number of sentences 1664 919 1788 1082 1357 6810 Table 2: Our system compared again conventional and hierarchical phrase-based MT (Moses and Joshua). using uncased BLEUr4n4[%] and TER[%]. LexR indicates whether lexicalized reordering is enabled or not. We use randomization tests (Riezler and Maxwell, 2005) to determine significance of our best results (row 8) against Joshua (row 2) and Moses (row 4): differences marked in bold are significant at the p &lt; .01 level. that larger translation units, including discontinuous phrases, lead to better translations. The reference includes the translation enlarge strength of supervision of leading cadres, and our system is able to produce a translation that is almost identical (increase the intensity of supervision of leading cadres) using only two phrases, pulling together input words that are fairly far apart in the sentence. The third Chinese sentence h</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proc. of Workshop on Evaluation Measures, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Nicola Cancedda</author>
<author>Bruno Cavestro</author>
<author>Marc Dymetman</author>
</authors>
<title>Eric Gaussier, Cyril Goutte,</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP,</booktitle>
<pages>755--762</pages>
<location>Kenji Yamada, Philippe Langlais, and</location>
<contexts>
<context position="4357" citStr="Simard et al., 2005" startWordPosition="660" endWordPosition="663">work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics bm dn ap ct target: ak am b� bn ai bj ak bi am bn (i) s�urce: a� bj ck di (ii) ai bj (iii) Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009); (iii) “bonbon” (Simard et al., 2005). Standard phrasebased decoders cope with (i), but not (ii) and (iii). Our phrase-based decoder handles all three cases. Wu, 2009) has questioned the empirical adequacy of 2-SCFG systems, which are unable to perform any of the transformations shown in Fig. 1. For instance, using manually-aligned bitexts for 12 European languages pairs, Søgaard and Kuhn found that insideout and cross-serial discontinuous translation units (DTU) account for 1.6% (Danish-English) to 18.6% (French-English) of all translation units. The empirical adequacy of 2-SCFG models would presumably be lower with automaticall</context>
<context position="16153" citStr="Simard et al., 2005" startWordPosition="2684" endWordPosition="2687">ree continuous subphrases. In practice, we compute intra-phrase (shown with thin arrows in the figure) and inter-phrase linear distortion separately in order to produce two distinct features, since translation tends to improves when the intra-phrase cost has a lower feature weight. Finally, we add two features that are not present in Moses. First, we penalize target discontinuities by including a feature that is the sum of the lengths of all target gaps. The second feature is the count of discontinuous phrases that are in configurations (cross-serial DTU (Søgaard and Kuhn, 2009) and “bonbon” (Simard et al., 2005)) that can’t be handled by 2-SCFG systems. The advantage of such features is two-fold. First, similarly to hierarchical systems, they prevent many distorted reorderings that are unlikely to correspond to quality translations. Second, it imposes soft rather than hard constraints, which means that the decoder is entirely free to violate hierarchical constraints when these violations are supported by other features. last first si−1 + 1 − Si L i i=2 969 5 Experimental Setup Three systems are evaluated in this paper: Moses (Koehn et al., 2007), Joshua (Li et al., 2009) – a reimplementation of Hiero</context>
<context position="31308" citStr="Simard et al., 2005" startWordPosition="5181" endWordPosition="5184">l., 2006) exploit context-free decoding algorithms (CKY, Earley, etc.) to cope with discontinuities, our system preserves the simplicity and speed of conventional phrase-based decoders, and in particular does not build any intermediate tree structure, does not impose any hard reordering constraints other than the distortion limit, and still achieves translation performance that is superior to that of a state-of-the-art hierarchical system. A few previous non-hierarchical systems have also exploited phrasal discontinuities. The most notable previous attempt to incorporate gaps is described in (Simard et al., 2005). Simard et al. presents an extension to Moses that allows gaps in both source and target phrases, though each of their gap symbols must span exactly one word. This fact makes decoding simpler, since the position of all target words in a translation hypothesis is known as soon as the hypothesis is laid down, but fixed-size discontinuous phrases are less general and increase sparsity. By comparison, our gaps may span any number of words, so we have an increased ability to flexibly match the input sentence effectively. (Crego and Yvon, 2009) also handles gaps, though this work is applicable to a</context>
</contexts>
<marker>Simard, Cancedda, Cavestro, Dymetman, 2005</marker>
<rawString>Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada, Philippe Langlais, and Arne Mauser. 2005. Translating with non-contiguous phrases. In Proc. of HLT-EMNLP, pages 755–762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="20304" citStr="Snover et al., 2006" startWordPosition="3379" endWordPosition="3382">nese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translations of our development set MT06, which were selected because our system makes a crucial use of discontinuous phrases. In the first example, the Chinese input contains&apos;�...Hj, which typically translates as when. Lacking an entry for the input phrase Hj in its phrase table, Moses is unable to translate this segment appropriately, and must instead split th</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of AMTA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Jonas Kuhn</author>
</authors>
<title>Empirical lower bounds on alignment error rates in syntax-based machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Third Workshop on Syntax and Structure in Statistical Translation (SSST3) at NAACL HLT</booktitle>
<pages>pages</pages>
<contexts>
<context position="3790" citStr="Søgaard and Kuhn, 2009" startWordPosition="562" endWordPosition="565"> partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics bm dn ap ct target: ak am b� bn ai bj ak bi am bn (i) s�urce: a� bj ck di (ii) ai bj (iii) Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009); (iii) “bonbon” (Simard et al., 2005). Standard phrasebased decoders c</context>
<context position="16118" citStr="Søgaard and Kuhn, 2009" startWordPosition="2678" endWordPosition="2681"> discontinuous phrase s2 split into three continuous subphrases. In practice, we compute intra-phrase (shown with thin arrows in the figure) and inter-phrase linear distortion separately in order to produce two distinct features, since translation tends to improves when the intra-phrase cost has a lower feature weight. Finally, we add two features that are not present in Moses. First, we penalize target discontinuities by including a feature that is the sum of the lengths of all target gaps. The second feature is the count of discontinuous phrases that are in configurations (cross-serial DTU (Søgaard and Kuhn, 2009) and “bonbon” (Simard et al., 2005)) that can’t be handled by 2-SCFG systems. The advantage of such features is two-fold. First, similarly to hierarchical systems, they prevent many distorted reorderings that are unlikely to correspond to quality translations. Second, it imposes soft rather than hard constraints, which means that the decoder is entirely free to violate hierarchical constraints when these violations are supported by other features. last first si−1 + 1 − Si L i i=2 969 5 Experimental Setup Three systems are evaluated in this paper: Moses (Koehn et al., 2007), Joshua (Li et al., </context>
<context position="21251" citStr="Søgaard and Kuhn, 2009" startWordPosition="3529" endWordPosition="3532"> crucial use of discontinuous phrases. In the first example, the Chinese input contains&apos;�...Hj, which typically translates as when. Lacking an entry for the input phrase Hj in its phrase table, Moses is unable to translate this segment appropriately, and must instead split this phrase to generate the translation when the right was deprived of, where Hj is translated into of. This is evidently a poor translation. Conversely, our system uses a discontinuous phrase to translate&apos;�...Hj, and translates the intervening words separately. The remaining three translations all contain crossserial DTUs (Søgaard and Kuhn, 2009) and thus would be difficult to generate using 2-SCFG systems. The second example motivates the idea 970 t tap-V aME (AÍÅµ_F 1!1 ,� , it right to life , Hf , when he was deprived of the can only resort to violence ... under such circumstances ��4j7 C, �f_€&apos;z ... MT06 — segment 1589 Reference: Under such circumstances, when the right of existence was deprived, the only way remaining was to overthrow the existing dynasty by force and try to replace it. Joshua: Under such circumstances, when life be deprived, can only resort to violence to overthrow the current dynasty, trying to replace, Moses: </context>
</contexts>
<marker>Søgaard, Kuhn, 2009</marker>
<rawString>Anders Søgaard and Jonas Kuhn. 2009. Empirical lower bounds on alignment error rates in syntax-based machine translation. In Proc. of the Third Workshop on Syntax and Structure in Statistical Translation (SSST3) at NAACL HLT 2009, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Dekai Wu</author>
</authors>
<title>Empirical lower bounds on translation unit error rate for the full class of inversion transduction grammars.</title>
<date>2009</date>
<booktitle>In Proc. of IWPT,</booktitle>
<pages>33--36</pages>
<marker>Søgaard, Wu, 2009</marker>
<rawString>Anders Søgaard and Dekai Wu. 2009. Empirical lower bounds on translation unit error rate for the full class of inversion transduction grammars. In Proc. of IWPT, pages 33–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="19513" citStr="Stolcke, 2002" startWordPosition="3244" endWordPosition="3245">ent the same information with hierarchical rules of the form uX, Xu, and XuX. Hiero actually models lexicalized reordering patterns that (Tillmann, 2004) does not account for, e.g., a transformation from X1uX2v to X2u′v′X1. of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in SRILM (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data. For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tu</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. of ICSLP, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of HLTNAACL,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="17431" citStr="Tillmann, 2004" startWordPosition="2908" endWordPosition="2909">e our system comparable to Moses. That is, when no discontinuous phrases are provided to our system, it generates an output that is almost identical to Moses (only about 1% of translations differ on average). In both systems, we use the default settings of Moses, i.e., we set the beam size to 200, the distortion limit to 6, we limit to 20 the number of target phrases that are loaded for each source phrase, and we use the same default eight features of Moses. We use version 1.3 of Joshua with its default settings. Both Moses and our system are evaluated with and without lexicalized reordering (Tillmann, 2004).4 We believe it to be fair to compare Joshua against phrase-based systems that exploit lexicalized reordering, since Hiero’s hierarchical rules are also lexically sensitive.5 The language pair for our experiments is Chineseto-English. The training data consists of about 28 million English words and 23.3 million Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC). In order to provide experiments comparable to previous work, we used the same corpora as (Wang et al., 2007). We performed word alignment using a crossEM word aligner (Liang et a</context>
<context position="18764" citStr="Tillmann, 2004" startWordPosition="3119" endWordPosition="3120">rd alignment from cross-EM Viterbi alignment using the Moses grow-diag heuristic in the case Moses and our system. In the case of Joshua, we used the growdiag-final heuristic since this gave better results. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections 4We use Moses’ default orientations: monotone, swap, and discontinuous. As far as this reordering model is concerned, we treat discontinuous phrases as continuous, i.e., we simply ignore what lies within gaps to determine phrase orientation. 5(Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. As noted in (Lopez, 2008), Hiero can represent the same information with hierarchical rules of the form uX, Xu, and XuX. Hiero actually models lexicalized reordering patterns that (Tillmann, 2004) does not account for, e.g., a transformation from X1uX2v to X2u′v′X1. of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that overlap with t</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proc. of HLTNAACL, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="17961" citStr="Wang et al., 2007" startWordPosition="2989" endWordPosition="2992">es and our system are evaluated with and without lexicalized reordering (Tillmann, 2004).4 We believe it to be fair to compare Joshua against phrase-based systems that exploit lexicalized reordering, since Hiero’s hierarchical rules are also lexically sensitive.5 The language pair for our experiments is Chineseto-English. The training data consists of about 28 million English words and 23.3 million Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC). In order to provide experiments comparable to previous work, we used the same corpora as (Wang et al., 2007). We performed word alignment using a crossEM word aligner (Liang et al., 2006). For this, we ran two iterations of IBM Model 1 and two HMM iterations. Finally, we generated a symmetric word alignment from cross-EM Viterbi alignment using the Moses grow-diag heuristic in the case Moses and our system. In the case of Joshua, we used the growdiag-final heuristic since this gave better results. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections 4We use Moses’ default orientations: monotone, swap, and</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="30697" citStr="Watanabe et al., 2006" startWordPosition="5091" endWordPosition="5095">its disposal: While the Moses baseline translates MT06 with an average 1.73 words per phrase, adding support for discontinuities increases this average to 2.16, and reduces by 43% the use of single word phrases. On MT06, 53% of the translated sentences produced by our best system use at least one source-discontinuous phrase, and 9% of them exploit one or more target-discontinuous phrases. 7 Related Work The main goal of this paper is to show that discontinuous phrases can greatly improve the performance of phrase-based systems. While some of the most recent phrase-based systems (Chiang, 2007; Watanabe et al., 2006) exploit context-free decoding algorithms (CKY, Earley, etc.) to cope with discontinuities, our system preserves the simplicity and speed of conventional phrase-based decoders, and in particular does not build any intermediate tree structure, does not impose any hard reordering constraints other than the distortion limit, and still achieves translation performance that is superior to that of a state-of-the-art hierarchical system. A few previous non-hierarchical systems have also exploited phrasal discontinuities. The most notable previous attempt to incorporate gaps is described in (Simard et</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<title>Empirical lower bounds on the complexity of translational equivalence.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>977--984</pages>
<contexts>
<context position="3766" citStr="Wellington et al., 2006" startWordPosition="558" endWordPosition="561">at may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics bm dn ap ct target: ak am b� bn ai bj ak bi am bn (i) s�urce: a� bj ck di (ii) ai bj (iii) Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009); (iii) “bonbon” (Simard et al., 2005). Standar</context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical lower bounds on the complexity of translational equivalence. In Proc. of COLING-ACL, pages 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4271" citStr="Wu, 1997" startWordPosition="649" endWordPosition="650">impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics bm dn ap ct target: ak am b� bn ai bj ak bi am bn (i) s�urce: a� bj ck di (ii) ai bj (iii) Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009); (iii) “bonbon” (Simard et al., 2005). Standard phrasebased decoders cope with (i), but not (ii) and (iii). Our phrase-based decoder handles all three cases. Wu, 2009) has questioned the empirical adequacy of 2-SCFG systems, which are unable to perform any of the transformations shown in Fig. 1. For instance, using manually-aligned bitexts for 12 European languages pairs, Søgaard and Kuhn found that insideout and cross-serial discontinuous translation units (DTU) account for 1.6% (Danish-English) to 18.6% (French-English) of all translation unit</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>An efficient phrase-to-phrase alignment model for arbitrarily long phrase and large corpora.</title>
<date>2005</date>
<booktitle>In Proc. of EAMT.</booktitle>
<contexts>
<context position="1899" citStr="Zhang and Vogel, 2005" startWordPosition="268" endWordPosition="271">inst Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne ... pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow var</context>
</contexts>
<marker>Zhang, Vogel, 2005</marker>
<rawString>Ying Zhang and Stephan Vogel. 2005. An efficient phrase-to-phrase alignment model for arbitrarily long phrase and large corpora. In Proc. of EAMT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>